repo_name,file_path,file_html,class_name,me_name,me_code,old_code,chatGPT_code,element_str,slice_str,truth_code
mars,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/mars/learn/datasets/samples_generator.py,https://github.com/mars-project/mars/tree/master/mars/learn/datasets/samples_generator.py,,make_blobs$404,"def make_blobs(
    n_samples=100,
    n_features=2,
    centers=None,
    cluster_std=1.0,
    center_box=(-10.0, 10.0),
    shuffle=True,
    random_state=None,
):
    """"""Generate isotropic Gaussian blobs for clustering.

    Read more in the :ref:`User Guide <sample_generators>`.

    Parameters
    ----------
    n_samples : int or array-like, optional (default=100)
        If int, it is the total number of points equally divided among
        clusters.
        If array-like, each element of the sequence indicates
        the number of samples per cluster.

    n_features : int, optional (default=2)
        The number of features for each sample.

    centers : int or array of shape [n_centers, n_features], optional
        (default=None)
        The number of centers to generate, or the fixed center locations.
        If n_samples is an int and centers is None, 3 centers are generated.
        If n_samples is array-like, centers must be
        either None or an array of length equal to the length of n_samples.

    cluster_std : float or sequence of floats, optional (default=1.0)
        The standard deviation of the clusters.

    center_box : pair of floats (min, max), optional (default=(-10.0, 10.0))
        The bounding box for each cluster center when centers are
        generated at random.

    shuffle : boolean, optional (default=True)
        Shuffle the samples.

    random_state : int, RandomState instance or None (default)
        Determines random number generation for dataset creation. Pass an int
        for reproducible output across multiple function calls.
        See :term:`Glossary <random_state>`.

    Returns
    -------
    X : tensor of shape [n_samples, n_features]
        The generated samples.

    y : tensor of shape [n_samples]
        The integer labels for cluster membership of each sample.

    Examples
    --------
    >>> from sklearn.datasets import make_blobs
    >>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,
    ...                   random_state=0)
    >>> print(X.shape)
    (10, 2)
    >>> y
    array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])
    >>> X, y = make_blobs(n_samples=[3, 3, 4], centers=None, n_features=2,
    ...                   random_state=0)
    >>> print(X.shape)
    (10, 2)
    >>> y
    array([0, 1, 2, 0, 2, 2, 2, 1, 1, 0])

    See also
    --------
    make_classification: a more intricate variant
    """"""
    from ..utils.checks import AssertAllFinite

    generator = check_random_state(random_state)

    if isinstance(n_samples, numbers.Integral):
        # Set n_centers by looking at centers arg
        if centers is None:
            centers = 3

        if isinstance(centers, numbers.Integral):
            n_centers = centers
            centers = generator.uniform(
                center_box[0], center_box[1], size=(n_centers, n_features)
            )

        else:
            centers = check_array(centers)
            n_features = centers.shape[1]
            n_centers = centers.shape[0]

    else:
        # Set n_centers by looking at [n_samples] arg
        n_centers = len(n_samples)
        if centers is None:
            centers = generator.uniform(
                center_box[0], center_box[1], size=(n_centers, n_features)
            )
        try:
            assert len(centers) == n_centers
        except TypeError:
            raise ValueError(
                ""Parameter `centers` must be array-like. "" f""Got {centers!r} instead""
            )
        except AssertionError:
            raise ValueError(
                ""Length of `n_samples` not consistent""
                f"" with number of centers. Got n_samples = {n_samples} ""
                f""and centers = {centers}""
            )
        else:
            centers = check_array(centers)
            n_features = centers.shape[1]

    # stds: if cluster_std is given as list, it must be consistent
    # with the n_centers
    if hasattr(cluster_std, ""__len__"") and len(cluster_std) != n_centers:
        if isinstance(centers.op, AssertAllFinite):
            centers = centers.op.inputs[0]
        raise ValueError(
            ""Length of `clusters_std` not consistent with ""
            f""number of centers. Got centers = {centers} ""
            f""and cluster_std = {cluster_std}""
        )

    if isinstance(cluster_std, numbers.Real):
        cluster_std = mt.full(len(centers), cluster_std)

    X = []
    y = []

    if isinstance(n_samples, Iterable):
        n_samples_per_center = n_samples
    else:
        n_samples_per_center = [int(n_samples // n_centers)] * n_centers

        for i in range(n_samples % n_centers):
            n_samples_per_center[i] += 1

    for i, (n, std) in enumerate(zip(n_samples_per_center, cluster_std)):
        if n == 0:
            continue
        X.append(generator.normal(loc=centers[i], scale=std, size=(n, n_features)))
        y += [i] * n

    X = mt.concatenate(X)
    y = mt.array(y)

    if shuffle:
        X, y = util_shuffle(X, y, random_state=generator)

    return X, y","generator.uniform(center_box[0], center_box[1], size=(n_centers, n_features))","generator.uniform(*center_box[:2], size=(n_centers, n_features))","iterable_zj[0], iterable_zj[1]",*center_box[:2],*center_box[:2],1
mars,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/mars/learn/datasets/samples_generator.py,https://github.com/mars-project/mars/tree/master/mars/learn/datasets/samples_generator.py,,make_blobs$404,"def make_blobs(
    n_samples=100,
    n_features=2,
    centers=None,
    cluster_std=1.0,
    center_box=(-10.0, 10.0),
    shuffle=True,
    random_state=None,
):
    """"""Generate isotropic Gaussian blobs for clustering.

    Read more in the :ref:`User Guide <sample_generators>`.

    Parameters
    ----------
    n_samples : int or array-like, optional (default=100)
        If int, it is the total number of points equally divided among
        clusters.
        If array-like, each element of the sequence indicates
        the number of samples per cluster.

    n_features : int, optional (default=2)
        The number of features for each sample.

    centers : int or array of shape [n_centers, n_features], optional
        (default=None)
        The number of centers to generate, or the fixed center locations.
        If n_samples is an int and centers is None, 3 centers are generated.
        If n_samples is array-like, centers must be
        either None or an array of length equal to the length of n_samples.

    cluster_std : float or sequence of floats, optional (default=1.0)
        The standard deviation of the clusters.

    center_box : pair of floats (min, max), optional (default=(-10.0, 10.0))
        The bounding box for each cluster center when centers are
        generated at random.

    shuffle : boolean, optional (default=True)
        Shuffle the samples.

    random_state : int, RandomState instance or None (default)
        Determines random number generation for dataset creation. Pass an int
        for reproducible output across multiple function calls.
        See :term:`Glossary <random_state>`.

    Returns
    -------
    X : tensor of shape [n_samples, n_features]
        The generated samples.

    y : tensor of shape [n_samples]
        The integer labels for cluster membership of each sample.

    Examples
    --------
    >>> from sklearn.datasets import make_blobs
    >>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,
    ...                   random_state=0)
    >>> print(X.shape)
    (10, 2)
    >>> y
    array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])
    >>> X, y = make_blobs(n_samples=[3, 3, 4], centers=None, n_features=2,
    ...                   random_state=0)
    >>> print(X.shape)
    (10, 2)
    >>> y
    array([0, 1, 2, 0, 2, 2, 2, 1, 1, 0])

    See also
    --------
    make_classification: a more intricate variant
    """"""
    from ..utils.checks import AssertAllFinite

    generator = check_random_state(random_state)

    if isinstance(n_samples, numbers.Integral):
        # Set n_centers by looking at centers arg
        if centers is None:
            centers = 3

        if isinstance(centers, numbers.Integral):
            n_centers = centers
            centers = generator.uniform(
                center_box[0], center_box[1], size=(n_centers, n_features)
            )

        else:
            centers = check_array(centers)
            n_features = centers.shape[1]
            n_centers = centers.shape[0]

    else:
        # Set n_centers by looking at [n_samples] arg
        n_centers = len(n_samples)
        if centers is None:
            centers = generator.uniform(
                center_box[0], center_box[1], size=(n_centers, n_features)
            )
        try:
            assert len(centers) == n_centers
        except TypeError:
            raise ValueError(
                ""Parameter `centers` must be array-like. "" f""Got {centers!r} instead""
            )
        except AssertionError:
            raise ValueError(
                ""Length of `n_samples` not consistent""
                f"" with number of centers. Got n_samples = {n_samples} ""
                f""and centers = {centers}""
            )
        else:
            centers = check_array(centers)
            n_features = centers.shape[1]

    # stds: if cluster_std is given as list, it must be consistent
    # with the n_centers
    if hasattr(cluster_std, ""__len__"") and len(cluster_std) != n_centers:
        if isinstance(centers.op, AssertAllFinite):
            centers = centers.op.inputs[0]
        raise ValueError(
            ""Length of `clusters_std` not consistent with ""
            f""number of centers. Got centers = {centers} ""
            f""and cluster_std = {cluster_std}""
        )

    if isinstance(cluster_std, numbers.Real):
        cluster_std = mt.full(len(centers), cluster_std)

    X = []
    y = []

    if isinstance(n_samples, Iterable):
        n_samples_per_center = n_samples
    else:
        n_samples_per_center = [int(n_samples // n_centers)] * n_centers

        for i in range(n_samples % n_centers):
            n_samples_per_center[i] += 1

    for i, (n, std) in enumerate(zip(n_samples_per_center, cluster_std)):
        if n == 0:
            continue
        X.append(generator.normal(loc=centers[i], scale=std, size=(n, n_features)))
        y += [i] * n

    X = mt.concatenate(X)
    y = mt.array(y)

    if shuffle:
        X, y = util_shuffle(X, y, random_state=generator)

    return X, y","generator.uniform(center_box[0], center_box[1], size=(n_centers, n_features))","generator.uniform(*center_box[:2], size=(n_centers, n_features))","iterable_zj[0], iterable_zj[1]",*center_box[:2],*center_box[:2],1
ActualVim,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ActualVim/lib/neovim/msgpack_rpc/async_session.py,https://github.com/lunixbochs/ActualVim/tree/master/lib/neovim/msgpack_rpc/async_session.py,AsyncSession,_on_response$86,"def _on_response(self, msg):
        # response to a previous request:
        #   - msg[1]: the id
        #   - msg[2]: error(if any)
        #   - msg[3]: result(if not errored)
        with self._lock:
            self._pending_requests.pop(msg[1])(msg[2], msg[3])","self._pending_requests.pop(msg[1])(msg[2], msg[3])",self._pending_requests.pop(msg[1])(*msg[2:4]),"iterable_zj[2], iterable_zj[3]",*msg[2:4],*msg[2:4],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(center[0], center[1])",diffvg.Vector2f(*center[:2]),"iterable_zj[0], iterable_zj[1]",*center[:2],*center[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector4f(color[0], color[1], color[2], color[3])","diffvg.Vector4f(*color[:3], color[3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*color[:3],*color[:4],0
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector4f(color[0], color[1], color[2], color[3])","diffvg.Vector4f(color[0], *color[1:4])","iterable_zj[1], iterable_zj[2], iterable_zj[3]",*color[1:4],*color[:4],0
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(radius[0], radius[1])",diffvg.Vector2f(*radius[:2]),"iterable_zj[0], iterable_zj[1]",*radius[:2],*radius[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(center[0], center[1])",diffvg.Vector2f(*center[:2]),"iterable_zj[0], iterable_zj[1]",*center[:2],*center[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(beg[0], beg[1])",diffvg.Vector2f(*beg[:2]),"iterable_zj[0], iterable_zj[1]",*beg[:2],*beg[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(end[0], end[1])",diffvg.Vector2f(*end[:2]),"iterable_zj[0], iterable_zj[1]",*end[:2],*end[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(beg[0], beg[1])",diffvg.Vector2f(*beg[:2]),"iterable_zj[0], iterable_zj[1]",*beg[:2],*beg[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(end[0], end[1])",diffvg.Vector2f(*end[:2]),"iterable_zj[0], iterable_zj[1]",*end[:2],*end[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","torch.ones(background_image.shape[0], background_image.shape[1], 1, device=background_image.device)","torch.ones(*background_image.shape[:2], 1, device=background_image.device)","iterable_zj[0], iterable_zj[1]",*background_image.shape[:2],*background_image.shape[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(center[0], center[1])",diffvg.Vector2f(*center[:2]),"iterable_zj[0], iterable_zj[1]",*center[:2],*center[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(radius[0], radius[1])",diffvg.Vector2f(*radius[:2]),"iterable_zj[0], iterable_zj[1]",*radius[:2],*radius[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(center[0], center[1])",diffvg.Vector2f(*center[:2]),"iterable_zj[0], iterable_zj[1]",*center[:2],*center[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(radius[0], radius[1])",diffvg.Vector2f(*radius[:2]),"iterable_zj[0], iterable_zj[1]",*radius[:2],*radius[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(p_min[0], p_min[1])",diffvg.Vector2f(*p_min[:2]),"iterable_zj[0], iterable_zj[1]",*p_min[:2],*p_min[:2],1
diffvg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diffvg/pydiffvg/render_pytorch.py,https://github.com/BachiLi/diffvg/tree/master/pydiffvg/render_pytorch.py,RenderFunction,forward$175,"def forward(ctx,
                width,
                height,
                num_samples_x,
                num_samples_y,
                seed,
                background_image,
                *args):
        """"""
            Forward rendering pass.
        """"""
        # Unpack arguments
        current_index = 0
        canvas_width = args[current_index]
        current_index += 1
        canvas_height = args[current_index]
        current_index += 1
        num_shapes = args[current_index]
        current_index += 1
        num_shape_groups = args[current_index]
        current_index += 1
        output_type = args[current_index]
        current_index += 1
        use_prefiltering = args[current_index]
        current_index += 1
        eval_positions = args[current_index]
        current_index += 1
        shapes = []
        shape_groups = []
        shape_contents = [] # Important to avoid GC deleting the shapes
        color_contents = [] # Same as above
        for shape_id in range(num_shapes):
            shape_type = args[current_index]
            current_index += 1
            if shape_type == diffvg.ShapeType.circle:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Circle(radius, diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.ellipse:
                radius = args[current_index]
                current_index += 1
                center = args[current_index]
                current_index += 1
                shape = diffvg.Ellipse(diffvg.Vector2f(radius[0], radius[1]),
                                       diffvg.Vector2f(center[0], center[1]))
            elif shape_type == diffvg.ShapeType.path:
                num_control_points = args[current_index]
                current_index += 1
                points = args[current_index]
                current_index += 1
                thickness = args[current_index]
                current_index += 1
                is_closed = args[current_index]
                current_index += 1
                use_distance_approx = args[current_index]
                current_index += 1
                shape = diffvg.Path(diffvg.int_ptr(num_control_points.data_ptr()),
                                    diffvg.float_ptr(points.data_ptr()),
                                    diffvg.float_ptr(thickness.data_ptr() if thickness is not None else 0),
                                    num_control_points.shape[0],
                                    points.shape[0],
                                    is_closed,
                                    use_distance_approx)
            elif shape_type == diffvg.ShapeType.rect:
                p_min = args[current_index]
                current_index += 1
                p_max = args[current_index]
                current_index += 1
                shape = diffvg.Rect(diffvg.Vector2f(p_min[0], p_min[1]),
                                    diffvg.Vector2f(p_max[0], p_max[1]))
            else:
                assert(False)
            stroke_width = args[current_index]
            current_index += 1
            shapes.append(diffvg.Shape(\
                shape_type, shape.get_ptr(), stroke_width.item()))
            shape_contents.append(shape)

        for shape_group_id in range(num_shape_groups):
            shape_ids = args[current_index]
            current_index += 1
            fill_color_type = args[current_index]
            current_index += 1
            if fill_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                fill_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif fill_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                   diffvg.Vector2f(end[0], end[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                fill_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                   diffvg.Vector2f(radius[0], radius[1]),
                                                   offsets.shape[0],
                                                   diffvg.float_ptr(offsets.data_ptr()),
                                                   diffvg.float_ptr(stop_colors.data_ptr()))
            elif fill_color_type is None:
                fill_color = None
            else:
                assert(False)
            stroke_color_type = args[current_index]
            current_index += 1
            if stroke_color_type == diffvg.ColorType.constant:
                color = args[current_index]
                current_index += 1
                stroke_color = diffvg.Constant(\
                    diffvg.Vector4f(color[0], color[1], color[2], color[3]))
            elif stroke_color_type == diffvg.ColorType.linear_gradient:
                beg = args[current_index]
                current_index += 1
                end = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.LinearGradient(diffvg.Vector2f(beg[0], beg[1]),
                                                     diffvg.Vector2f(end[0], end[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type == diffvg.ColorType.radial_gradient:
                center = args[current_index]
                current_index += 1
                radius = args[current_index]
                current_index += 1
                offsets = args[current_index]
                current_index += 1
                stop_colors = args[current_index]
                current_index += 1
                assert(offsets.shape[0] == stop_colors.shape[0])
                stroke_color = diffvg.RadialGradient(diffvg.Vector2f(center[0], center[1]),
                                                     diffvg.Vector2f(radius[0], radius[1]),
                                                     offsets.shape[0],
                                                     diffvg.float_ptr(offsets.data_ptr()),
                                                     diffvg.float_ptr(stop_colors.data_ptr()))
            elif stroke_color_type is None:
                stroke_color = None
            else:
                assert(False)
            use_even_odd_rule = args[current_index]
            current_index += 1
            shape_to_canvas = args[current_index]
            current_index += 1

            if fill_color is not None:
                color_contents.append(fill_color)
            if stroke_color is not None:
                color_contents.append(stroke_color)
            shape_groups.append(diffvg.ShapeGroup(\
                diffvg.int_ptr(shape_ids.data_ptr()),
                shape_ids.shape[0],
                diffvg.ColorType.constant if fill_color_type is None else fill_color_type,
                diffvg.void_ptr(0) if fill_color is None else fill_color.get_ptr(),
                diffvg.ColorType.constant if stroke_color_type is None else stroke_color_type,
                diffvg.void_ptr(0) if stroke_color is None else stroke_color.get_ptr(),
                use_even_odd_rule,
                diffvg.float_ptr(shape_to_canvas.data_ptr())))

        filter_type = args[current_index]
        current_index += 1
        filter_radius = args[current_index]
        current_index += 1
        filt = diffvg.Filter(filter_type, filter_radius)

        start = time.time()
        scene = diffvg.Scene(canvas_width, canvas_height,
            shapes, shape_groups, filt, pydiffvg.get_use_gpu(),
            pydiffvg.get_device().index if pydiffvg.get_device().index is not None else -1)
        time_elapsed = time.time() - start
        global print_timing
        if print_timing:
            print('Scene construction, time: %.5f s' % time_elapsed)

        if output_type == OutputType.color:
            assert(eval_positions.shape[0] == 0)
            rendered_image = torch.zeros(height, width, 4, device = pydiffvg.get_device())
        else:
            assert(output_type == OutputType.sdf)          
            if eval_positions.shape[0] == 0:
                rendered_image = torch.zeros(height, width, 1, device = pydiffvg.get_device())
            else:
                rendered_image = torch.zeros(eval_positions.shape[0], 1, device = pydiffvg.get_device())

        if background_image is not None:
            background_image = background_image.to(pydiffvg.get_device())
            if background_image.shape[2] == 3:
                background_image = torch.cat((\
                    background_image, torch.ones(background_image.shape[0], background_image.shape[1], 1,
                        device = background_image.device)), dim = 2)
            background_image = background_image.contiguous()
            assert(background_image.shape[0] == rendered_image.shape[0])
            assert(background_image.shape[1] == rendered_image.shape[1])
            assert(background_image.shape[2] == 4)

        start = time.time()
        diffvg.render(scene,
                      diffvg.float_ptr(background_image.data_ptr() if background_image is not None else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.color else 0),
                      diffvg.float_ptr(rendered_image.data_ptr() if output_type == OutputType.sdf else 0),
                      width,
                      height,
                      num_samples_x,
                      num_samples_y,
                      seed,
                      diffvg.float_ptr(0), # d_background_image
                      diffvg.float_ptr(0), # d_render_image
                      diffvg.float_ptr(0), # d_render_sdf
                      diffvg.float_ptr(0), # d_translation
                      use_prefiltering,
                      diffvg.float_ptr(eval_positions.data_ptr()),
                      eval_positions.shape[0])
        assert(torch.isfinite(rendered_image).all())
        time_elapsed = time.time() - start
        if print_timing:
            print('Forward pass, time: %.5f s' % time_elapsed)

        ctx.scene = scene
        ctx.background_image = background_image
        ctx.shape_contents = shape_contents
        ctx.color_contents = color_contents
        ctx.filter = filt
        ctx.width = width
        ctx.height = height
        ctx.num_samples_x = num_samples_x
        ctx.num_samples_y = num_samples_y
        ctx.seed = seed
        ctx.output_type = output_type
        ctx.use_prefiltering = use_prefiltering
        ctx.eval_positions = eval_positions
        return rendered_image","diffvg.Vector2f(p_max[0], p_max[1])",diffvg.Vector2f(*p_max[:2]),"iterable_zj[0], iterable_zj[1]",*p_max[:2],*p_max[:2],1
pytracking,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytracking/ltr/models/layers/filter.py,https://github.com/visionml/pytracking/tree/master/ltr/models/layers/filter.py,,_apply_filter_ksz1$60,"def _apply_filter_ksz1(feat, filter):
    """"""Applies the filter on the input features (feat). The number of groups is automatically calculated.
    args:
        feat: These are the input features. Must have dimensions (images_in_sequence, sequences, feat_dim, H, W)
        filter: The filter to apply. Must have dimensions (sequences, feat_dim, fH, fW) or (sequences, filters, feat_dim/groups, fH, fW)
    output:
        scores: Output of filtering. Dimensions (images_in_sequence, sequences, yH, yW) or (images_in_sequence, sequences, filters, yH, yW)
    """"""

    multiple_filters = (filter.dim() == 5)

    assert filter.shape[-2] == 1 and filter.shape[-1] == 1

    num_images = feat.shape[0]
    num_sequences = feat.shape[1] if feat.dim() == 5 else 1
    num_channels = feat.shape[-3]
    groups = num_channels // filter.shape[-3]

    assert groups == 1

    # scores = torch.einsum('nc, incs->nis', filter.reshape(filter.shape[:-2]), feat.reshape(*feat.shape[:-2], -1))
    scores = torch.matmul(filter.reshape(num_sequences, 1, 1, num_channels),
                          feat.reshape(num_sequences, num_images, num_channels, -1))

    if multiple_filters:
        return scores.reshape(num_images, num_sequences, -1, feat.shape[-2], feat.shape[-1])

    return scores.reshape(num_images, num_sequences, feat.shape[-2], feat.shape[-1])","scores.reshape(num_images, num_sequences, feat.shape[-2], feat.shape[-1])","scores.reshape(num_images, num_sequences, *feat.shape[-2:])","iterable_zj[-2], iterable_zj[-1]",*feat.shape[-2:],*feat.shape[-2:0],0
pytracking,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytracking/ltr/models/layers/filter.py,https://github.com/visionml/pytracking/tree/master/ltr/models/layers/filter.py,,_apply_filter_ksz1$60,"def _apply_filter_ksz1(feat, filter):
    """"""Applies the filter on the input features (feat). The number of groups is automatically calculated.
    args:
        feat: These are the input features. Must have dimensions (images_in_sequence, sequences, feat_dim, H, W)
        filter: The filter to apply. Must have dimensions (sequences, feat_dim, fH, fW) or (sequences, filters, feat_dim/groups, fH, fW)
    output:
        scores: Output of filtering. Dimensions (images_in_sequence, sequences, yH, yW) or (images_in_sequence, sequences, filters, yH, yW)
    """"""

    multiple_filters = (filter.dim() == 5)

    assert filter.shape[-2] == 1 and filter.shape[-1] == 1

    num_images = feat.shape[0]
    num_sequences = feat.shape[1] if feat.dim() == 5 else 1
    num_channels = feat.shape[-3]
    groups = num_channels // filter.shape[-3]

    assert groups == 1

    # scores = torch.einsum('nc, incs->nis', filter.reshape(filter.shape[:-2]), feat.reshape(*feat.shape[:-2], -1))
    scores = torch.matmul(filter.reshape(num_sequences, 1, 1, num_channels),
                          feat.reshape(num_sequences, num_images, num_channels, -1))

    if multiple_filters:
        return scores.reshape(num_images, num_sequences, -1, feat.shape[-2], feat.shape[-1])

    return scores.reshape(num_images, num_sequences, feat.shape[-2], feat.shape[-1])","scores.reshape(num_images, num_sequences, -1, feat.shape[-2], feat.shape[-1])","scores.reshape(num_images, num_sequences, -1, *feat.shape[-2:])","iterable_zj[-2], iterable_zj[-1]",*feat.shape[-2:],*feat.shape[-2:0],0
PaddleSeg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSeg/paddleseg/models/unet.py,https://github.com/PaddlePaddle/PaddleSeg/tree/master/paddleseg/models/unet.py,Decoder,__init__$105,"def __init__(self, align_corners, use_deconv=False):
        super().__init__()

        up_channels = [[512, 256], [256, 128], [128, 64], [64, 64]]
        self.up_sample_list = nn.LayerList([
            UpSampling(channel[0], channel[1], align_corners, use_deconv)
            for channel in up_channels
        ])","UpSampling(channel[0], channel[1], align_corners, use_deconv)","UpSampling(*channel[:2], align_corners, use_deconv)","iterable_zj[0], iterable_zj[1]",*channel[:2],*channel[:2],1
DSView,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DSView/libsigrokdecode4DSL/decoders/adf435x/pd.py,https://github.com/DreamSourceLab/DSView/tree/master/libsigrokdecode4DSL/decoders/adf435x/pd.py,Decoder,decode$127,"def decode(self, ss, es, data):

        ptype, data1, data2 = data

        if ptype == 'CS-CHANGE':
            if data1 == 1:
                if len(self.bits) == 32:
                    reg_value, reg_pos = self.decode_bits(0, 3)
                    self.put(reg_pos[0], reg_pos[1], self.out_ann, [ANN_REG,
                        ['Register: %d' % reg_value, 'Reg: %d' % reg_value,
                         '[%d]' % reg_value]])
                    if reg_value < len(regs):
                        field_descs = regs[reg_value]
                        for field_desc in field_descs:
                            field = self.decode_field(*field_desc)
                self.bits = []
        if ptype == 'BITS':
            self.bits = data1 + self.bits","self.put(reg_pos[0], reg_pos[1], self.out_ann, [ANN_REG, ['Register: %d' % reg_value, 'Reg: %d' % reg_value, '[%d]' % reg_value]])","self.put(*reg_pos[:2], self.out_ann, [ANN_REG, ['Register: %d' % reg_value, 'Reg: %d' % reg_value, '[%d]' % reg_value]])","iterable_zj[0], iterable_zj[1]",*reg_pos[:2],*reg_pos[:2],1
hyde,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hyde/hyde/ext/plugins/images.py,https://github.com/hyde/hyde/tree/master/hyde/ext/plugins/images.py,ImageThumbnailsPlugin,thumb$266,"def thumb(self, resource, width, height, prefix, crop_type,
              preserve_orientation=False):
        """"""
        Generate a thumbnail for the given image
        """"""
        name = os.path.basename(resource.get_relative_deploy_path())
        # don't make thumbnails for thumbnails
        if name.startswith(prefix):
            return
        # Prepare path, make all thumnails in single place(content/.thumbnails)
        # for simple maintenance but keep original deploy path to preserve
        # naming logic in generated site
        path = os.path.join("".thumbnails"",
                            os.path.dirname(
                                resource.get_relative_deploy_path()),
                            ""%s%s"" % (prefix, name))
        target = resource.site.config.content_root_path.child_file(path)
        res = self.site.content.add_resource(target)
        res.set_relative_deploy_path(
            res.get_relative_deploy_path().replace('.thumbnails/', '', 1))

        target.parent.make()
        if (os.path.exists(target.path) and os.path.getmtime(resource.path) <=
                os.path.getmtime(target.path)):
            return
        self.logger.debug(""Making thumbnail for [%s]"" % resource)

        im = self.Image.open(resource.path)
        if im.mode != 'RGBA':
            im = im.convert('RGBA')
        format = im.format

        if preserve_orientation and im.size[1] > im.size[0]:
            width, height = height, width

        resize_width, resize_height = thumb_scale_size(
            im.size[0], im.size[1], width, height)

        self.logger.debug(""Resize to: %d,%d"" % (resize_width, resize_height))
        im = im.resize((resize_width, resize_height), self.Image.ANTIALIAS)
        if width is not None and height is not None:
            shiftx = shifty = 0
            if crop_type == ""center"":
                shiftx = (im.size[0] - width) / 2
                shifty = (im.size[1] - height) / 2
            elif crop_type == ""bottomright"":
                shiftx = (im.size[0] - width)
                shifty = (im.size[1] - height)
            im = im.crop((shiftx, shifty, width + shiftx, height + shifty))
            im.load()

        options = dict(optimize=True)
        if format == ""JPEG"":
            options['quality'] = 75

        im.save(target.path, **options)","thumb_scale_size(im.size[0], im.size[1], width, height)","thumb_scale_size(*im.size[:2], width, height)","iterable_zj[0], iterable_zj[1]",*im.size[:2],*im.size[:2],1
xonsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xonsh/xonsh/parsers/base.py,https://github.com/xonsh/xonsh/tree/master/xonsh/parsers/base.py,BaseParser,p_typedargslist_t11$923,"def p_typedargslist_t11(self, p):
        """"""typedargslist : tfpdef equals_test_opt comma_tfpdef_list_opt comma_opt TIMES tfpdef_opt comma_tfpdef_list COMMA POW tfpdef""""""
        # x, *args, **kwargs
        p0 = ast.arguments(
            args=[],
            vararg=None,
            kwonlyargs=[],
            kw_defaults=[],
            kwarg=p[10],
            defaults=[],
        )
        self._set_regular_args(p0, p[1], p[2], p[3], p[4])
        self._set_var_args(p0, p[6], p[7])
        p[0] = p0","self._set_regular_args(p0, p[1], p[2], p[3], p[4])","self._set_regular_args(p0, *p[1:5])","iterable_zj[1], iterable_zj[2], iterable_zj[3], iterable_zj[4]",*p[1:5],*p[1:5],1
xonsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xonsh/xonsh/parsers/base.py,https://github.com/xonsh/xonsh/tree/master/xonsh/parsers/base.py,BaseParser,p_typedargslist_t11$923,"def p_typedargslist_t11(self, p):
        """"""typedargslist : tfpdef equals_test_opt comma_tfpdef_list_opt comma_opt TIMES tfpdef_opt comma_tfpdef_list COMMA POW tfpdef""""""
        # x, *args, **kwargs
        p0 = ast.arguments(
            args=[],
            vararg=None,
            kwonlyargs=[],
            kw_defaults=[],
            kwarg=p[10],
            defaults=[],
        )
        self._set_regular_args(p0, p[1], p[2], p[3], p[4])
        self._set_var_args(p0, p[6], p[7])
        p[0] = p0","self._set_var_args(p0, p[6], p[7])","self._set_var_args(p0, *p[6:8])","iterable_zj[6], iterable_zj[7]",*p[6:8],*p[6:8],1
airflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/tests/providers/postgres/hooks/test_postgres.py,https://github.com/apache/airflow/tree/master/tests/providers/postgres/hooks/test_postgres.py,TestPostgresHook,test_insert_rows_replace$289,"def test_insert_rows_replace(self):
        table = ""table""
        rows = [
            (
                1,
                ""hello"",
            ),
            (
                2,
                ""world"",
            ),
        ]
        fields = (""id"", ""value"")

        self.db_hook.insert_rows(table, rows, fields, replace=True, replace_index=fields[0])

        assert self.conn.close.call_count == 1
        assert self.cur.close.call_count == 1

        commit_count = 2  # The first and last commit
        assert commit_count == self.conn.commit.call_count

        sql = (
            ""INSERT INTO {0} ({1}, {2}) VALUES (%s,%s) ""
            ""ON CONFLICT ({1}) DO UPDATE SET {2} = excluded.{2}"".format(table, fields[0], fields[1])
        )
        for row in rows:
            self.cur.execute.assert_any_call(sql, row)","'INSERT INTO {0} ({1}, {2}) VALUES (%s,%s) ON CONFLICT ({1}) DO UPDATE SET {2} = excluded.{2}'.format(table, fields[0], fields[1])","'INSERT INTO {0} ({1}, {2}) VALUES (%s,%s) ON CONFLICT ({1}) DO UPDATE SET {2} = excluded.{2}'.format(table, *fields[:2])","iterable_zj[0], iterable_zj[1]",*fields[:2],*fields[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/unittest/test_te_schedule_tensorize.py,https://github.com/apache/tvm/tree/master/tests/python/unittest/test_te_schedule_tensorize.py,,_intrin_vadd$152,"def _intrin_vadd():
            def _intrin_func(ins, outs):
                return tvm.tir.call_packed(""vadd"", ins[0], ins[1], outs[0])

            return tvm.te.decl_tensor_intrin(z.op, _intrin_func)","tvm.tir.call_packed('vadd', ins[0], ins[1], outs[0])","tvm.tir.call_packed('vadd', *ins[:2], outs[0])","iterable_zj[0], iterable_zj[1]",*ins[:2],*ins[:2],1
github-cve-monitor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/github-cve-monitor/github_cve_monitor.py,https://github.com/yhy0/github-cve-monitor/tree/master//github_cve_monitor.py,,create_database$66,"def create_database():
    conn = sqlite3.connect('data.db')
    # print(""[]create_database  "")
    # logging.info(""create_database  "")
    cur = conn.cursor()
    try:
        cur.execute('''CREATE TABLE IF NOT EXISTS cve_monitor
                   (cve_name varchar(255),
                    pushed_at varchar(255),
                    cve_url varchar(255));''')
        print(""CVE"")
        cur.execute('''CREATE TABLE IF NOT EXISTS keyword_monitor
                   (keyword_name varchar(255),
                    pushed_at varchar(255),
                    keyword_url varchar(255));''')
        print("""")
        cur.execute('''CREATE TABLE IF NOT EXISTS redteam_tools_monitor
                   (tools_name varchar(255),
                    pushed_at varchar(255),
                    tag_name varchar(255));''')
        print("""")
        cur.execute('''CREATE TABLE IF NOT EXISTS user_monitor
                   (repo_name varchar(255));''')
        print("""")
    except Exception as e:
        print(""{}"".format(e))
    conn.commit()  # commit  
    conn.close()
    if load_config()[0] == ""dingding"":
        dingding(""test"", """", load_config()[2], load_config()[3])
    elif load_config()[0] == ""server"":
        server(""test"", """", load_config()[2])
    elif load_config()[0] == ""pushplus"":
        pushplus(""test"", """", load_config()[2])        
    elif load_config()[0] == ""tgbot"":
        tgbot(""test"", """", load_config()[2], load_config()[3])","dingding('test', '', load_config()[2], load_config()[3])","dingding('test', '', *load_config()[2:4])","iterable_zj[2], iterable_zj[3]",*load_config()[2:4],*load_config()[2:4],1
github-cve-monitor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/github-cve-monitor/github_cve_monitor.py,https://github.com/yhy0/github-cve-monitor/tree/master//github_cve_monitor.py,,create_database$66,"def create_database():
    conn = sqlite3.connect('data.db')
    # print(""[]create_database  "")
    # logging.info(""create_database  "")
    cur = conn.cursor()
    try:
        cur.execute('''CREATE TABLE IF NOT EXISTS cve_monitor
                   (cve_name varchar(255),
                    pushed_at varchar(255),
                    cve_url varchar(255));''')
        print(""CVE"")
        cur.execute('''CREATE TABLE IF NOT EXISTS keyword_monitor
                   (keyword_name varchar(255),
                    pushed_at varchar(255),
                    keyword_url varchar(255));''')
        print("""")
        cur.execute('''CREATE TABLE IF NOT EXISTS redteam_tools_monitor
                   (tools_name varchar(255),
                    pushed_at varchar(255),
                    tag_name varchar(255));''')
        print("""")
        cur.execute('''CREATE TABLE IF NOT EXISTS user_monitor
                   (repo_name varchar(255));''')
        print("""")
    except Exception as e:
        print(""{}"".format(e))
    conn.commit()  # commit  
    conn.close()
    if load_config()[0] == ""dingding"":
        dingding(""test"", """", load_config()[2], load_config()[3])
    elif load_config()[0] == ""server"":
        server(""test"", """", load_config()[2])
    elif load_config()[0] == ""pushplus"":
        pushplus(""test"", """", load_config()[2])        
    elif load_config()[0] == ""tgbot"":
        tgbot(""test"", """", load_config()[2], load_config()[3])","tgbot('test', '', load_config()[2], load_config()[3])","tgbot('test', '', *load_config()[2:4])","iterable_zj[2], iterable_zj[3]",*load_config()[2:4],*load_config()[2:4],1
openpilot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openpilot/tools/replay/lib/ui_helpers.py,https://github.com/commaai/openpilot/tree/master/tools/replay/lib/ui_helpers.py,Calibration,__init__$62,"def __init__(self, num_px, rpy, intrinsic):
    self.intrinsic = intrinsic
    self.extrinsics_matrix = get_view_frame_from_calib_frame(rpy[0], rpy[1], rpy[2], 0.0)[:,:3]
    self.zoom = _CALIB_BB_TO_FULL[num_px][0, 0]","get_view_frame_from_calib_frame(rpy[0], rpy[1], rpy[2], 0.0)","get_view_frame_from_calib_frame(*rpy[:3], 0.0)","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*rpy[:3],*rpy[:3],1
EasyClangComplete,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyClangComplete/plugin/clang/cindex33.py,https://github.com/niosus/EasyClangComplete/tree/master/plugin/clang/cindex33.py,TranslationUnit,get_extent$2154,"def get_extent(self, filename, locations):
        """"""Obtain a SourceRange from this translation unit.

        The bounds of the SourceRange must ultimately be defined by a start and
        end SourceLocation. For the locations argument, you can pass:

          - 2 SourceLocation instances in a 2-tuple or list.
          - 2 int file offsets via a 2-tuple or list.
          - 2 2-tuple or lists of (line, column) pairs in a 2-tuple or list.

        e.g.

        get_extent('foo.c', (5, 10))
        get_extent('foo.c', ((1, 1), (1, 15)))
        """"""
        f = self.get_file(filename)

        if len(locations) < 2:
            raise Exception('Must pass object with at least 2 elements')

        start_location, end_location = locations

        if hasattr(start_location, '__len__'):
            start_location = SourceLocation.from_position(self, f,
                start_location[0], start_location[1])
        elif isinstance(start_location, int):
            start_location = SourceLocation.from_offset(self, f,
                start_location)

        if hasattr(end_location, '__len__'):
            end_location = SourceLocation.from_position(self, f,
                end_location[0], end_location[1])
        elif isinstance(end_location, int):
            end_location = SourceLocation.from_offset(self, f, end_location)

        assert isinstance(start_location, SourceLocation)
        assert isinstance(end_location, SourceLocation)

        return SourceRange.from_locations(start_location, end_location)","SourceLocation.from_position(self, f, start_location[0], start_location[1])","SourceLocation.from_position(self, f, *start_location[:2])","iterable_zj[0], iterable_zj[1]",*start_location[:2],*start_location[:2],1
EasyClangComplete,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyClangComplete/plugin/clang/cindex33.py,https://github.com/niosus/EasyClangComplete/tree/master/plugin/clang/cindex33.py,TranslationUnit,get_extent$2154,"def get_extent(self, filename, locations):
        """"""Obtain a SourceRange from this translation unit.

        The bounds of the SourceRange must ultimately be defined by a start and
        end SourceLocation. For the locations argument, you can pass:

          - 2 SourceLocation instances in a 2-tuple or list.
          - 2 int file offsets via a 2-tuple or list.
          - 2 2-tuple or lists of (line, column) pairs in a 2-tuple or list.

        e.g.

        get_extent('foo.c', (5, 10))
        get_extent('foo.c', ((1, 1), (1, 15)))
        """"""
        f = self.get_file(filename)

        if len(locations) < 2:
            raise Exception('Must pass object with at least 2 elements')

        start_location, end_location = locations

        if hasattr(start_location, '__len__'):
            start_location = SourceLocation.from_position(self, f,
                start_location[0], start_location[1])
        elif isinstance(start_location, int):
            start_location = SourceLocation.from_offset(self, f,
                start_location)

        if hasattr(end_location, '__len__'):
            end_location = SourceLocation.from_position(self, f,
                end_location[0], end_location[1])
        elif isinstance(end_location, int):
            end_location = SourceLocation.from_offset(self, f, end_location)

        assert isinstance(start_location, SourceLocation)
        assert isinstance(end_location, SourceLocation)

        return SourceRange.from_locations(start_location, end_location)","SourceLocation.from_position(self, f, end_location[0], end_location[1])","SourceLocation.from_position(self, f, *end_location[:2])","iterable_zj[0], iterable_zj[1]",*end_location[:2],*end_location[:2],1
ursina,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ursina/ursina/entity.py,https://github.com/pokepetter/ursina/tree/master/ursina/entity.py,Entity,fade_out$1147,"def fade_out(self, value=0, duration=.5, **kwargs):
        return self.animate('color', Vec4(self.color[0], self.color[1], self.color[2], value), duration,  **kwargs)","Vec4(self.color[0], self.color[1], self.color[2], value)","Vec4(*self.color[:3], value)","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*self.color[:3],*self.color[:3],1
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/dragknife.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/dragknife.py,Tool,initPoint$115,"def initPoint(P, direction, offset):
            P = Vector(P[0], P[1])

            if direction == ""X+"":
                P[0] += offset
            elif direction == ""X-"":
                P[0] -= offset
            elif direction == ""Y+"":
                P[1] += offset
            elif direction == ""Y-"":
                P[1] -= offset
            return P","Vector(P[0], P[1])",Vector(*P[:2]),"iterable_zj[0], iterable_zj[1]",*P[:2],*P[:2],1
FSA-Net,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FSA-Net/lib/old_version/FSANET_model.py,https://github.com/shamangary/FSA-Net/tree/master/lib/old_version/FSANET_model.py,FSA_net_Capsule_FC,ssr_S_model_build$1609,"def ssr_S_model_build(num_primcaps, m_dim):
            input_s1_preS = Input((8,8,64))
            input_s2_preS = Input((8,8,64))
            input_s3_preS = Input((8,8,64))

            SR_matrix_s1,feat_s1_preS = ssr_feat_S_model(input_s1_preS)
            SR_matrix_s2,feat_s2_preS = ssr_feat_S_model(input_s2_preS)
            SR_matrix_s3,feat_s3_preS = ssr_feat_S_model(input_s3_preS)
            
            feat_pre_concat = Concatenate()([feat_s1_preS,feat_s2_preS,feat_s3_preS])
            SL_matrix = Dense(int(num_primcaps/3)*m_dim,activation='sigmoid')(feat_pre_concat)
            SL_matrix = Reshape((int(num_primcaps/3),m_dim))(SL_matrix)
            
            S_matrix_s1 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s1')([SL_matrix,SR_matrix_s1])
            S_matrix_s2 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s2')([SL_matrix,SR_matrix_s2])
            S_matrix_s3 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s3')([SL_matrix,SR_matrix_s3])

            # Very important!!! Without this training won't converge.
            # norm_S = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix)
            norm_S_s1 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s1)
            norm_S_s2 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s2)
            norm_S_s3 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s3)

            feat_s1_pre = Reshape((-1,64))(input_s1_preS)
            feat_s2_pre = Reshape((-1,64))(input_s2_preS)
            feat_s3_pre = Reshape((-1,64))(input_s3_preS)
            feat_pre_concat = Concatenate(axis=1)([feat_s1_pre, feat_s2_pre, feat_s3_pre])
            
            # Warining: don't use keras's 'K.dot'. It is very weird when high dimension is used.
            # https://github.com/keras-team/keras/issues/9779
            # Make sure 'tf.matmul' is used
            # primcaps = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix,feat_pre_concat, norm_S])
            primcaps_s1 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s1,feat_pre_concat, norm_S_s1])
            primcaps_s2 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s2,feat_pre_concat, norm_S_s2])
            primcaps_s3 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s3,feat_pre_concat, norm_S_s3])
            primcaps = Concatenate(axis=1)([primcaps_s1,primcaps_s2,primcaps_s3])

            ssr_S_model = Model(inputs=[input_s1_preS, input_s2_preS, input_s3_preS],outputs=primcaps, name='ssr_S_model')
            return ssr_S_model","tf.matmul(x[0], x[1])",tf.matmul(*x[:2]),"iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
FSA-Net,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FSA-Net/lib/old_version/FSANET_model.py,https://github.com/shamangary/FSA-Net/tree/master/lib/old_version/FSANET_model.py,FSA_net_Capsule_FC,ssr_S_model_build$1609,"def ssr_S_model_build(num_primcaps, m_dim):
            input_s1_preS = Input((8,8,64))
            input_s2_preS = Input((8,8,64))
            input_s3_preS = Input((8,8,64))

            SR_matrix_s1,feat_s1_preS = ssr_feat_S_model(input_s1_preS)
            SR_matrix_s2,feat_s2_preS = ssr_feat_S_model(input_s2_preS)
            SR_matrix_s3,feat_s3_preS = ssr_feat_S_model(input_s3_preS)
            
            feat_pre_concat = Concatenate()([feat_s1_preS,feat_s2_preS,feat_s3_preS])
            SL_matrix = Dense(int(num_primcaps/3)*m_dim,activation='sigmoid')(feat_pre_concat)
            SL_matrix = Reshape((int(num_primcaps/3),m_dim))(SL_matrix)
            
            S_matrix_s1 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s1')([SL_matrix,SR_matrix_s1])
            S_matrix_s2 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s2')([SL_matrix,SR_matrix_s2])
            S_matrix_s3 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s3')([SL_matrix,SR_matrix_s3])

            # Very important!!! Without this training won't converge.
            # norm_S = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix)
            norm_S_s1 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s1)
            norm_S_s2 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s2)
            norm_S_s3 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s3)

            feat_s1_pre = Reshape((-1,64))(input_s1_preS)
            feat_s2_pre = Reshape((-1,64))(input_s2_preS)
            feat_s3_pre = Reshape((-1,64))(input_s3_preS)
            feat_pre_concat = Concatenate(axis=1)([feat_s1_pre, feat_s2_pre, feat_s3_pre])
            
            # Warining: don't use keras's 'K.dot'. It is very weird when high dimension is used.
            # https://github.com/keras-team/keras/issues/9779
            # Make sure 'tf.matmul' is used
            # primcaps = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix,feat_pre_concat, norm_S])
            primcaps_s1 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s1,feat_pre_concat, norm_S_s1])
            primcaps_s2 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s2,feat_pre_concat, norm_S_s2])
            primcaps_s3 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s3,feat_pre_concat, norm_S_s3])
            primcaps = Concatenate(axis=1)([primcaps_s1,primcaps_s2,primcaps_s3])

            ssr_S_model = Model(inputs=[input_s1_preS, input_s2_preS, input_s3_preS],outputs=primcaps, name='ssr_S_model')
            return ssr_S_model","tf.matmul(x[0], x[1])",tf.matmul(*x[:2]),"iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
FSA-Net,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FSA-Net/lib/old_version/FSANET_model.py,https://github.com/shamangary/FSA-Net/tree/master/lib/old_version/FSANET_model.py,FSA_net_Capsule_FC,ssr_S_model_build$1609,"def ssr_S_model_build(num_primcaps, m_dim):
            input_s1_preS = Input((8,8,64))
            input_s2_preS = Input((8,8,64))
            input_s3_preS = Input((8,8,64))

            SR_matrix_s1,feat_s1_preS = ssr_feat_S_model(input_s1_preS)
            SR_matrix_s2,feat_s2_preS = ssr_feat_S_model(input_s2_preS)
            SR_matrix_s3,feat_s3_preS = ssr_feat_S_model(input_s3_preS)
            
            feat_pre_concat = Concatenate()([feat_s1_preS,feat_s2_preS,feat_s3_preS])
            SL_matrix = Dense(int(num_primcaps/3)*m_dim,activation='sigmoid')(feat_pre_concat)
            SL_matrix = Reshape((int(num_primcaps/3),m_dim))(SL_matrix)
            
            S_matrix_s1 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s1')([SL_matrix,SR_matrix_s1])
            S_matrix_s2 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s2')([SL_matrix,SR_matrix_s2])
            S_matrix_s3 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s3')([SL_matrix,SR_matrix_s3])

            # Very important!!! Without this training won't converge.
            # norm_S = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix)
            norm_S_s1 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s1)
            norm_S_s2 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s2)
            norm_S_s3 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s3)

            feat_s1_pre = Reshape((-1,64))(input_s1_preS)
            feat_s2_pre = Reshape((-1,64))(input_s2_preS)
            feat_s3_pre = Reshape((-1,64))(input_s3_preS)
            feat_pre_concat = Concatenate(axis=1)([feat_s1_pre, feat_s2_pre, feat_s3_pre])
            
            # Warining: don't use keras's 'K.dot'. It is very weird when high dimension is used.
            # https://github.com/keras-team/keras/issues/9779
            # Make sure 'tf.matmul' is used
            # primcaps = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix,feat_pre_concat, norm_S])
            primcaps_s1 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s1,feat_pre_concat, norm_S_s1])
            primcaps_s2 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s2,feat_pre_concat, norm_S_s2])
            primcaps_s3 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s3,feat_pre_concat, norm_S_s3])
            primcaps = Concatenate(axis=1)([primcaps_s1,primcaps_s2,primcaps_s3])

            ssr_S_model = Model(inputs=[input_s1_preS, input_s2_preS, input_s3_preS],outputs=primcaps, name='ssr_S_model')
            return ssr_S_model","tf.matmul(x[0], x[1])",tf.matmul(*x[:2]),"iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
FSA-Net,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FSA-Net/lib/old_version/FSANET_model.py,https://github.com/shamangary/FSA-Net/tree/master/lib/old_version/FSANET_model.py,FSA_net_Capsule_FC,ssr_S_model_build$1609,"def ssr_S_model_build(num_primcaps, m_dim):
            input_s1_preS = Input((8,8,64))
            input_s2_preS = Input((8,8,64))
            input_s3_preS = Input((8,8,64))

            SR_matrix_s1,feat_s1_preS = ssr_feat_S_model(input_s1_preS)
            SR_matrix_s2,feat_s2_preS = ssr_feat_S_model(input_s2_preS)
            SR_matrix_s3,feat_s3_preS = ssr_feat_S_model(input_s3_preS)
            
            feat_pre_concat = Concatenate()([feat_s1_preS,feat_s2_preS,feat_s3_preS])
            SL_matrix = Dense(int(num_primcaps/3)*m_dim,activation='sigmoid')(feat_pre_concat)
            SL_matrix = Reshape((int(num_primcaps/3),m_dim))(SL_matrix)
            
            S_matrix_s1 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s1')([SL_matrix,SR_matrix_s1])
            S_matrix_s2 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s2')([SL_matrix,SR_matrix_s2])
            S_matrix_s3 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s3')([SL_matrix,SR_matrix_s3])

            # Very important!!! Without this training won't converge.
            # norm_S = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix)
            norm_S_s1 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s1)
            norm_S_s2 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s2)
            norm_S_s3 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s3)

            feat_s1_pre = Reshape((-1,64))(input_s1_preS)
            feat_s2_pre = Reshape((-1,64))(input_s2_preS)
            feat_s3_pre = Reshape((-1,64))(input_s3_preS)
            feat_pre_concat = Concatenate(axis=1)([feat_s1_pre, feat_s2_pre, feat_s3_pre])
            
            # Warining: don't use keras's 'K.dot'. It is very weird when high dimension is used.
            # https://github.com/keras-team/keras/issues/9779
            # Make sure 'tf.matmul' is used
            # primcaps = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix,feat_pre_concat, norm_S])
            primcaps_s1 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s1,feat_pre_concat, norm_S_s1])
            primcaps_s2 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s2,feat_pre_concat, norm_S_s2])
            primcaps_s3 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s3,feat_pre_concat, norm_S_s3])
            primcaps = Concatenate(axis=1)([primcaps_s1,primcaps_s2,primcaps_s3])

            ssr_S_model = Model(inputs=[input_s1_preS, input_s2_preS, input_s3_preS],outputs=primcaps, name='ssr_S_model')
            return ssr_S_model","tf.matmul(x[0], x[1])",tf.matmul(*x[:2]),"iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
FSA-Net,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FSA-Net/lib/old_version/FSANET_model.py,https://github.com/shamangary/FSA-Net/tree/master/lib/old_version/FSANET_model.py,FSA_net_Capsule_FC,ssr_S_model_build$1609,"def ssr_S_model_build(num_primcaps, m_dim):
            input_s1_preS = Input((8,8,64))
            input_s2_preS = Input((8,8,64))
            input_s3_preS = Input((8,8,64))

            SR_matrix_s1,feat_s1_preS = ssr_feat_S_model(input_s1_preS)
            SR_matrix_s2,feat_s2_preS = ssr_feat_S_model(input_s2_preS)
            SR_matrix_s3,feat_s3_preS = ssr_feat_S_model(input_s3_preS)
            
            feat_pre_concat = Concatenate()([feat_s1_preS,feat_s2_preS,feat_s3_preS])
            SL_matrix = Dense(int(num_primcaps/3)*m_dim,activation='sigmoid')(feat_pre_concat)
            SL_matrix = Reshape((int(num_primcaps/3),m_dim))(SL_matrix)
            
            S_matrix_s1 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s1')([SL_matrix,SR_matrix_s1])
            S_matrix_s2 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s2')([SL_matrix,SR_matrix_s2])
            S_matrix_s3 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s3')([SL_matrix,SR_matrix_s3])

            # Very important!!! Without this training won't converge.
            # norm_S = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix)
            norm_S_s1 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s1)
            norm_S_s2 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s2)
            norm_S_s3 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s3)

            feat_s1_pre = Reshape((-1,64))(input_s1_preS)
            feat_s2_pre = Reshape((-1,64))(input_s2_preS)
            feat_s3_pre = Reshape((-1,64))(input_s3_preS)
            feat_pre_concat = Concatenate(axis=1)([feat_s1_pre, feat_s2_pre, feat_s3_pre])
            
            # Warining: don't use keras's 'K.dot'. It is very weird when high dimension is used.
            # https://github.com/keras-team/keras/issues/9779
            # Make sure 'tf.matmul' is used
            # primcaps = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix,feat_pre_concat, norm_S])
            primcaps_s1 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s1,feat_pre_concat, norm_S_s1])
            primcaps_s2 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s2,feat_pre_concat, norm_S_s2])
            primcaps_s3 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s3,feat_pre_concat, norm_S_s3])
            primcaps = Concatenate(axis=1)([primcaps_s1,primcaps_s2,primcaps_s3])

            ssr_S_model = Model(inputs=[input_s1_preS, input_s2_preS, input_s3_preS],outputs=primcaps, name='ssr_S_model')
            return ssr_S_model","tf.matmul(x[0], x[1])",tf.matmul(*x[:2]),"iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
FSA-Net,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FSA-Net/lib/old_version/FSANET_model.py,https://github.com/shamangary/FSA-Net/tree/master/lib/old_version/FSANET_model.py,FSA_net_Capsule_FC,ssr_S_model_build$1609,"def ssr_S_model_build(num_primcaps, m_dim):
            input_s1_preS = Input((8,8,64))
            input_s2_preS = Input((8,8,64))
            input_s3_preS = Input((8,8,64))

            SR_matrix_s1,feat_s1_preS = ssr_feat_S_model(input_s1_preS)
            SR_matrix_s2,feat_s2_preS = ssr_feat_S_model(input_s2_preS)
            SR_matrix_s3,feat_s3_preS = ssr_feat_S_model(input_s3_preS)
            
            feat_pre_concat = Concatenate()([feat_s1_preS,feat_s2_preS,feat_s3_preS])
            SL_matrix = Dense(int(num_primcaps/3)*m_dim,activation='sigmoid')(feat_pre_concat)
            SL_matrix = Reshape((int(num_primcaps/3),m_dim))(SL_matrix)
            
            S_matrix_s1 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s1')([SL_matrix,SR_matrix_s1])
            S_matrix_s2 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s2')([SL_matrix,SR_matrix_s2])
            S_matrix_s3 = Lambda(lambda x: tf.matmul(x[0],x[1]),name='S_matrix_s3')([SL_matrix,SR_matrix_s3])

            # Very important!!! Without this training won't converge.
            # norm_S = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix)
            norm_S_s1 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s1)
            norm_S_s2 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s2)
            norm_S_s3 = Lambda(lambda x: K.tile(K.sum(x,axis=-1,keepdims=True),(1,1,64)))(S_matrix_s3)

            feat_s1_pre = Reshape((-1,64))(input_s1_preS)
            feat_s2_pre = Reshape((-1,64))(input_s2_preS)
            feat_s3_pre = Reshape((-1,64))(input_s3_preS)
            feat_pre_concat = Concatenate(axis=1)([feat_s1_pre, feat_s2_pre, feat_s3_pre])
            
            # Warining: don't use keras's 'K.dot'. It is very weird when high dimension is used.
            # https://github.com/keras-team/keras/issues/9779
            # Make sure 'tf.matmul' is used
            # primcaps = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix,feat_pre_concat, norm_S])
            primcaps_s1 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s1,feat_pre_concat, norm_S_s1])
            primcaps_s2 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s2,feat_pre_concat, norm_S_s2])
            primcaps_s3 = Lambda(lambda x: tf.matmul(x[0],x[1])/x[2])([S_matrix_s3,feat_pre_concat, norm_S_s3])
            primcaps = Concatenate(axis=1)([primcaps_s1,primcaps_s2,primcaps_s3])

            ssr_S_model = Model(inputs=[input_s1_preS, input_s2_preS, input_s3_preS],outputs=primcaps, name='ssr_S_model')
            return ssr_S_model","tf.matmul(x[0], x[1])",tf.matmul(*x[:2]),"iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
openpilot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openpilot/selfdrive/controls/lib/latcontrol_indi.py,https://github.com/commaai/openpilot/tree/master/selfdrive/controls/lib/latcontrol_indi.py,LatControlINDI,G$56,"def G(self):
    return interp(self.speed, self._G[0], self._G[1])","interp(self.speed, self._G[0], self._G[1])","interp(self.speed, *self._G[:2])","iterable_zj[0], iterable_zj[1]",*self._G[:2],*self._G[:2],1
Attention-Gated-Networks,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Attention-Gated-Networks/models/feedforward_seg_model.py,https://github.com/ozan-oktay/Attention-Gated-Networks/tree/master/models/feedforward_seg_model.py,FeedForwardSegmentation,set_input$66,"def set_input(self, *inputs):
        # self.input.resize_(inputs[0].size()).copy_(inputs[0])
        for idx, _input in enumerate(inputs):
            # If it's a 5D array and 2D model then (B x C x H x W x Z) -> (BZ x C x H x W)
            bs = _input.size()
            if (self.tensor_dim == '2D') and (len(bs) > 4):
                _input = _input.permute(0,4,1,2,3).contiguous().view(bs[0]*bs[4], bs[1], bs[2], bs[3])

            # Define that it's a cuda array
            if idx == 0:
                self.input = _input.cuda() if self.use_cuda else _input
            elif idx == 1:
                self.target = Variable(_input.cuda()) if self.use_cuda else Variable(_input)
                assert self.input.size() == self.target.size()","_input.permute(0, 4, 1, 2, 3).contiguous().view(bs[0] * bs[4], bs[1], bs[2], bs[3])","_input.permute(0, 4, 1, 2, 3).contiguous().view(bs[0] * bs[4], *bs[1:4])","iterable_zj[1], iterable_zj[2], iterable_zj[3]",*bs[1:4],*bs[1:4],1
MetPy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MetPy/tests/calc/test_thermo.py,https://github.com/Unidata/MetPy/tree/master/tests/calc/test_thermo.py,,test_brunt_vaisala_frequency_squared$1587,"def test_brunt_vaisala_frequency_squared(bv_data):
    """"""Test Brunt-Vaisala frequency squared function.""""""
    truth = [[1.35264138e-04, 2.02896207e-04, 3.04344310e-04, 1.69080172e-04],
             [1.34337671e-04, 2.00818771e-04, 1.00409386e-04, 1.00753253e-04],
             [1.33423810e-04, 6.62611486e-05, 0, 1.33879181e-04],
             [1.32522297e-04, -1.99457288e-04, 0., 2.65044595e-04]] * units('s^-2')
    bv_freq_sqr = brunt_vaisala_frequency_squared(bv_data[0], bv_data[1])
    assert_almost_equal(bv_freq_sqr, truth, 6)","brunt_vaisala_frequency_squared(bv_data[0], bv_data[1])",brunt_vaisala_frequency_squared(*bv_data[:2]),"iterable_zj[0], iterable_zj[1]",*bv_data[:2],*bv_data[:2],1
sclack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sclack/sclack/loading.py,https://github.com/haskellcamargo/sclack/tree/master/sclack/loading.py,SlackBot,__init__$95,"def __init__(self):
        super(SlackBot, self).__init__([
            urwid.Text([
                (urwid.AttrSpec(pair[1], pair[2]), pair[0]) for pair in row
            ], align='center')
            for row in self._matrix
        ])","urwid.AttrSpec(pair[1], pair[2])",urwid.AttrSpec(*pair[1:3]),"iterable_zj[1], iterable_zj[2]",*pair[1:3],*pair[1:3],1
EssayKiller_V2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EssayKiller_V2/RecognizaitonNetwork/ctpn/text_detect.py,https://github.com/EssayKillerBrain/EssayKiller_V2/tree/master/RecognizaitonNetwork/ctpn/text_detect.py,,resize_im$14,"def resize_im(im, scale, max_scale=None):
    f = float(scale) / min(im.shape[0], im.shape[1])
    if max_scale != None and f * max(im.shape[0], im.shape[1]) > max_scale:
        f = float(max_scale) / max(im.shape[0], im.shape[1])
    return cv2.resize(im, None, None, fx=f, fy=f, interpolation=cv2.INTER_LINEAR), f","min(im.shape[0], im.shape[1])",min(*im.shape[:2]),"iterable_zj[0], iterable_zj[1]",*im.shape[:2],*im.shape[:2],1
EssayKiller_V2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EssayKiller_V2/RecognizaitonNetwork/ctpn/text_detect.py,https://github.com/EssayKillerBrain/EssayKiller_V2/tree/master/RecognizaitonNetwork/ctpn/text_detect.py,,resize_im$14,"def resize_im(im, scale, max_scale=None):
    f = float(scale) / min(im.shape[0], im.shape[1])
    if max_scale != None and f * max(im.shape[0], im.shape[1]) > max_scale:
        f = float(max_scale) / max(im.shape[0], im.shape[1])
    return cv2.resize(im, None, None, fx=f, fy=f, interpolation=cv2.INTER_LINEAR), f","max(im.shape[0], im.shape[1])",max(*im.shape[:2]),"iterable_zj[0], iterable_zj[1]",*im.shape[:2],*im.shape[:2],1
EssayKiller_V2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EssayKiller_V2/RecognizaitonNetwork/ctpn/text_detect.py,https://github.com/EssayKillerBrain/EssayKiller_V2/tree/master/RecognizaitonNetwork/ctpn/text_detect.py,,resize_im$14,"def resize_im(im, scale, max_scale=None):
    f = float(scale) / min(im.shape[0], im.shape[1])
    if max_scale != None and f * max(im.shape[0], im.shape[1]) > max_scale:
        f = float(max_scale) / max(im.shape[0], im.shape[1])
    return cv2.resize(im, None, None, fx=f, fy=f, interpolation=cv2.INTER_LINEAR), f","max(im.shape[0], im.shape[1])",max(*im.shape[:2]),"iterable_zj[0], iterable_zj[1]",*im.shape[:2],*im.shape[:2],1
delorean,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/delorean/delorean/dates.py,https://github.com/myusuf3/delorean/tree/master/delorean/dates.py,Delorean,__getattr__$271,"def __getattr__(self, name):
        """"""
        Implement __getattr__ to call `shift_date` function when function
        called does not exist
        """"""
        func_parts = name.split('_')
        # is the func we are trying to call the right length?
        if len(func_parts) != 2:
            raise AttributeError

        # is the function we are trying to call valid?
        if (func_parts[0] not in self._VALID_SHIFT_DIRECTIONS or
                    func_parts[1] not in self._VALID_SHIFT_UNITS):
            return AttributeError

        # dispatch our function
        func = partial(self._shift_date, func_parts[0], func_parts[1])
        # update our partial with self.shift_date attributes
        update_wrapper(func, self._shift_date)
        return func","partial(self._shift_date, func_parts[0], func_parts[1])","partial(self._shift_date, *func_parts[:2])","iterable_zj[0], iterable_zj[1]",*func_parts[:2],*func_parts[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/relay/frontend/tflite.py,https://github.com/apache/tvm/tree/master/python/tvm/relay/frontend/tflite.py,,prepare_dense_matrix_from_sparse$3702,"def prepare_dense_matrix_from_sparse(sparse_tensor, sparse_tensor_value, sparse_tensor_type):
    """"""Prepare sparse indices and dense matrix from TFLite sparse parameters.""""""
    # The function is implemented based on TFLite sparse parameter specifications
    # Please refer
    # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs#L89
    # for details about each parameters
    sparsity = sparse_tensor.Sparsity()
    dense_shape = sparse_tensor.ShapeAsNumpy()
    orig_rank = len(dense_shape)

    # The traversal order of the dimensions defined in the `shape` field of the to be dense tensor.
    traversal_order = sparsity.TraversalOrderAsNumpy()

    # For an n-dimensional tensor with a k-dimensional block (0 <= k <= n),
    # stores how a block dimension in (dn, ..., dn+k-1) maps to the original
    # tensor dimension in (d0, ..., dn). It's stored in the order of (dn, ..., dn+k-1).
    # If not block-sparse, this field is NULL.
    block_map = sparsity.BlockMapAsNumpy()

    total_rank = sparsity.TraversalOrderLength()
    dense_mat = np.full(shape=dense_shape, fill_value=0, dtype=sparse_tensor_type).flatten()

    from enum import Enum

    # NOTE: Here the Vector term is borrowed from TFLite spec.
    class VectorType(Enum):
        Empty = 0
        Int32 = 1
        Uint16 = 2
        Uint8 = 3

    def _get_vector_flag(v_type):
        if VectorType(v_type) == VectorType.Int32:
            return N.Int32Flags
        elif VectorType(v_type) == VectorType.Uint16:
            return N.Uint16Flags
        elif VectorType(v_type) == VectorType.Uint8:
            return N.Uint8Flags
        else:
            raise tvm.error.OpNotImplemented(""The provided type {} is not supported"".format(v_type))

    def _get_flattened_index(indices, shape):
        index = 0
        sub_elements = 1
        for i in reversed(range(0, len(dense_shape))):
            index += indices[i] * sub_elements
            sub_elements *= shape[i]
        return index

    # DimensionMetadata per dimension: the metadata needed for
    #     each dimension to locate the non-zero values in the original dense tensor
    #     inline with traversal order parameter.
    #
    # sp_format has 2 possible values: {DENSE = 0, SPARSE_CSR = 1}
    # If format = DENSE{0} : DenseSize represents size of that dimension
    # If format = SPARSE_CSR{1} : array_segments represents how to segment the indices array,
    #      each segment corresponds to one element in the previous dimension. array_indices
    #      represents the index of the non-zero elements within this dimension
    #      (as those in the CSR matrix format, where the first array is row pointers
    #       and the second array is column indices).
    sp_format = np.zeros(sparsity.DimMetadataLength())
    dim_metadata = [None] * (2 * sparsity.DimMetadataLength())

    # Below loop will fetch all meta data per dimension based on format type
    # Dense or Sparse and will put it in an agnostic array for easy access
    # while preparing dense buffer or indices.
    for i in range(sparsity.DimMetadataLength()):
        sp_format[i] = sparsity.DimMetadata(i).Format()
        if sp_format[i] == 0:
            dim_metadata[2 * i] = [sparsity.DimMetadata(i).DenseSize()]
        else:
            from flatbuffers import number_types as N

            dim_metadata[2 * i] = (
                sparsity.DimMetadata(i)
                .ArraySegments()
                .GetVectorAsNumpy(
                    flags=_get_vector_flag(sparsity.DimMetadata(i).ArraySegmentsType()), off=4
                )
            )
            dim_metadata[2 * i + 1] = (
                sparsity.DimMetadata(i)
                .ArrayIndices()
                .GetVectorAsNumpy(
                    flags=_get_vector_flag(sparsity.DimMetadata(i).ArrayIndicesType()), off=4
                )
            )

    block_dim = 0
    block_size = np.zeros(sparsity.BlockMapLength())

    # Block size parameter if encoded in BSR format
    for i in range(orig_rank):
        if block_dim < sparsity.BlockMapLength() and block_map[block_dim] == i:
            orig_dim = traversal_order[orig_rank + block_dim]
            block_size[block_dim] = sparsity.DimMetadata(orig_dim).DenseSize()
            block_dim += 1

    indices_list = []

    # Below function iterates through each applicable indices per dimension
    # based on format type specified and finally produce the dense matrix and the NZ indices.
    def _def_prepare_dense_matrix_from_sparse(indices, level, prev_idx):
        if level == len(indices):
            start_pos = 0
            orig_idx = np.zeros(orig_rank, dtype=""int32"")
            while start_pos < orig_rank:
                orig_idx[traversal_order[start_pos]] = indices[start_pos]
                start_pos += 1
            while start_pos < len(indices):
                block_idx = traversal_order[start_pos] - orig_rank
                orig_dim = block_map[block_idx]
                orig_idx[orig_dim] = orig_idx[orig_dim] * block_size[block_idx] + indices[start_pos]
                start_pos += 1
            indices_list.append(orig_idx)
            nonlocal value_idx
            dense_mat[_get_flattened_index(orig_idx, dense_shape)] = sparse_tensor_value[value_idx]
            value_idx += 1
        else:
            metadata_idx = 2 * level
            if sp_format[level] == 0:
                shape_of_level = dim_metadata[metadata_idx][0]
                for idx in range(shape_of_level):
                    indices[level] = idx
                    _def_prepare_dense_matrix_from_sparse(
                        indices, level + 1, prev_idx * shape_of_level + idx
                    )
            else:
                array_segments = dim_metadata[metadata_idx]
                array_indices = dim_metadata[metadata_idx + 1]
                for idx in range(array_segments[prev_idx], array_segments[prev_idx + 1]):
                    indices[level] = array_indices[idx]
                    _def_prepare_dense_matrix_from_sparse(indices, level + 1, idx)

    indices = np.zeros(total_rank)
    value_idx = 0
    _def_prepare_dense_matrix_from_sparse(indices, 0, 0)
    return np.array(indices_list, dtype=""int32""), dense_mat.reshape(dense_shape)","range(array_segments[prev_idx], array_segments[prev_idx + 1])",range(*array_segments[prev_idx:prev_idx + 2]),"iterable_zj[prev_idx], iterable_zj[prev_idx + 1]",*array_segments[prev_idx:prev_idx+2],*array_segments[prev_idx:prev_idx + 2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/topi/arm_cpu/injective.py,https://github.com/apache/tvm/tree/master/python/tvm/topi/arm_cpu/injective.py,,schedule_concatenate$81,"def schedule_concatenate(outs):
    """"""Schedule for concatenate op.

    Parameters
    ----------
    outs: Array of Tensor
          The computation graph description of concatenate in the format
          of an array of tensors.

    Returns
    -------
    sch: Schedule
        The computation schedule for the op.
    """"""
    outs = [outs] if isinstance(outs, te.tensor.Tensor) else outs
    s = te.create_schedule([x.op for x in outs])
    x = outs[0]
    tvm.te.schedule.AutoInlineInjective(s)
    if len(s[x].op.axis) >= 4:
        fused = s[x].fuse(s[x].op.axis[0], s[x].op.axis[1], s[x].op.axis[2])
        s[x].parallel(fused)
    elif len(s[x].op.axis) >= 3:
        fused = s[x].fuse(s[x].op.axis[0], s[x].op.axis[1])
        s[x].parallel(fused)
    elif len(s[x].op.axis) >= 2:
        s[x].parallel(s[x].op.axis[0])
    return s","s[x].fuse(s[x].op.axis[0], s[x].op.axis[1], s[x].op.axis[2])",s[x].fuse(*s[x].op.axis[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*s[x].op.axis[:3],*s[x].op.axis[:3],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/topi/arm_cpu/injective.py,https://github.com/apache/tvm/tree/master/python/tvm/topi/arm_cpu/injective.py,,schedule_concatenate$81,"def schedule_concatenate(outs):
    """"""Schedule for concatenate op.

    Parameters
    ----------
    outs: Array of Tensor
          The computation graph description of concatenate in the format
          of an array of tensors.

    Returns
    -------
    sch: Schedule
        The computation schedule for the op.
    """"""
    outs = [outs] if isinstance(outs, te.tensor.Tensor) else outs
    s = te.create_schedule([x.op for x in outs])
    x = outs[0]
    tvm.te.schedule.AutoInlineInjective(s)
    if len(s[x].op.axis) >= 4:
        fused = s[x].fuse(s[x].op.axis[0], s[x].op.axis[1], s[x].op.axis[2])
        s[x].parallel(fused)
    elif len(s[x].op.axis) >= 3:
        fused = s[x].fuse(s[x].op.axis[0], s[x].op.axis[1])
        s[x].parallel(fused)
    elif len(s[x].op.axis) >= 2:
        s[x].parallel(s[x].op.axis[0])
    return s","s[x].fuse(s[x].op.axis[0], s[x].op.axis[1])",s[x].fuse(*s[x].op.axis[:2]),"iterable_zj[0], iterable_zj[1]",*s[x].op.axis[:2],*s[x].op.axis[:2],1
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/elastic/datadog_checks/elastic/config.py,https://github.com/DataDog/integrations-core/tree/master/elastic/datadog_checks/elastic/config.py,,from_instance$31,"def from_instance(instance):
    """"""
    Create a config object from an instance dictionary
    """"""
    url = instance.get('url')
    if not url:
        raise ConfigurationError(""A URL must be specified in the instance"")

    pshard_stats = is_affirmative(instance.get('pshard_stats', False))
    pshard_graceful_to = is_affirmative(instance.get('pshard_graceful_timeout', False))
    node_name_as_host = is_affirmative(instance.get('node_name_as_host', False))
    index_stats = is_affirmative(instance.get('index_stats', False))
    cluster_stats = is_affirmative(instance.get('cluster_stats', False))
    detailed_index_stats = is_affirmative(instance.get('detailed_index_stats', False))
    slm_stats = is_affirmative(instance.get('slm_stats', False))
    if 'is_external' in instance:
        cluster_stats = is_affirmative(instance.get('is_external', False))
    pending_task_stats = is_affirmative(instance.get('pending_task_stats', True))
    admin_forwarder = is_affirmative(instance.get('admin_forwarder', False))
    cat_allocation_stats = is_affirmative(instance.get('cat_allocation_stats', False))

    # Support URLs that have a path in them from the config, for
    # backwards-compatibility.
    parsed = urlparse(url)
    if parsed[2] and not admin_forwarder:
        url = '{}://{}'.format(parsed[0], parsed[1])
    port = parsed.port
    host = parsed.hostname

    custom_tags = instance.get('tags', [])
    service_check_tags = ['host:{}'.format(host), 'port:{}'.format(port)]
    service_check_tags.extend(custom_tags)

    custom_queries = instance.get('custom_queries', [])

    # Tag by URL so we can differentiate the metrics
    # from multiple instances
    tags = ['url:{}'.format(url)]
    tags.extend(custom_tags)

    config = ESInstanceConfig(
        admin_forwarder=admin_forwarder,
        pshard_stats=pshard_stats,
        pshard_graceful_to=pshard_graceful_to,
        node_name_as_host=node_name_as_host,
        cluster_stats=cluster_stats,
        detailed_index_stats=detailed_index_stats,
        slm_stats=slm_stats,
        index_stats=index_stats,
        service_check_tags=service_check_tags,
        tags=tags,
        url=url,
        pending_task_stats=pending_task_stats,
        cat_allocation_stats=cat_allocation_stats,
        custom_queries=custom_queries,
    )
    return config","'{}://{}'.format(parsed[0], parsed[1])",'{}://{}'.format(*parsed[:2]),"iterable_zj[0], iterable_zj[1]",*parsed[:2],*parsed[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/contrib/test_ethosn/test_conv2d.py,https://github.com/apache/tvm/tree/master/tests/python/contrib/test_ethosn/test_conv2d.py,,test_conv2d$124,"def test_conv2d(
    dtype,
    shape,
    out_channels,
    kernel_size,
    pad,
    stride,
    qnn_per_channel,
):
    """"""Compare Conv2D output with TVM.""""""
    np.random.seed(0)

    dilation = (1, 1)
    groups = 1
    weight_format = ""HWIO""

    outputs = []
    inputs = {
        ""a"": tvm.nd.array(
            np.random.randint(
                np.iinfo(dtype).min,
                np.iinfo(dtype).max + 1,
                size=shape,
                dtype=dtype,
            )
        ),
    }
    input_zp = np.random.randint(np.iinfo(dtype).min, np.iinfo(dtype).max)
    input_sc = np.random.random() * 2
    if qnn_per_channel:
        kernel_sc = tvm.nd.array(
            np.random.uniform(low=0, high=2, size=(out_channels,)).astype(np.float32)
        )
    else:
        kernel_sc = np.random.random() * 2
    kernel_zp = (
        0 if dtype == ""int8"" else np.random.randint(np.iinfo(dtype).min, np.iinfo(dtype).max)
    )
    output_zp, output_sc = tei.get_conv2d_qnn_params(
        dtype, input_zp, input_sc, kernel_zp, kernel_sc, kernel_size[0], kernel_size[1], shape[3]
    )
    model, params = _get_model(
        shape,
        kernel_size[0],
        kernel_size[1],
        input_zp,
        input_sc,
        kernel_zp,
        kernel_sc,
        output_zp,
        output_sc,
        pad,
        stride,
        dilation,
        groups,
        dtype,
        out_channels,
        weight_format,
    )
    for npu in [False, True]:
        mod = tei.make_module(model, params)
        outputs.append(tei.build_and_run(mod, inputs, 1, params, npu=npu))

    tei.verify(outputs, dtype, 1)","tei.get_conv2d_qnn_params(dtype, input_zp, input_sc, kernel_zp, kernel_sc, kernel_size[0], kernel_size[1], shape[3])","tei.get_conv2d_qnn_params(dtype, input_zp, input_sc, kernel_zp, kernel_sc, *kernel_size[:2], shape[3])","iterable_zj[0], iterable_zj[1]",*kernel_size[:2],*kernel_size[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/contrib/test_ethosn/test_conv2d.py,https://github.com/apache/tvm/tree/master/tests/python/contrib/test_ethosn/test_conv2d.py,,test_conv2d$124,"def test_conv2d(
    dtype,
    shape,
    out_channels,
    kernel_size,
    pad,
    stride,
    qnn_per_channel,
):
    """"""Compare Conv2D output with TVM.""""""
    np.random.seed(0)

    dilation = (1, 1)
    groups = 1
    weight_format = ""HWIO""

    outputs = []
    inputs = {
        ""a"": tvm.nd.array(
            np.random.randint(
                np.iinfo(dtype).min,
                np.iinfo(dtype).max + 1,
                size=shape,
                dtype=dtype,
            )
        ),
    }
    input_zp = np.random.randint(np.iinfo(dtype).min, np.iinfo(dtype).max)
    input_sc = np.random.random() * 2
    if qnn_per_channel:
        kernel_sc = tvm.nd.array(
            np.random.uniform(low=0, high=2, size=(out_channels,)).astype(np.float32)
        )
    else:
        kernel_sc = np.random.random() * 2
    kernel_zp = (
        0 if dtype == ""int8"" else np.random.randint(np.iinfo(dtype).min, np.iinfo(dtype).max)
    )
    output_zp, output_sc = tei.get_conv2d_qnn_params(
        dtype, input_zp, input_sc, kernel_zp, kernel_sc, kernel_size[0], kernel_size[1], shape[3]
    )
    model, params = _get_model(
        shape,
        kernel_size[0],
        kernel_size[1],
        input_zp,
        input_sc,
        kernel_zp,
        kernel_sc,
        output_zp,
        output_sc,
        pad,
        stride,
        dilation,
        groups,
        dtype,
        out_channels,
        weight_format,
    )
    for npu in [False, True]:
        mod = tei.make_module(model, params)
        outputs.append(tei.build_and_run(mod, inputs, 1, params, npu=npu))

    tei.verify(outputs, dtype, 1)","_get_model(shape, kernel_size[0], kernel_size[1], input_zp, input_sc, kernel_zp, kernel_sc, output_zp, output_sc, pad, stride, dilation, groups, dtype, out_channels, weight_format)","_get_model(shape, *kernel_size[:2], input_zp, input_sc, kernel_zp, kernel_sc, output_zp, output_sc, pad, stride, dilation, groups, dtype, out_channels, weight_format)","iterable_zj[0], iterable_zj[1]",*kernel_size[:2],*kernel_size[:2],1
AugLy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AugLy/augly/image/functional.py,https://github.com/facebookresearch/AugLy/tree/master/augly/image/functional.py,,overlay_onto_screenshot$1281,"def overlay_onto_screenshot(
    image: Union[str, Image.Image],
    output_path: Optional[str] = None,
    template_filepath: str = utils.TEMPLATE_PATH,
    template_bboxes_filepath: str = utils.BBOXES_PATH,
    max_image_size_pixels: Optional[int] = None,
    crop_src_to_fit: bool = False,
    resize_src_to_match_template: bool = True,
    metadata: Optional[List[Dict[str, Any]]] = None,
    bboxes: Optional[List[Tuple]] = None,
    bbox_format: Optional[str] = None,
) -> Image.Image:
    """"""
    Overlay the image onto a screenshot template so it looks like it was
    screenshotted on Instagram

    @param image: the path to an image or a variable of type PIL.Image.Image
        to be augmented

    @param output_path: the path in which the resulting image will be stored.
        If None, the resulting PIL Image will still be returned

    @param template_filepath: iopath uri to the screenshot template

    @param template_bboxes_filepath: iopath uri to the file containing the
        bounding box for each template

    @param max_image_size_pixels: if provided, the template image and/or src image
        will be scaled down to avoid an output image with an area greater than this
        size (in pixels)

    @param crop_src_to_fit: if True, the src image will be cropped if necessary to fit
        into the template image if the aspect ratios are different. If False, the src
        image will instead be resized if needed

    @param resize_src_to_match_template: if True, the src image will be resized if it is
        too big or small in both dimensions to better match the template image. If False,
        the template image will be resized to match the src image instead. It can be
        useful to set this to True if the src image is very large so that the augmented
        image isn't huge, but instead is the same size as the template image

    @param metadata: if set to be a list, metadata about the function execution
        including its name, the source & dest width, height, etc. will be appended
        to the inputted list. If set to None, no metadata will be appended or returned

    @param bboxes: a list of bounding boxes can be passed in here if desired. If
        provided, this list will be modified in place such that each bounding box is
        transformed according to this function

    @param bbox_format: signifies what bounding box format was used in `bboxes`. Must
        specify `bbox_format` if `bboxes` is provided. Supported bbox_format values are
        ""pascal_voc"", ""pascal_voc_norm"", ""coco"", and ""yolo""

    @returns: the augmented PIL Image
    """"""
    image = imutils.validate_and_load_image(image)

    func_kwargs = imutils.get_func_kwargs(metadata, locals())
    src_mode = image.mode

    template, bbox = imutils.get_template_and_bbox(
        template_filepath, template_bboxes_filepath
    )

    if resize_src_to_match_template:
        bbox_w, bbox_h = bbox[2] - bbox[0], bbox[3] - bbox[1]
        image = scale(image, factor=min(bbox_w / image.width, bbox_h / image.height))
    else:
        template, bbox = imutils.scale_template_image(
            image.size[0],
            image.size[1],
            template,
            bbox,
            max_image_size_pixels,
            crop_src_to_fit,
        )
        bbox_w, bbox_h = bbox[2] - bbox[0], bbox[3] - bbox[1]

    cropped_src = imutils.resize_and_pad_to_given_size(
        image, bbox_w, bbox_h, crop=crop_src_to_fit
    )
    template.paste(cropped_src, box=bbox)

    imutils.get_metadata(
        metadata=metadata,
        function_name=""overlay_onto_screenshot"",
        aug_image=template,
        **func_kwargs,
    )

    return imutils.ret_and_save_image(template, output_path, src_mode)","imutils.scale_template_image(image.size[0], image.size[1], template, bbox, max_image_size_pixels, crop_src_to_fit)","imutils.scale_template_image(*image.size[:2], template, bbox, max_image_size_pixels, crop_src_to_fit)","iterable_zj[0], iterable_zj[1]",*image.size[:2],*image.size[:2],1
adversarial-attacks-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adversarial-attacks-pytorch/torchattacks/attacks/fab.py,https://github.com/Harry24k/adversarial-attacks-pytorch/tree/master/torchattacks/attacks/fab.py,FAB,attack_single_run$128,"def attack_single_run(self, x, y=None, use_rand_start=False):
        """"""
        :param x:    clean images
        :param y:    clean labels, if None we use the predicted labels
        """"""

        # self.device = x.device
        self.orig_dim = list(x.shape[1:])
        self.ndims = len(self.orig_dim)

        x = x.detach().clone().float().to(self.device)
        # assert next(self.model.parameters()).device == x.device

        y_pred = self._get_predicted_label(x)
        if y is None:
            y = y_pred.detach().clone().long().to(self.device)
        else:
            y = y.detach().clone().long().to(self.device)
        pred = y_pred == y
        corr_classified = pred.float().sum()
        if self.verbose:
            print('Clean accuracy: {:.2%}'.format(pred.float().mean()))
        if pred.sum() == 0:
            return x
        pred = self.check_shape(pred.nonzero().squeeze())

        startt = time.time()
        # runs the attack only on correctly classified points
        im2 = x[pred].detach().clone()
        la2 = y[pred].detach().clone()
        if len(im2.shape) == self.ndims:
            im2 = im2.unsqueeze(0)
        bs = im2.shape[0]
        u1 = torch.arange(bs)
        adv = im2.clone()
        adv_c = x.clone()
        res2 = 1e10 * torch.ones([bs]).to(self.device)
        res_c = torch.zeros([x.shape[0]]).to(self.device)
        x1 = im2.clone()
        x0 = im2.clone().reshape([bs, -1])
        counter_restarts = 0

        while counter_restarts < 1:
            if use_rand_start:
                if self.norm == 'Linf':
                    t = 2 * torch.rand(x1.shape).to(self.device) - 1
                    x1 = im2 + (torch.min(res2,
                                          self.eps * torch.ones(res2.shape)
                                          .to(self.device)
                                          ).reshape([-1, *[1]*self.ndims])
                                ) * t / (t.reshape([t.shape[0], -1]).abs()
                                         .max(dim=1, keepdim=True)[0]
                                         .reshape([-1, *[1]*self.ndims])) * .5
                elif self.norm == 'L2':
                    t = torch.randn(x1.shape).to(self.device)
                    x1 = im2 + (torch.min(res2,
                                          self.eps * torch.ones(res2.shape)
                                          .to(self.device)
                                          ).reshape([-1, *[1]*self.ndims])
                                ) * t / ((t ** 2)
                                         .view(t.shape[0], -1)
                                         .sum(dim=-1)
                                         .sqrt()
                                         .view(t.shape[0], *[1]*self.ndims)) * .5
                elif self.norm == 'L1':
                    t = torch.randn(x1.shape).to(self.device)
                    x1 = im2 + (torch.min(res2,
                                          self.eps * torch.ones(res2.shape)
                                          .to(self.device)
                                          ).reshape([-1, *[1]*self.ndims])
                                ) * t / (t.abs().view(t.shape[0], -1)
                                         .sum(dim=-1)
                                         .view(t.shape[0], *[1]*self.ndims)) / 2

                x1 = x1.clamp(0.0, 1.0)

            counter_iter = 0
            while counter_iter < self.steps:
                with torch.no_grad():
                    df, dg = self.get_diff_logits_grads_batch(x1, la2)
                    if self.norm == 'Linf':
                        dist1 = df.abs() / (1e-12 +
                                            dg.abs()
                                            .view(dg.shape[0], dg.shape[1], -1)
                                            .sum(dim=-1))
                    elif self.norm == 'L2':
                        dist1 = df.abs() / (1e-12 + (dg ** 2)
                                            .view(dg.shape[0], dg.shape[1], -1)
                                            .sum(dim=-1).sqrt())
                    elif self.norm == 'L1':
                        dist1 = df.abs() / (1e-12 + dg.abs().reshape(
                            [df.shape[0], df.shape[1], -1]).max(dim=2)[0])
                    else:
                        raise ValueError('norm not supported')
                    ind = dist1.min(dim=1)[1]
                    dg2 = dg[u1, ind]
                    b = (- df[u1, ind] + (dg2 * x1).view(x1.shape[0], -1)
                                         .sum(dim=-1))
                    w = dg2.reshape([bs, -1])

                    if self.norm == 'Linf':
                        d3 = projection_linf(
                            torch.cat((x1.reshape([bs, -1]), x0), 0),
                            torch.cat((w, w), 0),
                            torch.cat((b, b), 0))
                    elif self.norm == 'L2':
                        d3 = projection_l2(
                            torch.cat((x1.reshape([bs, -1]), x0), 0),
                            torch.cat((w, w), 0),
                            torch.cat((b, b), 0))
                    elif self.norm == 'L1':
                        d3 = projection_l1(
                            torch.cat((x1.reshape([bs, -1]), x0), 0),
                            torch.cat((w, w), 0),
                            torch.cat((b, b), 0))
                    d1 = torch.reshape(d3[:bs], x1.shape)
                    d2 = torch.reshape(d3[-bs:], x1.shape)
                    if self.norm == 'Linf':
                        a0 = d3.abs().max(dim=1, keepdim=True)[0]\
                            .view(-1, *[1]*self.ndims)
                    elif self.norm == 'L2':
                        a0 = (d3 ** 2).sum(dim=1, keepdim=True).sqrt()\
                            .view(-1, *[1]*self.ndims)
                    elif self.norm == 'L1':
                        a0 = d3.abs().sum(dim=1, keepdim=True)\
                            .view(-1, *[1]*self.ndims)
                    a0 = torch.max(a0, 1e-8 * torch.ones(
                        a0.shape).to(self.device))
                    a1 = a0[:bs]
                    a2 = a0[-bs:]
                    alpha = torch.min(torch.max(a1 / (a1 + a2),
                                                torch.zeros(a1.shape)
                                                .to(self.device)),
                                      self.alpha_max * torch.ones(a1.shape)
                                      .to(self.device))
                    x1 = ((x1 + self.eta * d1) * (1 - alpha) +
                          (im2 + d2 * self.eta) * alpha).clamp(0.0, 1.0)

                    is_adv = self._get_predicted_label(x1) != la2

                    if is_adv.sum() > 0:
                        ind_adv = is_adv.nonzero().squeeze()
                        ind_adv = self.check_shape(ind_adv)
                        if self.norm == 'Linf':
                            t = (x1[ind_adv] - im2[ind_adv]).reshape(
                                [ind_adv.shape[0], -1]).abs().max(dim=1)[0]
                        elif self.norm == 'L2':
                            t = ((x1[ind_adv] - im2[ind_adv]) ** 2)\
                                .view(ind_adv.shape[0], -1).sum(dim=-1).sqrt()
                        elif self.norm == 'L1':
                            t = (x1[ind_adv] - im2[ind_adv])\
                                .abs().view(ind_adv.shape[0], -1).sum(dim=-1)
                        adv[ind_adv] = x1[ind_adv] * (t < res2[ind_adv]).\
                            float().reshape([-1, *[1]*self.ndims]) + adv[ind_adv]\
                            * (t >= res2[ind_adv]).float().reshape(
                            [-1, *[1]*self.ndims])
                        res2[ind_adv] = t * (t < res2[ind_adv]).float()\
                            + res2[ind_adv] * (t >= res2[ind_adv]).float()
                        x1[ind_adv] = im2[ind_adv] + (
                            x1[ind_adv] - im2[ind_adv]) * self.beta

                    counter_iter += 1

            counter_restarts += 1

        ind_succ = res2 < 1e10
        if self.verbose:
            print('success rate: {:.0f}/{:.0f}'
                  .format(ind_succ.float().sum(), corr_classified) +
                  ' (on correctly classified points) in {:.1f} s'
                  .format(time.time() - startt))

        res_c[pred] = res2 * ind_succ.float() + 1e10 * (1 - ind_succ.float())
        ind_succ = self.check_shape(ind_succ.nonzero().squeeze())
        adv_c[pred[ind_succ]] = adv[ind_succ].clone()

        return adv_c","dg.abs().view(dg.shape[0], dg.shape[1], -1)","dg.abs().view(*dg.shape[:2], -1)","iterable_zj[0], iterable_zj[1]",*dg.shape[:2],*dg.shape[:2],1
adversarial-attacks-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adversarial-attacks-pytorch/torchattacks/attacks/fab.py,https://github.com/Harry24k/adversarial-attacks-pytorch/tree/master/torchattacks/attacks/fab.py,FAB,attack_single_run$128,"def attack_single_run(self, x, y=None, use_rand_start=False):
        """"""
        :param x:    clean images
        :param y:    clean labels, if None we use the predicted labels
        """"""

        # self.device = x.device
        self.orig_dim = list(x.shape[1:])
        self.ndims = len(self.orig_dim)

        x = x.detach().clone().float().to(self.device)
        # assert next(self.model.parameters()).device == x.device

        y_pred = self._get_predicted_label(x)
        if y is None:
            y = y_pred.detach().clone().long().to(self.device)
        else:
            y = y.detach().clone().long().to(self.device)
        pred = y_pred == y
        corr_classified = pred.float().sum()
        if self.verbose:
            print('Clean accuracy: {:.2%}'.format(pred.float().mean()))
        if pred.sum() == 0:
            return x
        pred = self.check_shape(pred.nonzero().squeeze())

        startt = time.time()
        # runs the attack only on correctly classified points
        im2 = x[pred].detach().clone()
        la2 = y[pred].detach().clone()
        if len(im2.shape) == self.ndims:
            im2 = im2.unsqueeze(0)
        bs = im2.shape[0]
        u1 = torch.arange(bs)
        adv = im2.clone()
        adv_c = x.clone()
        res2 = 1e10 * torch.ones([bs]).to(self.device)
        res_c = torch.zeros([x.shape[0]]).to(self.device)
        x1 = im2.clone()
        x0 = im2.clone().reshape([bs, -1])
        counter_restarts = 0

        while counter_restarts < 1:
            if use_rand_start:
                if self.norm == 'Linf':
                    t = 2 * torch.rand(x1.shape).to(self.device) - 1
                    x1 = im2 + (torch.min(res2,
                                          self.eps * torch.ones(res2.shape)
                                          .to(self.device)
                                          ).reshape([-1, *[1]*self.ndims])
                                ) * t / (t.reshape([t.shape[0], -1]).abs()
                                         .max(dim=1, keepdim=True)[0]
                                         .reshape([-1, *[1]*self.ndims])) * .5
                elif self.norm == 'L2':
                    t = torch.randn(x1.shape).to(self.device)
                    x1 = im2 + (torch.min(res2,
                                          self.eps * torch.ones(res2.shape)
                                          .to(self.device)
                                          ).reshape([-1, *[1]*self.ndims])
                                ) * t / ((t ** 2)
                                         .view(t.shape[0], -1)
                                         .sum(dim=-1)
                                         .sqrt()
                                         .view(t.shape[0], *[1]*self.ndims)) * .5
                elif self.norm == 'L1':
                    t = torch.randn(x1.shape).to(self.device)
                    x1 = im2 + (torch.min(res2,
                                          self.eps * torch.ones(res2.shape)
                                          .to(self.device)
                                          ).reshape([-1, *[1]*self.ndims])
                                ) * t / (t.abs().view(t.shape[0], -1)
                                         .sum(dim=-1)
                                         .view(t.shape[0], *[1]*self.ndims)) / 2

                x1 = x1.clamp(0.0, 1.0)

            counter_iter = 0
            while counter_iter < self.steps:
                with torch.no_grad():
                    df, dg = self.get_diff_logits_grads_batch(x1, la2)
                    if self.norm == 'Linf':
                        dist1 = df.abs() / (1e-12 +
                                            dg.abs()
                                            .view(dg.shape[0], dg.shape[1], -1)
                                            .sum(dim=-1))
                    elif self.norm == 'L2':
                        dist1 = df.abs() / (1e-12 + (dg ** 2)
                                            .view(dg.shape[0], dg.shape[1], -1)
                                            .sum(dim=-1).sqrt())
                    elif self.norm == 'L1':
                        dist1 = df.abs() / (1e-12 + dg.abs().reshape(
                            [df.shape[0], df.shape[1], -1]).max(dim=2)[0])
                    else:
                        raise ValueError('norm not supported')
                    ind = dist1.min(dim=1)[1]
                    dg2 = dg[u1, ind]
                    b = (- df[u1, ind] + (dg2 * x1).view(x1.shape[0], -1)
                                         .sum(dim=-1))
                    w = dg2.reshape([bs, -1])

                    if self.norm == 'Linf':
                        d3 = projection_linf(
                            torch.cat((x1.reshape([bs, -1]), x0), 0),
                            torch.cat((w, w), 0),
                            torch.cat((b, b), 0))
                    elif self.norm == 'L2':
                        d3 = projection_l2(
                            torch.cat((x1.reshape([bs, -1]), x0), 0),
                            torch.cat((w, w), 0),
                            torch.cat((b, b), 0))
                    elif self.norm == 'L1':
                        d3 = projection_l1(
                            torch.cat((x1.reshape([bs, -1]), x0), 0),
                            torch.cat((w, w), 0),
                            torch.cat((b, b), 0))
                    d1 = torch.reshape(d3[:bs], x1.shape)
                    d2 = torch.reshape(d3[-bs:], x1.shape)
                    if self.norm == 'Linf':
                        a0 = d3.abs().max(dim=1, keepdim=True)[0]\
                            .view(-1, *[1]*self.ndims)
                    elif self.norm == 'L2':
                        a0 = (d3 ** 2).sum(dim=1, keepdim=True).sqrt()\
                            .view(-1, *[1]*self.ndims)
                    elif self.norm == 'L1':
                        a0 = d3.abs().sum(dim=1, keepdim=True)\
                            .view(-1, *[1]*self.ndims)
                    a0 = torch.max(a0, 1e-8 * torch.ones(
                        a0.shape).to(self.device))
                    a1 = a0[:bs]
                    a2 = a0[-bs:]
                    alpha = torch.min(torch.max(a1 / (a1 + a2),
                                                torch.zeros(a1.shape)
                                                .to(self.device)),
                                      self.alpha_max * torch.ones(a1.shape)
                                      .to(self.device))
                    x1 = ((x1 + self.eta * d1) * (1 - alpha) +
                          (im2 + d2 * self.eta) * alpha).clamp(0.0, 1.0)

                    is_adv = self._get_predicted_label(x1) != la2

                    if is_adv.sum() > 0:
                        ind_adv = is_adv.nonzero().squeeze()
                        ind_adv = self.check_shape(ind_adv)
                        if self.norm == 'Linf':
                            t = (x1[ind_adv] - im2[ind_adv]).reshape(
                                [ind_adv.shape[0], -1]).abs().max(dim=1)[0]
                        elif self.norm == 'L2':
                            t = ((x1[ind_adv] - im2[ind_adv]) ** 2)\
                                .view(ind_adv.shape[0], -1).sum(dim=-1).sqrt()
                        elif self.norm == 'L1':
                            t = (x1[ind_adv] - im2[ind_adv])\
                                .abs().view(ind_adv.shape[0], -1).sum(dim=-1)
                        adv[ind_adv] = x1[ind_adv] * (t < res2[ind_adv]).\
                            float().reshape([-1, *[1]*self.ndims]) + adv[ind_adv]\
                            * (t >= res2[ind_adv]).float().reshape(
                            [-1, *[1]*self.ndims])
                        res2[ind_adv] = t * (t < res2[ind_adv]).float()\
                            + res2[ind_adv] * (t >= res2[ind_adv]).float()
                        x1[ind_adv] = im2[ind_adv] + (
                            x1[ind_adv] - im2[ind_adv]) * self.beta

                    counter_iter += 1

            counter_restarts += 1

        ind_succ = res2 < 1e10
        if self.verbose:
            print('success rate: {:.0f}/{:.0f}'
                  .format(ind_succ.float().sum(), corr_classified) +
                  ' (on correctly classified points) in {:.1f} s'
                  .format(time.time() - startt))

        res_c[pred] = res2 * ind_succ.float() + 1e10 * (1 - ind_succ.float())
        ind_succ = self.check_shape(ind_succ.nonzero().squeeze())
        adv_c[pred[ind_succ]] = adv[ind_succ].clone()

        return adv_c","(dg ** 2).view(dg.shape[0], dg.shape[1], -1)","(dg ** 2).view(*dg.shape[:2], -1)","iterable_zj[0], iterable_zj[1]",*dg.shape[:2],*dg.shape[:2],1
federated,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/federated/tensorflow_federated/python/core/impl/compiler/transformation_utils_test.py,https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/core/impl/compiler/transformation_utils_test.py,TransformationUtilsTest,test_get_count_of_references_to_variables_sequential_overwrite_in_block_locals$1456,"def test_get_count_of_references_to_variables_sequential_overwrite_in_block_locals(
      self):

    tensor_type = computation_types.TensorType(tf.int32)
    make_10 = building_block_factory.create_tensorflow_constant(tensor_type, 10)

    whimsy_x_reference = building_blocks.Reference('x', tf.int32)

    make_13 = building_blocks.Block([
        ('x', make_10),
        ('x', whimsy_x_reference),
        ('x', whimsy_x_reference),
        ('x', whimsy_x_reference),
    ], whimsy_x_reference)

    references = transformation_utils.get_count_of_references_to_variables(
        make_13)

    child_id = list(references.active_node.children.keys())[0]

    def _make_context_tree():
      constructed_context_stack = transformation_utils.SymbolTree(
          transformation_utils.ReferenceCounter)
      constructed_context_stack.drop_scope_down(child_id)
      constructed_context_stack.ingest_variable_binding(make_13.locals[0][0],
                                                        make_13.locals[0][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[1][0],
                                                        make_13.locals[1][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[2][0],
                                                        make_13.locals[2][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[3][0],
                                                        make_13.locals[3][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.walk_to_scope_beginning()
      return constructed_context_stack

    constructed_tree = _make_context_tree()
    self.assertEqual(references, constructed_tree)","constructed_context_stack.ingest_variable_binding(make_13.locals[0][0], make_13.locals[0][1])",constructed_context_stack.ingest_variable_binding(*make_13.locals[0][:2]),"iterable_zj[0], iterable_zj[1]",*make_13.locals[0][:2],*make_13.locals[0][:2],1
federated,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/federated/tensorflow_federated/python/core/impl/compiler/transformation_utils_test.py,https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/core/impl/compiler/transformation_utils_test.py,TransformationUtilsTest,test_get_count_of_references_to_variables_sequential_overwrite_in_block_locals$1456,"def test_get_count_of_references_to_variables_sequential_overwrite_in_block_locals(
      self):

    tensor_type = computation_types.TensorType(tf.int32)
    make_10 = building_block_factory.create_tensorflow_constant(tensor_type, 10)

    whimsy_x_reference = building_blocks.Reference('x', tf.int32)

    make_13 = building_blocks.Block([
        ('x', make_10),
        ('x', whimsy_x_reference),
        ('x', whimsy_x_reference),
        ('x', whimsy_x_reference),
    ], whimsy_x_reference)

    references = transformation_utils.get_count_of_references_to_variables(
        make_13)

    child_id = list(references.active_node.children.keys())[0]

    def _make_context_tree():
      constructed_context_stack = transformation_utils.SymbolTree(
          transformation_utils.ReferenceCounter)
      constructed_context_stack.drop_scope_down(child_id)
      constructed_context_stack.ingest_variable_binding(make_13.locals[0][0],
                                                        make_13.locals[0][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[1][0],
                                                        make_13.locals[1][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[2][0],
                                                        make_13.locals[2][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[3][0],
                                                        make_13.locals[3][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.walk_to_scope_beginning()
      return constructed_context_stack

    constructed_tree = _make_context_tree()
    self.assertEqual(references, constructed_tree)","constructed_context_stack.ingest_variable_binding(make_13.locals[1][0], make_13.locals[1][1])",constructed_context_stack.ingest_variable_binding(*make_13.locals[1][:2]),"iterable_zj[0], iterable_zj[1]",*make_13.locals[1][:2],*make_13.locals[1][:2],1
federated,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/federated/tensorflow_federated/python/core/impl/compiler/transformation_utils_test.py,https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/core/impl/compiler/transformation_utils_test.py,TransformationUtilsTest,test_get_count_of_references_to_variables_sequential_overwrite_in_block_locals$1456,"def test_get_count_of_references_to_variables_sequential_overwrite_in_block_locals(
      self):

    tensor_type = computation_types.TensorType(tf.int32)
    make_10 = building_block_factory.create_tensorflow_constant(tensor_type, 10)

    whimsy_x_reference = building_blocks.Reference('x', tf.int32)

    make_13 = building_blocks.Block([
        ('x', make_10),
        ('x', whimsy_x_reference),
        ('x', whimsy_x_reference),
        ('x', whimsy_x_reference),
    ], whimsy_x_reference)

    references = transformation_utils.get_count_of_references_to_variables(
        make_13)

    child_id = list(references.active_node.children.keys())[0]

    def _make_context_tree():
      constructed_context_stack = transformation_utils.SymbolTree(
          transformation_utils.ReferenceCounter)
      constructed_context_stack.drop_scope_down(child_id)
      constructed_context_stack.ingest_variable_binding(make_13.locals[0][0],
                                                        make_13.locals[0][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[1][0],
                                                        make_13.locals[1][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[2][0],
                                                        make_13.locals[2][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[3][0],
                                                        make_13.locals[3][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.walk_to_scope_beginning()
      return constructed_context_stack

    constructed_tree = _make_context_tree()
    self.assertEqual(references, constructed_tree)","constructed_context_stack.ingest_variable_binding(make_13.locals[2][0], make_13.locals[2][1])",constructed_context_stack.ingest_variable_binding(*make_13.locals[2][:2]),"iterable_zj[0], iterable_zj[1]",*make_13.locals[2][:2],*make_13.locals[2][:2],1
federated,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/federated/tensorflow_federated/python/core/impl/compiler/transformation_utils_test.py,https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/core/impl/compiler/transformation_utils_test.py,TransformationUtilsTest,test_get_count_of_references_to_variables_sequential_overwrite_in_block_locals$1456,"def test_get_count_of_references_to_variables_sequential_overwrite_in_block_locals(
      self):

    tensor_type = computation_types.TensorType(tf.int32)
    make_10 = building_block_factory.create_tensorflow_constant(tensor_type, 10)

    whimsy_x_reference = building_blocks.Reference('x', tf.int32)

    make_13 = building_blocks.Block([
        ('x', make_10),
        ('x', whimsy_x_reference),
        ('x', whimsy_x_reference),
        ('x', whimsy_x_reference),
    ], whimsy_x_reference)

    references = transformation_utils.get_count_of_references_to_variables(
        make_13)

    child_id = list(references.active_node.children.keys())[0]

    def _make_context_tree():
      constructed_context_stack = transformation_utils.SymbolTree(
          transformation_utils.ReferenceCounter)
      constructed_context_stack.drop_scope_down(child_id)
      constructed_context_stack.ingest_variable_binding(make_13.locals[0][0],
                                                        make_13.locals[0][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[1][0],
                                                        make_13.locals[1][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[2][0],
                                                        make_13.locals[2][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.ingest_variable_binding(make_13.locals[3][0],
                                                        make_13.locals[3][1])
      constructed_context_stack.update_payload_with_name(
          whimsy_x_reference.name)
      constructed_context_stack.walk_to_scope_beginning()
      return constructed_context_stack

    constructed_tree = _make_context_tree()
    self.assertEqual(references, constructed_tree)","constructed_context_stack.ingest_variable_binding(make_13.locals[3][0], make_13.locals[3][1])",constructed_context_stack.ingest_variable_binding(*make_13.locals[3][:2]),"iterable_zj[0], iterable_zj[1]",*make_13.locals[3][:2],*make_13.locals[3][:2],1
rope,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rope/rope/refactor/similarfinder.py,https://github.com/python-rope/rope/tree/master/rope/refactor/similarfinder.py,CodeTemplate,substitute$284,"def substitute(self, mapping):
        collector = codeanalyze.ChangeCollector(self.template)
        for name, occurrences in self.names.items():
            for region in occurrences:
                collector.add_change(region[0], region[1], mapping[name])
        result = collector.get_changed()
        if result is None:
            return self.template
        return result","collector.add_change(region[0], region[1], mapping[name])","collector.add_change(*region[:2], mapping[name])","iterable_zj[0], iterable_zj[1]",*region[:2],*region[:2],1
pyrobot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyrobot/src/pyrobot/locobot/base.py,https://github.com/facebookresearch/pyrobot/tree/master/src/pyrobot/locobot/base.py,LoCoBotBase,get_plan$471,"def get_plan(self, xyt_position):
        """"""
        Generates a plan that can take take the robot to given goal state.

        :param xyt_position: The goal state of the form (x,y,t)

        :type xyt_position: list
        """"""
        plan, status = self.planner.get_plan_absolute(
            xyt_position[0], xyt_position[1], xyt_position[2]
        )
        if not status:
            raise ValueError(""Failed to find a valid plan!"")
        return self.planner.parse_plan(plan)","self.planner.get_plan_absolute(xyt_position[0], xyt_position[1], xyt_position[2])",self.planner.get_plan_absolute(*xyt_position[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*xyt_position[:3],*xyt_position[:3],1
PhiFlow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/torch/_torch_nn_util.py,https://github.com/tum-pbs/PhiFlow/tree/master/phi/torch/_torch_nn_util.py,UNet,__init__$61,"def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 filter_counts: tuple,
                 batch_norm=True):
        super(UNet, self).__init__()
        assert batch_norm, ""Not yet implemented""  # TODO
        self._levels = len(filter_counts)
        self.inc = DoubleConv(in_channels, filter_counts[0])
        for i in range(1, self._levels):
            self.add_module(f'down{i}', Down(filter_counts[i - 1], filter_counts[i]))
            self.add_module(f'up{i}', Up(filter_counts[i] + filter_counts[i-1], filter_counts[i - 1]))
        self.outc = OutConv(filter_counts[0], out_channels)","Down(filter_counts[i - 1], filter_counts[i])",Down(*filter_counts[i - 1:i + 1]),"iterable_zj[i - 1], iterable_zj[i]",*filter_counts[i-1:i+1],*filter_counts[i - 1:i + 1],1
spot-sdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spot-sdk/python/examples/arm_gcode/gcode.py,https://github.com/boston-dynamics/spot-sdk/tree/master/python/examples/arm_gcode/gcode.py,,run_gcode_program$517,"def run_gcode_program(config):
    """"""A simple example of using the Boston Dynamics API to command a Spot robot.""""""

    config_parser = configparser.ConfigParser()
    config_parser.read_file(open('gcode.cfg'))
    gcode_file = config_parser.get(""General"", ""gcode_file"")
    scale = config_parser.getfloat(""General"", ""scale"")
    min_dist_to_goal = config_parser.getfloat(""General"", ""min_dist_to_goal"")
    allow_walking = config_parser.getboolean(""General"", ""allow_walking"")
    velocity = config_parser.getfloat(""General"", ""velocity"")
    press_force_percent = config_parser.getfloat(""General"", ""press_force_percent"")
    below_z_is_admittance = config_parser.getfloat(""General"", ""below_z_is_admittance"")
    travel_z = config_parser.getfloat(""General"", ""travel_z"")
    gcode_start_x = config_parser.getfloat(""General"", ""gcode_start_x"")
    gcode_start_y = config_parser.getfloat(""General"", ""gcode_start_y"")
    draw_on_wall = config_parser.getboolean(""General"", ""draw_on_wall"")
    use_vision_frame = config_parser.getboolean(""General"", ""use_vision_frame"")
    use_xy_to_z_cross_term = config_parser.getboolean(""General"", ""use_xy_to_z_cross_term"")
    bias_force_x = config_parser.getfloat(""General"", ""bias_force_x"")

    if config_parser.has_option(""General"",
                                ""walk_to_at_end_rt_gcode_origin_x"") and config_parser.has_option(
                                    ""General"", ""walk_to_at_end_rt_gcode_origin_y""):
        walk_to_at_end_rt_gcode_origin_x = config_parser.getfloat(
            ""General"", ""walk_to_at_end_rt_gcode_origin_x"")
        walk_to_at_end_rt_gcode_origin_y = config_parser.getfloat(
            ""General"", ""walk_to_at_end_rt_gcode_origin_y"")
    else:
        walk_to_at_end_rt_gcode_origin_x = None
        walk_to_at_end_rt_gcode_origin_y = None

    if velocity <= 0:
        print('Velocity must be greater than 0.  Currently is: ', velocity)
        return

    if use_vision_frame:
        api_send_frame = VISION_FRAME_NAME
    else:
        api_send_frame = ODOM_FRAME_NAME

    # The Boston Dynamics Python library uses Python's logging module to
    # generate output. Applications using the library can specify how
    # the logging information should be output.
    bosdyn.client.util.setup_logging(config.verbose)

    # The SDK object is the primary entry point to the Boston Dynamics API.
    # create_standard_sdk will initialize an SDK object with typical default
    # parameters. The argument passed in is a string identifying the client.
    sdk = bosdyn.client.create_standard_sdk('GcodeClient')

    # A Robot object represents a single robot. Clients using the Boston
    # Dynamics API can manage multiple robots, but this tutorial limits
    # access to just one. The network address of the robot needs to be
    # specified to reach it. This can be done with a DNS name
    # (e.g. spot.intranet.example.com) or an IP literal (e.g. 10.0.63.1)
    robot = sdk.create_robot(config.hostname)

    gcode = GcodeReader(gcode_file, scale, robot.logger, below_z_is_admittance, travel_z,
                        draw_on_wall, gcode_start_x, gcode_start_y)

    if config.test_file_parsing:
        gcode.test_file_parsing()
        return

    # Clients need to authenticate to a robot before being able to use it.
    bosdyn.client.util.authenticate(robot)

    # Establish time sync with the robot. This kicks off a background thread to establish time sync.
    # Time sync is required to issue commands to the robot. After starting time sync thread, block
    # until sync is established.
    robot.time_sync.wait_for_sync()

    # Verify the robot has an arm.
    assert robot.has_arm(), ""Robot requires an arm to run the gcode example.""

    # Verify the robot is not estopped and that an external application has registered and holds
    # an estop endpoint.
    assert not robot.is_estopped(), ""Robot is estopped. Please use an external E-Stop client, "" \
                                    ""such as the estop SDK example, to configure E-Stop.""

    arm_surface_contact_client = robot.ensure_client(ArmSurfaceContactClient.default_service_name)

    # Only one client at a time can operate a robot. Clients acquire a lease to
    # indicate that they want to control a robot. Acquiring may fail if another
    # client is currently controlling the robot. When the client is done
    # controlling the robot, it should return the lease so other clients can
    # control it. Note that the lease is returned as the ""finally"" condition in this
    # try-catch-finally block.
    lease_client = robot.ensure_client(bosdyn.client.lease.LeaseClient.default_service_name)
    with bosdyn.client.lease.LeaseKeepAlive(lease_client, must_acquire=True, return_at_exit=True):
        # Now, we are ready to power on the robot. This call will block until the power
        # is on. Commands would fail if this did not happen. We can also check that the robot is
        # powered at any point.
        robot.logger.info(""Powering on robot... This may take a several seconds."")
        robot.power_on(timeout_sec=20)
        assert robot.is_powered_on(), ""Robot power on failed.""
        robot.logger.info(""Robot powered on."")

        # Tell the robot to stand up. The command service is used to issue commands to a robot.
        # The set of valid commands for a robot depends on hardware configuration. See
        # RobotCommandBuilder for more detailed examples on command building. The robot
        # command service requires timesync between the robot and the client.
        robot.logger.info(""Commanding robot to stand..."")
        command_client = robot.ensure_client(RobotCommandClient.default_service_name)
        blocking_stand(command_client, timeout_sec=10)
        robot.logger.info(""Robot standing."")

        robot_state_client = robot.ensure_client(RobotStateClient.default_service_name)
        # Update state
        robot_state = robot_state_client.get_robot_state()

        # Prep arm

        # Build a position to move the arm to (in meters, relative to the body frame's origin)
        x = 0.75
        y = 0

        if not draw_on_wall:
            z = -0.35

            qw = .707
            qx = 0
            qy = .707
            qz = 0
        else:
            z = -0.25

            qw = 1
            qx = 0
            qy = 0
            qz = 0

        flat_body_T_hand = math_helpers.SE3Pose(x, y, z, math_helpers.Quat(w=qw, x=qx, y=qy, z=qz))
        odom_T_flat_body = get_a_tform_b(robot_state.kinematic_state.transforms_snapshot,
                                         ODOM_FRAME_NAME, GRAV_ALIGNED_BODY_FRAME_NAME)
        odom_T_hand = odom_T_flat_body * flat_body_T_hand

        robot.logger.info('Moving arm to starting position.')

        # Send the request
        odom_T_hand_obj = odom_T_hand.to_proto()

        move_time = 0.000001  # move as fast as possible because we will use (default) velocity/accel limiting.

        arm_command = RobotCommandBuilder.arm_pose_command(
            odom_T_hand_obj.position.x, odom_T_hand_obj.position.y, odom_T_hand_obj.position.z,
            odom_T_hand_obj.rotation.w, odom_T_hand_obj.rotation.x, odom_T_hand_obj.rotation.y,
            odom_T_hand_obj.rotation.z, ODOM_FRAME_NAME, move_time)

        command = RobotCommandBuilder.build_synchro_command(arm_command)

        cmd_id = command_client.robot_command(command)

        # Wait for the move to complete
        block_until_arm_arrives(command_client, cmd_id)

        # Update state and Get the hand position
        robot_state = robot_state_client.get_robot_state()
        (world_T_body, body_T_hand, world_T_hand, odom_T_body) = get_transforms(
            use_vision_frame, robot_state)

        world_T_admittance_frame = geometry_pb2.SE3Pose(
            position=geometry_pb2.Vec3(x=0, y=0, z=0),
            rotation=geometry_pb2.Quaternion(w=1, x=0, y=0, z=0))
        if draw_on_wall:
            # Create an admittance frame that has Z- along the robot's X axis
            xhat_ewrt_robot = [0, 0, 1]
            xhat_ewrt_vo = [0, 0, 0]
            (xhat_ewrt_vo[0], xhat_ewrt_vo[1], xhat_ewrt_vo[2]) = world_T_body.rot.transform_point(
                xhat_ewrt_robot[0], xhat_ewrt_robot[1], xhat_ewrt_robot[2])
            (z1, z2, z3) = world_T_body.rot.transform_point(-1, 0, 0)
            zhat_temp = [z1, z2, z3]
            zhat = make_orthogonal(xhat_ewrt_vo, zhat_temp)
            yhat = np.cross(zhat, xhat_ewrt_vo)
            mat = np.array([xhat_ewrt_vo, yhat, zhat]).transpose()
            q_wall = Quat.from_matrix(mat)

            zero_vec3 = geometry_pb2.Vec3(x=0, y=0, z=0)
            q_wall_proto = geometry_pb2.Quaternion(w=q_wall.w, x=q_wall.x, y=q_wall.y, z=q_wall.z)

            world_T_admittance_frame = geometry_pb2.SE3Pose(position=zero_vec3,
                                                            rotation=q_wall_proto)

        # Touch the ground/wall
        move_arm(robot_state, True, [world_T_hand], arm_surface_contact_client, velocity,
                 allow_walking, world_T_admittance_frame, press_force_percent, api_send_frame,
                 use_xy_to_z_cross_term, bias_force_x)

        time.sleep(4.0)
        last_admittance = True

        # Update state
        robot_state = robot_state_client.get_robot_state()

        # Get the hand position
        (world_T_body, body_T_hand, world_T_hand, odom_T_body) = get_transforms(
            use_vision_frame, robot_state)

        odom_T_ground_plane = get_a_tform_b(robot_state.kinematic_state.transforms_snapshot, ""odom"",
                                            ""gpe"")
        world_T_odom = world_T_body * odom_T_body.inverse()

        (gx, gy, gz) = world_T_odom.transform_point(odom_T_ground_plane.x, odom_T_ground_plane.y,
                                                    odom_T_ground_plane.z)
        ground_plane_rt_vo = [gx, gy, gz]

        # Compute the robot's position on the ground plane.
        #ground_plane_T_robot = odom_T_ground_plane.inverse() *

        # Compute an origin.
        if not draw_on_wall:
            # For on the ground:
            #   xhat = body x
            #   zhat = (0,0,1)

            # Ensure the origin is gravity aligned, otherwise we get some height drift.
            zhat = [0.0, 0.0, 1.0]
            (x1, x2, x3) = world_T_body.rot.transform_point(1.0, 0.0, 0.0)
            xhat_temp = [x1, x2, x3]
            xhat = make_orthogonal(zhat, xhat_temp)
            yhat = np.cross(zhat, xhat)
            mat = np.array([xhat, yhat, zhat]).transpose()
            vo_Q_origin = Quat.from_matrix(mat)

            world_T_origin = SE3Pose(world_T_hand.x, world_T_hand.y, world_T_hand.z, vo_Q_origin)
        else:
            # todo should I use the same one?
            world_T_origin = world_T_hand

        gcode.set_origin(world_T_origin, world_T_admittance_frame)
        robot.logger.info('Origin set')

        (is_admittance, world_T_goals, is_pause) = gcode.get_next_world_T_goals(ground_plane_rt_vo)

        while is_pause:
            do_pause()
            (is_admittance, world_T_goals,
             is_pause) = gcode.get_next_world_T_goals(ground_plane_rt_vo)

        if world_T_goals is None:
            # we're done!
            done = True

        move_arm(robot_state, is_admittance, world_T_goals, arm_surface_contact_client, velocity,
                 allow_walking, world_T_admittance_frame, press_force_percent, api_send_frame,
                 use_xy_to_z_cross_term, bias_force_x)
        odom_T_hand_goal = world_T_odom.inverse() * world_T_goals[-1]
        last_admittance = is_admittance

        done = False
        while not done:

            # Update state
            robot_state = robot_state_client.get_robot_state()

            # Determine if we are at the goal point
            (world_T_body, body_T_hand, world_T_hand, odom_T_body) = get_transforms(
                use_vision_frame, robot_state)

            (gx, gy, gz) = world_T_odom.transform_point(odom_T_ground_plane.x,
                                                        odom_T_ground_plane.y,
                                                        odom_T_ground_plane.z)
            ground_plane_rt_vo = [gx, gy, gz]

            world_T_odom = world_T_body * odom_T_body.inverse()
            odom_T_hand = odom_T_body * body_T_hand

            admittance_frame_T_world = math_helpers.SE3Pose.from_obj(
                world_T_admittance_frame).inverse()
            admit_frame_T_hand = admittance_frame_T_world * world_T_odom * odom_T_body * body_T_hand
            admit_frame_T_hand_goal = admittance_frame_T_world * world_T_odom * odom_T_hand_goal

            if is_admittance:
                dist = math.sqrt((admit_frame_T_hand.x - admit_frame_T_hand_goal.x)**2 +
                                 (admit_frame_T_hand.y - admit_frame_T_hand_goal.y)**2)
                #+ (admit_frame_T_hand.z - admit_frame_T_hand_goal.z)**2 )
            else:
                dist = math.sqrt((admit_frame_T_hand.x - admit_frame_T_hand_goal.x)**2 +
                                 (admit_frame_T_hand.y - admit_frame_T_hand_goal.y)**2 +
                                 (admit_frame_T_hand.z - admit_frame_T_hand_goal.z)**2)

            arm_near_goal = dist < min_dist_to_goal

            if arm_near_goal:
                # Compute where to go.
                (is_admittance, world_T_goals,
                 is_pause) = gcode.get_next_world_T_goals(ground_plane_rt_vo)

                while is_pause:
                    do_pause()
                    (is_admittance, world_T_goals,
                     is_pause) = gcode.get_next_world_T_goals(ground_plane_rt_vo)

                if world_T_goals is None:
                    # we're done!
                    done = True
                    robot.logger.info(""Gcode program finished."")
                    break

                move_arm(robot_state, is_admittance, world_T_goals, arm_surface_contact_client,
                         velocity, allow_walking, world_T_admittance_frame, press_force_percent,
                         api_send_frame, use_xy_to_z_cross_term, bias_force_x)
                odom_T_hand_goal = world_T_odom.inverse() * world_T_goals[-1]

                if is_admittance != last_admittance:
                    if is_admittance:
                        print('Waiting for touchdown...')
                        time.sleep(3.0)  # pause to wait for touchdown
                    else:
                        time.sleep(1.0)
                last_admittance = is_admittance
            elif not is_admittance:
                # We are in a travel move, so we'll keep updating to account for a changing
                # ground plane.
                (is_admittance, world_T_goals, is_pause) = gcode.get_next_world_T_goals(
                    ground_plane_rt_vo, read_new_line=False)

        # At the end, walk back to the start.
        robot.logger.info('Done with gcode, going to stand...')
        blocking_stand(command_client, timeout_sec=10)
        robot.logger.info(""Robot standing"")

        # Compute walking location
        if walk_to_at_end_rt_gcode_origin_x is not None and walk_to_at_end_rt_gcode_origin_y is not None:
            robot.logger.info(""Walking to end position..."")
            gcode_origin_T_walk = SE3Pose(walk_to_at_end_rt_gcode_origin_x * scale,
                                          walk_to_at_end_rt_gcode_origin_y * scale, 0,
                                          Quat(1, 0, 0, 0))

            odom_T_walk = world_T_odom.inverse() * gcode.world_T_origin * gcode_origin_T_walk

            odom_T_walk_se2 = SE2Pose.flatten(odom_T_walk)

            # Command the robot to go to the end point.
            walk_cmd = RobotCommandBuilder.trajectory_command(goal_x=odom_T_walk_se2.x,
                                                              goal_y=odom_T_walk_se2.y,
                                                              goal_heading=odom_T_walk_se2.angle,
                                                              frame_name=""odom"")
            end_time = 15.0
            #Issue the command to the robot
            command_client.robot_command(command=walk_cmd, end_time_secs=time.time() + end_time)
            block_for_trajectory_cmd(command_client, 1,
                                     basic_command_pb2.SE2TrajectoryCommand.Feedback.STATUS_At_Goal,
                                     None, 0.1, 15.0)

        robot.logger.info('Done.')

        # Power the robot off. By specifying ""cut_immediately=False"", a safe power off command
        # is issued to the robot. This will attempt to sit the robot before powering off.
        robot.power_off(cut_immediately=False, timeout_sec=20)
        assert not robot.is_powered_on(), ""Robot power off failed.""
        robot.logger.info(""Robot safely powered off."")","world_T_body.rot.transform_point(xhat_ewrt_robot[0], xhat_ewrt_robot[1], xhat_ewrt_robot[2])",world_T_body.rot.transform_point(*xhat_ewrt_robot[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*xhat_ewrt_robot[:3],*xhat_ewrt_robot[:3],1
pyNES,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyNES/pynes/composer.py,https://github.com/gutomaia/pyNES/tree/master/pynes/composer.py,PyNesVisitor,visit_Call$240,"def visit_Call(self, node):
        global game
        if 'id' in dir(node.func):
            self.stack.store()
            if len(node.args) > 0:
                self.generic_visit(node.args)
                args = self.stack.current()
                self.stack.wipe()
            else:
                args = []

            # check this condition, seens strange
            if node.func.id not in game.bitpaks:
                obj = getattr(pynes.bitbag, node.func.id, None)
                if (obj):
                    try:
                        bp = obj(game)
                        game.bitpaks[node.func.id] = bp
                        self.stack(bp(*args))
                        game += bp.asm()
                    except TypeError as ex:
                        msg = ex.message.replace('__call__', node.func.id, 1)
                        raise(TypeError(msg))
                else:
                    raise(NameError(""name '%s' is not defined"" % node.func.id))
            else:
                bp = game.bitpaks[node.func.id]
                self.stack(bp(*args))
                game += bp.asm()
        else:
            self.generic_visit(node)
            attrib = getattr(
                self.stack.current()[0], self.stack.current()[1], None)
            self.stack.wipe()
            if callable(attrib):
                attrib()","getattr(self.stack.current()[0], self.stack.current()[1], None)","getattr(*self.stack.current()[:2], None)","iterable_zj[0], iterable_zj[1]",*self.stack.current()[:2],*self.stack.current()[:2],1
matplotlib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/matplotlib/examples/text_labels_and_annotations/angle_annotation.py,https://github.com/matplotlib/matplotlib/tree/master/examples/text_labels_and_annotations/angle_annotation.py,,plot_angle$263,"def plot_angle(ax, pos, angle, length=0.95, acol=""C0"", **kwargs):
    vec2 = np.array([np.cos(np.deg2rad(angle)), np.sin(np.deg2rad(angle))])
    xy = np.c_[[length, 0], [0, 0], vec2*length].T + np.array(pos)
    ax.plot(*xy.T, color=acol)
    return AngleAnnotation(pos, xy[0], xy[2], ax=ax, **kwargs)","AngleAnnotation(pos, xy[0], xy[2], ax=ax, **kwargs)","AngleAnnotation(pos, *xy[::2], ax=ax, **kwargs)","iterable_zj[0], iterable_zj[2]",*xy[::2],*xy[:4:2],0
External-Attention-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/External-Attention-pytorch/model/attention/BAM.py,https://github.com/xmu-xiaoma666/External-Attention-pytorch/tree/master/model/attention/BAM.py,ChannelAttention,__init__$11,"def __init__(self,channel,reduction=16,num_layers=3):
        super().__init__()
        self.avgpool=nn.AdaptiveAvgPool2d(1)
        gate_channels=[channel]
        gate_channels+=[channel//reduction]*num_layers
        gate_channels+=[channel]


        self.ca=nn.Sequential()
        self.ca.add_module('flatten',Flatten())
        for i in range(len(gate_channels)-2):
            self.ca.add_module('fc%d'%i,nn.Linear(gate_channels[i],gate_channels[i+1]))
            self.ca.add_module('bn%d'%i,nn.BatchNorm1d(gate_channels[i+1]))
            self.ca.add_module('relu%d'%i,nn.ReLU())
        self.ca.add_module('last_fc',nn.Linear(gate_channels[-2],gate_channels[-1]))","nn.Linear(gate_channels[-2], gate_channels[-1])",nn.Linear(*gate_channels[-2:]),"iterable_zj[-2], iterable_zj[-1]",*gate_channels[-2:],*gate_channels[-2:0],0
MONAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MONAI/tests/utils.py,https://github.com/Project-MONAI/MONAI/tree/master/tests/utils.py,NumpyImageTestCase2D,setUp$531,"def setUp(self):
        im, msk = create_test_image_2d(
            self.im_shape[0], self.im_shape[1], num_objs=4, rad_max=20, noise_max=0.0, num_seg_classes=self.num_classes
        )

        self.imt = im[None, None]
        self.seg1 = (msk[None, None] > 0).astype(np.float32)
        self.segn = msk[None, None]","create_test_image_2d(self.im_shape[0], self.im_shape[1], num_objs=4, rad_max=20, noise_max=0.0, num_seg_classes=self.num_classes)","create_test_image_2d(*self.im_shape[:2], num_objs=4, rad_max=20, noise_max=0.0, num_seg_classes=self.num_classes)","iterable_zj[0], iterable_zj[1]",*self.im_shape[:2],*self.im_shape[:2],1
pycls,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycls/pycls/sweep/plotting.py,https://github.com/facebookresearch/pycls/tree/master/pycls/sweep/plotting.py,,plot_values$113,"def plot_values(sweeps, names, metrics, filters):
    """"""Plots scatter plot of error versus metric for each metric and sweep.""""""
    m, n, c = len(metrics), len(sweeps), _COLOR_FILL
    fig, axes = fig_make(m, n, False, sharex=""row"", sharey=True)
    e_min = min(min(get_vals(sweep, ""error"")) for sweep in sweeps)
    e_max = max(max(get_vals(sweep, ""error"")) for sweep in sweeps)
    e_min, e_max = e_min - (e_max - e_min) / 5, e_max + (e_max - e_min) / 5
    for i, j in [(i, j) for i in range(m) for j in range(n)]:
        metric, sweep, ax, f = metrics[i], sweeps[j], axes[i][j], filters[j]
        errs = get_vals(sweep, ""error"")
        vals = get_vals(sweep, metric)
        v_min, v_med, v_max = f[metric]
        f = [float(str(""{:.3e}"".format(f))) for f in f[metric]]
        l_rng, l_med = ""[{}, {}]"".format(f[0], f[2]), ""best: {}"".format(f[1])
        ax.scatter(vals, errs, color=get_color(j), alpha=0.8)
        ax.plot([v_med, v_med], [e_min, e_max], c=""k"", label=l_med)
        ax.fill_between([v_min, v_max], e_min, e_max, alpha=0.1, color=c, label=l_rng)
        ax.legend(loc=""upper left"")
        ax.set_ylabel(""error"" if j == 0 else """")
        ax.set_xlabel(get_info(metric)[1])
    fig_legend(fig, n, names)
    return fig","'[{}, {}]'.format(f[0], f[2])","'[{}, {}]'.format(*f[::2])","iterable_zj[0], iterable_zj[2]",*f[::2],*f[:4:2],0
dask,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dask/dask/array/tests/test_slicing.py,https://github.com/dask/dask/tree/master/dask/array/tests/test_slicing.py,,test_slicing_consistent_names$559,"def test_slicing_consistent_names():
    x = np.arange(100).reshape((10, 10))
    a = da.from_array(x, chunks=(5, 5))
    assert same_keys(a[0], a[0])
    assert same_keys(a[:, [1, 2, 3]], a[:, [1, 2, 3]])
    assert same_keys(a[:, 5:2:-1], a[:, 5:2:-1])
    assert same_keys(a[0, ...], a[0, ...])
    assert same_keys(a[...], a[...])
    assert same_keys(a[[1, 3, 5]], a[[1, 3, 5]])
    assert same_keys(a[-11:11], a[:])
    assert same_keys(a[-11:-9], a[:1])
    assert same_keys(a[-1], a[9])
    assert same_keys(a[0::-1], a[0:-11:-1])","same_keys(a[-1], a[9])",Cannot refactor,"iterable_zj[-1], iterable_zj[9]",,*a[-1:19:10],0
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/io/fits/diff.py,https://github.com/astropy/astropy/tree/master/astropy/io/fits/diff.py,,report_diff_keyword_attr$1599,"def report_diff_keyword_attr(fileobj, attr, diffs, keyword, ind=0):
    """"""
    Write a diff between two header keyword values or comments to the specified
    file-like object.
    """"""

    if keyword in diffs:
        vals = diffs[keyword]
        for idx, val in enumerate(vals):
            if val is None:
                continue
            if idx == 0:
                dup = """"
            else:
                dup = f""[{idx + 1}]""
            fileobj.write(
                fixed_width_indent(
                    f"" Keyword {keyword:8}{dup} has different {attr}:\n"",
                    ind,
                )
            )
            report_diff_values(val[0], val[1], fileobj=fileobj, indent_width=ind + 1)","report_diff_values(val[0], val[1], fileobj=fileobj, indent_width=ind + 1)","report_diff_values(*val[:2], fileobj=fileobj, indent_width=ind + 1)","iterable_zj[0], iterable_zj[1]",*val[:2],*val[:2],1
WatchAD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WatchAD/libs/kek/krb5.py,https://github.com/Qianlitp/WatchAD/tree/master/libs/kek/krb5.py,,decrypt_ticket_enc_part$464,"def decrypt_ticket_enc_part(ticket, key):
    ticket_enc = str(ticket['enc-part']['cipher'])
    ticket_enc = decrypt(key[0], key[1], 2, ticket_enc)
    return decode(ticket_enc, asn1Spec=EncTicketPart())[0]","decrypt(key[0], key[1], 2, ticket_enc)","decrypt(*key[:2], 2, ticket_enc)","iterable_zj[0], iterable_zj[1]",*key[:2],*key[:2],1
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/scripts/datasets/ucf101.py,https://github.com/dmlc/gluon-cv/tree/master/scripts/datasets/ucf101.py,,build_split_list$242,"def build_split_list(split, frame_info, shuffle=False):

    def build_set_list(set_list):
        rgb_list, flow_list = list(), list()
        for item in set_list:
            if item[0] not in frame_info:
                # print(""item:"", item)
                continue
            elif frame_info[item[0]][1] > 0:
                rgb_cnt = frame_info[item[0]][1]
                flow_cnt = frame_info[item[0]][2]
                rgb_list.append('{} {} {}\n'.format(
                    item[0], rgb_cnt, item[1]))
                flow_list.append('{} {} {}\n'.format(
                    item[0], flow_cnt, item[1]))
            else:
                rgb_list.append('{} {}\n'.format(
                    item[0], item[1]))
                flow_list.append('{} {}\n'.format(
                    item[0], item[1]))
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return rgb_list, flow_list

    train_rgb_list, train_flow_list = build_set_list(split[0])
    test_rgb_list, test_flow_list = build_set_list(split[1])
    return (train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list)","'{} {}\n'.format(item[0], item[1])",'{} {}\n'.format(*item[:2]),"iterable_zj[0], iterable_zj[1]",*item[:2],*item[:2],1
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/scripts/datasets/ucf101.py,https://github.com/dmlc/gluon-cv/tree/master/scripts/datasets/ucf101.py,,build_split_list$242,"def build_split_list(split, frame_info, shuffle=False):

    def build_set_list(set_list):
        rgb_list, flow_list = list(), list()
        for item in set_list:
            if item[0] not in frame_info:
                # print(""item:"", item)
                continue
            elif frame_info[item[0]][1] > 0:
                rgb_cnt = frame_info[item[0]][1]
                flow_cnt = frame_info[item[0]][2]
                rgb_list.append('{} {} {}\n'.format(
                    item[0], rgb_cnt, item[1]))
                flow_list.append('{} {} {}\n'.format(
                    item[0], flow_cnt, item[1]))
            else:
                rgb_list.append('{} {}\n'.format(
                    item[0], item[1]))
                flow_list.append('{} {}\n'.format(
                    item[0], item[1]))
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return rgb_list, flow_list

    train_rgb_list, train_flow_list = build_set_list(split[0])
    test_rgb_list, test_flow_list = build_set_list(split[1])
    return (train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list)","'{} {}\n'.format(item[0], item[1])",'{} {}\n'.format(*item[:2]),"iterable_zj[0], iterable_zj[1]",*item[:2],*item[:2],1
Source-Code-from-Tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Source-Code-from-Tutorials/Pygame/83_PythonGameDevelopment.py,https://github.com/buckyroberts/Source-Code-from-Tutorials/tree/master/Pygame/83_PythonGameDevelopment.py,,fireShell$280,"def fireShell(xy,tankx,tanky,turPos,gun_power,xlocation,barrier_width,randomHeight,enemyTankX, enemyTankY):
    pygame.mixer.Sound.play(fire_sound)
    fire = True
    damage = 0

    startingShell = list(xy)

    print(""FIRE!"",xy)

    while fire:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()

        #print(startingShell[0],startingShell[1])
        pygame.draw.circle(gameDisplay, red, (startingShell[0],startingShell[1]),5)


        startingShell[0] -= (12 - turPos)*2

        # y = x**2
        startingShell[1] += int((((startingShell[0]-xy[0])*0.015/(gun_power/50))**2) - (turPos+turPos/(12-turPos)))

        if startingShell[1] > display_height-ground_height:
            print(""Last shell:"",startingShell[0], startingShell[1])
            hit_x = int((startingShell[0]*display_height-ground_height)/startingShell[1])
            hit_y = int(display_height-ground_height)
            print(""Impact:"", hit_x,hit_y)
            
            if enemyTankX + 10 > hit_x > enemyTankX - 10:
                print(""Critical Hit!"")
                damage = 25
            elif enemyTankX + 15 > hit_x > enemyTankX - 15:
                print(""Hard Hit!"")
                damage = 18
            elif enemyTankX + 25 > hit_x > enemyTankX - 25:
                print(""Medium Hit"")
                damage = 10
            elif enemyTankX + 35 > hit_x > enemyTankX - 35:
                print(""Light Hit"")
                damage = 5
            
            
            explosion(hit_x,hit_y)
            fire = False

        check_x_1 = startingShell[0] <= xlocation + barrier_width
        check_x_2 = startingShell[0] >= xlocation

        check_y_1 = startingShell[1] <= display_height
        check_y_2 = startingShell[1] >= display_height - randomHeight

        if check_x_1 and check_x_2 and check_y_1 and check_y_2:
            print(""Last shell:"",startingShell[0], startingShell[1])
            hit_x = int((startingShell[0]))
            hit_y = int(startingShell[1])
            print(""Impact:"", hit_x,hit_y)
            explosion(hit_x,hit_y)
            fire = False
            

        pygame.display.update()
        clock.tick(60)
    return damage","print('Last shell:', startingShell[0], startingShell[1])","print('Last shell:', *startingShell[:2])","iterable_zj[0], iterable_zj[1]",*startingShell[:2],*startingShell[:2],1
Source-Code-from-Tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Source-Code-from-Tutorials/Pygame/83_PythonGameDevelopment.py,https://github.com/buckyroberts/Source-Code-from-Tutorials/tree/master/Pygame/83_PythonGameDevelopment.py,,fireShell$280,"def fireShell(xy,tankx,tanky,turPos,gun_power,xlocation,barrier_width,randomHeight,enemyTankX, enemyTankY):
    pygame.mixer.Sound.play(fire_sound)
    fire = True
    damage = 0

    startingShell = list(xy)

    print(""FIRE!"",xy)

    while fire:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()

        #print(startingShell[0],startingShell[1])
        pygame.draw.circle(gameDisplay, red, (startingShell[0],startingShell[1]),5)


        startingShell[0] -= (12 - turPos)*2

        # y = x**2
        startingShell[1] += int((((startingShell[0]-xy[0])*0.015/(gun_power/50))**2) - (turPos+turPos/(12-turPos)))

        if startingShell[1] > display_height-ground_height:
            print(""Last shell:"",startingShell[0], startingShell[1])
            hit_x = int((startingShell[0]*display_height-ground_height)/startingShell[1])
            hit_y = int(display_height-ground_height)
            print(""Impact:"", hit_x,hit_y)
            
            if enemyTankX + 10 > hit_x > enemyTankX - 10:
                print(""Critical Hit!"")
                damage = 25
            elif enemyTankX + 15 > hit_x > enemyTankX - 15:
                print(""Hard Hit!"")
                damage = 18
            elif enemyTankX + 25 > hit_x > enemyTankX - 25:
                print(""Medium Hit"")
                damage = 10
            elif enemyTankX + 35 > hit_x > enemyTankX - 35:
                print(""Light Hit"")
                damage = 5
            
            
            explosion(hit_x,hit_y)
            fire = False

        check_x_1 = startingShell[0] <= xlocation + barrier_width
        check_x_2 = startingShell[0] >= xlocation

        check_y_1 = startingShell[1] <= display_height
        check_y_2 = startingShell[1] >= display_height - randomHeight

        if check_x_1 and check_x_2 and check_y_1 and check_y_2:
            print(""Last shell:"",startingShell[0], startingShell[1])
            hit_x = int((startingShell[0]))
            hit_y = int(startingShell[1])
            print(""Impact:"", hit_x,hit_y)
            explosion(hit_x,hit_y)
            fire = False
            

        pygame.display.update()
        clock.tick(60)
    return damage","print('Last shell:', startingShell[0], startingShell[1])","print('Last shell:', *startingShell[:2])","iterable_zj[0], iterable_zj[1]",*startingShell[:2],*startingShell[:2],1
electrum,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electrum/electrum/gui/qt/qrreader/qtmultimedia/video_overlay.py,https://github.com/spesmilo/electrum/tree/master/electrum/gui/qt/qrreader/qtmultimedia/video_overlay.py,QrReaderVideoOverlay,paintEvent$81,"def paintEvent(self, _event: QPaintEvent):
        if not self.crop or not self.resolution:
            return

        painter = QPainter(self)

        # Keep a backup of the transform and create a new one
        transform = painter.worldTransform()

        # Set scaling transform
        transform = transform.scale(self.width() / self.resolution.width(),
                                    self.height() / self.resolution.height())

        # Compute the transform to flip the coordinate system on the x axis
        transform_flip = QTransform()
        if self.flip_x:
            transform_flip = transform_flip.translate(self.resolution.width(), 0.0)
            transform_flip = transform_flip.scale(-1.0, 1.0)

        # Small helper for tuple to QPoint
        def toqp(point):
            return QPoint(point[0], point[1])

        # Starting from here we care about AA
        painter.setRenderHint(QPainter.Antialiasing)

        # Draw all the QR code results
        for res in self.results:
            painter.setWorldTransform(transform_flip * transform, False)

            # Draw lines between all of the QR code points
            pen = QPen(self.qr_outline_pen)
            if res in self.validator_results.result_colors:
                pen.setColor(self.validator_results.result_colors[res])
            painter.setPen(pen)
            num_points = len(res.points)
            for i in range(0, num_points):
                i_n = i + 1

                line_from = toqp(res.points[i])
                line_from += self.crop.topLeft()

                line_to = toqp(res.points[i_n] if i_n < num_points else res.points[0])
                line_to += self.crop.topLeft()

                painter.drawLine(line_from, line_to)

            # Draw the QR code data
            # Note that we reset the world transform to only the scaled transform
            # because otherwise the text could be flipped. We only use transform_flip
            # to map the center point of the result.
            painter.setWorldTransform(transform, False)
            font_metrics = painter.fontMetrics()
            data_metrics = QSize(font_metrics.horizontalAdvance(res.data), font_metrics.capHeight())

            center_pos = toqp(res.center)
            center_pos += self.crop.topLeft()
            center_pos = transform_flip.map(center_pos)

            text_offset = QPoint(data_metrics.width(), data_metrics.height())
            text_offset = text_offset / 2
            text_offset.setX(-text_offset.x())
            center_pos += text_offset

            padding = self.BG_RECT_PADDING
            bg_rect_pos = center_pos - QPoint(padding, data_metrics.height() + padding)
            bg_rect_size = data_metrics + (QSize(padding, padding) * 2)
            bg_rect = QRect(bg_rect_pos, bg_rect_size)
            bg_rect_path = QPainterPath()
            radius = self.BG_RECT_CORNER_RADIUS
            bg_rect_path.addRoundedRect(QRectF(bg_rect), radius, radius, Qt.AbsoluteSize)
            painter.setPen(self.bg_rect_pen)
            painter.fillPath(bg_rect_path, self.bg_rect_fill)
            painter.drawPath(bg_rect_path)

            painter.setPen(self.text_pen)
            painter.drawText(center_pos, res.data)","QPoint(point[0], point[1])",QPoint(*point[:2]),"iterable_zj[0], iterable_zj[1]",*point[:2],*point[:2],1
KL-Loss,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/KL-Loss/detectron/core/test.py,https://github.com/yihui-he/KL-Loss/tree/master/detectron/core/test.py,,im_detect_all$55,"def im_detect_all(model, im, box_proposals, timers=None):
    if timers is None:
        timers = defaultdict(Timer)

    # Handle RetinaNet testing separately for now
    if cfg.RETINANET.RETINANET_ON:
        cls_boxes = test_retinanet.im_detect_bbox(model, im, timers)
        return cls_boxes, None, None

    timers['im_detect_bbox'].tic()
    if cfg.TEST.BBOX_AUG.ENABLED:
        scores, boxes, im_scale = im_detect_bbox_aug(model, im, box_proposals)
    else:
        if cfg.STD_NMS:
            scores, boxes, im_scale, confs = im_detect_bbox(
                model, im, cfg.TEST.SCALE, cfg.TEST.MAX_SIZE, boxes=box_proposals
            )
        else:
            scores, boxes, im_scale = im_detect_bbox(
                model, im, cfg.TEST.SCALE, cfg.TEST.MAX_SIZE, boxes=box_proposals
            )
    timers['im_detect_bbox'].toc()

    # score and boxes are from the whole image after score thresholding and nms
    # (they are not separated by class)
    # cls_boxes boxes and scores are separated by class and in the format used
    # for evaluating results
    timers['misc_bbox'].tic()
    if cfg.STD_NMS:
        scores, boxes, cls_boxes = box_results_with_nms_and_limit(scores, boxes, confs=confs)
    else:
        scores, boxes, cls_boxes = box_results_with_nms_and_limit(scores, boxes)
    timers['misc_bbox'].toc()

    if cfg.MODEL.MASK_ON and boxes.shape[0] > 0:
        timers['im_detect_mask'].tic()
        if cfg.TEST.MASK_AUG.ENABLED:
            masks = im_detect_mask_aug(model, im, boxes)
        else:
            masks = im_detect_mask(model, im_scale, boxes)
        timers['im_detect_mask'].toc()

        timers['misc_mask'].tic()
        cls_segms = segm_results(
            cls_boxes, masks, boxes, im.shape[0], im.shape[1]
        )
        timers['misc_mask'].toc()
    else:
        cls_segms = None

    if cfg.MODEL.KEYPOINTS_ON and boxes.shape[0] > 0:
        timers['im_detect_keypoints'].tic()
        if cfg.TEST.KPS_AUG.ENABLED:
            heatmaps = im_detect_keypoints_aug(model, im, boxes)
        else:
            heatmaps = im_detect_keypoints(model, im_scale, boxes)
        timers['im_detect_keypoints'].toc()

        timers['misc_keypoints'].tic()
        cls_keyps = keypoint_results(cls_boxes, heatmaps, boxes)
        timers['misc_keypoints'].toc()
    else:
        cls_keyps = None

    return cls_boxes, cls_segms, cls_keyps","segm_results(cls_boxes, masks, boxes, im.shape[0], im.shape[1])","segm_results(cls_boxes, masks, boxes, *im.shape[:2])","iterable_zj[0], iterable_zj[1]",*im.shape[:2],*im.shape[:2],1
pyclustering,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyclustering/pyclustering/cluster/ema.py,https://github.com/annoviko/pyclustering/tree/master/pyclustering/cluster/ema.py,ema_visualizer,__draw_ellipses$419,"def __draw_ellipses(figure, visualizer, clusters, covariances, means):
        ax = figure.get_axes()[0]
        
        for index in range(len(clusters)):
            angle, width, height = calculate_ellipse_description(covariances[index])
            color = visualizer.get_cluster_color(index, 0)

            ema_visualizer.__draw_ellipse(ax, means[index][0], means[index][1], angle, width, height, color)","ema_visualizer.__draw_ellipse(ax, means[index][0], means[index][1], angle, width, height, color)","ema_visualizer.__draw_ellipse(ax, *means[index][:2], angle, width, height, color)","iterable_zj[0], iterable_zj[1]",*means[index][:2],*means[index][:2],1
SickChill,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SickChill/sickchill/views/home.py,https://github.com/SickChill/SickChill/tree/master/sickchill/views/home.py,Home,doRename$1589,"def doRename(self, show=None, eps=None):
        if not (show and eps):
            return self._genericMessage(_(""Error""), _(""You must specify a show and at least one episode""))

        show_obj = Show.find(settings.showList, int(show))
        if not show_obj:
            return self._genericMessage(_(""Error""), _(""Show not in show list""))

        try:
            show_obj.location
        except ShowDirectoryNotFoundException:
            return self._genericMessage(_(""Error""), _(""Can't rename episodes when the show dir is missing.""))

        if not eps:
            return self.redirect(""/home/displayShow?show="" + show)

        main_db_con = db.DBConnection()
        for cur_ep in eps.split(""|""):

            epInfo = cur_ep.split(""x"")

            # this is probably the worst possible way to deal with double eps but I've kinda painted myself into a corner here with this stupid database
            ep_result = main_db_con.select(
                ""SELECT location FROM tv_episodes WHERE showid = ? AND season = ? AND episode = ? AND 5=5"", [show, epInfo[0], epInfo[1]]
            )
            if not ep_result:
                logger.warning(""Unable to find an episode for "" + cur_ep + "", skipping"")
                continue
            related_eps_result = main_db_con.select(
                ""SELECT season, episode FROM tv_episodes WHERE location = ? AND episode != ?"", [ep_result[0][""location""], epInfo[1]]
            )

            root_ep_obj = show_obj.getEpisode(epInfo[0], epInfo[1])
            root_ep_obj.relatedEps = []

            for cur_related_ep in related_eps_result:
                related_ep_obj = show_obj.getEpisode(cur_related_ep[""season""], cur_related_ep[""episode""])
                if related_ep_obj not in root_ep_obj.relatedEps:
                    root_ep_obj.relatedEps.append(related_ep_obj)

            root_ep_obj.rename()

        return self.redirect(""/home/displayShow?show="" + show)","show_obj.getEpisode(epInfo[0], epInfo[1])",show_obj.getEpisode(*epInfo[:2]),"iterable_zj[0], iterable_zj[1]",*epInfo[:2],*epInfo[:2],1
friture,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/friture/sandbox/log2scale.py,https://github.com/tlecomte/friture/tree/master/sandbox/log2scale.py,CustomScaleEngine,divideScale$62,"def divideScale(self, x1, x2, maxMajSteps, maxMinSteps, stepSize):
		interval = Qwt.QwtDoubleInterval(x1, x2).normalized()
		interval = interval.limited(LOG2_MIN, LOG2_MAX)
		
		if interval.width() <= 0:
			return Qwt.QwtScaleDiv()
		
		base = 2.
		if interval.maxValue() / interval.minValue() < base:
			# scale width is less than one octave -> build linear scale
			
			linearScaler = Qwt.QwtLinearScaleEngine()
			linearScaler.setAttributes(self.attributes())
			linearScaler.setReference(self.reference())
			linearScaler.setMargins(self.lowerMargin(), self.upperMargin())

			return linearScaler.divideScale(x1, x2, maxMajSteps, maxMinSteps, stepSize)

		stepSize = abs(stepSize)
		
		if stepSize == 0.:
			if maxMajSteps < 1:
				maxMajSteps = 1
			
			stepSize = self.divideInterval(self.log2(interval).width(), maxMajSteps)
			
			if stepSize < 1.:
				stepSize = 1. # major step must be >= 1 decade
		
		scaleDiv = Qwt.QwtScaleDiv()
		
		if stepSize != 0.:
			ticks = self.buildTicks(interval, stepSize, maxMinSteps)
			scaleDiv = Qwt.QwtScaleDiv(interval, ticks[0], ticks[1], ticks[2])
		
		if x1 > x2:
			 scaleDiv.invert()
		
		return scaleDiv","Qwt.QwtScaleDiv(interval, ticks[0], ticks[1], ticks[2])","Qwt.QwtScaleDiv(interval, *ticks[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*ticks[:3],*ticks[:3],1
asciimatics,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/asciimatics/tests/test_renderers.py,https://github.com/peterbrittain/asciimatics/tree/master/tests/test_renderers.py,TestRenderers,internal_checks$212,"def internal_checks(screen):
            # Check the original FG only rendering
            renderer = ColourImageFile(
                screen,
                os.path.join(os.path.dirname(__file__), ""globe.gif""),
                height=10)

            # Check renderer got all images from the file.
            count = 0
            for image in renderer.images:
                count += 1
                self.assertIsNotNone(image)
                self.assertIsNotNone(len(image) <= renderer.max_height)
            self.assertEqual(count, 11)

            # Check an image looks plausible
            image = next(renderer.images)
            self.maxDiff = None
            self.assertEqual(
                image,
                ['',
                 '     ##########     ',
                 '  ###############   ',
                 ' ################## ',
                 '####################',
                 '####################',
                 '####################',
                 '################### ',
                 ' #################  ',
                 '   ##############   ',
                 '      #######       '])

            # Also check the BG rendering
            renderer2 = ColourImageFile(
                screen,
                os.path.join(os.path.dirname(__file__), ""globe.gif""),
                fill_background=True,
                height=10)

            # Check BG rendering doesn't change the visible text output.
            # Note that BG rendering needs to print dots for some terminals.
            image2 = [x.replace(""."", "" "") for x in next(renderer2.images)]
            self.assertEqual(image, image2)

            # Check BG rendering gives same colours for FG and BG as original
            # rendering
            for a, b in zip(renderer.rendered_text[1],
                            renderer2.rendered_text[1]):
                for attr1, attr2 in zip(a, b):
                    if attr1[0] is None:
                        self.assertEqual(0, attr2[0])
                        self.assertEqual(0, attr2[2])
                    else:
                        self.assertEqual(attr1[0], attr2[0])
                        self.assertEqual(attr2[0], attr2[2])","self.assertEqual(attr2[0], attr2[2])",self.assertEqual(*attr2[::2]),"iterable_zj[0], iterable_zj[2]",*attr2[::2],*attr2[:4:2],0
PixelLib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PixelLib/pixellib/torchbackend/instance/modeling/roi_heads/cascade_rcnn.py,https://github.com/ayoolaolafenwa/PixelLib/tree/master/pixellib/torchbackend/instance/modeling/roi_heads/cascade_rcnn.py,CascadeROIHeads,_forward_box$156,"def _forward_box(self, features, proposals, targets=None):
        """"""
        Args:
            features, targets: the same as in
                Same as in :meth:`ROIHeads.forward`.
            proposals (list[Instances]): the per-image object proposals with
                their matching ground truth.
                Each has fields ""proposal_boxes"", and ""objectness_logits"",
                ""gt_classes"", ""gt_boxes"".
        """"""
        features = [features[f] for f in self.box_in_features]
        head_outputs = []  # (predictor, predictions, proposals)
        prev_pred_boxes = None
        image_sizes = [x.image_size for x in proposals]
        for k in range(self.num_cascade_stages):
            if k > 0:
                # The output boxes of the previous stage are used to create the input
                # proposals of the next stage.
                proposals = self._create_proposals_from_boxes(prev_pred_boxes, image_sizes)
                if self.training:
                    proposals = self._match_and_label_boxes(proposals, k, targets)
            predictions = self._run_stage(features, proposals, k)
            prev_pred_boxes = self.box_predictor[k].predict_boxes(predictions, proposals)
            head_outputs.append((self.box_predictor[k], predictions, proposals))

        if self.training:
            losses = {}
            storage = get_event_storage()
            for stage, (predictor, predictions, proposals) in enumerate(head_outputs):
                with storage.name_scope(""stage{}"".format(stage)):
                    stage_losses = predictor.losses(predictions, proposals)
                losses.update({k + ""_stage{}"".format(stage): v for k, v in stage_losses.items()})
            return losses
        else:
            # Each is a list[Tensor] of length #image. Each tensor is Ri x (K+1)
            scores_per_stage = [h[0].predict_probs(h[1], h[2]) for h in head_outputs]

            # Average the scores across heads
            scores = [
                sum(list(scores_per_image)) * (1.0 / self.num_cascade_stages)
                for scores_per_image in zip(*scores_per_stage)
            ]
            # Use the boxes of the last head
            predictor, predictions, proposals = head_outputs[-1]
            boxes = predictor.predict_boxes(predictions, proposals)
            pred_instances, _ = fast_rcnn_inference(
                boxes,
                scores,
                image_sizes,
                predictor.test_score_thresh,
                predictor.test_nms_thresh,
                predictor.test_topk_per_image,
            )
            return pred_instances","h[0].predict_probs(h[1], h[2])",h[0].predict_probs(*h[1:3]),"iterable_zj[1], iterable_zj[2]",*h[1:3],*h[1:3],1
gaphor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gaphor/gaphor/diagram/presentation.py,https://github.com/gaphor/gaphor/tree/master/gaphor/diagram/presentation.py,LinePresentation,draw$253,"def draw(self, context):
        def draw_line_end(end_handle, second_handle, draw):
            pos, p1 = end_handle.pos, second_handle.pos
            angle = atan2(p1.y - pos.y, p1.x - pos.x)
            cr.save()
            try:
                cr.translate(*pos)
                cr.rotate(angle)
                draw(context)
            finally:
                cr.restore()

        style = merge_styles(context.style, self.style)
        context = replace(context, style=style)

        self.update_shape_bounds(context)
        cr = context.cairo

        handles = self._handles
        draw_line_end(handles[0], handles[1], self.draw_head)

        for h in self._handles[1:-1]:
            cr.line_to(*h.pos)

        draw_line_end(handles[-1], handles[-2], self.draw_tail)

        stroke(context)

        for shape, rect in (
            (self.shape_head, self._shape_head_rect),
            (self.shape_middle, self._shape_middle_rect),
            (self.shape_tail, self._shape_tail_rect),
        ):
            if shape:
                shape.draw(context, rect)","draw_line_end(handles[0], handles[1], self.draw_head)","draw_line_end(*handles[:2], self.draw_head)","iterable_zj[0], iterable_zj[1]",*handles[:2],*handles[:2],1
lbry-sdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lbry-sdk/lbry/dht/protocol/protocol.py,https://github.com/lbryio/lbry-sdk/tree/master/lbry/dht/protocol/protocol.py,KademliaProtocol,handle_response_datagram$488,"def handle_response_datagram(self, address: typing.Tuple[str, int], response_datagram: ResponseDatagram):
        # Find the message that triggered this response
        if response_datagram.rpc_id in self.sent_messages:
            peer, future, _ = self.sent_messages[response_datagram.rpc_id]
            if peer.address != address[0]:
                future.set_exception(
                    RemoteException(f""response from {address[0]}, expected {peer.address}"")
                )
                return

            # We got a result from the RPC
            if peer.node_id == self.node_id:
                future.set_exception(RemoteException(""node has our node id""))
                return
            elif response_datagram.node_id == self.node_id:
                future.set_exception(RemoteException(""incoming message is from our node id""))
                return
            peer = make_kademlia_peer(response_datagram.node_id, address[0], address[1])
            self.peer_manager.report_last_replied(address[0], address[1])
            self.peer_manager.update_contact_triple(peer.node_id, address[0], address[1])
            if not future.cancelled():
                future.set_result(response_datagram)
                self.add_peer(peer)
            else:
                log.warning(""%s:%i replied, but after we cancelled the request attempt"",
                            peer.address, peer.udp_port)
        else:
            # If the original message isn't found, it must have timed out
            # TODO: we should probably do something with this...
            pass","make_kademlia_peer(response_datagram.node_id, address[0], address[1])","make_kademlia_peer(response_datagram.node_id, *address[:2])","iterable_zj[0], iterable_zj[1]",*address[:2],*address[:2],1
lbry-sdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lbry-sdk/lbry/dht/protocol/protocol.py,https://github.com/lbryio/lbry-sdk/tree/master/lbry/dht/protocol/protocol.py,KademliaProtocol,handle_response_datagram$488,"def handle_response_datagram(self, address: typing.Tuple[str, int], response_datagram: ResponseDatagram):
        # Find the message that triggered this response
        if response_datagram.rpc_id in self.sent_messages:
            peer, future, _ = self.sent_messages[response_datagram.rpc_id]
            if peer.address != address[0]:
                future.set_exception(
                    RemoteException(f""response from {address[0]}, expected {peer.address}"")
                )
                return

            # We got a result from the RPC
            if peer.node_id == self.node_id:
                future.set_exception(RemoteException(""node has our node id""))
                return
            elif response_datagram.node_id == self.node_id:
                future.set_exception(RemoteException(""incoming message is from our node id""))
                return
            peer = make_kademlia_peer(response_datagram.node_id, address[0], address[1])
            self.peer_manager.report_last_replied(address[0], address[1])
            self.peer_manager.update_contact_triple(peer.node_id, address[0], address[1])
            if not future.cancelled():
                future.set_result(response_datagram)
                self.add_peer(peer)
            else:
                log.warning(""%s:%i replied, but after we cancelled the request attempt"",
                            peer.address, peer.udp_port)
        else:
            # If the original message isn't found, it must have timed out
            # TODO: we should probably do something with this...
            pass","self.peer_manager.report_last_replied(address[0], address[1])",self.peer_manager.report_last_replied(*address[:2]),"iterable_zj[0], iterable_zj[1]",*address[:2],*address[:2],1
lbry-sdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lbry-sdk/lbry/dht/protocol/protocol.py,https://github.com/lbryio/lbry-sdk/tree/master/lbry/dht/protocol/protocol.py,KademliaProtocol,handle_response_datagram$488,"def handle_response_datagram(self, address: typing.Tuple[str, int], response_datagram: ResponseDatagram):
        # Find the message that triggered this response
        if response_datagram.rpc_id in self.sent_messages:
            peer, future, _ = self.sent_messages[response_datagram.rpc_id]
            if peer.address != address[0]:
                future.set_exception(
                    RemoteException(f""response from {address[0]}, expected {peer.address}"")
                )
                return

            # We got a result from the RPC
            if peer.node_id == self.node_id:
                future.set_exception(RemoteException(""node has our node id""))
                return
            elif response_datagram.node_id == self.node_id:
                future.set_exception(RemoteException(""incoming message is from our node id""))
                return
            peer = make_kademlia_peer(response_datagram.node_id, address[0], address[1])
            self.peer_manager.report_last_replied(address[0], address[1])
            self.peer_manager.update_contact_triple(peer.node_id, address[0], address[1])
            if not future.cancelled():
                future.set_result(response_datagram)
                self.add_peer(peer)
            else:
                log.warning(""%s:%i replied, but after we cancelled the request attempt"",
                            peer.address, peer.udp_port)
        else:
            # If the original message isn't found, it must have timed out
            # TODO: we should probably do something with this...
            pass","self.peer_manager.update_contact_triple(peer.node_id, address[0], address[1])","self.peer_manager.update_contact_triple(peer.node_id, *address[:2])","iterable_zj[0], iterable_zj[1]",*address[:2],*address[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/topi/cuda/sort.py,https://github.com/apache/tvm/tree/master/python/tvm/topi/cuda/sort.py,,sort_by_key$1074,"def sort_by_key(keys, values, axis=-1, is_ascend=1):
    """"""Sort values with respect to keys. Both keys and values will
     be sorted and returned.

    Parameters
    ----------
    keys: tvm.te.Tensor
        The input keys.

    values : tvm.te.Tensor,
        The input values.

    axis : int, optional
        Axis long which to sort the input tensor.

    is_ascend : boolean, optional
        Whether to sort in ascending or descending order.

    Returns
    -------
    keys_sorted : tvm.te.Tensor
        The sorted keys

    values_sorted : tvm.te.Tensor
        The values sorted with respect to the keys
    """"""
    keys_buf = tvm.tir.decl_buffer(keys.shape, keys.dtype, ""keys_buf"", data_alignment=8)
    values_buf = tvm.tir.decl_buffer(values.shape, values.dtype, ""values_buf"", data_alignment=8)

    out_bufs = [
        tvm.tir.decl_buffer(keys.shape, keys.dtype, ""keys_buf"", data_alignment=8),
        tvm.tir.decl_buffer(values.shape, values.dtype, ""values_buf"", data_alignment=8),
        tvm.tir.decl_buffer(keys.shape, keys.dtype, ""keys_swap_buf"", data_alignment=8),
        tvm.tir.decl_buffer(values.shape, values.dtype, ""values_swap_buf"", data_alignment=8),
    ]
    out = te.extern(
        [keys.shape, values.shape, keys.shape, values.shape],
        [keys, values],
        lambda ins, outs: sort_by_key_ir(
            ins[0], ins[1], outs[0], outs[1], outs[2], outs[3], axis, is_ascend
        ),
        in_buffers=[keys_buf, values_buf],
        out_buffers=out_bufs,
        dtype=[keys.dtype, values.dtype],
        name=""sort_by_key"",
        tag=""sort_by_key"",
    )
    return out[0], out[1]","sort_by_key_ir(ins[0], ins[1], outs[0], outs[1], outs[2], outs[3], axis, is_ascend)","sort_by_key_ir(*ins[:2], outs[0], outs[1], outs[2], outs[3], axis, is_ascend)","iterable_zj[0], iterable_zj[1]",*ins[:2],*ins[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/topi/cuda/sort.py,https://github.com/apache/tvm/tree/master/python/tvm/topi/cuda/sort.py,,sort_by_key$1074,"def sort_by_key(keys, values, axis=-1, is_ascend=1):
    """"""Sort values with respect to keys. Both keys and values will
     be sorted and returned.

    Parameters
    ----------
    keys: tvm.te.Tensor
        The input keys.

    values : tvm.te.Tensor,
        The input values.

    axis : int, optional
        Axis long which to sort the input tensor.

    is_ascend : boolean, optional
        Whether to sort in ascending or descending order.

    Returns
    -------
    keys_sorted : tvm.te.Tensor
        The sorted keys

    values_sorted : tvm.te.Tensor
        The values sorted with respect to the keys
    """"""
    keys_buf = tvm.tir.decl_buffer(keys.shape, keys.dtype, ""keys_buf"", data_alignment=8)
    values_buf = tvm.tir.decl_buffer(values.shape, values.dtype, ""values_buf"", data_alignment=8)

    out_bufs = [
        tvm.tir.decl_buffer(keys.shape, keys.dtype, ""keys_buf"", data_alignment=8),
        tvm.tir.decl_buffer(values.shape, values.dtype, ""values_buf"", data_alignment=8),
        tvm.tir.decl_buffer(keys.shape, keys.dtype, ""keys_swap_buf"", data_alignment=8),
        tvm.tir.decl_buffer(values.shape, values.dtype, ""values_swap_buf"", data_alignment=8),
    ]
    out = te.extern(
        [keys.shape, values.shape, keys.shape, values.shape],
        [keys, values],
        lambda ins, outs: sort_by_key_ir(
            ins[0], ins[1], outs[0], outs[1], outs[2], outs[3], axis, is_ascend
        ),
        in_buffers=[keys_buf, values_buf],
        out_buffers=out_bufs,
        dtype=[keys.dtype, values.dtype],
        name=""sort_by_key"",
        tag=""sort_by_key"",
    )
    return out[0], out[1]","sort_by_key_ir(ins[0], ins[1], outs[0], outs[1], outs[2], outs[3], axis, is_ascend)","sort_by_key_ir(ins[0], ins[1], *outs[:4], axis, is_ascend)","iterable_zj[0], iterable_zj[1], iterable_zj[2], iterable_zj[3]",*outs[:4],*outs[:4],1
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/align_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/align_uv.py,MUV_OT_AlignUV_Axis,__align_to_y_axis_w_transmission$938,"def __align_to_y_axis_w_transmission(self, loop_seqs, uv_layer,
                                         uv_min, width, height):
        # reverse if the UV coordinate is not sorted by position
        need_revese = loop_seqs[0][0][0][uv_layer].uv.y > \
            loop_seqs[-1][0][-1][uv_layer].uv.y
        if need_revese:
            loop_seqs.reverse()
            for hidx, hseq in enumerate(loop_seqs):
                for vidx in range(len(hseq)):
                    tmp = loop_seqs[hidx][vidx][0]
                    loop_seqs[hidx][vidx][0] = loop_seqs[hidx][vidx][1]
                    loop_seqs[hidx][vidx][1] = tmp

        # get offset UVs when the UVs are aligned to Y-axis
        align_diff_uvs = self.__get_y_axis_align_diff_uvs(loop_seqs,
                                                          uv_layer, uv_min,
                                                          width, height)
        base_uv = loop_seqs[0][0][0][uv_layer].uv.copy()
        offset_uvs = []
        for hseq, aduv in zip(loop_seqs, align_diff_uvs):
            luv0 = hseq[0][0][uv_layer]
            luv1 = hseq[0][1][uv_layer]
            offset_uvs.append([luv0.uv + aduv[0] - base_uv,
                               luv1.uv + aduv[1] - base_uv])

        # get UV differential
        diff_uvs = []
        # hseq[vertical][loop]
        for hidx, hseq in enumerate(loop_seqs):
            # pair[loop]
            diffs = []
            for vidx in range(0, len(hseq), 2):
                if self.horizontal:
                    hdiff_uvs = [
                        _get_hdiff_uv_vinfl(uv_layer, loop_seqs, vidx, hidx, 0,
                                            self.mesh_infl),
                        _get_hdiff_uv_vinfl(uv_layer, loop_seqs, vidx, hidx, 1,
                                            self.mesh_infl),
                        _get_hdiff_uv_vinfl(uv_layer, loop_seqs, vidx + 1,
                                            hidx, 0, self.mesh_infl),
                        _get_hdiff_uv_vinfl(uv_layer, loop_seqs, vidx + 1,
                                            hidx, 1, self.mesh_infl),
                    ]
                    hdiff_uvs[0].x = hdiff_uvs[0].x + offset_uvs[hidx][0].x
                    hdiff_uvs[1].x = hdiff_uvs[1].x + offset_uvs[hidx][1].x
                    hdiff_uvs[2].x = hdiff_uvs[2].x + offset_uvs[hidx][0].x
                    hdiff_uvs[3].x = hdiff_uvs[3].x + offset_uvs[hidx][1].x
                else:
                    hdiff_uvs = [
                        offset_uvs[hidx][0],
                        offset_uvs[hidx][1],
                        offset_uvs[hidx][0],
                        offset_uvs[hidx][1],
                    ]
                if self.vertical:
                    vdiff_uvs = [
                        _get_vdiff_uv_vinfl(uv_layer, loop_seqs, vidx, hidx, 0,
                                            self.mesh_infl),
                        _get_vdiff_uv_vinfl(uv_layer, loop_seqs, vidx, hidx, 1,
                                            self.mesh_infl),
                        _get_vdiff_uv_vinfl(uv_layer, loop_seqs, vidx + 1,
                                            hidx, 0, self.mesh_infl),
                        _get_vdiff_uv_vinfl(uv_layer, loop_seqs, vidx + 1,
                                            hidx, 1, self.mesh_infl),
                    ]
                else:
                    vdiff_uvs = [
                        _get_vdiff_uv(uv_layer, loop_seqs, vidx, hidx),
                        _get_vdiff_uv(uv_layer, loop_seqs, vidx, hidx),
                        _get_vdiff_uv(uv_layer, loop_seqs, vidx + 1, hidx),
                        _get_vdiff_uv(uv_layer, loop_seqs, vidx + 1, hidx)
                    ]
                diffs.append([hdiff_uvs, vdiff_uvs])
            diff_uvs.append(diffs)

        # update UV
        for hseq, diffs in zip(loop_seqs, diff_uvs):
            for vidx in range(0, len(hseq), 2):
                loops = [
                    hseq[vidx][0], hseq[vidx][1],
                    hseq[vidx + 1][0], hseq[vidx + 1][1]
                ]
                for l, hdiff, vdiff in zip(loops, diffs[int(vidx / 2)][0],
                                           diffs[int(vidx / 2)][1]):
                    l[uv_layer].uv = base_uv + hdiff + vdiff
                    if self.select:
                        l[uv_layer].select = True","zip(loops, diffs[int(vidx / 2)][0], diffs[int(vidx / 2)][1])","zip(loops, *diffs[int(vidx / 2)][:2])","iterable_zj[0], iterable_zj[1]",*diffs[int(vidx / 2)][:2],*diffs[int(vidx / 2)][:2],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_lookahead_swap.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_lookahead_swap.py,TestLookaheadSwap,test_lookahead_swap_finds_minimal_swap_solution$73,"def test_lookahead_swap_finds_minimal_swap_solution(self):
        """"""Of many valid SWAPs, test that LookaheadSwap finds the cheapest path.

        For a two CNOT circuit: cx q[0],q[2]; cx q[0],q[1]
        on the initial layout: qN -> qN
        (At least) two solutions exist:
        - SWAP q[0],[1], cx q[0],q[2], cx q[0],q[1]
        - SWAP q[1],[2], cx q[0],q[2], SWAP q[1],q[2], cx q[0],q[1]

        Verify that we find the first solution, as it requires fewer SWAPs.
        """"""

        qr = QuantumRegister(3, ""q"")
        circuit = QuantumCircuit(qr)
        circuit.cx(qr[0], qr[2])
        circuit.cx(qr[0], qr[1])

        dag_circuit = circuit_to_dag(circuit)

        coupling_map = CouplingMap([[0, 1], [1, 2]])

        mapped_dag = LookaheadSwap(coupling_map).run(dag_circuit)

        self.assertEqual(
            mapped_dag.count_ops().get(""swap"", 0), dag_circuit.count_ops().get(""swap"", 0) + 1
        )","circuit.cx(qr[0], qr[2])",circuit.cx(*qr[::2]),"iterable_zj[0], iterable_zj[2]",*qr[::2],*qr[:4:2],0
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_lookahead_swap.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_lookahead_swap.py,TestLookaheadSwap,test_lookahead_swap_finds_minimal_swap_solution$73,"def test_lookahead_swap_finds_minimal_swap_solution(self):
        """"""Of many valid SWAPs, test that LookaheadSwap finds the cheapest path.

        For a two CNOT circuit: cx q[0],q[2]; cx q[0],q[1]
        on the initial layout: qN -> qN
        (At least) two solutions exist:
        - SWAP q[0],[1], cx q[0],q[2], cx q[0],q[1]
        - SWAP q[1],[2], cx q[0],q[2], SWAP q[1],q[2], cx q[0],q[1]

        Verify that we find the first solution, as it requires fewer SWAPs.
        """"""

        qr = QuantumRegister(3, ""q"")
        circuit = QuantumCircuit(qr)
        circuit.cx(qr[0], qr[2])
        circuit.cx(qr[0], qr[1])

        dag_circuit = circuit_to_dag(circuit)

        coupling_map = CouplingMap([[0, 1], [1, 2]])

        mapped_dag = LookaheadSwap(coupling_map).run(dag_circuit)

        self.assertEqual(
            mapped_dag.count_ops().get(""swap"", 0), dag_circuit.count_ops().get(""swap"", 0) + 1
        )","circuit.cx(qr[0], qr[1])",circuit.cx(*qr[:2]),"iterable_zj[0], iterable_zj[1]",*qr[:2],*qr[:2],1
rewriting,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rewriting/utils/segdata.py,https://github.com/davidbau/rewriting/tree/master/utils/segdata.py,MultiSegmentDataset,__getitem__$57,"def __getitem__(self, index):
        img, segimg = self.segdataset[index + self.first]
        segin = numpy.array(segimg, numpy.uint8, copy=False)
        segout = torch.zeros(len(self.categories),
                segin.shape[0], segin.shape[1], dtype=torch.int64)
        for i, field in enumerate(self.fields):
            fielddata = ((torch.from_numpy(segin[:, :, field.index])
                    >> field.bitshift) & field.bitmask)
            segout[i] = field.firstchannel + fielddata - 1
        bincount = numpy.bincount(segout.flatten(),
                minlength=len(self.labels))
        return img, segout, bincount","torch.zeros(len(self.categories), segin.shape[0], segin.shape[1], dtype=torch.int64)","torch.zeros(len(self.categories), *segin.shape[:2], dtype=torch.int64)","iterable_zj[0], iterable_zj[1]",*segin.shape[:2],*segin.shape[:2],1
SinGAN,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SinGAN/SinGAN/functions.py,https://github.com/tamarott/SinGAN/tree/master/SinGAN/functions.py,,dilate_mask$338,"def dilate_mask(mask,opt):
    if opt.mode == ""harmonization"":
        element = morphology.disk(radius=7)
    if opt.mode == ""editing"":
        element = morphology.disk(radius=20)
    mask = torch2uint8(mask)
    mask = mask[:,:,0]
    mask = morphology.binary_dilation(mask,selem=element)
    mask = filters.gaussian(mask, sigma=5)
    nc_im = opt.nc_im
    opt.nc_im = 1
    mask = np2torch(mask,opt)
    opt.nc_im = nc_im
    mask = mask.expand(1, 3, mask.shape[2], mask.shape[3])
    plt.imsave('%s/%s_mask_dilated.png' % (opt.ref_dir, opt.ref_name[:-4]), convert_image_np(mask), vmin=0,vmax=1)
    mask = (mask-mask.min())/(mask.max()-mask.min())
    return mask","mask.expand(1, 3, mask.shape[2], mask.shape[3])","mask.expand(1, 3, *mask.shape[2:4])","iterable_zj[2], iterable_zj[3]",*mask.shape[2:4],*mask.shape[2:4],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/visualization/test_circuit_text_drawer.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/visualization/test_circuit_text_drawer.py,TestTextConditional,test_text_conditional_cz_cregbundle$2515,"def test_text_conditional_cz_cregbundle(self):
        """"""Conditional CZ with a wire in the middle""""""
        qr = QuantumRegister(3, ""qr"")
        cr = ClassicalRegister(1, ""cr"")
        circuit = QuantumCircuit(qr, cr)
        circuit.cz(qr[0], qr[1]).c_if(cr, 1)

        expected = ""\n"".join(
            [
                ""                "",
                ""qr_0: |0>"",
                ""               "",
                ""qr_1: |0>"",
                ""               "",
                ""qr_2: |0>"",
                ""         "",
                "" cr: 0 1/ 0x1 "",
                ""         "",
            ]
        )

        self.assertEqual(str(_text_circuit_drawer(circuit, cregbundle=True)), expected)","circuit.cz(qr[0], qr[1])",circuit.cz(*qr[:2]),"iterable_zj[0], iterable_zj[1]",*qr[:2],*qr[:2],1
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_shed/galaxy_install/migrate/check.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_shed/galaxy_install/migrate/check.py,,verify_tools$19,"def verify_tools(app, url, galaxy_config_file=None, engine_options=None):
    # Check the value in the migrate_tools.version database table column to verify that the number is in
    # sync with the number of version scripts in ~/lib/galaxy/tools/migrate/versions.
    # Create engine and metadata
    engine_options = engine_options or {}
    engine = create_engine(url, **engine_options)
    meta = MetaData(bind=engine)
    # The migrate_tools table was created in database version script 0092_add_migrate_tools_table.py.
    Table(""migrate_tools"", meta, autoload=True)
    # Verify that the code and the database are in sync.
    db_schema = schema.ControlledSchema(engine, migrate_repository)
    latest_tool_migration_script_number = migrate_repository.versions.latest
    if latest_tool_migration_script_number != db_schema.version:
        # The default behavior is that the tool shed is down.
        tool_shed_accessible = False
        if app.new_installation:
            # New installations will not be missing tools, so we don't need to worry about them.
            missing_tool_configs_dict = {}
        else:
            tool_panel_configs = common_util.get_non_shed_tool_panel_configs(app)
            if tool_panel_configs:
                # The missing_tool_configs_dict contents are something like:
                # {'emboss_antigenic.xml': [('emboss', '5.0.0', 'package', '\nreadme blah blah blah\n')]}
                tool_shed_accessible, missing_tool_configs_dict = common_util.check_for_missing_tools(app,
                                                                                                      tool_panel_configs,
                                                                                                      latest_tool_migration_script_number)
            else:
                # It doesn't matter if the tool shed is accessible since there are no migrated tools defined in the local Galaxy instance, but
                # we have to set the value of tool_shed_accessible to True so that the value of migrate_tools.version can be correctly set in
                # the database.
                tool_shed_accessible = True
                missing_tool_configs_dict = {}
        have_tool_dependencies = False
        for v in missing_tool_configs_dict.values():
            if v:
                have_tool_dependencies = True
                break
        if not app.config.running_functional_tests:
            if tool_shed_accessible:
                # Automatically update the value of the migrate_tools.version database table column.
                manage_tools = os.path.abspath(os.path.join(os.path.dirname(__file__), 'scripts', 'manage_tools.py'))
                cmd = [get_executable(), manage_tools, 'upgrade', 'tools']
                if galaxy_config_file:
                    cmd[2:2] = ['-c', galaxy_config_file]
                proc = subprocess.Popen(args=cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
                return_code = proc.wait()
                output = proc.stdout.read(32768)
                if return_code != 0:
                    raise Exception(f""Error attempting to update the value of migrate_tools.version: {output}"")
                elif missing_tool_configs_dict:
                    if len(tool_panel_configs) == 1:
                        plural = ''
                        tool_panel_config_file_names = tool_panel_configs[0]
                    else:
                        plural = 's'
                        tool_panel_config_file_names = ', '.join(tool_panel_configs)
                    msg = ""\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>""
                    msg += ""\n\nThe list of files at the end of this message refers to tools that are configured to load into the tool panel for\n""
                    msg += ""this Galaxy instance, but have been removed from the Galaxy distribution.  These tools and their dependencies can be\n""
                    msg += ""automatically installed from the Galaxy tool shed at http://toolshed.g2.bx.psu.edu.\n\n""
                    msg += ""To skip this process, attempt to start your Galaxy server again (e.g., sh run.sh or whatever you use).  If you do this,\n""
                    msg += ""be aware that these tools will no longer be available in your Galaxy tool panel, and entries for each of them should\n""
                    msg += f""be removed from your file{plural} named {tool_panel_config_file_names}.\n\n""
                    msg += ""CRITICAL NOTE IF YOU PLAN TO INSTALL\n""
                    msg += ""The location in which the tool repositories will be installed is the value of the 'tool_path' attribute in the <tool>\n""
                    msg += 'tag of the file named ./migrated_tool_conf.xml (i.e., <toolbox tool_path=""database/shed_tools"">).  The default location\n'
                    msg += ""setting is 'database/shed_tools', which may be problematic for some cluster environments, so make sure to change it before\n""
                    msg += ""you execute the installation process if appropriate.  The configured location must be outside of the Galaxy installation\n""
                    msg += ""directory or it must be in a sub-directory protected by a properly configured .hgignore file if the directory is within\n""
                    msg += ""the Galaxy installation directory hierarchy.  This is because tool shed repositories will be installed using mercurial's\n""
                    msg += ""clone feature, which creates .hg directories and associated mercurial repository files.  Not having .hgignore properly\n""
                    msg += ""configured could result in undesired behavior when modifying or updating your local Galaxy instance or the tool shed\n""
                    msg += ""repositories if they are in directories that pose conflicts.  See mercurial's .hgignore documentation at the following\n""
                    msg += ""URL for details.\n\nhttp://mercurial.selenic.com/wiki/.hgignore\n\n""
                    if have_tool_dependencies:
                        msg += ""The following tool dependencies can also optionally be installed (see the option flag in the command below).  If you\n""
                        msg += ""choose to install them (recommended), they will be installed within the location specified by the 'tool_dependency_dir'\n""
                        msg += ""setting in your main Galaxy configuration file (e.g., uninverse_wsgi.ini).\n""
                        processed_tool_dependencies = []
                        for missing_tool_config in missing_tool_configs_dict.keys():
                            for tool_dependencies_tup in missing_tool_configs_dict[missing_tool_config]['tool_dependencies']:
                                if tool_dependencies_tup not in processed_tool_dependencies:
                                    msg += ""------------------------------------\n""
                                    msg += ""Tool Dependency\n""
                                    msg += ""------------------------------------\n""
                                    msg += ""Name: {}, Version: {}, Type: {}\n"".format(tool_dependencies_tup[0],
                                                                                  tool_dependencies_tup[1],
                                                                                  tool_dependencies_tup[2])
                                    if len(tool_dependencies_tup) >= 4:
                                        msg += ""Requirements and installation information:\n""
                                        msg += f""{tool_dependencies_tup[3]}\n""
                                    else:
                                        msg += ""\n""
                                    msg += ""------------------------------------\n""
                                    processed_tool_dependencies.append(tool_dependencies_tup)
                        msg += ""\n""
                    msg += f""{output.replace('done', '')}""
                    msg += ""vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n""
                    msg += ""sh ./scripts/migrate_tools/%04d_tools.sh\n"" % latest_tool_migration_script_number
                    msg += ""^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n""
                    if have_tool_dependencies:
                        msg += ""The tool dependencies listed above will be installed along with the repositories if you add the 'install_dependencies'\n""
                        msg += ""option to the above command like this:\n\n""
                        msg += ""vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n""
                        msg += ""sh ./scripts/migrate_tools/%04d_tools.sh install_dependencies\n"" % latest_tool_migration_script_number
                        msg += ""^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n""
                        msg += ""Tool dependencies can be installed after the repositories have been installed as well.\n\n""
                    msg += ""After the installation process finishes, you can start your Galaxy server.  As part of this installation process,\n""
                    msg += ""entries for each of the following tool config files will be added to the file named ./migrated_tool_conf.xml, so these\n""
                    msg += ""tools will continue to be loaded into your tool panel.  Because of this, existing entries for these tools have been\n""
                    msg += f""removed from your file{plural} named {tool_panel_config_file_names}.\n\n""
                    for missing_tool_config in missing_tool_configs_dict.keys():
                        msg += f""{missing_tool_config}\n""
                    msg += ""<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n""
                    raise Exception(msg)
            else:
                log.debug(f""The main Galaxy tool shed is not currently available, so skipped tool migration {db_schema.version} until next server startup"")
    else:
        log.info(""At migrate_tools version %d"" % db_schema.version)","'Name: {}, Version: {}, Type: {}\n'.format(tool_dependencies_tup[0], tool_dependencies_tup[1], tool_dependencies_tup[2])","'Name: {}, Version: {}, Type: {}\n'.format(*tool_dependencies_tup[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*tool_dependencies_tup[:3],*tool_dependencies_tup[:3],1
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/visualization/wcsaxes/coordinate_helpers.py,https://github.com/astropy/astropy/tree/master/astropy/visualization/wcsaxes/coordinate_helpers.py,CoordinateHelper,_update_grid_lines$940,"def _update_grid_lines(self):
        # For 3-d WCS with a correlated third axis, the *proper* way of
        # drawing a grid should be to find the world coordinates of all pixels
        # and drawing contours. What we are doing here assumes that we can
        # define the grid lines with just two of the coordinates (and
        # therefore assumes that the other coordinates are fixed and set to
        # the value in the slice). Here we basically assume that if the WCS
        # had a third axis, it has been abstracted away in the transformation.

        if self.coord_index is None:
            return

        coord_range = self.parent_map.get_coord_range()

        tick_world_coordinates, spacing = self.locator(*coord_range[self.coord_index])
        tick_world_coordinates_values = tick_world_coordinates.to_value(self.coord_unit)

        n_coord = len(tick_world_coordinates_values)

        from . import conf

        n_samples = conf.grid_samples

        xy_world = np.zeros((n_samples * n_coord, 2))

        self.grid_lines = []

        for iw, w in enumerate(tick_world_coordinates_values):
            subset = slice(iw * n_samples, (iw + 1) * n_samples)
            if self.coord_index == 0:
                xy_world[subset, 0] = np.repeat(w, n_samples)
                xy_world[subset, 1] = np.linspace(
                    coord_range[1][0], coord_range[1][1], n_samples
                )
            else:
                xy_world[subset, 0] = np.linspace(
                    coord_range[0][0], coord_range[0][1], n_samples
                )
                xy_world[subset, 1] = np.repeat(w, n_samples)

        # We now convert all the world coordinates to pixel coordinates in a
        # single go rather than doing this in the gridline to path conversion
        # to fully benefit from vectorized coordinate transformations.

        # Transform line to pixel coordinates
        pixel = self.transform.inverted().transform(xy_world)

        # Create round-tripped values for checking
        xy_world_round = self.transform.transform(pixel)

        for iw in range(n_coord):
            subset = slice(iw * n_samples, (iw + 1) * n_samples)
            self.grid_lines.append(
                self._get_gridline(
                    xy_world[subset], pixel[subset], xy_world_round[subset]
                )
            )","np.linspace(coord_range[1][0], coord_range[1][1], n_samples)","np.linspace(*coord_range[1][:2], n_samples)","iterable_zj[0], iterable_zj[1]",*coord_range[1][:2],*coord_range[1][:2],1
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/visualization/wcsaxes/coordinate_helpers.py,https://github.com/astropy/astropy/tree/master/astropy/visualization/wcsaxes/coordinate_helpers.py,CoordinateHelper,_update_grid_lines$940,"def _update_grid_lines(self):
        # For 3-d WCS with a correlated third axis, the *proper* way of
        # drawing a grid should be to find the world coordinates of all pixels
        # and drawing contours. What we are doing here assumes that we can
        # define the grid lines with just two of the coordinates (and
        # therefore assumes that the other coordinates are fixed and set to
        # the value in the slice). Here we basically assume that if the WCS
        # had a third axis, it has been abstracted away in the transformation.

        if self.coord_index is None:
            return

        coord_range = self.parent_map.get_coord_range()

        tick_world_coordinates, spacing = self.locator(*coord_range[self.coord_index])
        tick_world_coordinates_values = tick_world_coordinates.to_value(self.coord_unit)

        n_coord = len(tick_world_coordinates_values)

        from . import conf

        n_samples = conf.grid_samples

        xy_world = np.zeros((n_samples * n_coord, 2))

        self.grid_lines = []

        for iw, w in enumerate(tick_world_coordinates_values):
            subset = slice(iw * n_samples, (iw + 1) * n_samples)
            if self.coord_index == 0:
                xy_world[subset, 0] = np.repeat(w, n_samples)
                xy_world[subset, 1] = np.linspace(
                    coord_range[1][0], coord_range[1][1], n_samples
                )
            else:
                xy_world[subset, 0] = np.linspace(
                    coord_range[0][0], coord_range[0][1], n_samples
                )
                xy_world[subset, 1] = np.repeat(w, n_samples)

        # We now convert all the world coordinates to pixel coordinates in a
        # single go rather than doing this in the gridline to path conversion
        # to fully benefit from vectorized coordinate transformations.

        # Transform line to pixel coordinates
        pixel = self.transform.inverted().transform(xy_world)

        # Create round-tripped values for checking
        xy_world_round = self.transform.transform(pixel)

        for iw in range(n_coord):
            subset = slice(iw * n_samples, (iw + 1) * n_samples)
            self.grid_lines.append(
                self._get_gridline(
                    xy_world[subset], pixel[subset], xy_world_round[subset]
                )
            )","np.linspace(coord_range[0][0], coord_range[0][1], n_samples)","np.linspace(*coord_range[0][:2], n_samples)","iterable_zj[0], iterable_zj[1]",*coord_range[0][:2],*coord_range[0][:2],1
fuzzbench,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fuzzbench/experiment/stop_experiment.py,https://github.com/google/fuzzbench/tree/master/experiment/stop_experiment.py,,main$67,"def main():
    """"""Stop the experiment.""""""
    if len(sys.argv) != 3:
        print('Usage {0} <experiment-name> <experiment-config.yaml>')
        return 1
    logs.initialize()
    return 0 if stop_experiment(sys.argv[1], sys.argv[2]) else 1","stop_experiment(sys.argv[1], sys.argv[2])",stop_experiment(*sys.argv[1:3]),"iterable_zj[1], iterable_zj[2]",*sys.argv[1:3],*sys.argv[1:3],1
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/scripts/live_test/sendemail.py,https://github.com/Azure/azure-cli/tree/master/scripts/live_test/sendemail.py,,get_content$250,"def get_content(container, testdata):
    """"""
    Compose content of email
    :return:
    """"""
    logger.warning('Enter get_content()')

    content = """"""
    <!DOCTYPE html>
    <html>
    <head>
    <style>
    table, th, td {
      border: 1px solid black;
      border-collapse: collapse;
    }
    </style>
    </head>
    <body>
    """"""

    link = 'https://dev.azure.com/azure-sdk/internal/_build/results?buildId={}&view=ms.vss-test-web.build-test-results-tab'.format(BUILD_ID)
    content += """"""
    <p>Hi Azure CLI team,</p>
    <p>[Please move this mail to normal folder if it is in junk box, otherwise, the HTML and CSS content may not be displayed correctly]</p>
    <p>
    Here are test results of Azure CLI.<br>
    Repository: {}<br>
    Branch: {}<br>
    Link: {}
    </p>
    """""".format(USER_REPO, USER_BRANCH, link)

    content += """"""
    <p>
    <b>User Manual of Live Test Pipeline</b><br>
    <a href=https://microsoft-my.sharepoint.com/:w:/p/fey/EZGC9LwrN3RAscVS5ylG4HMBX9h7W0ZSA7CDrhXN5Lvx6g?e=V8HUmd>Word</a>  
    <a href=https://microsoft.sharepoint.com/teams/IoTToolingTeam/_layouts/OneNote.aspx?id=%2Fteams%2FIoTToolingTeam%2FShared%20Documents%2FAzure%20Management%20Experience%2FAzure%20Management%20Experience&wd=target%28AZ%20CLI%2FKnowledge%20base.one%7C18BC64EE-9328-497D-804E-6436006CA9A5%2FUser%20Manual%20of%20Live%20Test%20Pipeline%7C243EFA3E-FC7F-4612-9DA5-8E6BB2A11BD3%2F%29>OneNote</a>
    </p>
    """"""

    if container != '':
        content += """"""
        <p>
        <b>Test results location</b><br>
        Storage account: /subscriptions/0b1f6471-1bf0-4dda-aec3-cb9272f09590/resourceGroups/clitestresult/providers/Microsoft.Storage/storageAccounts/clitestresultstac <br>
        Container: {}
        </p>
        """""".format(container)

    table = """"""
    <p><b>Test results summary</b></p>
    <table>
      <tr>
        <th>Module</th>
        <th>Passed</th>
        <th>Failed</th>
        <th>Pass rate</th>
      </tr>
    """"""

    for module, passed, failed, rate in testdata.modules:
        table += """"""
          <tr>
            <td>{}</td>
            <td>{}</td>
            <td>{}</td>
            <td>{}</td>
          </tr>
        """""".format(module, passed, failed, rate)

    table += """"""
      <tr>
        <td>Total</td>
        <td>{}</td>
        <td>{}</td>
        <td>{}</td>
      </tr>
    </table>
    """""".format(testdata.total[1], testdata.total[2], testdata.total[3])

    content += table

    content += """"""
    </body>
    </html>
    """"""

    logger.warning(content)
    logger.warning('Exit get_content()')
    return content","'\n      <tr>\n        <td>Total</td>\n        <td>{}</td>\n        <td>{}</td>\n        <td>{}</td>\n      </tr>\n    </table>\n    '.format(testdata.total[1], testdata.total[2], testdata.total[3])",'\n      <tr>\n        <td>Total</td>\n        <td>{}</td>\n        <td>{}</td>\n        <td>{}</td>\n      </tr>\n    </table>\n    '.format(*testdata.total[1:4]),"iterable_zj[1], iterable_zj[2], iterable_zj[3]",*testdata.total[1:4],*testdata.total[1:4],1
pyglet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglet/tools/wraptypes/cparser.py,https://github.com/pyglet/pyglet/tree/master/tools/wraptypes/cparser.py,,p_shift_expression$521,"def p_shift_expression(p):
    '''shift_expression : additive_expression
                        | shift_expression LEFT_OP additive_expression
                        | shift_expression RIGHT_OP additive_expression
    '''
    if len(p) == 2:
        p[0] = p[1]
    else:
        p[0] = BinaryExpressionNode({
            '<<': operator.lshift,
            '>>': operator.rshift}[p[2]], p[2], p[1], p[3])","BinaryExpressionNode({'<<': operator.lshift, '>>': operator.rshift}[p[2]], p[2], p[1], p[3])","BinaryExpressionNode({'<<': operator.lshift, '>>': operator.rshift}[p[2]], *p[1:4][::-1])","iterable_zj[2], iterable_zj[1], iterable_zj[3]",*p[1:4][::-1],*p[1:5:2],0
mmf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmf/mmf/modules/layers.py,https://github.com/facebookresearch/mmf/tree/master/mmf/modules/layers.py,AttnPool2d,forward$776,"def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(
            2, 0, 1
        )  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC
        x, _ = nn.functional.multi_head_attention_forward(
            query=x,
            key=x,
            value=x,
            embed_dim_to_check=x.shape[-1],
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat(
                [self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]
            ),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False,
        )

        return x[0]","x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3])",x.reshape(*x.shape[:3] * x.shape[3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*x.shape[:3],*x.shape[:2],0
pyquil,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/test/unit/test_quil.py,https://github.com/rigetti/pyquil/tree/master/test/unit/test_quil.py,,test_out_vs_str$1042,"def test_out_vs_str():
    qs = QubitPlaceholder.register(6)
    pq = Program(
        Declare(""ro"", ""BIT"", 6),
        X(qs[0]),
        CNOT(qs[0], qs[4]),
        MEASURE(qs[5], MemoryReference(""ro"", 5)),
    )

    with pytest.raises(RuntimeError) as e:
        pq.out()
    assert e.match(r""Qubit q\d+ has not been assigned an index"")

    string_version = str(pq)
    should_be_re = r""DECLARE ro BIT\[6\]\nX \{q\d+\}\nCNOT \{q\d+\} \{q\d+\}\nMEASURE \{q\d+\} ro\[5\]\n""
    assert re.fullmatch(should_be_re, string_version, flags=re.MULTILINE)","CNOT(qs[0], qs[4])",CNOT(*qs[0:5:4]),"iterable_zj[0], iterable_zj[4]",*qs[0:5:4],*qs[:8:4],0
centermask2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/centermask2/centermask/modeling/backbone/vovnet.py,https://github.com/youngwanLEE/centermask2/tree/master/centermask/modeling/backbone/vovnet.py,VoVNet,__init__$382,"def __init__(self, cfg, input_ch, out_features=None):
        """"""
        Args:
            input_ch(int) : the number of input channel
            out_features (list[str]): name of the layers whose outputs should
                be returned in forward. Can be anything in ""stem"", ""stage2"" ...
        """"""
        super(VoVNet, self).__init__()

        global _NORM
        _NORM = cfg.MODEL.VOVNET.NORM
            
        stage_specs = _STAGE_SPECS[cfg.MODEL.VOVNET.CONV_BODY]

        stem_ch = stage_specs[""stem""]
        config_stage_ch = stage_specs[""stage_conv_ch""]
        config_concat_ch = stage_specs[""stage_out_ch""]
        block_per_stage = stage_specs[""block_per_stage""]
        layer_per_block = stage_specs[""layer_per_block""]
        SE = stage_specs[""eSE""]
        depthwise = stage_specs[""dw""]

        self._out_features = out_features


        # Stem module
        conv_type = dw_conv3x3 if depthwise else conv3x3
        stem = conv3x3(input_ch, stem_ch[0], ""stem"", ""1"", 2)
        stem += conv_type(stem_ch[0], stem_ch[1], ""stem"", ""2"", 1)
        stem += conv_type(stem_ch[1], stem_ch[2], ""stem"", ""3"", 2)
        self.add_module(""stem"", nn.Sequential((OrderedDict(stem))))
        current_stirde = 4
        self._out_feature_strides = {""stem"": current_stirde, ""stage2"": current_stirde}
        self._out_feature_channels = {""stem"": stem_ch[2]}

        stem_out_ch = [stem_ch[2]]
        in_ch_list = stem_out_ch + config_concat_ch[:-1]
        # OSA stages
        self.stage_names = []
        for i in range(4):  # num_stages
            name = ""stage%d"" % (i + 2) # stage 2 ... stage 5
            self.stage_names.append(name)
            self.add_module(name, _OSA_stage(in_ch_list[i],
                                             config_stage_ch[i],
                                             config_concat_ch[i],
                                             block_per_stage[i],
                                             layer_per_block,
                                             i + 2,
                                             SE,
                                             depthwise,
                                             dcn_config = {
                                                 ""stage_with_dcn"": cfg.MODEL.VOVNET.STAGE_WITH_DCN[i],
                                                 ""with_modulated_dcn"": cfg.MODEL.VOVNET.WITH_MODULATED_DCN,
                                                 ""deformable_groups"": cfg.MODEL.VOVNET.DEFORMABLE_GROUPS,
                                             }
            ))
            
            self._out_feature_channels[name] = config_concat_ch[i]
            if not i == 0:
                self._out_feature_strides[name] = current_stirde = int(
                    current_stirde * 2) 

        # initialize weights
        # self._initialize_weights()
        # Optionally freeze (requires_grad=False) parts of the backbone
        self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_AT)","conv_type(stem_ch[0], stem_ch[1], 'stem', '2', 1)","conv_type(*stem_ch[:2], 'stem', '2', 1)","iterable_zj[0], iterable_zj[1]",*stem_ch[:2],*stem_ch[:2],1
centermask2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/centermask2/centermask/modeling/backbone/vovnet.py,https://github.com/youngwanLEE/centermask2/tree/master/centermask/modeling/backbone/vovnet.py,VoVNet,__init__$382,"def __init__(self, cfg, input_ch, out_features=None):
        """"""
        Args:
            input_ch(int) : the number of input channel
            out_features (list[str]): name of the layers whose outputs should
                be returned in forward. Can be anything in ""stem"", ""stage2"" ...
        """"""
        super(VoVNet, self).__init__()

        global _NORM
        _NORM = cfg.MODEL.VOVNET.NORM
            
        stage_specs = _STAGE_SPECS[cfg.MODEL.VOVNET.CONV_BODY]

        stem_ch = stage_specs[""stem""]
        config_stage_ch = stage_specs[""stage_conv_ch""]
        config_concat_ch = stage_specs[""stage_out_ch""]
        block_per_stage = stage_specs[""block_per_stage""]
        layer_per_block = stage_specs[""layer_per_block""]
        SE = stage_specs[""eSE""]
        depthwise = stage_specs[""dw""]

        self._out_features = out_features


        # Stem module
        conv_type = dw_conv3x3 if depthwise else conv3x3
        stem = conv3x3(input_ch, stem_ch[0], ""stem"", ""1"", 2)
        stem += conv_type(stem_ch[0], stem_ch[1], ""stem"", ""2"", 1)
        stem += conv_type(stem_ch[1], stem_ch[2], ""stem"", ""3"", 2)
        self.add_module(""stem"", nn.Sequential((OrderedDict(stem))))
        current_stirde = 4
        self._out_feature_strides = {""stem"": current_stirde, ""stage2"": current_stirde}
        self._out_feature_channels = {""stem"": stem_ch[2]}

        stem_out_ch = [stem_ch[2]]
        in_ch_list = stem_out_ch + config_concat_ch[:-1]
        # OSA stages
        self.stage_names = []
        for i in range(4):  # num_stages
            name = ""stage%d"" % (i + 2) # stage 2 ... stage 5
            self.stage_names.append(name)
            self.add_module(name, _OSA_stage(in_ch_list[i],
                                             config_stage_ch[i],
                                             config_concat_ch[i],
                                             block_per_stage[i],
                                             layer_per_block,
                                             i + 2,
                                             SE,
                                             depthwise,
                                             dcn_config = {
                                                 ""stage_with_dcn"": cfg.MODEL.VOVNET.STAGE_WITH_DCN[i],
                                                 ""with_modulated_dcn"": cfg.MODEL.VOVNET.WITH_MODULATED_DCN,
                                                 ""deformable_groups"": cfg.MODEL.VOVNET.DEFORMABLE_GROUPS,
                                             }
            ))
            
            self._out_feature_channels[name] = config_concat_ch[i]
            if not i == 0:
                self._out_feature_strides[name] = current_stirde = int(
                    current_stirde * 2) 

        # initialize weights
        # self._initialize_weights()
        # Optionally freeze (requires_grad=False) parts of the backbone
        self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_AT)","conv_type(stem_ch[1], stem_ch[2], 'stem', '3', 2)","conv_type(*stem_ch[1:3], 'stem', '3', 2)","iterable_zj[1], iterable_zj[2]",*stem_ch[1:3],*stem_ch[1:3],1
Multi-Camera-Live-Object-Tracking,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Multi-Camera-Live-Object-Tracking/traffic_counting/camera_yolo.py,https://github.com/LeonLok/Multi-Camera-Live-Object-Tracking/tree/master/traffic_counting/camera_yolo.py,Camera,yolo_frames$42,"def yolo_frames(image_hub, unique_name):
        device = unique_name[1]

        show_detections = False

        gdet = import_module('tools.generate_detections')
        nn_matching = import_module('deep_sort.nn_matching')
        Tracker = import_module('deep_sort.tracker').Tracker

        # Definition of the parameters
        max_cosine_distance = 0.3
        nn_budget = None

        # deep_sort
        model_filename = 'model_data/mars-small128.pb'
        encoder = gdet.create_box_encoder(model_filename, batch_size=1)

        metric = nn_matching.NearestNeighborDistanceMetric(""cosine"", max_cosine_distance, nn_budget)
        tracker = Tracker(metric)

        yolo = YOLO()
        nms_max_overlap = 1.0

        num_frames = 0

        current_date = datetime.datetime.now().date()
        count_dict = {}  # initiate dict for storing counts

        total_counter = 0
        up_count = 0
        down_count = 0

        class_counter = Counter()  # store counts of each detected class
        already_counted = deque(maxlen=50)  # temporary memory for storing counted IDs
        intersect_info = []  # initialise intersection list

        memory = {}
        while True:
            cam_id, frame = image_hub.recv_image()
            image_hub.send_reply(b'OK')  # this is needed for the stream to work with REQ/REP pattern
            # image_height, image_width = frame.shape[:2]

            if frame is None:
                break

            num_frames += 1

            '''
            if num_frames % 2 != 0:  # only process frames at set number of frame intervals
                continue
            '''

            image = Image.fromarray(frame[..., ::-1])  # convert bgr to rgb
            boxes, confidence, classes = yolo.detect_image(image)
            features = encoder(frame, boxes)

            detections = [Detection(bbox, confidence, cls, feature) for bbox, confidence, cls, feature in
                          zip(boxes, confidence, classes, features)]

            # Run non-maxima suppression.
            boxes = np.array([d.tlwh for d in detections])
            scores = np.array([d.confidence for d in detections])
            classes = np.array([d.cls for d in detections])
            indices = preprocessing.non_max_suppression(boxes, nms_max_overlap, scores)
            detections = [detections[i] for i in indices]

            # Call the tracker
            tracker.predict()
            tracker.update(detections)

            if cam_id == 'Camera 1':
                line = [(0, int(0.5 * frame.shape[0])), (int(frame.shape[1]), int(0.5 * frame.shape[0]))]
            else:
                line = [(0, int(0.33 * frame.shape[0])), (int(frame.shape[1]), int(0.33 * frame.shape[0]))]

            # draw yellow line
            cv2.line(frame, line[0], line[1], (0, 255, 255), 2)

            for track in tracker.tracks:
                if not track.is_confirmed() or track.time_since_update > 1:
                    continue
                bbox = track.to_tlbr()
                track_cls = track.cls  # most common detection class for track

                midpoint = track.tlbr_midpoint(bbox)
                origin_midpoint = (midpoint[0], frame.shape[0] - midpoint[1])  # get midpoint respective to botton-left

                if track.track_id not in memory:
                    memory[track.track_id] = deque(maxlen=2)

                memory[track.track_id].append(midpoint)
                previous_midpoint = memory[track.track_id][0]

                origin_previous_midpoint = (previous_midpoint[0], frame.shape[0] - previous_midpoint[1])

                cv2.line(frame, midpoint, previous_midpoint, (0, 255, 0), 2)

                # Add to counter and get intersection details
                if Camera.intersect(midpoint, previous_midpoint, line[0], line[1]) and track.track_id not in already_counted:
                    class_counter[track_cls] += 1
                    total_counter += 1

                    # draw red line
                    cv2.line(frame, line[0], line[1], (0, 0, 255), 2)

                    already_counted.append(track.track_id)  # Set already counted for ID to true.

                    intersection_time = datetime.datetime.now() - datetime.timedelta(microseconds=datetime.datetime.now().microsecond)
                    angle = Camera.vector_angle(origin_midpoint, origin_previous_midpoint)
                    intersect_info.append([track_cls, origin_midpoint, angle, intersection_time])

                    if angle > 0:
                        up_count += 1
                    if angle < 0:
                        down_count += 1

                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 255, 255), 2)  # WHITE BOX
                cv2.putText(frame, ""ID: "" + str(track.track_id), (int(bbox[0]), int(bbox[1])), 0,
                            1.5e-3 * frame.shape[0], (0, 255, 0), 2)

                if not show_detections:
                    adc = ""%.2f"" % (track.adc * 100) + ""%""  # Average detection confidence
                    cv2.putText(frame, str(track_cls), (int(bbox[0]), int(bbox[3])), 0,
                                1e-3 * frame.shape[0], (0, 255, 0), 2)
                    cv2.putText(frame, 'ADC: ' + adc, (int(bbox[0]), int(bbox[3] + 2e-2 * frame.shape[1])), 0,
                                1e-3 * frame.shape[0], (0, 255, 0), 2)

            # Delete memory of old tracks.
            # This needs to be larger than the number of tracked objects in the frame.
            if len(memory) > 50:
                del memory[list(memory)[0]]

            # Draw total count.
            cv2.putText(frame, ""Total: {} ({} up, {} down)"".format(str(total_counter), str(up_count),
                        str(down_count)), (int(0.05 * frame.shape[1]), int(0.1 * frame.shape[0])), 0,
                        1.5e-3 * frame.shape[0], (0, 255, 255), 2)

            if show_detections:
                for det in detections:
                    bbox = det.to_tlbr()
                    score = ""%.2f"" % (det.confidence * 100) + ""%""
                    cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)  # BLUE BOX
                    if len(classes) > 0:
                        det_cls = det.cls
                        cv2.putText(frame, str(det_cls) + "" "" + score, (int(bbox[0]), int(bbox[3])), 0,
                                    1.5e-3 * frame.shape[0], (0, 255, 0), 2)

            # display counts for each class as they appear
            y = 0.2 * frame.shape[0]
            for cls in class_counter:
                class_count = class_counter[cls]
                cv2.putText(frame, str(cls) + "" "" + str(class_count), (int(0.05 * frame.shape[1]), int(y)), 0,
                            1.5e-3 * frame.shape[0], (0, 255, 255), 2)
                y += 0.05 * frame.shape[0]

            # calculate current minute
            now = datetime.datetime.now()
            rounded_now = now - datetime.timedelta(microseconds=now.microsecond)  # round to nearest second
            current_minute = now.time().minute

            if current_minute == 0 and len(count_dict) > 1:
                count_dict = {}  # reset counts every hour
            else:
                # write counts to file for every set interval of the hour
                write_interval = 5
                if current_minute % write_interval == 0:  # write to file once only every write_interval minutes
                    if current_minute not in count_dict:
                        count_dict[current_minute] = True
                        total_filename = 'Total counts for {}, {}.txt'.format(current_date, cam_id)
                        counts_folder = './counts/'
                        if not os.access(counts_folder + str(current_date) + '/total', os.W_OK):
                            os.makedirs(counts_folder + str(current_date) + '/total')
                        total_count_file = open(counts_folder + str(current_date) + '/total/' + total_filename, 'a')
                        print('{} writing...'.format(rounded_now))
                        print('Writing current total count ({}) and directional counts to file.'.format(total_counter))
                        total_count_file.write('{}, {}, {}, {}, {}\n'.format(str(rounded_now), device,
                                                                             str(total_counter), up_count, down_count))
                        total_count_file.close()

                        # if class exists in class counter, create file and write counts

                        if not os.access(counts_folder + str(current_date) + '/classes', os.W_OK):
                            os.makedirs(counts_folder + str(current_date) + '/classes')
                        for cls in class_counter:
                            class_count = class_counter[cls]
                            print('Writing current {} count ({}) to file.'.format(cls, class_count))
                            class_filename = 'Class counts for {}, {}.txt'.format(current_date, cam_id)
                            class_count_file = open(counts_folder + str(current_date) + '/classes/' + class_filename, 'a')
                            class_count_file.write(""{}, {}, {}\n"".format(rounded_now, device, str(class_count)))
                            class_count_file.close()

                        # write intersection details
                        if not os.access(counts_folder + str(current_date) + '/intersections', os.W_OK):
                            os.makedirs(counts_folder + str(current_date) + '/intersections')
                        print('Writing intersection details for {}'.format(cam_id))
                        intersection_filename = 'Intersection details for {}, {}.txt'.format(current_date, cam_id)
                        intersection_file = open(counts_folder + str(current_date) + '/intersections/' + intersection_filename, 'a')
                        for i in intersect_info:
                            cls = i[0]

                            midpoint = i[1]
                            x = midpoint[0]
                            y = midpoint[1]

                            angle = i[2]

                            intersect_time = i[3]

                            intersection_file.write(""{}, {}, {}, {}, {}, {}\n"".format(str(intersect_time), device, cls,
                                                                                      x, y, str(angle)))
                        intersection_file.close()
                        intersect_info = []  # reset list after writing

            yield cam_id, frame","cv2.line(frame, line[0], line[1], (0, 255, 255), 2)","cv2.line(frame, *line[:2], (0, 255, 255), 2)","iterable_zj[0], iterable_zj[1]",*line[:2],*line[:2],1
Multi-Camera-Live-Object-Tracking,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Multi-Camera-Live-Object-Tracking/traffic_counting/camera_yolo.py,https://github.com/LeonLok/Multi-Camera-Live-Object-Tracking/tree/master/traffic_counting/camera_yolo.py,Camera,yolo_frames$42,"def yolo_frames(image_hub, unique_name):
        device = unique_name[1]

        show_detections = False

        gdet = import_module('tools.generate_detections')
        nn_matching = import_module('deep_sort.nn_matching')
        Tracker = import_module('deep_sort.tracker').Tracker

        # Definition of the parameters
        max_cosine_distance = 0.3
        nn_budget = None

        # deep_sort
        model_filename = 'model_data/mars-small128.pb'
        encoder = gdet.create_box_encoder(model_filename, batch_size=1)

        metric = nn_matching.NearestNeighborDistanceMetric(""cosine"", max_cosine_distance, nn_budget)
        tracker = Tracker(metric)

        yolo = YOLO()
        nms_max_overlap = 1.0

        num_frames = 0

        current_date = datetime.datetime.now().date()
        count_dict = {}  # initiate dict for storing counts

        total_counter = 0
        up_count = 0
        down_count = 0

        class_counter = Counter()  # store counts of each detected class
        already_counted = deque(maxlen=50)  # temporary memory for storing counted IDs
        intersect_info = []  # initialise intersection list

        memory = {}
        while True:
            cam_id, frame = image_hub.recv_image()
            image_hub.send_reply(b'OK')  # this is needed for the stream to work with REQ/REP pattern
            # image_height, image_width = frame.shape[:2]

            if frame is None:
                break

            num_frames += 1

            '''
            if num_frames % 2 != 0:  # only process frames at set number of frame intervals
                continue
            '''

            image = Image.fromarray(frame[..., ::-1])  # convert bgr to rgb
            boxes, confidence, classes = yolo.detect_image(image)
            features = encoder(frame, boxes)

            detections = [Detection(bbox, confidence, cls, feature) for bbox, confidence, cls, feature in
                          zip(boxes, confidence, classes, features)]

            # Run non-maxima suppression.
            boxes = np.array([d.tlwh for d in detections])
            scores = np.array([d.confidence for d in detections])
            classes = np.array([d.cls for d in detections])
            indices = preprocessing.non_max_suppression(boxes, nms_max_overlap, scores)
            detections = [detections[i] for i in indices]

            # Call the tracker
            tracker.predict()
            tracker.update(detections)

            if cam_id == 'Camera 1':
                line = [(0, int(0.5 * frame.shape[0])), (int(frame.shape[1]), int(0.5 * frame.shape[0]))]
            else:
                line = [(0, int(0.33 * frame.shape[0])), (int(frame.shape[1]), int(0.33 * frame.shape[0]))]

            # draw yellow line
            cv2.line(frame, line[0], line[1], (0, 255, 255), 2)

            for track in tracker.tracks:
                if not track.is_confirmed() or track.time_since_update > 1:
                    continue
                bbox = track.to_tlbr()
                track_cls = track.cls  # most common detection class for track

                midpoint = track.tlbr_midpoint(bbox)
                origin_midpoint = (midpoint[0], frame.shape[0] - midpoint[1])  # get midpoint respective to botton-left

                if track.track_id not in memory:
                    memory[track.track_id] = deque(maxlen=2)

                memory[track.track_id].append(midpoint)
                previous_midpoint = memory[track.track_id][0]

                origin_previous_midpoint = (previous_midpoint[0], frame.shape[0] - previous_midpoint[1])

                cv2.line(frame, midpoint, previous_midpoint, (0, 255, 0), 2)

                # Add to counter and get intersection details
                if Camera.intersect(midpoint, previous_midpoint, line[0], line[1]) and track.track_id not in already_counted:
                    class_counter[track_cls] += 1
                    total_counter += 1

                    # draw red line
                    cv2.line(frame, line[0], line[1], (0, 0, 255), 2)

                    already_counted.append(track.track_id)  # Set already counted for ID to true.

                    intersection_time = datetime.datetime.now() - datetime.timedelta(microseconds=datetime.datetime.now().microsecond)
                    angle = Camera.vector_angle(origin_midpoint, origin_previous_midpoint)
                    intersect_info.append([track_cls, origin_midpoint, angle, intersection_time])

                    if angle > 0:
                        up_count += 1
                    if angle < 0:
                        down_count += 1

                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 255, 255), 2)  # WHITE BOX
                cv2.putText(frame, ""ID: "" + str(track.track_id), (int(bbox[0]), int(bbox[1])), 0,
                            1.5e-3 * frame.shape[0], (0, 255, 0), 2)

                if not show_detections:
                    adc = ""%.2f"" % (track.adc * 100) + ""%""  # Average detection confidence
                    cv2.putText(frame, str(track_cls), (int(bbox[0]), int(bbox[3])), 0,
                                1e-3 * frame.shape[0], (0, 255, 0), 2)
                    cv2.putText(frame, 'ADC: ' + adc, (int(bbox[0]), int(bbox[3] + 2e-2 * frame.shape[1])), 0,
                                1e-3 * frame.shape[0], (0, 255, 0), 2)

            # Delete memory of old tracks.
            # This needs to be larger than the number of tracked objects in the frame.
            if len(memory) > 50:
                del memory[list(memory)[0]]

            # Draw total count.
            cv2.putText(frame, ""Total: {} ({} up, {} down)"".format(str(total_counter), str(up_count),
                        str(down_count)), (int(0.05 * frame.shape[1]), int(0.1 * frame.shape[0])), 0,
                        1.5e-3 * frame.shape[0], (0, 255, 255), 2)

            if show_detections:
                for det in detections:
                    bbox = det.to_tlbr()
                    score = ""%.2f"" % (det.confidence * 100) + ""%""
                    cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)  # BLUE BOX
                    if len(classes) > 0:
                        det_cls = det.cls
                        cv2.putText(frame, str(det_cls) + "" "" + score, (int(bbox[0]), int(bbox[3])), 0,
                                    1.5e-3 * frame.shape[0], (0, 255, 0), 2)

            # display counts for each class as they appear
            y = 0.2 * frame.shape[0]
            for cls in class_counter:
                class_count = class_counter[cls]
                cv2.putText(frame, str(cls) + "" "" + str(class_count), (int(0.05 * frame.shape[1]), int(y)), 0,
                            1.5e-3 * frame.shape[0], (0, 255, 255), 2)
                y += 0.05 * frame.shape[0]

            # calculate current minute
            now = datetime.datetime.now()
            rounded_now = now - datetime.timedelta(microseconds=now.microsecond)  # round to nearest second
            current_minute = now.time().minute

            if current_minute == 0 and len(count_dict) > 1:
                count_dict = {}  # reset counts every hour
            else:
                # write counts to file for every set interval of the hour
                write_interval = 5
                if current_minute % write_interval == 0:  # write to file once only every write_interval minutes
                    if current_minute not in count_dict:
                        count_dict[current_minute] = True
                        total_filename = 'Total counts for {}, {}.txt'.format(current_date, cam_id)
                        counts_folder = './counts/'
                        if not os.access(counts_folder + str(current_date) + '/total', os.W_OK):
                            os.makedirs(counts_folder + str(current_date) + '/total')
                        total_count_file = open(counts_folder + str(current_date) + '/total/' + total_filename, 'a')
                        print('{} writing...'.format(rounded_now))
                        print('Writing current total count ({}) and directional counts to file.'.format(total_counter))
                        total_count_file.write('{}, {}, {}, {}, {}\n'.format(str(rounded_now), device,
                                                                             str(total_counter), up_count, down_count))
                        total_count_file.close()

                        # if class exists in class counter, create file and write counts

                        if not os.access(counts_folder + str(current_date) + '/classes', os.W_OK):
                            os.makedirs(counts_folder + str(current_date) + '/classes')
                        for cls in class_counter:
                            class_count = class_counter[cls]
                            print('Writing current {} count ({}) to file.'.format(cls, class_count))
                            class_filename = 'Class counts for {}, {}.txt'.format(current_date, cam_id)
                            class_count_file = open(counts_folder + str(current_date) + '/classes/' + class_filename, 'a')
                            class_count_file.write(""{}, {}, {}\n"".format(rounded_now, device, str(class_count)))
                            class_count_file.close()

                        # write intersection details
                        if not os.access(counts_folder + str(current_date) + '/intersections', os.W_OK):
                            os.makedirs(counts_folder + str(current_date) + '/intersections')
                        print('Writing intersection details for {}'.format(cam_id))
                        intersection_filename = 'Intersection details for {}, {}.txt'.format(current_date, cam_id)
                        intersection_file = open(counts_folder + str(current_date) + '/intersections/' + intersection_filename, 'a')
                        for i in intersect_info:
                            cls = i[0]

                            midpoint = i[1]
                            x = midpoint[0]
                            y = midpoint[1]

                            angle = i[2]

                            intersect_time = i[3]

                            intersection_file.write(""{}, {}, {}, {}, {}, {}\n"".format(str(intersect_time), device, cls,
                                                                                      x, y, str(angle)))
                        intersection_file.close()
                        intersect_info = []  # reset list after writing

            yield cam_id, frame","Camera.intersect(midpoint, previous_midpoint, line[0], line[1])","Camera.intersect(midpoint, previous_midpoint, *line[:2])","iterable_zj[0], iterable_zj[1]",*line[:2],*line[:2],1
Multi-Camera-Live-Object-Tracking,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Multi-Camera-Live-Object-Tracking/traffic_counting/camera_yolo.py,https://github.com/LeonLok/Multi-Camera-Live-Object-Tracking/tree/master/traffic_counting/camera_yolo.py,Camera,yolo_frames$42,"def yolo_frames(image_hub, unique_name):
        device = unique_name[1]

        show_detections = False

        gdet = import_module('tools.generate_detections')
        nn_matching = import_module('deep_sort.nn_matching')
        Tracker = import_module('deep_sort.tracker').Tracker

        # Definition of the parameters
        max_cosine_distance = 0.3
        nn_budget = None

        # deep_sort
        model_filename = 'model_data/mars-small128.pb'
        encoder = gdet.create_box_encoder(model_filename, batch_size=1)

        metric = nn_matching.NearestNeighborDistanceMetric(""cosine"", max_cosine_distance, nn_budget)
        tracker = Tracker(metric)

        yolo = YOLO()
        nms_max_overlap = 1.0

        num_frames = 0

        current_date = datetime.datetime.now().date()
        count_dict = {}  # initiate dict for storing counts

        total_counter = 0
        up_count = 0
        down_count = 0

        class_counter = Counter()  # store counts of each detected class
        already_counted = deque(maxlen=50)  # temporary memory for storing counted IDs
        intersect_info = []  # initialise intersection list

        memory = {}
        while True:
            cam_id, frame = image_hub.recv_image()
            image_hub.send_reply(b'OK')  # this is needed for the stream to work with REQ/REP pattern
            # image_height, image_width = frame.shape[:2]

            if frame is None:
                break

            num_frames += 1

            '''
            if num_frames % 2 != 0:  # only process frames at set number of frame intervals
                continue
            '''

            image = Image.fromarray(frame[..., ::-1])  # convert bgr to rgb
            boxes, confidence, classes = yolo.detect_image(image)
            features = encoder(frame, boxes)

            detections = [Detection(bbox, confidence, cls, feature) for bbox, confidence, cls, feature in
                          zip(boxes, confidence, classes, features)]

            # Run non-maxima suppression.
            boxes = np.array([d.tlwh for d in detections])
            scores = np.array([d.confidence for d in detections])
            classes = np.array([d.cls for d in detections])
            indices = preprocessing.non_max_suppression(boxes, nms_max_overlap, scores)
            detections = [detections[i] for i in indices]

            # Call the tracker
            tracker.predict()
            tracker.update(detections)

            if cam_id == 'Camera 1':
                line = [(0, int(0.5 * frame.shape[0])), (int(frame.shape[1]), int(0.5 * frame.shape[0]))]
            else:
                line = [(0, int(0.33 * frame.shape[0])), (int(frame.shape[1]), int(0.33 * frame.shape[0]))]

            # draw yellow line
            cv2.line(frame, line[0], line[1], (0, 255, 255), 2)

            for track in tracker.tracks:
                if not track.is_confirmed() or track.time_since_update > 1:
                    continue
                bbox = track.to_tlbr()
                track_cls = track.cls  # most common detection class for track

                midpoint = track.tlbr_midpoint(bbox)
                origin_midpoint = (midpoint[0], frame.shape[0] - midpoint[1])  # get midpoint respective to botton-left

                if track.track_id not in memory:
                    memory[track.track_id] = deque(maxlen=2)

                memory[track.track_id].append(midpoint)
                previous_midpoint = memory[track.track_id][0]

                origin_previous_midpoint = (previous_midpoint[0], frame.shape[0] - previous_midpoint[1])

                cv2.line(frame, midpoint, previous_midpoint, (0, 255, 0), 2)

                # Add to counter and get intersection details
                if Camera.intersect(midpoint, previous_midpoint, line[0], line[1]) and track.track_id not in already_counted:
                    class_counter[track_cls] += 1
                    total_counter += 1

                    # draw red line
                    cv2.line(frame, line[0], line[1], (0, 0, 255), 2)

                    already_counted.append(track.track_id)  # Set already counted for ID to true.

                    intersection_time = datetime.datetime.now() - datetime.timedelta(microseconds=datetime.datetime.now().microsecond)
                    angle = Camera.vector_angle(origin_midpoint, origin_previous_midpoint)
                    intersect_info.append([track_cls, origin_midpoint, angle, intersection_time])

                    if angle > 0:
                        up_count += 1
                    if angle < 0:
                        down_count += 1

                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 255, 255), 2)  # WHITE BOX
                cv2.putText(frame, ""ID: "" + str(track.track_id), (int(bbox[0]), int(bbox[1])), 0,
                            1.5e-3 * frame.shape[0], (0, 255, 0), 2)

                if not show_detections:
                    adc = ""%.2f"" % (track.adc * 100) + ""%""  # Average detection confidence
                    cv2.putText(frame, str(track_cls), (int(bbox[0]), int(bbox[3])), 0,
                                1e-3 * frame.shape[0], (0, 255, 0), 2)
                    cv2.putText(frame, 'ADC: ' + adc, (int(bbox[0]), int(bbox[3] + 2e-2 * frame.shape[1])), 0,
                                1e-3 * frame.shape[0], (0, 255, 0), 2)

            # Delete memory of old tracks.
            # This needs to be larger than the number of tracked objects in the frame.
            if len(memory) > 50:
                del memory[list(memory)[0]]

            # Draw total count.
            cv2.putText(frame, ""Total: {} ({} up, {} down)"".format(str(total_counter), str(up_count),
                        str(down_count)), (int(0.05 * frame.shape[1]), int(0.1 * frame.shape[0])), 0,
                        1.5e-3 * frame.shape[0], (0, 255, 255), 2)

            if show_detections:
                for det in detections:
                    bbox = det.to_tlbr()
                    score = ""%.2f"" % (det.confidence * 100) + ""%""
                    cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)  # BLUE BOX
                    if len(classes) > 0:
                        det_cls = det.cls
                        cv2.putText(frame, str(det_cls) + "" "" + score, (int(bbox[0]), int(bbox[3])), 0,
                                    1.5e-3 * frame.shape[0], (0, 255, 0), 2)

            # display counts for each class as they appear
            y = 0.2 * frame.shape[0]
            for cls in class_counter:
                class_count = class_counter[cls]
                cv2.putText(frame, str(cls) + "" "" + str(class_count), (int(0.05 * frame.shape[1]), int(y)), 0,
                            1.5e-3 * frame.shape[0], (0, 255, 255), 2)
                y += 0.05 * frame.shape[0]

            # calculate current minute
            now = datetime.datetime.now()
            rounded_now = now - datetime.timedelta(microseconds=now.microsecond)  # round to nearest second
            current_minute = now.time().minute

            if current_minute == 0 and len(count_dict) > 1:
                count_dict = {}  # reset counts every hour
            else:
                # write counts to file for every set interval of the hour
                write_interval = 5
                if current_minute % write_interval == 0:  # write to file once only every write_interval minutes
                    if current_minute not in count_dict:
                        count_dict[current_minute] = True
                        total_filename = 'Total counts for {}, {}.txt'.format(current_date, cam_id)
                        counts_folder = './counts/'
                        if not os.access(counts_folder + str(current_date) + '/total', os.W_OK):
                            os.makedirs(counts_folder + str(current_date) + '/total')
                        total_count_file = open(counts_folder + str(current_date) + '/total/' + total_filename, 'a')
                        print('{} writing...'.format(rounded_now))
                        print('Writing current total count ({}) and directional counts to file.'.format(total_counter))
                        total_count_file.write('{}, {}, {}, {}, {}\n'.format(str(rounded_now), device,
                                                                             str(total_counter), up_count, down_count))
                        total_count_file.close()

                        # if class exists in class counter, create file and write counts

                        if not os.access(counts_folder + str(current_date) + '/classes', os.W_OK):
                            os.makedirs(counts_folder + str(current_date) + '/classes')
                        for cls in class_counter:
                            class_count = class_counter[cls]
                            print('Writing current {} count ({}) to file.'.format(cls, class_count))
                            class_filename = 'Class counts for {}, {}.txt'.format(current_date, cam_id)
                            class_count_file = open(counts_folder + str(current_date) + '/classes/' + class_filename, 'a')
                            class_count_file.write(""{}, {}, {}\n"".format(rounded_now, device, str(class_count)))
                            class_count_file.close()

                        # write intersection details
                        if not os.access(counts_folder + str(current_date) + '/intersections', os.W_OK):
                            os.makedirs(counts_folder + str(current_date) + '/intersections')
                        print('Writing intersection details for {}'.format(cam_id))
                        intersection_filename = 'Intersection details for {}, {}.txt'.format(current_date, cam_id)
                        intersection_file = open(counts_folder + str(current_date) + '/intersections/' + intersection_filename, 'a')
                        for i in intersect_info:
                            cls = i[0]

                            midpoint = i[1]
                            x = midpoint[0]
                            y = midpoint[1]

                            angle = i[2]

                            intersect_time = i[3]

                            intersection_file.write(""{}, {}, {}, {}, {}, {}\n"".format(str(intersect_time), device, cls,
                                                                                      x, y, str(angle)))
                        intersection_file.close()
                        intersect_info = []  # reset list after writing

            yield cam_id, frame","cv2.line(frame, line[0], line[1], (0, 0, 255), 2)","cv2.line(frame, *line[:2], (0, 0, 255), 2)","iterable_zj[0], iterable_zj[1]",*line[:2],*line[:2],1
SMARTS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/ultra/ultra/scenarios/generate_scenarios.py,https://github.com/huawei-noah/SMARTS/tree/master/ultra/ultra/scenarios/generate_scenarios.py,,generate_social_vehicles$498,"def generate_social_vehicles(
    route_distribution,
    start_end_on_different_lanes_probability,
    num_vehicles,
    route_direction,
    route_lanes,
    route_has_turn,
    stopwatcher_info,
    traffic_params,
    stops,
    pos_offsets,
    begin_time_init=None,
    deadlock_optimization=True,
):
    flows = []
    behaviors = []
    log_info = {
        ""num_vehicles"": 0,
        ""route_distribution"": None,
        ""start_end_on_different_lanes_probability"": 0.0,
    }
    stopwatcher_added = False
    # populate random routes based on their probability
    start_lane, end_lane = route_direction
    if stopwatcher_info and route_direction == stopwatcher_info[""direction""]:
        if stopwatcher_info[""behavior""] not in behaviors:
            behaviors.append(f'stopwatcher_{stopwatcher_info[""behavior""]}')
            num_vehicles = max(0, num_vehicles - 1)  # used 1 for stopwatcher
            stopwatcher_added = True

    behaviors.extend(
        random.choices(
            list(route_distribution.keys()),
            weights=list(route_distribution.values()),
            k=num_vehicles,
        )
    )

    random.shuffle(behaviors)  # because stopwatcher is always at the beginning

    if begin_time_init is None:
        begin_time_init_func = basic_begin_time_init_func
        begin_time_init_params = {""probability"": 0.0}
    else:
        begin_time_init_func = begin_time_init[""func""]
        begin_time_init_params = begin_time_init[""params""]

    begin_time_init_params = (
        {} if begin_time_init_params is None else begin_time_init_params
    )
    begin_times = begin_time_init_func(
        route_lanes[start_lane],
        len(behaviors),
        traffic_params,
        **begin_time_init_params,
    )
    begin_time_idx = [0 for _ in range(route_lanes[start_lane])]

    for behavior_idx in behaviors:
        if behavior_idx not in log_info:
            log_info[behavior_idx] = {""count"": 0, ""start_end_different_lanes"": 0}
        if deadlock_optimization and route_has_turn:
            # if route has a turn, start on the left-most lane
            start_lane_id = route_lanes[start_lane] - 1
            lane_changing_options = [
                lane_id for lane_id in range(route_lanes[end_lane])
            ]
            if (
                len(lane_changing_options) > 0
                or random.uniform(0, 1) < start_end_on_different_lanes_probability
            ):
                end_lane_id = random.choice(lane_changing_options)
            else:
                end_lane_id = start_lane_id

        else:
            start_lane_id = random.randint(0, route_lanes[start_lane] - 1)
            end_lane_id = random.randint(0, route_lanes[end_lane] - 1)

        # set begin/end time
        begin_time = begin_times[start_lane_id][begin_time_idx[start_lane_id]]
        begin_time_idx[start_lane_id] += 1
        end_time = begin_time + 3600  # 1 hour

        if ""stopwatcher"" in behavior_idx:
            start_lane_id = route_lanes[stopwatcher_info[""direction""][0]] - 1
            end_lane_id = route_lanes[stopwatcher_info[""direction""][1]] - 1
            # To ensure that the stopwatcher spawns in all scenarios the
            # stopwatcher's begin time is bounded between 10s to 50s
            # (100ts to 500ts, if 1s = 1 ts). During analysis, the
            # stopwatcher is guaranteed to spawn before the 500ts
            # and no less then 100ts
            begin_time = random.randint(10, 200)
            flows.append(
                generate_stopwatcher(
                    stopwatcher_behavior=stopwatcher_info[""behavior""],
                    stopwatcher_route=stopwatcher_info[""direction""],
                    begin_time=begin_time,
                    start_lane_id=start_lane_id,
                    end_lane_id=end_lane_id,
                )
            )
        else:
            behavior = get_social_vehicle_behavior(behavior_idx)
            if pos_offsets:
                start_offset = random.randint(
                    pos_offsets[""start""][0], pos_offsets[""start""][1]
                )
                end_offset = random.randint(
                    pos_offsets[""end""][0], pos_offsets[""end""][1]
                )
            else:
                start_offset = ""base""
                end_offset = ""max""
            flows.append(
                Flow(
                    begin=begin_time,
                    end=end_time,
                    route=Route(
                        begin=(f""edge-{start_lane}"", start_lane_id, start_offset),
                        end=(f""edge-{end_lane}"", end_lane_id, end_offset),
                    ),
                    rate=1,
                    actors={behavior: 1.0},
                )
            )
        log_info[behavior_idx][""count""] += 1
        log_info[""route_distribution""] = route_distribution
        log_info[""num_vehicles""] = (
            num_vehicles + 1 if stopwatcher_added else num_vehicles
        )
        log_info[
            ""start_end_on_different_lanes_probability""
        ] = start_end_on_different_lanes_probability
        log_info[behavior_idx][""start_end_different_lanes""] += (
            1 if start_lane_id != end_lane_id else 0
        )

    return flows, log_info","random.randint(pos_offsets['start'][0], pos_offsets['start'][1])",random.randint(*pos_offsets['start'][:2]),"iterable_zj[0], iterable_zj[1]",*pos_offsets['start'][:2],*pos_offsets['start'][:2],1
SMARTS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/ultra/ultra/scenarios/generate_scenarios.py,https://github.com/huawei-noah/SMARTS/tree/master/ultra/ultra/scenarios/generate_scenarios.py,,generate_social_vehicles$498,"def generate_social_vehicles(
    route_distribution,
    start_end_on_different_lanes_probability,
    num_vehicles,
    route_direction,
    route_lanes,
    route_has_turn,
    stopwatcher_info,
    traffic_params,
    stops,
    pos_offsets,
    begin_time_init=None,
    deadlock_optimization=True,
):
    flows = []
    behaviors = []
    log_info = {
        ""num_vehicles"": 0,
        ""route_distribution"": None,
        ""start_end_on_different_lanes_probability"": 0.0,
    }
    stopwatcher_added = False
    # populate random routes based on their probability
    start_lane, end_lane = route_direction
    if stopwatcher_info and route_direction == stopwatcher_info[""direction""]:
        if stopwatcher_info[""behavior""] not in behaviors:
            behaviors.append(f'stopwatcher_{stopwatcher_info[""behavior""]}')
            num_vehicles = max(0, num_vehicles - 1)  # used 1 for stopwatcher
            stopwatcher_added = True

    behaviors.extend(
        random.choices(
            list(route_distribution.keys()),
            weights=list(route_distribution.values()),
            k=num_vehicles,
        )
    )

    random.shuffle(behaviors)  # because stopwatcher is always at the beginning

    if begin_time_init is None:
        begin_time_init_func = basic_begin_time_init_func
        begin_time_init_params = {""probability"": 0.0}
    else:
        begin_time_init_func = begin_time_init[""func""]
        begin_time_init_params = begin_time_init[""params""]

    begin_time_init_params = (
        {} if begin_time_init_params is None else begin_time_init_params
    )
    begin_times = begin_time_init_func(
        route_lanes[start_lane],
        len(behaviors),
        traffic_params,
        **begin_time_init_params,
    )
    begin_time_idx = [0 for _ in range(route_lanes[start_lane])]

    for behavior_idx in behaviors:
        if behavior_idx not in log_info:
            log_info[behavior_idx] = {""count"": 0, ""start_end_different_lanes"": 0}
        if deadlock_optimization and route_has_turn:
            # if route has a turn, start on the left-most lane
            start_lane_id = route_lanes[start_lane] - 1
            lane_changing_options = [
                lane_id for lane_id in range(route_lanes[end_lane])
            ]
            if (
                len(lane_changing_options) > 0
                or random.uniform(0, 1) < start_end_on_different_lanes_probability
            ):
                end_lane_id = random.choice(lane_changing_options)
            else:
                end_lane_id = start_lane_id

        else:
            start_lane_id = random.randint(0, route_lanes[start_lane] - 1)
            end_lane_id = random.randint(0, route_lanes[end_lane] - 1)

        # set begin/end time
        begin_time = begin_times[start_lane_id][begin_time_idx[start_lane_id]]
        begin_time_idx[start_lane_id] += 1
        end_time = begin_time + 3600  # 1 hour

        if ""stopwatcher"" in behavior_idx:
            start_lane_id = route_lanes[stopwatcher_info[""direction""][0]] - 1
            end_lane_id = route_lanes[stopwatcher_info[""direction""][1]] - 1
            # To ensure that the stopwatcher spawns in all scenarios the
            # stopwatcher's begin time is bounded between 10s to 50s
            # (100ts to 500ts, if 1s = 1 ts). During analysis, the
            # stopwatcher is guaranteed to spawn before the 500ts
            # and no less then 100ts
            begin_time = random.randint(10, 200)
            flows.append(
                generate_stopwatcher(
                    stopwatcher_behavior=stopwatcher_info[""behavior""],
                    stopwatcher_route=stopwatcher_info[""direction""],
                    begin_time=begin_time,
                    start_lane_id=start_lane_id,
                    end_lane_id=end_lane_id,
                )
            )
        else:
            behavior = get_social_vehicle_behavior(behavior_idx)
            if pos_offsets:
                start_offset = random.randint(
                    pos_offsets[""start""][0], pos_offsets[""start""][1]
                )
                end_offset = random.randint(
                    pos_offsets[""end""][0], pos_offsets[""end""][1]
                )
            else:
                start_offset = ""base""
                end_offset = ""max""
            flows.append(
                Flow(
                    begin=begin_time,
                    end=end_time,
                    route=Route(
                        begin=(f""edge-{start_lane}"", start_lane_id, start_offset),
                        end=(f""edge-{end_lane}"", end_lane_id, end_offset),
                    ),
                    rate=1,
                    actors={behavior: 1.0},
                )
            )
        log_info[behavior_idx][""count""] += 1
        log_info[""route_distribution""] = route_distribution
        log_info[""num_vehicles""] = (
            num_vehicles + 1 if stopwatcher_added else num_vehicles
        )
        log_info[
            ""start_end_on_different_lanes_probability""
        ] = start_end_on_different_lanes_probability
        log_info[behavior_idx][""start_end_different_lanes""] += (
            1 if start_lane_id != end_lane_id else 0
        )

    return flows, log_info","random.randint(pos_offsets['end'][0], pos_offsets['end'][1])",random.randint(*pos_offsets['end'][:2]),"iterable_zj[0], iterable_zj[1]",*pos_offsets['end'][:2],*pos_offsets['end'][:2],1
vega,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/hpo/ea/ga.py,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/hpo/ea/ga.py,GeneticAlgorithm,evolute$79,"def evolute(self):
        """"""Evolution.""""""
        inds = self.selection()
        if not inds:
            return None
        ind = self.crossover(inds[0], inds[1], self.prob_crossover)
        ind = self.mutatation(ind, self.prob_mutatation)
        ind = self.search_space.verify_constraints(ind)
        return ind","self.crossover(inds[0], inds[1], self.prob_crossover)","self.crossover(*inds[:2], self.prob_crossover)","iterable_zj[0], iterable_zj[1]",*inds[:2],*inds[:2],1
HRNet-Facial-Landmark-Detection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HRNet-Facial-Landmark-Detection/lib/datasets/cofw.py,https://github.com/HRNet/HRNet-Facial-Landmark-Detection/tree/master/lib/datasets/cofw.py,COFW,__getitem__$53,"def __getitem__(self, idx):

        img = self.images[idx][0]

        if len(img.shape) == 2:
            img = img.reshape(img.shape[0], img.shape[1], 1)
            img = np.repeat(img, 3, axis=2)

        pts = self.pts[idx][0:58].reshape(2, -1).transpose()

        xmin = np.min(pts[:, 0])
        xmax = np.max(pts[:, 0])
        ymin = np.min(pts[:, 1])
        ymax = np.max(pts[:, 1])

        center_w = (math.floor(xmin) + math.ceil(xmax)) / 2.0
        center_h = (math.floor(ymin) + math.ceil(ymax)) / 2.0

        scale = max(math.ceil(xmax) - math.floor(xmin), math.ceil(ymax) - math.floor(ymin)) / 200.0
        center = torch.Tensor([center_w, center_h])

        scale *= 1.25
        nparts = pts.shape[0]

        r = 0
        if self.is_train:
            scale = scale * (random.uniform(1 - self.scale_factor,
                                            1 + self.scale_factor))
            r = random.uniform(-self.rot_factor, self.rot_factor) \
                if random.random() <= 0.6 else 0

            if random.random() <= 0.5 and self.flip:
                img = np.fliplr(img)
                pts = fliplr_joints(pts, width=img.shape[1], dataset='COFW')
                center[0] = img.shape[1] - center[0]

        img = crop(img, center, scale, self.input_size, rot=r)

        target = np.zeros((nparts, self.output_size[0], self.output_size[1]))
        tpts = pts.copy()

        for i in range(nparts):
            if tpts[i, 1] > 0:
                tpts[i, 0:2] = transform_pixel(tpts[i, 0:2]+1, center,
                                               scale, self.output_size, rot=r)
                target[i] = generate_target(target[i], tpts[i]-1, self.sigma,
                                            label_type=self.label_type)
        img = img.astype(np.float32)
        img = (img/255 - self.mean) / self.std
        img = img.transpose([2, 0, 1])
        target = torch.Tensor(target)
        tpts = torch.Tensor(tpts)
        center = torch.Tensor(center)

        meta = {'index': idx, 'center': center, 'scale': scale,
                'pts': torch.Tensor(pts), 'tpts': tpts}

        return img, target, meta","img.reshape(img.shape[0], img.shape[1], 1)","img.reshape(*img.shape[:2], 1)","iterable_zj[0], iterable_zj[1]",*img.shape[:2],*img.shape[:2],1
CenterMask,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CenterMask/maskrcnn_benchmark/layers/dcn/deform_conv_func.py,https://github.com/youngwanLEE/CenterMask/tree/master/maskrcnn_benchmark/layers/dcn/deform_conv_func.py,ModulatedDeformConvFunction,backward$204,"def backward(ctx, grad_output):
        if not grad_output.is_cuda:
            raise NotImplementedError
        input, offset, mask, weight, bias = ctx.saved_tensors
        grad_input = torch.zeros_like(input)
        grad_offset = torch.zeros_like(offset)
        grad_mask = torch.zeros_like(mask)
        grad_weight = torch.zeros_like(weight)
        grad_bias = torch.zeros_like(bias)
        _C.modulated_deform_conv_backward(
            input, 
            weight, 
            bias, 
            ctx._bufs[0], 
            offset, 
            mask, 
            ctx._bufs[1],
            grad_input, 
            grad_weight, 
            grad_bias, 
            grad_offset, 
            grad_mask,
            grad_output, 
            weight.shape[2], 
            weight.shape[3], 
            ctx.stride,
            ctx.stride, 
            ctx.padding, 
            ctx.padding, 
            ctx.dilation, 
            ctx.dilation,
            ctx.groups, 
            ctx.deformable_groups, 
            ctx.with_bias
        )
        if not ctx.with_bias:
            grad_bias = None

        return (grad_input, grad_offset, grad_mask, grad_weight, grad_bias,
                None, None, None, None, None)","_C.modulated_deform_conv_backward(input, weight, bias, ctx._bufs[0], offset, mask, ctx._bufs[1], grad_input, grad_weight, grad_bias, grad_offset, grad_mask, grad_output, weight.shape[2], weight.shape[3], ctx.stride, ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation, ctx.groups, ctx.deformable_groups, ctx.with_bias)","_C.modulated_deform_conv_backward(input, weight, bias, ctx._bufs[0], offset, mask, ctx._bufs[1], grad_input, grad_weight, grad_bias, grad_offset, grad_mask, grad_output, *weight.shape[2:4], ctx.stride, ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation, ctx.groups, ctx.deformable_groups, ctx.with_bias)","iterable_zj[2], iterable_zj[3]",*weight.shape[2:4],*weight.shape[2:4],1
colour,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/colour/colour/recovery/otsu2018.py,https://github.com/colour-science/colour/tree/master/colour/recovery/otsu2018.py,Tree_Otsu2018,optimise$1167,"def optimise(self,
                 iterations=8,
                 minimum_cluster_size=None,
                 print_callable=print):
        """"""
        Optimises the tree by repeatedly performing optimal partitioning of the
        nodes, creating a tree that minimises the total reconstruction error.

        Parameters
        ----------
        iterations : int, optional
            Maximum number of splits. If the dataset is too small, this number
            might not be reached. The default is to create 8 clusters, like in
            :cite:`Otsu2018`.
        minimum_cluster_size : int, optional
            Smallest acceptable cluster size. By default, it is chosen
            automatically, based on the size of the dataset and desired number
            of clusters. It must be at least 3 or the
            *Principal Component Analysis* (PCA) is not be possible.
        print_callable : callable, optional
            Callable used to print progress and diagnostic information.

        Examples
        --------
        >>> from colour.colorimetry import sds_and_msds_to_msds
        >>> from colour import MSDS_CMFS, SDS_COLOURCHECKERS, SDS_ILLUMINANTS
        >>> cmfs = (
        ...     MSDS_CMFS['CIE 1931 2 Degree Standard Observer']
        ...     .copy().align(SpectralShape(360, 780, 10))
        ... )
        >>> illuminant = SDS_ILLUMINANTS['D65'].copy().align(cmfs.shape)
        >>> reflectances = sds_and_msds_to_msds(
        ...     SDS_COLOURCHECKERS['ColorChecker N Ohta'].values()
        ... )
        >>> node_tree = Tree_Otsu2018(reflectances, cmfs, illuminant)
        >>> node_tree.optimise(iterations=2)  # doctest: +ELLIPSIS
        ======================================================================\
=========
        *                                                                     \
        *
        *   ""Otsu et al. (2018)"" Tree Optimisation                            \
        *
        *                                                                     \
        *
        ======================================================================\
=========
        Initial branch error is: 4.8705353...
        <BLANKLINE>
        Iteration 1 of 2:
        <BLANKLINE>
        Optimising ""Tree_Otsu2018#...(Data_Otsu2018(24 Reflectances))""...
        <BLANKLINE>
        Splitting ""Tree_Otsu2018#...(Data_Otsu2018(24 Reflectances))"" into \
""Node_Otsu2018#...(Data_Otsu2018(10 Reflectances))"" and \
""Node_Otsu2018#...(Data_Otsu2018(14 Reflectances))"" along \
""PartitionAxis(horizontal partition at y = 0.3240945...)"".
        Error is reduced by 0.0054840... and is now 4.8650513..., 99.9% of \
the initial error.
        <BLANKLINE>
        Iteration 2 of 2:
        <BLANKLINE>
        Optimising ""Node_Otsu2018#...(Data_Otsu2018(10 Reflectances))""...
        Optimisation failed: Could not find the best partition!
        Optimising ""Node_Otsu2018#...(Data_Otsu2018(14 Reflectances))""...
        <BLANKLINE>
        Splitting ""Node_Otsu2018#...(Data_Otsu2018(14 Reflectances))"" into \
""Node_Otsu2018#...(Data_Otsu2018(7 Reflectances))"" and \
""Node_Otsu2018#...(Data_Otsu2018(7 Reflectances))"" along \
""PartitionAxis(horizontal partition at y = 0.3600663...)"".
        Error is reduced by 0.9681059... and is now 3.8969453..., 80.0% of \
the initial error.
        Tree optimisation is complete!
        >>> print(node_tree.render())  # doctest: +ELLIPSIS
        |----""Tree_Otsu2018#...""
            |----""Node_Otsu2018#...""
            |----""Node_Otsu2018#...""
                |----""Node_Otsu2018#...""
                |----""Node_Otsu2018#...""
        <BLANKLINE>
        >>> len(node_tree)
        4
        """"""

        default_cluster_size = len(self.data) / iterations // 2
        minimum_cluster_size = max(
            (minimum_cluster_size
             if minimum_cluster_size is not None else default_cluster_size), 3)

        initial_branch_error = self.branch_reconstruction_error()

        message_box(
            '""Otsu et al. (2018)"" Tree Optimisation',
            print_callable=print_callable)

        print_callable(
            'Initial branch error is: {0}'.format(initial_branch_error))

        best_leaf, best_partition, best_axis, partition_error = [None] * 4

        for i in range(iterations):
            print_callable('\nIteration {0} of {1}:\n'.format(
                i + 1, iterations))

            total_error = self.branch_reconstruction_error()
            optimised_total_error = None

            for leaf in self.leaves:
                print_callable('Optimising ""{0}""...'.format(leaf))

                try:
                    partition, axis, partition_error = leaf.minimise(
                        minimum_cluster_size)
                except RuntimeError as error:
                    print_callable('Optimisation failed: {0}'.format(error))
                    continue

                new_total_error = (
                    total_error - leaf.leaf_reconstruction_error() +
                    partition_error)

                if (optimised_total_error is None or
                        new_total_error < optimised_total_error):
                    optimised_total_error = new_total_error
                    best_axis = axis
                    best_leaf = leaf
                    best_partition = partition

            if optimised_total_error is None:
                print_callable('\nNo further improvement is possible!\n'
                               'Terminating at iteration {0}.\n'.format(i))
                break

            print_callable(
                '\nSplitting ""{0}"" into ""{1}"" and ""{2}"" along ""{3}"".'.format(
                    best_leaf, best_partition[0], best_partition[1],
                    best_axis))

            print_callable(
                'Error is reduced by {0} and is now {1}, '
                '{2:.1f}% of the initial error.'.format(
                    leaf.leaf_reconstruction_error() - partition_error,
                    optimised_total_error,
                    100 * optimised_total_error / initial_branch_error))

            best_leaf.split(best_partition, best_axis)

        print_callable('Tree optimisation is complete!')","'\nSplitting ""{0}"" into ""{1}"" and ""{2}"" along ""{3}"".'.format(best_leaf, best_partition[0], best_partition[1], best_axis)","'\nSplitting ""{0}"" into ""{1}"" and ""{2}"" along ""{3}"".'.format(best_leaf, *best_partition[:2], best_axis)","iterable_zj[0], iterable_zj[1]",*best_partition[:2],*best_partition[:2],1
Tuxemon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Tuxemon/tuxemon/event/actions/random_encounter.py,https://github.com/Tuxemon/Tuxemon/tree/master/tuxemon/event/actions/random_encounter.py,,_create_monster_npc$151,"def _create_monster_npc(
    encounter: JSONEncounterItem,
    world: WorldState,
) -> NPC:
    current_monster = monster.Monster()
    current_monster.load_from_db(encounter[""monster""])
    # Set the monster's level based on the specified level range
    if len(encounter[""level_range""]) > 1:
        level = random.randrange(
            encounter[""level_range""][0],
            encounter[""level_range""][1],
        )
    else:
        level = encounter[""level_range""][0]
    # Set the monster's level
    current_monster.level = 1
    current_monster.set_level(level)
    current_monster.current_hp = current_monster.hp

    # Create an NPC object which will be this monster's ""trainer""
    npc = NPC(""maple_girl"", world=world)
    npc.add_monster(current_monster)
    # NOTE: random battles are implemented as trainer battles.
    #       this is a hack. remove this once trainer/random battlers are fixed
    current_monster.owner = None
    npc.party_limit = 0

    # Set the NPC object's AI model.
    npc.ai = ai.RandomAI()
    return npc","random.randrange(encounter['level_range'][0], encounter['level_range'][1])",random.randrange(*encounter['level_range'][:2]),"iterable_zj[0], iterable_zj[1]",*encounter['level_range'][:2],*encounter['level_range'][:2],1
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/storage/tests/hybrid_2020_09_01/test_storage_batch_operations.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/storage/tests/hybrid_2020_09_01/test_storage_batch_operations.py,StorageBatchOperationScenarios,test_storage_blob_batch_copy$200,"def test_storage_blob_batch_copy(self, src_account_info, dst_account_info, test_dir):
        from datetime import datetime, timedelta
        expiry = (datetime.utcnow() + timedelta(hours=1)).strftime('%Y-%m-%dT%H:%MZ')

        src_container = self.create_container(src_account_info)
        self.storage_cmd('storage blob upload-batch -s ""{}"" -d {}', src_account_info, test_dir, src_container)

        src_share = self.create_share(src_account_info)
        self.storage_cmd('storage file upload-batch -s ""{}"" -d {}', src_account_info, test_dir, src_share)

        # from blob container to container with a sas in same account
        dst_container = self.create_container(src_account_info)

        self.storage_cmd('storage blob copy start-batch --source-container {} '
                         '--destination-container {}', src_account_info, src_container, dst_container)
        self.storage_cmd('storage blob list -c {}',
                         src_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 41))

        # from blob container to container with a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        sas_token = self.storage_cmd('storage container generate-sas -n {} --permissions rl '
                                     '--expiry {}', src_account_info, src_container, expiry).output
        self.storage_cmd('storage blob copy start-batch --source-container {} '
                         '--destination-container {} --source-sas {} --pattern apple/* '
                         '--source-account-name {}', dst_account_info, src_container, dst_container, sas_token,
                         src_account_info[0])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 10))

        # from blob container to container without a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        self.storage_cmd('storage blob copy start-batch --source-container {} '
                         '--destination-container {} --source-account-name {} --source-account-key {}'
                         ' --pattern */file_0', dst_account_info, src_container, dst_container, src_account_info[0],
                         src_account_info[1])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 4))

        # from file share to blob container with a sas in same account
        dst_container = self.create_container(src_account_info)

        self.storage_cmd('storage blob copy start-batch --source-share {} --destination-container {}',
                         src_account_info, src_share, dst_container)
        self.storage_cmd('storage blob list -c {}',
                         src_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 41))

        # from file share to blob container with a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        sas_token = self.storage_cmd('storage share generate-sas -n {} --permissions rl --expiry {} -otsv',
                                     src_account_info, src_share, expiry).output.strip()
        self.storage_cmd('storage blob copy start-batch --source-share {} '
                         '--destination-container {} --source-sas {} --pattern apple/* '
                         '--source-account-name {}', dst_account_info, src_share, dst_container, sas_token,
                         src_account_info[0])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 10))

        # from file share to blob container without a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        self.storage_cmd('storage blob copy start-batch --source-share {} '
                         '--destination-container {} --source-account-name {} --source-account-key {}'
                         ' --pattern */file_0', dst_account_info, src_share, dst_container, src_account_info[0],
                         src_account_info[1])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 4))

        # from file share to blob container while specifying destination path
        dst_container = self.create_container(dst_account_info)
        self.storage_cmd('storage blob copy start-batch --source-share {} '
                         '--destination-container {} --source-account-name {} --source-account-key {}'
                         ' --pattern */file_0 --destination-path some_dir', dst_account_info, src_share, dst_container,
                         src_account_info[0], src_account_info[1])
        self.storage_cmd('storage blob list -c {} --prefix some_dir',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 4))","self.storage_cmd('storage blob copy start-batch --source-container {} --destination-container {} --source-account-name {} --source-account-key {} --pattern */file_0', dst_account_info, src_container, dst_container, src_account_info[0], src_account_info[1])","self.storage_cmd('storage blob copy start-batch --source-container {} --destination-container {} --source-account-name {} --source-account-key {} --pattern */file_0', dst_account_info, src_container, dst_container, *src_account_info[:2])","iterable_zj[0], iterable_zj[1]",*src_account_info[:2],*src_account_info[:2],1
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/storage/tests/hybrid_2020_09_01/test_storage_batch_operations.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/storage/tests/hybrid_2020_09_01/test_storage_batch_operations.py,StorageBatchOperationScenarios,test_storage_blob_batch_copy$200,"def test_storage_blob_batch_copy(self, src_account_info, dst_account_info, test_dir):
        from datetime import datetime, timedelta
        expiry = (datetime.utcnow() + timedelta(hours=1)).strftime('%Y-%m-%dT%H:%MZ')

        src_container = self.create_container(src_account_info)
        self.storage_cmd('storage blob upload-batch -s ""{}"" -d {}', src_account_info, test_dir, src_container)

        src_share = self.create_share(src_account_info)
        self.storage_cmd('storage file upload-batch -s ""{}"" -d {}', src_account_info, test_dir, src_share)

        # from blob container to container with a sas in same account
        dst_container = self.create_container(src_account_info)

        self.storage_cmd('storage blob copy start-batch --source-container {} '
                         '--destination-container {}', src_account_info, src_container, dst_container)
        self.storage_cmd('storage blob list -c {}',
                         src_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 41))

        # from blob container to container with a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        sas_token = self.storage_cmd('storage container generate-sas -n {} --permissions rl '
                                     '--expiry {}', src_account_info, src_container, expiry).output
        self.storage_cmd('storage blob copy start-batch --source-container {} '
                         '--destination-container {} --source-sas {} --pattern apple/* '
                         '--source-account-name {}', dst_account_info, src_container, dst_container, sas_token,
                         src_account_info[0])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 10))

        # from blob container to container without a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        self.storage_cmd('storage blob copy start-batch --source-container {} '
                         '--destination-container {} --source-account-name {} --source-account-key {}'
                         ' --pattern */file_0', dst_account_info, src_container, dst_container, src_account_info[0],
                         src_account_info[1])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 4))

        # from file share to blob container with a sas in same account
        dst_container = self.create_container(src_account_info)

        self.storage_cmd('storage blob copy start-batch --source-share {} --destination-container {}',
                         src_account_info, src_share, dst_container)
        self.storage_cmd('storage blob list -c {}',
                         src_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 41))

        # from file share to blob container with a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        sas_token = self.storage_cmd('storage share generate-sas -n {} --permissions rl --expiry {} -otsv',
                                     src_account_info, src_share, expiry).output.strip()
        self.storage_cmd('storage blob copy start-batch --source-share {} '
                         '--destination-container {} --source-sas {} --pattern apple/* '
                         '--source-account-name {}', dst_account_info, src_share, dst_container, sas_token,
                         src_account_info[0])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 10))

        # from file share to blob container without a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        self.storage_cmd('storage blob copy start-batch --source-share {} '
                         '--destination-container {} --source-account-name {} --source-account-key {}'
                         ' --pattern */file_0', dst_account_info, src_share, dst_container, src_account_info[0],
                         src_account_info[1])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 4))

        # from file share to blob container while specifying destination path
        dst_container = self.create_container(dst_account_info)
        self.storage_cmd('storage blob copy start-batch --source-share {} '
                         '--destination-container {} --source-account-name {} --source-account-key {}'
                         ' --pattern */file_0 --destination-path some_dir', dst_account_info, src_share, dst_container,
                         src_account_info[0], src_account_info[1])
        self.storage_cmd('storage blob list -c {} --prefix some_dir',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 4))","self.storage_cmd('storage blob copy start-batch --source-share {} --destination-container {} --source-account-name {} --source-account-key {} --pattern */file_0', dst_account_info, src_share, dst_container, src_account_info[0], src_account_info[1])","self.storage_cmd('storage blob copy start-batch --source-share {} --destination-container {} --source-account-name {} --source-account-key {} --pattern */file_0', dst_account_info, src_share, dst_container, *src_account_info[:2])","iterable_zj[0], iterable_zj[1]",*src_account_info[:2],*src_account_info[:2],1
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/storage/tests/hybrid_2020_09_01/test_storage_batch_operations.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/storage/tests/hybrid_2020_09_01/test_storage_batch_operations.py,StorageBatchOperationScenarios,test_storage_blob_batch_copy$200,"def test_storage_blob_batch_copy(self, src_account_info, dst_account_info, test_dir):
        from datetime import datetime, timedelta
        expiry = (datetime.utcnow() + timedelta(hours=1)).strftime('%Y-%m-%dT%H:%MZ')

        src_container = self.create_container(src_account_info)
        self.storage_cmd('storage blob upload-batch -s ""{}"" -d {}', src_account_info, test_dir, src_container)

        src_share = self.create_share(src_account_info)
        self.storage_cmd('storage file upload-batch -s ""{}"" -d {}', src_account_info, test_dir, src_share)

        # from blob container to container with a sas in same account
        dst_container = self.create_container(src_account_info)

        self.storage_cmd('storage blob copy start-batch --source-container {} '
                         '--destination-container {}', src_account_info, src_container, dst_container)
        self.storage_cmd('storage blob list -c {}',
                         src_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 41))

        # from blob container to container with a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        sas_token = self.storage_cmd('storage container generate-sas -n {} --permissions rl '
                                     '--expiry {}', src_account_info, src_container, expiry).output
        self.storage_cmd('storage blob copy start-batch --source-container {} '
                         '--destination-container {} --source-sas {} --pattern apple/* '
                         '--source-account-name {}', dst_account_info, src_container, dst_container, sas_token,
                         src_account_info[0])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 10))

        # from blob container to container without a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        self.storage_cmd('storage blob copy start-batch --source-container {} '
                         '--destination-container {} --source-account-name {} --source-account-key {}'
                         ' --pattern */file_0', dst_account_info, src_container, dst_container, src_account_info[0],
                         src_account_info[1])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 4))

        # from file share to blob container with a sas in same account
        dst_container = self.create_container(src_account_info)

        self.storage_cmd('storage blob copy start-batch --source-share {} --destination-container {}',
                         src_account_info, src_share, dst_container)
        self.storage_cmd('storage blob list -c {}',
                         src_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 41))

        # from file share to blob container with a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        sas_token = self.storage_cmd('storage share generate-sas -n {} --permissions rl --expiry {} -otsv',
                                     src_account_info, src_share, expiry).output.strip()
        self.storage_cmd('storage blob copy start-batch --source-share {} '
                         '--destination-container {} --source-sas {} --pattern apple/* '
                         '--source-account-name {}', dst_account_info, src_share, dst_container, sas_token,
                         src_account_info[0])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 10))

        # from file share to blob container without a sas between different accounts with pattern
        dst_container = self.create_container(dst_account_info)
        self.storage_cmd('storage blob copy start-batch --source-share {} '
                         '--destination-container {} --source-account-name {} --source-account-key {}'
                         ' --pattern */file_0', dst_account_info, src_share, dst_container, src_account_info[0],
                         src_account_info[1])
        self.storage_cmd('storage blob list -c {}',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 4))

        # from file share to blob container while specifying destination path
        dst_container = self.create_container(dst_account_info)
        self.storage_cmd('storage blob copy start-batch --source-share {} '
                         '--destination-container {} --source-account-name {} --source-account-key {}'
                         ' --pattern */file_0 --destination-path some_dir', dst_account_info, src_share, dst_container,
                         src_account_info[0], src_account_info[1])
        self.storage_cmd('storage blob list -c {} --prefix some_dir',
                         dst_account_info, dst_container).assert_with_checks(JMESPathCheck('length(@)', 4))","self.storage_cmd('storage blob copy start-batch --source-share {} --destination-container {} --source-account-name {} --source-account-key {} --pattern */file_0 --destination-path some_dir', dst_account_info, src_share, dst_container, src_account_info[0], src_account_info[1])","self.storage_cmd('storage blob copy start-batch --source-share {} --destination-container {} --source-account-name {} --source-account-key {} --pattern */file_0 --destination-path some_dir', dst_account_info, src_share, dst_container, *src_account_info[:2])","iterable_zj[0], iterable_zj[1]",*src_account_info[:2],*src_account_info[:2],1
aliyun-accesskey-Tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aliyun-accesskey-Tools/OSSTools.py,https://github.com/mrknow001/aliyun-accesskey-Tools/tree/master//OSSTools.py,,incident_key$431,"def incident_key(ui):
    ui.pushButton_2.clicked.connect(partial(function.commad_check_input, ui, uc, 0))
    ui.pushButton_3.clicked.connect(partial(show_message, ui, ''))
    ui.pushButton_5.clicked.connect(partial(function.get_InstanceId, ui))
    ui.pushButton_6.clicked.connect(partial(Echo_Command, ui, 1))

    class Queryhost_MyThread(QThread):
        my_signal = pyqtSignal(object, str)

        def __init__(self):
            super().__init__()

        def run(self):
            massage = function.server_check_input(ui)
            if massage is not None:
                self.my_signal.emit(massage[0], massage[1])

    Queryhost_thread = Queryhost_MyThread()
    Queryhost_thread.my_signal.connect(show_message)
    ui.pushButton.clicked.connect(partial(Queryhost_thread.start))","self.my_signal.emit(massage[0], massage[1])",self.my_signal.emit(*massage[:2]),"iterable_zj[0], iterable_zj[1]",*massage[:2],*massage[:2],1
gandissect,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gandissect/netdissect/segmenter.py,https://github.com/CSAILVision/gandissect/tree/master/netdissect/segmenter.py,UnifiedParsingSegmenter,raw_seg_prediction$138,"def raw_seg_prediction(self, tensor_images, downsample=1):
        '''
        Generates a segmentation by applying multiresolution voting on
        the segmentation model, using (rounded to 32 pixels) a set of
        resolutions in the example benchmark code.
        '''
        y, x = tensor_images.shape[2:]
        b = len(tensor_images)
        tensor_images = (tensor_images + 1) / 2 * 255
        tensor_images = torch.flip(tensor_images, (1,)) # BGR!!!?
        tensor_images -= torch.tensor([102.9801, 115.9465, 122.7717]).to(
                   dtype=tensor_images.dtype, device=tensor_images.device
                   )[None,:,None,None]
        seg_shape = (y // downsample, x // downsample)
        # We want these to be multiples of 32 for the model.
        sizes = [(s, s) for s in self.segsizes]
        pred = {category: torch.zeros(
            len(tensor_images), len(self.segmodel.labeldata[category]),
            seg_shape[0], seg_shape[1]).cuda()
            for category in ['object', 'material']}
        part_pred = {partobj_index: torch.zeros(
            len(tensor_images), len(partindex),
            seg_shape[0], seg_shape[1]).cuda()
            for partobj_index, partindex in enumerate(self.part_index)}
        for size in sizes:
            if size == tensor_images.shape[2:]:
                resized = tensor_images
            else:
                resized = torch.nn.AdaptiveAvgPool2d(size)(tensor_images)
            r_pred = self.segmodel(
                dict(img=resized), seg_size=seg_shape)
            for k in pred:
                pred[k] += r_pred[k]
            for k in part_pred:
                part_pred[k] += r_pred['part'][k]
        return pred, part_pred","torch.zeros(len(tensor_images), len(self.segmodel.labeldata[category]), seg_shape[0], seg_shape[1])","torch.zeros(len(tensor_images), len(self.segmodel.labeldata[category]), *seg_shape[:2])","iterable_zj[0], iterable_zj[1]",*seg_shape[:2],*seg_shape[:2],1
gandissect,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gandissect/netdissect/segmenter.py,https://github.com/CSAILVision/gandissect/tree/master/netdissect/segmenter.py,UnifiedParsingSegmenter,raw_seg_prediction$138,"def raw_seg_prediction(self, tensor_images, downsample=1):
        '''
        Generates a segmentation by applying multiresolution voting on
        the segmentation model, using (rounded to 32 pixels) a set of
        resolutions in the example benchmark code.
        '''
        y, x = tensor_images.shape[2:]
        b = len(tensor_images)
        tensor_images = (tensor_images + 1) / 2 * 255
        tensor_images = torch.flip(tensor_images, (1,)) # BGR!!!?
        tensor_images -= torch.tensor([102.9801, 115.9465, 122.7717]).to(
                   dtype=tensor_images.dtype, device=tensor_images.device
                   )[None,:,None,None]
        seg_shape = (y // downsample, x // downsample)
        # We want these to be multiples of 32 for the model.
        sizes = [(s, s) for s in self.segsizes]
        pred = {category: torch.zeros(
            len(tensor_images), len(self.segmodel.labeldata[category]),
            seg_shape[0], seg_shape[1]).cuda()
            for category in ['object', 'material']}
        part_pred = {partobj_index: torch.zeros(
            len(tensor_images), len(partindex),
            seg_shape[0], seg_shape[1]).cuda()
            for partobj_index, partindex in enumerate(self.part_index)}
        for size in sizes:
            if size == tensor_images.shape[2:]:
                resized = tensor_images
            else:
                resized = torch.nn.AdaptiveAvgPool2d(size)(tensor_images)
            r_pred = self.segmodel(
                dict(img=resized), seg_size=seg_shape)
            for k in pred:
                pred[k] += r_pred[k]
            for k in part_pred:
                part_pred[k] += r_pred['part'][k]
        return pred, part_pred","torch.zeros(len(tensor_images), len(partindex), seg_shape[0], seg_shape[1])","torch.zeros(len(tensor_images), len(partindex), *seg_shape[:2])","iterable_zj[0], iterable_zj[1]",*seg_shape[:2],*seg_shape[:2],1
SoftRas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SoftRas/examples/recon/models.py,https://github.com/ShichenLiu/SoftRas/tree/master/examples/recon/models.py,Decoder,__init__$39,"def __init__(self, filename_obj, dim_in=512, centroid_scale=0.1, bias_scale=1.0, centroid_lr=0.1, bias_lr=1.0):
        super(Decoder, self).__init__()
        # load .obj
        self.template_mesh = sr.Mesh.from_obj(filename_obj)
        self.register_buffer('vertices_base', self.template_mesh.vertices.cpu()[0])  # vertices_base)
        self.register_buffer('faces', self.template_mesh.faces.cpu()[0])  # faces)

        self.nv = self.vertices_base.size(0)
        self.nf = self.faces.size(0)
        self.centroid_scale = centroid_scale
        self.bias_scale = bias_scale
        self.obj_scale = 0.5

        dim = 1024
        dim_hidden = [dim, dim*2]
        self.fc1 = nn.Linear(dim_in, dim_hidden[0])
        self.fc2 = nn.Linear(dim_hidden[0], dim_hidden[1])
        self.fc_centroid = nn.Linear(dim_hidden[1], 3)
        self.fc_bias = nn.Linear(dim_hidden[1], self.nv*3)","nn.Linear(dim_hidden[0], dim_hidden[1])",nn.Linear(*dim_hidden[:2]),"iterable_zj[0], iterable_zj[1]",*dim_hidden[:2],*dim_hidden[:2],1
SegmenTron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SegmenTron/segmentron/models/contextnet.py,https://github.com/LikeLy-Journey/SegmenTron/tree/master/segmentron/models/contextnet.py,Deep_net,__init__$131,"def __init__(self, in_channels, block_channels,
                 t, num_blocks, **kwargs):
        super(Deep_net, self).__init__()
        self.block_channels = block_channels
        self.t = t
        self.num_blocks = num_blocks

        self.conv_ = Custom_Conv(3, in_channels, 3, 2)
        self.bottleneck1 = self._layer(LinearBottleneck, in_channels, block_channels[0], num_blocks[0], t[0], 1)
        self.bottleneck2 = self._layer(LinearBottleneck, block_channels[0], block_channels[1], num_blocks[1], t[1], 1)
        self.bottleneck3 = self._layer(LinearBottleneck, block_channels[1], block_channels[2], num_blocks[2], t[2], 2)
        self.bottleneck4 = self._layer(LinearBottleneck, block_channels[2], block_channels[3], num_blocks[3], t[3], 2)
        self.bottleneck5 = self._layer(LinearBottleneck, block_channels[3], block_channels[4], num_blocks[4], t[4], 1)
        self.bottleneck6 = self._layer(LinearBottleneck, block_channels[4], block_channels[5], num_blocks[5], t[5], 1)","self._layer(LinearBottleneck, block_channels[0], block_channels[1], num_blocks[1], t[1], 1)","self._layer(LinearBottleneck, *block_channels[:2], num_blocks[1], t[1], 1)","iterable_zj[0], iterable_zj[1]",*block_channels[:2],*block_channels[:2],1
SegmenTron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SegmenTron/segmentron/models/contextnet.py,https://github.com/LikeLy-Journey/SegmenTron/tree/master/segmentron/models/contextnet.py,Deep_net,__init__$131,"def __init__(self, in_channels, block_channels,
                 t, num_blocks, **kwargs):
        super(Deep_net, self).__init__()
        self.block_channels = block_channels
        self.t = t
        self.num_blocks = num_blocks

        self.conv_ = Custom_Conv(3, in_channels, 3, 2)
        self.bottleneck1 = self._layer(LinearBottleneck, in_channels, block_channels[0], num_blocks[0], t[0], 1)
        self.bottleneck2 = self._layer(LinearBottleneck, block_channels[0], block_channels[1], num_blocks[1], t[1], 1)
        self.bottleneck3 = self._layer(LinearBottleneck, block_channels[1], block_channels[2], num_blocks[2], t[2], 2)
        self.bottleneck4 = self._layer(LinearBottleneck, block_channels[2], block_channels[3], num_blocks[3], t[3], 2)
        self.bottleneck5 = self._layer(LinearBottleneck, block_channels[3], block_channels[4], num_blocks[4], t[4], 1)
        self.bottleneck6 = self._layer(LinearBottleneck, block_channels[4], block_channels[5], num_blocks[5], t[5], 1)","self._layer(LinearBottleneck, block_channels[1], block_channels[2], num_blocks[2], t[2], 2)","self._layer(LinearBottleneck, *block_channels[1:3], num_blocks[2], t[2], 2)","iterable_zj[1], iterable_zj[2]",*block_channels[1:3],*block_channels[1:3],1
SegmenTron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SegmenTron/segmentron/models/contextnet.py,https://github.com/LikeLy-Journey/SegmenTron/tree/master/segmentron/models/contextnet.py,Deep_net,__init__$131,"def __init__(self, in_channels, block_channels,
                 t, num_blocks, **kwargs):
        super(Deep_net, self).__init__()
        self.block_channels = block_channels
        self.t = t
        self.num_blocks = num_blocks

        self.conv_ = Custom_Conv(3, in_channels, 3, 2)
        self.bottleneck1 = self._layer(LinearBottleneck, in_channels, block_channels[0], num_blocks[0], t[0], 1)
        self.bottleneck2 = self._layer(LinearBottleneck, block_channels[0], block_channels[1], num_blocks[1], t[1], 1)
        self.bottleneck3 = self._layer(LinearBottleneck, block_channels[1], block_channels[2], num_blocks[2], t[2], 2)
        self.bottleneck4 = self._layer(LinearBottleneck, block_channels[2], block_channels[3], num_blocks[3], t[3], 2)
        self.bottleneck5 = self._layer(LinearBottleneck, block_channels[3], block_channels[4], num_blocks[4], t[4], 1)
        self.bottleneck6 = self._layer(LinearBottleneck, block_channels[4], block_channels[5], num_blocks[5], t[5], 1)","self._layer(LinearBottleneck, block_channels[2], block_channels[3], num_blocks[3], t[3], 2)","self._layer(LinearBottleneck, *block_channels[2:4], num_blocks[3], t[3], 2)","iterable_zj[2], iterable_zj[3]",*block_channels[2:4],*block_channels[2:4],1
SegmenTron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SegmenTron/segmentron/models/contextnet.py,https://github.com/LikeLy-Journey/SegmenTron/tree/master/segmentron/models/contextnet.py,Deep_net,__init__$131,"def __init__(self, in_channels, block_channels,
                 t, num_blocks, **kwargs):
        super(Deep_net, self).__init__()
        self.block_channels = block_channels
        self.t = t
        self.num_blocks = num_blocks

        self.conv_ = Custom_Conv(3, in_channels, 3, 2)
        self.bottleneck1 = self._layer(LinearBottleneck, in_channels, block_channels[0], num_blocks[0], t[0], 1)
        self.bottleneck2 = self._layer(LinearBottleneck, block_channels[0], block_channels[1], num_blocks[1], t[1], 1)
        self.bottleneck3 = self._layer(LinearBottleneck, block_channels[1], block_channels[2], num_blocks[2], t[2], 2)
        self.bottleneck4 = self._layer(LinearBottleneck, block_channels[2], block_channels[3], num_blocks[3], t[3], 2)
        self.bottleneck5 = self._layer(LinearBottleneck, block_channels[3], block_channels[4], num_blocks[4], t[4], 1)
        self.bottleneck6 = self._layer(LinearBottleneck, block_channels[4], block_channels[5], num_blocks[5], t[5], 1)","self._layer(LinearBottleneck, block_channels[3], block_channels[4], num_blocks[4], t[4], 1)","self._layer(LinearBottleneck, *block_channels[3:5], num_blocks[4], t[4], 1)","iterable_zj[3], iterable_zj[4]",*block_channels[3:5],*block_channels[3:5],1
SegmenTron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SegmenTron/segmentron/models/contextnet.py,https://github.com/LikeLy-Journey/SegmenTron/tree/master/segmentron/models/contextnet.py,Deep_net,__init__$131,"def __init__(self, in_channels, block_channels,
                 t, num_blocks, **kwargs):
        super(Deep_net, self).__init__()
        self.block_channels = block_channels
        self.t = t
        self.num_blocks = num_blocks

        self.conv_ = Custom_Conv(3, in_channels, 3, 2)
        self.bottleneck1 = self._layer(LinearBottleneck, in_channels, block_channels[0], num_blocks[0], t[0], 1)
        self.bottleneck2 = self._layer(LinearBottleneck, block_channels[0], block_channels[1], num_blocks[1], t[1], 1)
        self.bottleneck3 = self._layer(LinearBottleneck, block_channels[1], block_channels[2], num_blocks[2], t[2], 2)
        self.bottleneck4 = self._layer(LinearBottleneck, block_channels[2], block_channels[3], num_blocks[3], t[3], 2)
        self.bottleneck5 = self._layer(LinearBottleneck, block_channels[3], block_channels[4], num_blocks[4], t[4], 1)
        self.bottleneck6 = self._layer(LinearBottleneck, block_channels[4], block_channels[5], num_blocks[5], t[5], 1)","self._layer(LinearBottleneck, block_channels[4], block_channels[5], num_blocks[5], t[5], 1)","self._layer(LinearBottleneck, *block_channels[4:6], num_blocks[5], t[5], 1)","iterable_zj[4], iterable_zj[5]",*block_channels[4:6],*block_channels[4:6],1
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/sets/handlers/functions.py,https://github.com/sympy/sympy/tree/master/sympy/sets/handlers/functions.py,,_set_function$33,"def _set_function(f, x): # noqa:F811
    from sympy.solvers.solveset import solveset
    from sympy.series import limit
    # TODO: handle functions with infinitely many solutions (eg, sin, tan)
    # TODO: handle multivariate functions

    expr = f.expr
    if len(expr.free_symbols) > 1 or len(f.variables) != 1:
        return
    var = f.variables[0]
    if not var.is_real:
        if expr.subs(var, Dummy(real=True)).is_real is False:
            return

    if expr.is_Piecewise:
        result = S.EmptySet
        domain_set = x
        for (p_expr, p_cond) in expr.args:
            if p_cond is true:
                intrvl = domain_set
            else:
                intrvl = p_cond.as_set()
                intrvl = Intersection(domain_set, intrvl)

            if p_expr.is_Number:
                image = FiniteSet(p_expr)
            else:
                image = imageset(Lambda(var, p_expr), intrvl)
            result = Union(result, image)

            # remove the part which has been `imaged`
            domain_set = Complement(domain_set, intrvl)
            if domain_set is S.EmptySet:
                break
        return result

    if not x.start.is_comparable or not x.end.is_comparable:
        return

    try:
        from sympy.polys.polyutils import _nsort
        sing = list(singularities(expr, var, x))
        if len(sing) > 1:
            sing = _nsort(sing)
    except NotImplementedError:
        return

    if x.left_open:
        _start = limit(expr, var, x.start, dir=""+"")
    elif x.start not in sing:
        _start = f(x.start)
    if x.right_open:
        _end = limit(expr, var, x.end, dir=""-"")
    elif x.end not in sing:
        _end = f(x.end)

    if len(sing) == 0:
        soln_expr = solveset(diff(expr, var), var)
        if not (isinstance(soln_expr, FiniteSet)
                or soln_expr is S.EmptySet):
            return
        solns = list(soln_expr)

        extr = [_start, _end] + [f(i) for i in solns
                                 if i.is_real and i in x]
        start, end = Min(*extr), Max(*extr)

        left_open, right_open = False, False
        if _start <= _end:
            # the minimum or maximum value can occur simultaneously
            # on both the edge of the interval and in some interior
            # point
            if start == _start and start not in solns:
                left_open = x.left_open
            if end == _end and end not in solns:
                right_open = x.right_open
        else:
            if start == _end and start not in solns:
                left_open = x.right_open
            if end == _start and end not in solns:
                right_open = x.left_open

        return Interval(start, end, left_open, right_open)
    else:
        return imageset(f, Interval(x.start, sing[0],
                                    x.left_open, True)) + \
            Union(*[imageset(f, Interval(sing[i], sing[i + 1], True, True))
                    for i in range(0, len(sing) - 1)]) + \
            imageset(f, Interval(sing[-1], x.end, True, x.right_open))","Interval(sing[i], sing[i + 1], True, True)","Interval(*sing[i:i + 2], True, True)","iterable_zj[i], iterable_zj[i + 1]",*sing[i:i+2],*sing[i:i + 2],1
FSA-Net,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FSA-Net/data/TYY_create_db_biwi.py,https://github.com/shamangary/FSA-Net/tree/master/data/TYY_create_db_biwi.py,,main$32,"def main():
	args = get_args()
	mypath = args.db
	output_path = args.output
	img_size = args.img_size
	ad = args.ad

	isPlot = True
	detector = MTCNN()

	onlyfiles_png = []
	onlyfiles_txt = []
	for num in range(0,24):
		if num<9:
			mypath_obj = mypath+'/0'+str(num+1)
		else:
			mypath_obj = mypath+'/'+str(num+1)
		print(mypath_obj)
		onlyfiles_txt_temp = [f for f in listdir(mypath_obj) if isfile(join(mypath_obj, f)) and join(mypath_obj, f).endswith('.txt')]
		onlyfiles_png_temp = [f for f in listdir(mypath_obj) if isfile(join(mypath_obj, f)) and join(mypath_obj, f).endswith('.png')]
	
		onlyfiles_txt_temp.sort()
		onlyfiles_png_temp.sort()

		onlyfiles_txt.append(onlyfiles_txt_temp)
		onlyfiles_png.append(onlyfiles_png_temp)
	print(len(onlyfiles_txt))
	print(len(onlyfiles_png))
	
	out_imgs = []
	out_poses = []
	
	for i in range(len(onlyfiles_png)):
		print('object %d' %i)
		
		mypath_obj = ''
		if i<9:
			mypath_obj = mypath+'/0'+str(i+1)
		else:
			mypath_obj = mypath+'/'+str(i+1)

		for j in tqdm(range(len(onlyfiles_png[i]))):
			
			img_name = onlyfiles_png[i][j]
			txt_name = onlyfiles_txt[i][j]
			
			img_name_split = img_name.split('_')
			txt_name_split = txt_name.split('_')

			if img_name_split[1] != txt_name_split[1]:
				print('Mismatched!')
				sys.exit()


			pose_path = mypath_obj+'/'+txt_name
			# Load pose in degrees
			pose_annot = open(pose_path, 'r')
			R = []
			for line in pose_annot:
				line = line.strip('\n').split(' ')
				L = []
				if line[0] != '':
					for nb in line:
						if nb == '':
							continue
						L.append(float(nb))
					R.append(L)

			R = np.array(R)
			T = R[3,:]
			R = R[:3,:]
			pose_annot.close()

			R = np.transpose(R)

			roll = -np.arctan2(R[1][0], R[0][0]) * 180 / np.pi
			yaw = -np.arctan2(-R[2][0], np.sqrt(R[2][1] ** 2 + R[2][2] ** 2)) * 180 / np.pi
			pitch = np.arctan2(R[2][1], R[2][2]) * 180 / np.pi



			img = cv2.imread(mypath_obj+'/'+img_name)
			img_h = img.shape[0]
			img_w = img.shape[1]
			if j==0:
				[xw1_pre,xw2_pre,yw1_pre,yw2_pre] = [0,0,0,0]
			detected = detector.detect_faces(img)

			if len(detected) > 0:
				dis_list = []
				XY = []
				for i_d, d in enumerate(detected):
					
					xv = []
					yv = []
					for key, value in d['keypoints'].items():
						xv.append(value[0]) 
						yv.append(value[1])
					
					if d['confidence'] > 0.90:
						x1,y1,w,h = d['box']
						x2 = x1 + w
						y2 = y1 + h
						xw1 = max(int(x1 - ad * w), 0)
						yw1 = max(int(y1 - ad * h), 0)
						xw2 = min(int(x2 + ad * w), img_w - 1)
						yw2 = min(int(y2 + ad * h), img_h - 1)
						
						# Crop the face loosely
						# x_min = int(min(xv))
						# x_max = int(max(xv))
						# y_min = int(min(yv))
						# y_max = int(max(yv))
						
						# h = y_max-y_min
						# w = x_max-x_min

						# xw1 = max(int(x_min - ad * w), 0)
						# xw2 = min(int(x_max + ad * w), img_w - 1)
						# yw1 = max(int(y_min - ad * h), 0)
						# yw2 = min(int(y_max + ad * h), img_h - 1)

						XY.append([xw1,xw2,yw1,yw2])
						dis_betw_cen = np.abs(xw1-img_w*2/3)+np.abs(yw1-img_h*2/3)
						dis_list.append(dis_betw_cen)
				
				if len(dis_list)>0:
					min_id = np.argmin(dis_list)
					[xw1,xw2,yw1,yw2] = XY[min_id]


				dis_betw_frames = np.abs(xw1-xw1_pre)
				if dis_betw_frames < 80 or j==0:
					img = cv2.resize(img[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))
					[xw1_pre,xw2_pre,yw1_pre,yw2_pre] = [xw1,xw2,yw1,yw2]
					if isPlot:
						print([xw1_pre,xw2_pre,yw1_pre,yw2_pre])
						cv2.imshow('check',img)
						k=cv2.waitKey(10)
					img = cv2.resize(img, (img_size, img_size))
					cont_labels = np.array([yaw, pitch, roll])
					out_imgs.append(img)
					out_poses.append(cont_labels)
			# 	else:
			# 		img = cv2.resize(img[yw1_pre:yw2_pre + 1, xw1_pre:xw2_pre + 1, :], (img_size, img_size))
			# 		# Checking the cropped image
			# 		if isPlot:
			# 			print([xw1_pre,xw2_pre,yw1_pre,yw2_pre])
			# 			print('Distance between two frames too large! Use previous frame detected location.')
					
			# 			cv2.imshow('check',img)
			# 			k=cv2.waitKey(10)
			# 		img = cv2.resize(img, (img_size, img_size))
			# 		cont_labels = np.array([yaw, pitch, roll])
			# 		out_imgs.append(img)
			# 		out_poses.append(cont_labels)
			# else:
			# 	img = cv2.resize(img[yw1_pre:yw2_pre + 1, xw1_pre:xw2_pre + 1, :], (img_size, img_size))
			# 	if isPlot:
			# 		print('No face detected! Use previous frame detected location.')
				
			# 	# Checking the cropped image
			# 	if isPlot:
			# 		cv2.imshow('check',img)
			# 		k=cv2.waitKey(10)
			# 	img = cv2.resize(img, (img_size, img_size))
			# 	cont_labels = np.array([yaw, pitch, roll])
			# 	out_imgs.append(img)
			# 	out_poses.append(cont_labels)
	np.savez(output_path,image=np.array(out_imgs), pose=np.array(out_poses), img_size=img_size)","np.arctan2(R[2][1], R[2][2])",np.arctan2(*R[2][1:3]),"iterable_zj[1], iterable_zj[2]",*R[2][1:3],*R[2][1:3],1
EasyClangComplete,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyClangComplete/plugin/clang/cindex40.py,https://github.com/niosus/EasyClangComplete/tree/master/plugin/clang/cindex40.py,TranslationUnit,get_extent$2675,"def get_extent(self, filename, locations):
        """"""Obtain a SourceRange from this translation unit.

        The bounds of the SourceRange must ultimately be defined by a start and
        end SourceLocation. For the locations argument, you can pass:

          - 2 SourceLocation instances in a 2-tuple or list.
          - 2 int file offsets via a 2-tuple or list.
          - 2 2-tuple or lists of (line, column) pairs in a 2-tuple or list.

        e.g.

        get_extent('foo.c', (5, 10))
        get_extent('foo.c', ((1, 1), (1, 15)))
        """"""
        f = self.get_file(filename)

        if len(locations) < 2:
            raise Exception('Must pass object with at least 2 elements')

        start_location, end_location = locations

        if hasattr(start_location, '__len__'):
            start_location = SourceLocation.from_position(self, f,
                start_location[0], start_location[1])
        elif isinstance(start_location, int):
            start_location = SourceLocation.from_offset(self, f,
                start_location)

        if hasattr(end_location, '__len__'):
            end_location = SourceLocation.from_position(self, f,
                end_location[0], end_location[1])
        elif isinstance(end_location, int):
            end_location = SourceLocation.from_offset(self, f, end_location)

        assert isinstance(start_location, SourceLocation)
        assert isinstance(end_location, SourceLocation)

        return SourceRange.from_locations(start_location, end_location)","SourceLocation.from_position(self, f, start_location[0], start_location[1])","SourceLocation.from_position(self, f, *start_location[:2])","iterable_zj[0], iterable_zj[1]",*start_location[:2],*start_location[:2],1
EasyClangComplete,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyClangComplete/plugin/clang/cindex40.py,https://github.com/niosus/EasyClangComplete/tree/master/plugin/clang/cindex40.py,TranslationUnit,get_extent$2675,"def get_extent(self, filename, locations):
        """"""Obtain a SourceRange from this translation unit.

        The bounds of the SourceRange must ultimately be defined by a start and
        end SourceLocation. For the locations argument, you can pass:

          - 2 SourceLocation instances in a 2-tuple or list.
          - 2 int file offsets via a 2-tuple or list.
          - 2 2-tuple or lists of (line, column) pairs in a 2-tuple or list.

        e.g.

        get_extent('foo.c', (5, 10))
        get_extent('foo.c', ((1, 1), (1, 15)))
        """"""
        f = self.get_file(filename)

        if len(locations) < 2:
            raise Exception('Must pass object with at least 2 elements')

        start_location, end_location = locations

        if hasattr(start_location, '__len__'):
            start_location = SourceLocation.from_position(self, f,
                start_location[0], start_location[1])
        elif isinstance(start_location, int):
            start_location = SourceLocation.from_offset(self, f,
                start_location)

        if hasattr(end_location, '__len__'):
            end_location = SourceLocation.from_position(self, f,
                end_location[0], end_location[1])
        elif isinstance(end_location, int):
            end_location = SourceLocation.from_offset(self, f, end_location)

        assert isinstance(start_location, SourceLocation)
        assert isinstance(end_location, SourceLocation)

        return SourceRange.from_locations(start_location, end_location)","SourceLocation.from_position(self, f, end_location[0], end_location[1])","SourceLocation.from_position(self, f, *end_location[:2])","iterable_zj[0], iterable_zj[1]",*end_location[:2],*end_location[:2],1
imgclsmob,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/losses.py,https://github.com/osmr/imgclsmob/tree/master/gluon/losses.py,MixSoftmaxCrossEntropyLoss,hybrid_forward$92,"def hybrid_forward(self, F, preds, label, **kwargs):
        """"""
        Compute loss.
        """"""
        if self.aux:
            return self._aux_forward(F, preds[0], preds[1], label)
        else:
            return super(MixSoftmaxCrossEntropyLoss, self).hybrid_forward(F, preds, label)","self._aux_forward(F, preds[0], preds[1], label)","self._aux_forward(F, *preds[:2], label)","iterable_zj[0], iterable_zj[1]",*preds[:2],*preds[:2],1
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/modeling/tests/test_models.py,https://github.com/astropy/astropy/tree/master/astropy/modeling/tests/test_models.py,Fittable2DModelTester,test_fitter2D$279,"def test_fitter2D(self, model_class, test_parameters, fitter):
        """"""Test if the parametric model works with the fitter.""""""
        fitter = fitter()

        x_lim = test_parameters[""x_lim""]
        y_lim = test_parameters[""y_lim""]

        parameters = test_parameters[""parameters""]
        model = create_model(model_class, test_parameters)

        if isinstance(parameters, dict):
            parameters = [parameters[name] for name in model.param_names]

        if ""log_fit"" in test_parameters:
            if test_parameters[""log_fit""]:
                x = np.logspace(x_lim[0], x_lim[1], self.N)
                y = np.logspace(y_lim[0], y_lim[1], self.N)
        else:
            x = np.linspace(x_lim[0], x_lim[1], self.N)
            y = np.linspace(y_lim[0], y_lim[1], self.N)
        xv, yv = np.meshgrid(x, y)

        np.random.seed(0)
        # add 10% noise to the amplitude
        noise = np.random.rand(self.N, self.N) - 0.5
        data = model(xv, yv) + 0.1 * parameters[0] * noise
        new_model = fitter(model, xv, yv, data)

        params = [getattr(new_model, name) for name in new_model.param_names]
        fixed = [param.fixed for param in params]
        expected = np.array([val for val, fixed in zip(parameters, fixed) if not fixed])
        fitted = np.array([param.value for param in params if not param.fixed])
        assert_allclose(fitted, expected, atol=self.fit_error)","np.linspace(x_lim[0], x_lim[1], self.N)","np.linspace(*x_lim[:2], self.N)","iterable_zj[0], iterable_zj[1]",*x_lim[:2],*x_lim[:2],1
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/modeling/tests/test_models.py,https://github.com/astropy/astropy/tree/master/astropy/modeling/tests/test_models.py,Fittable2DModelTester,test_fitter2D$279,"def test_fitter2D(self, model_class, test_parameters, fitter):
        """"""Test if the parametric model works with the fitter.""""""
        fitter = fitter()

        x_lim = test_parameters[""x_lim""]
        y_lim = test_parameters[""y_lim""]

        parameters = test_parameters[""parameters""]
        model = create_model(model_class, test_parameters)

        if isinstance(parameters, dict):
            parameters = [parameters[name] for name in model.param_names]

        if ""log_fit"" in test_parameters:
            if test_parameters[""log_fit""]:
                x = np.logspace(x_lim[0], x_lim[1], self.N)
                y = np.logspace(y_lim[0], y_lim[1], self.N)
        else:
            x = np.linspace(x_lim[0], x_lim[1], self.N)
            y = np.linspace(y_lim[0], y_lim[1], self.N)
        xv, yv = np.meshgrid(x, y)

        np.random.seed(0)
        # add 10% noise to the amplitude
        noise = np.random.rand(self.N, self.N) - 0.5
        data = model(xv, yv) + 0.1 * parameters[0] * noise
        new_model = fitter(model, xv, yv, data)

        params = [getattr(new_model, name) for name in new_model.param_names]
        fixed = [param.fixed for param in params]
        expected = np.array([val for val, fixed in zip(parameters, fixed) if not fixed])
        fitted = np.array([param.value for param in params if not param.fixed])
        assert_allclose(fitted, expected, atol=self.fit_error)","np.linspace(y_lim[0], y_lim[1], self.N)","np.linspace(*y_lim[:2], self.N)","iterable_zj[0], iterable_zj[1]",*y_lim[:2],*y_lim[:2],1
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/modeling/tests/test_models.py,https://github.com/astropy/astropy/tree/master/astropy/modeling/tests/test_models.py,Fittable2DModelTester,test_fitter2D$279,"def test_fitter2D(self, model_class, test_parameters, fitter):
        """"""Test if the parametric model works with the fitter.""""""
        fitter = fitter()

        x_lim = test_parameters[""x_lim""]
        y_lim = test_parameters[""y_lim""]

        parameters = test_parameters[""parameters""]
        model = create_model(model_class, test_parameters)

        if isinstance(parameters, dict):
            parameters = [parameters[name] for name in model.param_names]

        if ""log_fit"" in test_parameters:
            if test_parameters[""log_fit""]:
                x = np.logspace(x_lim[0], x_lim[1], self.N)
                y = np.logspace(y_lim[0], y_lim[1], self.N)
        else:
            x = np.linspace(x_lim[0], x_lim[1], self.N)
            y = np.linspace(y_lim[0], y_lim[1], self.N)
        xv, yv = np.meshgrid(x, y)

        np.random.seed(0)
        # add 10% noise to the amplitude
        noise = np.random.rand(self.N, self.N) - 0.5
        data = model(xv, yv) + 0.1 * parameters[0] * noise
        new_model = fitter(model, xv, yv, data)

        params = [getattr(new_model, name) for name in new_model.param_names]
        fixed = [param.fixed for param in params]
        expected = np.array([val for val, fixed in zip(parameters, fixed) if not fixed])
        fitted = np.array([param.value for param in params if not param.fixed])
        assert_allclose(fitted, expected, atol=self.fit_error)","np.logspace(x_lim[0], x_lim[1], self.N)","np.logspace(*x_lim[:2], self.N)","iterable_zj[0], iterable_zj[1]",*x_lim[:2],*x_lim[:2],1
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/modeling/tests/test_models.py,https://github.com/astropy/astropy/tree/master/astropy/modeling/tests/test_models.py,Fittable2DModelTester,test_fitter2D$279,"def test_fitter2D(self, model_class, test_parameters, fitter):
        """"""Test if the parametric model works with the fitter.""""""
        fitter = fitter()

        x_lim = test_parameters[""x_lim""]
        y_lim = test_parameters[""y_lim""]

        parameters = test_parameters[""parameters""]
        model = create_model(model_class, test_parameters)

        if isinstance(parameters, dict):
            parameters = [parameters[name] for name in model.param_names]

        if ""log_fit"" in test_parameters:
            if test_parameters[""log_fit""]:
                x = np.logspace(x_lim[0], x_lim[1], self.N)
                y = np.logspace(y_lim[0], y_lim[1], self.N)
        else:
            x = np.linspace(x_lim[0], x_lim[1], self.N)
            y = np.linspace(y_lim[0], y_lim[1], self.N)
        xv, yv = np.meshgrid(x, y)

        np.random.seed(0)
        # add 10% noise to the amplitude
        noise = np.random.rand(self.N, self.N) - 0.5
        data = model(xv, yv) + 0.1 * parameters[0] * noise
        new_model = fitter(model, xv, yv, data)

        params = [getattr(new_model, name) for name in new_model.param_names]
        fixed = [param.fixed for param in params]
        expected = np.array([val for val, fixed in zip(parameters, fixed) if not fixed])
        fitted = np.array([param.value for param in params if not param.fixed])
        assert_allclose(fitted, expected, atol=self.fit_error)","np.logspace(y_lim[0], y_lim[1], self.N)","np.logspace(*y_lim[:2], self.N)","iterable_zj[0], iterable_zj[1]",*y_lim[:2],*y_lim[:2],1
TFSegmentation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TFSegmentation/data/coco/coco_dataset.py,https://github.com/MSiam/TFSegmentation/tree/master/data/coco/coco_dataset.py,MYCOCO,parse_mask$89,"def parse_mask(self, anns, img_mask):
            """"""
            Parse the specified annotations.
            :param anns (array of object): annotations to display
            :return: None
            """"""
            if len(anns) == 0:
                return img_mask
            for ann in anns:
                m = self.annToMask(ann, img_mask.shape[0], img_mask.shape[1])
                img_mask[m==1]= ann['category_id']
            train_mask= np.zeros_like(img_mask)
            for c in MYCOCO.labelID_2_trainID.keys():
                train_mask[img_mask==c]= MYCOCO.labelID_2_trainID[c]
            return train_mask","self.annToMask(ann, img_mask.shape[0], img_mask.shape[1])","self.annToMask(ann, *img_mask.shape[:2])","iterable_zj[0], iterable_zj[1]",*img_mask.shape[:2],*img_mask.shape[:2],1
singleshotpose,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/singleshotpose/py2/darknet.py,https://github.com/microsoft/singleshotpose/tree/master/py2/darknet.py,Darknet,save_weights$347,"def save_weights(self, outfile, cutoff=0):
        if cutoff <= 0:
            cutoff = len(self.blocks)-1

        fp = open(outfile, 'wb')
        self.header[3] = self.seen
        header = self.header
        header.numpy().tofile(fp)

        ind = -1
        for blockId in range(1, cutoff+1):
            ind = ind + 1
            block = self.blocks[blockId]
            if block['type'] == 'convolutional':
                model = self.models[ind]
                batch_normalize = int(block['batch_normalize'])
                if batch_normalize:
                    save_conv_bn(fp, model[0], model[1])
                else:
                    save_conv(fp, model[0])
            elif block['type'] == 'connected':
                model = self.models[ind]
                if block['activation'] != 'linear':
                    save_fc(fc, model)
                else:
                    save_fc(fc, model[0])
            elif block['type'] == 'maxpool':
                pass
            elif block['type'] == 'reorg':
                pass
            elif block['type'] == 'route':
                pass
            elif block['type'] == 'shortcut':
                pass
            elif block['type'] == 'region':
                pass
            elif block['type'] == 'avgpool':
                pass
            elif block['type'] == 'softmax':
                pass
            elif block['type'] == 'cost':
                pass
            else:
                print('unknown type %s' % (block['type']))
        fp.close()","save_conv_bn(fp, model[0], model[1])","save_conv_bn(fp, *model[:2])","iterable_zj[0], iterable_zj[1]",*model[:2],*model[:2],1
MEAnalyzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MEAnalyzer/MEA.py,https://github.com/platomav/MEAnalyzer/tree/master//MEA.py,,phy_anl$9209,"def phy_anl(mn2_info) :
    phy_platform = 'Unknown'
    phy_sku = 'Unknown'
    
    # mn2_info = [Major, Minor, Hotfix, Build, Release, RSA Key Hash, RSA Sig Hash, Date, SVN, PV bit, MEU Major, MEU Minor, MEU Hotfix,
    #               MEU Build, MN2 w/o RSA Hashes, MN2 Struct, MN2 Match Start, MN2 Match End]
    
    # $MN2 Manifest SVN = CSE_Ext_0F ARBSVN. The value is used for Anti-Rollback (ARB) and not Trusted Computing Base (TCB) purposes.
    
    phy_year,phy_month,phy_day = list(map(int, mn2_info[7].split('-')))
    
    # Detect PHY Variant
    phy_variant, _, _ = get_variant(reading, mn2_info[15], mn2_info[16], mn2_info[17], mn2_info[5], [phy_year, phy_month, phy_day],
                                 [mn2_info[0], mn2_info[1], mn2_info[2], mn2_info[3]])

    if not phy_variant.startswith('PHY') : phy_variant = 'Unknown'
    
    if phy_variant != 'Unknown' :
        phy_platform = phy_variant[-3:]
        phy_sku = 'G' if phy_variant.startswith('PHYDG') else phy_variant[3] # Set ""G"" for GSC/GFX to match regular PHY SKU naming
    
    phy_mn2_signed = 'Pre-Production' if mn2_info[4] == 'Debug' else 'Production'
    phy_mn2_signed_db = 'PRD' if phy_mn2_signed == 'Production' else 'PRE'
    
    # Fix Release of PRE firmware which are wrongly reported as PRD
    phy_mn2_signed, phy_mn2_signed_db = release_fix(phy_mn2_signed, phy_mn2_signed_db, mn2_info[5])
    
    phy_fw_ver = '%d.%d.%d.%0.4d' % (mn2_info[0], mn2_info[1], mn2_info[2], mn2_info[3])
    phy_meu_ver = '%d.%d.%d.%0.4d' % (mn2_info[10], mn2_info[11], mn2_info[12], mn2_info[13])
    if phy_variant.startswith('PHYDG') :
        phy_name_db = '%s_%s_%s_%s_%s_%s' % (phy_platform[:3], phy_sku, phy_fw_ver, mn2_info[7], phy_mn2_signed_db, mn2_info[6])
    else :
        phy_name_db = '%s_%s_%s_%s_%s' % (phy_platform[:3], phy_sku, phy_fw_ver, phy_mn2_signed_db, mn2_info[6])
    
    # Search DB for PHY firmware
    for line in mea_db_lines :
        if phy_name_db in line :
            break # Break loop at 1st name match
    else :
        note_new_fw('PHY %s %s' % (phy_platform, phy_sku))
    
    # Detect PHY RSA Public Key Recognition
    for line in mea_db_lines :
        if mn2_info[5] in line :
            break # Break loop at 1st hash match
    else :
        err_msg = [col_r + 'Error: Unknown %s %d.%d RSA Public Key!' % (phy_variant, mn2_info[0], mn2_info[1]) + col_e, True]
        if err_msg not in err_stor : err_stor.append(err_msg) # Do not store message twice at bare/non-stitched PHY firmware
    
    return phy_fw_ver, phy_sku, phy_mn2_signed, phy_mn2_signed_db, phy_platform, mn2_info[7], mn2_info[8], mn2_info[9], phy_meu_ver, mn2_info[3]","get_variant(reading, mn2_info[15], mn2_info[16], mn2_info[17], mn2_info[5], [phy_year, phy_month, phy_day], [mn2_info[0], mn2_info[1], mn2_info[2], mn2_info[3]])",Cannot refactor,"iterable_zj[15], iterable_zj[16], iterable_zj[17]",,*mn2_info[15:18],0
graph-rcnn.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/graph-rcnn.pytorch/lib/scene_parser/rcnn/layers/dcn/deform_conv_func.py,https://github.com/jwyang/graph-rcnn.pytorch/tree/master/lib/scene_parser/rcnn/layers/dcn/deform_conv_func.py,ModulatedDeformConvFunction,forward$153,"def forward(
        ctx,
        input,
        offset,
        mask,
        weight,
        bias=None,
        stride=1,
        padding=0,
        dilation=1,
        groups=1,
        deformable_groups=1
    ):
        ctx.stride = stride
        ctx.padding = padding
        ctx.dilation = dilation
        ctx.groups = groups
        ctx.deformable_groups = deformable_groups
        ctx.with_bias = bias is not None
        if not ctx.with_bias:
            bias = input.new_empty(1)  # fake tensor
        if not input.is_cuda:
            raise NotImplementedError
        if weight.requires_grad or mask.requires_grad or offset.requires_grad \
                or input.requires_grad:
            ctx.save_for_backward(input, offset, mask, weight, bias)
        output = input.new_empty(
            ModulatedDeformConvFunction._infer_shape(ctx, input, weight))
        ctx._bufs = [input.new_empty(0), input.new_empty(0)]
        _C.modulated_deform_conv_forward(
            input,
            weight,
            bias,
            ctx._bufs[0],
            offset,
            mask,
            output,
            ctx._bufs[1],
            weight.shape[2],
            weight.shape[3],
            ctx.stride,
            ctx.stride,
            ctx.padding,
            ctx.padding,
            ctx.dilation,
            ctx.dilation,
            ctx.groups,
            ctx.deformable_groups,
            ctx.with_bias
        )
        return output","_C.modulated_deform_conv_forward(input, weight, bias, ctx._bufs[0], offset, mask, output, ctx._bufs[1], weight.shape[2], weight.shape[3], ctx.stride, ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation, ctx.groups, ctx.deformable_groups, ctx.with_bias)","_C.modulated_deform_conv_forward(input, weight, bias, ctx._bufs[0], offset, mask, output, ctx._bufs[1], *weight.shape[2:4], ctx.stride, ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation, ctx.groups, ctx.deformable_groups, ctx.with_bias)","iterable_zj[2], iterable_zj[3]",*weight.shape[2:4],*weight.shape[2:4],1
godot-blender-exporter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/godot-blender-exporter/io_scene_godot/converters/animation/serializer.py,https://github.com/godotengine/godot-blender-exporter/tree/master/io_scene_godot/converters/animation/serializer.py,AnimationResource,add_obj_xform_track$529,"def add_obj_xform_track(self, node_type, track_path,
                            xform_frames_list, frame_range,
                            parent_mat_inverse=mathutils.Matrix.Identity(4)):
        """"""Add a object transform track to AnimationResource""""""
        track = TransformTrack(
            track_path,
            frames_iter=range(frame_range[0], frame_range[1]),
            values_iter=xform_frames_list,
        )
        track.set_parent_inverse(parent_mat_inverse)
        if node_type in (""SpotLight"", ""DirectionalLight"",
                         ""Camera"", ""CollisionShape""):
            track.is_directional = True

        self.add_track(track)","range(frame_range[0], frame_range[1])",range(*frame_range[:2]),"iterable_zj[0], iterable_zj[1]",*frame_range[:2],*frame_range[:2],1
elodie,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/elodie/elodie/geolocation.py,https://github.com/jmathai/elodie/tree/master/elodie/geolocation.py,,dms_string$92,"def dms_string(decimal, type='latitude'):
    # Example string -> 38 deg 14' 27.82"" S
    dms = decimal_to_dms(decimal)
    if type == 'latitude':
        direction = 'N' if decimal >= 0 else 'S'
    elif type == 'longitude':
        direction = 'E' if decimal >= 0 else 'W'
    return '{} deg {}\' {}"" {}'.format(dms[0], dms[1], dms[2], direction)","'{} deg {}\' {}"" {}'.format(dms[0], dms[1], dms[2], direction)","'{} deg {}\' {}"" {}'.format(*dms[:3], direction)","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*dms[:3],*dms[:3],1
yapf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yapf/yapf/yapflib/format_decision_state.py,https://github.com/google/yapf/tree/master/yapf/yapflib/format_decision_state.py,FormatDecisionState,MustSplit$167,"def MustSplit(self):
    """"""Returns True if the line must split before the next token.""""""
    current = self.next_token
    previous = current.previous_token

    if current.is_pseudo:
      return False

    if current.must_break_before:
      return True

    if not previous:
      return False

    if style.Get('SPLIT_ALL_COMMA_SEPARATED_VALUES') and previous.value == ',':
      return True

    if (style.Get('SPLIT_ALL_TOP_LEVEL_COMMA_SEPARATED_VALUES') and
        previous.value == ','):
      # Avoid breaking in a container that fits in the current line if possible
      opening = _GetOpeningBracket(current)

      # Can't find opening bracket, behave the same way as
      # SPLIT_ALL_COMMA_SEPARATED_VALUES.
      if not opening:
        return True

      if current.is_comment:
        # Don't require splitting before a comment, since it may be related to
        # the current line.
        return False

      # Allow the fallthrough code to handle the closing bracket.
      if current != opening.matching_bracket:
        # If the container doesn't fit in the current line, must split
        return not self._ContainerFitsOnStartLine(opening)

    if (self.stack[-1].split_before_closing_bracket and
        (current.value in '}]' and style.Get('SPLIT_BEFORE_CLOSING_BRACKET') or
         current.value in '}])' and style.Get('INDENT_CLOSING_BRACKETS'))):
      # Split before the closing bracket if we can.
      if subtypes.SUBSCRIPT_BRACKET not in current.subtypes:
        return current.node_split_penalty != split_penalty.UNBREAKABLE

    if (current.value == ')' and previous.value == ',' and
        not _IsSingleElementTuple(current.matching_bracket)):
      return True

    # Prevent splitting before the first argument in compound statements
    # with the exception of function declarations.
    if (style.Get('SPLIT_BEFORE_FIRST_ARGUMENT') and
        _IsCompoundStatement(self.line.first) and
        not _IsFunctionDef(self.line.first)):
      return False

    ###########################################################################
    # List Splitting
    if (style.Get('DEDENT_CLOSING_BRACKETS') or
        style.Get('INDENT_CLOSING_BRACKETS') or
        style.Get('SPLIT_BEFORE_FIRST_ARGUMENT')):
      bracket = current if current.ClosesScope() else previous
      if subtypes.SUBSCRIPT_BRACKET not in bracket.subtypes:
        if bracket.OpensScope():
          if style.Get('COALESCE_BRACKETS'):
            if current.OpensScope():
              # Prefer to keep all opening brackets together.
              return False

          if (not _IsLastScopeInLine(bracket) or
              logical_line.IsSurroundedByBrackets(bracket)):
            last_token = bracket.matching_bracket
          else:
            last_token = _LastTokenInLine(bracket.matching_bracket)

          if not self._FitsOnLine(bracket, last_token):
            # Split before the first element if the whole list can't fit on a
            # single line.
            self.stack[-1].split_before_closing_bracket = True
            return True

        elif (style.Get('DEDENT_CLOSING_BRACKETS') or
              style.Get('INDENT_CLOSING_BRACKETS')) and current.ClosesScope():
          # Split before and dedent the closing bracket.
          return self.stack[-1].split_before_closing_bracket

    if (style.Get('SPLIT_BEFORE_EXPRESSION_AFTER_OPENING_PAREN') and
        current.is_name):
      # An expression that's surrounded by parens gets split after the opening
      # parenthesis.
      def SurroundedByParens(token):
        """"""Check if it's an expression surrounded by parentheses.""""""
        while token:
          if token.value == ',':
            return False
          if token.value == ')':
            return not token.next_token
          if token.OpensScope():
            token = token.matching_bracket.next_token
          else:
            token = token.next_token
        return False

      if (previous.value == '(' and not previous.is_pseudo and
          not logical_line.IsSurroundedByBrackets(previous)):
        pptoken = previous.previous_token
        if (pptoken and not pptoken.is_name and not pptoken.is_keyword and
            SurroundedByParens(current)):
          return True

    if (current.is_name or current.is_string) and previous.value == ',':
      # If the list has function calls in it and the full list itself cannot
      # fit on the line, then we want to split. Otherwise, we'll get something
      # like this:
      #
      #     X = [
      #         Bar(xxx='some string',
      #             yyy='another long string',
      #             zzz='a third long string'), Bar(
      #                 xxx='some string',
      #                 yyy='another long string',
      #                 zzz='a third long string')
      #     ]
      #
      # or when a string formatting syntax.
      func_call_or_string_format = False
      tok = current.next_token
      if current.is_name:
        while tok and (tok.is_name or tok.value == '.'):
          tok = tok.next_token
        func_call_or_string_format = tok and tok.value == '('
      elif current.is_string:
        while tok and tok.is_string:
          tok = tok.next_token
        func_call_or_string_format = tok and tok.value == '%'
      if func_call_or_string_format:
        open_bracket = logical_line.IsSurroundedByBrackets(current)
        if open_bracket:
          if open_bracket.value in '[{':
            if not self._FitsOnLine(open_bracket,
                                    open_bracket.matching_bracket):
              return True
          elif tok.value == '(':
            if not self._FitsOnLine(current, tok.matching_bracket):
              return True

    if (current.OpensScope() and previous.value == ',' and
        subtypes.DICTIONARY_KEY not in current.next_token.subtypes):
      # If we have a list of tuples, then we can get a similar look as above. If
      # the full list cannot fit on the line, then we want a split.
      open_bracket = logical_line.IsSurroundedByBrackets(current)
      if (open_bracket and open_bracket.value in '[{' and
          subtypes.SUBSCRIPT_BRACKET not in open_bracket.subtypes):
        if not self._FitsOnLine(current, current.matching_bracket):
          return True

    ###########################################################################
    # Dict/Set Splitting
    if (style.Get('EACH_DICT_ENTRY_ON_SEPARATE_LINE') and
        subtypes.DICTIONARY_KEY in current.subtypes and not current.is_comment):
      # Place each dictionary entry onto its own line.
      if previous.value == '{' and previous.previous_token:
        opening = _GetOpeningBracket(previous.previous_token)
        if (opening and opening.value == '(' and opening.previous_token and
            opening.previous_token.is_name):
          # This is a dictionary that's an argument to a function.
          if (self._FitsOnLine(previous, previous.matching_bracket) and
              previous.matching_bracket.next_token and
              (not opening.matching_bracket.next_token or
               opening.matching_bracket.next_token.value != '.') and
              _ScopeHasNoCommas(previous)):
            # Don't split before the key if:
            #   - The dictionary fits on a line, and
            #   - The function call isn't part of a builder-style call and
            #   - The dictionary has one entry and no trailing comma
            return False
      return True

    if (style.Get('SPLIT_BEFORE_DICT_SET_GENERATOR') and
        subtypes.DICT_SET_GENERATOR in current.subtypes):
      # Split before a dict/set generator.
      return True

    if (subtypes.DICTIONARY_VALUE in current.subtypes or
        (previous.is_pseudo and previous.value == '(' and
         not current.is_comment)):
      # Split before the dictionary value if we can't fit every dictionary
      # entry on its own line.
      if not current.OpensScope():
        opening = _GetOpeningBracket(current)
        if not self._EachDictEntryFitsOnOneLine(opening):
          return style.Get('ALLOW_SPLIT_BEFORE_DICT_VALUE')

    if previous.value == '{':
      # Split if the dict/set cannot fit on one line and ends in a comma.
      closing = previous.matching_bracket
      if (not self._FitsOnLine(previous, closing) and
          closing.previous_token.value == ','):
        self.stack[-1].split_before_closing_bracket = True
        return True

    ###########################################################################
    # Argument List Splitting
    if (style.Get('SPLIT_BEFORE_NAMED_ASSIGNS') and not current.is_comment and
        subtypes.DEFAULT_OR_NAMED_ASSIGN_ARG_LIST in current.subtypes):
      if (previous.value not in {'=', ':', '*', '**'} and
          current.value not in ':=,)' and not _IsFunctionDefinition(previous)):
        # If we're going to split the lines because of named arguments, then we
        # want to split after the opening bracket as well. But not when this is
        # part of a function definition.
        if previous.value == '(':
          # Make sure we don't split after the opening bracket if the
          # continuation indent is greater than the opening bracket:
          #
          #  a(
          #      b=1,
          #      c=2)
          if (self._FitsOnLine(previous, previous.matching_bracket) and
              logical_line.IsSurroundedByBrackets(previous)):
            # An argument to a function is a function call with named
            # assigns.
            return False

          # Don't split if not required
          if (not style.Get('SPLIT_BEFORE_EXPRESSION_AFTER_OPENING_PAREN') and
              not style.Get('SPLIT_BEFORE_FIRST_ARGUMENT')):
            return False

          column = self.column - self.stack[-1].last_space
          return column > style.Get('CONTINUATION_INDENT_WIDTH')

        opening = _GetOpeningBracket(current)
        if opening:
          return not self._ContainerFitsOnStartLine(opening)

    if (current.value not in '{)' and previous.value == '(' and
        self._ArgumentListHasDictionaryEntry(current)):
      return True

    if style.Get('SPLIT_ARGUMENTS_WHEN_COMMA_TERMINATED'):
      # Split before arguments in a function call or definition if the
      # arguments are terminated by a comma.
      opening = _GetOpeningBracket(current)
      if opening and opening.previous_token and opening.previous_token.is_name:
        if previous.value in '(,':
          if opening.matching_bracket.previous_token.value == ',':
            return True

    if ((current.is_name or current.value in {'*', '**'}) and
        previous.value == ','):
      # If we have a function call within an argument list and it won't fit on
      # the remaining line, but it will fit on a line by itself, then go ahead
      # and split before the call.
      opening = _GetOpeningBracket(current)
      if (opening and opening.value == '(' and opening.previous_token and
          (opening.previous_token.is_name or
           opening.previous_token.value in {'*', '**'})):
        is_func_call = False
        opening = current
        while opening:
          if opening.value == '(':
            is_func_call = True
            break
          if (not (opening.is_name or opening.value in {'*', '**'}) and
              opening.value != '.'):
            break
          opening = opening.next_token

        if is_func_call:
          if (not self._FitsOnLine(current, opening.matching_bracket) or
              (opening.matching_bracket.next_token and
               opening.matching_bracket.next_token.value != ',' and
               not opening.matching_bracket.next_token.ClosesScope())):
            return True

    pprevious = previous.previous_token

    # A function call with a dictionary as its first argument may result in
    # unreadable formatting if the dictionary spans multiple lines. The
    # dictionary itself is formatted just fine, but the remaining arguments are
    # indented too far:
    #
    #     function_call({
    #         KEY_1: 'value one',
    #         KEY_2: 'value two',
    #     },
    #                   default=False)
    if (current.value == '{' and previous.value == '(' and pprevious and
        pprevious.is_name):
      dict_end = current.matching_bracket
      next_token = dict_end.next_token
      if next_token.value == ',' and not self._FitsOnLine(current, dict_end):
        return True

    if (current.is_name and pprevious and pprevious.is_name and
        previous.value == '('):

      if (not self._FitsOnLine(previous, previous.matching_bracket) and
          _IsFunctionCallWithArguments(current)):
        # There is a function call, with more than 1 argument, where the first
        # argument is itself a function call with arguments that does not fit
        # into the line.  In this specific case, if we split after the first
        # argument's opening '(', then the formatting will look bad for the
        # rest of the arguments. E.g.:
        #
        #     outer_function_call(inner_function_call(
        #         inner_arg1, inner_arg2),
        #                         outer_arg1, outer_arg2)
        #
        # Instead, enforce a split before that argument to keep things looking
        # good.
        if (style.Get('SPLIT_BEFORE_EXPRESSION_AFTER_OPENING_PAREN') or
            style.Get('SPLIT_BEFORE_FIRST_ARGUMENT')):
          return True

        opening = _GetOpeningBracket(current)
        if (opening and opening.value == '(' and opening.previous_token and
            (opening.previous_token.is_name or
             opening.previous_token.value in {'*', '**'})):
          is_func_call = False
          opening = current
          while opening:
            if opening.value == '(':
              is_func_call = True
              break
            if (not (opening.is_name or opening.value in {'*', '**'}) and
                opening.value != '.'):
              break
            opening = opening.next_token

          if is_func_call:
            if (not self._FitsOnLine(current, opening.matching_bracket) or
                (opening.matching_bracket.next_token and
                 opening.matching_bracket.next_token.value != ',' and
                 not opening.matching_bracket.next_token.ClosesScope())):
              return True

    if (previous.OpensScope() and not current.OpensScope() and
        not current.is_comment and
        subtypes.SUBSCRIPT_BRACKET not in previous.subtypes):
      if pprevious and not pprevious.is_keyword and not pprevious.is_name:
        # We want to split if there's a comment in the container.
        token = current
        while token != previous.matching_bracket:
          if token.is_comment:
            return True
          token = token.next_token
      if previous.value == '(':
        pptoken = previous.previous_token
        if not pptoken or not pptoken.is_name:
          # Split after the opening of a tuple if it doesn't fit on the current
          # line and it's not a function call.
          if self._FitsOnLine(previous, previous.matching_bracket):
            return False
        elif not self._FitsOnLine(previous, previous.matching_bracket):
          if len(previous.container_elements) == 1:
            return False

          elements = previous.container_elements + [previous.matching_bracket]
          i = 1
          while i < len(elements):
            if (not elements[i - 1].OpensScope() and
                not self._FitsOnLine(elements[i - 1], elements[i])):
              return True
            i += 1

          if (self.column_limit - self.column) / float(self.column_limit) < 0.3:
            # Try not to squish all of the arguments off to the right.
            return True
      else:
        # Split after the opening of a container if it doesn't fit on the
        # current line.
        if not self._FitsOnLine(previous, previous.matching_bracket):
          return True

    ###########################################################################
    # Original Formatting Splitting
    # These checks rely upon the original formatting. This is in order to
    # attempt to keep hand-written code in the same condition as it was before.
    # However, this may cause the formatter to fail to be idempotent.
    if (style.Get('SPLIT_BEFORE_BITWISE_OPERATOR') and current.value in '&|' and
        previous.lineno < current.lineno):
      # Retain the split before a bitwise operator.
      return True

    if (current.is_comment and
        previous.lineno < current.lineno - current.value.count('\n')):
      # If a comment comes in the middle of a logical line (like an if
      # conditional with comments interspersed), then we want to split if the
      # original comments were on a separate line.
      return True

    return False","self._FitsOnLine(elements[i - 1], elements[i])",self._FitsOnLine(*elements[i - 1:i + 1]),"iterable_zj[i - 1], iterable_zj[i]",*elements[i-1:i+1],*elements[i - 1:i + 1],1
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/cloud/clouds/opennebula.py,https://github.com/saltstack/salt/tree/master/salt/cloud/clouds/opennebula.py,,image_update$1764,"def image_update(call=None, kwargs=None):
    """"""
    Replaces the image template contents.

    .. versionadded:: 2016.3.0

    image_id
        The ID of the image to update. Can be used instead of ``image_name``.

    image_name
        The name of the image to update. Can be used instead of ``image_id``.

    path
        The path to a file containing the template of the image. Syntax within the
        file can be the usual attribute=value or XML. Can be used instead of ``data``.

    data
        Contains the template of the image. Syntax can be the usual attribute=value
        or XML. Can be used instead of ``path``.

    update_type
        There are two ways to update an image: ``replace`` the whole template
        or ``merge`` the new template with the existing one.

    CLI Example:

    .. code-block:: bash

        salt-cloud -f image_update opennebula image_id=0 file=/path/to/image_update_file.txt update_type=replace
        salt-cloud -f image_update opennebula image_name=""Ubuntu 14.04"" update_type=merge \\
            data='NAME=""Ubuntu Dev"" PATH=""/home/one_user/images/ubuntu_desktop.img"" \\
            DESCRIPTION = ""Ubuntu 14.04 for development.""'
    """"""
    if call != ""function"":
        raise SaltCloudSystemExit(
            ""The image_allocate function must be called with -f or --function.""
        )

    if kwargs is None:
        kwargs = {}

    image_id = kwargs.get(""image_id"", None)
    image_name = kwargs.get(""image_name"", None)
    path = kwargs.get(""path"", None)
    data = kwargs.get(""data"", None)
    update_type = kwargs.get(""update_type"", None)
    update_args = [""replace"", ""merge""]

    if update_type is None:
        raise SaltCloudSystemExit(
            ""The image_update function requires an 'update_type' to be provided.""
        )

    if update_type == update_args[0]:
        update_number = 0
    elif update_type == update_args[1]:
        update_number = 1
    else:
        raise SaltCloudSystemExit(
            ""The update_type argument must be either {} or {}."".format(
                update_args[0], update_args[1]
            )
        )

    if image_id:
        if image_name:
            log.warning(
                ""Both the 'image_id' and 'image_name' arguments were provided. ""
                ""'image_id' will take precedence.""
            )
    elif image_name:
        image_id = get_image_id(kwargs={""name"": image_name})
    else:
        raise SaltCloudSystemExit(
            ""The image_update function requires either an 'image_id' or an ""
            ""'image_name' to be provided.""
        )

    if data:
        if path:
            log.warning(
                ""Both the 'data' and 'path' arguments were provided. ""
                ""'data' will take precedence.""
            )
    elif path:
        with salt.utils.files.fopen(path, mode=""r"") as rfh:
            data = rfh.read()
    else:
        raise SaltCloudSystemExit(
            ""The image_update function requires either 'data' or a file 'path' ""
            ""to be provided.""
        )

    server, user, password = _get_xml_rpc()
    auth = "":"".join([user, password])
    response = server.one.image.update(auth, int(image_id), data, int(update_number))

    ret = {
        ""action"": ""image.update"",
        ""updated"": response[0],
        ""image_id"": response[1],
        ""error_code"": response[2],
    }

    return ret","'The update_type argument must be either {} or {}.'.format(update_args[0], update_args[1])",'The update_type argument must be either {} or {}.'.format(*update_args[:2]),"iterable_zj[0], iterable_zj[1]",*update_args[:2],*update_args[:2],1
lit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lit/lit_nlp/components/pdp.py,https://github.com/PAIR-code/lit/tree/master/lit_nlp/components/pdp.py,PdpInterpreter,run$56,"def run(self,
          inputs: List[types.JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[List[types.JsonDict]] = None,
          config: Optional[types.JsonDict] = None):
    """"""Create PDP chart info using provided inputs.

    Args:
      inputs: sequence of inputs, following model.input_spec()
      model: optional model to use to generate new examples.
      dataset: dataset which the current examples belong to.
      model_outputs: optional precomputed model outputs
      config: optional runtime config.

    Returns:
      a dict of alternate feature values to model outputs. The model
      outputs will be a number for regression models and a list of numbers for
      multiclass models.
    """"""

    pred_keys = utils.find_spec_keys(
        model.output_spec(), (types.MulticlassPreds, types.RegressionScore))
    if not pred_keys:
      logging.warning('PDP did not find any supported output fields.')
      return None

    assert 'feature' in config, 'No feature to test provided'
    feature = config['feature']
    provided_range = config['range'] if 'range' in config else []
    edited_outputs = {}
    for pred_key in pred_keys:
      edited_outputs[pred_key] = {}

    # If a range was provided, use that to create the possible values.
    vals_to_test = (
        np.linspace(provided_range[0], provided_range[1], 10)
        if len(provided_range) == 2
        else self.get_vals_to_test(feature, dataset))

    # If no specific inputs provided, use the entire dataset.
    inputs_to_use = inputs if inputs else dataset.examples

    # For each alternate value for a given feature.
    for new_val in vals_to_test:
      # Create copies of all provided inputs with the value replaced.
      edited_inputs = []
      for inp in inputs_to_use:
        edited_input = copy.deepcopy(inp)
        edited_input[feature] = new_val
        edited_inputs.append(edited_input)

      # Run prediction on the altered inputs.
      outputs = list(model.predict(edited_inputs))

      # Store the mean of the prediction for the alternate value.
      for pred_key in pred_keys:
        numeric = isinstance(
            model.output_spec()[pred_key], types.RegressionScore)
        if numeric:
          edited_outputs[pred_key][new_val] = np.mean(
              [output[pred_key] for output in outputs])
        else:
          edited_outputs[pred_key][new_val] = np.mean(
              [output[pred_key] for output in outputs], axis=0)

    return edited_outputs","np.linspace(provided_range[0], provided_range[1], 10)","np.linspace(*provided_range[:2], 10)","iterable_zj[0], iterable_zj[1]",*provided_range[:2],*provided_range[:2],1
augmix,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/augmix/third_party/WideResNet_pytorch/wideresnet.py,https://github.com/google-research/augmix/tree/master/third_party/WideResNet_pytorch/wideresnet.py,WideResNet,__init__$84,"def __init__(self, depth, num_classes, widen_factor=1, drop_rate=0.0):
    super(WideResNet, self).__init__()
    n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]
    assert (depth - 4) % 6 == 0
    n = (depth - 4) // 6
    block = BasicBlock
    # 1st conv before any network block
    self.conv1 = nn.Conv2d(
        3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=False)
    # 1st block
    self.block1 = NetworkBlock(n, n_channels[0], n_channels[1], block, 1,
                               drop_rate)
    # 2nd block
    self.block2 = NetworkBlock(n, n_channels[1], n_channels[2], block, 2,
                               drop_rate)
    # 3rd block
    self.block3 = NetworkBlock(n, n_channels[2], n_channels[3], block, 2,
                               drop_rate)
    # global average pooling and classifier
    self.bn1 = nn.BatchNorm2d(n_channels[3])
    self.relu = nn.ReLU(inplace=True)
    self.fc = nn.Linear(n_channels[3], num_classes)
    self.n_channels = n_channels[3]

    for m in self.modules():
      if isinstance(m, nn.Conv2d):
        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
        m.weight.data.normal_(0, math.sqrt(2. / n))
      elif isinstance(m, nn.BatchNorm2d):
        m.weight.data.fill_(1)
        m.bias.data.zero_()
      elif isinstance(m, nn.Linear):
        m.bias.data.zero_()","NetworkBlock(n, n_channels[0], n_channels[1], block, 1, drop_rate)","NetworkBlock(n, *n_channels[:2], block, 1, drop_rate)","iterable_zj[0], iterable_zj[1]",*n_channels[:2],*n_channels[:2],1
augmix,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/augmix/third_party/WideResNet_pytorch/wideresnet.py,https://github.com/google-research/augmix/tree/master/third_party/WideResNet_pytorch/wideresnet.py,WideResNet,__init__$84,"def __init__(self, depth, num_classes, widen_factor=1, drop_rate=0.0):
    super(WideResNet, self).__init__()
    n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]
    assert (depth - 4) % 6 == 0
    n = (depth - 4) // 6
    block = BasicBlock
    # 1st conv before any network block
    self.conv1 = nn.Conv2d(
        3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=False)
    # 1st block
    self.block1 = NetworkBlock(n, n_channels[0], n_channels[1], block, 1,
                               drop_rate)
    # 2nd block
    self.block2 = NetworkBlock(n, n_channels[1], n_channels[2], block, 2,
                               drop_rate)
    # 3rd block
    self.block3 = NetworkBlock(n, n_channels[2], n_channels[3], block, 2,
                               drop_rate)
    # global average pooling and classifier
    self.bn1 = nn.BatchNorm2d(n_channels[3])
    self.relu = nn.ReLU(inplace=True)
    self.fc = nn.Linear(n_channels[3], num_classes)
    self.n_channels = n_channels[3]

    for m in self.modules():
      if isinstance(m, nn.Conv2d):
        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
        m.weight.data.normal_(0, math.sqrt(2. / n))
      elif isinstance(m, nn.BatchNorm2d):
        m.weight.data.fill_(1)
        m.bias.data.zero_()
      elif isinstance(m, nn.Linear):
        m.bias.data.zero_()","NetworkBlock(n, n_channels[1], n_channels[2], block, 2, drop_rate)","NetworkBlock(n, *n_channels[1:3], block, 2, drop_rate)","iterable_zj[1], iterable_zj[2]",*n_channels[1:3],*n_channels[1:3],1
augmix,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/augmix/third_party/WideResNet_pytorch/wideresnet.py,https://github.com/google-research/augmix/tree/master/third_party/WideResNet_pytorch/wideresnet.py,WideResNet,__init__$84,"def __init__(self, depth, num_classes, widen_factor=1, drop_rate=0.0):
    super(WideResNet, self).__init__()
    n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]
    assert (depth - 4) % 6 == 0
    n = (depth - 4) // 6
    block = BasicBlock
    # 1st conv before any network block
    self.conv1 = nn.Conv2d(
        3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=False)
    # 1st block
    self.block1 = NetworkBlock(n, n_channels[0], n_channels[1], block, 1,
                               drop_rate)
    # 2nd block
    self.block2 = NetworkBlock(n, n_channels[1], n_channels[2], block, 2,
                               drop_rate)
    # 3rd block
    self.block3 = NetworkBlock(n, n_channels[2], n_channels[3], block, 2,
                               drop_rate)
    # global average pooling and classifier
    self.bn1 = nn.BatchNorm2d(n_channels[3])
    self.relu = nn.ReLU(inplace=True)
    self.fc = nn.Linear(n_channels[3], num_classes)
    self.n_channels = n_channels[3]

    for m in self.modules():
      if isinstance(m, nn.Conv2d):
        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
        m.weight.data.normal_(0, math.sqrt(2. / n))
      elif isinstance(m, nn.BatchNorm2d):
        m.weight.data.fill_(1)
        m.bias.data.zero_()
      elif isinstance(m, nn.Linear):
        m.bias.data.zero_()","NetworkBlock(n, n_channels[2], n_channels[3], block, 2, drop_rate)","NetworkBlock(n, *n_channels[2:4], block, 2, drop_rate)","iterable_zj[2], iterable_zj[3]",*n_channels[2:4],*n_channels[2:4],1
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/signal/_lti_conversion.py,https://github.com/scipy/scipy/tree/master/scipy/signal/_lti_conversion.py,,cont2discrete$335,"def cont2discrete(system, dt, method=""zoh"", alpha=None):
    """"""
    Transform a continuous to a discrete state-space system.

    Parameters
    ----------
    system : a tuple describing the system or an instance of `lti`
        The following gives the number of elements in the tuple and
        the interpretation:

            * 1: (instance of `lti`)
            * 2: (num, den)
            * 3: (zeros, poles, gain)
            * 4: (A, B, C, D)

    dt : float
        The discretization time step.
    method : str, optional
        Which method to use:

            * gbt: generalized bilinear transformation
            * bilinear: Tustin's approximation (""gbt"" with alpha=0.5)
            * euler: Euler (or forward differencing) method (""gbt"" with alpha=0)
            * backward_diff: Backwards differencing (""gbt"" with alpha=1.0)
            * zoh: zero-order hold (default)
            * foh: first-order hold (*versionadded: 1.3.0*)
            * impulse: equivalent impulse response (*versionadded: 1.3.0*)

    alpha : float within [0, 1], optional
        The generalized bilinear transformation weighting parameter, which
        should only be specified with method=""gbt"", and is ignored otherwise

    Returns
    -------
    sysd : tuple containing the discrete system
        Based on the input type, the output will be of the form

        * (num, den, dt)   for transfer function input
        * (zeros, poles, gain, dt)   for zeros-poles-gain input
        * (A, B, C, D, dt) for state-space system input

    Notes
    -----
    By default, the routine uses a Zero-Order Hold (zoh) method to perform
    the transformation. Alternatively, a generalized bilinear transformation
    may be used, which includes the common Tustin's bilinear approximation,
    an Euler's method technique, or a backwards differencing technique.

    The Zero-Order Hold (zoh) method is based on [1]_, the generalized bilinear
    approximation is based on [2]_ and [3]_, the First-Order Hold (foh) method
    is based on [4]_.

    Examples
    --------
    We can transform a continuous state-space system to a discrete one:

    >>> import matplotlib.pyplot as plt
    >>> from scipy.signal import cont2discrete, lti, dlti, dstep

    Define a continuous state-space system.

    >>> A = np.array([[0, 1],[-10., -3]])
    >>> B = np.array([[0],[10.]])
    >>> C = np.array([[1., 0]])
    >>> D = np.array([[0.]])
    >>> l_system = lti(A, B, C, D)
    >>> t, x = l_system.step(T=np.linspace(0, 5, 100))
    >>> fig, ax = plt.subplots()
    >>> ax.plot(t, x, label='Continuous', linewidth=3)

    Transform it to a discrete state-space system using several methods.

    >>> dt = 0.1
    >>> for method in ['zoh', 'bilinear', 'euler', 'backward_diff', 'foh', 'impulse']:
    ...    d_system = cont2discrete((A, B, C, D), dt, method=method)
    ...    s, x_d = dstep(d_system)
    ...    ax.step(s, np.squeeze(x_d), label=method, where='post')
    >>> ax.axis([t[0], t[-1], x[0], 1.4])
    >>> ax.legend(loc='best')
    >>> fig.tight_layout()
    >>> plt.show()

    References
    ----------
    .. [1] https://en.wikipedia.org/wiki/Discretization#Discretization_of_linear_state_space_models

    .. [2] http://techteach.no/publications/discretetime_signals_systems/discrete.pdf

    .. [3] G. Zhang, X. Chen, and T. Chen, Digital redesign via the generalized
        bilinear transformation, Int. J. Control, vol. 82, no. 4, pp. 741-754,
        2009.
        (https://www.mypolyuweb.hk/~magzhang/Research/ZCC09_IJC.pdf)

    .. [4] G. F. Franklin, J. D. Powell, and M. L. Workman, Digital control
        of dynamic systems, 3rd ed. Menlo Park, Calif: Addison-Wesley,
        pp. 204-206, 1998.

    """"""
    if len(system) == 1:
        return system.to_discrete()
    if len(system) == 2:
        sysd = cont2discrete(tf2ss(system[0], system[1]), dt, method=method,
                             alpha=alpha)
        return ss2tf(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)
    elif len(system) == 3:
        sysd = cont2discrete(zpk2ss(system[0], system[1], system[2]), dt,
                             method=method, alpha=alpha)
        return ss2zpk(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)
    elif len(system) == 4:
        a, b, c, d = system
    else:
        raise ValueError(""First argument must either be a tuple of 2 (tf), ""
                         ""3 (zpk), or 4 (ss) arrays."")

    if method == 'gbt':
        if alpha is None:
            raise ValueError(""Alpha parameter must be specified for the ""
                             ""generalized bilinear transform (gbt) method"")
        elif alpha < 0 or alpha > 1:
            raise ValueError(""Alpha parameter must be within the interval ""
                             ""[0,1] for the gbt method"")

    if method == 'gbt':
        # This parameter is used repeatedly - compute once here
        ima = np.eye(a.shape[0]) - alpha*dt*a
        ad = linalg.solve(ima, np.eye(a.shape[0]) + (1.0-alpha)*dt*a)
        bd = linalg.solve(ima, dt*b)

        # Similarly solve for the output equation matrices
        cd = linalg.solve(ima.transpose(), c.transpose())
        cd = cd.transpose()
        dd = d + alpha*np.dot(c, bd)

    elif method == 'bilinear' or method == 'tustin':
        return cont2discrete(system, dt, method=""gbt"", alpha=0.5)

    elif method == 'euler' or method == 'forward_diff':
        return cont2discrete(system, dt, method=""gbt"", alpha=0.0)

    elif method == 'backward_diff':
        return cont2discrete(system, dt, method=""gbt"", alpha=1.0)

    elif method == 'zoh':
        # Build an exponential matrix
        em_upper = np.hstack((a, b))

        # Need to stack zeros under the a and b matrices
        em_lower = np.hstack((np.zeros((b.shape[1], a.shape[0])),
                              np.zeros((b.shape[1], b.shape[1]))))

        em = np.vstack((em_upper, em_lower))
        ms = linalg.expm(dt * em)

        # Dispose of the lower rows
        ms = ms[:a.shape[0], :]

        ad = ms[:, 0:a.shape[1]]
        bd = ms[:, a.shape[1]:]

        cd = c
        dd = d

    elif method == 'foh':
        # Size parameters for convenience
        n = a.shape[0]
        m = b.shape[1]

        # Build an exponential matrix similar to 'zoh' method
        em_upper = linalg.block_diag(np.block([a, b]) * dt, np.eye(m))
        em_lower = zeros((m, n + 2 * m))
        em = np.block([[em_upper], [em_lower]])

        ms = linalg.expm(em)

        # Get the three blocks from upper rows
        ms11 = ms[:n, 0:n]
        ms12 = ms[:n, n:n + m]
        ms13 = ms[:n, n + m:]

        ad = ms11
        bd = ms12 - ms13 + ms11 @ ms13
        cd = c
        dd = d + c @ ms13

    elif method == 'impulse':
        if not np.allclose(d, 0):
            raise ValueError(""Impulse method is only applicable""
                             ""to strictly proper systems"")

        ad = linalg.expm(a * dt)
        bd = ad @ b * dt
        cd = c
        dd = c @ b * dt

    else:
        raise ValueError(""Unknown transformation method '%s'"" % method)

    return ad, bd, cd, dd, dt","tf2ss(system[0], system[1])",tf2ss(*system[:2]),"iterable_zj[0], iterable_zj[1]",*system[:2],*system[:2],1
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/signal/_lti_conversion.py,https://github.com/scipy/scipy/tree/master/scipy/signal/_lti_conversion.py,,cont2discrete$335,"def cont2discrete(system, dt, method=""zoh"", alpha=None):
    """"""
    Transform a continuous to a discrete state-space system.

    Parameters
    ----------
    system : a tuple describing the system or an instance of `lti`
        The following gives the number of elements in the tuple and
        the interpretation:

            * 1: (instance of `lti`)
            * 2: (num, den)
            * 3: (zeros, poles, gain)
            * 4: (A, B, C, D)

    dt : float
        The discretization time step.
    method : str, optional
        Which method to use:

            * gbt: generalized bilinear transformation
            * bilinear: Tustin's approximation (""gbt"" with alpha=0.5)
            * euler: Euler (or forward differencing) method (""gbt"" with alpha=0)
            * backward_diff: Backwards differencing (""gbt"" with alpha=1.0)
            * zoh: zero-order hold (default)
            * foh: first-order hold (*versionadded: 1.3.0*)
            * impulse: equivalent impulse response (*versionadded: 1.3.0*)

    alpha : float within [0, 1], optional
        The generalized bilinear transformation weighting parameter, which
        should only be specified with method=""gbt"", and is ignored otherwise

    Returns
    -------
    sysd : tuple containing the discrete system
        Based on the input type, the output will be of the form

        * (num, den, dt)   for transfer function input
        * (zeros, poles, gain, dt)   for zeros-poles-gain input
        * (A, B, C, D, dt) for state-space system input

    Notes
    -----
    By default, the routine uses a Zero-Order Hold (zoh) method to perform
    the transformation. Alternatively, a generalized bilinear transformation
    may be used, which includes the common Tustin's bilinear approximation,
    an Euler's method technique, or a backwards differencing technique.

    The Zero-Order Hold (zoh) method is based on [1]_, the generalized bilinear
    approximation is based on [2]_ and [3]_, the First-Order Hold (foh) method
    is based on [4]_.

    Examples
    --------
    We can transform a continuous state-space system to a discrete one:

    >>> import matplotlib.pyplot as plt
    >>> from scipy.signal import cont2discrete, lti, dlti, dstep

    Define a continuous state-space system.

    >>> A = np.array([[0, 1],[-10., -3]])
    >>> B = np.array([[0],[10.]])
    >>> C = np.array([[1., 0]])
    >>> D = np.array([[0.]])
    >>> l_system = lti(A, B, C, D)
    >>> t, x = l_system.step(T=np.linspace(0, 5, 100))
    >>> fig, ax = plt.subplots()
    >>> ax.plot(t, x, label='Continuous', linewidth=3)

    Transform it to a discrete state-space system using several methods.

    >>> dt = 0.1
    >>> for method in ['zoh', 'bilinear', 'euler', 'backward_diff', 'foh', 'impulse']:
    ...    d_system = cont2discrete((A, B, C, D), dt, method=method)
    ...    s, x_d = dstep(d_system)
    ...    ax.step(s, np.squeeze(x_d), label=method, where='post')
    >>> ax.axis([t[0], t[-1], x[0], 1.4])
    >>> ax.legend(loc='best')
    >>> fig.tight_layout()
    >>> plt.show()

    References
    ----------
    .. [1] https://en.wikipedia.org/wiki/Discretization#Discretization_of_linear_state_space_models

    .. [2] http://techteach.no/publications/discretetime_signals_systems/discrete.pdf

    .. [3] G. Zhang, X. Chen, and T. Chen, Digital redesign via the generalized
        bilinear transformation, Int. J. Control, vol. 82, no. 4, pp. 741-754,
        2009.
        (https://www.mypolyuweb.hk/~magzhang/Research/ZCC09_IJC.pdf)

    .. [4] G. F. Franklin, J. D. Powell, and M. L. Workman, Digital control
        of dynamic systems, 3rd ed. Menlo Park, Calif: Addison-Wesley,
        pp. 204-206, 1998.

    """"""
    if len(system) == 1:
        return system.to_discrete()
    if len(system) == 2:
        sysd = cont2discrete(tf2ss(system[0], system[1]), dt, method=method,
                             alpha=alpha)
        return ss2tf(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)
    elif len(system) == 3:
        sysd = cont2discrete(zpk2ss(system[0], system[1], system[2]), dt,
                             method=method, alpha=alpha)
        return ss2zpk(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)
    elif len(system) == 4:
        a, b, c, d = system
    else:
        raise ValueError(""First argument must either be a tuple of 2 (tf), ""
                         ""3 (zpk), or 4 (ss) arrays."")

    if method == 'gbt':
        if alpha is None:
            raise ValueError(""Alpha parameter must be specified for the ""
                             ""generalized bilinear transform (gbt) method"")
        elif alpha < 0 or alpha > 1:
            raise ValueError(""Alpha parameter must be within the interval ""
                             ""[0,1] for the gbt method"")

    if method == 'gbt':
        # This parameter is used repeatedly - compute once here
        ima = np.eye(a.shape[0]) - alpha*dt*a
        ad = linalg.solve(ima, np.eye(a.shape[0]) + (1.0-alpha)*dt*a)
        bd = linalg.solve(ima, dt*b)

        # Similarly solve for the output equation matrices
        cd = linalg.solve(ima.transpose(), c.transpose())
        cd = cd.transpose()
        dd = d + alpha*np.dot(c, bd)

    elif method == 'bilinear' or method == 'tustin':
        return cont2discrete(system, dt, method=""gbt"", alpha=0.5)

    elif method == 'euler' or method == 'forward_diff':
        return cont2discrete(system, dt, method=""gbt"", alpha=0.0)

    elif method == 'backward_diff':
        return cont2discrete(system, dt, method=""gbt"", alpha=1.0)

    elif method == 'zoh':
        # Build an exponential matrix
        em_upper = np.hstack((a, b))

        # Need to stack zeros under the a and b matrices
        em_lower = np.hstack((np.zeros((b.shape[1], a.shape[0])),
                              np.zeros((b.shape[1], b.shape[1]))))

        em = np.vstack((em_upper, em_lower))
        ms = linalg.expm(dt * em)

        # Dispose of the lower rows
        ms = ms[:a.shape[0], :]

        ad = ms[:, 0:a.shape[1]]
        bd = ms[:, a.shape[1]:]

        cd = c
        dd = d

    elif method == 'foh':
        # Size parameters for convenience
        n = a.shape[0]
        m = b.shape[1]

        # Build an exponential matrix similar to 'zoh' method
        em_upper = linalg.block_diag(np.block([a, b]) * dt, np.eye(m))
        em_lower = zeros((m, n + 2 * m))
        em = np.block([[em_upper], [em_lower]])

        ms = linalg.expm(em)

        # Get the three blocks from upper rows
        ms11 = ms[:n, 0:n]
        ms12 = ms[:n, n:n + m]
        ms13 = ms[:n, n + m:]

        ad = ms11
        bd = ms12 - ms13 + ms11 @ ms13
        cd = c
        dd = d + c @ ms13

    elif method == 'impulse':
        if not np.allclose(d, 0):
            raise ValueError(""Impulse method is only applicable""
                             ""to strictly proper systems"")

        ad = linalg.expm(a * dt)
        bd = ad @ b * dt
        cd = c
        dd = c @ b * dt

    else:
        raise ValueError(""Unknown transformation method '%s'"" % method)

    return ad, bd, cd, dd, dt","ss2tf(sysd[0], sysd[1], sysd[2], sysd[3])","ss2tf(*sysd[:3], sysd[3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*sysd[:3],*sysd[:4],0
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/signal/_lti_conversion.py,https://github.com/scipy/scipy/tree/master/scipy/signal/_lti_conversion.py,,cont2discrete$335,"def cont2discrete(system, dt, method=""zoh"", alpha=None):
    """"""
    Transform a continuous to a discrete state-space system.

    Parameters
    ----------
    system : a tuple describing the system or an instance of `lti`
        The following gives the number of elements in the tuple and
        the interpretation:

            * 1: (instance of `lti`)
            * 2: (num, den)
            * 3: (zeros, poles, gain)
            * 4: (A, B, C, D)

    dt : float
        The discretization time step.
    method : str, optional
        Which method to use:

            * gbt: generalized bilinear transformation
            * bilinear: Tustin's approximation (""gbt"" with alpha=0.5)
            * euler: Euler (or forward differencing) method (""gbt"" with alpha=0)
            * backward_diff: Backwards differencing (""gbt"" with alpha=1.0)
            * zoh: zero-order hold (default)
            * foh: first-order hold (*versionadded: 1.3.0*)
            * impulse: equivalent impulse response (*versionadded: 1.3.0*)

    alpha : float within [0, 1], optional
        The generalized bilinear transformation weighting parameter, which
        should only be specified with method=""gbt"", and is ignored otherwise

    Returns
    -------
    sysd : tuple containing the discrete system
        Based on the input type, the output will be of the form

        * (num, den, dt)   for transfer function input
        * (zeros, poles, gain, dt)   for zeros-poles-gain input
        * (A, B, C, D, dt) for state-space system input

    Notes
    -----
    By default, the routine uses a Zero-Order Hold (zoh) method to perform
    the transformation. Alternatively, a generalized bilinear transformation
    may be used, which includes the common Tustin's bilinear approximation,
    an Euler's method technique, or a backwards differencing technique.

    The Zero-Order Hold (zoh) method is based on [1]_, the generalized bilinear
    approximation is based on [2]_ and [3]_, the First-Order Hold (foh) method
    is based on [4]_.

    Examples
    --------
    We can transform a continuous state-space system to a discrete one:

    >>> import matplotlib.pyplot as plt
    >>> from scipy.signal import cont2discrete, lti, dlti, dstep

    Define a continuous state-space system.

    >>> A = np.array([[0, 1],[-10., -3]])
    >>> B = np.array([[0],[10.]])
    >>> C = np.array([[1., 0]])
    >>> D = np.array([[0.]])
    >>> l_system = lti(A, B, C, D)
    >>> t, x = l_system.step(T=np.linspace(0, 5, 100))
    >>> fig, ax = plt.subplots()
    >>> ax.plot(t, x, label='Continuous', linewidth=3)

    Transform it to a discrete state-space system using several methods.

    >>> dt = 0.1
    >>> for method in ['zoh', 'bilinear', 'euler', 'backward_diff', 'foh', 'impulse']:
    ...    d_system = cont2discrete((A, B, C, D), dt, method=method)
    ...    s, x_d = dstep(d_system)
    ...    ax.step(s, np.squeeze(x_d), label=method, where='post')
    >>> ax.axis([t[0], t[-1], x[0], 1.4])
    >>> ax.legend(loc='best')
    >>> fig.tight_layout()
    >>> plt.show()

    References
    ----------
    .. [1] https://en.wikipedia.org/wiki/Discretization#Discretization_of_linear_state_space_models

    .. [2] http://techteach.no/publications/discretetime_signals_systems/discrete.pdf

    .. [3] G. Zhang, X. Chen, and T. Chen, Digital redesign via the generalized
        bilinear transformation, Int. J. Control, vol. 82, no. 4, pp. 741-754,
        2009.
        (https://www.mypolyuweb.hk/~magzhang/Research/ZCC09_IJC.pdf)

    .. [4] G. F. Franklin, J. D. Powell, and M. L. Workman, Digital control
        of dynamic systems, 3rd ed. Menlo Park, Calif: Addison-Wesley,
        pp. 204-206, 1998.

    """"""
    if len(system) == 1:
        return system.to_discrete()
    if len(system) == 2:
        sysd = cont2discrete(tf2ss(system[0], system[1]), dt, method=method,
                             alpha=alpha)
        return ss2tf(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)
    elif len(system) == 3:
        sysd = cont2discrete(zpk2ss(system[0], system[1], system[2]), dt,
                             method=method, alpha=alpha)
        return ss2zpk(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)
    elif len(system) == 4:
        a, b, c, d = system
    else:
        raise ValueError(""First argument must either be a tuple of 2 (tf), ""
                         ""3 (zpk), or 4 (ss) arrays."")

    if method == 'gbt':
        if alpha is None:
            raise ValueError(""Alpha parameter must be specified for the ""
                             ""generalized bilinear transform (gbt) method"")
        elif alpha < 0 or alpha > 1:
            raise ValueError(""Alpha parameter must be within the interval ""
                             ""[0,1] for the gbt method"")

    if method == 'gbt':
        # This parameter is used repeatedly - compute once here
        ima = np.eye(a.shape[0]) - alpha*dt*a
        ad = linalg.solve(ima, np.eye(a.shape[0]) + (1.0-alpha)*dt*a)
        bd = linalg.solve(ima, dt*b)

        # Similarly solve for the output equation matrices
        cd = linalg.solve(ima.transpose(), c.transpose())
        cd = cd.transpose()
        dd = d + alpha*np.dot(c, bd)

    elif method == 'bilinear' or method == 'tustin':
        return cont2discrete(system, dt, method=""gbt"", alpha=0.5)

    elif method == 'euler' or method == 'forward_diff':
        return cont2discrete(system, dt, method=""gbt"", alpha=0.0)

    elif method == 'backward_diff':
        return cont2discrete(system, dt, method=""gbt"", alpha=1.0)

    elif method == 'zoh':
        # Build an exponential matrix
        em_upper = np.hstack((a, b))

        # Need to stack zeros under the a and b matrices
        em_lower = np.hstack((np.zeros((b.shape[1], a.shape[0])),
                              np.zeros((b.shape[1], b.shape[1]))))

        em = np.vstack((em_upper, em_lower))
        ms = linalg.expm(dt * em)

        # Dispose of the lower rows
        ms = ms[:a.shape[0], :]

        ad = ms[:, 0:a.shape[1]]
        bd = ms[:, a.shape[1]:]

        cd = c
        dd = d

    elif method == 'foh':
        # Size parameters for convenience
        n = a.shape[0]
        m = b.shape[1]

        # Build an exponential matrix similar to 'zoh' method
        em_upper = linalg.block_diag(np.block([a, b]) * dt, np.eye(m))
        em_lower = zeros((m, n + 2 * m))
        em = np.block([[em_upper], [em_lower]])

        ms = linalg.expm(em)

        # Get the three blocks from upper rows
        ms11 = ms[:n, 0:n]
        ms12 = ms[:n, n:n + m]
        ms13 = ms[:n, n + m:]

        ad = ms11
        bd = ms12 - ms13 + ms11 @ ms13
        cd = c
        dd = d + c @ ms13

    elif method == 'impulse':
        if not np.allclose(d, 0):
            raise ValueError(""Impulse method is only applicable""
                             ""to strictly proper systems"")

        ad = linalg.expm(a * dt)
        bd = ad @ b * dt
        cd = c
        dd = c @ b * dt

    else:
        raise ValueError(""Unknown transformation method '%s'"" % method)

    return ad, bd, cd, dd, dt","zpk2ss(system[0], system[1], system[2])",zpk2ss(*system[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*system[:3],*system[:3],1
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/signal/_lti_conversion.py,https://github.com/scipy/scipy/tree/master/scipy/signal/_lti_conversion.py,,cont2discrete$335,"def cont2discrete(system, dt, method=""zoh"", alpha=None):
    """"""
    Transform a continuous to a discrete state-space system.

    Parameters
    ----------
    system : a tuple describing the system or an instance of `lti`
        The following gives the number of elements in the tuple and
        the interpretation:

            * 1: (instance of `lti`)
            * 2: (num, den)
            * 3: (zeros, poles, gain)
            * 4: (A, B, C, D)

    dt : float
        The discretization time step.
    method : str, optional
        Which method to use:

            * gbt: generalized bilinear transformation
            * bilinear: Tustin's approximation (""gbt"" with alpha=0.5)
            * euler: Euler (or forward differencing) method (""gbt"" with alpha=0)
            * backward_diff: Backwards differencing (""gbt"" with alpha=1.0)
            * zoh: zero-order hold (default)
            * foh: first-order hold (*versionadded: 1.3.0*)
            * impulse: equivalent impulse response (*versionadded: 1.3.0*)

    alpha : float within [0, 1], optional
        The generalized bilinear transformation weighting parameter, which
        should only be specified with method=""gbt"", and is ignored otherwise

    Returns
    -------
    sysd : tuple containing the discrete system
        Based on the input type, the output will be of the form

        * (num, den, dt)   for transfer function input
        * (zeros, poles, gain, dt)   for zeros-poles-gain input
        * (A, B, C, D, dt) for state-space system input

    Notes
    -----
    By default, the routine uses a Zero-Order Hold (zoh) method to perform
    the transformation. Alternatively, a generalized bilinear transformation
    may be used, which includes the common Tustin's bilinear approximation,
    an Euler's method technique, or a backwards differencing technique.

    The Zero-Order Hold (zoh) method is based on [1]_, the generalized bilinear
    approximation is based on [2]_ and [3]_, the First-Order Hold (foh) method
    is based on [4]_.

    Examples
    --------
    We can transform a continuous state-space system to a discrete one:

    >>> import matplotlib.pyplot as plt
    >>> from scipy.signal import cont2discrete, lti, dlti, dstep

    Define a continuous state-space system.

    >>> A = np.array([[0, 1],[-10., -3]])
    >>> B = np.array([[0],[10.]])
    >>> C = np.array([[1., 0]])
    >>> D = np.array([[0.]])
    >>> l_system = lti(A, B, C, D)
    >>> t, x = l_system.step(T=np.linspace(0, 5, 100))
    >>> fig, ax = plt.subplots()
    >>> ax.plot(t, x, label='Continuous', linewidth=3)

    Transform it to a discrete state-space system using several methods.

    >>> dt = 0.1
    >>> for method in ['zoh', 'bilinear', 'euler', 'backward_diff', 'foh', 'impulse']:
    ...    d_system = cont2discrete((A, B, C, D), dt, method=method)
    ...    s, x_d = dstep(d_system)
    ...    ax.step(s, np.squeeze(x_d), label=method, where='post')
    >>> ax.axis([t[0], t[-1], x[0], 1.4])
    >>> ax.legend(loc='best')
    >>> fig.tight_layout()
    >>> plt.show()

    References
    ----------
    .. [1] https://en.wikipedia.org/wiki/Discretization#Discretization_of_linear_state_space_models

    .. [2] http://techteach.no/publications/discretetime_signals_systems/discrete.pdf

    .. [3] G. Zhang, X. Chen, and T. Chen, Digital redesign via the generalized
        bilinear transformation, Int. J. Control, vol. 82, no. 4, pp. 741-754,
        2009.
        (https://www.mypolyuweb.hk/~magzhang/Research/ZCC09_IJC.pdf)

    .. [4] G. F. Franklin, J. D. Powell, and M. L. Workman, Digital control
        of dynamic systems, 3rd ed. Menlo Park, Calif: Addison-Wesley,
        pp. 204-206, 1998.

    """"""
    if len(system) == 1:
        return system.to_discrete()
    if len(system) == 2:
        sysd = cont2discrete(tf2ss(system[0], system[1]), dt, method=method,
                             alpha=alpha)
        return ss2tf(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)
    elif len(system) == 3:
        sysd = cont2discrete(zpk2ss(system[0], system[1], system[2]), dt,
                             method=method, alpha=alpha)
        return ss2zpk(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)
    elif len(system) == 4:
        a, b, c, d = system
    else:
        raise ValueError(""First argument must either be a tuple of 2 (tf), ""
                         ""3 (zpk), or 4 (ss) arrays."")

    if method == 'gbt':
        if alpha is None:
            raise ValueError(""Alpha parameter must be specified for the ""
                             ""generalized bilinear transform (gbt) method"")
        elif alpha < 0 or alpha > 1:
            raise ValueError(""Alpha parameter must be within the interval ""
                             ""[0,1] for the gbt method"")

    if method == 'gbt':
        # This parameter is used repeatedly - compute once here
        ima = np.eye(a.shape[0]) - alpha*dt*a
        ad = linalg.solve(ima, np.eye(a.shape[0]) + (1.0-alpha)*dt*a)
        bd = linalg.solve(ima, dt*b)

        # Similarly solve for the output equation matrices
        cd = linalg.solve(ima.transpose(), c.transpose())
        cd = cd.transpose()
        dd = d + alpha*np.dot(c, bd)

    elif method == 'bilinear' or method == 'tustin':
        return cont2discrete(system, dt, method=""gbt"", alpha=0.5)

    elif method == 'euler' or method == 'forward_diff':
        return cont2discrete(system, dt, method=""gbt"", alpha=0.0)

    elif method == 'backward_diff':
        return cont2discrete(system, dt, method=""gbt"", alpha=1.0)

    elif method == 'zoh':
        # Build an exponential matrix
        em_upper = np.hstack((a, b))

        # Need to stack zeros under the a and b matrices
        em_lower = np.hstack((np.zeros((b.shape[1], a.shape[0])),
                              np.zeros((b.shape[1], b.shape[1]))))

        em = np.vstack((em_upper, em_lower))
        ms = linalg.expm(dt * em)

        # Dispose of the lower rows
        ms = ms[:a.shape[0], :]

        ad = ms[:, 0:a.shape[1]]
        bd = ms[:, a.shape[1]:]

        cd = c
        dd = d

    elif method == 'foh':
        # Size parameters for convenience
        n = a.shape[0]
        m = b.shape[1]

        # Build an exponential matrix similar to 'zoh' method
        em_upper = linalg.block_diag(np.block([a, b]) * dt, np.eye(m))
        em_lower = zeros((m, n + 2 * m))
        em = np.block([[em_upper], [em_lower]])

        ms = linalg.expm(em)

        # Get the three blocks from upper rows
        ms11 = ms[:n, 0:n]
        ms12 = ms[:n, n:n + m]
        ms13 = ms[:n, n + m:]

        ad = ms11
        bd = ms12 - ms13 + ms11 @ ms13
        cd = c
        dd = d + c @ ms13

    elif method == 'impulse':
        if not np.allclose(d, 0):
            raise ValueError(""Impulse method is only applicable""
                             ""to strictly proper systems"")

        ad = linalg.expm(a * dt)
        bd = ad @ b * dt
        cd = c
        dd = c @ b * dt

    else:
        raise ValueError(""Unknown transformation method '%s'"" % method)

    return ad, bd, cd, dd, dt","ss2zpk(sysd[0], sysd[1], sysd[2], sysd[3])","ss2zpk(*sysd[:3], sysd[3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*sysd[:3],*sysd[:4],0
grafana-backup-tool,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/grafana-backup-tool/grafana_backup/delete_datasources.py,https://github.com/ysde/grafana-backup-tool/tree/master/grafana_backup/delete_datasources.py,,get_all_datasources_and_delete$18,"def get_all_datasources_and_delete(grafana_url, http_get_headers, verify_ssl, client_cert, debug, pretty_print, uid_support):
    status_code_and_content = search_datasource(grafana_url, http_get_headers, verify_ssl, client_cert, debug)
    if status_code_and_content[0] == 200:
        datasources = status_code_and_content[1]
        print(""There are {0} datasources:"".format(len(datasources)))
        for datasource in datasources:
            print(datasource)
            if uid_support:
                status = delete_datasource_by_uid(datasource['uid'], grafana_url, http_get_headers, verify_ssl, client_cert, debug)
            else:
                status = delete_datasource_by_id(datasource['id'], grafana_url, http_get_headers, verify_ssl, client_cert, debug)

            if status == 200:
                print(""datasource:{0} is deleted"".format(datasource['name']))
            else:
                print(""deleting of datasource {0} failed with: {1}"".format(datasource['name'], status))
    else:
        print(""query datasource failed, status: {0}, msg: {1}"".format(status_code_and_content[0],
                                                                      status_code_and_content[1]))","'query datasource failed, status: {0}, msg: {1}'.format(status_code_and_content[0], status_code_and_content[1])","'query datasource failed, status: {0}, msg: {1}'.format(*status_code_and_content[:2])","iterable_zj[0], iterable_zj[1]",*status_code_and_content[:2],*status_code_and_content[:2],1
singleshotpose,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/singleshotpose/multi_obj_pose_estimation/darknet_multi.py,https://github.com/microsoft/singleshotpose/tree/master/multi_obj_pose_estimation/darknet_multi.py,Darknet,load_weights_until_last$293,"def load_weights_until_last(self, weightfile):
        fp = open(weightfile, 'rb')
        header = np.fromfile(fp, count=4, dtype=np.int32)
        self.header = torch.from_numpy(header)
        self.seen = self.header[3]
        buf = np.fromfile(fp, dtype = np.float32)
        fp.close()

        start = 0
        ind = -2
        blocklen = len(self.blocks)
        for i in range(blocklen-2):
            block = self.blocks[i]
            if start >= buf.size:
                break
            ind = ind + 1
            if block['type'] == 'net':
                continue
            elif block['type'] == 'convolutional':
                model = self.models[ind]
                batch_normalize = int(block['batch_normalize'])
                if batch_normalize:
                    start = load_conv_bn(buf, start, model[0], model[1])
                else:
                    start = load_conv(buf, start, model[0])
            elif block['type'] == 'connected':
                model = self.models[ind]
                if block['activation'] != 'linear':
                    start = load_fc(buf, start, model[0])
                else:
                    start = load_fc(buf, start, model)
            elif block['type'] == 'maxpool':
                pass
            elif block['type'] == 'reorg':
                pass
            elif block['type'] == 'route':
                pass
            elif block['type'] == 'shortcut':
                pass
            elif block['type'] == 'region':
                pass
            elif block['type'] == 'avgpool':
                pass
            elif block['type'] == 'softmax':
                pass
            elif block['type'] == 'cost':
                pass
            else:
                print('unknown type %s' % (block['type']))","load_conv_bn(buf, start, model[0], model[1])","load_conv_bn(buf, start, *model[:2])","iterable_zj[0], iterable_zj[1]",*model[:2],*model[:2],1
d2l-vn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/d2l-vn/d2l/torch.py,https://github.com/mlbvn/d2l-vn/tree/master/d2l/torch.py,,transpose_qkv$1151,"def transpose_qkv(X, num_heads):
    # Input `X` shape: (`batch_size`, `seq_len`, `num_hiddens`).
    # Output `X` shape:
    # (`batch_size`, `seq_len`, `num_heads`, `num_hiddens` / `num_heads`)
    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

    # `X` shape:
    # (`batch_size`, `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
    X = X.permute(0, 2, 1, 3)

    # `output` shape:
    # (`batch_size` * `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
    output = X.reshape(-1, X.shape[2], X.shape[3])
    return output","X.reshape(X.shape[0], X.shape[1], num_heads, -1)","X.reshape(*X.shape[:2], num_heads, -1)","iterable_zj[0], iterable_zj[1]",*X.shape[:2],*X.shape[:2],1
d2l-vn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/d2l-vn/d2l/torch.py,https://github.com/mlbvn/d2l-vn/tree/master/d2l/torch.py,,transpose_qkv$1151,"def transpose_qkv(X, num_heads):
    # Input `X` shape: (`batch_size`, `seq_len`, `num_hiddens`).
    # Output `X` shape:
    # (`batch_size`, `seq_len`, `num_heads`, `num_hiddens` / `num_heads`)
    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

    # `X` shape:
    # (`batch_size`, `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
    X = X.permute(0, 2, 1, 3)

    # `output` shape:
    # (`batch_size` * `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
    output = X.reshape(-1, X.shape[2], X.shape[3])
    return output","X.reshape(-1, X.shape[2], X.shape[3])","X.reshape(-1, *X.shape[2:4])","iterable_zj[2], iterable_zj[3]",*X.shape[2:4],*X.shape[2:4],1
51bitquant,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/51bitquant/weixinbot/wechat_orderbot.py,https://github.com/51bitquant/51bitquant/tree/master/weixinbot/wechat_orderbot.py,,receive_message$36,"def receive_message(msg: Message):
    txt = msg.text
    # txt #ex=binance, price=300.8, 'amount'=1, symbol=BTC/USDT, type=limit#
    # #symbol, price, amount, type, side# .
    print(txt)

    try:
        txt = txt.strip(' ')

        if txt.find("""") >= 0:
            bitquant.send(""#binance/huobipro/okex,BTC/USDT,,,limit/market,buy/sell#""
                          )

        if txt.startswith('#') and txt.endswith(""#""):
            txt = txt.strip('#')  # #.
            order_fields = txt.split(',')  # .  #binance, BTC/USDT, 10000, 0.01, limit, buy#
            order_datas = []

            for item in order_fields:
                order_datas.append(item.strip("" ""))
                #  .
                #binance,  BTC/USDT, 9000, 0.01, limit, buy#

            print(order_datas)
            if len(order_datas) >= 6:
                create_order(order_datas[0], order_datas[1], order_datas[2], order_datas[3], order_datas[4], order_datas[5])
    except Exception as error:
        bitquant.send(error)","create_order(order_datas[0], order_datas[1], order_datas[2], order_datas[3], order_datas[4], order_datas[5])",create_order(*order_datas[:6]),"iterable_zj[0], iterable_zj[1], iterable_zj[2], iterable_zj[3], iterable_zj[4], iterable_zj[5]",*order_datas[:6],*order_datas[:6],1
seirsplus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/seirsplus/build/lib/seirsplus/models.py,https://github.com/ryansmcgee/seirsplus/tree/master/build/lib/seirsplus/models.py,ExtSEIRSNetworkModel,update_parameters$1913,"def update_parameters(self):

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # Model graphs:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        self.G = self.parameters['G']
        # Adjacency matrix:
        if type(self.G)==numpy.ndarray:
            self.A = scipy.sparse.csr_matrix(self.G)
        elif type(self.G)==networkx.classes.graph.Graph:
            self.A = networkx.adj_matrix(self.G) # adj_matrix gives scipy.sparse csr_matrix
        else:
            raise BaseException(""Input an adjacency matrix or networkx object only."")
        self.numNodes   = int(self.A.shape[1])
        self.degree     = numpy.asarray(self.node_degrees(self.A)).astype(float)
        #----------------------------------------
        if(self.parameters['G_Q'] is None):
            self.G_Q = self.G # If no Q graph is provided, use G in its place
        else:
            self.G_Q = self.parameters['G_Q']
        # Quarantine Adjacency matrix:
        if type(self.G_Q)==numpy.ndarray:
            self.A_Q = scipy.sparse.csr_matrix(self.G_Q)
        elif type(self.G_Q)==networkx.classes.graph.Graph:
            self.A_Q = networkx.adj_matrix(self.G_Q) # adj_matrix gives scipy.sparse csr_matrix
        else:
            raise BaseException(""Input an adjacency matrix or networkx object only."")
        self.numNodes_Q   = int(self.A_Q.shape[1])
        self.degree_Q     = numpy.asarray(self.node_degrees(self.A_Q)).astype(float)
        #----------------------------------------
        assert(self.numNodes == self.numNodes_Q), ""The normal and quarantine adjacency graphs must be of the same size.""

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # Model parameters:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        self.beta           = numpy.array(self.parameters['beta']).reshape((self.numNodes, 1))          if isinstance(self.parameters['beta'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta'], shape=(self.numNodes,1))
        self.beta_asym      = (numpy.array(self.parameters['beta_asym']).reshape((self.numNodes, 1))    if isinstance(self.parameters['beta_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta_asym'], shape=(self.numNodes,1))) if self.parameters['beta_asym'] is not None else self.beta
        self.sigma          = numpy.array(self.parameters['sigma']).reshape((self.numNodes, 1))         if isinstance(self.parameters['sigma'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['sigma'], shape=(self.numNodes,1))
        self.lamda          = numpy.array(self.parameters['lamda']).reshape((self.numNodes, 1))         if isinstance(self.parameters['lamda'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['lamda'], shape=(self.numNodes,1))
        self.gamma          = numpy.array(self.parameters['gamma']).reshape((self.numNodes, 1))         if isinstance(self.parameters['gamma'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma'], shape=(self.numNodes,1))
        self.eta            = numpy.array(self.parameters['eta']).reshape((self.numNodes, 1))           if isinstance(self.parameters['eta'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['eta'], shape=(self.numNodes,1))
        self.gamma_asym     = (numpy.array(self.parameters['gamma_asym']).reshape((self.numNodes, 1))   if isinstance(self.parameters['gamma_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_asym'], shape=(self.numNodes,1))) if self.parameters['gamma_asym'] is not None else self.gamma
        self.gamma_H        = (numpy.array(self.parameters['gamma_H']).reshape((self.numNodes, 1))      if isinstance(self.parameters['gamma_H'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_H'], shape=(self.numNodes,1))) if self.parameters['gamma_H'] is not None else self.gamma
        self.mu_H           = numpy.array(self.parameters['mu_H']).reshape((self.numNodes, 1))          if isinstance(self.parameters['mu_H'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['mu_H'], shape=(self.numNodes,1))
        self.alpha          = numpy.array(self.parameters['alpha']).reshape((self.numNodes, 1))         if isinstance(self.parameters['alpha'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['alpha'], shape=(self.numNodes,1))
        self.xi             = numpy.array(self.parameters['xi']).reshape((self.numNodes, 1))            if isinstance(self.parameters['xi'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['xi'], shape=(self.numNodes,1))
        self.mu_0           = numpy.array(self.parameters['mu_0']).reshape((self.numNodes, 1))          if isinstance(self.parameters['mu_0'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['mu_0'], shape=(self.numNodes,1))
        self.nu             = numpy.array(self.parameters['nu']).reshape((self.numNodes, 1))            if isinstance(self.parameters['nu'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['nu'], shape=(self.numNodes,1))
        self.a              = numpy.array(self.parameters['a']).reshape((self.numNodes, 1))             if isinstance(self.parameters['a'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['a'], shape=(self.numNodes,1))
        self.h              = numpy.array(self.parameters['h']).reshape((self.numNodes, 1))             if isinstance(self.parameters['h'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['h'], shape=(self.numNodes,1))
        self.f              = numpy.array(self.parameters['f']).reshape((self.numNodes, 1))             if isinstance(self.parameters['f'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['f'], shape=(self.numNodes,1))
        self.p              = numpy.array(self.parameters['p']).reshape((self.numNodes, 1))             if isinstance(self.parameters['p'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['p'], shape=(self.numNodes,1))
        self.o              = numpy.array(self.parameters['o']).reshape((self.numNodes, 1))             if isinstance(self.parameters['o'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['o'], shape=(self.numNodes,1))

        self.rand_a = numpy.random.rand(self.a.shape[0], self.a.shape[1])
        self.rand_h = numpy.random.rand(self.h.shape[0], self.h.shape[1])
        self.rand_f = numpy.random.rand(self.f.shape[0], self.f.shape[1])

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # External infection introduction variables:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        self.prevalence_ext     = numpy.array(self.parameters['prevalence_ext']).reshape((self.numNodes, 1)) if isinstance(self.parameters['prevalence_ext'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['prevalence_ext'], shape=(self.numNodes,1))
       
        #----------------------------------------
        # Testing-related parameters:
        #----------------------------------------
        self.beta_Q         = (numpy.array(self.parameters['beta_Q']).reshape((self.numNodes, 1))       if isinstance(self.parameters['beta_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta_Q'], shape=(self.numNodes,1))) if self.parameters['beta_Q'] is not None else self.beta
        self.sigma_Q        = (numpy.array(self.parameters['sigma_Q']).reshape((self.numNodes, 1))      if isinstance(self.parameters['sigma_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['sigma_Q'], shape=(self.numNodes,1))) if self.parameters['sigma_Q'] is not None else self.sigma
        self.lamda_Q        = (numpy.array(self.parameters['lamda_Q']).reshape((self.numNodes, 1))      if isinstance(self.parameters['lamda_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['lamda_Q'], shape=(self.numNodes,1))) if self.parameters['lamda_Q'] is not None else self.lamda
        self.gamma_Q_sym    = (numpy.array(self.parameters['gamma_Q_sym']).reshape((self.numNodes, 1))  if isinstance(self.parameters['gamma_Q_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_Q_sym'], shape=(self.numNodes,1))) if self.parameters['gamma_Q_sym'] is not None else self.gamma
        self.gamma_Q_asym   = (numpy.array(self.parameters['gamma_Q_asym']).reshape((self.numNodes, 1)) if isinstance(self.parameters['gamma_Q_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_Q_asym'], shape=(self.numNodes,1))) if self.parameters['gamma_Q_asym'] is not None else self.gamma
        self.eta_Q          = (numpy.array(self.parameters['eta_Q']).reshape((self.numNodes, 1))        if isinstance(self.parameters['eta_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['eta_Q'], shape=(self.numNodes,1))) if self.parameters['eta_Q'] is not None else self.eta
        self.alpha_Q        = (numpy.array(self.parameters['alpha_Q']).reshape((self.numNodes, 1))      if isinstance(self.parameters['alpha_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['alpha_Q'], shape=(self.numNodes,1))) if self.parameters['alpha_Q'] is not None else self.alpha
        self.theta_S        = numpy.array(self.parameters['theta_S']).reshape((self.numNodes, 1))       if isinstance(self.parameters['theta_S'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_S'], shape=(self.numNodes,1))
        self.theta_E        = numpy.array(self.parameters['theta_E']).reshape((self.numNodes, 1))       if isinstance(self.parameters['theta_E'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_E'], shape=(self.numNodes,1))
        self.theta_pre      = numpy.array(self.parameters['theta_pre']).reshape((self.numNodes, 1))     if isinstance(self.parameters['theta_pre'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_pre'], shape=(self.numNodes,1))
        self.theta_sym      = numpy.array(self.parameters['theta_sym']).reshape((self.numNodes, 1))     if isinstance(self.parameters['theta_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_sym'], shape=(self.numNodes,1))
        self.theta_asym     = numpy.array(self.parameters['theta_asym']).reshape((self.numNodes, 1))    if isinstance(self.parameters['theta_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_asym'], shape=(self.numNodes,1))
        self.phi_S          = numpy.array(self.parameters['phi_S']).reshape((self.numNodes, 1))         if isinstance(self.parameters['phi_S'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_S'], shape=(self.numNodes,1))
        self.phi_E          = numpy.array(self.parameters['phi_E']).reshape((self.numNodes, 1))         if isinstance(self.parameters['phi_E'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_E'], shape=(self.numNodes,1))
        self.phi_pre        = numpy.array(self.parameters['phi_pre']).reshape((self.numNodes, 1))       if isinstance(self.parameters['phi_pre'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_pre'], shape=(self.numNodes,1))
        self.phi_sym        = numpy.array(self.parameters['phi_sym']).reshape((self.numNodes, 1))       if isinstance(self.parameters['phi_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_sym'], shape=(self.numNodes,1))
        self.phi_asym       = numpy.array(self.parameters['phi_asym']).reshape((self.numNodes, 1))      if isinstance(self.parameters['phi_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_asym'], shape=(self.numNodes,1))
        self.psi_S            = numpy.array(self.parameters['psi_S']).reshape((self.numNodes, 1))           if isinstance(self.parameters['psi_S'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_S'], shape=(self.numNodes,1))
        self.psi_E            = numpy.array(self.parameters['psi_E']).reshape((self.numNodes, 1))           if isinstance(self.parameters['psi_E'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_E'], shape=(self.numNodes,1))
        self.psi_pre          = numpy.array(self.parameters['psi_pre']).reshape((self.numNodes, 1))         if isinstance(self.parameters['psi_pre'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_pre'], shape=(self.numNodes,1))
        self.psi_sym          = numpy.array(self.parameters['psi_sym']).reshape((self.numNodes, 1))         if isinstance(self.parameters['psi_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_sym'], shape=(self.numNodes,1))
        self.psi_asym         = numpy.array(self.parameters['psi_asym']).reshape((self.numNodes, 1))        if isinstance(self.parameters['psi_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_asym'], shape=(self.numNodes,1))
        self.q              = numpy.array(self.parameters['q']).reshape((self.numNodes, 1))             if isinstance(self.parameters['q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['q'], shape=(self.numNodes,1))

        #----------------------------------------

        self.beta_pairwise_mode = self.parameters['beta_pairwise_mode']

        #----------------------------------------
        # Global transmission parameters:
        #----------------------------------------
        if(self.beta_pairwise_mode == 'infected' or self.beta_pairwise_mode is None):
            self.beta_global         = numpy.full_like(self.beta, fill_value=numpy.mean(self.beta))
            self.beta_Q_global       = numpy.full_like(self.beta_Q, fill_value=numpy.mean(self.beta_Q))
            self.beta_asym_global    = numpy.full_like(self.beta_asym, fill_value=numpy.mean(self.beta_asym))
        elif(self.beta_pairwise_mode == 'infectee'):
            self.beta_global         = self.beta      
            self.beta_Q_global       = self.beta_Q    
            self.beta_asym_global    = self.beta_asym
        elif(self.beta_pairwise_mode == 'min'):
            self.beta_global         = numpy.minimum(self.beta, numpy.mean(beta)) 
            self.beta_Q_global       = numpy.minimum(self.beta_Q, numpy.mean(beta_Q)) 
            self.beta_asym_global    = numpy.minimum(self.beta_asym, numpy.mean(beta_asym))
        elif(self.beta_pairwise_mode == 'max'):
            self.beta_global         = numpy.maximum(self.beta, numpy.mean(beta)) 
            self.beta_Q_global       = numpy.maximum(self.beta_Q, numpy.mean(beta_Q)) 
            self.beta_asym_global    = numpy.maximum(self.beta_asym, numpy.mean(beta_asym))
        elif(self.beta_pairwise_mode == 'mean'):
            self.beta_global         = (self.beta + numpy.full_like(self.beta, fill_value=numpy.mean(self.beta)))/2
            self.beta_Q_global       = (self.beta_Q + numpy.full_like(self.beta_Q, fill_value=numpy.mean(self.beta_Q)))/2
            self.beta_asym_global    = (self.beta_asym + numpy.full_like(self.beta_asym, fill_value=numpy.mean(self.beta_asym)))/2
            
        #----------------------------------------
        # Local transmission parameters:
        #----------------------------------------
        self.beta_local         = self.beta      if self.parameters['beta_local'] is None      else numpy.array(self.parameters['beta_local'])      if isinstance(self.parameters['beta_local'], (list, numpy.ndarray))      else numpy.full(fill_value=self.parameters['beta_local'], shape=(self.numNodes,1))
        self.beta_Q_local       = self.beta_Q    if self.parameters['beta_Q_local'] is None    else numpy.array(self.parameters['beta_Q_local'])    if isinstance(self.parameters['beta_Q_local'], (list, numpy.ndarray))    else numpy.full(fill_value=self.parameters['beta_Q_local'], shape=(self.numNodes,1))
        self.beta_asym_local    = None           if self.parameters['beta_asym_local'] is None else numpy.array(self.parameters['beta_asym_local']) if isinstance(self.parameters['beta_asym_local'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta_asym_local'], shape=(self.numNodes,1))
        #----------------------------------------
        if(self.beta_local.ndim == 2 and self.beta_local.shape[0] == self.numNodes and self.beta_local.shape[1] == self.numNodes):
            self.A_beta_pairwise = self.beta_local
        elif((self.beta_local.ndim == 1 and self.beta_local.shape[0] == self.numNodes) or (self.beta_local.ndim == 2 and (self.beta_local.shape[0] == self.numNodes or self.beta_local.shape[1] == self.numNodes))):
            self.beta_local = self.beta_local.reshape((self.numNodes,1))
            # Pre-multiply beta values by the adjacency matrix (""transmission weight connections"")
            A_beta_pairwise_byInfected = scipy.sparse.csr_matrix.multiply(self.A, self.beta_local.T).tocsr()
            A_beta_pairwise_byInfectee = scipy.sparse.csr_matrix.multiply(self.A, self.beta_local).tocsr()    
            #------------------------------
            # Compute the effective pairwise beta values as a function of the infected/infectee pair:
            if(self.beta_pairwise_mode == 'infected'):
                self.A_beta_pairwise = A_beta_pairwise_byInfected
            elif(self.beta_pairwise_mode == 'infectee'):
                self.A_beta_pairwise = A_beta_pairwise_byInfectee
            elif(self.beta_pairwise_mode == 'min'):
                self.A_beta_pairwise = scipy.sparse.csr_matrix.minimum(A_beta_pairwise_byInfected, A_beta_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'max'):
                self.A_beta_pairwise = scipy.sparse.csr_matrix.maximum(A_beta_pairwise_byInfected, A_beta_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'mean' or self.beta_pairwise_mode is None):
                self.A_beta_pairwise = (A_beta_pairwise_byInfected + A_beta_pairwise_byInfectee)/2
            else:
                print(""Unrecognized beta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for beta_local (expected 1xN list/array or NxN 2d array)"")
        #----------------------------------------
        if(self.beta_Q_local.ndim == 2 and self.beta_Q_local.shape[0] == self.numNodes and self.beta_Q_local.shape[1] == self.numNodes):
            self.A_Q_beta_Q_pairwise = self.beta_Q_local
        elif((self.beta_Q_local.ndim == 1 and self.beta_Q_local.shape[0] == self.numNodes) or (self.beta_Q_local.ndim == 2 and (self.beta_Q_local.shape[0] == self.numNodes or self.beta_Q_local.shape[1] == self.numNodes))):
            self.beta_Q_local = self.beta_Q_local.reshape((self.numNodes,1))
            # Pre-multiply beta_Q values by the isolation adjacency matrix (""transmission weight connections"")
            A_Q_beta_Q_pairwise_byInfected      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.beta_Q_local.T).tocsr()
            A_Q_beta_Q_pairwise_byInfectee      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.beta_Q_local).tocsr()
            #------------------------------
            # Compute the effective pairwise beta values as a function of the infected/infectee pair:
            if(self.beta_pairwise_mode == 'infected'):
                self.A_Q_beta_Q_pairwise = A_Q_beta_Q_pairwise_byInfected
            elif(self.beta_pairwise_mode == 'infectee'):
                self.A_Q_beta_Q_pairwise = A_Q_beta_Q_pairwise_byInfectee
            elif(self.beta_pairwise_mode == 'min'):
                self.A_Q_beta_Q_pairwise = scipy.sparse.csr_matrix.minimum(A_Q_beta_Q_pairwise_byInfected, A_Q_beta_Q_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'max'):
                self.A_Q_beta_Q_pairwise = scipy.sparse.csr_matrix.maximum(A_Q_beta_Q_pairwise_byInfected, A_Q_beta_Q_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'mean' or self.beta_pairwise_mode is None):
                self.A_Q_beta_Q_pairwise = (A_Q_beta_Q_pairwise_byInfected + A_Q_beta_Q_pairwise_byInfectee)/2
            else:
                print(""Unrecognized beta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for beta_Q_local (expected 1xN list/array or NxN 2d array)"")
        #----------------------------------------
        if(self.beta_asym_local is None):
            self.A_beta_asym_pairwise = None
        elif(self.beta_asym_local.ndim == 2 and self.beta_asym_local.shape[0] == self.numNodes and self.beta_asym_local.shape[1] == self.numNodes):
            self.A_beta_asym_pairwise = self.beta_asym_local
        elif((self.beta_asym_local.ndim == 1 and self.beta_asym_local.shape[0] == self.numNodes) or (self.beta_asym_local.ndim == 2 and (self.beta_asym_local.shape[0] == self.numNodes or self.beta_asym_local.shape[1] == self.numNodes))):
            self.beta_asym_local = self.beta_asym_local.reshape((self.numNodes,1))
            # Pre-multiply beta_asym values by the adjacency matrix (""transmission weight connections"")
            A_beta_asym_pairwise_byInfected      = scipy.sparse.csr_matrix.multiply(self.A, self.beta_asym_local.T).tocsr()
            A_beta_asym_pairwise_byInfectee      = scipy.sparse.csr_matrix.multiply(self.A, self.beta_asym_local).tocsr()
            #------------------------------
            # Compute the effective pairwise beta values as a function of the infected/infectee pair:
            if(self.beta_pairwise_mode == 'infected'):
                self.A_beta_asym_pairwise = A_beta_asym_pairwise_byInfected
            elif(self.beta_pairwise_mode == 'infectee'):
                self.A_beta_asym_pairwise = A_beta_asym_pairwise_byInfectee
            elif(self.beta_pairwise_mode == 'min'):
                self.A_beta_asym_pairwise = scipy.sparse.csr_matrix.minimum(A_beta_asym_pairwise_byInfected, A_beta_asym_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'max'):
                self.A_beta_asym_pairwise = scipy.sparse.csr_matrix.maximum(A_beta_asym_pairwise_byInfected, A_beta_asym_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'mean' or self.beta_pairwise_mode is None):
                self.A_beta_asym_pairwise = (A_beta_asym_pairwise_byInfected + A_beta_asym_pairwise_byInfectee)/2
            else:
                print(""Unrecognized beta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for beta_asym_local (expected 1xN list/array or NxN 2d array)"")

        #----------------------------------------
        # Degree-based transmission scaling parameters:
        #----------------------------------------
        self.delta_pairwise_mode = self.parameters['delta_pairwise_mode']
        with numpy.errstate(divide='ignore'): # ignore log(0) warning, then convert log(0) = -inf -> 0.0
            self.delta               = numpy.log(self.degree)/numpy.log(numpy.mean(self.degree))     if self.parameters['delta'] is None   else numpy.array(self.parameters['delta'])   if isinstance(self.parameters['delta'], (list, numpy.ndarray))   else numpy.full(fill_value=self.parameters['delta'], shape=(self.numNodes,1))
            self.delta_Q             = numpy.log(self.degree_Q)/numpy.log(numpy.mean(self.degree_Q)) if self.parameters['delta_Q'] is None else numpy.array(self.parameters['delta_Q']) if isinstance(self.parameters['delta_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['delta_Q'], shape=(self.numNodes,1))
        self.delta[numpy.isneginf(self.delta)] = 0.0
        self.delta_Q[numpy.isneginf(self.delta_Q)] = 0.0
        #----------------------------------------
        if(self.delta.ndim == 2 and self.delta.shape[0] == self.numNodes and self.delta.shape[1] == self.numNodes):
            self.A_delta_pairwise = self.delta
        elif((self.delta.ndim == 1 and self.delta.shape[0] == self.numNodes) or (self.delta.ndim == 2 and (self.delta.shape[0] == self.numNodes or self.delta.shape[1] == self.numNodes))):
            self.delta = self.delta.reshape((self.numNodes,1))
            # Pre-multiply delta values by the adjacency matrix (""transmission weight connections"")
            A_delta_pairwise_byInfected = scipy.sparse.csr_matrix.multiply(self.A, self.delta.T).tocsr()
            A_delta_pairwise_byInfectee = scipy.sparse.csr_matrix.multiply(self.A, self.delta).tocsr()    
            #------------------------------
            # Compute the effective pairwise delta values as a function of the infected/infectee pair:
            if(self.delta_pairwise_mode == 'infected'):
                self.A_delta_pairwise = A_delta_pairwise_byInfected
            elif(self.delta_pairwise_mode == 'infectee'):
                self.A_delta_pairwise = A_delta_pairwise_byInfectee
            elif(self.delta_pairwise_mode == 'min'):
                self.A_delta_pairwise = scipy.sparse.csr_matrix.minimum(A_delta_pairwise_byInfected, A_delta_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'max'):
                self.A_delta_pairwise = scipy.sparse.csr_matrix.maximum(A_delta_pairwise_byInfected, A_delta_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'mean'):
                self.A_delta_pairwise = (A_delta_pairwise_byInfected + A_delta_pairwise_byInfectee)/2
            elif(self.delta_pairwise_mode is None):
                self.A_delta_pairwise = self.A
            else:
                print(""Unrecognized delta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for delta (expected 1xN list/array or NxN 2d array)"")
        #----------------------------------------
        if(self.delta_Q.ndim == 2 and self.delta_Q.shape[0] == self.numNodes and self.delta_Q.shape[1] == self.numNodes):
            self.A_Q_delta_Q_pairwise = self.delta_Q
        elif((self.delta_Q.ndim == 1 and self.delta_Q.shape[0] == self.numNodes) or (self.delta_Q.ndim == 2 and (self.delta_Q.shape[0] == self.numNodes or self.delta_Q.shape[1] == self.numNodes))):
            self.delta_Q = self.delta_Q.reshape((self.numNodes,1))
            # Pre-multiply delta_Q values by the isolation adjacency matrix (""transmission weight connections"")
            A_Q_delta_Q_pairwise_byInfected      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.delta_Q).tocsr()
            A_Q_delta_Q_pairwise_byInfectee      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.delta_Q.T).tocsr()
            #------------------------------
            # Compute the effective pairwise delta values as a function of the infected/infectee pair:
            if(self.delta_pairwise_mode == 'infected'):
                self.A_Q_delta_Q_pairwise = A_Q_delta_Q_pairwise_byInfected
            elif(self.delta_pairwise_mode == 'infectee'):
                self.A_Q_delta_Q_pairwise = A_Q_delta_Q_pairwise_byInfectee
            elif(self.delta_pairwise_mode == 'min'):
                self.A_Q_delta_Q_pairwise = scipy.sparse.csr_matrix.minimum(A_Q_delta_Q_pairwise_byInfected, A_Q_delta_Q_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'max'):
                self.A_Q_delta_Q_pairwise = scipy.sparse.csr_matrix.maximum(A_Q_delta_Q_pairwise_byInfected, A_Q_delta_Q_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'mean'):
                self.A_Q_delta_Q_pairwise = (A_Q_delta_Q_pairwise_byInfected + A_Q_delta_Q_pairwise_byInfectee)/2
            elif(self.delta_pairwise_mode is None):
                self.A_Q_delta_Q_pairwise = self.A
            else:
                print(""Unrecognized delta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for delta_Q (expected 1xN list/array or NxN 2d array)"")

        #----------------------------------------
        # Pre-calculate the pairwise delta*beta values:
        #----------------------------------------
        self.A_deltabeta          = scipy.sparse.csr_matrix.multiply(self.A_delta_pairwise, self.A_beta_pairwise)
        self.A_Q_deltabeta_Q      = scipy.sparse.csr_matrix.multiply(self.A_Q_delta_Q_pairwise, self.A_Q_beta_Q_pairwise)
        if(self.A_beta_asym_pairwise is not None):
            self.A_deltabeta_asym = scipy.sparse.csr_matrix.multiply(self.A_delta_pairwise, self.A_beta_asym_pairwise)
        else:
            self.A_deltabeta_asym = None","numpy.random.rand(self.a.shape[0], self.a.shape[1])",numpy.random.rand(*self.a.shape[:2]),"iterable_zj[0], iterable_zj[1]",*self.a.shape[:2],*self.a.shape[:2],1
seirsplus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/seirsplus/build/lib/seirsplus/models.py,https://github.com/ryansmcgee/seirsplus/tree/master/build/lib/seirsplus/models.py,ExtSEIRSNetworkModel,update_parameters$1913,"def update_parameters(self):

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # Model graphs:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        self.G = self.parameters['G']
        # Adjacency matrix:
        if type(self.G)==numpy.ndarray:
            self.A = scipy.sparse.csr_matrix(self.G)
        elif type(self.G)==networkx.classes.graph.Graph:
            self.A = networkx.adj_matrix(self.G) # adj_matrix gives scipy.sparse csr_matrix
        else:
            raise BaseException(""Input an adjacency matrix or networkx object only."")
        self.numNodes   = int(self.A.shape[1])
        self.degree     = numpy.asarray(self.node_degrees(self.A)).astype(float)
        #----------------------------------------
        if(self.parameters['G_Q'] is None):
            self.G_Q = self.G # If no Q graph is provided, use G in its place
        else:
            self.G_Q = self.parameters['G_Q']
        # Quarantine Adjacency matrix:
        if type(self.G_Q)==numpy.ndarray:
            self.A_Q = scipy.sparse.csr_matrix(self.G_Q)
        elif type(self.G_Q)==networkx.classes.graph.Graph:
            self.A_Q = networkx.adj_matrix(self.G_Q) # adj_matrix gives scipy.sparse csr_matrix
        else:
            raise BaseException(""Input an adjacency matrix or networkx object only."")
        self.numNodes_Q   = int(self.A_Q.shape[1])
        self.degree_Q     = numpy.asarray(self.node_degrees(self.A_Q)).astype(float)
        #----------------------------------------
        assert(self.numNodes == self.numNodes_Q), ""The normal and quarantine adjacency graphs must be of the same size.""

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # Model parameters:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        self.beta           = numpy.array(self.parameters['beta']).reshape((self.numNodes, 1))          if isinstance(self.parameters['beta'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta'], shape=(self.numNodes,1))
        self.beta_asym      = (numpy.array(self.parameters['beta_asym']).reshape((self.numNodes, 1))    if isinstance(self.parameters['beta_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta_asym'], shape=(self.numNodes,1))) if self.parameters['beta_asym'] is not None else self.beta
        self.sigma          = numpy.array(self.parameters['sigma']).reshape((self.numNodes, 1))         if isinstance(self.parameters['sigma'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['sigma'], shape=(self.numNodes,1))
        self.lamda          = numpy.array(self.parameters['lamda']).reshape((self.numNodes, 1))         if isinstance(self.parameters['lamda'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['lamda'], shape=(self.numNodes,1))
        self.gamma          = numpy.array(self.parameters['gamma']).reshape((self.numNodes, 1))         if isinstance(self.parameters['gamma'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma'], shape=(self.numNodes,1))
        self.eta            = numpy.array(self.parameters['eta']).reshape((self.numNodes, 1))           if isinstance(self.parameters['eta'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['eta'], shape=(self.numNodes,1))
        self.gamma_asym     = (numpy.array(self.parameters['gamma_asym']).reshape((self.numNodes, 1))   if isinstance(self.parameters['gamma_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_asym'], shape=(self.numNodes,1))) if self.parameters['gamma_asym'] is not None else self.gamma
        self.gamma_H        = (numpy.array(self.parameters['gamma_H']).reshape((self.numNodes, 1))      if isinstance(self.parameters['gamma_H'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_H'], shape=(self.numNodes,1))) if self.parameters['gamma_H'] is not None else self.gamma
        self.mu_H           = numpy.array(self.parameters['mu_H']).reshape((self.numNodes, 1))          if isinstance(self.parameters['mu_H'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['mu_H'], shape=(self.numNodes,1))
        self.alpha          = numpy.array(self.parameters['alpha']).reshape((self.numNodes, 1))         if isinstance(self.parameters['alpha'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['alpha'], shape=(self.numNodes,1))
        self.xi             = numpy.array(self.parameters['xi']).reshape((self.numNodes, 1))            if isinstance(self.parameters['xi'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['xi'], shape=(self.numNodes,1))
        self.mu_0           = numpy.array(self.parameters['mu_0']).reshape((self.numNodes, 1))          if isinstance(self.parameters['mu_0'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['mu_0'], shape=(self.numNodes,1))
        self.nu             = numpy.array(self.parameters['nu']).reshape((self.numNodes, 1))            if isinstance(self.parameters['nu'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['nu'], shape=(self.numNodes,1))
        self.a              = numpy.array(self.parameters['a']).reshape((self.numNodes, 1))             if isinstance(self.parameters['a'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['a'], shape=(self.numNodes,1))
        self.h              = numpy.array(self.parameters['h']).reshape((self.numNodes, 1))             if isinstance(self.parameters['h'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['h'], shape=(self.numNodes,1))
        self.f              = numpy.array(self.parameters['f']).reshape((self.numNodes, 1))             if isinstance(self.parameters['f'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['f'], shape=(self.numNodes,1))
        self.p              = numpy.array(self.parameters['p']).reshape((self.numNodes, 1))             if isinstance(self.parameters['p'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['p'], shape=(self.numNodes,1))
        self.o              = numpy.array(self.parameters['o']).reshape((self.numNodes, 1))             if isinstance(self.parameters['o'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['o'], shape=(self.numNodes,1))

        self.rand_a = numpy.random.rand(self.a.shape[0], self.a.shape[1])
        self.rand_h = numpy.random.rand(self.h.shape[0], self.h.shape[1])
        self.rand_f = numpy.random.rand(self.f.shape[0], self.f.shape[1])

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # External infection introduction variables:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        self.prevalence_ext     = numpy.array(self.parameters['prevalence_ext']).reshape((self.numNodes, 1)) if isinstance(self.parameters['prevalence_ext'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['prevalence_ext'], shape=(self.numNodes,1))
       
        #----------------------------------------
        # Testing-related parameters:
        #----------------------------------------
        self.beta_Q         = (numpy.array(self.parameters['beta_Q']).reshape((self.numNodes, 1))       if isinstance(self.parameters['beta_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta_Q'], shape=(self.numNodes,1))) if self.parameters['beta_Q'] is not None else self.beta
        self.sigma_Q        = (numpy.array(self.parameters['sigma_Q']).reshape((self.numNodes, 1))      if isinstance(self.parameters['sigma_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['sigma_Q'], shape=(self.numNodes,1))) if self.parameters['sigma_Q'] is not None else self.sigma
        self.lamda_Q        = (numpy.array(self.parameters['lamda_Q']).reshape((self.numNodes, 1))      if isinstance(self.parameters['lamda_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['lamda_Q'], shape=(self.numNodes,1))) if self.parameters['lamda_Q'] is not None else self.lamda
        self.gamma_Q_sym    = (numpy.array(self.parameters['gamma_Q_sym']).reshape((self.numNodes, 1))  if isinstance(self.parameters['gamma_Q_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_Q_sym'], shape=(self.numNodes,1))) if self.parameters['gamma_Q_sym'] is not None else self.gamma
        self.gamma_Q_asym   = (numpy.array(self.parameters['gamma_Q_asym']).reshape((self.numNodes, 1)) if isinstance(self.parameters['gamma_Q_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_Q_asym'], shape=(self.numNodes,1))) if self.parameters['gamma_Q_asym'] is not None else self.gamma
        self.eta_Q          = (numpy.array(self.parameters['eta_Q']).reshape((self.numNodes, 1))        if isinstance(self.parameters['eta_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['eta_Q'], shape=(self.numNodes,1))) if self.parameters['eta_Q'] is not None else self.eta
        self.alpha_Q        = (numpy.array(self.parameters['alpha_Q']).reshape((self.numNodes, 1))      if isinstance(self.parameters['alpha_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['alpha_Q'], shape=(self.numNodes,1))) if self.parameters['alpha_Q'] is not None else self.alpha
        self.theta_S        = numpy.array(self.parameters['theta_S']).reshape((self.numNodes, 1))       if isinstance(self.parameters['theta_S'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_S'], shape=(self.numNodes,1))
        self.theta_E        = numpy.array(self.parameters['theta_E']).reshape((self.numNodes, 1))       if isinstance(self.parameters['theta_E'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_E'], shape=(self.numNodes,1))
        self.theta_pre      = numpy.array(self.parameters['theta_pre']).reshape((self.numNodes, 1))     if isinstance(self.parameters['theta_pre'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_pre'], shape=(self.numNodes,1))
        self.theta_sym      = numpy.array(self.parameters['theta_sym']).reshape((self.numNodes, 1))     if isinstance(self.parameters['theta_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_sym'], shape=(self.numNodes,1))
        self.theta_asym     = numpy.array(self.parameters['theta_asym']).reshape((self.numNodes, 1))    if isinstance(self.parameters['theta_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_asym'], shape=(self.numNodes,1))
        self.phi_S          = numpy.array(self.parameters['phi_S']).reshape((self.numNodes, 1))         if isinstance(self.parameters['phi_S'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_S'], shape=(self.numNodes,1))
        self.phi_E          = numpy.array(self.parameters['phi_E']).reshape((self.numNodes, 1))         if isinstance(self.parameters['phi_E'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_E'], shape=(self.numNodes,1))
        self.phi_pre        = numpy.array(self.parameters['phi_pre']).reshape((self.numNodes, 1))       if isinstance(self.parameters['phi_pre'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_pre'], shape=(self.numNodes,1))
        self.phi_sym        = numpy.array(self.parameters['phi_sym']).reshape((self.numNodes, 1))       if isinstance(self.parameters['phi_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_sym'], shape=(self.numNodes,1))
        self.phi_asym       = numpy.array(self.parameters['phi_asym']).reshape((self.numNodes, 1))      if isinstance(self.parameters['phi_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_asym'], shape=(self.numNodes,1))
        self.psi_S            = numpy.array(self.parameters['psi_S']).reshape((self.numNodes, 1))           if isinstance(self.parameters['psi_S'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_S'], shape=(self.numNodes,1))
        self.psi_E            = numpy.array(self.parameters['psi_E']).reshape((self.numNodes, 1))           if isinstance(self.parameters['psi_E'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_E'], shape=(self.numNodes,1))
        self.psi_pre          = numpy.array(self.parameters['psi_pre']).reshape((self.numNodes, 1))         if isinstance(self.parameters['psi_pre'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_pre'], shape=(self.numNodes,1))
        self.psi_sym          = numpy.array(self.parameters['psi_sym']).reshape((self.numNodes, 1))         if isinstance(self.parameters['psi_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_sym'], shape=(self.numNodes,1))
        self.psi_asym         = numpy.array(self.parameters['psi_asym']).reshape((self.numNodes, 1))        if isinstance(self.parameters['psi_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_asym'], shape=(self.numNodes,1))
        self.q              = numpy.array(self.parameters['q']).reshape((self.numNodes, 1))             if isinstance(self.parameters['q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['q'], shape=(self.numNodes,1))

        #----------------------------------------

        self.beta_pairwise_mode = self.parameters['beta_pairwise_mode']

        #----------------------------------------
        # Global transmission parameters:
        #----------------------------------------
        if(self.beta_pairwise_mode == 'infected' or self.beta_pairwise_mode is None):
            self.beta_global         = numpy.full_like(self.beta, fill_value=numpy.mean(self.beta))
            self.beta_Q_global       = numpy.full_like(self.beta_Q, fill_value=numpy.mean(self.beta_Q))
            self.beta_asym_global    = numpy.full_like(self.beta_asym, fill_value=numpy.mean(self.beta_asym))
        elif(self.beta_pairwise_mode == 'infectee'):
            self.beta_global         = self.beta      
            self.beta_Q_global       = self.beta_Q    
            self.beta_asym_global    = self.beta_asym
        elif(self.beta_pairwise_mode == 'min'):
            self.beta_global         = numpy.minimum(self.beta, numpy.mean(beta)) 
            self.beta_Q_global       = numpy.minimum(self.beta_Q, numpy.mean(beta_Q)) 
            self.beta_asym_global    = numpy.minimum(self.beta_asym, numpy.mean(beta_asym))
        elif(self.beta_pairwise_mode == 'max'):
            self.beta_global         = numpy.maximum(self.beta, numpy.mean(beta)) 
            self.beta_Q_global       = numpy.maximum(self.beta_Q, numpy.mean(beta_Q)) 
            self.beta_asym_global    = numpy.maximum(self.beta_asym, numpy.mean(beta_asym))
        elif(self.beta_pairwise_mode == 'mean'):
            self.beta_global         = (self.beta + numpy.full_like(self.beta, fill_value=numpy.mean(self.beta)))/2
            self.beta_Q_global       = (self.beta_Q + numpy.full_like(self.beta_Q, fill_value=numpy.mean(self.beta_Q)))/2
            self.beta_asym_global    = (self.beta_asym + numpy.full_like(self.beta_asym, fill_value=numpy.mean(self.beta_asym)))/2
            
        #----------------------------------------
        # Local transmission parameters:
        #----------------------------------------
        self.beta_local         = self.beta      if self.parameters['beta_local'] is None      else numpy.array(self.parameters['beta_local'])      if isinstance(self.parameters['beta_local'], (list, numpy.ndarray))      else numpy.full(fill_value=self.parameters['beta_local'], shape=(self.numNodes,1))
        self.beta_Q_local       = self.beta_Q    if self.parameters['beta_Q_local'] is None    else numpy.array(self.parameters['beta_Q_local'])    if isinstance(self.parameters['beta_Q_local'], (list, numpy.ndarray))    else numpy.full(fill_value=self.parameters['beta_Q_local'], shape=(self.numNodes,1))
        self.beta_asym_local    = None           if self.parameters['beta_asym_local'] is None else numpy.array(self.parameters['beta_asym_local']) if isinstance(self.parameters['beta_asym_local'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta_asym_local'], shape=(self.numNodes,1))
        #----------------------------------------
        if(self.beta_local.ndim == 2 and self.beta_local.shape[0] == self.numNodes and self.beta_local.shape[1] == self.numNodes):
            self.A_beta_pairwise = self.beta_local
        elif((self.beta_local.ndim == 1 and self.beta_local.shape[0] == self.numNodes) or (self.beta_local.ndim == 2 and (self.beta_local.shape[0] == self.numNodes or self.beta_local.shape[1] == self.numNodes))):
            self.beta_local = self.beta_local.reshape((self.numNodes,1))
            # Pre-multiply beta values by the adjacency matrix (""transmission weight connections"")
            A_beta_pairwise_byInfected = scipy.sparse.csr_matrix.multiply(self.A, self.beta_local.T).tocsr()
            A_beta_pairwise_byInfectee = scipy.sparse.csr_matrix.multiply(self.A, self.beta_local).tocsr()    
            #------------------------------
            # Compute the effective pairwise beta values as a function of the infected/infectee pair:
            if(self.beta_pairwise_mode == 'infected'):
                self.A_beta_pairwise = A_beta_pairwise_byInfected
            elif(self.beta_pairwise_mode == 'infectee'):
                self.A_beta_pairwise = A_beta_pairwise_byInfectee
            elif(self.beta_pairwise_mode == 'min'):
                self.A_beta_pairwise = scipy.sparse.csr_matrix.minimum(A_beta_pairwise_byInfected, A_beta_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'max'):
                self.A_beta_pairwise = scipy.sparse.csr_matrix.maximum(A_beta_pairwise_byInfected, A_beta_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'mean' or self.beta_pairwise_mode is None):
                self.A_beta_pairwise = (A_beta_pairwise_byInfected + A_beta_pairwise_byInfectee)/2
            else:
                print(""Unrecognized beta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for beta_local (expected 1xN list/array or NxN 2d array)"")
        #----------------------------------------
        if(self.beta_Q_local.ndim == 2 and self.beta_Q_local.shape[0] == self.numNodes and self.beta_Q_local.shape[1] == self.numNodes):
            self.A_Q_beta_Q_pairwise = self.beta_Q_local
        elif((self.beta_Q_local.ndim == 1 and self.beta_Q_local.shape[0] == self.numNodes) or (self.beta_Q_local.ndim == 2 and (self.beta_Q_local.shape[0] == self.numNodes or self.beta_Q_local.shape[1] == self.numNodes))):
            self.beta_Q_local = self.beta_Q_local.reshape((self.numNodes,1))
            # Pre-multiply beta_Q values by the isolation adjacency matrix (""transmission weight connections"")
            A_Q_beta_Q_pairwise_byInfected      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.beta_Q_local.T).tocsr()
            A_Q_beta_Q_pairwise_byInfectee      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.beta_Q_local).tocsr()
            #------------------------------
            # Compute the effective pairwise beta values as a function of the infected/infectee pair:
            if(self.beta_pairwise_mode == 'infected'):
                self.A_Q_beta_Q_pairwise = A_Q_beta_Q_pairwise_byInfected
            elif(self.beta_pairwise_mode == 'infectee'):
                self.A_Q_beta_Q_pairwise = A_Q_beta_Q_pairwise_byInfectee
            elif(self.beta_pairwise_mode == 'min'):
                self.A_Q_beta_Q_pairwise = scipy.sparse.csr_matrix.minimum(A_Q_beta_Q_pairwise_byInfected, A_Q_beta_Q_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'max'):
                self.A_Q_beta_Q_pairwise = scipy.sparse.csr_matrix.maximum(A_Q_beta_Q_pairwise_byInfected, A_Q_beta_Q_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'mean' or self.beta_pairwise_mode is None):
                self.A_Q_beta_Q_pairwise = (A_Q_beta_Q_pairwise_byInfected + A_Q_beta_Q_pairwise_byInfectee)/2
            else:
                print(""Unrecognized beta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for beta_Q_local (expected 1xN list/array or NxN 2d array)"")
        #----------------------------------------
        if(self.beta_asym_local is None):
            self.A_beta_asym_pairwise = None
        elif(self.beta_asym_local.ndim == 2 and self.beta_asym_local.shape[0] == self.numNodes and self.beta_asym_local.shape[1] == self.numNodes):
            self.A_beta_asym_pairwise = self.beta_asym_local
        elif((self.beta_asym_local.ndim == 1 and self.beta_asym_local.shape[0] == self.numNodes) or (self.beta_asym_local.ndim == 2 and (self.beta_asym_local.shape[0] == self.numNodes or self.beta_asym_local.shape[1] == self.numNodes))):
            self.beta_asym_local = self.beta_asym_local.reshape((self.numNodes,1))
            # Pre-multiply beta_asym values by the adjacency matrix (""transmission weight connections"")
            A_beta_asym_pairwise_byInfected      = scipy.sparse.csr_matrix.multiply(self.A, self.beta_asym_local.T).tocsr()
            A_beta_asym_pairwise_byInfectee      = scipy.sparse.csr_matrix.multiply(self.A, self.beta_asym_local).tocsr()
            #------------------------------
            # Compute the effective pairwise beta values as a function of the infected/infectee pair:
            if(self.beta_pairwise_mode == 'infected'):
                self.A_beta_asym_pairwise = A_beta_asym_pairwise_byInfected
            elif(self.beta_pairwise_mode == 'infectee'):
                self.A_beta_asym_pairwise = A_beta_asym_pairwise_byInfectee
            elif(self.beta_pairwise_mode == 'min'):
                self.A_beta_asym_pairwise = scipy.sparse.csr_matrix.minimum(A_beta_asym_pairwise_byInfected, A_beta_asym_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'max'):
                self.A_beta_asym_pairwise = scipy.sparse.csr_matrix.maximum(A_beta_asym_pairwise_byInfected, A_beta_asym_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'mean' or self.beta_pairwise_mode is None):
                self.A_beta_asym_pairwise = (A_beta_asym_pairwise_byInfected + A_beta_asym_pairwise_byInfectee)/2
            else:
                print(""Unrecognized beta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for beta_asym_local (expected 1xN list/array or NxN 2d array)"")

        #----------------------------------------
        # Degree-based transmission scaling parameters:
        #----------------------------------------
        self.delta_pairwise_mode = self.parameters['delta_pairwise_mode']
        with numpy.errstate(divide='ignore'): # ignore log(0) warning, then convert log(0) = -inf -> 0.0
            self.delta               = numpy.log(self.degree)/numpy.log(numpy.mean(self.degree))     if self.parameters['delta'] is None   else numpy.array(self.parameters['delta'])   if isinstance(self.parameters['delta'], (list, numpy.ndarray))   else numpy.full(fill_value=self.parameters['delta'], shape=(self.numNodes,1))
            self.delta_Q             = numpy.log(self.degree_Q)/numpy.log(numpy.mean(self.degree_Q)) if self.parameters['delta_Q'] is None else numpy.array(self.parameters['delta_Q']) if isinstance(self.parameters['delta_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['delta_Q'], shape=(self.numNodes,1))
        self.delta[numpy.isneginf(self.delta)] = 0.0
        self.delta_Q[numpy.isneginf(self.delta_Q)] = 0.0
        #----------------------------------------
        if(self.delta.ndim == 2 and self.delta.shape[0] == self.numNodes and self.delta.shape[1] == self.numNodes):
            self.A_delta_pairwise = self.delta
        elif((self.delta.ndim == 1 and self.delta.shape[0] == self.numNodes) or (self.delta.ndim == 2 and (self.delta.shape[0] == self.numNodes or self.delta.shape[1] == self.numNodes))):
            self.delta = self.delta.reshape((self.numNodes,1))
            # Pre-multiply delta values by the adjacency matrix (""transmission weight connections"")
            A_delta_pairwise_byInfected = scipy.sparse.csr_matrix.multiply(self.A, self.delta.T).tocsr()
            A_delta_pairwise_byInfectee = scipy.sparse.csr_matrix.multiply(self.A, self.delta).tocsr()    
            #------------------------------
            # Compute the effective pairwise delta values as a function of the infected/infectee pair:
            if(self.delta_pairwise_mode == 'infected'):
                self.A_delta_pairwise = A_delta_pairwise_byInfected
            elif(self.delta_pairwise_mode == 'infectee'):
                self.A_delta_pairwise = A_delta_pairwise_byInfectee
            elif(self.delta_pairwise_mode == 'min'):
                self.A_delta_pairwise = scipy.sparse.csr_matrix.minimum(A_delta_pairwise_byInfected, A_delta_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'max'):
                self.A_delta_pairwise = scipy.sparse.csr_matrix.maximum(A_delta_pairwise_byInfected, A_delta_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'mean'):
                self.A_delta_pairwise = (A_delta_pairwise_byInfected + A_delta_pairwise_byInfectee)/2
            elif(self.delta_pairwise_mode is None):
                self.A_delta_pairwise = self.A
            else:
                print(""Unrecognized delta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for delta (expected 1xN list/array or NxN 2d array)"")
        #----------------------------------------
        if(self.delta_Q.ndim == 2 and self.delta_Q.shape[0] == self.numNodes and self.delta_Q.shape[1] == self.numNodes):
            self.A_Q_delta_Q_pairwise = self.delta_Q
        elif((self.delta_Q.ndim == 1 and self.delta_Q.shape[0] == self.numNodes) or (self.delta_Q.ndim == 2 and (self.delta_Q.shape[0] == self.numNodes or self.delta_Q.shape[1] == self.numNodes))):
            self.delta_Q = self.delta_Q.reshape((self.numNodes,1))
            # Pre-multiply delta_Q values by the isolation adjacency matrix (""transmission weight connections"")
            A_Q_delta_Q_pairwise_byInfected      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.delta_Q).tocsr()
            A_Q_delta_Q_pairwise_byInfectee      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.delta_Q.T).tocsr()
            #------------------------------
            # Compute the effective pairwise delta values as a function of the infected/infectee pair:
            if(self.delta_pairwise_mode == 'infected'):
                self.A_Q_delta_Q_pairwise = A_Q_delta_Q_pairwise_byInfected
            elif(self.delta_pairwise_mode == 'infectee'):
                self.A_Q_delta_Q_pairwise = A_Q_delta_Q_pairwise_byInfectee
            elif(self.delta_pairwise_mode == 'min'):
                self.A_Q_delta_Q_pairwise = scipy.sparse.csr_matrix.minimum(A_Q_delta_Q_pairwise_byInfected, A_Q_delta_Q_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'max'):
                self.A_Q_delta_Q_pairwise = scipy.sparse.csr_matrix.maximum(A_Q_delta_Q_pairwise_byInfected, A_Q_delta_Q_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'mean'):
                self.A_Q_delta_Q_pairwise = (A_Q_delta_Q_pairwise_byInfected + A_Q_delta_Q_pairwise_byInfectee)/2
            elif(self.delta_pairwise_mode is None):
                self.A_Q_delta_Q_pairwise = self.A
            else:
                print(""Unrecognized delta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for delta_Q (expected 1xN list/array or NxN 2d array)"")

        #----------------------------------------
        # Pre-calculate the pairwise delta*beta values:
        #----------------------------------------
        self.A_deltabeta          = scipy.sparse.csr_matrix.multiply(self.A_delta_pairwise, self.A_beta_pairwise)
        self.A_Q_deltabeta_Q      = scipy.sparse.csr_matrix.multiply(self.A_Q_delta_Q_pairwise, self.A_Q_beta_Q_pairwise)
        if(self.A_beta_asym_pairwise is not None):
            self.A_deltabeta_asym = scipy.sparse.csr_matrix.multiply(self.A_delta_pairwise, self.A_beta_asym_pairwise)
        else:
            self.A_deltabeta_asym = None","numpy.random.rand(self.h.shape[0], self.h.shape[1])",numpy.random.rand(*self.h.shape[:2]),"iterable_zj[0], iterable_zj[1]",*self.h.shape[:2],*self.h.shape[:2],1
seirsplus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/seirsplus/build/lib/seirsplus/models.py,https://github.com/ryansmcgee/seirsplus/tree/master/build/lib/seirsplus/models.py,ExtSEIRSNetworkModel,update_parameters$1913,"def update_parameters(self):

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # Model graphs:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        self.G = self.parameters['G']
        # Adjacency matrix:
        if type(self.G)==numpy.ndarray:
            self.A = scipy.sparse.csr_matrix(self.G)
        elif type(self.G)==networkx.classes.graph.Graph:
            self.A = networkx.adj_matrix(self.G) # adj_matrix gives scipy.sparse csr_matrix
        else:
            raise BaseException(""Input an adjacency matrix or networkx object only."")
        self.numNodes   = int(self.A.shape[1])
        self.degree     = numpy.asarray(self.node_degrees(self.A)).astype(float)
        #----------------------------------------
        if(self.parameters['G_Q'] is None):
            self.G_Q = self.G # If no Q graph is provided, use G in its place
        else:
            self.G_Q = self.parameters['G_Q']
        # Quarantine Adjacency matrix:
        if type(self.G_Q)==numpy.ndarray:
            self.A_Q = scipy.sparse.csr_matrix(self.G_Q)
        elif type(self.G_Q)==networkx.classes.graph.Graph:
            self.A_Q = networkx.adj_matrix(self.G_Q) # adj_matrix gives scipy.sparse csr_matrix
        else:
            raise BaseException(""Input an adjacency matrix or networkx object only."")
        self.numNodes_Q   = int(self.A_Q.shape[1])
        self.degree_Q     = numpy.asarray(self.node_degrees(self.A_Q)).astype(float)
        #----------------------------------------
        assert(self.numNodes == self.numNodes_Q), ""The normal and quarantine adjacency graphs must be of the same size.""

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # Model parameters:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        self.beta           = numpy.array(self.parameters['beta']).reshape((self.numNodes, 1))          if isinstance(self.parameters['beta'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta'], shape=(self.numNodes,1))
        self.beta_asym      = (numpy.array(self.parameters['beta_asym']).reshape((self.numNodes, 1))    if isinstance(self.parameters['beta_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta_asym'], shape=(self.numNodes,1))) if self.parameters['beta_asym'] is not None else self.beta
        self.sigma          = numpy.array(self.parameters['sigma']).reshape((self.numNodes, 1))         if isinstance(self.parameters['sigma'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['sigma'], shape=(self.numNodes,1))
        self.lamda          = numpy.array(self.parameters['lamda']).reshape((self.numNodes, 1))         if isinstance(self.parameters['lamda'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['lamda'], shape=(self.numNodes,1))
        self.gamma          = numpy.array(self.parameters['gamma']).reshape((self.numNodes, 1))         if isinstance(self.parameters['gamma'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma'], shape=(self.numNodes,1))
        self.eta            = numpy.array(self.parameters['eta']).reshape((self.numNodes, 1))           if isinstance(self.parameters['eta'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['eta'], shape=(self.numNodes,1))
        self.gamma_asym     = (numpy.array(self.parameters['gamma_asym']).reshape((self.numNodes, 1))   if isinstance(self.parameters['gamma_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_asym'], shape=(self.numNodes,1))) if self.parameters['gamma_asym'] is not None else self.gamma
        self.gamma_H        = (numpy.array(self.parameters['gamma_H']).reshape((self.numNodes, 1))      if isinstance(self.parameters['gamma_H'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_H'], shape=(self.numNodes,1))) if self.parameters['gamma_H'] is not None else self.gamma
        self.mu_H           = numpy.array(self.parameters['mu_H']).reshape((self.numNodes, 1))          if isinstance(self.parameters['mu_H'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['mu_H'], shape=(self.numNodes,1))
        self.alpha          = numpy.array(self.parameters['alpha']).reshape((self.numNodes, 1))         if isinstance(self.parameters['alpha'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['alpha'], shape=(self.numNodes,1))
        self.xi             = numpy.array(self.parameters['xi']).reshape((self.numNodes, 1))            if isinstance(self.parameters['xi'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['xi'], shape=(self.numNodes,1))
        self.mu_0           = numpy.array(self.parameters['mu_0']).reshape((self.numNodes, 1))          if isinstance(self.parameters['mu_0'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['mu_0'], shape=(self.numNodes,1))
        self.nu             = numpy.array(self.parameters['nu']).reshape((self.numNodes, 1))            if isinstance(self.parameters['nu'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['nu'], shape=(self.numNodes,1))
        self.a              = numpy.array(self.parameters['a']).reshape((self.numNodes, 1))             if isinstance(self.parameters['a'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['a'], shape=(self.numNodes,1))
        self.h              = numpy.array(self.parameters['h']).reshape((self.numNodes, 1))             if isinstance(self.parameters['h'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['h'], shape=(self.numNodes,1))
        self.f              = numpy.array(self.parameters['f']).reshape((self.numNodes, 1))             if isinstance(self.parameters['f'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['f'], shape=(self.numNodes,1))
        self.p              = numpy.array(self.parameters['p']).reshape((self.numNodes, 1))             if isinstance(self.parameters['p'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['p'], shape=(self.numNodes,1))
        self.o              = numpy.array(self.parameters['o']).reshape((self.numNodes, 1))             if isinstance(self.parameters['o'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['o'], shape=(self.numNodes,1))

        self.rand_a = numpy.random.rand(self.a.shape[0], self.a.shape[1])
        self.rand_h = numpy.random.rand(self.h.shape[0], self.h.shape[1])
        self.rand_f = numpy.random.rand(self.f.shape[0], self.f.shape[1])

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # External infection introduction variables:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        self.prevalence_ext     = numpy.array(self.parameters['prevalence_ext']).reshape((self.numNodes, 1)) if isinstance(self.parameters['prevalence_ext'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['prevalence_ext'], shape=(self.numNodes,1))
       
        #----------------------------------------
        # Testing-related parameters:
        #----------------------------------------
        self.beta_Q         = (numpy.array(self.parameters['beta_Q']).reshape((self.numNodes, 1))       if isinstance(self.parameters['beta_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta_Q'], shape=(self.numNodes,1))) if self.parameters['beta_Q'] is not None else self.beta
        self.sigma_Q        = (numpy.array(self.parameters['sigma_Q']).reshape((self.numNodes, 1))      if isinstance(self.parameters['sigma_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['sigma_Q'], shape=(self.numNodes,1))) if self.parameters['sigma_Q'] is not None else self.sigma
        self.lamda_Q        = (numpy.array(self.parameters['lamda_Q']).reshape((self.numNodes, 1))      if isinstance(self.parameters['lamda_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['lamda_Q'], shape=(self.numNodes,1))) if self.parameters['lamda_Q'] is not None else self.lamda
        self.gamma_Q_sym    = (numpy.array(self.parameters['gamma_Q_sym']).reshape((self.numNodes, 1))  if isinstance(self.parameters['gamma_Q_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_Q_sym'], shape=(self.numNodes,1))) if self.parameters['gamma_Q_sym'] is not None else self.gamma
        self.gamma_Q_asym   = (numpy.array(self.parameters['gamma_Q_asym']).reshape((self.numNodes, 1)) if isinstance(self.parameters['gamma_Q_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['gamma_Q_asym'], shape=(self.numNodes,1))) if self.parameters['gamma_Q_asym'] is not None else self.gamma
        self.eta_Q          = (numpy.array(self.parameters['eta_Q']).reshape((self.numNodes, 1))        if isinstance(self.parameters['eta_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['eta_Q'], shape=(self.numNodes,1))) if self.parameters['eta_Q'] is not None else self.eta
        self.alpha_Q        = (numpy.array(self.parameters['alpha_Q']).reshape((self.numNodes, 1))      if isinstance(self.parameters['alpha_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['alpha_Q'], shape=(self.numNodes,1))) if self.parameters['alpha_Q'] is not None else self.alpha
        self.theta_S        = numpy.array(self.parameters['theta_S']).reshape((self.numNodes, 1))       if isinstance(self.parameters['theta_S'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_S'], shape=(self.numNodes,1))
        self.theta_E        = numpy.array(self.parameters['theta_E']).reshape((self.numNodes, 1))       if isinstance(self.parameters['theta_E'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_E'], shape=(self.numNodes,1))
        self.theta_pre      = numpy.array(self.parameters['theta_pre']).reshape((self.numNodes, 1))     if isinstance(self.parameters['theta_pre'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_pre'], shape=(self.numNodes,1))
        self.theta_sym      = numpy.array(self.parameters['theta_sym']).reshape((self.numNodes, 1))     if isinstance(self.parameters['theta_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_sym'], shape=(self.numNodes,1))
        self.theta_asym     = numpy.array(self.parameters['theta_asym']).reshape((self.numNodes, 1))    if isinstance(self.parameters['theta_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['theta_asym'], shape=(self.numNodes,1))
        self.phi_S          = numpy.array(self.parameters['phi_S']).reshape((self.numNodes, 1))         if isinstance(self.parameters['phi_S'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_S'], shape=(self.numNodes,1))
        self.phi_E          = numpy.array(self.parameters['phi_E']).reshape((self.numNodes, 1))         if isinstance(self.parameters['phi_E'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_E'], shape=(self.numNodes,1))
        self.phi_pre        = numpy.array(self.parameters['phi_pre']).reshape((self.numNodes, 1))       if isinstance(self.parameters['phi_pre'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_pre'], shape=(self.numNodes,1))
        self.phi_sym        = numpy.array(self.parameters['phi_sym']).reshape((self.numNodes, 1))       if isinstance(self.parameters['phi_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_sym'], shape=(self.numNodes,1))
        self.phi_asym       = numpy.array(self.parameters['phi_asym']).reshape((self.numNodes, 1))      if isinstance(self.parameters['phi_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['phi_asym'], shape=(self.numNodes,1))
        self.psi_S            = numpy.array(self.parameters['psi_S']).reshape((self.numNodes, 1))           if isinstance(self.parameters['psi_S'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_S'], shape=(self.numNodes,1))
        self.psi_E            = numpy.array(self.parameters['psi_E']).reshape((self.numNodes, 1))           if isinstance(self.parameters['psi_E'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_E'], shape=(self.numNodes,1))
        self.psi_pre          = numpy.array(self.parameters['psi_pre']).reshape((self.numNodes, 1))         if isinstance(self.parameters['psi_pre'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_pre'], shape=(self.numNodes,1))
        self.psi_sym          = numpy.array(self.parameters['psi_sym']).reshape((self.numNodes, 1))         if isinstance(self.parameters['psi_sym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_sym'], shape=(self.numNodes,1))
        self.psi_asym         = numpy.array(self.parameters['psi_asym']).reshape((self.numNodes, 1))        if isinstance(self.parameters['psi_asym'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['psi_asym'], shape=(self.numNodes,1))
        self.q              = numpy.array(self.parameters['q']).reshape((self.numNodes, 1))             if isinstance(self.parameters['q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['q'], shape=(self.numNodes,1))

        #----------------------------------------

        self.beta_pairwise_mode = self.parameters['beta_pairwise_mode']

        #----------------------------------------
        # Global transmission parameters:
        #----------------------------------------
        if(self.beta_pairwise_mode == 'infected' or self.beta_pairwise_mode is None):
            self.beta_global         = numpy.full_like(self.beta, fill_value=numpy.mean(self.beta))
            self.beta_Q_global       = numpy.full_like(self.beta_Q, fill_value=numpy.mean(self.beta_Q))
            self.beta_asym_global    = numpy.full_like(self.beta_asym, fill_value=numpy.mean(self.beta_asym))
        elif(self.beta_pairwise_mode == 'infectee'):
            self.beta_global         = self.beta      
            self.beta_Q_global       = self.beta_Q    
            self.beta_asym_global    = self.beta_asym
        elif(self.beta_pairwise_mode == 'min'):
            self.beta_global         = numpy.minimum(self.beta, numpy.mean(beta)) 
            self.beta_Q_global       = numpy.minimum(self.beta_Q, numpy.mean(beta_Q)) 
            self.beta_asym_global    = numpy.minimum(self.beta_asym, numpy.mean(beta_asym))
        elif(self.beta_pairwise_mode == 'max'):
            self.beta_global         = numpy.maximum(self.beta, numpy.mean(beta)) 
            self.beta_Q_global       = numpy.maximum(self.beta_Q, numpy.mean(beta_Q)) 
            self.beta_asym_global    = numpy.maximum(self.beta_asym, numpy.mean(beta_asym))
        elif(self.beta_pairwise_mode == 'mean'):
            self.beta_global         = (self.beta + numpy.full_like(self.beta, fill_value=numpy.mean(self.beta)))/2
            self.beta_Q_global       = (self.beta_Q + numpy.full_like(self.beta_Q, fill_value=numpy.mean(self.beta_Q)))/2
            self.beta_asym_global    = (self.beta_asym + numpy.full_like(self.beta_asym, fill_value=numpy.mean(self.beta_asym)))/2
            
        #----------------------------------------
        # Local transmission parameters:
        #----------------------------------------
        self.beta_local         = self.beta      if self.parameters['beta_local'] is None      else numpy.array(self.parameters['beta_local'])      if isinstance(self.parameters['beta_local'], (list, numpy.ndarray))      else numpy.full(fill_value=self.parameters['beta_local'], shape=(self.numNodes,1))
        self.beta_Q_local       = self.beta_Q    if self.parameters['beta_Q_local'] is None    else numpy.array(self.parameters['beta_Q_local'])    if isinstance(self.parameters['beta_Q_local'], (list, numpy.ndarray))    else numpy.full(fill_value=self.parameters['beta_Q_local'], shape=(self.numNodes,1))
        self.beta_asym_local    = None           if self.parameters['beta_asym_local'] is None else numpy.array(self.parameters['beta_asym_local']) if isinstance(self.parameters['beta_asym_local'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['beta_asym_local'], shape=(self.numNodes,1))
        #----------------------------------------
        if(self.beta_local.ndim == 2 and self.beta_local.shape[0] == self.numNodes and self.beta_local.shape[1] == self.numNodes):
            self.A_beta_pairwise = self.beta_local
        elif((self.beta_local.ndim == 1 and self.beta_local.shape[0] == self.numNodes) or (self.beta_local.ndim == 2 and (self.beta_local.shape[0] == self.numNodes or self.beta_local.shape[1] == self.numNodes))):
            self.beta_local = self.beta_local.reshape((self.numNodes,1))
            # Pre-multiply beta values by the adjacency matrix (""transmission weight connections"")
            A_beta_pairwise_byInfected = scipy.sparse.csr_matrix.multiply(self.A, self.beta_local.T).tocsr()
            A_beta_pairwise_byInfectee = scipy.sparse.csr_matrix.multiply(self.A, self.beta_local).tocsr()    
            #------------------------------
            # Compute the effective pairwise beta values as a function of the infected/infectee pair:
            if(self.beta_pairwise_mode == 'infected'):
                self.A_beta_pairwise = A_beta_pairwise_byInfected
            elif(self.beta_pairwise_mode == 'infectee'):
                self.A_beta_pairwise = A_beta_pairwise_byInfectee
            elif(self.beta_pairwise_mode == 'min'):
                self.A_beta_pairwise = scipy.sparse.csr_matrix.minimum(A_beta_pairwise_byInfected, A_beta_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'max'):
                self.A_beta_pairwise = scipy.sparse.csr_matrix.maximum(A_beta_pairwise_byInfected, A_beta_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'mean' or self.beta_pairwise_mode is None):
                self.A_beta_pairwise = (A_beta_pairwise_byInfected + A_beta_pairwise_byInfectee)/2
            else:
                print(""Unrecognized beta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for beta_local (expected 1xN list/array or NxN 2d array)"")
        #----------------------------------------
        if(self.beta_Q_local.ndim == 2 and self.beta_Q_local.shape[0] == self.numNodes and self.beta_Q_local.shape[1] == self.numNodes):
            self.A_Q_beta_Q_pairwise = self.beta_Q_local
        elif((self.beta_Q_local.ndim == 1 and self.beta_Q_local.shape[0] == self.numNodes) or (self.beta_Q_local.ndim == 2 and (self.beta_Q_local.shape[0] == self.numNodes or self.beta_Q_local.shape[1] == self.numNodes))):
            self.beta_Q_local = self.beta_Q_local.reshape((self.numNodes,1))
            # Pre-multiply beta_Q values by the isolation adjacency matrix (""transmission weight connections"")
            A_Q_beta_Q_pairwise_byInfected      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.beta_Q_local.T).tocsr()
            A_Q_beta_Q_pairwise_byInfectee      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.beta_Q_local).tocsr()
            #------------------------------
            # Compute the effective pairwise beta values as a function of the infected/infectee pair:
            if(self.beta_pairwise_mode == 'infected'):
                self.A_Q_beta_Q_pairwise = A_Q_beta_Q_pairwise_byInfected
            elif(self.beta_pairwise_mode == 'infectee'):
                self.A_Q_beta_Q_pairwise = A_Q_beta_Q_pairwise_byInfectee
            elif(self.beta_pairwise_mode == 'min'):
                self.A_Q_beta_Q_pairwise = scipy.sparse.csr_matrix.minimum(A_Q_beta_Q_pairwise_byInfected, A_Q_beta_Q_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'max'):
                self.A_Q_beta_Q_pairwise = scipy.sparse.csr_matrix.maximum(A_Q_beta_Q_pairwise_byInfected, A_Q_beta_Q_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'mean' or self.beta_pairwise_mode is None):
                self.A_Q_beta_Q_pairwise = (A_Q_beta_Q_pairwise_byInfected + A_Q_beta_Q_pairwise_byInfectee)/2
            else:
                print(""Unrecognized beta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for beta_Q_local (expected 1xN list/array or NxN 2d array)"")
        #----------------------------------------
        if(self.beta_asym_local is None):
            self.A_beta_asym_pairwise = None
        elif(self.beta_asym_local.ndim == 2 and self.beta_asym_local.shape[0] == self.numNodes and self.beta_asym_local.shape[1] == self.numNodes):
            self.A_beta_asym_pairwise = self.beta_asym_local
        elif((self.beta_asym_local.ndim == 1 and self.beta_asym_local.shape[0] == self.numNodes) or (self.beta_asym_local.ndim == 2 and (self.beta_asym_local.shape[0] == self.numNodes or self.beta_asym_local.shape[1] == self.numNodes))):
            self.beta_asym_local = self.beta_asym_local.reshape((self.numNodes,1))
            # Pre-multiply beta_asym values by the adjacency matrix (""transmission weight connections"")
            A_beta_asym_pairwise_byInfected      = scipy.sparse.csr_matrix.multiply(self.A, self.beta_asym_local.T).tocsr()
            A_beta_asym_pairwise_byInfectee      = scipy.sparse.csr_matrix.multiply(self.A, self.beta_asym_local).tocsr()
            #------------------------------
            # Compute the effective pairwise beta values as a function of the infected/infectee pair:
            if(self.beta_pairwise_mode == 'infected'):
                self.A_beta_asym_pairwise = A_beta_asym_pairwise_byInfected
            elif(self.beta_pairwise_mode == 'infectee'):
                self.A_beta_asym_pairwise = A_beta_asym_pairwise_byInfectee
            elif(self.beta_pairwise_mode == 'min'):
                self.A_beta_asym_pairwise = scipy.sparse.csr_matrix.minimum(A_beta_asym_pairwise_byInfected, A_beta_asym_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'max'):
                self.A_beta_asym_pairwise = scipy.sparse.csr_matrix.maximum(A_beta_asym_pairwise_byInfected, A_beta_asym_pairwise_byInfectee)
            elif(self.beta_pairwise_mode == 'mean' or self.beta_pairwise_mode is None):
                self.A_beta_asym_pairwise = (A_beta_asym_pairwise_byInfected + A_beta_asym_pairwise_byInfectee)/2
            else:
                print(""Unrecognized beta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for beta_asym_local (expected 1xN list/array or NxN 2d array)"")

        #----------------------------------------
        # Degree-based transmission scaling parameters:
        #----------------------------------------
        self.delta_pairwise_mode = self.parameters['delta_pairwise_mode']
        with numpy.errstate(divide='ignore'): # ignore log(0) warning, then convert log(0) = -inf -> 0.0
            self.delta               = numpy.log(self.degree)/numpy.log(numpy.mean(self.degree))     if self.parameters['delta'] is None   else numpy.array(self.parameters['delta'])   if isinstance(self.parameters['delta'], (list, numpy.ndarray))   else numpy.full(fill_value=self.parameters['delta'], shape=(self.numNodes,1))
            self.delta_Q             = numpy.log(self.degree_Q)/numpy.log(numpy.mean(self.degree_Q)) if self.parameters['delta_Q'] is None else numpy.array(self.parameters['delta_Q']) if isinstance(self.parameters['delta_Q'], (list, numpy.ndarray)) else numpy.full(fill_value=self.parameters['delta_Q'], shape=(self.numNodes,1))
        self.delta[numpy.isneginf(self.delta)] = 0.0
        self.delta_Q[numpy.isneginf(self.delta_Q)] = 0.0
        #----------------------------------------
        if(self.delta.ndim == 2 and self.delta.shape[0] == self.numNodes and self.delta.shape[1] == self.numNodes):
            self.A_delta_pairwise = self.delta
        elif((self.delta.ndim == 1 and self.delta.shape[0] == self.numNodes) or (self.delta.ndim == 2 and (self.delta.shape[0] == self.numNodes or self.delta.shape[1] == self.numNodes))):
            self.delta = self.delta.reshape((self.numNodes,1))
            # Pre-multiply delta values by the adjacency matrix (""transmission weight connections"")
            A_delta_pairwise_byInfected = scipy.sparse.csr_matrix.multiply(self.A, self.delta.T).tocsr()
            A_delta_pairwise_byInfectee = scipy.sparse.csr_matrix.multiply(self.A, self.delta).tocsr()    
            #------------------------------
            # Compute the effective pairwise delta values as a function of the infected/infectee pair:
            if(self.delta_pairwise_mode == 'infected'):
                self.A_delta_pairwise = A_delta_pairwise_byInfected
            elif(self.delta_pairwise_mode == 'infectee'):
                self.A_delta_pairwise = A_delta_pairwise_byInfectee
            elif(self.delta_pairwise_mode == 'min'):
                self.A_delta_pairwise = scipy.sparse.csr_matrix.minimum(A_delta_pairwise_byInfected, A_delta_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'max'):
                self.A_delta_pairwise = scipy.sparse.csr_matrix.maximum(A_delta_pairwise_byInfected, A_delta_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'mean'):
                self.A_delta_pairwise = (A_delta_pairwise_byInfected + A_delta_pairwise_byInfectee)/2
            elif(self.delta_pairwise_mode is None):
                self.A_delta_pairwise = self.A
            else:
                print(""Unrecognized delta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for delta (expected 1xN list/array or NxN 2d array)"")
        #----------------------------------------
        if(self.delta_Q.ndim == 2 and self.delta_Q.shape[0] == self.numNodes and self.delta_Q.shape[1] == self.numNodes):
            self.A_Q_delta_Q_pairwise = self.delta_Q
        elif((self.delta_Q.ndim == 1 and self.delta_Q.shape[0] == self.numNodes) or (self.delta_Q.ndim == 2 and (self.delta_Q.shape[0] == self.numNodes or self.delta_Q.shape[1] == self.numNodes))):
            self.delta_Q = self.delta_Q.reshape((self.numNodes,1))
            # Pre-multiply delta_Q values by the isolation adjacency matrix (""transmission weight connections"")
            A_Q_delta_Q_pairwise_byInfected      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.delta_Q).tocsr()
            A_Q_delta_Q_pairwise_byInfectee      = scipy.sparse.csr_matrix.multiply(self.A_Q, self.delta_Q.T).tocsr()
            #------------------------------
            # Compute the effective pairwise delta values as a function of the infected/infectee pair:
            if(self.delta_pairwise_mode == 'infected'):
                self.A_Q_delta_Q_pairwise = A_Q_delta_Q_pairwise_byInfected
            elif(self.delta_pairwise_mode == 'infectee'):
                self.A_Q_delta_Q_pairwise = A_Q_delta_Q_pairwise_byInfectee
            elif(self.delta_pairwise_mode == 'min'):
                self.A_Q_delta_Q_pairwise = scipy.sparse.csr_matrix.minimum(A_Q_delta_Q_pairwise_byInfected, A_Q_delta_Q_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'max'):
                self.A_Q_delta_Q_pairwise = scipy.sparse.csr_matrix.maximum(A_Q_delta_Q_pairwise_byInfected, A_Q_delta_Q_pairwise_byInfectee)
            elif(self.delta_pairwise_mode == 'mean'):
                self.A_Q_delta_Q_pairwise = (A_Q_delta_Q_pairwise_byInfected + A_Q_delta_Q_pairwise_byInfectee)/2
            elif(self.delta_pairwise_mode is None):
                self.A_Q_delta_Q_pairwise = self.A
            else:
                print(""Unrecognized delta_pairwise_mode value (support for 'infected', 'infectee', 'min', 'max', and 'mean')."")
        else:
            print(""Invalid values given for delta_Q (expected 1xN list/array or NxN 2d array)"")

        #----------------------------------------
        # Pre-calculate the pairwise delta*beta values:
        #----------------------------------------
        self.A_deltabeta          = scipy.sparse.csr_matrix.multiply(self.A_delta_pairwise, self.A_beta_pairwise)
        self.A_Q_deltabeta_Q      = scipy.sparse.csr_matrix.multiply(self.A_Q_delta_Q_pairwise, self.A_Q_beta_Q_pairwise)
        if(self.A_beta_asym_pairwise is not None):
            self.A_deltabeta_asym = scipy.sparse.csr_matrix.multiply(self.A_delta_pairwise, self.A_beta_asym_pairwise)
        else:
            self.A_deltabeta_asym = None","numpy.random.rand(self.f.shape[0], self.f.shape[1])",numpy.random.rand(*self.f.shape[:2]),"iterable_zj[0], iterable_zj[1]",*self.f.shape[:2],*self.f.shape[:2],1
liif,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/liif/models/misc.py,https://github.com/yinboc/liif/tree/master/models/misc.py,MetaSR,query_rgb$31,"def query_rgb(self, coord, cell=None):
        feat = self.feat
        feat = F.unfold(feat, 3, padding=1).view(
            feat.shape[0], feat.shape[1] * 9, feat.shape[2], feat.shape[3])

        feat_coord = make_coord(feat.shape[-2:], flatten=False).cuda()
        feat_coord[:, :, 0] -= (2 / feat.shape[-2]) / 2
        feat_coord[:, :, 1] -= (2 / feat.shape[-1]) / 2
        feat_coord = feat_coord.permute(2, 0, 1) \
            .unsqueeze(0).expand(feat.shape[0], 2, *feat.shape[-2:])

        coord_ = coord.clone()
        coord_[:, :, 0] -= cell[:, :, 0] / 2
        coord_[:, :, 1] -= cell[:, :, 1] / 2
        coord_q = (coord_ + 1e-6).clamp(-1 + 1e-6, 1 - 1e-6)
        q_feat = F.grid_sample(
            feat, coord_q.flip(-1).unsqueeze(1),
            mode='nearest', align_corners=False)[:, :, 0, :] \
            .permute(0, 2, 1)
        q_coord = F.grid_sample(
            feat_coord, coord_q.flip(-1).unsqueeze(1),
            mode='nearest', align_corners=False)[:, :, 0, :] \
            .permute(0, 2, 1)

        rel_coord = coord_ - q_coord
        rel_coord[:, :, 0] *= feat.shape[-2] / 2
        rel_coord[:, :, 1] *= feat.shape[-1] / 2

        r_rev = cell[:, :, 0] * (feat.shape[-2] / 2)
        inp = torch.cat([rel_coord, r_rev.unsqueeze(-1)], dim=-1)

        bs, q = coord.shape[:2]
        pred = self.imnet(inp.view(bs * q, -1)).view(bs * q, feat.shape[1], 3)
        pred = torch.bmm(q_feat.contiguous().view(bs * q, 1, -1), pred)
        pred = pred.view(bs, q, 3)
        return pred","F.unfold(feat, 3, padding=1).view(feat.shape[0], feat.shape[1] * 9, feat.shape[2], feat.shape[3])","F.unfold(feat, 3, padding=1).view(*feat.shape[0:3:2] = [feat.shape[0], feat.shape[1] * 9, feat.shape[2]], feat.shape[3])","iterable_zj[0], iterable_zj[1] * 9, iterable_zj[2]","*feat.shape[0:3:2] = [feat.shape[0], feat.shape[1] * 9, feat.shape[2]]",*feat.shape[2:4],0
torchdrug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torchdrug/torchdrug/models/neuralfp.py,https://github.com/DeepGraphLearning/torchdrug/tree/master/torchdrug/models/neuralfp.py,NeuralFingerprint,__init__$31,"def __init__(self, input_dim, output_dim, hidden_dims, edge_input_dim=None, short_cut=False, batch_norm=False,
                 activation=""relu"", concat_hidden=False, readout=""sum""):
        super(NeuralFingerprint, self).__init__()

        if not isinstance(hidden_dims, Sequence):
            hidden_dims = [hidden_dims]
        self.input_dim = input_dim
        self.output_dim = output_dim * (len(hidden_dims) if concat_hidden else 1)
        self.dims = [input_dim] + list(hidden_dims)
        self.short_cut = short_cut
        self.concat_hidden = concat_hidden

        self.layers = nn.ModuleList()
        self.linears = nn.ModuleList()
        for i in range(len(self.dims) - 1):
            self.layers.append(layers.NeuralFingerprintConv(self.dims[i], self.dims[i + 1], edge_input_dim,
                                                            batch_norm, activation))
            self.linears.append(nn.Linear(self.dims[i + 1], output_dim))

        if readout == ""sum"":
            self.readout = layers.SumReadout()
        elif readout == ""mean"":
            self.readout = layers.MeanReadout()
        else:
            raise ValueError(""Unknown readout `%s`"" % readout)","layers.NeuralFingerprintConv(self.dims[i], self.dims[i + 1], edge_input_dim, batch_norm, activation)","layers.NeuralFingerprintConv(*self.dims[i:i + 2], edge_input_dim, batch_norm, activation)","iterable_zj[i], iterable_zj[i + 1]",*self.dims[i:i+2],*self.dims[i:i + 2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/topi/cuda/sort.py,https://github.com/apache/tvm/tree/master/python/tvm/topi/cuda/sort.py,,argsort$738,"def argsort(data, axis=-1, is_ascend=1, dtype=""float32"", ret_type=""indices""):
    """"""Performs sorting along the given axis and returns an array of indices
    having same shape as an input array that index data in sorted order.

    Parameters
    ----------
    data: tvm.te.Tensor
        The input array.

    axis : int, optional
        Axis long which to sort the input tensor.

    is_ascend : boolean, optional
        Whether to sort in ascending or descending order.

    dtype : string, optional
        DType of the output indices.

    ret_type : string, optional
        The return type [both, indices].
        ""both"": return both sorted data and indices.
        ""indices"": return sorted indices only.

    Returns
    -------
    out : tvm.te.Tensor
        The output of this function.
    """"""
    ndim = len(data.shape)
    axis = ndim + axis if axis < 0 else axis
    if axis != ndim - 1:
        # Prepare for sorting along axis -1.
        axes = swap(list(range(ndim)), axis)
        data = transpose(data, axes)

    value_buf = tvm.tir.decl_buffer(data.shape, data.dtype, ""value_buf"", data_alignment=8)
    value_swap_buf = tvm.tir.decl_buffer(data.shape, data.dtype, ""value_swap_buf"", data_alignment=8)
    indices_buf = tvm.tir.decl_buffer(data.shape, dtype, ""out_buf"", data_alignment=8)
    indices_swap_buf = tvm.tir.decl_buffer(data.shape, dtype, ""out_swap_buf"", data_alignment=8)

    outs = te.extern(
        [data.shape, data.shape, data.shape, data.shape],
        [data],
        lambda ins, outs: sort_ir(
            ins[0],
            outs[0],
            outs[2],
            -1,
            is_ascend,
            indices_out=outs[1],
            indices_out_swap=outs[3],
        ),
        out_buffers=[value_buf, indices_buf, value_swap_buf, indices_swap_buf],
        name=""argsort_gpu"",
        tag=""argsort_gpu"",
    )

    if axis != ndim - 1:
        axes = swap(list(range(ndim)), axis)
        outs = [transpose(out, axes) for out in outs]

    if ret_type == ""indices"":
        return outs[1]

    return outs[0], outs[1]","sort_ir(ins[0], outs[0], outs[2], -1, is_ascend, indices_out=outs[1], indices_out_swap=outs[3])",Cannot refactor,"iterable_zj[0], outs[0], outs[2]",,*outs[:4:2],0
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->a', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->ab', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->bc', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->cd', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1
mmcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmcv/mmcv/ops/deform_roi_pool.py,https://github.com/open-mmlab/mmcv/tree/master/mmcv/ops/deform_roi_pool.py,ModulatedDeformRoIPoolPack,forward$190,"def forward(self, input, rois):
        assert input.size(1) == self.output_channels
        x = deform_roi_pool(input, rois, None, self.output_size,
                            self.spatial_scale, self.sampling_ratio,
                            self.gamma)
        rois_num = rois.size(0)
        offset = self.offset_fc(x.view(rois_num, -1))
        offset = offset.view(rois_num, 2, self.output_size[0],
                             self.output_size[1])
        mask = self.mask_fc(x.view(rois_num, -1))
        mask = mask.view(rois_num, 1, self.output_size[0], self.output_size[1])
        d = deform_roi_pool(input, rois, offset, self.output_size,
                            self.spatial_scale, self.sampling_ratio,
                            self.gamma)
        return d * mask","offset.view(rois_num, 2, self.output_size[0], self.output_size[1])","offset.view(rois_num, 2, *self.output_size[:2])","iterable_zj[0], iterable_zj[1]",*self.output_size[:2],*self.output_size[:2],1
mmcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmcv/mmcv/ops/deform_roi_pool.py,https://github.com/open-mmlab/mmcv/tree/master/mmcv/ops/deform_roi_pool.py,ModulatedDeformRoIPoolPack,forward$190,"def forward(self, input, rois):
        assert input.size(1) == self.output_channels
        x = deform_roi_pool(input, rois, None, self.output_size,
                            self.spatial_scale, self.sampling_ratio,
                            self.gamma)
        rois_num = rois.size(0)
        offset = self.offset_fc(x.view(rois_num, -1))
        offset = offset.view(rois_num, 2, self.output_size[0],
                             self.output_size[1])
        mask = self.mask_fc(x.view(rois_num, -1))
        mask = mask.view(rois_num, 1, self.output_size[0], self.output_size[1])
        d = deform_roi_pool(input, rois, offset, self.output_size,
                            self.spatial_scale, self.sampling_ratio,
                            self.gamma)
        return d * mask","mask.view(rois_num, 1, self.output_size[0], self.output_size[1])","mask.view(rois_num, 1, *self.output_size[:2])","iterable_zj[0], iterable_zj[1]",*self.output_size[:2],*self.output_size[:2],1
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/ppdet/slim/distill.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/ppdet/slim/distill.py,DistillYOLOv3Loss,forward$93,"def forward(self, teacher_model, student_model):
        teacher_distill_pairs = teacher_model.yolo_head.loss.distill_pairs
        student_distill_pairs = student_model.yolo_head.loss.distill_pairs
        distill_reg_loss, distill_cls_loss, distill_obj_loss = [], [], []
        for s_pair, t_pair in zip(student_distill_pairs, teacher_distill_pairs):
            distill_reg_loss.append(
                self.obj_weighted_reg(s_pair[0], s_pair[1], s_pair[2], s_pair[
                    3], t_pair[0], t_pair[1], t_pair[2], t_pair[3], t_pair[4]))
            distill_cls_loss.append(
                self.obj_weighted_cls(s_pair[5], t_pair[5], t_pair[4]))
            distill_obj_loss.append(self.obj_loss(s_pair[4], t_pair[4]))
        distill_reg_loss = paddle.add_n(distill_reg_loss)
        distill_cls_loss = paddle.add_n(distill_cls_loss)
        distill_obj_loss = paddle.add_n(distill_obj_loss)
        loss = (distill_reg_loss + distill_cls_loss + distill_obj_loss
                ) * self.weight
        return loss","self.obj_weighted_reg(s_pair[0], s_pair[1], s_pair[2], s_pair[3], t_pair[0], t_pair[1], t_pair[2], t_pair[3], t_pair[4])","self.obj_weighted_reg(*s_pair[:4], t_pair[0], t_pair[1], t_pair[2], t_pair[3], t_pair[4])","iterable_zj[0], iterable_zj[1], iterable_zj[2], iterable_zj[3]",*s_pair[:4],*s_pair[:4],1
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/ppdet/slim/distill.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/ppdet/slim/distill.py,DistillYOLOv3Loss,forward$93,"def forward(self, teacher_model, student_model):
        teacher_distill_pairs = teacher_model.yolo_head.loss.distill_pairs
        student_distill_pairs = student_model.yolo_head.loss.distill_pairs
        distill_reg_loss, distill_cls_loss, distill_obj_loss = [], [], []
        for s_pair, t_pair in zip(student_distill_pairs, teacher_distill_pairs):
            distill_reg_loss.append(
                self.obj_weighted_reg(s_pair[0], s_pair[1], s_pair[2], s_pair[
                    3], t_pair[0], t_pair[1], t_pair[2], t_pair[3], t_pair[4]))
            distill_cls_loss.append(
                self.obj_weighted_cls(s_pair[5], t_pair[5], t_pair[4]))
            distill_obj_loss.append(self.obj_loss(s_pair[4], t_pair[4]))
        distill_reg_loss = paddle.add_n(distill_reg_loss)
        distill_cls_loss = paddle.add_n(distill_cls_loss)
        distill_obj_loss = paddle.add_n(distill_obj_loss)
        loss = (distill_reg_loss + distill_cls_loss + distill_obj_loss
                ) * self.weight
        return loss","self.obj_weighted_reg(s_pair[0], s_pair[1], s_pair[2], s_pair[3], t_pair[0], t_pair[1], t_pair[2], t_pair[3], t_pair[4])","self.obj_weighted_reg(s_pair[0], s_pair[1], s_pair[2], s_pair[3], *t_pair[:3], t_pair[3], t_pair[4])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*t_pair[:3],*t_pair[:5],0
cms,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cms/cms/service/scoringoperations.py,https://github.com/amfoss/cms/tree/master/cms/service/scoringoperations.py,,get_operations$49,"def get_operations(session):
    """"""Return all the operations to do for all submissions.

    session (Session): the database session to use.

    return ([ScoringOperation, float]): a list of operations and
        timestamps.

    """"""
    # Retrieve all the compilation operations for submissions
    # already having a result for a dataset to judge.
    results = session.query(Submission)\
        .join(Submission.task)\
        .join(Submission.results)\
        .join(SubmissionResult.dataset)\
        .filter(
            (FILTER_DATASETS_TO_JUDGE) &
            (FILTER_SUBMISSION_RESULTS_TO_SCORE))\
        .with_entities(Submission.id, Dataset.id, Submission.timestamp)\
        .all()

    return [(ScoringOperation(result[0], result[1]), result[2])
            for result in results]","ScoringOperation(result[0], result[1])",ScoringOperation(*result[:2]),"iterable_zj[0], iterable_zj[1]",*result[:2],*result[:2],1
textflint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/textflint/textflint/input/component/sample/wsd_sample.py,https://github.com/textflint/textflint/tree/master/textflint/input/component/sample/wsd_sample.py,WSDSample,get_idx_list$114,"def get_idx_list(self, indices):
        r""""""
        Get index list from indices
        :param list indices: a list of index varying in type(int,list,slice)
        :return: list idx_list:a list of index(int)
        """"""
        idx_list = []
        # check validity
        for span in indices:
            assert isinstance(span, (int, list, slice))
            if isinstance(span, int):
                assert span >= 0, 'invalid indices'
            if isinstance(span, list):
                assert len(span) == 2, 'invalid indices'
            if isinstance(span, slice):
                assert span.start >= 0, 'invalid indices'
                assert span.stop >= span.start, 'invalid indices'
                assert span.step == 1 or span.step is None, 'invalid indices'
        for span in indices:
            if isinstance(span, int):
                idx_list.append(span)
            elif isinstance(span, list):
                idx_list.extend(range(span[0], span[1]))
            elif isinstance(span, slice):
                idx_list.extend(range(span.stop)[span])
        # get all positions according to the mixed list
        idx_list = sorted(list(set(idx_list)))
        return idx_list","range(span[0], span[1])",range(*span[:2]),"iterable_zj[0], iterable_zj[1]",*span[:2],*span[:2],1
DSB2017,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DSB2017/training/classifier/net_detector_3.py,https://github.com/lfz/DSB2017/tree/master/training/classifier/net_detector_3.py,Net,forward$103,"def forward(self, x, coord):
        #x = (x-128.)/128.
        out = self.preBlock(x)#16
        out_pool,indices0 = self.maxpool1(out)
        out1 = self.forw1(out_pool)#32
        out1_pool,indices1 = self.maxpool2(out1)
        out2 = self.forw2(out1_pool)#64
        #out2 = self.drop(out2)
        out2_pool,indices2 = self.maxpool3(out2)
        out3 = self.forw3(out2_pool)#96
        out3_pool,indices3 = self.maxpool4(out3)
        out4 = self.forw4(out3_pool)#96
        #out4 = self.drop(out4)
        
        rev3 = self.path1(out4)
        comb3 = self.back3(torch.cat((rev3, out3), 1))#96+96
        #comb3 = self.drop(comb3)
        rev2 = self.path2(comb3)
        
        feat = self.back2(torch.cat((rev2, out2,coord), 1))#64+64
        comb2 = self.drop(feat)
        out = self.output(comb2)
        size = out.size()
        out = out.view(out.size(0), out.size(1), -1)
        #out = out.transpose(1, 4).transpose(1, 2).transpose(2, 3).contiguous()
        out = out.transpose(1, 2).contiguous().view(size[0], size[2], size[3], size[4], len(config['anchors']), 5)
        #out = out.view(-1, 5)
        return feat,out","out.transpose(1, 2).contiguous().view(size[0], size[2], size[3], size[4], len(config['anchors']), 5)","out.transpose(1, 2).contiguous().view(*size[::2], len(config['anchors']), 5)","iterable_zj[0], iterable_zj[2], iterable_zj[3], iterable_zj[4]",*size[::2],*size[:4:2],0
fgmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fgmk/fgmk/dock/charas_palette_wdgt.py,https://github.com/ericoporto/fgmk/tree/master/fgmk/dock/charas_palette_wdgt.py,CharasPalWidget,addCharaAction$44,"def addCharaAction(self, position=(0, 0), chara=None, onmap=True):
        if (chara == None):
            chara = self.myCharaSelector.getSelected()

        if (chara != None):
            scale = self.mapWdgt.myScale / 2.0
            if(self.positionEmpty(position)):
                item = persona.MiniCharaTile(
                    None, current_project.settings, chara, (0, 0), scale)
                item.rightClicked.connect(self.autodelete)
                self.mapWdgt.Grid.addWidget(item, position[1], position[0])
                if(onmap):
                    self.pMap.insertChara(position[0], position[1], chara)
                self.charaslist.append((chara, position, item))","self.pMap.insertChara(position[0], position[1], chara)","self.pMap.insertChara(*position[:2], chara)","iterable_zj[0], iterable_zj[1]",*position[:2],*position[:2],1
tf-cpn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-cpn/models/COCO.res101.384x288.CPN/dataset.py,https://github.com/chenyilun95/tf-cpn/tree/master/models/COCO.res101.384x288.CPN/dataset.py,,Preprocessing$162,"def Preprocessing(d, stage='train'):
    height, width = cfg.data_shape
    imgs = []
    labels = []
    valids = []
    if cfg.use_seg:
        segms = []

    vis = False
    img = cv2.imread(os.path.join(cfg.img_path, d['imgpath']))
    #hack(multiprocessing data provider)
    while img is None:
        print('read none image')
        time.sleep(np.random.rand() * 5)
        img = cv2.imread(os.path.join(cfg.img_path, d['imgpath']))
    add = max(img.shape[0], img.shape[1])
    bimg = cv2.copyMakeBorder(img, add, add, add, add, borderType=cv2.BORDER_CONSTANT,
                              value=cfg.pixel_means.reshape(-1))

    bbox = np.array(d['bbox']).reshape(4, ).astype(np.float32)
    bbox[:2] += add

    if 'joints' in d:
        joints = np.array(d['joints']).reshape(cfg.nr_skeleton, 3).astype(np.float32)
        joints[:, :2] += add
        inds = np.where(joints[:, -1] == 0)
        joints[inds, :2] = -1000000

    crop_width = bbox[2] * (1 + cfg.imgExtXBorder * 2)
    crop_height = bbox[3] * (1 + cfg.imgExtYBorder * 2)
    objcenter = np.array([bbox[0] + bbox[2] / 2., bbox[1] + bbox[3] / 2.])

    if stage == 'train':
        crop_width = crop_width * (1 + 0.25)
        crop_height = crop_height * (1 + 0.25)

    if crop_height / height > crop_width / width:
        crop_size = crop_height
        min_shape = height
    else:
        crop_size = crop_width
        min_shape = width
    crop_size = min(crop_size, objcenter[0] / width * min_shape * 2. - 1.)
    crop_size = min(crop_size, (bimg.shape[1] - objcenter[0]) / width * min_shape * 2. - 1)
    crop_size = min(crop_size, objcenter[1] / height * min_shape * 2. - 1.)
    crop_size = min(crop_size, (bimg.shape[0] - objcenter[1]) / height * min_shape * 2. - 1)

    min_x = int(objcenter[0] - crop_size / 2. / min_shape * width)
    max_x = int(objcenter[0] + crop_size / 2. / min_shape * width)
    min_y = int(objcenter[1] - crop_size / 2. / min_shape * height)
    max_y = int(objcenter[1] + crop_size / 2. / min_shape * height)

    x_ratio = float(width) / (max_x - min_x)
    y_ratio = float(height) / (max_y - min_y)

    if 'joints' in d:
        joints[:, 0] = joints[:, 0] - min_x
        joints[:, 1] = joints[:, 1] - min_y

        joints[:, 0] *= x_ratio
        joints[:, 1] *= y_ratio
        label = joints[:, :2].copy()
        valid = joints[:, 2].copy()

    img = cv2.resize(bimg[min_y:max_y, min_x:max_x, :], (width, height))

    if stage != 'train':
        details = np.asarray([min_x - add, min_y - add, max_x - add, max_y - add])

    if cfg.use_seg is True and 'segmentation' in d:
        seg = get_seg(ori_img.shape[0], ori_img.shape[1], d['segmentation'])
        add = max(seg.shape[0], seg.shape[1])
        bimg = cv2.copyMakeBorder(seg, add, add, add, add, borderType=cv2.BORDER_CONSTANT, value=(0, 0, 0))
        seg = cv2.resize(bimg[min_y:max_y, min_x:max_x], (width, height))
        segms.append(seg)

    if vis:
        tmpimg = img.copy()
        from utils.visualize import draw_skeleton
        draw_skeleton(tmpimg, label.astype(int))
        cv2.imwrite('vis.jpg', tmpimg)
        from IPython import embed; embed()

    img = img - cfg.pixel_means
    if cfg.pixel_norm:
        img = img / 255.
    img = img.transpose(2, 0, 1)
    imgs.append(img)
    if 'joints' in d:
        labels.append(label.reshape(-1))
        valids.append(valid.reshape(-1))

    if stage == 'train':
        imgs, labels, valids = data_augmentation(imgs, labels, valids)
        heatmaps15 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                        gaussian_kernel=cfg.gk15)
        heatmaps11 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                        gaussian_kernel=cfg.gk11)
        heatmaps9 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                       gaussian_kernel=cfg.gk9)
        heatmaps7 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                       gaussian_kernel=cfg.gk7)

        return [imgs.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps15.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps11.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps9.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps7.astype(np.float32).transpose(0, 2, 3, 1),
                valids.astype(np.float32)]
    else:
        return [np.asarray(imgs).astype(np.float32), details]","max(img.shape[0], img.shape[1])",max(*img.shape[:2]),"iterable_zj[0], iterable_zj[1]",*img.shape[:2],*img.shape[:2],1
tf-cpn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-cpn/models/COCO.res101.384x288.CPN/dataset.py,https://github.com/chenyilun95/tf-cpn/tree/master/models/COCO.res101.384x288.CPN/dataset.py,,Preprocessing$162,"def Preprocessing(d, stage='train'):
    height, width = cfg.data_shape
    imgs = []
    labels = []
    valids = []
    if cfg.use_seg:
        segms = []

    vis = False
    img = cv2.imread(os.path.join(cfg.img_path, d['imgpath']))
    #hack(multiprocessing data provider)
    while img is None:
        print('read none image')
        time.sleep(np.random.rand() * 5)
        img = cv2.imread(os.path.join(cfg.img_path, d['imgpath']))
    add = max(img.shape[0], img.shape[1])
    bimg = cv2.copyMakeBorder(img, add, add, add, add, borderType=cv2.BORDER_CONSTANT,
                              value=cfg.pixel_means.reshape(-1))

    bbox = np.array(d['bbox']).reshape(4, ).astype(np.float32)
    bbox[:2] += add

    if 'joints' in d:
        joints = np.array(d['joints']).reshape(cfg.nr_skeleton, 3).astype(np.float32)
        joints[:, :2] += add
        inds = np.where(joints[:, -1] == 0)
        joints[inds, :2] = -1000000

    crop_width = bbox[2] * (1 + cfg.imgExtXBorder * 2)
    crop_height = bbox[3] * (1 + cfg.imgExtYBorder * 2)
    objcenter = np.array([bbox[0] + bbox[2] / 2., bbox[1] + bbox[3] / 2.])

    if stage == 'train':
        crop_width = crop_width * (1 + 0.25)
        crop_height = crop_height * (1 + 0.25)

    if crop_height / height > crop_width / width:
        crop_size = crop_height
        min_shape = height
    else:
        crop_size = crop_width
        min_shape = width
    crop_size = min(crop_size, objcenter[0] / width * min_shape * 2. - 1.)
    crop_size = min(crop_size, (bimg.shape[1] - objcenter[0]) / width * min_shape * 2. - 1)
    crop_size = min(crop_size, objcenter[1] / height * min_shape * 2. - 1.)
    crop_size = min(crop_size, (bimg.shape[0] - objcenter[1]) / height * min_shape * 2. - 1)

    min_x = int(objcenter[0] - crop_size / 2. / min_shape * width)
    max_x = int(objcenter[0] + crop_size / 2. / min_shape * width)
    min_y = int(objcenter[1] - crop_size / 2. / min_shape * height)
    max_y = int(objcenter[1] + crop_size / 2. / min_shape * height)

    x_ratio = float(width) / (max_x - min_x)
    y_ratio = float(height) / (max_y - min_y)

    if 'joints' in d:
        joints[:, 0] = joints[:, 0] - min_x
        joints[:, 1] = joints[:, 1] - min_y

        joints[:, 0] *= x_ratio
        joints[:, 1] *= y_ratio
        label = joints[:, :2].copy()
        valid = joints[:, 2].copy()

    img = cv2.resize(bimg[min_y:max_y, min_x:max_x, :], (width, height))

    if stage != 'train':
        details = np.asarray([min_x - add, min_y - add, max_x - add, max_y - add])

    if cfg.use_seg is True and 'segmentation' in d:
        seg = get_seg(ori_img.shape[0], ori_img.shape[1], d['segmentation'])
        add = max(seg.shape[0], seg.shape[1])
        bimg = cv2.copyMakeBorder(seg, add, add, add, add, borderType=cv2.BORDER_CONSTANT, value=(0, 0, 0))
        seg = cv2.resize(bimg[min_y:max_y, min_x:max_x], (width, height))
        segms.append(seg)

    if vis:
        tmpimg = img.copy()
        from utils.visualize import draw_skeleton
        draw_skeleton(tmpimg, label.astype(int))
        cv2.imwrite('vis.jpg', tmpimg)
        from IPython import embed; embed()

    img = img - cfg.pixel_means
    if cfg.pixel_norm:
        img = img / 255.
    img = img.transpose(2, 0, 1)
    imgs.append(img)
    if 'joints' in d:
        labels.append(label.reshape(-1))
        valids.append(valid.reshape(-1))

    if stage == 'train':
        imgs, labels, valids = data_augmentation(imgs, labels, valids)
        heatmaps15 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                        gaussian_kernel=cfg.gk15)
        heatmaps11 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                        gaussian_kernel=cfg.gk11)
        heatmaps9 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                       gaussian_kernel=cfg.gk9)
        heatmaps7 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                       gaussian_kernel=cfg.gk7)

        return [imgs.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps15.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps11.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps9.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps7.astype(np.float32).transpose(0, 2, 3, 1),
                valids.astype(np.float32)]
    else:
        return [np.asarray(imgs).astype(np.float32), details]","max(seg.shape[0], seg.shape[1])",max(*seg.shape[:2]),"iterable_zj[0], iterable_zj[1]",*seg.shape[:2],*seg.shape[:2],1
meta-dataset,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meta-dataset/meta_dataset/data/reader_test.py,https://github.com/google-research/meta-dataset/tree/master/meta_dataset/data/reader_test.py,EpisodeReaderTest,test_non_deterministic_shuffle$439,"def test_non_deterministic_shuffle(self):
    """"""Different Readers generate different episode compositions.

    Even with the same episode descriptions, the content should be different.
    """"""
    num_episodes = 10
    init_rng = sampling.RNG
    seed = 20181120
    episode_streams = []
    chunk_sizes = []
    try:
      for _ in range(2):
        sampling.RNG = np.random.RandomState(seed)
        sampler = sampling.EpisodeDescriptionSampler(
            self.dataset_spec,
            self.split,
            episode_descr_config=config.EpisodeDescriptionConfig())
        episodes = self.generate_episodes(sampler, num_episodes)
        episode_streams.append(episodes)
        chunk_size = sampler.compute_chunk_sizes()
        chunk_sizes.append(chunk_size)
        for examples, targets in episodes:
          self.check_episode_consistency(examples, targets, chunk_size)

    finally:
      # Restore the original RNG
      sampling.RNG = init_rng

    self.assertEqual(chunk_sizes[0], chunk_sizes[1])

    # It is unlikely that all episodes will be the same
    num_identical_episodes = 0
    for ((examples1, targets1), (examples2, targets2)) in zip(*episode_streams):
      self.check_episode_consistency(examples1, targets1, chunk_sizes[0])
      self.check_episode_consistency(examples2, targets2, chunk_sizes[1])
      self.assertAllEqual(targets1, targets2)
      if all(examples1 == examples2):
        num_identical_episodes += 1

    self.assertNotEqual(num_identical_episodes, num_episodes)","self.assertEqual(chunk_sizes[0], chunk_sizes[1])",self.assertEqual(*chunk_sizes[:2]),"iterable_zj[0], iterable_zj[1]",*chunk_sizes[:2],*chunk_sizes[:2],1
practical-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/practical-python/Solutions/5_8/report.py,https://github.com/dabeaz-course/practical-python/tree/master/Solutions/5_8/report.py,,main$64,"def main(args):
    if len(args) != 4:
        raise SystemExit('Usage: %s portfile pricefile format' % args[0])
    portfolio_report(args[1], args[2], args[3])","portfolio_report(args[1], args[2], args[3])",portfolio_report(*args[1:4]),"iterable_zj[1], iterable_zj[2], iterable_zj[3]",*args[1:4],*args[1:4],1
ASFF,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ASFF/utils/DCN/functions/deform_conv2d_func.py,https://github.com/GOATmessi7/ASFF/tree/master/utils/DCN/functions/deform_conv2d_func.py,DeformConv2dFunction,backward$42,"def backward(ctx, grad_output):
        input, offset, weight, bias = ctx.saved_tensors
        grad_input, grad_offset, grad_weight, grad_bias = \
            DCN.deform_conv2d_backward(input, weight,
                                     bias,
                                     offset,
                                     grad_output,
                                     ctx.kernel_size[0], ctx.kernel_size[1],
                                     ctx.stride[0], ctx.stride[1],
                                     ctx.padding[0], ctx.padding[1],
                                     ctx.dilation[0], ctx.dilation[1],
                                     ctx.group,
                                     ctx.deformable_groups,
                                     ctx.im2col_step)

        return grad_input, grad_offset, grad_weight, grad_bias,\
            None, None, None, None, None, None","DCN.deform_conv2d_backward(input, weight, bias, offset, grad_output, ctx.kernel_size[0], ctx.kernel_size[1], ctx.stride[0], ctx.stride[1], ctx.padding[0], ctx.padding[1], ctx.dilation[0], ctx.dilation[1], ctx.group, ctx.deformable_groups, ctx.im2col_step)","DCN.deform_conv2d_backward(input, weight, bias, offset, grad_output, ctx.kernel_size[0], ctx.kernel_size[1], ctx.stride[0], ctx.stride[1], ctx.padding[0], ctx.padding[1], *ctx.dilation[:2], ctx.group, ctx.deformable_groups, ctx.im2col_step)","iterable_zj[0], iterable_zj[1]",*ctx.dilation[:2],*ctx.dilation[:2],1
ASFF,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ASFF/utils/DCN/functions/deform_conv2d_func.py,https://github.com/GOATmessi7/ASFF/tree/master/utils/DCN/functions/deform_conv2d_func.py,DeformConv2dFunction,backward$42,"def backward(ctx, grad_output):
        input, offset, weight, bias = ctx.saved_tensors
        grad_input, grad_offset, grad_weight, grad_bias = \
            DCN.deform_conv2d_backward(input, weight,
                                     bias,
                                     offset,
                                     grad_output,
                                     ctx.kernel_size[0], ctx.kernel_size[1],
                                     ctx.stride[0], ctx.stride[1],
                                     ctx.padding[0], ctx.padding[1],
                                     ctx.dilation[0], ctx.dilation[1],
                                     ctx.group,
                                     ctx.deformable_groups,
                                     ctx.im2col_step)

        return grad_input, grad_offset, grad_weight, grad_bias,\
            None, None, None, None, None, None","DCN.deform_conv2d_backward(input, weight, bias, offset, grad_output, ctx.kernel_size[0], ctx.kernel_size[1], ctx.stride[0], ctx.stride[1], ctx.padding[0], ctx.padding[1], ctx.dilation[0], ctx.dilation[1], ctx.group, ctx.deformable_groups, ctx.im2col_step)","DCN.deform_conv2d_backward(input, weight, bias, offset, grad_output, *ctx.kernel_size[:2], ctx.stride[0], ctx.stride[1], ctx.padding[0], ctx.padding[1], ctx.dilation[0], ctx.dilation[1], ctx.group, ctx.deformable_groups, ctx.im2col_step)","iterable_zj[0], iterable_zj[1]",*ctx.kernel_size[:2],*ctx.kernel_size[:2],1
ASFF,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ASFF/utils/DCN/functions/deform_conv2d_func.py,https://github.com/GOATmessi7/ASFF/tree/master/utils/DCN/functions/deform_conv2d_func.py,DeformConv2dFunction,backward$42,"def backward(ctx, grad_output):
        input, offset, weight, bias = ctx.saved_tensors
        grad_input, grad_offset, grad_weight, grad_bias = \
            DCN.deform_conv2d_backward(input, weight,
                                     bias,
                                     offset,
                                     grad_output,
                                     ctx.kernel_size[0], ctx.kernel_size[1],
                                     ctx.stride[0], ctx.stride[1],
                                     ctx.padding[0], ctx.padding[1],
                                     ctx.dilation[0], ctx.dilation[1],
                                     ctx.group,
                                     ctx.deformable_groups,
                                     ctx.im2col_step)

        return grad_input, grad_offset, grad_weight, grad_bias,\
            None, None, None, None, None, None","DCN.deform_conv2d_backward(input, weight, bias, offset, grad_output, ctx.kernel_size[0], ctx.kernel_size[1], ctx.stride[0], ctx.stride[1], ctx.padding[0], ctx.padding[1], ctx.dilation[0], ctx.dilation[1], ctx.group, ctx.deformable_groups, ctx.im2col_step)","DCN.deform_conv2d_backward(input, weight, bias, offset, grad_output, ctx.kernel_size[0], ctx.kernel_size[1], ctx.stride[0], ctx.stride[1], *ctx.padding[:2], ctx.dilation[0], ctx.dilation[1], ctx.group, ctx.deformable_groups, ctx.im2col_step)","iterable_zj[0], iterable_zj[1]",*ctx.padding[:2],*ctx.padding[:2],1
ASFF,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ASFF/utils/DCN/functions/deform_conv2d_func.py,https://github.com/GOATmessi7/ASFF/tree/master/utils/DCN/functions/deform_conv2d_func.py,DeformConv2dFunction,backward$42,"def backward(ctx, grad_output):
        input, offset, weight, bias = ctx.saved_tensors
        grad_input, grad_offset, grad_weight, grad_bias = \
            DCN.deform_conv2d_backward(input, weight,
                                     bias,
                                     offset,
                                     grad_output,
                                     ctx.kernel_size[0], ctx.kernel_size[1],
                                     ctx.stride[0], ctx.stride[1],
                                     ctx.padding[0], ctx.padding[1],
                                     ctx.dilation[0], ctx.dilation[1],
                                     ctx.group,
                                     ctx.deformable_groups,
                                     ctx.im2col_step)

        return grad_input, grad_offset, grad_weight, grad_bias,\
            None, None, None, None, None, None","DCN.deform_conv2d_backward(input, weight, bias, offset, grad_output, ctx.kernel_size[0], ctx.kernel_size[1], ctx.stride[0], ctx.stride[1], ctx.padding[0], ctx.padding[1], ctx.dilation[0], ctx.dilation[1], ctx.group, ctx.deformable_groups, ctx.im2col_step)","DCN.deform_conv2d_backward(input, weight, bias, offset, grad_output, ctx.kernel_size[0], ctx.kernel_size[1], *ctx.stride[:2], ctx.padding[0], ctx.padding[1], ctx.dilation[0], ctx.dilation[1], ctx.group, ctx.deformable_groups, ctx.im2col_step)","iterable_zj[0], iterable_zj[1]",*ctx.stride[:2],*ctx.stride[:2],1
freedoom,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freedoom/graphics/text/tint.py,https://github.com/freedoom/freedoom/tree/master/graphics/text/tint.py,,if_main_my$55,"if __name__ == ""__main__"":
    import os
    import sys

    main(sys.argv[1], sys.argv[2], sys.argv[3])","main(sys.argv[1], sys.argv[2], sys.argv[3])",main(*sys.argv[1:4]),"iterable_zj[1], iterable_zj[2], iterable_zj[3]",*sys.argv[1:4],*sys.argv[1:4],1
OpenPCDet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenPCDet/pcdet/datasets/augmentor/augmentor_utils.py,https://github.com/open-mmlab/OpenPCDet/tree/master/pcdet/datasets/augmentor/augmentor_utils.py,,global_frustum_dropout_right$270,"def global_frustum_dropout_right(gt_boxes, points, intensity_range):
    """"""
    Args:
        gt_boxes: (N, 7), [x, y, z, dx, dy, dz, heading, [vx], [vy]],
        points: (M, 3 + C),
        intensity: [min, max]
    Returns:
    """"""
    intensity = np.random.uniform(intensity_range[0], intensity_range[1])
    
    threshold = np.min(points[:, 1]) + intensity * (np.max(points[:, 1]) - np.min(points[:, 1]))
    points = points[points[:, 1] > threshold]
    gt_boxes = gt_boxes[gt_boxes[:, 1] > threshold]
    
    return gt_boxes, points","np.random.uniform(intensity_range[0], intensity_range[1])",np.random.uniform(*intensity_range[:2]),"iterable_zj[0], iterable_zj[1]",*intensity_range[:2],*intensity_range[:2],1
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/baselines/models_pytorch/classifier_pytorch/transformers/modeling_xlnet.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master/baselines/models_pytorch/classifier_pytorch/transformers/modeling_xlnet.py,XLNetRelativeAttention,rel_shift$230,"def rel_shift(x, klen=-1):
        """"""perform relative shift to form the relative attention score.""""""
        x_size = x.shape

        x = x.reshape(x_size[1], x_size[0], x_size[2], x_size[3])
        x = x[1:, ...]
        x = x.reshape(x_size[0], x_size[1] - 1, x_size[2], x_size[3])
        # x = x[:, 0:klen, :, :]
        x = torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))

        return x","x.reshape(x_size[1], x_size[0], x_size[2], x_size[3])","x.reshape(*(x_size[0:2][::-1] or x_size[0:2][::-1]), x_size[2], x_size[3])","iterable_zj[1], iterable_zj[0]",*x_size[0:2][::-1] or x_size[0:2][::-1],*x_size[:4:2],0
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/baselines/models_pytorch/classifier_pytorch/transformers/modeling_xlnet.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master/baselines/models_pytorch/classifier_pytorch/transformers/modeling_xlnet.py,XLNetRelativeAttention,rel_shift$230,"def rel_shift(x, klen=-1):
        """"""perform relative shift to form the relative attention score.""""""
        x_size = x.shape

        x = x.reshape(x_size[1], x_size[0], x_size[2], x_size[3])
        x = x[1:, ...]
        x = x.reshape(x_size[0], x_size[1] - 1, x_size[2], x_size[3])
        # x = x[:, 0:klen, :, :]
        x = torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))

        return x","x.reshape(x_size[0], x_size[1] - 1, x_size[2], x_size[3])","x.reshape(*x_size[0:2] + [x_size[2]], x_size[3])","iterable_zj[0], iterable_zj[1] - 1, iterable_zj[2]",*x_size[0:2] + [x_size[2]],*x_size[2:4],0
packnet-sfm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/packnet-sfm/packnet_sfm/networks/pose/PoseNet.py,https://github.com/TRI-ML/packnet-sfm/tree/master/packnet_sfm/networks/pose/PoseNet.py,PoseNet,__init__$41,"def __init__(self, nb_ref_imgs=2, rotation_mode='euler', **kwargs):
        super().__init__()
        self.nb_ref_imgs = nb_ref_imgs
        self.rotation_mode = rotation_mode

        conv_channels = [16, 32, 64, 128, 256, 256, 256]
        self.conv1 = conv_gn(3 * (1 + self.nb_ref_imgs), conv_channels[0], kernel_size=7)
        self.conv2 = conv_gn(conv_channels[0], conv_channels[1], kernel_size=5)
        self.conv3 = conv_gn(conv_channels[1], conv_channels[2])
        self.conv4 = conv_gn(conv_channels[2], conv_channels[3])
        self.conv5 = conv_gn(conv_channels[3], conv_channels[4])
        self.conv6 = conv_gn(conv_channels[4], conv_channels[5])
        self.conv7 = conv_gn(conv_channels[5], conv_channels[6])

        self.pose_pred = nn.Conv2d(conv_channels[6], 6 * self.nb_ref_imgs,
                                   kernel_size=1, padding=0)

        self.init_weights()","conv_gn(conv_channels[0], conv_channels[1], kernel_size=5)","conv_gn(*conv_channels[:2], kernel_size=5)","iterable_zj[0], iterable_zj[1]",*conv_channels[:2],*conv_channels[:2],1
packnet-sfm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/packnet-sfm/packnet_sfm/networks/pose/PoseNet.py,https://github.com/TRI-ML/packnet-sfm/tree/master/packnet_sfm/networks/pose/PoseNet.py,PoseNet,__init__$41,"def __init__(self, nb_ref_imgs=2, rotation_mode='euler', **kwargs):
        super().__init__()
        self.nb_ref_imgs = nb_ref_imgs
        self.rotation_mode = rotation_mode

        conv_channels = [16, 32, 64, 128, 256, 256, 256]
        self.conv1 = conv_gn(3 * (1 + self.nb_ref_imgs), conv_channels[0], kernel_size=7)
        self.conv2 = conv_gn(conv_channels[0], conv_channels[1], kernel_size=5)
        self.conv3 = conv_gn(conv_channels[1], conv_channels[2])
        self.conv4 = conv_gn(conv_channels[2], conv_channels[3])
        self.conv5 = conv_gn(conv_channels[3], conv_channels[4])
        self.conv6 = conv_gn(conv_channels[4], conv_channels[5])
        self.conv7 = conv_gn(conv_channels[5], conv_channels[6])

        self.pose_pred = nn.Conv2d(conv_channels[6], 6 * self.nb_ref_imgs,
                                   kernel_size=1, padding=0)

        self.init_weights()","conv_gn(conv_channels[1], conv_channels[2])",conv_gn(*conv_channels[1:3]),"iterable_zj[1], iterable_zj[2]",*conv_channels[1:3],*conv_channels[1:3],1
packnet-sfm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/packnet-sfm/packnet_sfm/networks/pose/PoseNet.py,https://github.com/TRI-ML/packnet-sfm/tree/master/packnet_sfm/networks/pose/PoseNet.py,PoseNet,__init__$41,"def __init__(self, nb_ref_imgs=2, rotation_mode='euler', **kwargs):
        super().__init__()
        self.nb_ref_imgs = nb_ref_imgs
        self.rotation_mode = rotation_mode

        conv_channels = [16, 32, 64, 128, 256, 256, 256]
        self.conv1 = conv_gn(3 * (1 + self.nb_ref_imgs), conv_channels[0], kernel_size=7)
        self.conv2 = conv_gn(conv_channels[0], conv_channels[1], kernel_size=5)
        self.conv3 = conv_gn(conv_channels[1], conv_channels[2])
        self.conv4 = conv_gn(conv_channels[2], conv_channels[3])
        self.conv5 = conv_gn(conv_channels[3], conv_channels[4])
        self.conv6 = conv_gn(conv_channels[4], conv_channels[5])
        self.conv7 = conv_gn(conv_channels[5], conv_channels[6])

        self.pose_pred = nn.Conv2d(conv_channels[6], 6 * self.nb_ref_imgs,
                                   kernel_size=1, padding=0)

        self.init_weights()","conv_gn(conv_channels[2], conv_channels[3])",Cannot refactor,"iterable_zj[2], iterable_zj[3]",,*conv_channels[2:4],0
packnet-sfm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/packnet-sfm/packnet_sfm/networks/pose/PoseNet.py,https://github.com/TRI-ML/packnet-sfm/tree/master/packnet_sfm/networks/pose/PoseNet.py,PoseNet,__init__$41,"def __init__(self, nb_ref_imgs=2, rotation_mode='euler', **kwargs):
        super().__init__()
        self.nb_ref_imgs = nb_ref_imgs
        self.rotation_mode = rotation_mode

        conv_channels = [16, 32, 64, 128, 256, 256, 256]
        self.conv1 = conv_gn(3 * (1 + self.nb_ref_imgs), conv_channels[0], kernel_size=7)
        self.conv2 = conv_gn(conv_channels[0], conv_channels[1], kernel_size=5)
        self.conv3 = conv_gn(conv_channels[1], conv_channels[2])
        self.conv4 = conv_gn(conv_channels[2], conv_channels[3])
        self.conv5 = conv_gn(conv_channels[3], conv_channels[4])
        self.conv6 = conv_gn(conv_channels[4], conv_channels[5])
        self.conv7 = conv_gn(conv_channels[5], conv_channels[6])

        self.pose_pred = nn.Conv2d(conv_channels[6], 6 * self.nb_ref_imgs,
                                   kernel_size=1, padding=0)

        self.init_weights()","conv_gn(conv_channels[3], conv_channels[4])",conv_gn(*conv_channels[3:5]),"iterable_zj[3], iterable_zj[4]",*conv_channels[3:5],*conv_channels[3:5],1
packnet-sfm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/packnet-sfm/packnet_sfm/networks/pose/PoseNet.py,https://github.com/TRI-ML/packnet-sfm/tree/master/packnet_sfm/networks/pose/PoseNet.py,PoseNet,__init__$41,"def __init__(self, nb_ref_imgs=2, rotation_mode='euler', **kwargs):
        super().__init__()
        self.nb_ref_imgs = nb_ref_imgs
        self.rotation_mode = rotation_mode

        conv_channels = [16, 32, 64, 128, 256, 256, 256]
        self.conv1 = conv_gn(3 * (1 + self.nb_ref_imgs), conv_channels[0], kernel_size=7)
        self.conv2 = conv_gn(conv_channels[0], conv_channels[1], kernel_size=5)
        self.conv3 = conv_gn(conv_channels[1], conv_channels[2])
        self.conv4 = conv_gn(conv_channels[2], conv_channels[3])
        self.conv5 = conv_gn(conv_channels[3], conv_channels[4])
        self.conv6 = conv_gn(conv_channels[4], conv_channels[5])
        self.conv7 = conv_gn(conv_channels[5], conv_channels[6])

        self.pose_pred = nn.Conv2d(conv_channels[6], 6 * self.nb_ref_imgs,
                                   kernel_size=1, padding=0)

        self.init_weights()","conv_gn(conv_channels[4], conv_channels[5])",conv_gn(*conv_channels[4:6]),"iterable_zj[4], iterable_zj[5]",*conv_channels[4:6],*conv_channels[4:6],1
packnet-sfm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/packnet-sfm/packnet_sfm/networks/pose/PoseNet.py,https://github.com/TRI-ML/packnet-sfm/tree/master/packnet_sfm/networks/pose/PoseNet.py,PoseNet,__init__$41,"def __init__(self, nb_ref_imgs=2, rotation_mode='euler', **kwargs):
        super().__init__()
        self.nb_ref_imgs = nb_ref_imgs
        self.rotation_mode = rotation_mode

        conv_channels = [16, 32, 64, 128, 256, 256, 256]
        self.conv1 = conv_gn(3 * (1 + self.nb_ref_imgs), conv_channels[0], kernel_size=7)
        self.conv2 = conv_gn(conv_channels[0], conv_channels[1], kernel_size=5)
        self.conv3 = conv_gn(conv_channels[1], conv_channels[2])
        self.conv4 = conv_gn(conv_channels[2], conv_channels[3])
        self.conv5 = conv_gn(conv_channels[3], conv_channels[4])
        self.conv6 = conv_gn(conv_channels[4], conv_channels[5])
        self.conv7 = conv_gn(conv_channels[5], conv_channels[6])

        self.pose_pred = nn.Conv2d(conv_channels[6], 6 * self.nb_ref_imgs,
                                   kernel_size=1, padding=0)

        self.init_weights()","conv_gn(conv_channels[5], conv_channels[6])",Cannot refactor,"iterable_zj[5], iterable_zj[6]",,*conv_channels[5:7],0
SMARTS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/ultra/ultra/scenarios/common/visualization.py,https://github.com/huawei-noah/SMARTS/tree/master/ultra/ultra/scenarios/common/visualization.py,,profile_vehicles$62,"def profile_vehicles(vehicle_states, save_dir):
    def plot(ax, x_label, y_label, state):
        ax.set_xlabel(x_label, fontsize=10)
        ax.set_ylabel(y_label, fontsize=10)
        ax.plot(range(len(state[y_label])), state[y_label])
        if ""in_junction"" in state:
            ax.axvspan(
                state[""in_junction""][0], state[""in_junction""][1], color=""red"", alpha=0.5
            )

    for v_id, state in vehicle_states.items():
        if not state[""teleported""]:
            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))
            plot(ax[0], ""time"", ""speed"", state)
            plot(ax[1], ""time"", ""accel"", state)
            behavior_key, behavior_color = get_social_vehicle_color(state[""behavior""])
            temp_save_dir = f""{save_dir}/{behavior_key}""
            if not os.path.exists(temp_save_dir):
                os.makedirs(temp_save_dir)
            plt.savefig(f""{temp_save_dir}/{state['route']}_{v_id}.png"")

    plt.close(""all"")","ax.axvspan(state['in_junction'][0], state['in_junction'][1], color='red', alpha=0.5)","ax.axvspan(*state['in_junction'][:2], color='red', alpha=0.5)","iterable_zj[0], iterable_zj[1]",*state['in_junction'][:2],*state['in_junction'][:2],1
SSL4MIS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SSL4MIS/code/networks/attention_unet.py,https://github.com/HiLab-git/SSL4MIS/tree/master/code/networks/attention_unet.py,Attention_UNet,__init__$11,"def __init__(self, feature_scale=4, n_classes=21, is_deconv=True, in_channels=3,
                 nonlocal_mode='concatenation', attention_dsample=(2,2,2), is_batchnorm=True):
        super(Attention_UNet, self).__init__()
        self.is_deconv = is_deconv
        self.in_channels = in_channels
        self.is_batchnorm = is_batchnorm
        self.feature_scale = feature_scale

        filters = [64, 128, 256, 512, 1024]
        filters = [int(x / self.feature_scale) for x in filters]

        # downsampling
        self.conv1 = UnetConv3(self.in_channels, filters[0], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool1 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv2 = UnetConv3(filters[0], filters[1], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv3 = UnetConv3(filters[1], filters[2], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool3 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv4 = UnetConv3(filters[2], filters[3], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool4 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.center = UnetConv3(filters[3], filters[4], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.gating = UnetGridGatingSignal3(filters[4], filters[4], kernel_size=(1, 1, 1), is_batchnorm=self.is_batchnorm)

        # attention blocks
        self.attentionblock2 = MultiAttentionBlock(in_size=filters[1], gate_size=filters[2], inter_size=filters[1],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)
        self.attentionblock3 = MultiAttentionBlock(in_size=filters[2], gate_size=filters[3], inter_size=filters[2],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)
        self.attentionblock4 = MultiAttentionBlock(in_size=filters[3], gate_size=filters[4], inter_size=filters[3],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)

        # upsampling
        self.up_concat4 = UnetUp3_CT(filters[4], filters[3], is_batchnorm)
        self.up_concat3 = UnetUp3_CT(filters[3], filters[2], is_batchnorm)
        self.up_concat2 = UnetUp3_CT(filters[2], filters[1], is_batchnorm)
        self.up_concat1 = UnetUp3_CT(filters[1], filters[0], is_batchnorm)

        # deep supervision
        self.dsv4 = UnetDsv3(in_size=filters[3], out_size=n_classes, scale_factor=8)
        self.dsv3 = UnetDsv3(in_size=filters[2], out_size=n_classes, scale_factor=4)
        self.dsv2 = UnetDsv3(in_size=filters[1], out_size=n_classes, scale_factor=2)
        self.dsv1 = nn.Conv3d(in_channels=filters[0], out_channels=n_classes, kernel_size=1)

        # final conv (without any concat)
        self.final = nn.Conv3d(n_classes*4, n_classes, 1)

        # initialise weights
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                init_weights(m, init_type='kaiming')
            elif isinstance(m, nn.BatchNorm3d):
                init_weights(m, init_type='kaiming')","UnetConv3(filters[0], filters[1], self.is_batchnorm, kernel_size=(3, 3, 3), padding_size=(1, 1, 1))","UnetConv3(*filters[:2], self.is_batchnorm, kernel_size=(3, 3, 3), padding_size=(1, 1, 1))","iterable_zj[0], iterable_zj[1]",*filters[:2],*filters[:2],1
SSL4MIS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SSL4MIS/code/networks/attention_unet.py,https://github.com/HiLab-git/SSL4MIS/tree/master/code/networks/attention_unet.py,Attention_UNet,__init__$11,"def __init__(self, feature_scale=4, n_classes=21, is_deconv=True, in_channels=3,
                 nonlocal_mode='concatenation', attention_dsample=(2,2,2), is_batchnorm=True):
        super(Attention_UNet, self).__init__()
        self.is_deconv = is_deconv
        self.in_channels = in_channels
        self.is_batchnorm = is_batchnorm
        self.feature_scale = feature_scale

        filters = [64, 128, 256, 512, 1024]
        filters = [int(x / self.feature_scale) for x in filters]

        # downsampling
        self.conv1 = UnetConv3(self.in_channels, filters[0], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool1 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv2 = UnetConv3(filters[0], filters[1], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv3 = UnetConv3(filters[1], filters[2], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool3 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv4 = UnetConv3(filters[2], filters[3], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool4 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.center = UnetConv3(filters[3], filters[4], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.gating = UnetGridGatingSignal3(filters[4], filters[4], kernel_size=(1, 1, 1), is_batchnorm=self.is_batchnorm)

        # attention blocks
        self.attentionblock2 = MultiAttentionBlock(in_size=filters[1], gate_size=filters[2], inter_size=filters[1],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)
        self.attentionblock3 = MultiAttentionBlock(in_size=filters[2], gate_size=filters[3], inter_size=filters[2],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)
        self.attentionblock4 = MultiAttentionBlock(in_size=filters[3], gate_size=filters[4], inter_size=filters[3],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)

        # upsampling
        self.up_concat4 = UnetUp3_CT(filters[4], filters[3], is_batchnorm)
        self.up_concat3 = UnetUp3_CT(filters[3], filters[2], is_batchnorm)
        self.up_concat2 = UnetUp3_CT(filters[2], filters[1], is_batchnorm)
        self.up_concat1 = UnetUp3_CT(filters[1], filters[0], is_batchnorm)

        # deep supervision
        self.dsv4 = UnetDsv3(in_size=filters[3], out_size=n_classes, scale_factor=8)
        self.dsv3 = UnetDsv3(in_size=filters[2], out_size=n_classes, scale_factor=4)
        self.dsv2 = UnetDsv3(in_size=filters[1], out_size=n_classes, scale_factor=2)
        self.dsv1 = nn.Conv3d(in_channels=filters[0], out_channels=n_classes, kernel_size=1)

        # final conv (without any concat)
        self.final = nn.Conv3d(n_classes*4, n_classes, 1)

        # initialise weights
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                init_weights(m, init_type='kaiming')
            elif isinstance(m, nn.BatchNorm3d):
                init_weights(m, init_type='kaiming')","UnetConv3(filters[1], filters[2], self.is_batchnorm, kernel_size=(3, 3, 3), padding_size=(1, 1, 1))","UnetConv3(*filters[1:3], self.is_batchnorm, kernel_size=(3, 3, 3), padding_size=(1, 1, 1))","iterable_zj[1], iterable_zj[2]",*filters[1:3],*filters[1:3],1
SSL4MIS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SSL4MIS/code/networks/attention_unet.py,https://github.com/HiLab-git/SSL4MIS/tree/master/code/networks/attention_unet.py,Attention_UNet,__init__$11,"def __init__(self, feature_scale=4, n_classes=21, is_deconv=True, in_channels=3,
                 nonlocal_mode='concatenation', attention_dsample=(2,2,2), is_batchnorm=True):
        super(Attention_UNet, self).__init__()
        self.is_deconv = is_deconv
        self.in_channels = in_channels
        self.is_batchnorm = is_batchnorm
        self.feature_scale = feature_scale

        filters = [64, 128, 256, 512, 1024]
        filters = [int(x / self.feature_scale) for x in filters]

        # downsampling
        self.conv1 = UnetConv3(self.in_channels, filters[0], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool1 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv2 = UnetConv3(filters[0], filters[1], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv3 = UnetConv3(filters[1], filters[2], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool3 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv4 = UnetConv3(filters[2], filters[3], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool4 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.center = UnetConv3(filters[3], filters[4], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.gating = UnetGridGatingSignal3(filters[4], filters[4], kernel_size=(1, 1, 1), is_batchnorm=self.is_batchnorm)

        # attention blocks
        self.attentionblock2 = MultiAttentionBlock(in_size=filters[1], gate_size=filters[2], inter_size=filters[1],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)
        self.attentionblock3 = MultiAttentionBlock(in_size=filters[2], gate_size=filters[3], inter_size=filters[2],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)
        self.attentionblock4 = MultiAttentionBlock(in_size=filters[3], gate_size=filters[4], inter_size=filters[3],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)

        # upsampling
        self.up_concat4 = UnetUp3_CT(filters[4], filters[3], is_batchnorm)
        self.up_concat3 = UnetUp3_CT(filters[3], filters[2], is_batchnorm)
        self.up_concat2 = UnetUp3_CT(filters[2], filters[1], is_batchnorm)
        self.up_concat1 = UnetUp3_CT(filters[1], filters[0], is_batchnorm)

        # deep supervision
        self.dsv4 = UnetDsv3(in_size=filters[3], out_size=n_classes, scale_factor=8)
        self.dsv3 = UnetDsv3(in_size=filters[2], out_size=n_classes, scale_factor=4)
        self.dsv2 = UnetDsv3(in_size=filters[1], out_size=n_classes, scale_factor=2)
        self.dsv1 = nn.Conv3d(in_channels=filters[0], out_channels=n_classes, kernel_size=1)

        # final conv (without any concat)
        self.final = nn.Conv3d(n_classes*4, n_classes, 1)

        # initialise weights
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                init_weights(m, init_type='kaiming')
            elif isinstance(m, nn.BatchNorm3d):
                init_weights(m, init_type='kaiming')","UnetConv3(filters[2], filters[3], self.is_batchnorm, kernel_size=(3, 3, 3), padding_size=(1, 1, 1))","UnetConv3(*filters[2:4], self.is_batchnorm, kernel_size=(3, 3, 3), padding_size=(1, 1, 1))","iterable_zj[2], iterable_zj[3]",*filters[2:4],*filters[2:4],1
SSL4MIS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SSL4MIS/code/networks/attention_unet.py,https://github.com/HiLab-git/SSL4MIS/tree/master/code/networks/attention_unet.py,Attention_UNet,__init__$11,"def __init__(self, feature_scale=4, n_classes=21, is_deconv=True, in_channels=3,
                 nonlocal_mode='concatenation', attention_dsample=(2,2,2), is_batchnorm=True):
        super(Attention_UNet, self).__init__()
        self.is_deconv = is_deconv
        self.in_channels = in_channels
        self.is_batchnorm = is_batchnorm
        self.feature_scale = feature_scale

        filters = [64, 128, 256, 512, 1024]
        filters = [int(x / self.feature_scale) for x in filters]

        # downsampling
        self.conv1 = UnetConv3(self.in_channels, filters[0], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool1 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv2 = UnetConv3(filters[0], filters[1], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv3 = UnetConv3(filters[1], filters[2], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool3 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.conv4 = UnetConv3(filters[2], filters[3], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.maxpool4 = nn.MaxPool3d(kernel_size=(2, 2, 2))

        self.center = UnetConv3(filters[3], filters[4], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))
        self.gating = UnetGridGatingSignal3(filters[4], filters[4], kernel_size=(1, 1, 1), is_batchnorm=self.is_batchnorm)

        # attention blocks
        self.attentionblock2 = MultiAttentionBlock(in_size=filters[1], gate_size=filters[2], inter_size=filters[1],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)
        self.attentionblock3 = MultiAttentionBlock(in_size=filters[2], gate_size=filters[3], inter_size=filters[2],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)
        self.attentionblock4 = MultiAttentionBlock(in_size=filters[3], gate_size=filters[4], inter_size=filters[3],
                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)

        # upsampling
        self.up_concat4 = UnetUp3_CT(filters[4], filters[3], is_batchnorm)
        self.up_concat3 = UnetUp3_CT(filters[3], filters[2], is_batchnorm)
        self.up_concat2 = UnetUp3_CT(filters[2], filters[1], is_batchnorm)
        self.up_concat1 = UnetUp3_CT(filters[1], filters[0], is_batchnorm)

        # deep supervision
        self.dsv4 = UnetDsv3(in_size=filters[3], out_size=n_classes, scale_factor=8)
        self.dsv3 = UnetDsv3(in_size=filters[2], out_size=n_classes, scale_factor=4)
        self.dsv2 = UnetDsv3(in_size=filters[1], out_size=n_classes, scale_factor=2)
        self.dsv1 = nn.Conv3d(in_channels=filters[0], out_channels=n_classes, kernel_size=1)

        # final conv (without any concat)
        self.final = nn.Conv3d(n_classes*4, n_classes, 1)

        # initialise weights
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                init_weights(m, init_type='kaiming')
            elif isinstance(m, nn.BatchNorm3d):
                init_weights(m, init_type='kaiming')","UnetConv3(filters[3], filters[4], self.is_batchnorm, kernel_size=(3, 3, 3), padding_size=(1, 1, 1))","UnetConv3(*filters[3:5], self.is_batchnorm, kernel_size=(3, 3, 3), padding_size=(1, 1, 1))","iterable_zj[3], iterable_zj[4]",*filters[3:5],*filters[3:5],1
kazoo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kazoo/kazoo/handlers/utils.py,https://github.com/python-zk/kazoo/tree/master/kazoo/handlers/utils.py,,create_tcp_connection$191,"def create_tcp_connection(module, address, timeout=None,
                          use_ssl=False, ca=None, certfile=None,
                          keyfile=None, keyfile_password=None,
                          verify_certs=True, options=None, ciphers=None):
    end = None
    if timeout is None:
        # thanks to create_connection() developers for
        # this ugliness...
        timeout = module.getdefaulttimeout()
    if timeout is not None:
        end = time.time() + timeout
    sock = None

    while True:
        timeout_at = end if end is None else end - time.time()
        # The condition is not '< 0' here because socket.settimeout treats 0 as
        # a special case to put the socket in non-blocking mode.
        if timeout_at is not None and timeout_at <= 0:
            break

        if use_ssl:
            # Disallow use of SSLv2 and V3 (meaning we require TLSv1.0+)
            context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)

            if options is not None:
                context.options = options
            else:
                context.options |= ssl.OP_NO_SSLv2
                context.options |= ssl.OP_NO_SSLv3

            if ciphers:
                context.set_ciphers(ciphers)

            # Load default CA certs
            context.load_default_certs(ssl.Purpose.SERVER_AUTH)
            context.verify_mode = (
                ssl.CERT_OPTIONAL if verify_certs else ssl.CERT_NONE
            )
            if ca:
                context.load_verify_locations(ca)
            if certfile and keyfile:
                context.verify_mode = (
                    ssl.CERT_REQUIRED if verify_certs else ssl.CERT_NONE
                )
                context.load_cert_chain(certfile=certfile,
                                        keyfile=keyfile,
                                        password=keyfile_password)
            try:
                # Query the address to get back it's address family
                addrs = socket.getaddrinfo(address[0], address[1], 0,
                                           socket.SOCK_STREAM)
                conn = context.wrap_socket(module.socket(addrs[0][0]))
                conn.settimeout(timeout_at)
                conn.connect(address)
                sock = conn
                break
            except ssl.SSLError:
                raise
        else:
            try:
                # if we got a timeout, lets ensure that we decrement the time
                # otherwise there is no timeout set and we'll call it as such
                sock = module.create_connection(address, timeout_at)
                break
            except Exception as ex:
                errnum = ex.errno if isinstance(ex, OSError) else ex[0]
                if errnum == errno.EINTR:
                    continue
                raise

    if sock is None:
        raise module.error

    _set_default_tcpsock_options(module, sock)
    return sock","socket.getaddrinfo(address[0], address[1], 0, socket.SOCK_STREAM)","socket.getaddrinfo(*address[:2], 0, socket.SOCK_STREAM)","iterable_zj[0], iterable_zj[1]",*address[:2],*address[:2],1
keras-rl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras-rl/rl/policy.py,https://github.com/keras-rl/keras-rl/tree/master/rl/policy.py,MaxBoltzmannQPolicy,select_action$257,"def select_action(self, q_values):
        """"""Return the selected action
        The selected action follows the BoltzmannQPolicy with probability epsilon
        or return the Greedy Policy with probability (1 - epsilon)

        # Arguments
            q_values (np.ndarray): List of the estimations of Q for each action

        # Returns
            Selection action
        """"""
        assert q_values.ndim == 1
        q_values = q_values.astype('float64')
        nb_actions = q_values.shape[0]

        if np.random.uniform() < self.eps:
            exp_values = np.exp(np.clip(q_values / self.tau, self.clip[0], self.clip[1]))
            probs = exp_values / np.sum(exp_values)
            action = np.random.choice(range(nb_actions), p=probs)
        else:
            action = np.argmax(q_values)
        return action","np.clip(q_values / self.tau, self.clip[0], self.clip[1])","np.clip(q_values / self.tau, *self.clip[:2])","iterable_zj[0], iterable_zj[1]",*self.clip[:2],*self.clip[:2],1
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/optimizer/adamax.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/optimizer/adamax.py,Adamax,_append_optimize_op$188,"def _append_optimize_op(self, block, param_and_grad):
        assert isinstance(block, framework.Block)
        if isinstance(param_and_grad, dict):
            param_and_grad = self._update_param_group(param_and_grad)

        moment = self._get_accumulator(self._moment_acc_str, param_and_grad[0])
        inf_norm = self._get_accumulator(
            self._inf_norm_acc_str, param_and_grad[0]
        )
        beta1_pow_acc = self._get_accumulator(
            self._beta1_pow_acc_str, param_and_grad[0]
        )

        if framework.in_dygraph_mode():
            _C_ops.adamax_(
                param_and_grad[0],
                param_and_grad[1],
                self._create_param_lr(param_and_grad),
                moment,
                inf_norm,
                beta1_pow_acc,
                self._beta1,
                self._beta2,
                self._epsilon,
            )
        elif framework._in_legacy_dygraph():
            _legacy_C_ops.adamax(
                param_and_grad[0],
                param_and_grad[1],
                self._create_param_lr(param_and_grad),
                moment,
                inf_norm,
                beta1_pow_acc,
                param_and_grad[0],
                moment,
                inf_norm,
                ""beta1"",
                self._beta1,
                ""beta2"",
                self._beta2,
                ""epsilon"",
                self._epsilon,
            )
        else:
            # create the adamax optimize op
            adamax_op = block.append_op(
                type=self.type,
                inputs={
                    ""Param"": param_and_grad[0],
                    ""Grad"": param_and_grad[1],
                    ""LearningRate"": self._create_param_lr(param_and_grad),
                    ""Moment"": moment,
                    ""InfNorm"": inf_norm,
                    ""Beta1Pow"": beta1_pow_acc,
                },
                outputs={
                    ""ParamOut"": param_and_grad[0],
                    ""MomentOut"": moment,
                    ""InfNormOut"": inf_norm,
                },
                attrs={
                    ""beta1"": self._beta1,
                    ""beta2"": self._beta2,
                    ""epsilon"": self._epsilon,
                },
                stop_gradient=True,
            )

            return adamax_op","_C_ops.adamax_(param_and_grad[0], param_and_grad[1], self._create_param_lr(param_and_grad), moment, inf_norm, beta1_pow_acc, self._beta1, self._beta2, self._epsilon)","_C_ops.adamax_(*param_and_grad[:2], self._create_param_lr(param_and_grad), moment, inf_norm, beta1_pow_acc, self._beta1, self._beta2, self._epsilon)","iterable_zj[0], iterable_zj[1]",*param_and_grad[:2],*param_and_grad[:2],1
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/optimizer/adamax.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/optimizer/adamax.py,Adamax,_append_optimize_op$188,"def _append_optimize_op(self, block, param_and_grad):
        assert isinstance(block, framework.Block)
        if isinstance(param_and_grad, dict):
            param_and_grad = self._update_param_group(param_and_grad)

        moment = self._get_accumulator(self._moment_acc_str, param_and_grad[0])
        inf_norm = self._get_accumulator(
            self._inf_norm_acc_str, param_and_grad[0]
        )
        beta1_pow_acc = self._get_accumulator(
            self._beta1_pow_acc_str, param_and_grad[0]
        )

        if framework.in_dygraph_mode():
            _C_ops.adamax_(
                param_and_grad[0],
                param_and_grad[1],
                self._create_param_lr(param_and_grad),
                moment,
                inf_norm,
                beta1_pow_acc,
                self._beta1,
                self._beta2,
                self._epsilon,
            )
        elif framework._in_legacy_dygraph():
            _legacy_C_ops.adamax(
                param_and_grad[0],
                param_and_grad[1],
                self._create_param_lr(param_and_grad),
                moment,
                inf_norm,
                beta1_pow_acc,
                param_and_grad[0],
                moment,
                inf_norm,
                ""beta1"",
                self._beta1,
                ""beta2"",
                self._beta2,
                ""epsilon"",
                self._epsilon,
            )
        else:
            # create the adamax optimize op
            adamax_op = block.append_op(
                type=self.type,
                inputs={
                    ""Param"": param_and_grad[0],
                    ""Grad"": param_and_grad[1],
                    ""LearningRate"": self._create_param_lr(param_and_grad),
                    ""Moment"": moment,
                    ""InfNorm"": inf_norm,
                    ""Beta1Pow"": beta1_pow_acc,
                },
                outputs={
                    ""ParamOut"": param_and_grad[0],
                    ""MomentOut"": moment,
                    ""InfNormOut"": inf_norm,
                },
                attrs={
                    ""beta1"": self._beta1,
                    ""beta2"": self._beta2,
                    ""epsilon"": self._epsilon,
                },
                stop_gradient=True,
            )

            return adamax_op","_legacy_C_ops.adamax(param_and_grad[0], param_and_grad[1], self._create_param_lr(param_and_grad), moment, inf_norm, beta1_pow_acc, param_and_grad[0], moment, inf_norm, 'beta1', self._beta1, 'beta2', self._beta2, 'epsilon', self._epsilon)","_legacy_C_ops.adamax(*param_and_grad[:2], self._create_param_lr(param_and_grad), moment, inf_norm, beta1_pow_acc, param_and_grad[0], moment, inf_norm, 'beta1', self._beta1, 'beta2', self._beta2, 'epsilon', self._epsilon)","iterable_zj[0], iterable_zj[1]",*param_and_grad[:2],*param_and_grad[:2],1
d2l-vn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/d2l-vn/d2l/mxnet.py,https://github.com/mlbvn/d2l-vn/tree/master/d2l/mxnet.py,,transpose_qkv$1080,"def transpose_qkv(X, num_heads):
    # Input `X` shape: (`batch_size`, `seq_len`, `num_hiddens`).
    # Output `X` shape:
    # (`batch_size`, `seq_len`, `num_heads`, `num_hiddens` / `num_heads`)
    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

    # `X` shape:
    # (`batch_size`, `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
    X = X.transpose(0, 2, 1, 3)

    # `output` shape:
    # (`batch_size` * `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
    output = X.reshape(-1, X.shape[2], X.shape[3])
    return output","X.reshape(X.shape[0], X.shape[1], num_heads, -1)","X.reshape(*X.shape[:2], num_heads, -1)","iterable_zj[0], iterable_zj[1]",*X.shape[:2],*X.shape[:2],1
d2l-vn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/d2l-vn/d2l/mxnet.py,https://github.com/mlbvn/d2l-vn/tree/master/d2l/mxnet.py,,transpose_qkv$1080,"def transpose_qkv(X, num_heads):
    # Input `X` shape: (`batch_size`, `seq_len`, `num_hiddens`).
    # Output `X` shape:
    # (`batch_size`, `seq_len`, `num_heads`, `num_hiddens` / `num_heads`)
    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

    # `X` shape:
    # (`batch_size`, `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
    X = X.transpose(0, 2, 1, 3)

    # `output` shape:
    # (`batch_size` * `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
    output = X.reshape(-1, X.shape[2], X.shape[3])
    return output","X.reshape(-1, X.shape[2], X.shape[3])","X.reshape(-1, *X.shape[2:4])","iterable_zj[2], iterable_zj[3]",*X.shape[2:4],*X.shape[2:4],1
Sprytile,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Sprytile/sprytile_tools/tool_fill.py,https://github.com/Sprytile/Sprytile/tree/master/sprytile_tools/tool_fill.py,ToolFill,execute_fill$48,"def execute_fill(self, context, scene, ray_origin, ray_vector):
        up_vector, right_vector, plane_normal = sprytile_utils.get_current_grid_vectors(scene, with_rotation=False)

        # Intersect on the virtual plane
        plane_hit = intersect_line_plane(ray_origin, ray_origin + ray_vector, scene.cursor.location, plane_normal)
        # Didn't hit the plane exit
        if plane_hit is None:
            return
        grid = sprytile_utils.get_grid(context, context.object.sprytile_gridid)
        sprytile_data = scene.sprytile_data

        world_pixels = sprytile_data.world_pixels
        grid_x = grid.grid[0]
        grid_y = grid.grid[1]

        # Find the position of the plane hit, in terms of grid coordinates
        hit_coord, grid_right, grid_up = sprytile_utils.get_grid_pos(
            plane_hit, scene.cursor.location,
            right_vector.copy(), up_vector.copy(),
            world_pixels, grid_x, grid_y, as_coord=True
        )

        # Check hit_coord is inside the work plane grid
        plane_size = sprytile_data.fill_plane_size

        grid_min, grid_max = sprytile_utils.get_workplane_area(plane_size[0], plane_size[1])

        x_offset = 1
        if plane_size[0] % 2 == 1:
            grid_min[0] += x_offset
            grid_max[0] += x_offset

        if hit_coord.x < grid_min[0] or hit_coord.x >= grid_max[0]:
            return
        if hit_coord.y < grid_min[1] or hit_coord.y >= grid_max[1]:
            return

        # Build the fill map
        sel_coords, sel_size, sel_ids = sprytile_utils.get_grid_selection_ids(context, grid)
        fill_map, face_idx_array = self.build_fill_map(context, grid_up, grid_right, plane_normal,
                                                       plane_size, grid_min, grid_max, sel_ids)

        # Convert from grid coordinate to map coordinate
        hit_array_coord = [int(hit_coord.x) - grid_min[0],
                           int(hit_coord.y) - grid_min[1]]

        # For getting paint settings later
        paint_setting_layer = self.modal.bmesh.faces.layers.int.get(UvDataLayers.PAINT_SETTINGS)

        # Get vectors again, to apply tile rotations in UV stage
        up_vector, right_vector, plane_normal = sprytile_utils.get_current_grid_vectors(scene)

        # Get the content in hit coordinate
        hit_coord_content = int(fill_map[hit_array_coord[1]][hit_array_coord[0]])
        # Get the coordinates that would be flood filled
        fill_coords = self.flood_fill(fill_map, hit_array_coord, -2, hit_coord_content)

        # If lock transform on, cache the paint settings before doing any operations
        paint_setting_cache = None
        if sprytile_data.fill_lock_transform and paint_setting_layer is not None:
            paint_setting_cache = [None]*len(fill_coords)
            for idx, cell_coord in enumerate(fill_coords):
                face_index = face_idx_array[cell_coord[1]][cell_coord[0]]
                if face_index > -1:
                    face = self.modal.bmesh.faces[face_index]
                    paint_setting_cache[idx] = face[paint_setting_layer]

        # Get the work layer filter, based on layer settings
        work_layer_mask = sprytile_utils.get_work_layer_data(sprytile_data)
        require_base_layer = sprytile_data.work_layer != 'BASE'

        origin_xy = (grid.tile_selection[0], grid.tile_selection[1])
        data = scene.sprytile_data
        # Loop through list of coords to be filled
        for idx, cell_coord in enumerate(fill_coords):
            # Fetch the paint settings from cache
            if paint_setting_cache is not None:
                paint_setting = paint_setting_cache[idx]
                if paint_setting is not None:
                    sprytile_utils.from_paint_settings(data, paint_setting)

            # Convert map coord to grid coord
            grid_coord = [grid_min[0] + cell_coord[0],
                          grid_min[1] + cell_coord[1]]

            sub_x = (grid_coord[0] - int(hit_coord.x)) % sel_size[0]
            sub_y = (grid_coord[1] - int(hit_coord.y)) % sel_size[1]
            sub_xy = sel_coords[(sub_y * sel_size[0]) + sub_x]
            self.modal.construct_face(context, grid_coord, [1,1],
                                      sub_xy, origin_xy,
                                      grid_up, grid_right,
                                      up_vector, right_vector,
                                      plane_normal,
                                      require_base_layer=require_base_layer,
                                      work_layer_mask=work_layer_mask)","sprytile_utils.get_workplane_area(plane_size[0], plane_size[1])",sprytile_utils.get_workplane_area(*plane_size[:2]),"iterable_zj[0], iterable_zj[1]",*plane_size[:2],*plane_size[:2],1
nut,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nut/Server/Controller/Api.py,https://github.com/blawar/nut/tree/master/Server/Controller/Api.py,,getImportRegions$717,"def getImportRegions(request, response):
	nut.importRegion(request.bits[2], request.bits[3])

	return success(request, response, ""Fin"")","nut.importRegion(request.bits[2], request.bits[3])",nut.importRegion(*request.bits[2:4]),"iterable_zj[2], iterable_zj[3]",*request.bits[2:4],*request.bits[2:4],1
Mario-Level-1,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mario-Level-1/data/states/main_menu.py,https://github.com/justinmeister/Mario-Level-1/tree/master/data/states/main_menu.py,Menu,update$91,"def update(self, surface, keys, current_time):
        """"""Updates the state every refresh""""""
        self.current_time = current_time
        self.game_info[c.CURRENT_TIME] = self.current_time
        self.update_cursor(keys)
        self.overhead_info.update(self.game_info)

        surface.blit(self.background, self.viewport, self.viewport)
        surface.blit(self.image_dict['GAME_NAME_BOX'][0],
                     self.image_dict['GAME_NAME_BOX'][1])
        surface.blit(self.mario.image, self.mario.rect)
        surface.blit(self.cursor.image, self.cursor.rect)
        self.overhead_info.draw(surface)","surface.blit(self.image_dict['GAME_NAME_BOX'][0], self.image_dict['GAME_NAME_BOX'][1])",surface.blit(*self.image_dict['GAME_NAME_BOX'][:2]),"iterable_zj[0], iterable_zj[1]",*self.image_dict['GAME_NAME_BOX'][:2],*self.image_dict['GAME_NAME_BOX'][:2],1
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/big_bird/modeling_big_bird.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/big_bird/modeling_big_bird.py,BigBirdBlockSparseAttention,_bigbird_block_rand_mask_with_head$1116,"def _bigbird_block_rand_mask_with_head(
        self,
        from_seq_length,
        to_seq_length,
        from_block_size,
        to_block_size,
        num_heads,
        plan_from_length,
        plan_num_rand_blocks,
        window_block_left=1,
        window_block_right=1,
        global_block_top=1,
        global_block_bottom=1,
        global_block_left=1,
        global_block_right=1,
    ):
        """"""
        Create adjacency list of random attention.

        Args:
            from_seq_length: int. length of from sequence.
            to_seq_length: int. length of to sequence.
            from_block_size: int. size of block in from sequence.
            to_block_size: int. size of block in to sequence.
            num_heads: int. total number of heads.
            plan_from_length: list. plan from length where num_random_blocks are chosen from.
            plan_num_rand_blocks: list. number of rand blocks within the plan.
            window_block_left: int. number of blocks of window to left of a block.
            window_block_right: int. number of blocks of window to right of a block.
            global_block_top: int. number of blocks at the top.
            global_block_bottom: int. number of blocks at the bottom.
            global_block_left: int. Number of blocks globally used to the left.
            global_block_right: int. Number of blocks globally used to the right.

        Returns:
            adjacency list of size num_head where each element is of size from_seq_length//from_block_size-2 by
            num_rand_blocks
        """"""
        # using this method when from_seq_length not in [1024, 3072, 4096]

        if from_seq_length // from_block_size != to_seq_length // to_block_size:
            raise ValueError(""Error the number of blocks needs to be same!"")

        if from_seq_length not in plan_from_length:
            raise ValueError(""Error from sequence length not in plan!"")

        # Total number of blocks in the mmask
        num_blocks = from_seq_length // from_block_size
        # Number of blocks per plan
        plan_block_length = np.array(plan_from_length) // from_block_size
        # till when to follow plan
        max_plan_idx = plan_from_length.index(from_seq_length)
        # Random Attention adjacency list
        rand_attn = [
            np.zeros((num_blocks, np.sum(plan_num_rand_blocks[: max_plan_idx + 1])), dtype=np.int32)
            for i in range(num_heads)
        ]

        # We will go iteratively over the plan blocks and pick random number of
        # Attention blocks from the legally allowed blocks
        for plan_idx in range(max_plan_idx + 1):
            rnd_r_cnt = 0
            if plan_idx > 0:
                # set the row for all from_blocks starting from 0 to
                # plan_block_length[plan_idx-1]
                # column indx start fromm plan_block_length[plan_idx-1] and ends at
                # plan_block_length[plan_idx]
                if plan_num_rand_blocks[plan_idx] > 0:
                    rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx]))
                    curr_r_cnt = int(np.sum(plan_num_rand_blocks[: plan_idx + 1]))
                    for blk_rw_idx in range(global_block_top, plan_block_length[plan_idx - 1]):
                        for h in range(num_heads):
                            rand_attn[h][blk_rw_idx, rnd_r_cnt:curr_r_cnt] = self._get_single_block_row_attention(
                                block_id=blk_rw_idx,
                                to_start_block_id=plan_block_length[plan_idx - 1],
                                to_end_block_id=plan_block_length[plan_idx],
                                num_rand_blocks=plan_num_rand_blocks[plan_idx],
                                window_block_left=window_block_left,
                                window_block_right=window_block_right,
                                global_block_left=global_block_left,
                                global_block_right=global_block_right,
                            )

                for pl_id in range(plan_idx):
                    if plan_num_rand_blocks[pl_id] == 0:
                        continue
                    for blk_rw_idx in range(plan_block_length[plan_idx - 1], plan_block_length[plan_idx]):
                        rnd_r_cnt = 0
                        to_start_block_id = 0
                        if pl_id > 0:
                            rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:pl_id]))
                            to_start_block_id = plan_block_length[pl_id - 1]
                        curr_r_cnt = int(np.sum(plan_num_rand_blocks[: pl_id + 1]))
                        for h in range(num_heads):
                            rand_attn[h][blk_rw_idx, rnd_r_cnt:curr_r_cnt] = self._get_single_block_row_attention(
                                block_id=blk_rw_idx,
                                to_start_block_id=to_start_block_id,
                                to_end_block_id=plan_block_length[pl_id],
                                num_rand_blocks=plan_num_rand_blocks[pl_id],
                                window_block_left=window_block_left,
                                window_block_right=window_block_right,
                                global_block_left=global_block_left,
                                global_block_right=global_block_right,
                            )

            if plan_num_rand_blocks[plan_idx] == 0:
                continue
            curr_r_cnt = int(np.sum(plan_num_rand_blocks[: plan_idx + 1]))
            from_start_block_id = global_block_top
            to_start_block_id = 0
            if plan_idx > 0:
                rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx]))
                from_start_block_id = plan_block_length[plan_idx - 1]
                to_start_block_id = plan_block_length[plan_idx - 1]

            for blk_rw_idx in range(from_start_block_id, plan_block_length[plan_idx]):
                for h in range(num_heads):
                    rand_attn[h][blk_rw_idx, rnd_r_cnt:curr_r_cnt] = self._get_single_block_row_attention(
                        block_id=blk_rw_idx,
                        to_start_block_id=to_start_block_id,
                        to_end_block_id=plan_block_length[plan_idx],
                        num_rand_blocks=plan_num_rand_blocks[plan_idx],
                        window_block_left=window_block_left,
                        window_block_right=window_block_right,
                        global_block_left=global_block_left,
                        global_block_right=global_block_right,
                    )

        for nh in range(num_heads):
            rand_attn[nh] = rand_attn[nh][global_block_top : num_blocks - global_block_bottom, :]

        return rand_attn","range(plan_block_length[plan_idx - 1], plan_block_length[plan_idx])",range(*plan_block_length[plan_idx - 1:plan_idx + 1]),"iterable_zj[plan_idx - 1], iterable_zj[plan_idx]",*plan_block_length[plan_idx-1:plan_idx+1],*plan_block_length[plan_idx - 1:plan_idx + 1],1
MillionHeroAssistant,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MillionHeroAssistant/core/crawler/pmi.py,https://github.com/smileboywtu/MillionHeroAssistant/tree/master/core/crawler/pmi.py,,baidu_count_daemon$151,"def baidu_count_daemon(exchage_queue, outputqueue, timeout=5):
    """"""
    count words
    
    :return: 
    """"""
    while True:
        question, answers, true_flag = exchage_queue.get()
        try:
            baidu_summary = baidu_count(question, answers, timeout=timeout)
            sougou_summary = sougou_count(question, answers, timeout=timeout)
            summary = {
                key: baidu_summary[key] + sougou_summary[key]
                for key in baidu_summary
            }
            summary_li = sorted(summary.items(), key=operator.itemgetter(1), reverse=True)
            if true_flag:
                recommend = ""{0}\n{1}"".format(
                    ""(**) {0}"".format(summary_li[0][0]),
                    ""(  ) {0}"".format(summary_li[-1][0]))
            else:
                recommend = ""{0}\n{1}"".format(
                    ""(  ) {0}"".format(summary_li[0][0]),
                    ""(**) {0}"".format(summary_li[-1][0]))
            outputqueue.put(stdout_template.BAIDU_TPL.format(
                ""\n"".join(map(lambda item: ""{0}: {1}"".format(item[0], item[1]), summary_li)),
                recommend
            ))
        except Exception as e:
            logger.error(str(e), exc_info=True)","'{0}: {1}'.format(item[0], item[1])",'{0}: {1}'.format(*item[:2]),"iterable_zj[0], iterable_zj[1]",*item[:2],*item[:2],1
pennylane,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pennylane/tests/devices/test_default_qubit_tf.py,https://github.com/PennyLaneAI/pennylane/tree/master/tests/devices/test_default_qubit_tf.py,TestPassthruIntegration,circuit$2037,"def circuit(x, weights, w):
            """"""In this example, a mixture of scalar
            arguments, array arguments, and keyword arguments are used.""""""
            qml.QubitStateVector(1j * np.array([1, -1]) / np.sqrt(2), wires=w)
            operation(x, weights[0], weights[1], wires=w)
            return qml.expval(qml.PauliX(w))","operation(x, weights[0], weights[1], wires=w)","operation(x, *weights[:2], wires=w)","iterable_zj[0], iterable_zj[1]",*weights[:2],*weights[:2],1
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/tests/unit/core/test_notes.py,https://github.com/bspaans/python-mingus/tree/master/tests/unit/core/test_notes.py,test_notes,test_int_to_note$37,"def test_int_to_note(self):
        known = {
            (0, ""#""): ""C"",
            (3, ""#""): ""D#"",
            (8, ""#""): ""G#"",
            (11, ""#""): ""B"",
            (0, ""b""): ""C"",
            (3, ""b""): ""Eb"",
            (8, ""b""): ""Ab"",
            (11, ""b""): ""B"",
        }
        for k in known:
            self.assertEqual(
                known[k],
                notes.int_to_note(k[0], k[1]),
                '%s with ""%s"" not corrisponding to %s, expecting %s'
                % (k[0], k[1], notes.int_to_note(k[0], k[1]), known[k]),
            )","notes.int_to_note(k[0], k[1])",notes.int_to_note(*k[:2]),"iterable_zj[0], iterable_zj[1]",*k[:2],*k[:2],1
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/tests/unit/core/test_notes.py,https://github.com/bspaans/python-mingus/tree/master/tests/unit/core/test_notes.py,test_notes,test_int_to_note$37,"def test_int_to_note(self):
        known = {
            (0, ""#""): ""C"",
            (3, ""#""): ""D#"",
            (8, ""#""): ""G#"",
            (11, ""#""): ""B"",
            (0, ""b""): ""C"",
            (3, ""b""): ""Eb"",
            (8, ""b""): ""Ab"",
            (11, ""b""): ""B"",
        }
        for k in known:
            self.assertEqual(
                known[k],
                notes.int_to_note(k[0], k[1]),
                '%s with ""%s"" not corrisponding to %s, expecting %s'
                % (k[0], k[1], notes.int_to_note(k[0], k[1]), known[k]),
            )","notes.int_to_note(k[0], k[1])",notes.int_to_note(*k[:2]),"iterable_zj[0], iterable_zj[1]",*k[:2],*k[:2],1
wdb,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wdb/client/wdb/ui.py,https://github.com/Kozea/wdb/tree/master/client/wdb/ui.py,Interaction,do_diff$835,"def do_diff(self, data):
        if '?' not in data and '<>' not in data:
            self.fail(
                'Diff',
                'Diff error',
                'You must provide two expression '
                'separated by ""?"" or ""<>"" to make a diff',
            )
            return
        pretty = '?' in data
        expressions = [
            expression.strip()
            for expression in (
                data.split('?') if '?' in data else data.split('<>')
            )
        ]
        strings = []
        for expression in expressions:
            try:
                strings.append(
                    eval_(expression, self.get_globals(), self.current_locals)
                )
            except Exception:
                self.fail(
                    'Diff',
                    ""Diff failed: Expression %s ""
                    ""failed to evaluate to a string"" % expression,
                )
                return

        render = (
            (
                (
                    lambda x: self.db.better_repr(x, html=False)
                    or self.db.safe_repr(x)
                )
            )
            if pretty
            else str
        )
        strings = [
            render(string) if not is_str(string) else string
            for string in strings
        ]
        self.db.send(
            'RawHTML|%s'
            % dump(
                {
                    'for': u('Difference between %s')
                    % (' and '.join(expressions)),
                    'val': self.htmldiff.make_table(
                        strings[0].splitlines(keepends=True),
                        strings[1].splitlines(keepends=True),
                        expressions[0],
                        expressions[1],
                    ),
                }
            )
        )","self.htmldiff.make_table(strings[0].splitlines(keepends=True), strings[1].splitlines(keepends=True), expressions[0], expressions[1])","self.htmldiff.make_table(strings[0].splitlines(keepends=True), strings[1].splitlines(keepends=True), *expressions[:2])","iterable_zj[0], iterable_zj[1]",*expressions[:2],*expressions[:2],1
Algorithm_Interview_Notes-Chinese,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Algorithm_Interview_Notes-Chinese/_codes/machine_learning/KMeans/kmeans.py,https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese/tree/master/_codes/machine_learning/KMeans/kmeans.py,,_test$87,"def _test():
    """"""""""""
    file_path = r""./data.txt""

    data = load_data(file_path)
    print(data)
    print(np.shape(data)[1])

    s = score_euclidean(data[0], data[1])
    print(s)

    centers = rand_center(data, 3)
    print(centers)","score_euclidean(data[0], data[1])",score_euclidean(*data[:2]),"iterable_zj[0], iterable_zj[1]",*data[:2],*data[:2],1
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/tests/test_tokenization_common.py,https://github.com/huggingface/transformers/tree/master/tests/test_tokenization_common.py,TokenizerTesterMixin,test_add_tokens_tokenizer$822,"def test_add_tokens_tokenizer(self):
        tokenizers = self.get_tokenizers(do_lower_case=False)
        for tokenizer in tokenizers:
            with self.subTest(f""{tokenizer.__class__.__name__}""):
                vocab_size = tokenizer.vocab_size
                all_size = len(tokenizer)

                self.assertNotEqual(vocab_size, 0)

                # We usually have added tokens from the start in tests because our vocab fixtures are
                # smaller than the original vocabs - let's not assert this
                # self.assertEqual(vocab_size, all_size)

                new_toks = [""aaaaa bbbbbb"", ""cccccccccdddddddd""]
                added_toks = tokenizer.add_tokens(new_toks)
                vocab_size_2 = tokenizer.vocab_size
                all_size_2 = len(tokenizer)

                self.assertNotEqual(vocab_size_2, 0)
                self.assertEqual(vocab_size, vocab_size_2)
                self.assertEqual(added_toks, len(new_toks))
                self.assertEqual(all_size_2, all_size + len(new_toks))

                tokens = tokenizer.encode(""aaaaa bbbbbb low cccccccccdddddddd l"", add_special_tokens=False)

                self.assertGreaterEqual(len(tokens), 4)
                self.assertGreater(tokens[0], tokenizer.vocab_size - 1)
                self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)

                new_toks_2 = {""eos_token"": "">>>>|||<||<<|<<"", ""pad_token"": ""<<<<<|||>|>>>>|>""}
                added_toks_2 = tokenizer.add_special_tokens(new_toks_2)
                vocab_size_3 = tokenizer.vocab_size
                all_size_3 = len(tokenizer)

                self.assertNotEqual(vocab_size_3, 0)
                self.assertEqual(vocab_size, vocab_size_3)
                self.assertEqual(added_toks_2, len(new_toks_2))
                self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))

                tokens = tokenizer.encode(
                    "">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd <<<<<|||>|>>>>|> l"", add_special_tokens=False
                )

                self.assertGreaterEqual(len(tokens), 6)
                self.assertGreater(tokens[0], tokenizer.vocab_size - 1)
                self.assertGreater(tokens[0], tokens[1])
                self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)
                self.assertGreater(tokens[-2], tokens[-3])
                self.assertEqual(tokens[0], tokenizer.eos_token_id)
                self.assertEqual(tokens[-2], tokenizer.pad_token_id)","self.assertGreater(tokens[0], tokens[1])",self.assertGreater(*tokens[:2]),"iterable_zj[0], iterable_zj[1]",*tokens[:2],*tokens[:2],1
gnss-ins-sim,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gnss-ins-sim/gnss_ins_sim/attitude/attitude.py,https://github.com/Aceinna/gnss-ins-sim/tree/master/gnss_ins_sim/attitude/attitude.py,,dcm2euler$496,"def dcm2euler(dcm, rot_seq='zyx'):
    """"""
    Convert direction cosine matrix to Euler angles.
    The Euler angles rotate the frame n to the frame b according to specified
    rotation sequency. The DCM is a 3x3 coordinate transformation matrix from n
    to b. That is v_b  = DCM * v_n. '_b' or '_n' mean the vector 'v' is expressed
    in the frame b or n.
    Args:
        dcm: 3x3 coordinate transformation matrix from n to b
        rot_seq: rotation sequence corresponding to the angles.
    Returns:
        angles: 3x1 Euler angles, rad.
    """"""
    if rot_seq == 'zyx':
        #     [          cy*cz,          cy*sz,            -sy]
        #     [ sy*sx*cz-sz*cx, sy*sx*sz+cz*cx,          cy*sx]
        #     [ sy*cx*cz+sz*sx, sy*cx*sz-cz*sx,          cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[0][1], dcm[0][0], -dcm[0][2],
                                      dcm[1][2], dcm[2][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zyz':
        #     [  cz2*cy*cz-sz2*sz,  cz2*cy*sz+sz2*cz,           -cz2*sy]
        #     [ -sz2*cy*cz-cz2*sz, -sz2*cy*sz+cz2*cz,            sz2*sy]
        #     [             sy*cz,             sy*sz,                cy]
        [r1, r2, r3] = two_axis_rot(dcm[2][1], dcm[2][0], dcm[2][2],
                                    dcm[1][2], -dcm[0][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zxy':
        #     [ cy*cz-sy*sx*sz, cy*sz+sy*sx*cz,         -sy*cx]
        #     [         -sz*cx,          cz*cx,             sx]
        #     [ sy*cz+cy*sx*sz, sy*sz-cy*sx*cz,          cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[1][0], dcm[1][1], dcm[1][2],
                                      -dcm[0][2], dcm[2][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zxz':
        #     [  cz2*cz-sz2*cx*sz,  cz2*sz+sz2*cx*cz,            sz2*sx]
        #     [ -sz2*cz-cz2*cx*sz, -sz2*sz+cz2*cx*cz,            cz2*sx]
        #     [             sz*sx,            -cz*sx,                cx]
        [r1, r2, r3] = two_axis_rot(dcm[2][0], -dcm[2][1], dcm[2][2],
                                    dcm[0][2], dcm[1][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yxz':
        #     [  cy*cz+sy*sx*sz,           sz*cx, -sy*cz+cy*sx*sz]
        #     [ -cy*sz+sy*sx*cz,           cz*cx,  sy*sz+cy*sx*cz]
        #     [           sy*cx,             -sx,           cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[2][0], dcm[2][2], -dcm[2][1],
                                      dcm[0][1], dcm[1][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yxy':
        #     [  cy2*cy-sy2*cx*sy,            sy2*sx, -cy2*sy-sy2*cx*cy]
        #     [             sy*sx,                cx,             cy*sx]
        #     [  sy2*cy+cy2*cx*sy,           -cy2*sx, -sy2*sy+cy2*cx*cy]
        [r1, r2, r3] = two_axis_rot(dcm[1][0], dcm[1][2], dcm[1][1],
                                    dcm[0][1], -dcm[2][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yzx':
        #     [           cy*cz,              sz,          -sy*cz]
        #     [ -sz*cx*cy+sy*sx,           cz*cx,  sy*cx*sz+cy*sx]
        #     [  cy*sx*sz+sy*cx,          -cz*sx, -sy*sx*sz+cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[0][2], dcm[0][0], dcm[0][1],
                                      -dcm[2][1], dcm[1][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yzy':
        #     [  cy2*cz*cy-sy2*sy,            cy2*sz, -cy2*cz*sy-sy2*cy]
        #     [            -cy*sz,                cz,             sy*sz]
        #     [  sy2*cz*cy+cy2*sy,            sy2*sz, -sy2*cz*sy+cy2*cy]
        [r1, r2, r3] = two_axis_rot(dcm[1][2], -dcm[1][0], dcm[1][1],
                                    dcm[2][1], dcm[0][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xyz':
        #     [          cy*cz, sz*cx+sy*sx*cz, sz*sx-sy*cx*cz]
        #     [         -cy*sz, cz*cx-sy*sx*sz, cz*sx+sy*cx*sz]
        #     [             sy,         -cy*sx,          cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[2][1], dcm[2][2], dcm[2][0],
                                      -dcm[1][0], dcm[0][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xyx':
        #     [                cy,             sy*sx,            -sy*cx]
        #     [            sx2*sy,  cx2*cx-sx2*cy*sx,  cx2*sx+sx2*cy*cx]
        #     [            cx2*sy, -sx2*cx-cx2*cy*sx, -sx2*sx+cx2*cy*cx]
        [r1, r2, r3] = two_axis_rot(dcm[0][1], -dcm[0][2], dcm[0][0],
                                    dcm[1][0], dcm[2][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xzy':
        #     [          cy*cz, sz*cx*cy+sy*sx, cy*sx*sz-sy*cx]
        #     [            -sz,          cz*cx,          cz*sx]
        #     [          sy*cz, sy*cx*sz-cy*sx, sy*sx*sz+cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[1][2], dcm[1][1], -dcm[1][0],
                                      dcm[2][0], dcm[0][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xzx':
        #     [                cz,             sz*cx,             sz*sx]
        #     [           -cx2*sz,  cx2*cz*cx-sx2*sx,  cx2*cz*sx+sx2*cx]
        #     [            sx2*sz, -sx2*cz*cx-cx2*sx, -sx2*cz*sx+cx2*cx]
        [r1, r2, r3] = two_axis_rot(dcm[0][2], dcm[0][1], dcm[0][0],
                                    dcm[2][0], -dcm[1][0])
        return np.array([r1, r2, r3])
    else:
        return False","two_axis_rot(dcm[2][1], dcm[2][0], dcm[2][2], dcm[1][2], -dcm[0][2])","two_axis_rot(*dcm[2][1::-1] + dcm[2][2:], dcm[1][2], -dcm[0][2])","iterable_zj[1], iterable_zj[0], iterable_zj[2]",*dcm[2][1::-1] + dcm[2][2:],*dcm[2][:4:2],0
gnss-ins-sim,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gnss-ins-sim/gnss_ins_sim/attitude/attitude.py,https://github.com/Aceinna/gnss-ins-sim/tree/master/gnss_ins_sim/attitude/attitude.py,,dcm2euler$496,"def dcm2euler(dcm, rot_seq='zyx'):
    """"""
    Convert direction cosine matrix to Euler angles.
    The Euler angles rotate the frame n to the frame b according to specified
    rotation sequency. The DCM is a 3x3 coordinate transformation matrix from n
    to b. That is v_b  = DCM * v_n. '_b' or '_n' mean the vector 'v' is expressed
    in the frame b or n.
    Args:
        dcm: 3x3 coordinate transformation matrix from n to b
        rot_seq: rotation sequence corresponding to the angles.
    Returns:
        angles: 3x1 Euler angles, rad.
    """"""
    if rot_seq == 'zyx':
        #     [          cy*cz,          cy*sz,            -sy]
        #     [ sy*sx*cz-sz*cx, sy*sx*sz+cz*cx,          cy*sx]
        #     [ sy*cx*cz+sz*sx, sy*cx*sz-cz*sx,          cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[0][1], dcm[0][0], -dcm[0][2],
                                      dcm[1][2], dcm[2][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zyz':
        #     [  cz2*cy*cz-sz2*sz,  cz2*cy*sz+sz2*cz,           -cz2*sy]
        #     [ -sz2*cy*cz-cz2*sz, -sz2*cy*sz+cz2*cz,            sz2*sy]
        #     [             sy*cz,             sy*sz,                cy]
        [r1, r2, r3] = two_axis_rot(dcm[2][1], dcm[2][0], dcm[2][2],
                                    dcm[1][2], -dcm[0][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zxy':
        #     [ cy*cz-sy*sx*sz, cy*sz+sy*sx*cz,         -sy*cx]
        #     [         -sz*cx,          cz*cx,             sx]
        #     [ sy*cz+cy*sx*sz, sy*sz-cy*sx*cz,          cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[1][0], dcm[1][1], dcm[1][2],
                                      -dcm[0][2], dcm[2][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zxz':
        #     [  cz2*cz-sz2*cx*sz,  cz2*sz+sz2*cx*cz,            sz2*sx]
        #     [ -sz2*cz-cz2*cx*sz, -sz2*sz+cz2*cx*cz,            cz2*sx]
        #     [             sz*sx,            -cz*sx,                cx]
        [r1, r2, r3] = two_axis_rot(dcm[2][0], -dcm[2][1], dcm[2][2],
                                    dcm[0][2], dcm[1][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yxz':
        #     [  cy*cz+sy*sx*sz,           sz*cx, -sy*cz+cy*sx*sz]
        #     [ -cy*sz+sy*sx*cz,           cz*cx,  sy*sz+cy*sx*cz]
        #     [           sy*cx,             -sx,           cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[2][0], dcm[2][2], -dcm[2][1],
                                      dcm[0][1], dcm[1][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yxy':
        #     [  cy2*cy-sy2*cx*sy,            sy2*sx, -cy2*sy-sy2*cx*cy]
        #     [             sy*sx,                cx,             cy*sx]
        #     [  sy2*cy+cy2*cx*sy,           -cy2*sx, -sy2*sy+cy2*cx*cy]
        [r1, r2, r3] = two_axis_rot(dcm[1][0], dcm[1][2], dcm[1][1],
                                    dcm[0][1], -dcm[2][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yzx':
        #     [           cy*cz,              sz,          -sy*cz]
        #     [ -sz*cx*cy+sy*sx,           cz*cx,  sy*cx*sz+cy*sx]
        #     [  cy*sx*sz+sy*cx,          -cz*sx, -sy*sx*sz+cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[0][2], dcm[0][0], dcm[0][1],
                                      -dcm[2][1], dcm[1][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yzy':
        #     [  cy2*cz*cy-sy2*sy,            cy2*sz, -cy2*cz*sy-sy2*cy]
        #     [            -cy*sz,                cz,             sy*sz]
        #     [  sy2*cz*cy+cy2*sy,            sy2*sz, -sy2*cz*sy+cy2*cy]
        [r1, r2, r3] = two_axis_rot(dcm[1][2], -dcm[1][0], dcm[1][1],
                                    dcm[2][1], dcm[0][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xyz':
        #     [          cy*cz, sz*cx+sy*sx*cz, sz*sx-sy*cx*cz]
        #     [         -cy*sz, cz*cx-sy*sx*sz, cz*sx+sy*cx*sz]
        #     [             sy,         -cy*sx,          cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[2][1], dcm[2][2], dcm[2][0],
                                      -dcm[1][0], dcm[0][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xyx':
        #     [                cy,             sy*sx,            -sy*cx]
        #     [            sx2*sy,  cx2*cx-sx2*cy*sx,  cx2*sx+sx2*cy*cx]
        #     [            cx2*sy, -sx2*cx-cx2*cy*sx, -sx2*sx+cx2*cy*cx]
        [r1, r2, r3] = two_axis_rot(dcm[0][1], -dcm[0][2], dcm[0][0],
                                    dcm[1][0], dcm[2][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xzy':
        #     [          cy*cz, sz*cx*cy+sy*sx, cy*sx*sz-sy*cx]
        #     [            -sz,          cz*cx,          cz*sx]
        #     [          sy*cz, sy*cx*sz-cy*sx, sy*sx*sz+cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[1][2], dcm[1][1], -dcm[1][0],
                                      dcm[2][0], dcm[0][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xzx':
        #     [                cz,             sz*cx,             sz*sx]
        #     [           -cx2*sz,  cx2*cz*cx-sx2*sx,  cx2*cz*sx+sx2*cx]
        #     [            sx2*sz, -sx2*cz*cx-cx2*sx, -sx2*cz*sx+cx2*cx]
        [r1, r2, r3] = two_axis_rot(dcm[0][2], dcm[0][1], dcm[0][0],
                                    dcm[2][0], -dcm[1][0])
        return np.array([r1, r2, r3])
    else:
        return False","three_axis_rot(-dcm[1][0], dcm[1][1], dcm[1][2], -dcm[0][2], dcm[2][2])","three_axis_rot(-*dcm[1][:3], -dcm[0][2], dcm[2][2])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*dcm[1][:3],*dcm[1][1:3],0
gnss-ins-sim,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gnss-ins-sim/gnss_ins_sim/attitude/attitude.py,https://github.com/Aceinna/gnss-ins-sim/tree/master/gnss_ins_sim/attitude/attitude.py,,dcm2euler$496,"def dcm2euler(dcm, rot_seq='zyx'):
    """"""
    Convert direction cosine matrix to Euler angles.
    The Euler angles rotate the frame n to the frame b according to specified
    rotation sequency. The DCM is a 3x3 coordinate transformation matrix from n
    to b. That is v_b  = DCM * v_n. '_b' or '_n' mean the vector 'v' is expressed
    in the frame b or n.
    Args:
        dcm: 3x3 coordinate transformation matrix from n to b
        rot_seq: rotation sequence corresponding to the angles.
    Returns:
        angles: 3x1 Euler angles, rad.
    """"""
    if rot_seq == 'zyx':
        #     [          cy*cz,          cy*sz,            -sy]
        #     [ sy*sx*cz-sz*cx, sy*sx*sz+cz*cx,          cy*sx]
        #     [ sy*cx*cz+sz*sx, sy*cx*sz-cz*sx,          cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[0][1], dcm[0][0], -dcm[0][2],
                                      dcm[1][2], dcm[2][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zyz':
        #     [  cz2*cy*cz-sz2*sz,  cz2*cy*sz+sz2*cz,           -cz2*sy]
        #     [ -sz2*cy*cz-cz2*sz, -sz2*cy*sz+cz2*cz,            sz2*sy]
        #     [             sy*cz,             sy*sz,                cy]
        [r1, r2, r3] = two_axis_rot(dcm[2][1], dcm[2][0], dcm[2][2],
                                    dcm[1][2], -dcm[0][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zxy':
        #     [ cy*cz-sy*sx*sz, cy*sz+sy*sx*cz,         -sy*cx]
        #     [         -sz*cx,          cz*cx,             sx]
        #     [ sy*cz+cy*sx*sz, sy*sz-cy*sx*cz,          cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[1][0], dcm[1][1], dcm[1][2],
                                      -dcm[0][2], dcm[2][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zxz':
        #     [  cz2*cz-sz2*cx*sz,  cz2*sz+sz2*cx*cz,            sz2*sx]
        #     [ -sz2*cz-cz2*cx*sz, -sz2*sz+cz2*cx*cz,            cz2*sx]
        #     [             sz*sx,            -cz*sx,                cx]
        [r1, r2, r3] = two_axis_rot(dcm[2][0], -dcm[2][1], dcm[2][2],
                                    dcm[0][2], dcm[1][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yxz':
        #     [  cy*cz+sy*sx*sz,           sz*cx, -sy*cz+cy*sx*sz]
        #     [ -cy*sz+sy*sx*cz,           cz*cx,  sy*sz+cy*sx*cz]
        #     [           sy*cx,             -sx,           cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[2][0], dcm[2][2], -dcm[2][1],
                                      dcm[0][1], dcm[1][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yxy':
        #     [  cy2*cy-sy2*cx*sy,            sy2*sx, -cy2*sy-sy2*cx*cy]
        #     [             sy*sx,                cx,             cy*sx]
        #     [  sy2*cy+cy2*cx*sy,           -cy2*sx, -sy2*sy+cy2*cx*cy]
        [r1, r2, r3] = two_axis_rot(dcm[1][0], dcm[1][2], dcm[1][1],
                                    dcm[0][1], -dcm[2][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yzx':
        #     [           cy*cz,              sz,          -sy*cz]
        #     [ -sz*cx*cy+sy*sx,           cz*cx,  sy*cx*sz+cy*sx]
        #     [  cy*sx*sz+sy*cx,          -cz*sx, -sy*sx*sz+cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[0][2], dcm[0][0], dcm[0][1],
                                      -dcm[2][1], dcm[1][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yzy':
        #     [  cy2*cz*cy-sy2*sy,            cy2*sz, -cy2*cz*sy-sy2*cy]
        #     [            -cy*sz,                cz,             sy*sz]
        #     [  sy2*cz*cy+cy2*sy,            sy2*sz, -sy2*cz*sy+cy2*cy]
        [r1, r2, r3] = two_axis_rot(dcm[1][2], -dcm[1][0], dcm[1][1],
                                    dcm[2][1], dcm[0][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xyz':
        #     [          cy*cz, sz*cx+sy*sx*cz, sz*sx-sy*cx*cz]
        #     [         -cy*sz, cz*cx-sy*sx*sz, cz*sx+sy*cx*sz]
        #     [             sy,         -cy*sx,          cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[2][1], dcm[2][2], dcm[2][0],
                                      -dcm[1][0], dcm[0][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xyx':
        #     [                cy,             sy*sx,            -sy*cx]
        #     [            sx2*sy,  cx2*cx-sx2*cy*sx,  cx2*sx+sx2*cy*cx]
        #     [            cx2*sy, -sx2*cx-cx2*cy*sx, -sx2*sx+cx2*cy*cx]
        [r1, r2, r3] = two_axis_rot(dcm[0][1], -dcm[0][2], dcm[0][0],
                                    dcm[1][0], dcm[2][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xzy':
        #     [          cy*cz, sz*cx*cy+sy*sx, cy*sx*sz-sy*cx]
        #     [            -sz,          cz*cx,          cz*sx]
        #     [          sy*cz, sy*cx*sz-cy*sx, sy*sx*sz+cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[1][2], dcm[1][1], -dcm[1][0],
                                      dcm[2][0], dcm[0][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xzx':
        #     [                cz,             sz*cx,             sz*sx]
        #     [           -cx2*sz,  cx2*cz*cx-sx2*sx,  cx2*cz*sx+sx2*cx]
        #     [            sx2*sz, -sx2*cz*cx-cx2*sx, -sx2*cz*sx+cx2*cx]
        [r1, r2, r3] = two_axis_rot(dcm[0][2], dcm[0][1], dcm[0][0],
                                    dcm[2][0], -dcm[1][0])
        return np.array([r1, r2, r3])
    else:
        return False","three_axis_rot(dcm[2][0], dcm[2][2], -dcm[2][1], dcm[0][1], dcm[1][1])","three_axis_rot(*dcm[2][::2], -dcm[2][1], dcm[0][1], dcm[1][1])","iterable_zj[0], iterable_zj[2]",*dcm[2][::2],*dcm[2][:4:2],0
gnss-ins-sim,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gnss-ins-sim/gnss_ins_sim/attitude/attitude.py,https://github.com/Aceinna/gnss-ins-sim/tree/master/gnss_ins_sim/attitude/attitude.py,,dcm2euler$496,"def dcm2euler(dcm, rot_seq='zyx'):
    """"""
    Convert direction cosine matrix to Euler angles.
    The Euler angles rotate the frame n to the frame b according to specified
    rotation sequency. The DCM is a 3x3 coordinate transformation matrix from n
    to b. That is v_b  = DCM * v_n. '_b' or '_n' mean the vector 'v' is expressed
    in the frame b or n.
    Args:
        dcm: 3x3 coordinate transformation matrix from n to b
        rot_seq: rotation sequence corresponding to the angles.
    Returns:
        angles: 3x1 Euler angles, rad.
    """"""
    if rot_seq == 'zyx':
        #     [          cy*cz,          cy*sz,            -sy]
        #     [ sy*sx*cz-sz*cx, sy*sx*sz+cz*cx,          cy*sx]
        #     [ sy*cx*cz+sz*sx, sy*cx*sz-cz*sx,          cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[0][1], dcm[0][0], -dcm[0][2],
                                      dcm[1][2], dcm[2][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zyz':
        #     [  cz2*cy*cz-sz2*sz,  cz2*cy*sz+sz2*cz,           -cz2*sy]
        #     [ -sz2*cy*cz-cz2*sz, -sz2*cy*sz+cz2*cz,            sz2*sy]
        #     [             sy*cz,             sy*sz,                cy]
        [r1, r2, r3] = two_axis_rot(dcm[2][1], dcm[2][0], dcm[2][2],
                                    dcm[1][2], -dcm[0][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zxy':
        #     [ cy*cz-sy*sx*sz, cy*sz+sy*sx*cz,         -sy*cx]
        #     [         -sz*cx,          cz*cx,             sx]
        #     [ sy*cz+cy*sx*sz, sy*sz-cy*sx*cz,          cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[1][0], dcm[1][1], dcm[1][2],
                                      -dcm[0][2], dcm[2][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zxz':
        #     [  cz2*cz-sz2*cx*sz,  cz2*sz+sz2*cx*cz,            sz2*sx]
        #     [ -sz2*cz-cz2*cx*sz, -sz2*sz+cz2*cx*cz,            cz2*sx]
        #     [             sz*sx,            -cz*sx,                cx]
        [r1, r2, r3] = two_axis_rot(dcm[2][0], -dcm[2][1], dcm[2][2],
                                    dcm[0][2], dcm[1][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yxz':
        #     [  cy*cz+sy*sx*sz,           sz*cx, -sy*cz+cy*sx*sz]
        #     [ -cy*sz+sy*sx*cz,           cz*cx,  sy*sz+cy*sx*cz]
        #     [           sy*cx,             -sx,           cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[2][0], dcm[2][2], -dcm[2][1],
                                      dcm[0][1], dcm[1][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yxy':
        #     [  cy2*cy-sy2*cx*sy,            sy2*sx, -cy2*sy-sy2*cx*cy]
        #     [             sy*sx,                cx,             cy*sx]
        #     [  sy2*cy+cy2*cx*sy,           -cy2*sx, -sy2*sy+cy2*cx*cy]
        [r1, r2, r3] = two_axis_rot(dcm[1][0], dcm[1][2], dcm[1][1],
                                    dcm[0][1], -dcm[2][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yzx':
        #     [           cy*cz,              sz,          -sy*cz]
        #     [ -sz*cx*cy+sy*sx,           cz*cx,  sy*cx*sz+cy*sx]
        #     [  cy*sx*sz+sy*cx,          -cz*sx, -sy*sx*sz+cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[0][2], dcm[0][0], dcm[0][1],
                                      -dcm[2][1], dcm[1][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yzy':
        #     [  cy2*cz*cy-sy2*sy,            cy2*sz, -cy2*cz*sy-sy2*cy]
        #     [            -cy*sz,                cz,             sy*sz]
        #     [  sy2*cz*cy+cy2*sy,            sy2*sz, -sy2*cz*sy+cy2*cy]
        [r1, r2, r3] = two_axis_rot(dcm[1][2], -dcm[1][0], dcm[1][1],
                                    dcm[2][1], dcm[0][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xyz':
        #     [          cy*cz, sz*cx+sy*sx*cz, sz*sx-sy*cx*cz]
        #     [         -cy*sz, cz*cx-sy*sx*sz, cz*sx+sy*cx*sz]
        #     [             sy,         -cy*sx,          cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[2][1], dcm[2][2], dcm[2][0],
                                      -dcm[1][0], dcm[0][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xyx':
        #     [                cy,             sy*sx,            -sy*cx]
        #     [            sx2*sy,  cx2*cx-sx2*cy*sx,  cx2*sx+sx2*cy*cx]
        #     [            cx2*sy, -sx2*cx-cx2*cy*sx, -sx2*sx+cx2*cy*cx]
        [r1, r2, r3] = two_axis_rot(dcm[0][1], -dcm[0][2], dcm[0][0],
                                    dcm[1][0], dcm[2][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xzy':
        #     [          cy*cz, sz*cx*cy+sy*sx, cy*sx*sz-sy*cx]
        #     [            -sz,          cz*cx,          cz*sx]
        #     [          sy*cz, sy*cx*sz-cy*sx, sy*sx*sz+cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[1][2], dcm[1][1], -dcm[1][0],
                                      dcm[2][0], dcm[0][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xzx':
        #     [                cz,             sz*cx,             sz*sx]
        #     [           -cx2*sz,  cx2*cz*cx-sx2*sx,  cx2*cz*sx+sx2*cx]
        #     [            sx2*sz, -sx2*cz*cx-cx2*sx, -sx2*cz*sx+cx2*cx]
        [r1, r2, r3] = two_axis_rot(dcm[0][2], dcm[0][1], dcm[0][0],
                                    dcm[2][0], -dcm[1][0])
        return np.array([r1, r2, r3])
    else:
        return False","two_axis_rot(dcm[1][0], dcm[1][2], dcm[1][1], dcm[0][1], -dcm[2][1])","two_axis_rot(*dcm[1][::2][::-1] + [dcm[1][1]], dcm[0][1], -dcm[2][1])","iterable_zj[0], iterable_zj[2], iterable_zj[1]",*dcm[1][::2][::-1] + [dcm[1][1]],*dcm[1][:4:2],0
gnss-ins-sim,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gnss-ins-sim/gnss_ins_sim/attitude/attitude.py,https://github.com/Aceinna/gnss-ins-sim/tree/master/gnss_ins_sim/attitude/attitude.py,,dcm2euler$496,"def dcm2euler(dcm, rot_seq='zyx'):
    """"""
    Convert direction cosine matrix to Euler angles.
    The Euler angles rotate the frame n to the frame b according to specified
    rotation sequency. The DCM is a 3x3 coordinate transformation matrix from n
    to b. That is v_b  = DCM * v_n. '_b' or '_n' mean the vector 'v' is expressed
    in the frame b or n.
    Args:
        dcm: 3x3 coordinate transformation matrix from n to b
        rot_seq: rotation sequence corresponding to the angles.
    Returns:
        angles: 3x1 Euler angles, rad.
    """"""
    if rot_seq == 'zyx':
        #     [          cy*cz,          cy*sz,            -sy]
        #     [ sy*sx*cz-sz*cx, sy*sx*sz+cz*cx,          cy*sx]
        #     [ sy*cx*cz+sz*sx, sy*cx*sz-cz*sx,          cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[0][1], dcm[0][0], -dcm[0][2],
                                      dcm[1][2], dcm[2][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zyz':
        #     [  cz2*cy*cz-sz2*sz,  cz2*cy*sz+sz2*cz,           -cz2*sy]
        #     [ -sz2*cy*cz-cz2*sz, -sz2*cy*sz+cz2*cz,            sz2*sy]
        #     [             sy*cz,             sy*sz,                cy]
        [r1, r2, r3] = two_axis_rot(dcm[2][1], dcm[2][0], dcm[2][2],
                                    dcm[1][2], -dcm[0][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zxy':
        #     [ cy*cz-sy*sx*sz, cy*sz+sy*sx*cz,         -sy*cx]
        #     [         -sz*cx,          cz*cx,             sx]
        #     [ sy*cz+cy*sx*sz, sy*sz-cy*sx*cz,          cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[1][0], dcm[1][1], dcm[1][2],
                                      -dcm[0][2], dcm[2][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'zxz':
        #     [  cz2*cz-sz2*cx*sz,  cz2*sz+sz2*cx*cz,            sz2*sx]
        #     [ -sz2*cz-cz2*cx*sz, -sz2*sz+cz2*cx*cz,            cz2*sx]
        #     [             sz*sx,            -cz*sx,                cx]
        [r1, r2, r3] = two_axis_rot(dcm[2][0], -dcm[2][1], dcm[2][2],
                                    dcm[0][2], dcm[1][2])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yxz':
        #     [  cy*cz+sy*sx*sz,           sz*cx, -sy*cz+cy*sx*sz]
        #     [ -cy*sz+sy*sx*cz,           cz*cx,  sy*sz+cy*sx*cz]
        #     [           sy*cx,             -sx,           cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[2][0], dcm[2][2], -dcm[2][1],
                                      dcm[0][1], dcm[1][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yxy':
        #     [  cy2*cy-sy2*cx*sy,            sy2*sx, -cy2*sy-sy2*cx*cy]
        #     [             sy*sx,                cx,             cy*sx]
        #     [  sy2*cy+cy2*cx*sy,           -cy2*sx, -sy2*sy+cy2*cx*cy]
        [r1, r2, r3] = two_axis_rot(dcm[1][0], dcm[1][2], dcm[1][1],
                                    dcm[0][1], -dcm[2][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yzx':
        #     [           cy*cz,              sz,          -sy*cz]
        #     [ -sz*cx*cy+sy*sx,           cz*cx,  sy*cx*sz+cy*sx]
        #     [  cy*sx*sz+sy*cx,          -cz*sx, -sy*sx*sz+cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[0][2], dcm[0][0], dcm[0][1],
                                      -dcm[2][1], dcm[1][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'yzy':
        #     [  cy2*cz*cy-sy2*sy,            cy2*sz, -cy2*cz*sy-sy2*cy]
        #     [            -cy*sz,                cz,             sy*sz]
        #     [  sy2*cz*cy+cy2*sy,            sy2*sz, -sy2*cz*sy+cy2*cy]
        [r1, r2, r3] = two_axis_rot(dcm[1][2], -dcm[1][0], dcm[1][1],
                                    dcm[2][1], dcm[0][1])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xyz':
        #     [          cy*cz, sz*cx+sy*sx*cz, sz*sx-sy*cx*cz]
        #     [         -cy*sz, cz*cx-sy*sx*sz, cz*sx+sy*cx*sz]
        #     [             sy,         -cy*sx,          cy*cx]
        [r1, r2, r3] = three_axis_rot(-dcm[2][1], dcm[2][2], dcm[2][0],
                                      -dcm[1][0], dcm[0][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xyx':
        #     [                cy,             sy*sx,            -sy*cx]
        #     [            sx2*sy,  cx2*cx-sx2*cy*sx,  cx2*sx+sx2*cy*cx]
        #     [            cx2*sy, -sx2*cx-cx2*cy*sx, -sx2*sx+cx2*cy*cx]
        [r1, r2, r3] = two_axis_rot(dcm[0][1], -dcm[0][2], dcm[0][0],
                                    dcm[1][0], dcm[2][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xzy':
        #     [          cy*cz, sz*cx*cy+sy*sx, cy*sx*sz-sy*cx]
        #     [            -sz,          cz*cx,          cz*sx]
        #     [          sy*cz, sy*cx*sz-cy*sx, sy*sx*sz+cy*cx]
        [r1, r2, r3] = three_axis_rot(dcm[1][2], dcm[1][1], -dcm[1][0],
                                      dcm[2][0], dcm[0][0])
        return np.array([r1, r2, r3])
    elif rot_seq == 'xzx':
        #     [                cz,             sz*cx,             sz*sx]
        #     [           -cx2*sz,  cx2*cz*cx-sx2*sx,  cx2*cz*sx+sx2*cx]
        #     [            sx2*sz, -sx2*cz*cx-cx2*sx, -sx2*cz*sx+cx2*cx]
        [r1, r2, r3] = two_axis_rot(dcm[0][2], dcm[0][1], dcm[0][0],
                                    dcm[2][0], -dcm[1][0])
        return np.array([r1, r2, r3])
    else:
        return False","three_axis_rot(-dcm[0][2], dcm[0][0], dcm[0][1], -dcm[2][1], dcm[1][1])","three_axis_rot(-dcm[0][2], *dcm[0][:2], -dcm[2][1], dcm[1][1])","iterable_zj[0], iterable_zj[1]",*dcm[0][:2],*dcm[0][:2],1
arcade,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/arcade/arcade/sprite.py,https://github.com/pythonarcade/arcade/tree/master/arcade/sprite.py,Sprite,get_adjusted_hit_box$400,"def get_adjusted_hit_box(self) -> PointList:
        """"""
        Get the points that make up the hit box for the rect that makes up the
        sprite, including rotation and scaling.
        """"""

        # If we've already calculated the adjusted hit box, use the cached version
        if self._point_list_cache is not None:
            return self._point_list_cache

        def _adjust_point(point):

            # Rotate the point
            if self._angle:
                point = rotate_point(point[0], point[1], 0, 0, self._angle)

            # Get a copy of the point
            point = [point[0] * self._scale + self.position[0], point[1] * self._scale + self.position[1]]

            return point

        point_list = [_adjust_point(point) for point in self.hit_box]

        # Cache the results
        self._point_list_cache = point_list

        return self._point_list_cache","rotate_point(point[0], point[1], 0, 0, self._angle)","rotate_point(*point[:2], 0, 0, self._angle)","iterable_zj[0], iterable_zj[1]",*point[:2],*point[:2],1
PaddleX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/paddlex/ppdet/modeling/necks/blazeface_fpn.py,https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex/ppdet/modeling/necks/blazeface_fpn.py,BlazeNeck,__init__$170,"def __init__(self, in_channel, neck_type=""None"", data_format='NCHW'):
        super(BlazeNeck, self).__init__()
        self.neck_type = neck_type
        self.reture_input = False
        self._out_channels = in_channel
        if self.neck_type == 'None':
            self.reture_input = True
        if ""fpn"" in self.neck_type:
            self.fpn = FPN(self._out_channels[0],
                           self._out_channels[1],
                           name='fpn')
            self._out_channels = [
                self._out_channels[0] // 2, self._out_channels[1] // 2
            ]
        if ""ssh"" in self.neck_type:
            self.ssh1 = SSH(self._out_channels[0],
                            self._out_channels[0],
                            name='ssh1')
            self.ssh2 = SSH(self._out_channels[1],
                            self._out_channels[1],
                            name='ssh2')
            self._out_channels = [self._out_channels[0], self._out_channels[1]]","FPN(self._out_channels[0], self._out_channels[1], name='fpn')","FPN(*self._out_channels[:2], name='fpn')","iterable_zj[0], iterable_zj[1]",*self._out_channels[:2],*self._out_channels[:2],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_cx_cancellation.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_cx_cancellation.py,TestCXCancellation,test_pass_cx_cancellation_intermixed_ops$51,"def test_pass_cx_cancellation_intermixed_ops(self):
        """"""Cancellation shouldn't be affected by the order of ops on different qubits.""""""
        qr = QuantumRegister(4)
        circuit = QuantumCircuit(qr)
        circuit.h(qr[0])
        circuit.h(qr[1])
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])

        pass_manager = PassManager()
        pass_manager.append(CXCancellation())
        out_circuit = pass_manager.run(circuit)

        expected = QuantumCircuit(qr)
        expected.h(qr[0])
        expected.h(qr[1])

        self.assertEqual(out_circuit, expected)","circuit.cx(qr[0], qr[1])",circuit.cx(*qr[:2]),"iterable_zj[0], iterable_zj[1]",*qr[:2],*qr[:2],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_cx_cancellation.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_cx_cancellation.py,TestCXCancellation,test_pass_cx_cancellation_intermixed_ops$51,"def test_pass_cx_cancellation_intermixed_ops(self):
        """"""Cancellation shouldn't be affected by the order of ops on different qubits.""""""
        qr = QuantumRegister(4)
        circuit = QuantumCircuit(qr)
        circuit.h(qr[0])
        circuit.h(qr[1])
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])

        pass_manager = PassManager()
        pass_manager.append(CXCancellation())
        out_circuit = pass_manager.run(circuit)

        expected = QuantumCircuit(qr)
        expected.h(qr[0])
        expected.h(qr[1])

        self.assertEqual(out_circuit, expected)","circuit.cx(qr[2], qr[3])",circuit.cx(*qr[2:4]),"iterable_zj[2], iterable_zj[3]",*qr[2:4],*qr[2:4],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_cx_cancellation.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_cx_cancellation.py,TestCXCancellation,test_pass_cx_cancellation_intermixed_ops$51,"def test_pass_cx_cancellation_intermixed_ops(self):
        """"""Cancellation shouldn't be affected by the order of ops on different qubits.""""""
        qr = QuantumRegister(4)
        circuit = QuantumCircuit(qr)
        circuit.h(qr[0])
        circuit.h(qr[1])
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])

        pass_manager = PassManager()
        pass_manager.append(CXCancellation())
        out_circuit = pass_manager.run(circuit)

        expected = QuantumCircuit(qr)
        expected.h(qr[0])
        expected.h(qr[1])

        self.assertEqual(out_circuit, expected)","circuit.cx(qr[0], qr[1])",circuit.cx(*qr[:2]),"iterable_zj[0], iterable_zj[1]",*qr[:2],*qr[:2],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_cx_cancellation.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_cx_cancellation.py,TestCXCancellation,test_pass_cx_cancellation_intermixed_ops$51,"def test_pass_cx_cancellation_intermixed_ops(self):
        """"""Cancellation shouldn't be affected by the order of ops on different qubits.""""""
        qr = QuantumRegister(4)
        circuit = QuantumCircuit(qr)
        circuit.h(qr[0])
        circuit.h(qr[1])
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])

        pass_manager = PassManager()
        pass_manager.append(CXCancellation())
        out_circuit = pass_manager.run(circuit)

        expected = QuantumCircuit(qr)
        expected.h(qr[0])
        expected.h(qr[1])

        self.assertEqual(out_circuit, expected)","circuit.cx(qr[2], qr[3])",circuit.cx(*qr[2:4]),"iterable_zj[2], iterable_zj[3]",*qr[2:4],*qr[2:4],1
NeuroKit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NeuroKit/neurokit2/signal/signal_power.py,https://github.com/neuropsychology/NeuroKit/tree/master/neurokit2/signal/signal_power.py,,_signal_power_continuous_get$224,"def _signal_power_continuous_get(signal, frequency_band, sampling_rate=1000, precision=20):

    try:
        import mne
    except ImportError:
        raise ImportError(
            ""NeuroKit error: signal_power(): the 'mne'"",
            ""module is required. "",
            ""Please install it first (`pip install mne`)."",
        )

    out = mne.time_frequency.tfr_array_morlet(
        [[signal]],
        sfreq=sampling_rate,
        freqs=np.linspace(frequency_band[0], frequency_band[1], precision),
        output=""power"",
    )
    power = np.mean(out[0][0], axis=0)

    out = {}
    out[""{:.2f}-{:.2f}Hz"".format(frequency_band[0], frequency_band[1])] = power
    return out","'{:.2f}-{:.2f}Hz'.format(frequency_band[0], frequency_band[1])",'{:.2f}-{:.2f}Hz'.format(*frequency_band[:2]),"iterable_zj[0], iterable_zj[1]",*frequency_band[:2],*frequency_band[:2],1
NeuroKit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NeuroKit/neurokit2/signal/signal_power.py,https://github.com/neuropsychology/NeuroKit/tree/master/neurokit2/signal/signal_power.py,,_signal_power_continuous_get$224,"def _signal_power_continuous_get(signal, frequency_band, sampling_rate=1000, precision=20):

    try:
        import mne
    except ImportError:
        raise ImportError(
            ""NeuroKit error: signal_power(): the 'mne'"",
            ""module is required. "",
            ""Please install it first (`pip install mne`)."",
        )

    out = mne.time_frequency.tfr_array_morlet(
        [[signal]],
        sfreq=sampling_rate,
        freqs=np.linspace(frequency_band[0], frequency_band[1], precision),
        output=""power"",
    )
    power = np.mean(out[0][0], axis=0)

    out = {}
    out[""{:.2f}-{:.2f}Hz"".format(frequency_band[0], frequency_band[1])] = power
    return out","np.linspace(frequency_band[0], frequency_band[1], precision)","np.linspace(*frequency_band[:2], precision)","iterable_zj[0], iterable_zj[1]",*frequency_band[:2],*frequency_band[:2],1
CenterMask,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CenterMask/maskrcnn_benchmark/layers/roi_align.py,https://github.com/youngwanLEE/CenterMask/tree/master/maskrcnn_benchmark/layers/roi_align.py,_ROIAlign,backward$26,"def backward(ctx, grad_output):
        rois, = ctx.saved_tensors
        output_size = ctx.output_size
        spatial_scale = ctx.spatial_scale
        sampling_ratio = ctx.sampling_ratio
        bs, ch, h, w = ctx.input_shape
        grad_input = _C.roi_align_backward(
            grad_output,
            rois,
            spatial_scale,
            output_size[0],
            output_size[1],
            bs,
            ch,
            h,
            w,
            sampling_ratio,
        )
        return grad_input, None, None, None, None","_C.roi_align_backward(grad_output, rois, spatial_scale, output_size[0], output_size[1], bs, ch, h, w, sampling_ratio)","_C.roi_align_backward(grad_output, rois, spatial_scale, *output_size[:2], bs, ch, h, w, sampling_ratio)","iterable_zj[0], iterable_zj[1]",*output_size[:2],*output_size[:2],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/dagcircuit/test_dagdependency.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/dagcircuit/test_dagdependency.py,TestDagProperties,setUp$280,"def setUp(self):
        #                       
        # q0_0:  H  X 
        #                            
        # q0_1:  H 
        #                           
        # q0_2:  H  T 
        #           
        # q0_3:  X  U(0,0.1,0.2) 
        #         
        # q1_0: 
        #                              
        # q1_1: 
        super().setUp()
        qr1 = QuantumRegister(4)
        qr2 = QuantumRegister(2)
        circ = QuantumCircuit(qr1, qr2)
        circ.h(qr1[0])
        circ.cx(qr1[2], qr1[3])
        circ.h(qr1[2])
        circ.t(qr1[2])
        circ.ch(qr1[2], qr1[1])
        circ.u(0.0, 0.1, 0.2, qr1[3])
        circ.ccx(qr2[0], qr2[1], qr1[0])

        self.dag = circuit_to_dagdependency(circ)","circ.cx(qr1[2], qr1[3])",circ.cx(*qr1[2:4]),"iterable_zj[2], iterable_zj[3]",*qr1[2:4],*qr1[2:4],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/dagcircuit/test_dagdependency.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/dagcircuit/test_dagdependency.py,TestDagProperties,setUp$280,"def setUp(self):
        #                       
        # q0_0:  H  X 
        #                            
        # q0_1:  H 
        #                           
        # q0_2:  H  T 
        #           
        # q0_3:  X  U(0,0.1,0.2) 
        #         
        # q1_0: 
        #                              
        # q1_1: 
        super().setUp()
        qr1 = QuantumRegister(4)
        qr2 = QuantumRegister(2)
        circ = QuantumCircuit(qr1, qr2)
        circ.h(qr1[0])
        circ.cx(qr1[2], qr1[3])
        circ.h(qr1[2])
        circ.t(qr1[2])
        circ.ch(qr1[2], qr1[1])
        circ.u(0.0, 0.1, 0.2, qr1[3])
        circ.ccx(qr2[0], qr2[1], qr1[0])

        self.dag = circuit_to_dagdependency(circ)","circ.ccx(qr2[0], qr2[1], qr1[0])","circ.ccx(*qr2[:2], qr1[0])","iterable_zj[0], iterable_zj[1]",*qr2[:2],*qr2[:2],1
imgaug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgaug/imgaug/parameters.py,https://github.com/aleju/imgaug/tree/master/imgaug/parameters.py,,handle_probability_param$431,"def handle_probability_param(param, name, tuple_to_uniform=False,
                             list_to_choice=False, prefetch=True):
    eps = 1e-6

    result = None

    if param in [True, False, 0, 1]:
        result = Deterministic(int(param))
    elif ia.is_single_number(param):
        assert 0.0 <= param <= 1.0, (
            ""Expected probability of parameter '%s' to be in the interval ""
            ""[0.0, 1.0], got %.4f."" % (name, param,))
        if 0.0-eps < param < 0.0+eps or 1.0-eps < param < 1.0+eps:
            return Deterministic(int(np.round(param)))
        result = Binomial(param)
    elif tuple_to_uniform and isinstance(param, tuple):
        assert all([ia.is_single_number(v) for v in param]), (
            ""Expected parameter '%s' of type tuple to only contain numbers, ""
            ""got %s."" % (name, [type(v) for v in param],))
        assert len(param) == 2, (
            ""Expected parameter '%s' of type tuple to contain exactly 2 ""
            ""entries, got %d."" % (name, len(param)))
        assert 0 <= param[0] <= 1.0 and 0 <= param[1] <= 1.0, (
            ""Expected parameter '%s' of type tuple to contain two ""
            ""probabilities in the interval [0.0, 1.0]. ""
            ""Got values %.4f and %.4f."" % (name, param[0], param[1]))
        result = Binomial(Uniform(param[0], param[1]))
    elif list_to_choice and ia.is_iterable(param):
        assert all([ia.is_single_number(v) for v in param]), (
            ""Expected iterable parameter '%s' to only contain numbers, ""
            ""got %s."" % (name, [type(v) for v in param],))
        assert all([0 <= p_i <= 1.0 for p_i in param]), (
            ""Expected iterable parameter '%s' to only contain probabilities ""
            ""in the interval [0.0, 1.0], got values %s."" % (
                name, "", "".join([""%.4f"" % (p_i,) for p_i in param])))
        result = Binomial(Choice(param))
    elif isinstance(param, StochasticParameter):
        result = param

    if result is not None:
        if prefetch:
            return _wrap_leafs_of_param_in_prefetchers(result, _NB_PREFETCH)
        return result

    raise Exception(
        ""Expected boolean or number or StochasticParameter for %s, ""
        ""got %s."" % (name, type(param),))","Uniform(param[0], param[1])",Uniform(*param[:2]),"iterable_zj[0], iterable_zj[1]",*param[:2],*param[:2],1
pygmt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmt/pygmt/tests/test_contour.py,https://github.com/GenericMappingTools/pygmt/tree/master/pygmt/tests/test_contour.py,,test_contour_deprecate_columns_to_incols$79,"def test_contour_deprecate_columns_to_incols(region):
    """"""
    Make sure that the old parameter ""columns"" is supported and it reports an
    warning.

    Modified from the test_contour_vec() test.
    """"""
    fig = Figure()
    x, y = np.meshgrid(
        np.linspace(region[0], region[1]), np.linspace(region[2], region[3])
    )
    x = x.flatten()
    y = y.flatten()
    z = (x - 0.5 * (region[0] + region[1])) ** 2 + 4 * y ** 2
    z = np.exp(-z / 10 ** 2 * np.log(2))

    # generate dataframe
    # switch x and y from here onwards to simulate different column order
    data = np.array([y, x, z]).T

    with pytest.warns(expected_warning=FutureWarning) as record:
        fig.contour(
            data,
            projection=""X10c"",
            region=region,
            frame=""a"",
            pen=True,
            columns=[1, 0, 2],
        )
        assert len(record) == 1  # check that only one warning was raised
    return fig","np.linspace(region[0], region[1])",np.linspace(*region[:2]),"iterable_zj[0], iterable_zj[1]",*region[:2],*region[:2],1
pygmt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmt/pygmt/tests/test_contour.py,https://github.com/GenericMappingTools/pygmt/tree/master/pygmt/tests/test_contour.py,,test_contour_deprecate_columns_to_incols$79,"def test_contour_deprecate_columns_to_incols(region):
    """"""
    Make sure that the old parameter ""columns"" is supported and it reports an
    warning.

    Modified from the test_contour_vec() test.
    """"""
    fig = Figure()
    x, y = np.meshgrid(
        np.linspace(region[0], region[1]), np.linspace(region[2], region[3])
    )
    x = x.flatten()
    y = y.flatten()
    z = (x - 0.5 * (region[0] + region[1])) ** 2 + 4 * y ** 2
    z = np.exp(-z / 10 ** 2 * np.log(2))

    # generate dataframe
    # switch x and y from here onwards to simulate different column order
    data = np.array([y, x, z]).T

    with pytest.warns(expected_warning=FutureWarning) as record:
        fig.contour(
            data,
            projection=""X10c"",
            region=region,
            frame=""a"",
            pen=True,
            columns=[1, 0, 2],
        )
        assert len(record) == 1  # check that only one warning was raised
    return fig","np.linspace(region[2], region[3])",np.linspace(*region[2:4]),"iterable_zj[2], iterable_zj[3]",*region[2:4],*region[2:4],1
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/espnet/vc/pytorch_backend/vc.py,https://github.com/espnet/espnet/tree/master/espnet/vc/pytorch_backend/vc.py,,decode$564,"def decode(args):
    """"""Decode with E2E VC model.""""""
    set_deterministic_pytorch(args)
    # read training config
    idim, odim, train_args = get_model_conf(args.model, args.model_conf)

    # show arguments
    for key in sorted(vars(args).keys()):
        logging.info(""args: "" + key + "": "" + str(vars(args)[key]))

    # define model
    model_class = dynamic_import(train_args.model_module)
    model = model_class(idim, odim, train_args)
    assert isinstance(model, TTSInterface)
    logging.info(model)

    # load trained model parameters
    logging.info(""reading model parameters from "" + args.model)
    torch_load(args.model, model)
    model.eval()

    # set torch device
    device = torch.device(""cuda"" if args.ngpu > 0 else ""cpu"")
    model = model.to(device)

    # read json data
    with open(args.json, ""rb"") as f:
        js = json.load(f)[""utts""]

    # check directory
    outdir = os.path.dirname(args.out)
    if len(outdir) != 0 and not os.path.exists(outdir):
        os.makedirs(outdir)

    load_inputs_and_targets = LoadInputsAndTargets(
        mode=""vc"",
        load_output=False,
        sort_in_input_length=False,
        use_speaker_embedding=train_args.use_speaker_embedding,
        preprocess_conf=train_args.preprocess_conf
        if args.preprocess_conf is None
        else args.preprocess_conf,
        preprocess_args={""train"": False},  # Switch the mode of preprocessing
    )

    # define function for plot prob and att_ws
    def _plot_and_save(array, figname, figsize=(6, 4), dpi=150):
        import matplotlib

        matplotlib.use(""Agg"")
        import matplotlib.pyplot as plt

        shape = array.shape
        if len(shape) == 1:
            # for eos probability
            plt.figure(figsize=figsize, dpi=dpi)
            plt.plot(array)
            plt.xlabel(""Frame"")
            plt.ylabel(""Probability"")
            plt.ylim([0, 1])
        elif len(shape) == 2:
            # for tacotron 2 attention weights, whose shape is (out_length, in_length)
            plt.figure(figsize=figsize, dpi=dpi)
            plt.imshow(array, aspect=""auto"")
            plt.xlabel(""Input"")
            plt.ylabel(""Output"")
        elif len(shape) == 4:
            # for transformer attention weights,
            # whose shape is (#leyers, #heads, out_length, in_length)
            plt.figure(figsize=(figsize[0] * shape[0], figsize[1] * shape[1]), dpi=dpi)
            for idx1, xs in enumerate(array):
                for idx2, x in enumerate(xs, 1):
                    plt.subplot(shape[0], shape[1], idx1 * shape[1] + idx2)
                    plt.imshow(x, aspect=""auto"")
                    plt.xlabel(""Input"")
                    plt.ylabel(""Output"")
        else:
            raise NotImplementedError(""Support only from 1D to 4D array."")
        plt.tight_layout()
        if not os.path.exists(os.path.dirname(figname)):
            # NOTE: exist_ok = True is needed for parallel process decoding
            os.makedirs(os.path.dirname(figname), exist_ok=True)
        plt.savefig(figname)
        plt.close()

    # define function to calculate focus rate
    # (see section 3.3 in https://arxiv.org/abs/1905.09263)
    def _calculate_focus_rete(att_ws):
        if att_ws is None:
            # fastspeech case -> None
            return 1.0
        elif len(att_ws.shape) == 2:
            # tacotron 2 case -> (L, T)
            return float(att_ws.max(dim=-1)[0].mean())
        elif len(att_ws.shape) == 4:
            # transformer case -> (#layers, #heads, L, T)
            return float(att_ws.max(dim=-1)[0].mean(dim=-1).max())
        else:
            raise ValueError(""att_ws should be 2 or 4 dimensional tensor."")

    # define function to convert attention to duration
    def _convert_att_to_duration(att_ws):
        if len(att_ws.shape) == 2:
            # tacotron 2 case -> (L, T)
            pass
        elif len(att_ws.shape) == 4:
            # transformer case -> (#layers, #heads, L, T)
            # get the most diagonal head according to focus rate
            att_ws = torch.cat(
                [att_w for att_w in att_ws], dim=0
            )  # (#heads * #layers, L, T)
            diagonal_scores = att_ws.max(dim=-1)[0].mean(dim=-1)  # (#heads * #layers,)
            diagonal_head_idx = diagonal_scores.argmax()
            att_ws = att_ws[diagonal_head_idx]  # (L, T)
        else:
            raise ValueError(""att_ws should be 2 or 4 dimensional tensor."")
        # calculate duration from 2d attention weight
        durations = torch.stack(
            [att_ws.argmax(-1).eq(i).sum() for i in range(att_ws.shape[1])]
        )
        return durations.view(-1, 1).float()

    # define writer instances
    feat_writer = kaldiio.WriteHelper(""ark,scp:{o}.ark,{o}.scp"".format(o=args.out))
    if args.save_durations:
        dur_writer = kaldiio.WriteHelper(
            ""ark,scp:{o}.ark,{o}.scp"".format(o=args.out.replace(""feats"", ""durations""))
        )
    if args.save_focus_rates:
        fr_writer = kaldiio.WriteHelper(
            ""ark,scp:{o}.ark,{o}.scp"".format(o=args.out.replace(""feats"", ""focus_rates""))
        )

    # start decoding
    for idx, utt_id in enumerate(js.keys()):
        # setup inputs
        batch = [(utt_id, js[utt_id])]
        data = load_inputs_and_targets(batch)
        x = torch.FloatTensor(data[0][0]).to(device)
        spemb = None
        if train_args.use_speaker_embedding:
            spemb = torch.FloatTensor(data[1][0]).to(device)

        # decode and write
        start_time = time.time()
        outs, probs, att_ws = model.inference(x, args, spemb=spemb)
        logging.info(
            ""inference speed = %.1f frames / sec.""
            % (int(outs.size(0)) / (time.time() - start_time))
        )
        if outs.size(0) == x.size(0) * args.maxlenratio:
            logging.warning(""output length reaches maximum length (%s)."" % utt_id)
        focus_rate = _calculate_focus_rete(att_ws)
        logging.info(
            ""(%d/%d) %s (size: %d->%d, focus rate: %.3f)""
            % (idx + 1, len(js.keys()), utt_id, x.size(0), outs.size(0), focus_rate)
        )
        feat_writer[utt_id] = outs.cpu().numpy()
        if args.save_durations:
            ds = _convert_att_to_duration(att_ws)
            dur_writer[utt_id] = ds.cpu().numpy()
        if args.save_focus_rates:
            fr_writer[utt_id] = np.array(focus_rate).reshape(1, 1)

        # plot and save prob and att_ws
        if probs is not None:
            _plot_and_save(
                probs.cpu().numpy(),
                os.path.dirname(args.out) + ""/probs/%s_prob.png"" % utt_id,
            )
        if att_ws is not None:
            _plot_and_save(
                att_ws.cpu().numpy(),
                os.path.dirname(args.out) + ""/att_ws/%s_att_ws.png"" % utt_id,
            )

    # close file object
    feat_writer.close()
    if args.save_durations:
        dur_writer.close()
    if args.save_focus_rates:
        fr_writer.close()","plt.subplot(shape[0], shape[1], idx1 * shape[1] + idx2)","plt.subplot(*shape[:2], idx1 * shape[1] + idx2)","iterable_zj[0], iterable_zj[1]",*shape[:2],*shape[:2],1
torch-points3d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torch-points3d/torch_points3d/models/registration/pointnet2.py,https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/models/registration/pointnet2.py,PatchPointNet2_D,set_last_mlp$35,"def set_last_mlp(self, last_mlp_opt):

        if len(last_mlp_opt.nn) > 2:
            self.FC_layer = MLP(last_mlp_opt.nn[: len(last_mlp_opt.nn) - 1])
            self.FC_layer.add_module(""last"", Lin(last_mlp_opt.nn[-2], last_mlp_opt.nn[-1]))
        elif len(last_mlp_opt.nn) == 2:
            self.FC_layer = Sequential(Lin(last_mlp_opt.nn[-2], last_mlp_opt.nn[-1]))
        else:
            self.FC_layer = torch.nn.Identity()","Lin(last_mlp_opt.nn[-2], last_mlp_opt.nn[-1])",Lin(*last_mlp_opt.nn[-2:]),"iterable_zj[-2], iterable_zj[-1]",*last_mlp_opt.nn[-2:],*last_mlp_opt.nn[-2:0],0
torch-points3d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torch-points3d/torch_points3d/models/registration/pointnet2.py,https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/models/registration/pointnet2.py,PatchPointNet2_D,set_last_mlp$35,"def set_last_mlp(self, last_mlp_opt):

        if len(last_mlp_opt.nn) > 2:
            self.FC_layer = MLP(last_mlp_opt.nn[: len(last_mlp_opt.nn) - 1])
            self.FC_layer.add_module(""last"", Lin(last_mlp_opt.nn[-2], last_mlp_opt.nn[-1]))
        elif len(last_mlp_opt.nn) == 2:
            self.FC_layer = Sequential(Lin(last_mlp_opt.nn[-2], last_mlp_opt.nn[-1]))
        else:
            self.FC_layer = torch.nn.Identity()","Lin(last_mlp_opt.nn[-2], last_mlp_opt.nn[-1])",Lin(*last_mlp_opt.nn[-2:]),"iterable_zj[-2], iterable_zj[-1]",*last_mlp_opt.nn[-2:],*last_mlp_opt.nn[-2:0],0
neutron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/tests/unit/objects/test_network_segment_range.py,https://github.com/openstack/neutron/tree/master/neutron/tests/unit/objects/test_network_segment_range.py,NetworkSegmentRangeDbObjectTestCase,_create_environment$199,"def _create_environment(self, default_range=True):
        self.projects = [uuidutils.generate_uuid() for _ in range(3)]
        self.segment_ranges = {
            'default': [100, 120], self.projects[0]: [90, 105],
            self.projects[1]: [109, 114], self.projects[2]: [117, 130]}
        self.seg_min = self.segment_ranges['default'][0]
        self.seg_max = self.segment_ranges['default'][1]

        for subclass in ml2_base.SegmentAllocation.__subclasses__():
            # Build segment ranges: default one and project specific ones.
            for name, ranges in self.segment_ranges.items():
                default = True if name == 'default' else False
                project = name if not default else None
                if default and not default_range:
                    continue

                self._create_network_segment_range(
                    ranges[0], ranges[1], network_type=subclass.network_type,
                    project_id=project, default=default,
                    shared=default).create()

            # Build allocations (non allocated).
            for segmentation_id in range(self.seg_min, self.seg_max + 1):
                self._create_allocation(subclass,
                                        segmentation_id=segmentation_id)","self._create_network_segment_range(ranges[0], ranges[1], network_type=subclass.network_type, project_id=project, default=default, shared=default)","self._create_network_segment_range(*ranges[:2], network_type=subclass.network_type, project_id=project, default=default, shared=default)","iterable_zj[0], iterable_zj[1]",*ranges[:2],*ranges[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/topi/cuda/searchsorted.py,https://github.com/apache/tvm/tree/master/python/tvm/topi/cuda/searchsorted.py,,searchsorted$25,"def searchsorted(sorted_sequence, values, right, out_dtype=""int64""):
    """"""Find indices where elements should be inserted to maintain order.
       If `sorted_sequence` is N-dimensional, the innermost dimension of
       `values` are searched in the corresponding dimension of `sorted_sequence`.

    Parameters
    ----------
    sorted_sequence : te.Tensor
        N-D or 1-D Tensor, containing monotonically increasing sequence
        on the innermost dimension.

    values : te.Tensor
        N-D Tensor containing the search values. When `sorted_sequence` is 1-D,
        the shape of `values` can be arbitrary. Otherwise, ranks of `sorted_sequence`
        and `values` must be the same, and outer N-1 axes must have the same size.

    right : bool, optional
        Controls which index is returned if a value lands exactly on one of sorted values. If
        False, the index of the first suitable location found is given. If true, return the
        last such index. If there is no suitable index, return either 0 or N (where N is the
        size of the innermost dimension).

    dtype : string, optional
        The data type of the output indices.

    Returns
    -------
    indices : te.Tensor
        Tensor with same shape as values, representing the indices of
        elements of `values` if they are inserted in `sorted_sequence`.
    """"""

    def ir(sorted_sequence, values, indices):
        ib = tvm.tir.ir_builder.create()
        sorted_sequence_shape = sorted_sequence.shape
        values_shape = values.shape
        num_search = utils.prod(values_shape)
        search_range = sorted_sequence_shape[-1]

        sorted_sequence = ib.buffer_ptr(sorted_sequence)
        values = ib.buffer_ptr(values)
        indices = ib.buffer_ptr(indices)

        max_threads = int(tvm.target.Target.current(allow_none=False).max_num_threads)
        bx = te.thread_axis(""blockIdx.x"")
        tx = te.thread_axis(""threadIdx.x"")
        ib.scope_attr(
            bx, ""thread_extent"", tvm.tir.indexdiv(num_search + max_threads - 1, max_threads)
        )
        ib.scope_attr(tx, ""thread_extent"", max_threads)
        tid = bx * max_threads + tx

        with ib.if_scope(tid < num_search):
            if len(sorted_sequence_shape) == 1:
                sequence_offset = 0
            else:
                sequence_id = tid // values_shape[-1]
                sequence_offset = sequence_id * search_range

            indices[tid] = binary_search(
                ib,
                sequence_offset,
                search_range,
                sorted_sequence,
                values[tid],
                right,
                out_dtype,
            )

        return ib.get()

    return te.extern(
        values.shape,
        [sorted_sequence, values],
        lambda ins, outs: ir(ins[0], ins[1], outs[0]),
        name=""searchsorted"",
        dtype=out_dtype,
    )","ir(ins[0], ins[1], outs[0])","ir(*ins[:2], outs[0])","iterable_zj[0], iterable_zj[1]",*ins[:2],*ins[:2],1
ezdxf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/render/mesh.py,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/render/mesh.py,,estimate_face_normals_direction$147,"def estimate_face_normals_direction(
    vertices: Sequence[Vec3], faces: Sequence[Face]
) -> float:
    """"""Returns the estimated face-normals direction as ``float`` value
    in the range [-1.0, 1.0] for a closed surface.

    This heuristic works well for simple convex hulls but struggles with
    more complex structures like a torus (doughnut).

    A counter-clockwise (ccw) vertex arrangement is assumed but a
    clockwise (cw) arrangement works too but the values are reversed.

    The closer the value to 1.0 (-1.0 for cw) the more likely all normals
    pointing outwards from the surface.

    The closer the value to -1.0 (1.0 for cw) the more likely all normals
    pointing inwards from the surface.

    """"""
    n_vertices = len(vertices)
    if n_vertices == 0:
        return 0.0

    mesh_centroid = Vec3.sum(vertices) / n_vertices
    count = 0
    direction_sum = 0.0
    for face in faces:
        if len(face) < 3:
            continue
        try:
            face_vertices = tuple(vertices[i] for i in face)
        except IndexError:
            continue
        face_centroid = Vec3.sum(face_vertices) / len(face)
        try:
            face_normal = normal_vector_3p(
                face_vertices[0], face_vertices[1], face_vertices[2]
            )
        except ZeroDivisionError:
            continue
        try:
            outward_vec = (face_centroid - mesh_centroid).normalize()
        except ZeroDivisionError:
            continue
        direction_sum += face_normal.dot(outward_vec)
        count += 1
    if count > 0:
        return direction_sum / count
    return 0.0","normal_vector_3p(face_vertices[0], face_vertices[1], face_vertices[2])",normal_vector_3p(*face_vertices[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*face_vertices[:3],*face_vertices[:3],1
PhiFlow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/torch/_torch_backend.py,https://github.com/tum-pbs/PhiFlow/tree/master/phi/torch/_torch_backend.py,TorchBackend,sparse_tensor$585,"def sparse_tensor(self, indices, values, shape):
        indices_ = self.to_int64(indices)
        values_ = self.to_float(values)
        if not self.is_available(values_):
            # the output of torch.sparse_coo_tensor is considered constant
            @torch.jit.script
            def sparse_coo_tensor(values, indices, cols: int, rows: int, dtype: torch.dtype) -> torch.sparse.Tensor:
                size = torch.Size([cols, rows])
                return torch.sparse_coo_tensor(indices, values, size=size, dtype=dtype)
            result = sparse_coo_tensor(values_, indices_, shape[0], shape[1], to_torch_dtype(self.float_type))
        else:
            result = torch.sparse_coo_tensor(indices_, values_, shape, dtype=to_torch_dtype(self.float_type))
        return result","sparse_coo_tensor(values_, indices_, shape[0], shape[1], to_torch_dtype(self.float_type))","sparse_coo_tensor(values_, indices_, *shape[:2], to_torch_dtype(self.float_type))","iterable_zj[0], iterable_zj[1]",*shape[:2],*shape[:2],1
TorchSeg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TorchSeg/furnace/utils/img_utils.py,https://github.com/ycszen/TorchSeg/tree/master/furnace/utils/img_utils.py,,pad_image_to_shape$60,"def pad_image_to_shape(img, shape, border_mode, value):
    margin = np.zeros(4, np.uint32)
    shape = get_2dshape(shape)
    pad_height = shape[0] - img.shape[0] if shape[0] - img.shape[0] > 0 else 0
    pad_width = shape[1] - img.shape[1] if shape[1] - img.shape[1] > 0 else 0

    margin[0] = pad_height // 2
    margin[1] = pad_height // 2 + pad_height % 2
    margin[2] = pad_width // 2
    margin[3] = pad_width // 2 + pad_width % 2

    img = cv2.copyMakeBorder(img, margin[0], margin[1], margin[2], margin[3],
                             border_mode, value=value)

    return img, margin","cv2.copyMakeBorder(img, margin[0], margin[1], margin[2], margin[3], border_mode, value=value)","cv2.copyMakeBorder(img, *margin[:4], border_mode, value=value)","iterable_zj[0], iterable_zj[1], iterable_zj[2], iterable_zj[3]",*margin[:4],*margin[:4],1
singleshotpose,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/singleshotpose/py2/multi_obj_pose_estimation/darknet_multi.py,https://github.com/microsoft/singleshotpose/tree/master/py2/multi_obj_pose_estimation/darknet_multi.py,Darknet,load_weights$245,"def load_weights(self, weightfile):
        fp = open(weightfile, 'rb')
        header = np.fromfile(fp, count=4, dtype=np.int32)
        self.header = torch.from_numpy(header)
        self.seen = self.header[3]
        buf = np.fromfile(fp, dtype = np.float32)
        fp.close()

        start = 0
        ind = -2
        for block in self.blocks:
            if start >= buf.size:
                break
            ind = ind + 1
            if block['type'] == 'net':
                continue
            elif block['type'] == 'convolutional':
                model = self.models[ind]
                batch_normalize = int(block['batch_normalize'])
                if batch_normalize:
                    start = load_conv_bn(buf, start, model[0], model[1])
                else:
                    start = load_conv(buf, start, model[0])
            elif block['type'] == 'connected':
                model = self.models[ind]
                if block['activation'] != 'linear':
                    start = load_fc(buf, start, model[0])
                else:
                    start = load_fc(buf, start, model)
            elif block['type'] == 'maxpool':
                pass
            elif block['type'] == 'reorg':
                pass
            elif block['type'] == 'route':
                pass
            elif block['type'] == 'shortcut':
                pass
            elif block['type'] == 'region':
                pass
            elif block['type'] == 'avgpool':
                pass
            elif block['type'] == 'softmax':
                pass
            elif block['type'] == 'cost':
                pass
            else:
                print('unknown type %s' % (block['type']))","load_conv_bn(buf, start, model[0], model[1])","load_conv_bn(buf, start, *model[:2])","iterable_zj[0], iterable_zj[1]",*model[:2],*model[:2],1
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[0], p[1])",self.add_line(*p[:2]),"iterable_zj[0], iterable_zj[1]",*p[:2],*p[:2],1
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[0], p[2])",self.add_line(*p[::2]),"iterable_zj[0], iterable_zj[2]",*p[::2],*p[:4:2],0
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[0], p[4])",self.add_line(*p[0:5:4]),"iterable_zj[0], iterable_zj[4]",*p[0:5:4],*p[:8:4],0
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[1], p[3])",Cannot refactor,"iterable_zj[1], iterable_zj[3]",,*p[1:5:2],0
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[1], p[5])",self.add_line(*p[1:6:4]),"iterable_zj[1], iterable_zj[5]",*p[1:6:4],*p[1:9:4],0
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[2], p[3])",self.add_line(*p[2:4]),"iterable_zj[2], iterable_zj[3]",*p[2:4],*p[2:4],1
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[3], p[7])",self.add_line(*p[3:8:4]),"iterable_zj[3], iterable_zj[7]",*p[3:8:4],*p[3:11:4],0
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[4], p[5])",self.add_line(*p[4:6]),"iterable_zj[4], iterable_zj[5]",*p[4:6],*p[4:6],1
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[4], p[6])",self.add_line(*p[4:7:2]),"iterable_zj[4], iterable_zj[6]",*p[4:7:2],*p[4:8:2],0
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[5], p[7])",self.add_line(*p[5:8][1]),"iterable_zj[5], iterable_zj[7]",*p[5:8][1],*p[5:9:2],0
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[6], p[7])",self.add_line(*p[6:8]),"iterable_zj[6], iterable_zj[7]",*p[6:8],*p[6:8],1
LightNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LightNet/modules/residual.py,https://github.com/linksense/LightNet/tree/master/modules/residual.py,IdentityResidualBlock,__init__$8,"def __init__(self, in_channels, channels, stride=1, dilation=1, groups=1, norm_act=ABN, is_se=False, dropout=None):
        """"""Configurable identity-mapping residual block

        Parameters
        ----------
        in_channels : int
            Number of input channels.
        channels : list of int
            Number of channels in the internal feature maps. Can either have two or three elements: if three construct
            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then
            `3 x 3` then `1 x 1` convolutions.
        stride : int
            Stride of the first `3 x 3` convolution
        dilation : int
            Dilation to apply to the `3 x 3` convolutions.
        groups : int
            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with
            bottleneck blocks.
        norm_act : callable
            Function to create normalization / activation Module.
        dropout: callable
            Function to create Dropout Module.
        """"""
        super(IdentityResidualBlock, self).__init__()

        # Check parameters for inconsistencies
        if len(channels) != 2 and len(channels) != 3:
            raise ValueError(""channels must contain either two or three values"")
        if len(channels) == 2 and groups != 1:
            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")

        is_bottleneck = len(channels) == 3
        need_proj_conv = stride != 1 or in_channels != channels[-1]

        self.bn1 = norm_act(in_channels)
        if not is_bottleneck:
            layers = [
                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,
                                    dilation=dilation)),
                (""bn2"", norm_act(channels[0])),
                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                    dilation=dilation))
            ]
            if dropout is not None:
                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]
        else:
            if not is_se:
                layers = [
                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),
                    (""bn2"", norm_act(channels[0])),
                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                        groups=groups, dilation=dilation)),
                    (""bn3"", norm_act(channels[1])),
                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))
                ]
            else:
                layers = [
                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),
                    (""bn2"", norm_act(channels[0])),
                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                        groups=groups, dilation=dilation)),
                    (""bn3"", norm_act(channels[1])),
                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False)),
                    (""se_block"", SEBlock(channels[2], 16))
                ]
            if dropout is not None:
                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]
        self.convs = nn.Sequential(OrderedDict(layers))

        if need_proj_conv:
            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)","nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False)","nn.Conv2d(*channels[1:3], 1, stride=1, padding=0, bias=False)","iterable_zj[1], iterable_zj[2]",*channels[1:3],*channels[1:3],1
LightNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LightNet/modules/residual.py,https://github.com/linksense/LightNet/tree/master/modules/residual.py,IdentityResidualBlock,__init__$8,"def __init__(self, in_channels, channels, stride=1, dilation=1, groups=1, norm_act=ABN, is_se=False, dropout=None):
        """"""Configurable identity-mapping residual block

        Parameters
        ----------
        in_channels : int
            Number of input channels.
        channels : list of int
            Number of channels in the internal feature maps. Can either have two or three elements: if three construct
            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then
            `3 x 3` then `1 x 1` convolutions.
        stride : int
            Stride of the first `3 x 3` convolution
        dilation : int
            Dilation to apply to the `3 x 3` convolutions.
        groups : int
            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with
            bottleneck blocks.
        norm_act : callable
            Function to create normalization / activation Module.
        dropout: callable
            Function to create Dropout Module.
        """"""
        super(IdentityResidualBlock, self).__init__()

        # Check parameters for inconsistencies
        if len(channels) != 2 and len(channels) != 3:
            raise ValueError(""channels must contain either two or three values"")
        if len(channels) == 2 and groups != 1:
            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")

        is_bottleneck = len(channels) == 3
        need_proj_conv = stride != 1 or in_channels != channels[-1]

        self.bn1 = norm_act(in_channels)
        if not is_bottleneck:
            layers = [
                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,
                                    dilation=dilation)),
                (""bn2"", norm_act(channels[0])),
                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                    dilation=dilation))
            ]
            if dropout is not None:
                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]
        else:
            if not is_se:
                layers = [
                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),
                    (""bn2"", norm_act(channels[0])),
                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                        groups=groups, dilation=dilation)),
                    (""bn3"", norm_act(channels[1])),
                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))
                ]
            else:
                layers = [
                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),
                    (""bn2"", norm_act(channels[0])),
                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                        groups=groups, dilation=dilation)),
                    (""bn3"", norm_act(channels[1])),
                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False)),
                    (""se_block"", SEBlock(channels[2], 16))
                ]
            if dropout is not None:
                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]
        self.convs = nn.Sequential(OrderedDict(layers))

        if need_proj_conv:
            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)","nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False)","nn.Conv2d(*channels[1:3], 1, stride=1, padding=0, bias=False)","iterable_zj[1], iterable_zj[2]",*channels[1:3],*channels[1:3],1
n-beats,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/n-beats/examples/rnn_example.py,https://github.com/philipperemy/n-beats/tree/master/examples/rnn_example.py,,univariate_data$33,"def univariate_data(dataset, start_index, end_index, history_size, target_size):
    data = []
    labels = []

    start_index = start_index + history_size
    if end_index is None:
        end_index = len(dataset) - target_size

    for i in range(start_index, end_index):
        indices = range(i - history_size, i)
        # Reshape data from (history_size,) to (history_size, 1)
        data.append(np.reshape(dataset[indices], (history_size, 1)))
        labels.append(dataset[i + target_size])
    data = np.array(data)
    labels = np.array(labels)
    return data.reshape(data.shape[0], data.shape[1], 1), labels.reshape(labels.shape[0], 1, 1)","data.reshape(data.shape[0], data.shape[1], 1)","data.reshape(*data.shape[:2], 1)","iterable_zj[0], iterable_zj[1]",*data.shape[:2],*data.shape[:2],1
TracKit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TracKit/tracking/vot.py,https://github.com/researchmm/TracKit/tree/master/tracking/vot.py,VOT,__init__$28,"def __init__(self, region_format, channels=None):
        """""" Constructor

        Args:
            region_format: Region format options
        """"""
        assert(region_format in [trax.Region.RECTANGLE, trax.Region.POLYGON, trax.Region.MASK])

        if channels is None:
            channels = ['color']
        elif channels == 'rgbd':
            channels = ['color', 'depth']
        elif channels == 'rgbt':
            channels = ['color', 'ir']
        elif channels == 'ir':
            channels = ['ir']
        else:
            raise Exception('Illegal configuration {}.'.format(channels))

        self._trax = trax.Server([region_format], [trax.Image.PATH], channels, customMetadata=dict(vot=""python""))

        request = self._trax.wait()
        assert(request.type == 'initialize')
        if isinstance(request.region, trax.Polygon):
            self._region = Polygon([Point(x[0], x[1]) for x in request.region])
        if isinstance(request.region, trax.Mask):
            self._region = request.region.array(True)
        else:
            self._region = Rectangle(*request.region.bounds())
        self._image = [x.path() for k, x in request.image.items()]
        if len(self._image) == 1:
            self._image = self._image[0]

        self._trax.status(request.region)","Point(x[0], x[1])",Point(*x[:2]),"iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
PhiFlow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/tf/_tf_cuda_resample.py,https://github.com/tum-pbs/PhiFlow/tree/master/phi/tf/_tf_cuda_resample.py,,_resample_gradient$40,"def _resample_gradient(op, gradient):
    gradients = resample_gradient_op.resample_gradient(
        gradient, op.inputs[0], op.inputs[1], op.inputs[2]
    )
    return [gradients[0], gradients[1], None]","resample_gradient_op.resample_gradient(gradient, op.inputs[0], op.inputs[1], op.inputs[2])","resample_gradient_op.resample_gradient(gradient, *op.inputs[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*op.inputs[:3],*op.inputs[:3],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_lookahead_swap.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_lookahead_swap.py,TestLookaheadSwap,test_lookahead_swap_hang_in_min_case$225,"def test_lookahead_swap_hang_in_min_case(self):
        """"""Verify LookaheadSwap does not stall in minimal case.""""""
        # ref: https://github.com/Qiskit/qiskit-terra/issues/2171

        qr = QuantumRegister(14, ""q"")
        qc = QuantumCircuit(qr)
        qc.cx(qr[0], qr[13])
        qc.cx(qr[1], qr[13])
        qc.cx(qr[1], qr[0])
        qc.cx(qr[13], qr[1])
        dag = circuit_to_dag(qc)

        cmap = CouplingMap(FakeMelbourne().configuration().coupling_map)

        out = LookaheadSwap(cmap, search_depth=4, search_width=4).run(dag)

        self.assertIsInstance(out, DAGCircuit)","qc.cx(qr[0], qr[13])","qc.cx(*qr[0], qr[13] can be obtained using the slice operator as qr[0:14:13])","iterable_zj[0], iterable_zj[13]","*qr[0], qr[13] can be obtained using the slice operator as qr[0:14:13]",*qr[:26:13],0
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_lookahead_swap.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_lookahead_swap.py,TestLookaheadSwap,test_lookahead_swap_hang_in_min_case$225,"def test_lookahead_swap_hang_in_min_case(self):
        """"""Verify LookaheadSwap does not stall in minimal case.""""""
        # ref: https://github.com/Qiskit/qiskit-terra/issues/2171

        qr = QuantumRegister(14, ""q"")
        qc = QuantumCircuit(qr)
        qc.cx(qr[0], qr[13])
        qc.cx(qr[1], qr[13])
        qc.cx(qr[1], qr[0])
        qc.cx(qr[13], qr[1])
        dag = circuit_to_dag(qc)

        cmap = CouplingMap(FakeMelbourne().configuration().coupling_map)

        out = LookaheadSwap(cmap, search_depth=4, search_width=4).run(dag)

        self.assertIsInstance(out, DAGCircuit)","qc.cx(qr[1], qr[13])",qc.cx(*qr[1:14:12]),"iterable_zj[1], iterable_zj[13]",*qr[1:14:12],*qr[1:25:12],0
pytracking,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytracking/ltr/actors/tracking.py,https://github.com/visionml/pytracking/tree/master/ltr/actors/tracking.py,KYSActor,__call__$207,"def __call__(self, data):
        sequence_length = data['test_images'].shape[0]
        num_sequences = data['test_images'].shape[1]

        valid_samples = data['test_valid_image'].to(self.device)
        test_visibility = data['test_visible_ratio'].to(self.device)

        # Initialize loss variables
        clf_loss_test_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        clf_loss_test_orig_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        dimp_loss_test_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        test_clf_acc = 0
        dimp_clf_acc = 0

        test_tracked_correct = torch.zeros(num_sequences, sequence_length - 1).long().to(self.device)
        test_seq_all_correct = torch.ones(num_sequences).to(self.device)
        dimp_seq_all_correct = torch.ones(num_sequences).to(self.device)

        is_target_loss_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        is_target_after_prop_loss_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)

        # Initialize target model using the training frames
        train_images = data['train_images'].to(self.device)
        train_anno = data['train_anno'].to(self.device)
        dimp_filters = self.net.train_classifier(train_images, train_anno)

        # Track in the first test frame
        test_image_cur = data['test_images'][0, ...].to(self.device)
        backbone_feat_prev_all = self.net.extract_backbone_features(test_image_cur)
        backbone_feat_prev = backbone_feat_prev_all[self.net.classification_layer]
        backbone_feat_prev = backbone_feat_prev.view(1, num_sequences, -1,
                                                     backbone_feat_prev.shape[-2], backbone_feat_prev.shape[-1])

        if self.net.motion_feat_extractor is not None:
            motion_feat_prev = self.net.motion_feat_extractor(backbone_feat_prev_all).view(1, num_sequences, -1,
                                                                                           backbone_feat_prev.shape[-2],
                                                                                           backbone_feat_prev.shape[-1])
        else:
            motion_feat_prev = backbone_feat_prev

        dimp_scores_prev = self.net.dimp_classifier.track_frame(dimp_filters, backbone_feat_prev)

        # Remove last row and col (added due to even kernel size in the target model)
        dimp_scores_prev = dimp_scores_prev[:, :, :-1, :-1].contiguous()

        # Set previous frame information
        label_prev = data['test_label'][0:1, ...].to(self.device)
        label_prev = label_prev[:, :, :-1, :-1].contiguous()

        anno_prev = data['test_anno'][0:1, ...].to(self.device)
        state_prev = None

        is_valid_prev = valid_samples[0, :].view(1, -1, 1, 1).byte()

        # Loop over the sequence
        for i in range(1, sequence_length):
            test_image_cur = data['test_images'][i, ...].to(self.device)
            test_label_cur = data['test_label'][i:i+1, ...].to(self.device)
            test_label_cur = test_label_cur[:, :, :-1, :-1].contiguous()

            test_anno_cur = data['test_anno'][i:i + 1, ...].to(self.device)

            # Extract features
            backbone_feat_cur_all = self.net.extract_backbone_features(test_image_cur)
            backbone_feat_cur = backbone_feat_cur_all[self.net.classification_layer]
            backbone_feat_cur = backbone_feat_cur.view(1, num_sequences, -1,
                                                       backbone_feat_cur.shape[-2], backbone_feat_cur.shape[-1])

            if self.net.motion_feat_extractor is not None:
                motion_feat_cur = self.net.motion_feat_extractor(backbone_feat_cur_all).view(1, num_sequences, -1,
                                                                                             backbone_feat_cur.shape[-2],
                                                                                             backbone_feat_cur.shape[-1])
            else:
                motion_feat_cur = backbone_feat_cur

            # Run target model
            dimp_scores_cur = self.net.dimp_classifier.track_frame(dimp_filters, backbone_feat_cur)
            dimp_scores_cur = dimp_scores_cur[:, :, :-1, :-1].contiguous()

            # Jitter target model output for augmentation
            jitter_info = None
            if self.dimp_jitter_fn is not None:
                dimp_scores_cur = self.dimp_jitter_fn(dimp_scores_cur, test_label_cur.clone())

            # Input target model output along with previous frame information to the predictor
            predictor_input_data = {'input1': motion_feat_prev, 'input2': motion_feat_cur,
                                    'label_prev': label_prev, 'anno_prev': anno_prev,
                                    'dimp_score_prev': dimp_scores_prev, 'dimp_score_cur': dimp_scores_cur,
                                    'state_prev': state_prev,
                                    'jitter_info': jitter_info}

            predictor_output = self.net.predictor(predictor_input_data)

            predicted_resp = predictor_output['response']
            state_prev = predictor_output['state_cur']
            aux_data = predictor_output['auxiliary_outputs']

            is_valid = valid_samples[i, :].view(1, -1, 1, 1).byte()
            uncertain_frame = (test_visibility[i, :].view(1, -1, 1, 1) < 0.75) * (test_visibility[i, :].view(1, -1, 1, 1) > 0.25)

            is_valid = is_valid * ~uncertain_frame

            # Calculate losses
            clf_loss_test_new = self.objective['test_clf'](predicted_resp, test_label_cur,
                                                           test_anno_cur, valid_samples=is_valid)
            clf_loss_test_all[:, i - 1] = clf_loss_test_new.squeeze()

            dimp_loss_test_new = self.objective['dimp_clf'](dimp_scores_cur, test_label_cur,
                                                            test_anno_cur, valid_samples=is_valid)
            dimp_loss_test_all[:, i - 1] = dimp_loss_test_new.squeeze()

            if 'fused_score_orig' in aux_data and 'test_clf_orig' in self.loss_weight.keys():
                aux_data['fused_score_orig'] = aux_data['fused_score_orig'].view(test_label_cur.shape)
                clf_loss_test_orig_new = self.objective['test_clf'](aux_data['fused_score_orig'], test_label_cur, test_anno_cur,  valid_samples=is_valid)
                clf_loss_test_orig_all[:, i - 1] = clf_loss_test_orig_new.squeeze()

            if 'is_target' in aux_data and 'is_target' in self.loss_weight.keys() and 'is_target' in self.objective.keys():
                is_target_loss_new = self.objective['is_target'](aux_data['is_target'], label_prev, is_valid_prev)
                is_target_loss_all[:, i - 1] = is_target_loss_new

            if 'is_target_after_prop' in aux_data and 'is_target_after_prop' in self.loss_weight.keys() and 'is_target' in self.objective.keys():
                is_target_after_prop_loss_new = self.objective['is_target'](aux_data['is_target_after_prop'],
                                                                            test_label_cur, is_valid)
                is_target_after_prop_loss_all[:, i - 1] = is_target_after_prop_loss_new

            test_clf_acc_new, test_pred_correct = self.objective['clf_acc'](predicted_resp, test_label_cur, valid_samples=is_valid)
            test_clf_acc += test_clf_acc_new

            test_seq_all_correct = test_seq_all_correct * (test_pred_correct.long() | (1 - is_valid).long()).float()
            test_tracked_correct[:, i - 1] = test_pred_correct

            dimp_clf_acc_new, dimp_pred_correct = self.objective['clf_acc'](dimp_scores_cur, test_label_cur, valid_samples=is_valid)
            dimp_clf_acc += dimp_clf_acc_new

            dimp_seq_all_correct = dimp_seq_all_correct * (dimp_pred_correct.long() | (1 - is_valid).long()).float()

            motion_feat_prev = motion_feat_cur.clone()
            dimp_scores_prev = dimp_scores_cur.clone()
            label_prev = test_label_cur.clone()
            is_valid_prev = is_valid.clone()

        # Compute average loss over the sequence
        clf_loss_test = clf_loss_test_all.mean()
        clf_loss_test_orig = clf_loss_test_orig_all.mean()
        dimp_loss_test = dimp_loss_test_all.mean()
        is_target_loss = is_target_loss_all.mean()
        is_target_after_prop_loss = is_target_after_prop_loss_all.mean()

        test_clf_acc /= (sequence_length - 1)
        dimp_clf_acc /= (sequence_length - 1)
        clf_loss_test_orig /= (sequence_length - 1)

        test_seq_clf_acc = test_seq_all_correct.mean()
        dimp_seq_clf_acc = dimp_seq_all_correct.mean()

        clf_loss_test_w = self.loss_weight['test_clf'] * clf_loss_test
        clf_loss_test_orig_w = self.loss_weight['test_clf_orig'] * clf_loss_test_orig
        dimp_loss_test_w = self.loss_weight.get('dimp_clf', 0.0) * dimp_loss_test

        is_target_loss_w = self.loss_weight.get('is_target', 0.0) * is_target_loss
        is_target_after_prop_loss_w = self.loss_weight.get('is_target_after_prop', 0.0) * is_target_after_prop_loss

        loss = clf_loss_test_w + dimp_loss_test_w + is_target_loss_w + is_target_after_prop_loss_w + clf_loss_test_orig_w

        stats = {'Loss/total': loss.item(),
                 'Loss/test_clf': clf_loss_test_w.item(),
                 'Loss/dimp_clf': dimp_loss_test_w.item(),
                 'Loss/raw/test_clf': clf_loss_test.item(),
                 'Loss/raw/test_clf_orig': clf_loss_test_orig.item(),
                 'Loss/raw/dimp_clf': dimp_loss_test.item(),
                 'Loss/raw/test_clf_acc': test_clf_acc.item(),
                 'Loss/raw/dimp_clf_acc': dimp_clf_acc.item(),
                 'Loss/raw/is_target': is_target_loss.item(),
                 'Loss/raw/is_target_after_prop': is_target_after_prop_loss.item(),
                 'Loss/raw/test_seq_acc': test_seq_clf_acc.item(),
                 'Loss/raw/dimp_seq_acc': dimp_seq_clf_acc.item(),
                 }

        return loss, stats","backbone_feat_prev.view(1, num_sequences, -1, backbone_feat_prev.shape[-2], backbone_feat_prev.shape[-1])","backbone_feat_prev.view(1, num_sequences, -1, *backbone_feat_prev.shape[-2:])","iterable_zj[-2], iterable_zj[-1]",*backbone_feat_prev.shape[-2:],*backbone_feat_prev.shape[-2:0],0
pytracking,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytracking/ltr/actors/tracking.py,https://github.com/visionml/pytracking/tree/master/ltr/actors/tracking.py,KYSActor,__call__$207,"def __call__(self, data):
        sequence_length = data['test_images'].shape[0]
        num_sequences = data['test_images'].shape[1]

        valid_samples = data['test_valid_image'].to(self.device)
        test_visibility = data['test_visible_ratio'].to(self.device)

        # Initialize loss variables
        clf_loss_test_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        clf_loss_test_orig_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        dimp_loss_test_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        test_clf_acc = 0
        dimp_clf_acc = 0

        test_tracked_correct = torch.zeros(num_sequences, sequence_length - 1).long().to(self.device)
        test_seq_all_correct = torch.ones(num_sequences).to(self.device)
        dimp_seq_all_correct = torch.ones(num_sequences).to(self.device)

        is_target_loss_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        is_target_after_prop_loss_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)

        # Initialize target model using the training frames
        train_images = data['train_images'].to(self.device)
        train_anno = data['train_anno'].to(self.device)
        dimp_filters = self.net.train_classifier(train_images, train_anno)

        # Track in the first test frame
        test_image_cur = data['test_images'][0, ...].to(self.device)
        backbone_feat_prev_all = self.net.extract_backbone_features(test_image_cur)
        backbone_feat_prev = backbone_feat_prev_all[self.net.classification_layer]
        backbone_feat_prev = backbone_feat_prev.view(1, num_sequences, -1,
                                                     backbone_feat_prev.shape[-2], backbone_feat_prev.shape[-1])

        if self.net.motion_feat_extractor is not None:
            motion_feat_prev = self.net.motion_feat_extractor(backbone_feat_prev_all).view(1, num_sequences, -1,
                                                                                           backbone_feat_prev.shape[-2],
                                                                                           backbone_feat_prev.shape[-1])
        else:
            motion_feat_prev = backbone_feat_prev

        dimp_scores_prev = self.net.dimp_classifier.track_frame(dimp_filters, backbone_feat_prev)

        # Remove last row and col (added due to even kernel size in the target model)
        dimp_scores_prev = dimp_scores_prev[:, :, :-1, :-1].contiguous()

        # Set previous frame information
        label_prev = data['test_label'][0:1, ...].to(self.device)
        label_prev = label_prev[:, :, :-1, :-1].contiguous()

        anno_prev = data['test_anno'][0:1, ...].to(self.device)
        state_prev = None

        is_valid_prev = valid_samples[0, :].view(1, -1, 1, 1).byte()

        # Loop over the sequence
        for i in range(1, sequence_length):
            test_image_cur = data['test_images'][i, ...].to(self.device)
            test_label_cur = data['test_label'][i:i+1, ...].to(self.device)
            test_label_cur = test_label_cur[:, :, :-1, :-1].contiguous()

            test_anno_cur = data['test_anno'][i:i + 1, ...].to(self.device)

            # Extract features
            backbone_feat_cur_all = self.net.extract_backbone_features(test_image_cur)
            backbone_feat_cur = backbone_feat_cur_all[self.net.classification_layer]
            backbone_feat_cur = backbone_feat_cur.view(1, num_sequences, -1,
                                                       backbone_feat_cur.shape[-2], backbone_feat_cur.shape[-1])

            if self.net.motion_feat_extractor is not None:
                motion_feat_cur = self.net.motion_feat_extractor(backbone_feat_cur_all).view(1, num_sequences, -1,
                                                                                             backbone_feat_cur.shape[-2],
                                                                                             backbone_feat_cur.shape[-1])
            else:
                motion_feat_cur = backbone_feat_cur

            # Run target model
            dimp_scores_cur = self.net.dimp_classifier.track_frame(dimp_filters, backbone_feat_cur)
            dimp_scores_cur = dimp_scores_cur[:, :, :-1, :-1].contiguous()

            # Jitter target model output for augmentation
            jitter_info = None
            if self.dimp_jitter_fn is not None:
                dimp_scores_cur = self.dimp_jitter_fn(dimp_scores_cur, test_label_cur.clone())

            # Input target model output along with previous frame information to the predictor
            predictor_input_data = {'input1': motion_feat_prev, 'input2': motion_feat_cur,
                                    'label_prev': label_prev, 'anno_prev': anno_prev,
                                    'dimp_score_prev': dimp_scores_prev, 'dimp_score_cur': dimp_scores_cur,
                                    'state_prev': state_prev,
                                    'jitter_info': jitter_info}

            predictor_output = self.net.predictor(predictor_input_data)

            predicted_resp = predictor_output['response']
            state_prev = predictor_output['state_cur']
            aux_data = predictor_output['auxiliary_outputs']

            is_valid = valid_samples[i, :].view(1, -1, 1, 1).byte()
            uncertain_frame = (test_visibility[i, :].view(1, -1, 1, 1) < 0.75) * (test_visibility[i, :].view(1, -1, 1, 1) > 0.25)

            is_valid = is_valid * ~uncertain_frame

            # Calculate losses
            clf_loss_test_new = self.objective['test_clf'](predicted_resp, test_label_cur,
                                                           test_anno_cur, valid_samples=is_valid)
            clf_loss_test_all[:, i - 1] = clf_loss_test_new.squeeze()

            dimp_loss_test_new = self.objective['dimp_clf'](dimp_scores_cur, test_label_cur,
                                                            test_anno_cur, valid_samples=is_valid)
            dimp_loss_test_all[:, i - 1] = dimp_loss_test_new.squeeze()

            if 'fused_score_orig' in aux_data and 'test_clf_orig' in self.loss_weight.keys():
                aux_data['fused_score_orig'] = aux_data['fused_score_orig'].view(test_label_cur.shape)
                clf_loss_test_orig_new = self.objective['test_clf'](aux_data['fused_score_orig'], test_label_cur, test_anno_cur,  valid_samples=is_valid)
                clf_loss_test_orig_all[:, i - 1] = clf_loss_test_orig_new.squeeze()

            if 'is_target' in aux_data and 'is_target' in self.loss_weight.keys() and 'is_target' in self.objective.keys():
                is_target_loss_new = self.objective['is_target'](aux_data['is_target'], label_prev, is_valid_prev)
                is_target_loss_all[:, i - 1] = is_target_loss_new

            if 'is_target_after_prop' in aux_data and 'is_target_after_prop' in self.loss_weight.keys() and 'is_target' in self.objective.keys():
                is_target_after_prop_loss_new = self.objective['is_target'](aux_data['is_target_after_prop'],
                                                                            test_label_cur, is_valid)
                is_target_after_prop_loss_all[:, i - 1] = is_target_after_prop_loss_new

            test_clf_acc_new, test_pred_correct = self.objective['clf_acc'](predicted_resp, test_label_cur, valid_samples=is_valid)
            test_clf_acc += test_clf_acc_new

            test_seq_all_correct = test_seq_all_correct * (test_pred_correct.long() | (1 - is_valid).long()).float()
            test_tracked_correct[:, i - 1] = test_pred_correct

            dimp_clf_acc_new, dimp_pred_correct = self.objective['clf_acc'](dimp_scores_cur, test_label_cur, valid_samples=is_valid)
            dimp_clf_acc += dimp_clf_acc_new

            dimp_seq_all_correct = dimp_seq_all_correct * (dimp_pred_correct.long() | (1 - is_valid).long()).float()

            motion_feat_prev = motion_feat_cur.clone()
            dimp_scores_prev = dimp_scores_cur.clone()
            label_prev = test_label_cur.clone()
            is_valid_prev = is_valid.clone()

        # Compute average loss over the sequence
        clf_loss_test = clf_loss_test_all.mean()
        clf_loss_test_orig = clf_loss_test_orig_all.mean()
        dimp_loss_test = dimp_loss_test_all.mean()
        is_target_loss = is_target_loss_all.mean()
        is_target_after_prop_loss = is_target_after_prop_loss_all.mean()

        test_clf_acc /= (sequence_length - 1)
        dimp_clf_acc /= (sequence_length - 1)
        clf_loss_test_orig /= (sequence_length - 1)

        test_seq_clf_acc = test_seq_all_correct.mean()
        dimp_seq_clf_acc = dimp_seq_all_correct.mean()

        clf_loss_test_w = self.loss_weight['test_clf'] * clf_loss_test
        clf_loss_test_orig_w = self.loss_weight['test_clf_orig'] * clf_loss_test_orig
        dimp_loss_test_w = self.loss_weight.get('dimp_clf', 0.0) * dimp_loss_test

        is_target_loss_w = self.loss_weight.get('is_target', 0.0) * is_target_loss
        is_target_after_prop_loss_w = self.loss_weight.get('is_target_after_prop', 0.0) * is_target_after_prop_loss

        loss = clf_loss_test_w + dimp_loss_test_w + is_target_loss_w + is_target_after_prop_loss_w + clf_loss_test_orig_w

        stats = {'Loss/total': loss.item(),
                 'Loss/test_clf': clf_loss_test_w.item(),
                 'Loss/dimp_clf': dimp_loss_test_w.item(),
                 'Loss/raw/test_clf': clf_loss_test.item(),
                 'Loss/raw/test_clf_orig': clf_loss_test_orig.item(),
                 'Loss/raw/dimp_clf': dimp_loss_test.item(),
                 'Loss/raw/test_clf_acc': test_clf_acc.item(),
                 'Loss/raw/dimp_clf_acc': dimp_clf_acc.item(),
                 'Loss/raw/is_target': is_target_loss.item(),
                 'Loss/raw/is_target_after_prop': is_target_after_prop_loss.item(),
                 'Loss/raw/test_seq_acc': test_seq_clf_acc.item(),
                 'Loss/raw/dimp_seq_acc': dimp_seq_clf_acc.item(),
                 }

        return loss, stats","self.net.motion_feat_extractor(backbone_feat_prev_all).view(1, num_sequences, -1, backbone_feat_prev.shape[-2], backbone_feat_prev.shape[-1])","self.net.motion_feat_extractor(backbone_feat_prev_all).view(1, num_sequences, -1, *backbone_feat_prev.shape[-2:])","iterable_zj[-2], iterable_zj[-1]",*backbone_feat_prev.shape[-2:],*backbone_feat_prev.shape[-2:0],0
pytracking,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytracking/ltr/actors/tracking.py,https://github.com/visionml/pytracking/tree/master/ltr/actors/tracking.py,KYSActor,__call__$207,"def __call__(self, data):
        sequence_length = data['test_images'].shape[0]
        num_sequences = data['test_images'].shape[1]

        valid_samples = data['test_valid_image'].to(self.device)
        test_visibility = data['test_visible_ratio'].to(self.device)

        # Initialize loss variables
        clf_loss_test_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        clf_loss_test_orig_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        dimp_loss_test_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        test_clf_acc = 0
        dimp_clf_acc = 0

        test_tracked_correct = torch.zeros(num_sequences, sequence_length - 1).long().to(self.device)
        test_seq_all_correct = torch.ones(num_sequences).to(self.device)
        dimp_seq_all_correct = torch.ones(num_sequences).to(self.device)

        is_target_loss_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        is_target_after_prop_loss_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)

        # Initialize target model using the training frames
        train_images = data['train_images'].to(self.device)
        train_anno = data['train_anno'].to(self.device)
        dimp_filters = self.net.train_classifier(train_images, train_anno)

        # Track in the first test frame
        test_image_cur = data['test_images'][0, ...].to(self.device)
        backbone_feat_prev_all = self.net.extract_backbone_features(test_image_cur)
        backbone_feat_prev = backbone_feat_prev_all[self.net.classification_layer]
        backbone_feat_prev = backbone_feat_prev.view(1, num_sequences, -1,
                                                     backbone_feat_prev.shape[-2], backbone_feat_prev.shape[-1])

        if self.net.motion_feat_extractor is not None:
            motion_feat_prev = self.net.motion_feat_extractor(backbone_feat_prev_all).view(1, num_sequences, -1,
                                                                                           backbone_feat_prev.shape[-2],
                                                                                           backbone_feat_prev.shape[-1])
        else:
            motion_feat_prev = backbone_feat_prev

        dimp_scores_prev = self.net.dimp_classifier.track_frame(dimp_filters, backbone_feat_prev)

        # Remove last row and col (added due to even kernel size in the target model)
        dimp_scores_prev = dimp_scores_prev[:, :, :-1, :-1].contiguous()

        # Set previous frame information
        label_prev = data['test_label'][0:1, ...].to(self.device)
        label_prev = label_prev[:, :, :-1, :-1].contiguous()

        anno_prev = data['test_anno'][0:1, ...].to(self.device)
        state_prev = None

        is_valid_prev = valid_samples[0, :].view(1, -1, 1, 1).byte()

        # Loop over the sequence
        for i in range(1, sequence_length):
            test_image_cur = data['test_images'][i, ...].to(self.device)
            test_label_cur = data['test_label'][i:i+1, ...].to(self.device)
            test_label_cur = test_label_cur[:, :, :-1, :-1].contiguous()

            test_anno_cur = data['test_anno'][i:i + 1, ...].to(self.device)

            # Extract features
            backbone_feat_cur_all = self.net.extract_backbone_features(test_image_cur)
            backbone_feat_cur = backbone_feat_cur_all[self.net.classification_layer]
            backbone_feat_cur = backbone_feat_cur.view(1, num_sequences, -1,
                                                       backbone_feat_cur.shape[-2], backbone_feat_cur.shape[-1])

            if self.net.motion_feat_extractor is not None:
                motion_feat_cur = self.net.motion_feat_extractor(backbone_feat_cur_all).view(1, num_sequences, -1,
                                                                                             backbone_feat_cur.shape[-2],
                                                                                             backbone_feat_cur.shape[-1])
            else:
                motion_feat_cur = backbone_feat_cur

            # Run target model
            dimp_scores_cur = self.net.dimp_classifier.track_frame(dimp_filters, backbone_feat_cur)
            dimp_scores_cur = dimp_scores_cur[:, :, :-1, :-1].contiguous()

            # Jitter target model output for augmentation
            jitter_info = None
            if self.dimp_jitter_fn is not None:
                dimp_scores_cur = self.dimp_jitter_fn(dimp_scores_cur, test_label_cur.clone())

            # Input target model output along with previous frame information to the predictor
            predictor_input_data = {'input1': motion_feat_prev, 'input2': motion_feat_cur,
                                    'label_prev': label_prev, 'anno_prev': anno_prev,
                                    'dimp_score_prev': dimp_scores_prev, 'dimp_score_cur': dimp_scores_cur,
                                    'state_prev': state_prev,
                                    'jitter_info': jitter_info}

            predictor_output = self.net.predictor(predictor_input_data)

            predicted_resp = predictor_output['response']
            state_prev = predictor_output['state_cur']
            aux_data = predictor_output['auxiliary_outputs']

            is_valid = valid_samples[i, :].view(1, -1, 1, 1).byte()
            uncertain_frame = (test_visibility[i, :].view(1, -1, 1, 1) < 0.75) * (test_visibility[i, :].view(1, -1, 1, 1) > 0.25)

            is_valid = is_valid * ~uncertain_frame

            # Calculate losses
            clf_loss_test_new = self.objective['test_clf'](predicted_resp, test_label_cur,
                                                           test_anno_cur, valid_samples=is_valid)
            clf_loss_test_all[:, i - 1] = clf_loss_test_new.squeeze()

            dimp_loss_test_new = self.objective['dimp_clf'](dimp_scores_cur, test_label_cur,
                                                            test_anno_cur, valid_samples=is_valid)
            dimp_loss_test_all[:, i - 1] = dimp_loss_test_new.squeeze()

            if 'fused_score_orig' in aux_data and 'test_clf_orig' in self.loss_weight.keys():
                aux_data['fused_score_orig'] = aux_data['fused_score_orig'].view(test_label_cur.shape)
                clf_loss_test_orig_new = self.objective['test_clf'](aux_data['fused_score_orig'], test_label_cur, test_anno_cur,  valid_samples=is_valid)
                clf_loss_test_orig_all[:, i - 1] = clf_loss_test_orig_new.squeeze()

            if 'is_target' in aux_data and 'is_target' in self.loss_weight.keys() and 'is_target' in self.objective.keys():
                is_target_loss_new = self.objective['is_target'](aux_data['is_target'], label_prev, is_valid_prev)
                is_target_loss_all[:, i - 1] = is_target_loss_new

            if 'is_target_after_prop' in aux_data and 'is_target_after_prop' in self.loss_weight.keys() and 'is_target' in self.objective.keys():
                is_target_after_prop_loss_new = self.objective['is_target'](aux_data['is_target_after_prop'],
                                                                            test_label_cur, is_valid)
                is_target_after_prop_loss_all[:, i - 1] = is_target_after_prop_loss_new

            test_clf_acc_new, test_pred_correct = self.objective['clf_acc'](predicted_resp, test_label_cur, valid_samples=is_valid)
            test_clf_acc += test_clf_acc_new

            test_seq_all_correct = test_seq_all_correct * (test_pred_correct.long() | (1 - is_valid).long()).float()
            test_tracked_correct[:, i - 1] = test_pred_correct

            dimp_clf_acc_new, dimp_pred_correct = self.objective['clf_acc'](dimp_scores_cur, test_label_cur, valid_samples=is_valid)
            dimp_clf_acc += dimp_clf_acc_new

            dimp_seq_all_correct = dimp_seq_all_correct * (dimp_pred_correct.long() | (1 - is_valid).long()).float()

            motion_feat_prev = motion_feat_cur.clone()
            dimp_scores_prev = dimp_scores_cur.clone()
            label_prev = test_label_cur.clone()
            is_valid_prev = is_valid.clone()

        # Compute average loss over the sequence
        clf_loss_test = clf_loss_test_all.mean()
        clf_loss_test_orig = clf_loss_test_orig_all.mean()
        dimp_loss_test = dimp_loss_test_all.mean()
        is_target_loss = is_target_loss_all.mean()
        is_target_after_prop_loss = is_target_after_prop_loss_all.mean()

        test_clf_acc /= (sequence_length - 1)
        dimp_clf_acc /= (sequence_length - 1)
        clf_loss_test_orig /= (sequence_length - 1)

        test_seq_clf_acc = test_seq_all_correct.mean()
        dimp_seq_clf_acc = dimp_seq_all_correct.mean()

        clf_loss_test_w = self.loss_weight['test_clf'] * clf_loss_test
        clf_loss_test_orig_w = self.loss_weight['test_clf_orig'] * clf_loss_test_orig
        dimp_loss_test_w = self.loss_weight.get('dimp_clf', 0.0) * dimp_loss_test

        is_target_loss_w = self.loss_weight.get('is_target', 0.0) * is_target_loss
        is_target_after_prop_loss_w = self.loss_weight.get('is_target_after_prop', 0.0) * is_target_after_prop_loss

        loss = clf_loss_test_w + dimp_loss_test_w + is_target_loss_w + is_target_after_prop_loss_w + clf_loss_test_orig_w

        stats = {'Loss/total': loss.item(),
                 'Loss/test_clf': clf_loss_test_w.item(),
                 'Loss/dimp_clf': dimp_loss_test_w.item(),
                 'Loss/raw/test_clf': clf_loss_test.item(),
                 'Loss/raw/test_clf_orig': clf_loss_test_orig.item(),
                 'Loss/raw/dimp_clf': dimp_loss_test.item(),
                 'Loss/raw/test_clf_acc': test_clf_acc.item(),
                 'Loss/raw/dimp_clf_acc': dimp_clf_acc.item(),
                 'Loss/raw/is_target': is_target_loss.item(),
                 'Loss/raw/is_target_after_prop': is_target_after_prop_loss.item(),
                 'Loss/raw/test_seq_acc': test_seq_clf_acc.item(),
                 'Loss/raw/dimp_seq_acc': dimp_seq_clf_acc.item(),
                 }

        return loss, stats","backbone_feat_cur.view(1, num_sequences, -1, backbone_feat_cur.shape[-2], backbone_feat_cur.shape[-1])","backbone_feat_cur.view(1, num_sequences, -1, *backbone_feat_cur.shape[-2:])","iterable_zj[-2], iterable_zj[-1]",*backbone_feat_cur.shape[-2:],*backbone_feat_cur.shape[-2:0],0
pytracking,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytracking/ltr/actors/tracking.py,https://github.com/visionml/pytracking/tree/master/ltr/actors/tracking.py,KYSActor,__call__$207,"def __call__(self, data):
        sequence_length = data['test_images'].shape[0]
        num_sequences = data['test_images'].shape[1]

        valid_samples = data['test_valid_image'].to(self.device)
        test_visibility = data['test_visible_ratio'].to(self.device)

        # Initialize loss variables
        clf_loss_test_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        clf_loss_test_orig_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        dimp_loss_test_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        test_clf_acc = 0
        dimp_clf_acc = 0

        test_tracked_correct = torch.zeros(num_sequences, sequence_length - 1).long().to(self.device)
        test_seq_all_correct = torch.ones(num_sequences).to(self.device)
        dimp_seq_all_correct = torch.ones(num_sequences).to(self.device)

        is_target_loss_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)
        is_target_after_prop_loss_all = torch.zeros(num_sequences, sequence_length - 1).to(self.device)

        # Initialize target model using the training frames
        train_images = data['train_images'].to(self.device)
        train_anno = data['train_anno'].to(self.device)
        dimp_filters = self.net.train_classifier(train_images, train_anno)

        # Track in the first test frame
        test_image_cur = data['test_images'][0, ...].to(self.device)
        backbone_feat_prev_all = self.net.extract_backbone_features(test_image_cur)
        backbone_feat_prev = backbone_feat_prev_all[self.net.classification_layer]
        backbone_feat_prev = backbone_feat_prev.view(1, num_sequences, -1,
                                                     backbone_feat_prev.shape[-2], backbone_feat_prev.shape[-1])

        if self.net.motion_feat_extractor is not None:
            motion_feat_prev = self.net.motion_feat_extractor(backbone_feat_prev_all).view(1, num_sequences, -1,
                                                                                           backbone_feat_prev.shape[-2],
                                                                                           backbone_feat_prev.shape[-1])
        else:
            motion_feat_prev = backbone_feat_prev

        dimp_scores_prev = self.net.dimp_classifier.track_frame(dimp_filters, backbone_feat_prev)

        # Remove last row and col (added due to even kernel size in the target model)
        dimp_scores_prev = dimp_scores_prev[:, :, :-1, :-1].contiguous()

        # Set previous frame information
        label_prev = data['test_label'][0:1, ...].to(self.device)
        label_prev = label_prev[:, :, :-1, :-1].contiguous()

        anno_prev = data['test_anno'][0:1, ...].to(self.device)
        state_prev = None

        is_valid_prev = valid_samples[0, :].view(1, -1, 1, 1).byte()

        # Loop over the sequence
        for i in range(1, sequence_length):
            test_image_cur = data['test_images'][i, ...].to(self.device)
            test_label_cur = data['test_label'][i:i+1, ...].to(self.device)
            test_label_cur = test_label_cur[:, :, :-1, :-1].contiguous()

            test_anno_cur = data['test_anno'][i:i + 1, ...].to(self.device)

            # Extract features
            backbone_feat_cur_all = self.net.extract_backbone_features(test_image_cur)
            backbone_feat_cur = backbone_feat_cur_all[self.net.classification_layer]
            backbone_feat_cur = backbone_feat_cur.view(1, num_sequences, -1,
                                                       backbone_feat_cur.shape[-2], backbone_feat_cur.shape[-1])

            if self.net.motion_feat_extractor is not None:
                motion_feat_cur = self.net.motion_feat_extractor(backbone_feat_cur_all).view(1, num_sequences, -1,
                                                                                             backbone_feat_cur.shape[-2],
                                                                                             backbone_feat_cur.shape[-1])
            else:
                motion_feat_cur = backbone_feat_cur

            # Run target model
            dimp_scores_cur = self.net.dimp_classifier.track_frame(dimp_filters, backbone_feat_cur)
            dimp_scores_cur = dimp_scores_cur[:, :, :-1, :-1].contiguous()

            # Jitter target model output for augmentation
            jitter_info = None
            if self.dimp_jitter_fn is not None:
                dimp_scores_cur = self.dimp_jitter_fn(dimp_scores_cur, test_label_cur.clone())

            # Input target model output along with previous frame information to the predictor
            predictor_input_data = {'input1': motion_feat_prev, 'input2': motion_feat_cur,
                                    'label_prev': label_prev, 'anno_prev': anno_prev,
                                    'dimp_score_prev': dimp_scores_prev, 'dimp_score_cur': dimp_scores_cur,
                                    'state_prev': state_prev,
                                    'jitter_info': jitter_info}

            predictor_output = self.net.predictor(predictor_input_data)

            predicted_resp = predictor_output['response']
            state_prev = predictor_output['state_cur']
            aux_data = predictor_output['auxiliary_outputs']

            is_valid = valid_samples[i, :].view(1, -1, 1, 1).byte()
            uncertain_frame = (test_visibility[i, :].view(1, -1, 1, 1) < 0.75) * (test_visibility[i, :].view(1, -1, 1, 1) > 0.25)

            is_valid = is_valid * ~uncertain_frame

            # Calculate losses
            clf_loss_test_new = self.objective['test_clf'](predicted_resp, test_label_cur,
                                                           test_anno_cur, valid_samples=is_valid)
            clf_loss_test_all[:, i - 1] = clf_loss_test_new.squeeze()

            dimp_loss_test_new = self.objective['dimp_clf'](dimp_scores_cur, test_label_cur,
                                                            test_anno_cur, valid_samples=is_valid)
            dimp_loss_test_all[:, i - 1] = dimp_loss_test_new.squeeze()

            if 'fused_score_orig' in aux_data and 'test_clf_orig' in self.loss_weight.keys():
                aux_data['fused_score_orig'] = aux_data['fused_score_orig'].view(test_label_cur.shape)
                clf_loss_test_orig_new = self.objective['test_clf'](aux_data['fused_score_orig'], test_label_cur, test_anno_cur,  valid_samples=is_valid)
                clf_loss_test_orig_all[:, i - 1] = clf_loss_test_orig_new.squeeze()

            if 'is_target' in aux_data and 'is_target' in self.loss_weight.keys() and 'is_target' in self.objective.keys():
                is_target_loss_new = self.objective['is_target'](aux_data['is_target'], label_prev, is_valid_prev)
                is_target_loss_all[:, i - 1] = is_target_loss_new

            if 'is_target_after_prop' in aux_data and 'is_target_after_prop' in self.loss_weight.keys() and 'is_target' in self.objective.keys():
                is_target_after_prop_loss_new = self.objective['is_target'](aux_data['is_target_after_prop'],
                                                                            test_label_cur, is_valid)
                is_target_after_prop_loss_all[:, i - 1] = is_target_after_prop_loss_new

            test_clf_acc_new, test_pred_correct = self.objective['clf_acc'](predicted_resp, test_label_cur, valid_samples=is_valid)
            test_clf_acc += test_clf_acc_new

            test_seq_all_correct = test_seq_all_correct * (test_pred_correct.long() | (1 - is_valid).long()).float()
            test_tracked_correct[:, i - 1] = test_pred_correct

            dimp_clf_acc_new, dimp_pred_correct = self.objective['clf_acc'](dimp_scores_cur, test_label_cur, valid_samples=is_valid)
            dimp_clf_acc += dimp_clf_acc_new

            dimp_seq_all_correct = dimp_seq_all_correct * (dimp_pred_correct.long() | (1 - is_valid).long()).float()

            motion_feat_prev = motion_feat_cur.clone()
            dimp_scores_prev = dimp_scores_cur.clone()
            label_prev = test_label_cur.clone()
            is_valid_prev = is_valid.clone()

        # Compute average loss over the sequence
        clf_loss_test = clf_loss_test_all.mean()
        clf_loss_test_orig = clf_loss_test_orig_all.mean()
        dimp_loss_test = dimp_loss_test_all.mean()
        is_target_loss = is_target_loss_all.mean()
        is_target_after_prop_loss = is_target_after_prop_loss_all.mean()

        test_clf_acc /= (sequence_length - 1)
        dimp_clf_acc /= (sequence_length - 1)
        clf_loss_test_orig /= (sequence_length - 1)

        test_seq_clf_acc = test_seq_all_correct.mean()
        dimp_seq_clf_acc = dimp_seq_all_correct.mean()

        clf_loss_test_w = self.loss_weight['test_clf'] * clf_loss_test
        clf_loss_test_orig_w = self.loss_weight['test_clf_orig'] * clf_loss_test_orig
        dimp_loss_test_w = self.loss_weight.get('dimp_clf', 0.0) * dimp_loss_test

        is_target_loss_w = self.loss_weight.get('is_target', 0.0) * is_target_loss
        is_target_after_prop_loss_w = self.loss_weight.get('is_target_after_prop', 0.0) * is_target_after_prop_loss

        loss = clf_loss_test_w + dimp_loss_test_w + is_target_loss_w + is_target_after_prop_loss_w + clf_loss_test_orig_w

        stats = {'Loss/total': loss.item(),
                 'Loss/test_clf': clf_loss_test_w.item(),
                 'Loss/dimp_clf': dimp_loss_test_w.item(),
                 'Loss/raw/test_clf': clf_loss_test.item(),
                 'Loss/raw/test_clf_orig': clf_loss_test_orig.item(),
                 'Loss/raw/dimp_clf': dimp_loss_test.item(),
                 'Loss/raw/test_clf_acc': test_clf_acc.item(),
                 'Loss/raw/dimp_clf_acc': dimp_clf_acc.item(),
                 'Loss/raw/is_target': is_target_loss.item(),
                 'Loss/raw/is_target_after_prop': is_target_after_prop_loss.item(),
                 'Loss/raw/test_seq_acc': test_seq_clf_acc.item(),
                 'Loss/raw/dimp_seq_acc': dimp_seq_clf_acc.item(),
                 }

        return loss, stats","self.net.motion_feat_extractor(backbone_feat_cur_all).view(1, num_sequences, -1, backbone_feat_cur.shape[-2], backbone_feat_cur.shape[-1])","self.net.motion_feat_extractor(backbone_feat_cur_all).view(1, num_sequences, -1, *backbone_feat_cur.shape[-2:])","iterable_zj[-2], iterable_zj[-1]",*backbone_feat_cur.shape[-2:],*backbone_feat_cur.shape[-2:0],0
NeoVintageous,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NeoVintageous/tests/unittest.py,https://github.com/NeoVintageous/NeoVintageous/tree/master/tests/unittest.py,,wrapper$1270,"def wrapper(f):
        @mock_bell()
        @mock.patch('sublime.View.em_width')
        @mock.patch('sublime.View.line_height')
        @mock.patch('sublime.View.viewport_extent')
        @mock.patch('sublime.View.visible_region')
        def wrapped(self, *args, **kwargs):
            self.em_width = args[-1]
            self.line_height = args[-2]
            self.viewport_extent = args[-3]
            self.visible_region = args[-4]

            def _viewport_extent():
                # Fixes UI mocking issue in Sublime Text 4. It seems that
                # layout_extent() needs to be invoked whenever this mock is used
                # because it updates the layout extent to the correct value.
                self.view.layout_extent()

                if screen_rows is not None:
                    rows = screen_rows
                else:
                    rows = self.view.rowcol(self.view.size())[0] + 1

                # Find the max cols of the view by cycling though all the lines.
                lines = self.view.lines(self.Region(0, self.view.size()))
                cols = max(lines, key=lambda item: item.size()).size()

                # The extent y position needs an additional ""1.0"". It's not
                # clear why Sublime needs to add it, but it always adds it.
                extent_x = em_width * (cols + 2)
                extent_y = (line_height * rows) + 1.0
                extent = (extent_x, extent_y)

                return extent

            # The default visible region uses the size of the current view. For
            # example when a test fixture is setup, the visible region will be
            # the same size as content. This makes most tests easier to setup.
            def _visible_region() -> Region:
                if visible_region:
                    return self.Region(visible_region[0], visible_region[1])

                region = self.Region(0, self.view.size())

                return region

            self.em_width.return_value = em_width
            self.line_height.return_value = line_height
            self.viewport_extent.side_effect = _viewport_extent
            self.visible_region.side_effect = _visible_region

            return f(self, *args[:-4], **kwargs)
        return wrapped","self.Region(visible_region[0], visible_region[1])",self.Region(*visible_region[:2]),"iterable_zj[0], iterable_zj[1]",*visible_region[:2],*visible_region[:2],1
sfepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sfepy/sfepy/homogenization/utils.py,https://github.com/sfepy/sfepy/tree/master/sfepy/homogenization/utils.py,,get_lattice_volume$230,"def get_lattice_volume(axes):
    r""""""
    Volume of a periodic cell in a rectangular 3D (or 2D) lattice.

    Parameters
    ----------
    axes : array
        The array with the periodic cell axes :math:`a_1, \dots, a_3` as rows.

    Returns
    -------
    volume : float
        The periodic cell volume :math:`V = (a_1 \times a_2) \cdot a_3`. In 2D
        :math:`V = |(a_1 \times a_2)|` with zeros as the third components of
        vectors :math:`a_1`, :math:`a_2`.
    """"""
    axes = nm.asarray(axes)

    dim = axes.shape[0]

    if dim == 2:
        volume = nm.abs(nm.cross(axes[0], axes[1]))

    elif dim == 3:
        volume = nm.dot(nm.cross(axes[0], axes[1]), axes[2])

    else:
        raise ValueError('wrong axes shape! (%s)' % axes.shape)

    return volume","nm.cross(axes[0], axes[1])",nm.cross(*axes[:2]),"iterable_zj[0], iterable_zj[1]",*axes[:2],*axes[:2],1
sfepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sfepy/sfepy/homogenization/utils.py,https://github.com/sfepy/sfepy/tree/master/sfepy/homogenization/utils.py,,get_lattice_volume$230,"def get_lattice_volume(axes):
    r""""""
    Volume of a periodic cell in a rectangular 3D (or 2D) lattice.

    Parameters
    ----------
    axes : array
        The array with the periodic cell axes :math:`a_1, \dots, a_3` as rows.

    Returns
    -------
    volume : float
        The periodic cell volume :math:`V = (a_1 \times a_2) \cdot a_3`. In 2D
        :math:`V = |(a_1 \times a_2)|` with zeros as the third components of
        vectors :math:`a_1`, :math:`a_2`.
    """"""
    axes = nm.asarray(axes)

    dim = axes.shape[0]

    if dim == 2:
        volume = nm.abs(nm.cross(axes[0], axes[1]))

    elif dim == 3:
        volume = nm.dot(nm.cross(axes[0], axes[1]), axes[2])

    else:
        raise ValueError('wrong axes shape! (%s)' % axes.shape)

    return volume","nm.cross(axes[0], axes[1])",nm.cross(*axes[:2]),"iterable_zj[0], iterable_zj[1]",*axes[:2],*axes[:2],1
Rasa_NLU_Chi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Rasa_NLU_Chi/rasa_nlu/training_data/formats/markdown.py,https://github.com/crownpku/Rasa_NLU_Chi/tree/master/rasa_nlu/training_data/formats/markdown.py,MarkdownReader,reads$40,"def reads(self, s, **kwargs):
        """"""Read markdown string and create TrainingData object""""""
        self.__init__()
        s = self._strip_comments(s)
        for line in s.splitlines():
            line = line.strip()
            header = self._find_section_header(line)
            if header:
                self._set_current_section(header[0], header[1])
            else:
                self._parse_item(line)

        return TrainingData(self.training_examples, self.entity_synonyms, self.regex_features)","self._set_current_section(header[0], header[1])",self._set_current_section(*header[:2]),"iterable_zj[0], iterable_zj[1]",*header[:2],*header[:2],1
yolov5-face,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yolov5-face/test_widerface.py,https://github.com/deepcam-cn/yolov5-face/tree/master//test_widerface.py,,dynamic_resize$20,"def dynamic_resize(shape, stride=64):
    max_size = max(shape[0], shape[1])
    if max_size % stride != 0:
        max_size = (int(max_size / stride) + 1) * stride 
    return max_size","max(shape[0], shape[1])",max(*shape[:2]),"iterable_zj[0], iterable_zj[1]",*shape[:2],*shape[:2],1
imgaug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgaug/test/augmenters/test_segmentation.py,https://github.com/aleju/imgaug/tree/master/test/augmenters/test_segmentation.py,TestUniformPointsSampler,test_many_images$1468,"def test_many_images(self):
        sampler = iaa.UniformPointsSampler(1000)
        images = [
            np.zeros((100, 500, 3), dtype=np.uint8),
            np.zeros((500, 100, 1), dtype=np.uint8)
        ]

        points = sampler.sample_points(images, 1)

        assert len(points) == 2
        assert len(points[0]) == 1000
        assert len(points[1]) == 1000
        assert not np.allclose(points[0], points[1])
        assert np.any(points[0][:, 1] < 20)
        assert np.any(points[0][:, 1] > 0.9*100)
        assert np.any(points[0][:, 0] < 20)
        assert np.any(points[0][:, 0] > 0.9*500)
        assert np.any(points[1][:, 1] < 20)
        assert np.any(points[1][:, 1] > 0.9*500)
        assert np.any(points[1][:, 0] < 20)
        assert np.any(points[1][:, 0] > 0.9*100)","np.allclose(points[0], points[1])",np.allclose(*points[:2]),"iterable_zj[0], iterable_zj[1]",*points[:2],*points[:2],1
pytorchvideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorchvideo/tests/test_transforms.py,https://github.com/facebookresearch/pytorchvideo/tree/master/tests/test_transforms.py,TestTransforms,test_horizontal_flip_with_boxes$310,"def test_horizontal_flip_with_boxes(self):
        video = thwc_to_cthw(create_dummy_video_frames(10, 20, 40)).to(
            dtype=torch.float32
        )
        boxes_inp = create_random_bbox(7, 20, 40)

        actual, boxes = horizontal_flip_with_boxes(0.0, video, boxes_inp)
        self.assertTrue(actual.equal(video))
        self.assertTrue(boxes.equal(boxes_inp))

        actual, boxes = horizontal_flip_with_boxes(1.0, video, boxes_inp)
        self.assertEqual(actual.shape, video.shape)
        self._check_boxes(7, actual.shape[-2], actual.shape[-1], boxes)
        self.assertTrue(actual.flip((-1)).equal(video))","self._check_boxes(7, actual.shape[-2], actual.shape[-1], boxes)","self._check_boxes(7, *actual.shape[-2:], boxes)","iterable_zj[-2], iterable_zj[-1]",*actual.shape[-2:],*actual.shape[-2:0],0
Detectron.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Detectron.pytorch/lib/datasets/voc_eval.py,https://github.com/roytseng-tw/Detectron.pytorch/tree/master/lib/datasets/voc_eval.py,,voc_ap$54,"def voc_ap(rec, prec, use_07_metric=False):
    """"""Compute VOC AP given precision and recall. If use_07_metric is true, uses
    the VOC 07 11-point method (default:False).
    """"""
    if use_07_metric:
        # 11 point metric
        ap = 0.
        for t in np.arange(0., 1.1, 0.1):
            if np.sum(rec >= t) == 0:
                p = 0
            else:
                p = np.max(prec[rec >= t])
            ap = ap + p / 11.
    else:
        # correct AP calculation
        # first append sentinel values at the end
        mrec = np.concatenate(([0.], rec, [1.]))
        mpre = np.concatenate(([0.], prec, [0.]))

        # compute the precision envelope
        for i in range(mpre.size - 1, 0, -1):
            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

        # to calculate area under PR curve, look for points
        # where X axis (recall) changes value
        i = np.where(mrec[1:] != mrec[:-1])[0]

        # and sum (\Delta recall) * prec
        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap","np.maximum(mpre[i - 1], mpre[i])",np.maximum(*mpre[i - 1:i + 1]),"iterable_zj[i - 1], iterable_zj[i]",*mpre[i-1:i+1],*mpre[i - 1:i + 1],1
byob,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/byob/byob/core/util.py,https://github.com/malwaredllc/byob/tree/master/byob/core/util.py,,ftp$512,"def ftp(source, host=None, user=None, password=None, filetype=None):
    """"""
    Upload file/data to FTP server

    `Required`
    :param str source:    data or readable file-like object
    :param str host:      FTP server hostname
    :param str user:      FTP account username
    :param str password:  FTP account password

    `Optional`
    :param str filetype:  target file type (default: .txt)

    """"""
    import os
    import time
    import ftplib

    try:
        from StringIO import StringIO  # Python 2
    except ImportError:
        from io import StringIO        # Python 3

    if host and user and password:
        path  = ''
        local = time.ctime().split()
        if os.path.isfile(str(source)):
            path   = source
            source = open(path, 'rb')
        elif hasattr(source, 'seek'):
            source.seek(0)
        else:
            source = StringIO(source)
        try:
            ftp = ftplib.FTP(host=host, user=user, password=password)
        except:
            return ""Upload failed - remote FTP server authorization error""
        addr = public_ip()
        if 'tmp' not in ftp.nlst():
            ftp.mkd('/tmp')
        if addr not in ftp.nlst('/tmp'):
            ftp.mkd('/tmp/{}'.format(addr))
        if path:
            path = '/tmp/{}/{}'.format(addr, os.path.basename(path))
        else:
            filetype = '.' + str(filetype) if not str(filetype).startswith('.') else str(filetype)
            path = '/tmp/{}/{}'.format(addr, '{}-{}_{}{}'.format(local[1], local[2], local[3], filetype))
        stor = ftp.storbinary('STOR ' + path, source)
        return path
    else:
        log('missing one or more required arguments: host, user, password')","'{}-{}_{}{}'.format(local[1], local[2], local[3], filetype)","'{}-{}_{}{}'.format(*local[1:4], filetype)","iterable_zj[1], iterable_zj[2], iterable_zj[3]",*local[1:4],*local[1:4],1
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/core/chords.py,https://github.com/bspaans/python-mingus/tree/master/mingus/core/chords.py,,inversion_exhauster$1035,"def inversion_exhauster(seventh, shorthand, tries, result, polychords):
        """"""Determine sevenths recursive functions.""""""
        # Check whether the first three notes of seventh are part of some triad.
        triads = determine_triad(seventh[:3], True, True)

        # Get the interval between the first and last note
        intval3 = intervals.determine(seventh[0], seventh[3])

        def add_result(short, poly=False):
            """"""Helper function.""""""
            result.append((short, tries, seventh[0], poly))

        # Recognizing polychords
        if tries == 1 and not no_polychords:
            polychords = polychords + determine_polychords(seventh, shorthand)

        # Recognizing sevenths
        for triad in triads:
            # Basic triads
            triad = triad[len(seventh[0]) :]
            if triad == ""m"":
                if intval3 == ""minor seventh"":
                    add_result(""m7"")
                elif intval3 == ""major seventh"":
                    add_result(""m/M7"")
                elif intval3 == ""major sixth"":
                    add_result(""m6"")
            elif triad == ""M"":
                if intval3 == ""major seventh"":
                    add_result(""M7"")
                elif intval3 == ""minor seventh"":
                    add_result(""7"")
                elif intval3 == ""major sixth"":
                    add_result(""M6"")
            elif triad == ""dim"":
                if intval3 == ""minor seventh"":
                    add_result(""m7b5"")
                elif intval3 == ""diminished seventh"":
                    add_result(""dim7"")
            elif triad == ""aug"":
                if intval3 == ""minor seventh"":
                    add_result(""m7+"")
                if intval3 == ""major seventh"":
                    add_result(""M7+"")
            elif triad == ""sus4"":
                if intval3 == ""minor seventh"":
                    add_result(""sus47"")
                elif intval3 == ""minor second"":
                    add_result(""sus4b9"")
            elif triad == ""m7"":

                # Other
                if intval3 == ""perfect fourth"":
                    add_result(""11"")
            elif triad == ""7b5"":
                if intval3 == ""minor seventh"":
                    add_result(""7b5"")

        if tries != 4 and not no_inversion:
            return inversion_exhauster(
                [seventh[-1]] + seventh[:-1], shorthand, tries + 1, result, polychords
            )
        else:
            # Return results
            res = []

            # Reset seventh
            seventh = [seventh[3]] + seventh[0:3]
            for x in result:
                if shorthand:
                    res.append(x[2] + x[0])
                else:
                    res.append(x[2] + chord_shorthand_meaning[x[0]] + int_desc(x[1]))
            return res + polychords","intervals.determine(seventh[0], seventh[3])","intervals.determine(*seventh[0], seventh[3])","iterable_zj[0], iterable_zj[3]","*seventh[0], seventh[3]",*seventh[:6:3],0
dragonfly,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dragonfly/dragonfly/nn/unittest_nn_gp.py,https://github.com/dragonfly/dragonfly/tree/master/dragonfly/nn/unittest_nn_gp.py,,fit_nngp_with_dataset$97,"def fit_nngp_with_dataset(dataset, kernel_type, dist_type):
  """""" Fits an NNGP to this dataset. """"""
  options = load_options(nn_gp.nn_gp_args, '')
  options.kernel_type = kernel_type
  options.dist_type = dist_type
  gp_fitter = nn_gp.NNGPFitter(dataset[0], dataset[1], dataset[-1],
                               options=options, reporter=None)
  fitted_result = gp_fitter.fit_gp()
  fitted_gp = fitted_result[1]
  return fitted_gp","nn_gp.NNGPFitter(dataset[0], dataset[1], dataset[-1], options=options, reporter=None)","nn_gp.NNGPFitter(*dataset[:2], dataset[-1], options=options, reporter=None)","iterable_zj[0], iterable_zj[1]",*dataset[:2],*dataset[:2],1
neon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neon/examples/ssd/datasets/ingest_pascalvoc.py,https://github.com/NervanaSystems/neon/tree/master/examples/ssd/datasets/ingest_pascalvoc.py,,ingest_pascal$160,"def ingest_pascal(data_dir, out_dir, img_reshape=(300, 300), overwrite=False, skip_untar=False):

    assert img_reshape is not None, ""Target image reshape required.""
    hw = '{}x{}'.format(img_reshape[0], img_reshape[1])

    datasets = ['VOC2007', 'VOC2012']
    tar_files = {'VOC2007': ['VOCtrainval_06-Nov-2007.tar', 'VOCtest_06-Nov-2007.tar'],
                 'VOC2012': ['VOCtrainval_11-May-2012.tar']}

    index_name = {'trainval': 'trainval.txt', 'test': 'test.txt'}
    manifest = {'trainval': [], 'test': []}

    root_dir = os.path.join(out_dir, 'VOCdevkit')

    train_manifest = os.path.join(root_dir, 'train_{}.csv'.format(hw))
    val_manifest = os.path.join(root_dir, 'val_{}.csv'.format(hw))

    if os.path.exists(train_manifest) and os.path.exists(val_manifest) and not overwrite:
        print(""Manifest files already found, skipping ingest."")
        print(""Use --overwrite flag to force re-ingest."")
        return

    for year in datasets:
        tags = {'trainval': [], 'test': []}

        # define paths
        if skip_untar is False:
            tarfiles = [os.path.join(data_dir, tar) for tar in tar_files[year]]
            extract_tarfiles(tarfiles, out_dir)

        # read the index files and build a list of tags to process
        # in PASCALVOC, each tag (e.g. '000032') refers to an image (000032.jpg)
        # and an annotation XML file (000032.xml)
        for sets in index_name.keys():
            index_file = os.path.join(root_dir, year, 'ImageSets', 'Main', index_name[sets])
            if os.path.exists(index_file):
                tag_list = get_tag_list(index_file)
                tags[sets].extend(tag_list)
                print('Found {} images in {}'.format(len(tag_list), index_file))

        img_folder = os.path.join(root_dir, year, 'JPEGImages')
        annot_folder = os.path.join(root_dir, year, 'Annotations')

        # create data folders to save converted images and annotations
        target_img_folder = os.path.join(root_dir, year, 'JPEGImages-converted')
        target_annot_folder = os.path.join(root_dir, year, 'Annotations-json')

        print('Processing {}'.format(year))

        util.make_dir(target_img_folder)
        util.make_dir(target_annot_folder)

        all_tags = tags['trainval'] + tags['test']  # process all the tags in our index files.

        for tag in tqdm(all_tags):

            image = os.path.join(img_folder, tag + '.jpg')
            annot = os.path.join(annot_folder, tag + '.xml')
            assert os.path.exists(image)
            assert os.path.exists(annot)

            target_image = os.path.join(target_img_folder, tag + '.jpg')
            target_annot = os.path.join(target_annot_folder, tag + '.json')

            # convert the annotations to json, including difficult objects
            convert_xml_to_json(annot, target_annot, difficult=True, img_reshape=None)
            util.resize_image(image, target_image, img_reshape=None)

            if tag in tags['trainval']:
                manifest['trainval'].append((target_image, target_annot))
            elif tag in tags['test']:
                manifest['test'].append((target_image, target_annot))

    np.random.seed(0)
    np.random.shuffle(manifest['trainval'])

    util.create_manifest(train_manifest, manifest['trainval'], root_dir)
    util.create_manifest(val_manifest, manifest['test'], root_dir)

    # write SSD CONFIG
    ssd_config = get_ssd_config(img_reshape)
    ssd_config_path = os.path.join(root_dir, 'pascalvoc_ssd_{}.cfg'.format(hw))
    util.write_ssd_config(ssd_config, ssd_config_path, True)

    # write SSD VAL CONFIG
    ssd_config_val = get_ssd_config(img_reshape, True)
    ssd_config_path_val = os.path.join(root_dir, 'pascalvoc_ssd_{}_val.cfg'.format(hw))
    util.write_ssd_config(ssd_config_val, ssd_config_path_val, True)

    config_path = os.path.join(root_dir, 'pascalvoc_{}.cfg'.format(hw))

    config = {'manifest': '[train:{}, val:{}]'.format(train_manifest, val_manifest),
              'manifest_root': root_dir,
              'epochs': 230,
              'height': img_reshape[0],
              'width': img_reshape[1],
              'ssd_config': '[train:{}, val:{}]'.format(ssd_config_path, ssd_config_path_val)
              }

    util.write_config(config, config_path)","'{}x{}'.format(img_reshape[0], img_reshape[1])",'{}x{}'.format(*img_reshape[:2]),"iterable_zj[0], iterable_zj[1]",*img_reshape[:2],*img_reshape[:2],1
LM-LSTM-CRF,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LM-LSTM-CRF/model/lm_lstm_crf.py,https://github.com/LiyuanLucasLiu/LM-LSTM-CRF/tree/master/model/lm_lstm_crf.py,LM_LSTM_CRF,word_pre_train_forward$136,"def word_pre_train_forward(self, sentence, position, hidden=None):
        """"""
        output of forward language model

        args:
            sentence (char_seq_len, batch_size): char-level representation of sentence
            position (word_seq_len, batch_size): position of blank space in char-level representation of sentence
            hidden: initial hidden state

        return:
            language model output (word_seq_len, in_doc_word), hidden
        """"""

        embeds = self.char_embeds(sentence)
        d_embeds = self.dropout(embeds)
        lstm_out, hidden = self.forw_char_lstm(d_embeds)

        tmpsize = position.size()
        position = position.unsqueeze(2).expand(tmpsize[0], tmpsize[1], self.char_hidden_dim)
        select_lstm_out = torch.gather(lstm_out, 0, position)
        d_lstm_out = self.dropout(select_lstm_out).view(-1, self.char_hidden_dim)

        if self.if_highway:
            char_out = self.forw2word(d_lstm_out)
            d_char_out = self.dropout(char_out)
        else:
            d_char_out = d_lstm_out

        pre_score = self.word_pre_train_out(d_char_out)
        return pre_score, hidden","position.unsqueeze(2).expand(tmpsize[0], tmpsize[1], self.char_hidden_dim)","position.unsqueeze(2).expand(*tmpsize[:2], self.char_hidden_dim)","iterable_zj[0], iterable_zj[1]",*tmpsize[:2],*tmpsize[:2],1
ros_comm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/tools/rosbag/scripts/fix_moved_messages.py,https://github.com/ros/ros_comm/tree/master/tools/rosbag/scripts/fix_moved_messages.py,,if_main_my$61,"if __name__ == '__main__':
    import sys
    if len(sys.argv) == 4:
        fixbags(sys.argv[1], sys.argv[2], sys.argv[3])
    else:
        print('usage: fix_moved_messages.py <name_md5_file> <inbag> <outbag>')
        exit(2)","fixbags(sys.argv[1], sys.argv[2], sys.argv[3])",fixbags(*sys.argv[1:4]),"iterable_zj[1], iterable_zj[2], iterable_zj[3]",*sys.argv[1:4],*sys.argv[1:4],1
DenseFusion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DenseFusion/lib/transformations.py,https://github.com/j96w/DenseFusion/tree/master/lib/transformations.py,,decompose_matrix$724,"def decompose_matrix(matrix):
    """"""Return sequence of transformations from transformation matrix.

    matrix : array_like
        Non-degenerative homogeneous transformation matrix

    Return tuple of:
        scale : vector of 3 scaling factors
        shear : list of shear factors for x-y, x-z, y-z axes
        angles : list of Euler angles about static x, y, z axes
        translate : translation vector along x, y, z axes
        perspective : perspective partition of matrix

    Raise ValueError if matrix is of wrong type or degenerative.

    >>> T0 = translation_matrix([1, 2, 3])
    >>> scale, shear, angles, trans, persp = decompose_matrix(T0)
    >>> T1 = translation_matrix(trans)
    >>> numpy.allclose(T0, T1)
    True
    >>> S = scale_matrix(0.123)
    >>> scale, shear, angles, trans, persp = decompose_matrix(S)
    >>> scale[0]
    0.123
    >>> R0 = euler_matrix(1, 2, 3)
    >>> scale, shear, angles, trans, persp = decompose_matrix(R0)
    >>> R1 = euler_matrix(*angles)
    >>> numpy.allclose(R0, R1)
    True

    """"""
    M = numpy.array(matrix, dtype=numpy.float64, copy=True).T
    if abs(M[3, 3]) < _EPS:
        raise ValueError('M[3, 3] is zero')
    M /= M[3, 3]
    P = M.copy()
    P[:, 3] = 0.0, 0.0, 0.0, 1.0
    if not numpy.linalg.det(P):
        raise ValueError('matrix is singular')

    scale = numpy.zeros((3, ))
    shear = [0.0, 0.0, 0.0]
    angles = [0.0, 0.0, 0.0]

    if any(abs(M[:3, 3]) > _EPS):
        perspective = numpy.dot(M[:, 3], numpy.linalg.inv(P.T))
        M[:, 3] = 0.0, 0.0, 0.0, 1.0
    else:
        perspective = numpy.array([0.0, 0.0, 0.0, 1.0])

    translate = M[3, :3].copy()
    M[3, :3] = 0.0

    row = M[:3, :3].copy()
    scale[0] = vector_norm(row[0])
    row[0] /= scale[0]
    shear[0] = numpy.dot(row[0], row[1])
    row[1] -= row[0] * shear[0]
    scale[1] = vector_norm(row[1])
    row[1] /= scale[1]
    shear[0] /= scale[1]
    shear[1] = numpy.dot(row[0], row[2])
    row[2] -= row[0] * shear[1]
    shear[2] = numpy.dot(row[1], row[2])
    row[2] -= row[1] * shear[2]
    scale[2] = vector_norm(row[2])
    row[2] /= scale[2]
    shear[1:] /= scale[2]

    if numpy.dot(row[0], numpy.cross(row[1], row[2])) < 0:
        numpy.negative(scale, scale)
        numpy.negative(row, row)

    angles[1] = math.asin(-row[0, 2])
    if math.cos(angles[1]):
        angles[0] = math.atan2(row[1, 2], row[2, 2])
        angles[2] = math.atan2(row[0, 1], row[0, 0])
    else:
        # angles[0] = math.atan2(row[1, 0], row[1, 1])
        angles[0] = math.atan2(-row[2, 1], row[1, 1])
        angles[2] = 0.0

    return scale, shear, angles, translate, perspective","numpy.dot(row[0], row[1])",numpy.dot(*row[:2]),"iterable_zj[0], iterable_zj[1]",*row[:2],*row[:2],1
DenseFusion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DenseFusion/lib/transformations.py,https://github.com/j96w/DenseFusion/tree/master/lib/transformations.py,,decompose_matrix$724,"def decompose_matrix(matrix):
    """"""Return sequence of transformations from transformation matrix.

    matrix : array_like
        Non-degenerative homogeneous transformation matrix

    Return tuple of:
        scale : vector of 3 scaling factors
        shear : list of shear factors for x-y, x-z, y-z axes
        angles : list of Euler angles about static x, y, z axes
        translate : translation vector along x, y, z axes
        perspective : perspective partition of matrix

    Raise ValueError if matrix is of wrong type or degenerative.

    >>> T0 = translation_matrix([1, 2, 3])
    >>> scale, shear, angles, trans, persp = decompose_matrix(T0)
    >>> T1 = translation_matrix(trans)
    >>> numpy.allclose(T0, T1)
    True
    >>> S = scale_matrix(0.123)
    >>> scale, shear, angles, trans, persp = decompose_matrix(S)
    >>> scale[0]
    0.123
    >>> R0 = euler_matrix(1, 2, 3)
    >>> scale, shear, angles, trans, persp = decompose_matrix(R0)
    >>> R1 = euler_matrix(*angles)
    >>> numpy.allclose(R0, R1)
    True

    """"""
    M = numpy.array(matrix, dtype=numpy.float64, copy=True).T
    if abs(M[3, 3]) < _EPS:
        raise ValueError('M[3, 3] is zero')
    M /= M[3, 3]
    P = M.copy()
    P[:, 3] = 0.0, 0.0, 0.0, 1.0
    if not numpy.linalg.det(P):
        raise ValueError('matrix is singular')

    scale = numpy.zeros((3, ))
    shear = [0.0, 0.0, 0.0]
    angles = [0.0, 0.0, 0.0]

    if any(abs(M[:3, 3]) > _EPS):
        perspective = numpy.dot(M[:, 3], numpy.linalg.inv(P.T))
        M[:, 3] = 0.0, 0.0, 0.0, 1.0
    else:
        perspective = numpy.array([0.0, 0.0, 0.0, 1.0])

    translate = M[3, :3].copy()
    M[3, :3] = 0.0

    row = M[:3, :3].copy()
    scale[0] = vector_norm(row[0])
    row[0] /= scale[0]
    shear[0] = numpy.dot(row[0], row[1])
    row[1] -= row[0] * shear[0]
    scale[1] = vector_norm(row[1])
    row[1] /= scale[1]
    shear[0] /= scale[1]
    shear[1] = numpy.dot(row[0], row[2])
    row[2] -= row[0] * shear[1]
    shear[2] = numpy.dot(row[1], row[2])
    row[2] -= row[1] * shear[2]
    scale[2] = vector_norm(row[2])
    row[2] /= scale[2]
    shear[1:] /= scale[2]

    if numpy.dot(row[0], numpy.cross(row[1], row[2])) < 0:
        numpy.negative(scale, scale)
        numpy.negative(row, row)

    angles[1] = math.asin(-row[0, 2])
    if math.cos(angles[1]):
        angles[0] = math.atan2(row[1, 2], row[2, 2])
        angles[2] = math.atan2(row[0, 1], row[0, 0])
    else:
        # angles[0] = math.atan2(row[1, 0], row[1, 1])
        angles[0] = math.atan2(-row[2, 1], row[1, 1])
        angles[2] = 0.0

    return scale, shear, angles, translate, perspective","numpy.dot(row[0], row[2])",numpy.dot(*row[::2]),"iterable_zj[0], iterable_zj[2]",*row[::2],*row[:4:2],0
DenseFusion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DenseFusion/lib/transformations.py,https://github.com/j96w/DenseFusion/tree/master/lib/transformations.py,,decompose_matrix$724,"def decompose_matrix(matrix):
    """"""Return sequence of transformations from transformation matrix.

    matrix : array_like
        Non-degenerative homogeneous transformation matrix

    Return tuple of:
        scale : vector of 3 scaling factors
        shear : list of shear factors for x-y, x-z, y-z axes
        angles : list of Euler angles about static x, y, z axes
        translate : translation vector along x, y, z axes
        perspective : perspective partition of matrix

    Raise ValueError if matrix is of wrong type or degenerative.

    >>> T0 = translation_matrix([1, 2, 3])
    >>> scale, shear, angles, trans, persp = decompose_matrix(T0)
    >>> T1 = translation_matrix(trans)
    >>> numpy.allclose(T0, T1)
    True
    >>> S = scale_matrix(0.123)
    >>> scale, shear, angles, trans, persp = decompose_matrix(S)
    >>> scale[0]
    0.123
    >>> R0 = euler_matrix(1, 2, 3)
    >>> scale, shear, angles, trans, persp = decompose_matrix(R0)
    >>> R1 = euler_matrix(*angles)
    >>> numpy.allclose(R0, R1)
    True

    """"""
    M = numpy.array(matrix, dtype=numpy.float64, copy=True).T
    if abs(M[3, 3]) < _EPS:
        raise ValueError('M[3, 3] is zero')
    M /= M[3, 3]
    P = M.copy()
    P[:, 3] = 0.0, 0.0, 0.0, 1.0
    if not numpy.linalg.det(P):
        raise ValueError('matrix is singular')

    scale = numpy.zeros((3, ))
    shear = [0.0, 0.0, 0.0]
    angles = [0.0, 0.0, 0.0]

    if any(abs(M[:3, 3]) > _EPS):
        perspective = numpy.dot(M[:, 3], numpy.linalg.inv(P.T))
        M[:, 3] = 0.0, 0.0, 0.0, 1.0
    else:
        perspective = numpy.array([0.0, 0.0, 0.0, 1.0])

    translate = M[3, :3].copy()
    M[3, :3] = 0.0

    row = M[:3, :3].copy()
    scale[0] = vector_norm(row[0])
    row[0] /= scale[0]
    shear[0] = numpy.dot(row[0], row[1])
    row[1] -= row[0] * shear[0]
    scale[1] = vector_norm(row[1])
    row[1] /= scale[1]
    shear[0] /= scale[1]
    shear[1] = numpy.dot(row[0], row[2])
    row[2] -= row[0] * shear[1]
    shear[2] = numpy.dot(row[1], row[2])
    row[2] -= row[1] * shear[2]
    scale[2] = vector_norm(row[2])
    row[2] /= scale[2]
    shear[1:] /= scale[2]

    if numpy.dot(row[0], numpy.cross(row[1], row[2])) < 0:
        numpy.negative(scale, scale)
        numpy.negative(row, row)

    angles[1] = math.asin(-row[0, 2])
    if math.cos(angles[1]):
        angles[0] = math.atan2(row[1, 2], row[2, 2])
        angles[2] = math.atan2(row[0, 1], row[0, 0])
    else:
        # angles[0] = math.atan2(row[1, 0], row[1, 1])
        angles[0] = math.atan2(-row[2, 1], row[1, 1])
        angles[2] = 0.0

    return scale, shear, angles, translate, perspective","numpy.dot(row[1], row[2])",numpy.dot(*row[1:3]),"iterable_zj[1], iterable_zj[2]",*row[1:3],*row[1:3],1
DenseFusion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DenseFusion/lib/transformations.py,https://github.com/j96w/DenseFusion/tree/master/lib/transformations.py,,decompose_matrix$724,"def decompose_matrix(matrix):
    """"""Return sequence of transformations from transformation matrix.

    matrix : array_like
        Non-degenerative homogeneous transformation matrix

    Return tuple of:
        scale : vector of 3 scaling factors
        shear : list of shear factors for x-y, x-z, y-z axes
        angles : list of Euler angles about static x, y, z axes
        translate : translation vector along x, y, z axes
        perspective : perspective partition of matrix

    Raise ValueError if matrix is of wrong type or degenerative.

    >>> T0 = translation_matrix([1, 2, 3])
    >>> scale, shear, angles, trans, persp = decompose_matrix(T0)
    >>> T1 = translation_matrix(trans)
    >>> numpy.allclose(T0, T1)
    True
    >>> S = scale_matrix(0.123)
    >>> scale, shear, angles, trans, persp = decompose_matrix(S)
    >>> scale[0]
    0.123
    >>> R0 = euler_matrix(1, 2, 3)
    >>> scale, shear, angles, trans, persp = decompose_matrix(R0)
    >>> R1 = euler_matrix(*angles)
    >>> numpy.allclose(R0, R1)
    True

    """"""
    M = numpy.array(matrix, dtype=numpy.float64, copy=True).T
    if abs(M[3, 3]) < _EPS:
        raise ValueError('M[3, 3] is zero')
    M /= M[3, 3]
    P = M.copy()
    P[:, 3] = 0.0, 0.0, 0.0, 1.0
    if not numpy.linalg.det(P):
        raise ValueError('matrix is singular')

    scale = numpy.zeros((3, ))
    shear = [0.0, 0.0, 0.0]
    angles = [0.0, 0.0, 0.0]

    if any(abs(M[:3, 3]) > _EPS):
        perspective = numpy.dot(M[:, 3], numpy.linalg.inv(P.T))
        M[:, 3] = 0.0, 0.0, 0.0, 1.0
    else:
        perspective = numpy.array([0.0, 0.0, 0.0, 1.0])

    translate = M[3, :3].copy()
    M[3, :3] = 0.0

    row = M[:3, :3].copy()
    scale[0] = vector_norm(row[0])
    row[0] /= scale[0]
    shear[0] = numpy.dot(row[0], row[1])
    row[1] -= row[0] * shear[0]
    scale[1] = vector_norm(row[1])
    row[1] /= scale[1]
    shear[0] /= scale[1]
    shear[1] = numpy.dot(row[0], row[2])
    row[2] -= row[0] * shear[1]
    shear[2] = numpy.dot(row[1], row[2])
    row[2] -= row[1] * shear[2]
    scale[2] = vector_norm(row[2])
    row[2] /= scale[2]
    shear[1:] /= scale[2]

    if numpy.dot(row[0], numpy.cross(row[1], row[2])) < 0:
        numpy.negative(scale, scale)
        numpy.negative(row, row)

    angles[1] = math.asin(-row[0, 2])
    if math.cos(angles[1]):
        angles[0] = math.atan2(row[1, 2], row[2, 2])
        angles[2] = math.atan2(row[0, 1], row[0, 0])
    else:
        # angles[0] = math.atan2(row[1, 0], row[1, 1])
        angles[0] = math.atan2(-row[2, 1], row[1, 1])
        angles[2] = 0.0

    return scale, shear, angles, translate, perspective","numpy.cross(row[1], row[2])",numpy.cross(*row[1:3]),"iterable_zj[1], iterable_zj[2]",*row[1:3],*row[1:3],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/visualization/test_circuit_text_drawer.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/visualization/test_circuit_text_drawer.py,TestTextDrawerGatesInCircuit,test_text_crx$505,"def test_text_crx(self):
        """"""crx drawing.""""""
        expected = ""\n"".join(
            [
                ""                   "",
                ""q_0: |0> Rx(/2) "",
                ""        "",
                ""q_1: |0> Rx(/2) "",
                ""                  "",
                ""q_2: |0>"",
                ""                              "",
            ]
        )
        qr = QuantumRegister(3, ""q"")
        circuit = QuantumCircuit(qr)
        circuit.crx(pi / 2, qr[0], qr[1])
        circuit.crx(pi / 2, qr[2], qr[0])
        self.assertEqual(str(_text_circuit_drawer(circuit)), expected)","circuit.crx(pi / 2, qr[0], qr[1])","circuit.crx(pi / 2, *qr[:2])","iterable_zj[0], iterable_zj[1]",*qr[:2],*qr[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/contrib/cudnn.py,https://github.com/apache/tvm/tree/master/python/tvm/contrib/cudnn.py,,conv_backward_filter$785,"def conv_backward_filter(
    dy, x, kernel_size, pad, stride, dilation, conv_mode, tensor_format, conv_dtype, groups=1
):
    """"""Create a CuDNN extern op that computes the gradient of 2D convolution with respect to weight.

    Parameters
    ----------
    dy: Tensor
        output gradient
    x: Tensor
        input tensor
    kernel_size: a pair of int
        The spatial size of the corresponding forward convolution kernel
    pad: int or list
        padding
    stride: int or list
        stride
    dilation: int or list
        dilation
    conv_mode: int
        0: CUDNN_CONVOLUTION
        1: CUDNN_CROSS_CORRELATION
    tensor_format: int
        0: CUDNN_TENSOR_NCHW
        1: CUDNN_TENSOR_NHWC
    conv_dtype: str
        convolution type
    groups: int
        the number of groups

    Returns
    -------
    dw: Tensor
        wgrad tensor
    """"""
    dims = len(x.shape)
    assert dims == 4

    conv_dtype = x.dtype if conv_dtype is None else conv_dtype
    pad, stride, dilation, _, _ = _prepare_global_func_params(dims - 2, pad, stride, dilation)
    filter_h, filter_w = kernel_size

    x_shape = list(x.shape)

    assert isinstance(
        x.shape[0], tvm.tir.expr.IntImm
    ), ""Dynamic batch is not supported for cudnn conv2d backwad filter yet.""

    ic_ind = 1 if tensor_format == 0 else 3

    if groups > 1:
        assert (
            x_shape[ic_ind] == dy.shape[ic_ind] and x_shape[ic_ind] == groups
        ), ""Only depthwise wgrad supported for groups > 1.""
        ic = 1
    else:
        ic = x_shape[ic_ind]

    if tensor_format == 0:
        dw_shape = [dy.shape[1], ic, filter_h, filter_w]
    else:
        dw_shape = [dy.shape[3], filter_h, filter_w, ic]

    algo = conv_backward_filter_find_algo(
        tensor_format,
        pad,
        stride,
        dilation,
        list(dy.shape),
        list(x.shape),
        dw_shape,
        x.dtype,
        conv_dtype,
        groups,
    )

    return te.extern(
        dw_shape,
        [dy, x],
        lambda ins, outs: tvm.tir.call_packed(
            ""tvm.contrib.cudnn.conv2d.backward_filter"",
            conv_mode,
            tensor_format,
            algo,
            pad[0],
            pad[1],
            stride[0],
            stride[1],
            dilation[0],
            dilation[1],
            ins[0],
            ins[1],
            outs[0],
            conv_dtype,
            groups,
        ),
        name=""dw"",
    )","tvm.tir.call_packed('tvm.contrib.cudnn.conv2d.backward_filter', conv_mode, tensor_format, algo, pad[0], pad[1], stride[0], stride[1], dilation[0], dilation[1], ins[0], ins[1], outs[0], conv_dtype, groups)","tvm.tir.call_packed('tvm.contrib.cudnn.conv2d.backward_filter', conv_mode, tensor_format, algo, pad[0], pad[1], stride[0], stride[1], *dilation[:2], ins[0], ins[1], outs[0], conv_dtype, groups)","iterable_zj[0], iterable_zj[1]",*dilation[:2],*dilation[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/contrib/cudnn.py,https://github.com/apache/tvm/tree/master/python/tvm/contrib/cudnn.py,,conv_backward_filter$785,"def conv_backward_filter(
    dy, x, kernel_size, pad, stride, dilation, conv_mode, tensor_format, conv_dtype, groups=1
):
    """"""Create a CuDNN extern op that computes the gradient of 2D convolution with respect to weight.

    Parameters
    ----------
    dy: Tensor
        output gradient
    x: Tensor
        input tensor
    kernel_size: a pair of int
        The spatial size of the corresponding forward convolution kernel
    pad: int or list
        padding
    stride: int or list
        stride
    dilation: int or list
        dilation
    conv_mode: int
        0: CUDNN_CONVOLUTION
        1: CUDNN_CROSS_CORRELATION
    tensor_format: int
        0: CUDNN_TENSOR_NCHW
        1: CUDNN_TENSOR_NHWC
    conv_dtype: str
        convolution type
    groups: int
        the number of groups

    Returns
    -------
    dw: Tensor
        wgrad tensor
    """"""
    dims = len(x.shape)
    assert dims == 4

    conv_dtype = x.dtype if conv_dtype is None else conv_dtype
    pad, stride, dilation, _, _ = _prepare_global_func_params(dims - 2, pad, stride, dilation)
    filter_h, filter_w = kernel_size

    x_shape = list(x.shape)

    assert isinstance(
        x.shape[0], tvm.tir.expr.IntImm
    ), ""Dynamic batch is not supported for cudnn conv2d backwad filter yet.""

    ic_ind = 1 if tensor_format == 0 else 3

    if groups > 1:
        assert (
            x_shape[ic_ind] == dy.shape[ic_ind] and x_shape[ic_ind] == groups
        ), ""Only depthwise wgrad supported for groups > 1.""
        ic = 1
    else:
        ic = x_shape[ic_ind]

    if tensor_format == 0:
        dw_shape = [dy.shape[1], ic, filter_h, filter_w]
    else:
        dw_shape = [dy.shape[3], filter_h, filter_w, ic]

    algo = conv_backward_filter_find_algo(
        tensor_format,
        pad,
        stride,
        dilation,
        list(dy.shape),
        list(x.shape),
        dw_shape,
        x.dtype,
        conv_dtype,
        groups,
    )

    return te.extern(
        dw_shape,
        [dy, x],
        lambda ins, outs: tvm.tir.call_packed(
            ""tvm.contrib.cudnn.conv2d.backward_filter"",
            conv_mode,
            tensor_format,
            algo,
            pad[0],
            pad[1],
            stride[0],
            stride[1],
            dilation[0],
            dilation[1],
            ins[0],
            ins[1],
            outs[0],
            conv_dtype,
            groups,
        ),
        name=""dw"",
    )","tvm.tir.call_packed('tvm.contrib.cudnn.conv2d.backward_filter', conv_mode, tensor_format, algo, pad[0], pad[1], stride[0], stride[1], dilation[0], dilation[1], ins[0], ins[1], outs[0], conv_dtype, groups)","tvm.tir.call_packed('tvm.contrib.cudnn.conv2d.backward_filter', conv_mode, tensor_format, algo, pad[0], pad[1], stride[0], stride[1], dilation[0], dilation[1], *ins[:2], outs[0], conv_dtype, groups)","iterable_zj[0], iterable_zj[1]",*ins[:2],*ins[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/contrib/cudnn.py,https://github.com/apache/tvm/tree/master/python/tvm/contrib/cudnn.py,,conv_backward_filter$785,"def conv_backward_filter(
    dy, x, kernel_size, pad, stride, dilation, conv_mode, tensor_format, conv_dtype, groups=1
):
    """"""Create a CuDNN extern op that computes the gradient of 2D convolution with respect to weight.

    Parameters
    ----------
    dy: Tensor
        output gradient
    x: Tensor
        input tensor
    kernel_size: a pair of int
        The spatial size of the corresponding forward convolution kernel
    pad: int or list
        padding
    stride: int or list
        stride
    dilation: int or list
        dilation
    conv_mode: int
        0: CUDNN_CONVOLUTION
        1: CUDNN_CROSS_CORRELATION
    tensor_format: int
        0: CUDNN_TENSOR_NCHW
        1: CUDNN_TENSOR_NHWC
    conv_dtype: str
        convolution type
    groups: int
        the number of groups

    Returns
    -------
    dw: Tensor
        wgrad tensor
    """"""
    dims = len(x.shape)
    assert dims == 4

    conv_dtype = x.dtype if conv_dtype is None else conv_dtype
    pad, stride, dilation, _, _ = _prepare_global_func_params(dims - 2, pad, stride, dilation)
    filter_h, filter_w = kernel_size

    x_shape = list(x.shape)

    assert isinstance(
        x.shape[0], tvm.tir.expr.IntImm
    ), ""Dynamic batch is not supported for cudnn conv2d backwad filter yet.""

    ic_ind = 1 if tensor_format == 0 else 3

    if groups > 1:
        assert (
            x_shape[ic_ind] == dy.shape[ic_ind] and x_shape[ic_ind] == groups
        ), ""Only depthwise wgrad supported for groups > 1.""
        ic = 1
    else:
        ic = x_shape[ic_ind]

    if tensor_format == 0:
        dw_shape = [dy.shape[1], ic, filter_h, filter_w]
    else:
        dw_shape = [dy.shape[3], filter_h, filter_w, ic]

    algo = conv_backward_filter_find_algo(
        tensor_format,
        pad,
        stride,
        dilation,
        list(dy.shape),
        list(x.shape),
        dw_shape,
        x.dtype,
        conv_dtype,
        groups,
    )

    return te.extern(
        dw_shape,
        [dy, x],
        lambda ins, outs: tvm.tir.call_packed(
            ""tvm.contrib.cudnn.conv2d.backward_filter"",
            conv_mode,
            tensor_format,
            algo,
            pad[0],
            pad[1],
            stride[0],
            stride[1],
            dilation[0],
            dilation[1],
            ins[0],
            ins[1],
            outs[0],
            conv_dtype,
            groups,
        ),
        name=""dw"",
    )","tvm.tir.call_packed('tvm.contrib.cudnn.conv2d.backward_filter', conv_mode, tensor_format, algo, pad[0], pad[1], stride[0], stride[1], dilation[0], dilation[1], ins[0], ins[1], outs[0], conv_dtype, groups)","tvm.tir.call_packed('tvm.contrib.cudnn.conv2d.backward_filter', conv_mode, tensor_format, algo, *pad[:2], stride[0], stride[1], dilation[0], dilation[1], ins[0], ins[1], outs[0], conv_dtype, groups)","iterable_zj[0], iterable_zj[1]",*pad[:2],*pad[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/contrib/cudnn.py,https://github.com/apache/tvm/tree/master/python/tvm/contrib/cudnn.py,,conv_backward_filter$785,"def conv_backward_filter(
    dy, x, kernel_size, pad, stride, dilation, conv_mode, tensor_format, conv_dtype, groups=1
):
    """"""Create a CuDNN extern op that computes the gradient of 2D convolution with respect to weight.

    Parameters
    ----------
    dy: Tensor
        output gradient
    x: Tensor
        input tensor
    kernel_size: a pair of int
        The spatial size of the corresponding forward convolution kernel
    pad: int or list
        padding
    stride: int or list
        stride
    dilation: int or list
        dilation
    conv_mode: int
        0: CUDNN_CONVOLUTION
        1: CUDNN_CROSS_CORRELATION
    tensor_format: int
        0: CUDNN_TENSOR_NCHW
        1: CUDNN_TENSOR_NHWC
    conv_dtype: str
        convolution type
    groups: int
        the number of groups

    Returns
    -------
    dw: Tensor
        wgrad tensor
    """"""
    dims = len(x.shape)
    assert dims == 4

    conv_dtype = x.dtype if conv_dtype is None else conv_dtype
    pad, stride, dilation, _, _ = _prepare_global_func_params(dims - 2, pad, stride, dilation)
    filter_h, filter_w = kernel_size

    x_shape = list(x.shape)

    assert isinstance(
        x.shape[0], tvm.tir.expr.IntImm
    ), ""Dynamic batch is not supported for cudnn conv2d backwad filter yet.""

    ic_ind = 1 if tensor_format == 0 else 3

    if groups > 1:
        assert (
            x_shape[ic_ind] == dy.shape[ic_ind] and x_shape[ic_ind] == groups
        ), ""Only depthwise wgrad supported for groups > 1.""
        ic = 1
    else:
        ic = x_shape[ic_ind]

    if tensor_format == 0:
        dw_shape = [dy.shape[1], ic, filter_h, filter_w]
    else:
        dw_shape = [dy.shape[3], filter_h, filter_w, ic]

    algo = conv_backward_filter_find_algo(
        tensor_format,
        pad,
        stride,
        dilation,
        list(dy.shape),
        list(x.shape),
        dw_shape,
        x.dtype,
        conv_dtype,
        groups,
    )

    return te.extern(
        dw_shape,
        [dy, x],
        lambda ins, outs: tvm.tir.call_packed(
            ""tvm.contrib.cudnn.conv2d.backward_filter"",
            conv_mode,
            tensor_format,
            algo,
            pad[0],
            pad[1],
            stride[0],
            stride[1],
            dilation[0],
            dilation[1],
            ins[0],
            ins[1],
            outs[0],
            conv_dtype,
            groups,
        ),
        name=""dw"",
    )","tvm.tir.call_packed('tvm.contrib.cudnn.conv2d.backward_filter', conv_mode, tensor_format, algo, pad[0], pad[1], stride[0], stride[1], dilation[0], dilation[1], ins[0], ins[1], outs[0], conv_dtype, groups)","tvm.tir.call_packed('tvm.contrib.cudnn.conv2d.backward_filter', conv_mode, tensor_format, algo, pad[0], pad[1], *stride[:2], dilation[0], dilation[1], ins[0], ins[1], outs[0], conv_dtype, groups)","iterable_zj[0], iterable_zj[1]",*stride[:2],*stride[:2],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_dense_layout.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_dense_layout.py,TestDenseLayout,test_5q_circuit_20q_with_if_else$197,"def test_5q_circuit_20q_with_if_else(self):
        """"""Test layout works finds a dense 5q subgraph in a 19q heavy hex target.""""""
        qr = QuantumRegister(5, ""q"")
        cr = ClassicalRegister(5)
        circuit = QuantumCircuit(qr, cr)
        true_body = QuantumCircuit(qr, cr)
        false_body = QuantumCircuit(qr, cr)
        true_body.cx(qr[0], qr[3])
        true_body.cx(qr[3], qr[4])
        false_body.cx(qr[3], qr[1])
        false_body.cx(qr[0], qr[2])
        circuit.if_else((cr, 0), true_body, false_body, qr, cr)
        circuit.cx(0, 4)

        dag = circuit_to_dag(circuit)
        pass_ = DenseLayout(CouplingMap(self.cmap20))
        pass_.run(dag)
        layout = pass_.property_set[""layout""]
        self.assertEqual(layout[qr[0]], 11)
        self.assertEqual(layout[qr[1]], 10)
        self.assertEqual(layout[qr[2]], 6)
        self.assertEqual(layout[qr[3]], 5)
        self.assertEqual(layout[qr[4]], 0)","true_body.cx(qr[0], qr[3])","true_body.cx(*qr[0], qr[3])","iterable_zj[0], iterable_zj[3]","*qr[0], qr[3]",*qr[:6:3],0
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_dense_layout.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_dense_layout.py,TestDenseLayout,test_5q_circuit_20q_with_if_else$197,"def test_5q_circuit_20q_with_if_else(self):
        """"""Test layout works finds a dense 5q subgraph in a 19q heavy hex target.""""""
        qr = QuantumRegister(5, ""q"")
        cr = ClassicalRegister(5)
        circuit = QuantumCircuit(qr, cr)
        true_body = QuantumCircuit(qr, cr)
        false_body = QuantumCircuit(qr, cr)
        true_body.cx(qr[0], qr[3])
        true_body.cx(qr[3], qr[4])
        false_body.cx(qr[3], qr[1])
        false_body.cx(qr[0], qr[2])
        circuit.if_else((cr, 0), true_body, false_body, qr, cr)
        circuit.cx(0, 4)

        dag = circuit_to_dag(circuit)
        pass_ = DenseLayout(CouplingMap(self.cmap20))
        pass_.run(dag)
        layout = pass_.property_set[""layout""]
        self.assertEqual(layout[qr[0]], 11)
        self.assertEqual(layout[qr[1]], 10)
        self.assertEqual(layout[qr[2]], 6)
        self.assertEqual(layout[qr[3]], 5)
        self.assertEqual(layout[qr[4]], 0)","true_body.cx(qr[3], qr[4])",true_body.cx(*qr[3:5]),"iterable_zj[3], iterable_zj[4]",*qr[3:5],*qr[3:5],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_dense_layout.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_dense_layout.py,TestDenseLayout,test_5q_circuit_20q_with_if_else$197,"def test_5q_circuit_20q_with_if_else(self):
        """"""Test layout works finds a dense 5q subgraph in a 19q heavy hex target.""""""
        qr = QuantumRegister(5, ""q"")
        cr = ClassicalRegister(5)
        circuit = QuantumCircuit(qr, cr)
        true_body = QuantumCircuit(qr, cr)
        false_body = QuantumCircuit(qr, cr)
        true_body.cx(qr[0], qr[3])
        true_body.cx(qr[3], qr[4])
        false_body.cx(qr[3], qr[1])
        false_body.cx(qr[0], qr[2])
        circuit.if_else((cr, 0), true_body, false_body, qr, cr)
        circuit.cx(0, 4)

        dag = circuit_to_dag(circuit)
        pass_ = DenseLayout(CouplingMap(self.cmap20))
        pass_.run(dag)
        layout = pass_.property_set[""layout""]
        self.assertEqual(layout[qr[0]], 11)
        self.assertEqual(layout[qr[1]], 10)
        self.assertEqual(layout[qr[2]], 6)
        self.assertEqual(layout[qr[3]], 5)
        self.assertEqual(layout[qr[4]], 0)","false_body.cx(qr[0], qr[2])",false_body.cx(*qr[::2]),"iterable_zj[0], iterable_zj[2]",*qr[::2],*qr[:4:2],0
object_detector_app,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/object_detector_app/object_detection/meta_architectures/faster_rcnn_meta_arch.py,https://github.com/datitran/object_detector_app/tree/master/object_detection/meta_architectures/faster_rcnn_meta_arch.py,FasterRCNNMetaArch,_format_groundtruth_data$925,"def _format_groundtruth_data(self, image_shape):
    """"""Helper function for preparing groundtruth data for target assignment.

    In order to be consistent with the model.DetectionModel interface,
    groundtruth boxes are specified in normalized coordinates and classes are
    specified as label indices with no assumed background category.  To prepare
    for target assignment, we:
    1) convert boxes to absolute coordinates,
    2) add a background class at class index 0

    Args:
      image_shape: A 1-D int32 tensor of shape [4] representing the shape of the
        input image batch.

    Returns:
      groundtruth_boxlists: A list of BoxLists containing (absolute) coordinates
        of the groundtruth boxes.
      groundtruth_classes_with_background_list: A list of 2-D one-hot
        (or k-hot) tensors of shape [num_boxes, num_classes+1] containing the
        class targets with the 0th index assumed to map to the background class.
    """"""
    groundtruth_boxlists = [
        box_list_ops.to_absolute_coordinates(
            box_list.BoxList(boxes), image_shape[1], image_shape[2])
        for boxes in self.groundtruth_lists(fields.BoxListFields.boxes)]
    groundtruth_classes_with_background_list = [
        tf.to_float(
            tf.pad(one_hot_encoding, [[0, 0], [1, 0]], mode='CONSTANT'))
        for one_hot_encoding in self.groundtruth_lists(
            fields.BoxListFields.classes)]
    return groundtruth_boxlists, groundtruth_classes_with_background_list","box_list_ops.to_absolute_coordinates(box_list.BoxList(boxes), image_shape[1], image_shape[2])","box_list_ops.to_absolute_coordinates(box_list.BoxList(boxes), *image_shape[1:3])","iterable_zj[1], iterable_zj[2]",*image_shape[1:3],*image_shape[1:3],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/contrib/rocblas.py,https://github.com/apache/tvm/tree/master/python/tvm/contrib/rocblas.py,,matmul$22,"def matmul(lhs, rhs, transa=False, transb=False):
    """"""Create an extern op that compute matrix mult of A and rhs with rocBLAS

    Parameters
    ----------
    lhs : Tensor
        The left matrix operand
    rhs : Tensor
        The right matrix operand
    transa : bool
        Whether transpose lhs
    transb : bool
        Whether transpose rhs

    Returns
    -------
    C : Tensor
        The result tensor.
    """"""
    n = lhs.shape[1] if transa else lhs.shape[0]
    m = rhs.shape[0] if transb else rhs.shape[1]
    return te.extern(
        (n, m),
        [lhs, rhs],
        lambda ins, outs: tvm.tir.call_packed(
            ""tvm.contrib.rocblas.matmul"", ins[0], ins[1], outs[0], transa, transb
        ),
        name=""C"",
    )","tvm.tir.call_packed('tvm.contrib.rocblas.matmul', ins[0], ins[1], outs[0], transa, transb)","tvm.tir.call_packed('tvm.contrib.rocblas.matmul', *ins[:2], outs[0], transa, transb)","iterable_zj[0], iterable_zj[1]",*ins[:2],*ins[:2],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_apply_layout.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_apply_layout.py,TestApplyLayout,test_circuit_with_swap_gate$74,"def test_circuit_with_swap_gate(self):
        """"""Test if a virtual circuit with one swap gate is transformed into
        a circuit with physical qubits.

        [Circuit with virtual qubits]
          v0:--X---.---M(v0->c0)
               |   |
          v1:--X---|---M(v1->c1)
                   |
          v2:-----(+)--M(v2->c2)

         Initial layout: {v[0]: 2, v[1]: 1, v[2]: 0}

        [Circuit with physical qubits]
          q2:--X---.---M(q2->c0)
               |   |
          q1:--X---|---M(q1->c1)
                   |
          q0:-----(+)--M(q0->c2)
        """"""
        v = QuantumRegister(3, ""v"")
        cr = ClassicalRegister(3, ""c"")
        circuit = QuantumCircuit(v, cr)
        circuit.swap(v[0], v[1])
        circuit.cx(v[0], v[2])
        circuit.measure(v[0], cr[0])
        circuit.measure(v[1], cr[1])
        circuit.measure(v[2], cr[2])

        q = QuantumRegister(3, ""q"")
        expected = QuantumCircuit(q, cr)
        expected.swap(q[2], q[1])
        expected.cx(q[2], q[0])
        expected.measure(q[2], cr[0])
        expected.measure(q[1], cr[1])
        expected.measure(q[0], cr[2])

        dag = circuit_to_dag(circuit)
        pass_ = ApplyLayout()
        pass_.property_set[""layout""] = Layout({v[0]: 2, v[1]: 1, v[2]: 0})
        after = pass_.run(dag)

        self.assertEqual(circuit_to_dag(expected), after)","circuit.swap(v[0], v[1])",circuit.swap(*v[:2]),"iterable_zj[0], iterable_zj[1]",*v[:2],*v[:2],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_apply_layout.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_apply_layout.py,TestApplyLayout,test_circuit_with_swap_gate$74,"def test_circuit_with_swap_gate(self):
        """"""Test if a virtual circuit with one swap gate is transformed into
        a circuit with physical qubits.

        [Circuit with virtual qubits]
          v0:--X---.---M(v0->c0)
               |   |
          v1:--X---|---M(v1->c1)
                   |
          v2:-----(+)--M(v2->c2)

         Initial layout: {v[0]: 2, v[1]: 1, v[2]: 0}

        [Circuit with physical qubits]
          q2:--X---.---M(q2->c0)
               |   |
          q1:--X---|---M(q1->c1)
                   |
          q0:-----(+)--M(q0->c2)
        """"""
        v = QuantumRegister(3, ""v"")
        cr = ClassicalRegister(3, ""c"")
        circuit = QuantumCircuit(v, cr)
        circuit.swap(v[0], v[1])
        circuit.cx(v[0], v[2])
        circuit.measure(v[0], cr[0])
        circuit.measure(v[1], cr[1])
        circuit.measure(v[2], cr[2])

        q = QuantumRegister(3, ""q"")
        expected = QuantumCircuit(q, cr)
        expected.swap(q[2], q[1])
        expected.cx(q[2], q[0])
        expected.measure(q[2], cr[0])
        expected.measure(q[1], cr[1])
        expected.measure(q[0], cr[2])

        dag = circuit_to_dag(circuit)
        pass_ = ApplyLayout()
        pass_.property_set[""layout""] = Layout({v[0]: 2, v[1]: 1, v[2]: 0})
        after = pass_.run(dag)

        self.assertEqual(circuit_to_dag(expected), after)","circuit.cx(v[0], v[2])",circuit.cx(*v[::2]),"iterable_zj[0], iterable_zj[2]",*v[::2],*v[:4:2],0
pynndescent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pynndescent/pynndescent/pynndescent_.py,https://github.com/lmcinnes/pynndescent/tree/master/pynndescent/pynndescent_.py,NNDescent,_init_search_function$1190,"def _init_search_function(self):

        if self.verbose:
            print(ts(), ""Building and compiling search function"")

        if self.tree_init:
            tree_hyperplanes = self._search_forest[0].hyperplanes
            tree_offsets = self._search_forest[0].offsets
            tree_indices = self._search_forest[0].indices
            tree_children = self._search_forest[0].children

            @numba.njit(
                [
                    numba.types.Array(numba.types.int32, 1, ""C"", readonly=True)(
                        numba.types.Array(numba.types.float32, 1, ""C"", readonly=True),
                        numba.types.Array(numba.types.int64, 1, ""C"", readonly=False),
                    )
                ],
                locals={""node"": numba.types.uint32, ""side"": numba.types.boolean},
            )
            def tree_search_closure(point, rng_state):
                node = 0
                while tree_children[node, 0] > 0:
                    side = select_side(
                        tree_hyperplanes[node], tree_offsets[node], point, rng_state
                    )
                    if side == 0:
                        node = tree_children[node, 0]
                    else:
                        node = tree_children[node, 1]

                return -tree_children[node]

            self._tree_search = tree_search_closure
        else:

            @numba.njit()
            def tree_search_closure(point, rng_state):
                return (0, 0)

            self._tree_search = tree_search_closure
            tree_indices = np.zeros(1, dtype=np.int64)

        alternative_dot = pynnd_dist.alternative_dot
        alternative_cosine = pynnd_dist.alternative_cosine

        data = self._raw_data
        indptr = self._search_graph.indptr
        indices = self._search_graph.indices
        dist = self._distance_func
        n_neighbors = self.n_neighbors
        parallel_search = self.parallel_batch_queries

        @numba.njit(
            fastmath=True,
            locals={
                ""current_query"": numba.types.float32[::1],
                ""i"": numba.types.uint32,
                ""j"": numba.types.uint32,
                ""heap_priorities"": numba.types.float32[::1],
                ""heap_indices"": numba.types.int32[::1],
                ""candidate"": numba.types.int32,
                ""vertex"": numba.types.int32,
                ""d"": numba.types.float32,
                ""d_vertex"": numba.types.float32,
                ""visited"": numba.types.uint8[::1],
                ""indices"": numba.types.int32[::1],
                ""indptr"": numba.types.int32[::1],
                ""data"": numba.types.float32[:, ::1],
                ""heap_size"": numba.types.int16,
                ""distance_scale"": numba.types.float32,
                ""distance_bound"": numba.types.float32,
                ""seed_scale"": numba.types.float32,
            },
            parallel=self.parallel_batch_queries,
        )
        def search_closure(query_points, k, epsilon, visited, rng_state):

            result = make_heap(query_points.shape[0], k)
            distance_scale = 1.0 + epsilon
            internal_rng_state = np.copy(rng_state)

            for i in numba.prange(query_points.shape[0]):
                # Avoid races on visited if parallel
                if parallel_search:
                    visited_nodes = np.zeros_like(visited)
                else:
                    visited_nodes = visited
                    visited_nodes[:] = 0

                if dist == alternative_dot or dist == alternative_cosine:
                    norm = np.sqrt((query_points[i] ** 2).sum())
                    if norm > 0.0:
                        current_query = query_points[i] / norm
                    else:
                        continue
                else:
                    current_query = query_points[i]

                heap_priorities = result[1][i]
                heap_indices = result[0][i]
                seed_set = [(np.float32(np.inf), np.int32(-1)) for j in range(0)]
                # heapq.heapify(seed_set)

                ############ Init ################
                index_bounds = tree_search_closure(current_query, internal_rng_state)
                candidate_indices = tree_indices[index_bounds[0] : index_bounds[1]]

                n_initial_points = candidate_indices.shape[0]
                n_random_samples = min(k, n_neighbors) - n_initial_points

                for j in range(n_initial_points):
                    candidate = candidate_indices[j]
                    d = np.float32(dist(data[candidate], current_query))
                    # indices are guaranteed different
                    simple_heap_push(heap_priorities, heap_indices, d, candidate)
                    heapq.heappush(seed_set, (d, candidate))
                    mark_visited(visited_nodes, candidate)

                if n_random_samples > 0:
                    for j in range(n_random_samples):
                        candidate = np.int32(
                            np.abs(tau_rand_int(internal_rng_state)) % data.shape[0]
                        )
                        if has_been_visited(visited_nodes, candidate) == 0:
                            d = np.float32(dist(data[candidate], current_query))
                            simple_heap_push(
                                heap_priorities, heap_indices, d, candidate
                            )
                            heapq.heappush(seed_set, (d, candidate))
                            mark_visited(visited_nodes, candidate)

                ############ Search ##############
                distance_bound = distance_scale * heap_priorities[0]

                # Find smallest seed point
                d_vertex, vertex = heapq.heappop(seed_set)

                while d_vertex < distance_bound:

                    for j in range(indptr[vertex], indptr[vertex + 1]):

                        candidate = indices[j]

                        if has_been_visited(visited_nodes, candidate) == 0:
                            mark_visited(visited_nodes, candidate)

                            d = np.float32(dist(data[candidate], current_query))

                            if d < distance_bound:
                                simple_heap_push(
                                    heap_priorities, heap_indices, d, candidate
                                )
                                heapq.heappush(seed_set, (d, candidate))
                                # Update bound
                                distance_bound = distance_scale * heap_priorities[0]

                    # find new smallest seed point
                    if len(seed_set) == 0:
                        break
                    else:
                        d_vertex, vertex = heapq.heappop(seed_set)

            return result

        self._search_function = search_closure
        if hasattr(deheap_sort, ""py_func""):
            self._deheap_function = numba.njit(parallel=self.parallel_batch_queries)(
                deheap_sort.py_func
            )
        else:
            self._deheap_function = deheap_sort

        # Force compilation of the search function (hardcoded k, epsilon)
        query_data = self._raw_data[:1]
        inds, dists, _ = self._search_function(
            query_data, 5, 0.0, self._visited, self.search_rng_state
        )
        _ = self._deheap_function(inds, dists)","range(indptr[vertex], indptr[vertex + 1])",range(*indptr[vertex:vertex + 2]),"iterable_zj[vertex], iterable_zj[vertex + 1]",*indptr[vertex:vertex+2],*indptr[vertex:vertex + 2],1
CudaText,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CudaText/app/py/cuda_comments/cd_comments.py,https://github.com/Alexey-T/CudaText/tree/master/app/py/cuda_comments/cd_comments.py,Command,cmt_toggle_stream$324,"def cmt_toggle_stream(self):
        ''' '''
        if ed.get_sel_mode() != app.SEL_NORMAL:
            return app.msg_status(f(_('{} works only with normal selection'), _('Commenting')))
        lex     = ed.get_prop(app.PROP_LEXER_CARET)
        ((bgn_sgn
        ,end_sgn)
        ,bOnlyLn)=self._get_cmt_pair(lex)
        if not bgn_sgn:
            return app.msg_status(f(_('No stream comment for lexer ""{}""'), lex))
        crts    = ed.get_carets()
        pass;                  #LOG and log('lex, get_carets()={}', (lex, crts))
        pass;                  #LOG and log('(bgn_sgn,end_sgn),bOnlyLn,bUseFLn={}', ((bgn_sgn,end_sgn),bOnlyLn,bUseFLn))
        for icrt, (cCrt, rCrt, cEnd, rEnd) in enumerate(crts):
            pass;              #LOG and log('(cCrt, rCrt), (cEnd, rEnd)={}', ((cCrt, rCrt), (cEnd, rEnd)))
            empty_sel     = -1==rEnd
            bDrtSel     = -1==rEnd or (rCrt, cCrt)>(rEnd, cEnd)
            bEntireLn   = (rEnd>=0) and (cEnd==0) and (cCrt==0)
            bEntireLn1  = bEntireLn and abs(rEnd-rCrt)==1
            bEntireLn2  = bEntireLn and abs(rEnd-rCrt)>1
            if False:pass
            elif empty_sel:
                # Use full line
                line        = ed.get_text_line(rCrt)
                (cTx1, rTx1), (cTx2, rTx2) = (0, rCrt), (len(line), rCrt)
            elif bOnlyLn: # and not empty_sel
                # Only full lines
                rTx1, rTx2  = apx.minmax(rCrt, rEnd)
                line    = ed.get_text_line(rTx2)
                (cTx1, rTx1), (cTx2, rTx2) = (0, rTx1), (len(line), rTx2)
            elif empty_sel: # and not bUseFLn and not bOnlyLn
                continue
            else:
                (rTx1, cTx1), (rTx2, cTx2) = apx.minmax((rCrt, cCrt), (rEnd, cEnd))
            selTx   = ed.get_text_substr(cTx1, rTx1, cTx2, rTx2)
            pass;              #LOG and log('(rTx1, cTx1), (rTx2, cTx2), selTx={}', ((rTx1, cTx1), (rTx2, cTx2), repr(selTx)))
            do_uncmt= selTx.startswith(bgn_sgn) #and selTx.endswith(end_sgn)
                # don't check for ending of selection - for HTML and entire selected line(s)
            pass;              #LOG and log('do_uncmt={}', (do_uncmt))

            cNSel1, rNSel1, cNSel2, rNSel2 = None, None, None, None

            if False:pass
            elif not do_uncmt and bOnlyLn:
                # Comment!
                ed.insert(0, rTx2+1, end_sgn+'\n')    #! true insert sequence
                ed.insert(0, rTx1,   bgn_sgn+'\n')    #! true insert sequence
                (cNSel1, rNSel1
                ,cNSel2, rNSel2)    = 0, rTx1, len(end_sgn), rTx2+2

            elif not do_uncmt:
                # Comment!
                if bEntireLn1:
                    s = ed.get_text_line(rTx1)
                    ed.set_text_line(rTx1, bgn_sgn+s+end_sgn)
                    (cNSel1, rNSel1
                    ,cNSel2, rNSel2) = (0, rTx1, 0, rTx2)

                elif bEntireLn2:
                    ed.insert(0, rTx2, end_sgn+'\n')
                    ed.insert(0, rTx1, bgn_sgn+'\n')
                    (cNSel1, rNSel1
                    ,cNSel2, rNSel2) = (0, rTx1, 0, rTx2+2)

                else:
                    ed.insert(cTx2, rTx2, end_sgn)        #! true insert sequence
                    ed.insert(cTx1, rTx1, bgn_sgn)        #! true insert sequence
                    if False:pass
                    elif rTx1==rTx2:
                        # sel into one row
                        (cNSel1, rNSel1
                        ,cNSel2, rNSel2)    = cTx1, rTx1, cTx2+len(bgn_sgn)+len(end_sgn), rTx2
                    elif rTx1!=rTx2:
                        # sel ends on diff rows
                        (cNSel1, rNSel1
                        ,cNSel2, rNSel2)    = cTx1, rTx1, cTx2             +len(end_sgn), rTx2

            elif do_uncmt and bOnlyLn:
                # UnComment!
                ed.delete(0, rTx2, 0, rTx2+1)    #! true delete sequence
                ed.delete(0, rTx1, 0, rTx1+1)    #! true delete sequence
                (cNSel1, rNSel1
                ,cNSel2, rNSel2)    = 0, rTx1, len(ed.get_text_line(rTx2-2)), rTx2-2

            elif do_uncmt:
                # UnComment!
                if selTx.endswith(end_sgn):
                    ed.delete(cTx2-len(end_sgn), rTx2, cTx2, rTx2)    #! true delete sequence
                    ed.delete(cTx1, rTx1, cTx1+len(bgn_sgn), rTx1)    #! true delete sequence
                    if False:pass
                    elif rTx1==rTx2:
                        # sel into one row
                        (cNSel1, rNSel1
                        ,cNSel2, rNSel2)    = cTx1, rTx1, cTx2-len(bgn_sgn)-len(end_sgn), rTx2
                    elif rTx1!=rTx2:
                        # sel ends on diff rows
                        (cNSel1, rNSel1
                        ,cNSel2, rNSel2)    = cTx1, rTx1, cTx2             -len(end_sgn), rTx2

                elif bEntireLn1:
                    s = ed.get_text_line(rTx1)
                    if s.startswith(bgn_sgn):
                        s = s[len(bgn_sgn):]
                    if s.endswith(end_sgn):
                        s = s[:-len(end_sgn)]
                    ed.set_text_line(rTx1, s)
                    (cNSel1, rNSel1
                    ,cNSel2, rNSel2) = (0, rTx1, 0, rTx2)

                elif bEntireLn2:
                    ed.delete(0, rTx2-1, 0, rTx2)
                    ed.delete(0, rTx1, 0, rTx1+1)
                    (cNSel1, rNSel1
                    ,cNSel2, rNSel2) = (0, rTx1, 0, rTx2-2)

            pass;              #LOG and log('bDrtSel, (cNSel1, rNSel1), (cNSel2, rNSel2)={}', (bDrtSel, (cNSel1, rNSel1), (cNSel2, rNSel2)))
            if cNSel1 is not None:
                if bDrtSel:
                    ed.set_caret(cNSel2, rNSel2, cNSel1, rNSel1, app.CARET_SET_INDEX+icrt)
                else:
                    ed.set_caret(cNSel1, rNSel1, cNSel2, rNSel2, app.CARET_SET_INDEX+icrt)
           #for icrt

        move_down = apx.get_opt('comment_move_down', True)
        if len(crts)==1 and empty_sel and move_down:
            apx._move_caret_down(cCrt, rCrt)
            if bOnlyLn and not do_uncmt:
                crt=ed.get_carets()[0]; apx._move_caret_down(crt[0], crt[1])
                crt=ed.get_carets()[0]; apx._move_caret_down(crt[0], crt[1])","apx._move_caret_down(crt[0], crt[1])",apx._move_caret_down(*crt[:2]),"iterable_zj[0], iterable_zj[1]",*crt[:2],*crt[:2],1
CudaText,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CudaText/app/py/cuda_comments/cd_comments.py,https://github.com/Alexey-T/CudaText/tree/master/app/py/cuda_comments/cd_comments.py,Command,cmt_toggle_stream$324,"def cmt_toggle_stream(self):
        ''' '''
        if ed.get_sel_mode() != app.SEL_NORMAL:
            return app.msg_status(f(_('{} works only with normal selection'), _('Commenting')))
        lex     = ed.get_prop(app.PROP_LEXER_CARET)
        ((bgn_sgn
        ,end_sgn)
        ,bOnlyLn)=self._get_cmt_pair(lex)
        if not bgn_sgn:
            return app.msg_status(f(_('No stream comment for lexer ""{}""'), lex))
        crts    = ed.get_carets()
        pass;                  #LOG and log('lex, get_carets()={}', (lex, crts))
        pass;                  #LOG and log('(bgn_sgn,end_sgn),bOnlyLn,bUseFLn={}', ((bgn_sgn,end_sgn),bOnlyLn,bUseFLn))
        for icrt, (cCrt, rCrt, cEnd, rEnd) in enumerate(crts):
            pass;              #LOG and log('(cCrt, rCrt), (cEnd, rEnd)={}', ((cCrt, rCrt), (cEnd, rEnd)))
            empty_sel     = -1==rEnd
            bDrtSel     = -1==rEnd or (rCrt, cCrt)>(rEnd, cEnd)
            bEntireLn   = (rEnd>=0) and (cEnd==0) and (cCrt==0)
            bEntireLn1  = bEntireLn and abs(rEnd-rCrt)==1
            bEntireLn2  = bEntireLn and abs(rEnd-rCrt)>1
            if False:pass
            elif empty_sel:
                # Use full line
                line        = ed.get_text_line(rCrt)
                (cTx1, rTx1), (cTx2, rTx2) = (0, rCrt), (len(line), rCrt)
            elif bOnlyLn: # and not empty_sel
                # Only full lines
                rTx1, rTx2  = apx.minmax(rCrt, rEnd)
                line    = ed.get_text_line(rTx2)
                (cTx1, rTx1), (cTx2, rTx2) = (0, rTx1), (len(line), rTx2)
            elif empty_sel: # and not bUseFLn and not bOnlyLn
                continue
            else:
                (rTx1, cTx1), (rTx2, cTx2) = apx.minmax((rCrt, cCrt), (rEnd, cEnd))
            selTx   = ed.get_text_substr(cTx1, rTx1, cTx2, rTx2)
            pass;              #LOG and log('(rTx1, cTx1), (rTx2, cTx2), selTx={}', ((rTx1, cTx1), (rTx2, cTx2), repr(selTx)))
            do_uncmt= selTx.startswith(bgn_sgn) #and selTx.endswith(end_sgn)
                # don't check for ending of selection - for HTML and entire selected line(s)
            pass;              #LOG and log('do_uncmt={}', (do_uncmt))

            cNSel1, rNSel1, cNSel2, rNSel2 = None, None, None, None

            if False:pass
            elif not do_uncmt and bOnlyLn:
                # Comment!
                ed.insert(0, rTx2+1, end_sgn+'\n')    #! true insert sequence
                ed.insert(0, rTx1,   bgn_sgn+'\n')    #! true insert sequence
                (cNSel1, rNSel1
                ,cNSel2, rNSel2)    = 0, rTx1, len(end_sgn), rTx2+2

            elif not do_uncmt:
                # Comment!
                if bEntireLn1:
                    s = ed.get_text_line(rTx1)
                    ed.set_text_line(rTx1, bgn_sgn+s+end_sgn)
                    (cNSel1, rNSel1
                    ,cNSel2, rNSel2) = (0, rTx1, 0, rTx2)

                elif bEntireLn2:
                    ed.insert(0, rTx2, end_sgn+'\n')
                    ed.insert(0, rTx1, bgn_sgn+'\n')
                    (cNSel1, rNSel1
                    ,cNSel2, rNSel2) = (0, rTx1, 0, rTx2+2)

                else:
                    ed.insert(cTx2, rTx2, end_sgn)        #! true insert sequence
                    ed.insert(cTx1, rTx1, bgn_sgn)        #! true insert sequence
                    if False:pass
                    elif rTx1==rTx2:
                        # sel into one row
                        (cNSel1, rNSel1
                        ,cNSel2, rNSel2)    = cTx1, rTx1, cTx2+len(bgn_sgn)+len(end_sgn), rTx2
                    elif rTx1!=rTx2:
                        # sel ends on diff rows
                        (cNSel1, rNSel1
                        ,cNSel2, rNSel2)    = cTx1, rTx1, cTx2             +len(end_sgn), rTx2

            elif do_uncmt and bOnlyLn:
                # UnComment!
                ed.delete(0, rTx2, 0, rTx2+1)    #! true delete sequence
                ed.delete(0, rTx1, 0, rTx1+1)    #! true delete sequence
                (cNSel1, rNSel1
                ,cNSel2, rNSel2)    = 0, rTx1, len(ed.get_text_line(rTx2-2)), rTx2-2

            elif do_uncmt:
                # UnComment!
                if selTx.endswith(end_sgn):
                    ed.delete(cTx2-len(end_sgn), rTx2, cTx2, rTx2)    #! true delete sequence
                    ed.delete(cTx1, rTx1, cTx1+len(bgn_sgn), rTx1)    #! true delete sequence
                    if False:pass
                    elif rTx1==rTx2:
                        # sel into one row
                        (cNSel1, rNSel1
                        ,cNSel2, rNSel2)    = cTx1, rTx1, cTx2-len(bgn_sgn)-len(end_sgn), rTx2
                    elif rTx1!=rTx2:
                        # sel ends on diff rows
                        (cNSel1, rNSel1
                        ,cNSel2, rNSel2)    = cTx1, rTx1, cTx2             -len(end_sgn), rTx2

                elif bEntireLn1:
                    s = ed.get_text_line(rTx1)
                    if s.startswith(bgn_sgn):
                        s = s[len(bgn_sgn):]
                    if s.endswith(end_sgn):
                        s = s[:-len(end_sgn)]
                    ed.set_text_line(rTx1, s)
                    (cNSel1, rNSel1
                    ,cNSel2, rNSel2) = (0, rTx1, 0, rTx2)

                elif bEntireLn2:
                    ed.delete(0, rTx2-1, 0, rTx2)
                    ed.delete(0, rTx1, 0, rTx1+1)
                    (cNSel1, rNSel1
                    ,cNSel2, rNSel2) = (0, rTx1, 0, rTx2-2)

            pass;              #LOG and log('bDrtSel, (cNSel1, rNSel1), (cNSel2, rNSel2)={}', (bDrtSel, (cNSel1, rNSel1), (cNSel2, rNSel2)))
            if cNSel1 is not None:
                if bDrtSel:
                    ed.set_caret(cNSel2, rNSel2, cNSel1, rNSel1, app.CARET_SET_INDEX+icrt)
                else:
                    ed.set_caret(cNSel1, rNSel1, cNSel2, rNSel2, app.CARET_SET_INDEX+icrt)
           #for icrt

        move_down = apx.get_opt('comment_move_down', True)
        if len(crts)==1 and empty_sel and move_down:
            apx._move_caret_down(cCrt, rCrt)
            if bOnlyLn and not do_uncmt:
                crt=ed.get_carets()[0]; apx._move_caret_down(crt[0], crt[1])
                crt=ed.get_carets()[0]; apx._move_caret_down(crt[0], crt[1])","apx._move_caret_down(crt[0], crt[1])",apx._move_caret_down(*crt[:2]),"iterable_zj[0], iterable_zj[1]",*crt[:2],*crt[:2],1
pennylane,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pennylane/tests/gradients/test_vjp.py,https://github.com/PennyLaneAI/pennylane/tree/master/tests/gradients/test_vjp.py,TestVJPGradients,test_autograd$285,"def test_autograd(self, tol):
        """"""Tests that the output of the VJP transform
        can be differentiated using autograd.""""""
        dev = qml.device(""default.qubit.autograd"", wires=2)
        params = np.array([0.543, -0.654], requires_grad=True)

        def cost_fn(x, dy):
            with qml.queuing.AnnotatedQueue() as q:
                ansatz(x[0], x[1])

            tape = qml.tape.QuantumScript.from_queue(q)
            tape.trainable_params = {0, 1}
            tapes, fn = qml.gradients.vjp(tape, dy, param_shift)
            vjp = fn(dev.batch_execute(tapes))
            return vjp

        dy = np.array([-1.0, 0.0, 0.0, 1.0], requires_grad=False)
        res = cost_fn(params, dy)
        assert np.allclose(res, expected(params), atol=tol, rtol=0)

        res = qml.jacobian(cost_fn)(params, dy)
        assert np.allclose(res, qml.jacobian(expected)(params), atol=tol, rtol=0)","ansatz(x[0], x[1])",ansatz(*x[:2]),"iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
Hardcode-Tray,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Hardcode-Tray/HardcodeTray/utils.py,https://github.com/bilelmoussaoui/Hardcode-Tray/tree/master/HardcodeTray/utils.py,,replace_to_6hex$254,"def replace_to_6hex(color):
    """"""Validate and replace 3hex colors to 6hex ones.""""""
    if match(r""^#(?:[0-9a-fA-F]{3}){1,2}$"", color):
        if len(color) == 4:
            color = ""#{0}{0}{1}{1}{2}{2}"".format(color[1], color[2], color[3])
        return color
    else:
        exit(_(""Invalid color {}"").format(color))","'#{0}{0}{1}{1}{2}{2}'.format(color[1], color[2], color[3])",'#{0}{0}{1}{1}{2}{2}'.format(*color[1:4]),"iterable_zj[1], iterable_zj[2], iterable_zj[3]",*color[1:4],*color[1:4],1
filterpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/filterpy/filterpy/memory/tests/test_fading_memory.py,https://github.com/rlabbe/filterpy/tree/master/filterpy/memory/tests/test_fading_memory.py,,dotest_2d_data$28,"def dotest_2d_data():
    """""" tests having multidimensional data for x""""""

    fm = FadingMemoryFilter(x0=np.array([[0.,2.],[0.,0.]]), dt=1, order=1, beta=.6)

    xs = [x for x in range(0,50)]

    for x in xs:
        data = [x+randn()*3, x+2+randn()*3]
        fm.update(data)
        plt.scatter(fm.x[0,0], fm.x[0,1], c = 'r')
        plt.scatter(data[0], data[1], c='b')","plt.scatter(data[0], data[1], c='b')","plt.scatter(*data[:2], c='b')","iterable_zj[0], iterable_zj[1]",*data[:2],*data[:2],1
armory,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/armory/blender/arm/lightmapper/utility/encoding.py,https://github.com/armory3d/armory/tree/master/blender/arm/lightmapper/utility/encoding.py,,encodeImageRGBDCPU$597,"def encodeImageRGBDCPU(image, maxRange, outDir, quality):
    input_image = bpy.data.images[image.name]
    image_name = input_image.name

    if input_image.colorspace_settings.name != 'Linear':
        input_image.colorspace_settings.name = 'Linear'

    # Removing .exr or .hdr prefix
    if image_name[-4:] == '.exr' or image_name[-4:] == '.hdr':
        image_name = image_name[:-4]

    target_image = bpy.data.images.get(image_name + '_encoded')
    if not target_image:
        target_image = bpy.data.images.new(
                name = image_name + '_encoded',
                width = input_image.size[0],
                height = input_image.size[1],
                alpha = True,
                float_buffer = False
                )
    
    num_pixels = len(input_image.pixels)
    result_pixel = list(input_image.pixels)

    rgbdMaxRange = 255.0

    for i in range(0,num_pixels,4):

        maxRGB = maxEps(max(result_pixel[i], result_pixel[i+1], result_pixel[i+2]))
        D = max(rgbdMaxRange/maxRGB, 1.0)
        D = np.clip((math.floor(D) / 255.0), 0.0, 1.0)

        result_pixel[i] = math.pow(result_pixel[i] * D, 1/2.2)
        result_pixel[i+1] = math.pow(result_pixel[i+1] * D, 1/2.2)
        result_pixel[i+2] = math.pow(result_pixel[i+2] * D, 1/2.2)
        result_pixel[i+3] = D
    
    target_image.pixels = result_pixel
    
    input_image = target_image

    #Save RGBD
    if bpy.context.scene.TLM_SceneProperties.tlm_verbose:
        print(input_image.name)
    input_image.filepath_raw = outDir + ""/"" + input_image.name + "".png""
    input_image.file_format = ""PNG""
    bpy.context.scene.render.image_settings.quality = quality
    input_image.save()","max(result_pixel[i], result_pixel[i + 1], result_pixel[i + 2])",max(*result_pixel[i:i + 3]),"iterable_zj[i], iterable_zj[i + 1], iterable_zj[i + 2]",*result_pixel[i:i+3],*result_pixel[i:i + 3],1
e2e-coref,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/e2e-coref/util.py,https://github.com/kentonl/e2e-coref/tree/master//util.py,CustomLSTMCell,_initializer$247,"def _initializer(shape, dtype=tf.float32, partition_info=None):
      M1 = np.random.randn(shape[0], shape[0]).astype(np.float32)
      M2 = np.random.randn(shape[1], shape[1]).astype(np.float32)
      Q1, R1 = np.linalg.qr(M1)
      Q2, R2 = np.linalg.qr(M2)
      Q1 = Q1 * np.sign(np.diag(R1))
      Q2 = Q2 * np.sign(np.diag(R2))
      n_min = min(shape[0], shape[1])
      params = np.dot(Q1[:, :n_min], Q2[:n_min, :]) * scale
      return params","min(shape[0], shape[1])",min(*shape[:2]),"iterable_zj[0], iterable_zj[1]",*shape[:2],*shape[:2],1
tslearn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tslearn/tslearn/tests/test_piecewise.py,https://github.com/tslearn-team/tslearn/tree/master/tslearn/tests/test_piecewise.py,,test_paa$12,"def test_paa():
    unfitted_paa = PiecewiseAggregateApproximation(n_segments=3)
    data = [[-1., 2., 0.1, -1., 1., -1.], [1., 3.2, -1., -3., 1., -1.]]
    np.testing.assert_raises(NotFittedError, unfitted_paa.distance,
                             data[0], data[1])

    paa_est = unfitted_paa
    n, sz, d = 2, 10, 3
    rng = np.random.RandomState(0)
    X = rng.randn(n, sz, d)
    paa_repr = paa_est.fit_transform(X)
    np.testing.assert_allclose(paa_est.distance(X[0], X[1]),
                               paa_est.distance_paa(paa_repr[0], paa_repr[1]))","np.testing.assert_raises(NotFittedError, unfitted_paa.distance, data[0], data[1])","np.testing.assert_raises(NotFittedError, unfitted_paa.distance, *data[:2])","iterable_zj[0], iterable_zj[1]",*data[:2],*data[:2],1
tslearn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tslearn/tslearn/tests/test_piecewise.py,https://github.com/tslearn-team/tslearn/tree/master/tslearn/tests/test_piecewise.py,,test_paa$12,"def test_paa():
    unfitted_paa = PiecewiseAggregateApproximation(n_segments=3)
    data = [[-1., 2., 0.1, -1., 1., -1.], [1., 3.2, -1., -3., 1., -1.]]
    np.testing.assert_raises(NotFittedError, unfitted_paa.distance,
                             data[0], data[1])

    paa_est = unfitted_paa
    n, sz, d = 2, 10, 3
    rng = np.random.RandomState(0)
    X = rng.randn(n, sz, d)
    paa_repr = paa_est.fit_transform(X)
    np.testing.assert_allclose(paa_est.distance(X[0], X[1]),
                               paa_est.distance_paa(paa_repr[0], paa_repr[1]))","paa_est.distance(X[0], X[1])",paa_est.distance(*X[:2]),"iterable_zj[0], iterable_zj[1]",*X[:2],*X[:2],1
tslearn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tslearn/tslearn/tests/test_piecewise.py,https://github.com/tslearn-team/tslearn/tree/master/tslearn/tests/test_piecewise.py,,test_paa$12,"def test_paa():
    unfitted_paa = PiecewiseAggregateApproximation(n_segments=3)
    data = [[-1., 2., 0.1, -1., 1., -1.], [1., 3.2, -1., -3., 1., -1.]]
    np.testing.assert_raises(NotFittedError, unfitted_paa.distance,
                             data[0], data[1])

    paa_est = unfitted_paa
    n, sz, d = 2, 10, 3
    rng = np.random.RandomState(0)
    X = rng.randn(n, sz, d)
    paa_repr = paa_est.fit_transform(X)
    np.testing.assert_allclose(paa_est.distance(X[0], X[1]),
                               paa_est.distance_paa(paa_repr[0], paa_repr[1]))","paa_est.distance_paa(paa_repr[0], paa_repr[1])",paa_est.distance_paa(*paa_repr[:2]),"iterable_zj[0], iterable_zj[1]",*paa_repr[:2],*paa_repr[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/relay/test_op_level5.py,https://github.com/apache/tvm/tree/master/tests/python/relay/test_op_level5.py,,test_multibox_transform_loc$628,"def test_multibox_transform_loc(executor_kind):
    def test_default_value():
        num_anchors = 3
        num_classes = 3

        np_cls_prob = np.array([[[0.2, 0.5, 0.3], [0.25, 0.3, 0.45], [0.7, 0.1, 0.2]]]).astype(
            ""float32""
        )
        np_loc_preds = np.array(
            [[0.1, -0.2, 0.3, 0.2, 0.2, 0.4, 0.5, -0.3, 0.7, -0.2, -0.4, -0.8]]
        ).astype(""float32"")
        np_anchors = np.array(
            [[[-0.1, -0.1, 0.1, 0.1], [-0.2, -0.2, 0.2, 0.2], [1.2, 1.2, 1.5, 1.5]]]
        ).astype(""float32"")

        expected_np_out = np.array(
            [
                [
                    [1, 0.69999999, 0, 0, 0.10818365, 0.10008108],
                    [0, 0.44999999, 1, 1, 1, 1],
                    [0, 0.30000001, 0, 0, 0.22903419, 0.20435292],
                ]
            ]
        )

        cls_prob = relay.var(
            ""cls_prob"", relay.ty.TensorType((1, num_anchors, num_classes), ""float32"")
        )
        loc_pred = relay.var(""loc_pred"", relay.ty.TensorType((1, num_anchors * 4), ""float32""))
        anchors = relay.var(""anchors"", relay.ty.TensorType((1, num_anchors, 4), ""float32""))

        mtl = relay.vision.multibox_transform_loc(
            cls_prob=cls_prob, loc_pred=loc_pred, anchor=anchors
        )
        ret = run_infer_type(mtl.astuple())
        ref_type = relay.ty.TupleType(
            tvm.runtime.convert(
                [
                    relay.ty.TensorType((1, num_anchors, 6), ""float32""),
                    relay.ty.TensorType((1,), ""int""),
                ]
            )
        )

        assert ret.checked_type == ref_type

        nms = relay.vision.non_max_suppression(mtl[0], mtl[1], mtl[0], return_indices=False)
        func = relay.Function([cls_prob, loc_pred, anchors], nms)
        func = run_infer_type(func)
        for target, dev in tvm.testing.enabled_targets():
            op_res = relay.create_executor(executor_kind, device=dev, target=target).evaluate(func)(
                np_cls_prob, np_loc_preds, np_anchors
            )
            tvm.testing.assert_allclose(op_res.numpy(), expected_np_out, rtol=1e-5)

    def test_threshold():
        num_anchors = 5
        num_classes = 5
        n = te.size_var(""n"")
        cls_prob = relay.var(
            ""cls_prob"", relay.ty.TensorType((n, num_anchors, num_classes), ""float32"")
        )
        loc_pred = relay.var(""loc_pred"", relay.ty.TensorType((n, num_anchors * 4), ""float32""))
        anchors = relay.var(""anchors"", relay.ty.TensorType((1, num_anchors, 4), ""float32""))
        threshold = 0.02
        variances = (0.2, 0.2, 0.3, 0.3)

        ret = relay.vision.multibox_transform_loc(
            cls_prob=cls_prob,
            loc_pred=loc_pred,
            anchor=anchors,
            threshold=threshold,
            variances=variances,
        )
        ret = run_infer_type(ret.astuple())
        ref_type = relay.ty.TupleType(
            tvm.runtime.convert(
                [
                    relay.ty.TensorType((n, num_anchors, 6), ""float32""),
                    relay.ty.TensorType((n,), ""int""),
                ]
            )
        )
        assert ret.checked_type == ref_type

    test_default_value()
    test_threshold()","relay.vision.non_max_suppression(mtl[0], mtl[1], mtl[0], return_indices=False)","relay.vision.non_max_suppression(*mtl[:2], mtl[0], return_indices=False)","iterable_zj[0], iterable_zj[1]",*mtl[:2],*mtl[:2],1
Cura,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Cura/plugins/X3DReader/X3DReader.py,https://github.com/Ultimaker/Cura/tree/master/plugins/X3DReader/X3DReader.py,X3DReader,processGeometryIndexedQuadSet$611,"def processGeometryIndexedQuadSet(self, node):
        index = readIntArray(node, ""index"", [])
        num_quads = len(index) // 4
        ccw = self.startCoordMesh(node, num_quads*2)

        for i in range(0, num_quads*4, 4):
            self.addQuadFlip(index[i], index[i+1], index[i+2], index[i+3], ccw)","self.addQuadFlip(index[i], index[i + 1], index[i + 2], index[i + 3], ccw)","self.addQuadFlip(*index[i:i + 4], ccw)","iterable_zj[i], iterable_zj[i + 1], iterable_zj[i + 2], iterable_zj[i + 3]",*index[i:i+4],*index[i:i + 4],1
imgclsmob,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/pytorch/pytorchcv/models/simpleposemobile_coco.py,https://github.com/osmr/imgclsmob/tree/master/pytorch/pytorchcv/models/simpleposemobile_coco.py,,_test$287,"def _test():
    in_size = (256, 192)
    keypoints = 17
    return_heatmap = False
    pretrained = False

    models = [
        simplepose_mobile_resnet18_coco,
        simplepose_mobile_resnet50b_coco,
        simplepose_mobile_mobilenet_w1_coco,
        simplepose_mobile_mobilenetv2b_w1_coco,
        simplepose_mobile_mobilenetv3_small_w1_coco,
        simplepose_mobile_mobilenetv3_large_w1_coco,
    ]

    for model in models:

        net = model(pretrained=pretrained, in_size=in_size, return_heatmap=return_heatmap)

        # net.train()
        net.eval()
        weight_count = _calc_width(net)
        print(""m={}, {}"".format(model.__name__, weight_count))
        assert (model != simplepose_mobile_resnet18_coco or weight_count == 12858208)
        assert (model != simplepose_mobile_resnet50b_coco or weight_count == 25582944)
        assert (model != simplepose_mobile_mobilenet_w1_coco or weight_count == 5019744)
        assert (model != simplepose_mobile_mobilenetv2b_w1_coco or weight_count == 4102176)
        assert (model != simplepose_mobile_mobilenetv3_small_w1_coco or weight_count == 2625088)
        assert (model != simplepose_mobile_mobilenetv3_large_w1_coco or weight_count == 4768336)

        batch = 14
        x = torch.randn(batch, 3, in_size[0], in_size[1])
        y = net(x)
        assert ((y.shape[0] == batch) and (y.shape[1] == keypoints))
        if return_heatmap:
            assert ((y.shape[2] == x.shape[2] // 4) and (y.shape[3] == x.shape[3] // 4))
        else:
            assert (y.shape[2] == 3)","torch.randn(batch, 3, in_size[0], in_size[1])","torch.randn(batch, 3, *in_size[:2])","iterable_zj[0], iterable_zj[1]",*in_size[:2],*in_size[:2],1
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/modeling/backbones/mobilenet.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/modeling/backbones/mobilenet.py,MobileNet,__call__$159,"def __call__(self, input):
        scale = self.conv_group_scale

        blocks = []
        # input 1/1
        out = self._conv_norm(
            input, 3, int(32 * scale), 2, 1, name=self.prefix_name + ""conv1"")
        # 1/2
        out = self.depthwise_separable(
            out, 32, 64, 32, 1, scale, name=self.prefix_name + ""conv2_1"")
        out = self.depthwise_separable(
            out, 64, 128, 64, 2, scale, name=self.prefix_name + ""conv2_2"")
        # 1/4
        out = self.depthwise_separable(
            out, 128, 128, 128, 1, scale, name=self.prefix_name + ""conv3_1"")
        out = self.depthwise_separable(
            out, 128, 256, 128, 2, scale, name=self.prefix_name + ""conv3_2"")
        # 1/8
        blocks.append(out)
        out = self.depthwise_separable(
            out, 256, 256, 256, 1, scale, name=self.prefix_name + ""conv4_1"")
        out = self.depthwise_separable(
            out, 256, 512, 256, 2, scale, name=self.prefix_name + ""conv4_2"")
        # 1/16
        blocks.append(out)
        for i in range(5):
            out = self.depthwise_separable(
                out,
                512,
                512,
                512,
                1,
                scale,
                name=self.prefix_name + ""conv5_"" + str(i + 1))
        module11 = out

        out = self.depthwise_separable(
            out, 512, 1024, 512, 2, scale, name=self.prefix_name + ""conv5_6"")
        # 1/32
        out = self.depthwise_separable(
            out, 1024, 1024, 1024, 1, scale, name=self.prefix_name + ""conv6"")
        module13 = out
        blocks.append(out)
        if not self.with_extra_blocks:
            return blocks

        num_filters = self.extra_block_filters
        module14 = self._extra_block(module13, num_filters[0][0],
                                     num_filters[0][1], 1, 2,
                                     self.prefix_name + ""conv7_1"")
        module15 = self._extra_block(module14, num_filters[1][0],
                                     num_filters[1][1], 1, 2,
                                     self.prefix_name + ""conv7_2"")
        module16 = self._extra_block(module15, num_filters[2][0],
                                     num_filters[2][1], 1, 2,
                                     self.prefix_name + ""conv7_3"")
        module17 = self._extra_block(module16, num_filters[3][0],
                                     num_filters[3][1], 1, 2,
                                     self.prefix_name + ""conv7_4"")
        return module11, module13, module14, module15, module16, module17","self._extra_block(module13, num_filters[0][0], num_filters[0][1], 1, 2, self.prefix_name + 'conv7_1')","self._extra_block(module13, *num_filters[0][:2], 1, 2, self.prefix_name + 'conv7_1')","iterable_zj[0], iterable_zj[1]",*num_filters[0][:2],*num_filters[0][:2],1
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/modeling/backbones/mobilenet.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/modeling/backbones/mobilenet.py,MobileNet,__call__$159,"def __call__(self, input):
        scale = self.conv_group_scale

        blocks = []
        # input 1/1
        out = self._conv_norm(
            input, 3, int(32 * scale), 2, 1, name=self.prefix_name + ""conv1"")
        # 1/2
        out = self.depthwise_separable(
            out, 32, 64, 32, 1, scale, name=self.prefix_name + ""conv2_1"")
        out = self.depthwise_separable(
            out, 64, 128, 64, 2, scale, name=self.prefix_name + ""conv2_2"")
        # 1/4
        out = self.depthwise_separable(
            out, 128, 128, 128, 1, scale, name=self.prefix_name + ""conv3_1"")
        out = self.depthwise_separable(
            out, 128, 256, 128, 2, scale, name=self.prefix_name + ""conv3_2"")
        # 1/8
        blocks.append(out)
        out = self.depthwise_separable(
            out, 256, 256, 256, 1, scale, name=self.prefix_name + ""conv4_1"")
        out = self.depthwise_separable(
            out, 256, 512, 256, 2, scale, name=self.prefix_name + ""conv4_2"")
        # 1/16
        blocks.append(out)
        for i in range(5):
            out = self.depthwise_separable(
                out,
                512,
                512,
                512,
                1,
                scale,
                name=self.prefix_name + ""conv5_"" + str(i + 1))
        module11 = out

        out = self.depthwise_separable(
            out, 512, 1024, 512, 2, scale, name=self.prefix_name + ""conv5_6"")
        # 1/32
        out = self.depthwise_separable(
            out, 1024, 1024, 1024, 1, scale, name=self.prefix_name + ""conv6"")
        module13 = out
        blocks.append(out)
        if not self.with_extra_blocks:
            return blocks

        num_filters = self.extra_block_filters
        module14 = self._extra_block(module13, num_filters[0][0],
                                     num_filters[0][1], 1, 2,
                                     self.prefix_name + ""conv7_1"")
        module15 = self._extra_block(module14, num_filters[1][0],
                                     num_filters[1][1], 1, 2,
                                     self.prefix_name + ""conv7_2"")
        module16 = self._extra_block(module15, num_filters[2][0],
                                     num_filters[2][1], 1, 2,
                                     self.prefix_name + ""conv7_3"")
        module17 = self._extra_block(module16, num_filters[3][0],
                                     num_filters[3][1], 1, 2,
                                     self.prefix_name + ""conv7_4"")
        return module11, module13, module14, module15, module16, module17","self._extra_block(module14, num_filters[1][0], num_filters[1][1], 1, 2, self.prefix_name + 'conv7_2')","self._extra_block(module14, *num_filters[1][:2], 1, 2, self.prefix_name + 'conv7_2')","iterable_zj[0], iterable_zj[1]",*num_filters[1][:2],*num_filters[1][:2],1
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/modeling/backbones/mobilenet.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/modeling/backbones/mobilenet.py,MobileNet,__call__$159,"def __call__(self, input):
        scale = self.conv_group_scale

        blocks = []
        # input 1/1
        out = self._conv_norm(
            input, 3, int(32 * scale), 2, 1, name=self.prefix_name + ""conv1"")
        # 1/2
        out = self.depthwise_separable(
            out, 32, 64, 32, 1, scale, name=self.prefix_name + ""conv2_1"")
        out = self.depthwise_separable(
            out, 64, 128, 64, 2, scale, name=self.prefix_name + ""conv2_2"")
        # 1/4
        out = self.depthwise_separable(
            out, 128, 128, 128, 1, scale, name=self.prefix_name + ""conv3_1"")
        out = self.depthwise_separable(
            out, 128, 256, 128, 2, scale, name=self.prefix_name + ""conv3_2"")
        # 1/8
        blocks.append(out)
        out = self.depthwise_separable(
            out, 256, 256, 256, 1, scale, name=self.prefix_name + ""conv4_1"")
        out = self.depthwise_separable(
            out, 256, 512, 256, 2, scale, name=self.prefix_name + ""conv4_2"")
        # 1/16
        blocks.append(out)
        for i in range(5):
            out = self.depthwise_separable(
                out,
                512,
                512,
                512,
                1,
                scale,
                name=self.prefix_name + ""conv5_"" + str(i + 1))
        module11 = out

        out = self.depthwise_separable(
            out, 512, 1024, 512, 2, scale, name=self.prefix_name + ""conv5_6"")
        # 1/32
        out = self.depthwise_separable(
            out, 1024, 1024, 1024, 1, scale, name=self.prefix_name + ""conv6"")
        module13 = out
        blocks.append(out)
        if not self.with_extra_blocks:
            return blocks

        num_filters = self.extra_block_filters
        module14 = self._extra_block(module13, num_filters[0][0],
                                     num_filters[0][1], 1, 2,
                                     self.prefix_name + ""conv7_1"")
        module15 = self._extra_block(module14, num_filters[1][0],
                                     num_filters[1][1], 1, 2,
                                     self.prefix_name + ""conv7_2"")
        module16 = self._extra_block(module15, num_filters[2][0],
                                     num_filters[2][1], 1, 2,
                                     self.prefix_name + ""conv7_3"")
        module17 = self._extra_block(module16, num_filters[3][0],
                                     num_filters[3][1], 1, 2,
                                     self.prefix_name + ""conv7_4"")
        return module11, module13, module14, module15, module16, module17","self._extra_block(module15, num_filters[2][0], num_filters[2][1], 1, 2, self.prefix_name + 'conv7_3')","self._extra_block(module15, *num_filters[2][:2], 1, 2, self.prefix_name + 'conv7_3')","iterable_zj[0], iterable_zj[1]",*num_filters[2][:2],*num_filters[2][:2],1
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/modeling/backbones/mobilenet.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/modeling/backbones/mobilenet.py,MobileNet,__call__$159,"def __call__(self, input):
        scale = self.conv_group_scale

        blocks = []
        # input 1/1
        out = self._conv_norm(
            input, 3, int(32 * scale), 2, 1, name=self.prefix_name + ""conv1"")
        # 1/2
        out = self.depthwise_separable(
            out, 32, 64, 32, 1, scale, name=self.prefix_name + ""conv2_1"")
        out = self.depthwise_separable(
            out, 64, 128, 64, 2, scale, name=self.prefix_name + ""conv2_2"")
        # 1/4
        out = self.depthwise_separable(
            out, 128, 128, 128, 1, scale, name=self.prefix_name + ""conv3_1"")
        out = self.depthwise_separable(
            out, 128, 256, 128, 2, scale, name=self.prefix_name + ""conv3_2"")
        # 1/8
        blocks.append(out)
        out = self.depthwise_separable(
            out, 256, 256, 256, 1, scale, name=self.prefix_name + ""conv4_1"")
        out = self.depthwise_separable(
            out, 256, 512, 256, 2, scale, name=self.prefix_name + ""conv4_2"")
        # 1/16
        blocks.append(out)
        for i in range(5):
            out = self.depthwise_separable(
                out,
                512,
                512,
                512,
                1,
                scale,
                name=self.prefix_name + ""conv5_"" + str(i + 1))
        module11 = out

        out = self.depthwise_separable(
            out, 512, 1024, 512, 2, scale, name=self.prefix_name + ""conv5_6"")
        # 1/32
        out = self.depthwise_separable(
            out, 1024, 1024, 1024, 1, scale, name=self.prefix_name + ""conv6"")
        module13 = out
        blocks.append(out)
        if not self.with_extra_blocks:
            return blocks

        num_filters = self.extra_block_filters
        module14 = self._extra_block(module13, num_filters[0][0],
                                     num_filters[0][1], 1, 2,
                                     self.prefix_name + ""conv7_1"")
        module15 = self._extra_block(module14, num_filters[1][0],
                                     num_filters[1][1], 1, 2,
                                     self.prefix_name + ""conv7_2"")
        module16 = self._extra_block(module15, num_filters[2][0],
                                     num_filters[2][1], 1, 2,
                                     self.prefix_name + ""conv7_3"")
        module17 = self._extra_block(module16, num_filters[3][0],
                                     num_filters[3][1], 1, 2,
                                     self.prefix_name + ""conv7_4"")
        return module11, module13, module14, module15, module16, module17","self._extra_block(module16, num_filters[3][0], num_filters[3][1], 1, 2, self.prefix_name + 'conv7_4')","self._extra_block(module16, *num_filters[3][:2], 1, 2, self.prefix_name + 'conv7_4')","iterable_zj[0], iterable_zj[1]",*num_filters[3][:2],*num_filters[3][:2],1
bindsnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bindsnet/bindsnet/datasets/alov300.py,https://github.com/BindsNET/bindsnet/tree/master/bindsnet/datasets/alov300.py,ALOV300,show_sample$204,"def show_sample(self, idx):
        """"""
        Helper function to display sample, which is passed to GOTURN.
        Shows previous frame and current frame with bounding box.
        """"""
        x, _ = self.get_sample(idx)
        prev_image = x[""previmg""]
        curr_image = x[""currimg""]
        bb = x[""currbb""]
        bbox = BoundingBox(bb[0], bb[1], bb[2], bb[3])
        bbox.unscale(curr_image)
        bb = bbox.get_bb_list()
        bb = [int(val) for val in bb]
        prev_image = cv2.cvtColor(prev_image, cv2.COLOR_RGB2BGR)
        curr_image = cv2.cvtColor(curr_image, cv2.COLOR_RGB2BGR)
        curr_image = cv2.rectangle(
            curr_image, (bb[0], bb[1]), (bb[2], bb[3]), (0, 255, 0), 2
        )
        concat_image = np.hstack((prev_image, curr_image))
        cv2.imshow(""alov dataset sample: "" + str(idx), concat_image)
        cv2.waitKey(0)","BoundingBox(bb[0], bb[1], bb[2], bb[3])","BoundingBox(*bb[:3], bb[3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*bb[:3],*bb[:4],0
uiautomator2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/uiautomator2/uiautomator2/swipe.py,https://github.com/openatx/uiautomator2/tree/master/uiautomator2/swipe.py,SwipeExt,_swipe$30,"def _swipe(_from, _to):
            self._d.swipe(_from[0], _from[1], _to[0], _to[1], **kwargs)","self._d.swipe(_from[0], _from[1], _to[0], _to[1], **kwargs)","self._d.swipe(*_from[:2], _to[0], _to[1], **kwargs)","iterable_zj[0], iterable_zj[1]",*_from[:2],*_from[:2],1
uiautomator2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/uiautomator2/uiautomator2/swipe.py,https://github.com/openatx/uiautomator2/tree/master/uiautomator2/swipe.py,SwipeExt,_swipe$30,"def _swipe(_from, _to):
            self._d.swipe(_from[0], _from[1], _to[0], _to[1], **kwargs)","self._d.swipe(_from[0], _from[1], _to[0], _to[1], **kwargs)","self._d.swipe(_from[0], _from[1], *_to[:2], **kwargs)","iterable_zj[0], iterable_zj[1]",*_to[:2],*_to[:2],1
opytimizer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opytimizer/tests/opytimizer/optimizers/swarm/test_kh.py,https://github.com/gugarosa/opytimizer/tree/master/tests/opytimizer/optimizers/swarm/test_kh.py,,test_kh_best_beta$289,"def test_kh_best_beta():
    search_space = search.SearchSpace(n_agents=5, n_variables=2,
                                      lower_bound=[0, 0], upper_bound=[10, 10])

    new_kh = kh.KH()
    new_kh.compile(search_space)

    beta = new_kh._best_beta(
        search_space.agents[0], search_space.agents[-1], search_space.agents[0])

    assert beta.shape == (2, 1)","new_kh._best_beta(search_space.agents[0], search_space.agents[-1], search_space.agents[0])","new_kh._best_beta(*search_space.agents[::len(search_space.agents) - 1], search_space.agents[0])","iterable_zj[0], iterable_zj[-1]",*search_space.agents[::len(search_space.agents)-1],*search_space.agents[-1:1],0
pyqtgraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyqtgraph/pyqtgraph/graphicsItems/ColorBarItem.py,https://github.com/pyqtgraph/pyqtgraph/tree/master/pyqtgraph/graphicsItems/ColorBarItem.py,ColorBarItem,_update_items$265,"def _update_items(self, update_cmap=False):
        """""" internal: update color maps for bar and assigned ImageItems """"""
        # update color bar:
        self.axis.setRange( self.values[0], self.values[1] )
        if update_cmap and self._colorMap is not None:
            self.bar.setLookupTable( self._colorMap.getLookupTable(nPts=256) )
        # update assigned ImageItems, too:
        for img_weakref in self.img_list:
            img = img_weakref()
            if img is None: continue # dereference weakref
            img.setLevels( self.values ) # (min,max) tuple
            if update_cmap and self._colorMap is not None:
                img.setLookupTable( self._colorMap.getLookupTable(nPts=256) )","self.axis.setRange(self.values[0], self.values[1])",self.axis.setRange(*self.values[:2]),"iterable_zj[0], iterable_zj[1]",*self.values[:2],*self.values[:2],1
PaddleSeg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSeg/EISeg/eiseg/widget/polygon.py,https://github.com/PaddlePaddle/PaddleSeg/tree/master/EISeg/eiseg/widget/polygon.py,PolygonAnnotation,setColor$280,"def setColor(self, insideColor, borderColor):
        i = insideColor
        self.insideColor = QtGui.QColor(i[0], i[1], i[2])
        self.insideColor.setAlphaF(self.opacity)
        self.halfInsideColor = QtGui.QColor(i[0], i[1], i[2])
        self.halfInsideColor.setAlphaF(self.opacity / 2)
        self.setBrush(self.halfInsideColor)
        b = borderColor
        self.borderColor = QtGui.QColor(b[0], b[1], b[2])
        self.borderColor.setAlphaF(0.8)
        self.setPen(QtGui.QPen(self.borderColor))
        for grip in self.m_items:
            grip.setColor(self.borderColor)
        for line in self.m_lines:
            line.setColor(self.borderColor)","QtGui.QColor(i[0], i[1], i[2])",QtGui.QColor(*i[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*i[:3],*i[:3],1
PaddleSeg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSeg/EISeg/eiseg/widget/polygon.py,https://github.com/PaddlePaddle/PaddleSeg/tree/master/EISeg/eiseg/widget/polygon.py,PolygonAnnotation,setColor$280,"def setColor(self, insideColor, borderColor):
        i = insideColor
        self.insideColor = QtGui.QColor(i[0], i[1], i[2])
        self.insideColor.setAlphaF(self.opacity)
        self.halfInsideColor = QtGui.QColor(i[0], i[1], i[2])
        self.halfInsideColor.setAlphaF(self.opacity / 2)
        self.setBrush(self.halfInsideColor)
        b = borderColor
        self.borderColor = QtGui.QColor(b[0], b[1], b[2])
        self.borderColor.setAlphaF(0.8)
        self.setPen(QtGui.QPen(self.borderColor))
        for grip in self.m_items:
            grip.setColor(self.borderColor)
        for line in self.m_lines:
            line.setColor(self.borderColor)","QtGui.QColor(i[0], i[1], i[2])",QtGui.QColor(*i[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*i[:3],*i[:3],1
PaddleSeg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSeg/EISeg/eiseg/widget/polygon.py,https://github.com/PaddlePaddle/PaddleSeg/tree/master/EISeg/eiseg/widget/polygon.py,PolygonAnnotation,setColor$280,"def setColor(self, insideColor, borderColor):
        i = insideColor
        self.insideColor = QtGui.QColor(i[0], i[1], i[2])
        self.insideColor.setAlphaF(self.opacity)
        self.halfInsideColor = QtGui.QColor(i[0], i[1], i[2])
        self.halfInsideColor.setAlphaF(self.opacity / 2)
        self.setBrush(self.halfInsideColor)
        b = borderColor
        self.borderColor = QtGui.QColor(b[0], b[1], b[2])
        self.borderColor.setAlphaF(0.8)
        self.setPen(QtGui.QPen(self.borderColor))
        for grip in self.m_items:
            grip.setColor(self.borderColor)
        for line in self.m_lines:
            line.setColor(self.borderColor)","QtGui.QColor(b[0], b[1], b[2])",QtGui.QColor(*b[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*b[:3],*b[:3],1
nilearn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nilearn/nilearn/decomposition/tests/test_canica.py,https://github.com/nilearn/nilearn/tree/master/nilearn/decomposition/tests/test_canica.py,,test_with_globbing_patterns_with_multi_subjects$228,"def test_with_globbing_patterns_with_multi_subjects():
    # Multi subjects
    data, mask_img, _, _ = _make_canica_test_data(n_subjects=3)
    n_components = 3
    canica = CanICA(n_components=n_components, mask=mask_img)
    with write_tmp_imgs(data[0], data[1], data[2], create_files=True,
                        use_wildcards=True) as img:
        input_image = _tmp_dir() + img
        canica.fit(input_image)
        components_img = canica.components_img_
        assert isinstance(components_img, nibabel.Nifti1Image)
        # n_components = 3
        check_shape = data[0].shape[:3] + (3,)
        assert components_img.shape, check_shape","write_tmp_imgs(data[0], data[1], data[2], create_files=True, use_wildcards=True)","write_tmp_imgs(*data[:3], create_files=True, use_wildcards=True)","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*data[:3],*data[:3],1
electrum,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electrum/electrum/plugins/revealer/qt.py,https://github.com/spesmilo/electrum/tree/master/electrum/plugins/revealer/qt.py,Plugin,calibration$402,"def calibration(self):
        img = QImage(self.SIZE[0], self.SIZE[1], QImage.Format_Mono)
        bitmap = QBitmap.fromImage(img, Qt.MonoOnly)
        bitmap.fill(Qt.black)
        self.make_calnoise()
        img = self.overlay_marks(self.calnoise.scaledToHeight(self.f_size.height()), False, True)
        self.calibration_pdf(img)
        QDesktopServices.openUrl(QUrl.fromLocalFile(self.get_path_to_calibration_file()))
        return img","QImage(self.SIZE[0], self.SIZE[1], QImage.Format_Mono)","QImage(*self.SIZE[:2], QImage.Format_Mono)","iterable_zj[0], iterable_zj[1]",*self.SIZE[:2],*self.SIZE[:2],1
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/sparse/_compressed.py,https://github.com/scipy/scipy/tree/master/scipy/sparse/_compressed.py,_cs_matrix,diagonal$543,"def diagonal(self, k=0):
        rows, cols = self.shape
        if k <= -rows or k >= cols:
            return np.empty(0, dtype=self.data.dtype)
        fn = getattr(_sparsetools, self.format + ""_diagonal"")
        y = np.empty(min(rows + min(k, 0), cols - max(k, 0)),
                     dtype=upcast(self.dtype))
        fn(k, self.shape[0], self.shape[1], self.indptr, self.indices,
           self.data, y)
        return y","fn(k, self.shape[0], self.shape[1], self.indptr, self.indices, self.data, y)","fn(k, *self.shape[:2], self.indptr, self.indices, self.data, y)","iterable_zj[0], iterable_zj[1]",*self.shape[:2],*self.shape[:2],1
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/xautodl/models/shape_infers/InferCifarResNet_width.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/xautodl/models/shape_infers/InferCifarResNet_width.py,ResNetBasicblock,__init__$59,"def __init__(self, iCs, stride):
        super(ResNetBasicblock, self).__init__()
        assert stride == 1 or stride == 2, ""invalid stride {:}"".format(stride)
        assert isinstance(iCs, tuple) or isinstance(
            iCs, list
        ), ""invalid type of iCs : {:}"".format(iCs)
        assert len(iCs) == 3, ""invalid lengths of iCs : {:}"".format(iCs)

        self.conv_a = ConvBNReLU(
            iCs[0],
            iCs[1],
            3,
            stride,
            1,
            False,
            has_avg=False,
            has_bn=True,
            has_relu=True,
        )
        self.conv_b = ConvBNReLU(
            iCs[1], iCs[2], 3, 1, 1, False, has_avg=False, has_bn=True, has_relu=False
        )
        residual_in = iCs[0]
        if stride == 2:
            self.downsample = ConvBNReLU(
                iCs[0],
                iCs[2],
                1,
                1,
                0,
                False,
                has_avg=True,
                has_bn=False,
                has_relu=False,
            )
            residual_in = iCs[2]
        elif iCs[0] != iCs[2]:
            self.downsample = ConvBNReLU(
                iCs[0],
                iCs[2],
                1,
                1,
                0,
                False,
                has_avg=False,
                has_bn=True,
                has_relu=False,
            )
        else:
            self.downsample = None
        # self.out_dim  = max(residual_in, iCs[2])
        self.out_dim = iCs[2]","ConvBNReLU(iCs[0], iCs[1], 3, stride, 1, False, has_avg=False, has_bn=True, has_relu=True)","ConvBNReLU(*iCs[:2], 3, stride, 1, False, has_avg=False, has_bn=True, has_relu=True)","iterable_zj[0], iterable_zj[1]",*iCs[:2],*iCs[:2],1
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/xautodl/models/shape_infers/InferCifarResNet_width.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/xautodl/models/shape_infers/InferCifarResNet_width.py,ResNetBasicblock,__init__$59,"def __init__(self, iCs, stride):
        super(ResNetBasicblock, self).__init__()
        assert stride == 1 or stride == 2, ""invalid stride {:}"".format(stride)
        assert isinstance(iCs, tuple) or isinstance(
            iCs, list
        ), ""invalid type of iCs : {:}"".format(iCs)
        assert len(iCs) == 3, ""invalid lengths of iCs : {:}"".format(iCs)

        self.conv_a = ConvBNReLU(
            iCs[0],
            iCs[1],
            3,
            stride,
            1,
            False,
            has_avg=False,
            has_bn=True,
            has_relu=True,
        )
        self.conv_b = ConvBNReLU(
            iCs[1], iCs[2], 3, 1, 1, False, has_avg=False, has_bn=True, has_relu=False
        )
        residual_in = iCs[0]
        if stride == 2:
            self.downsample = ConvBNReLU(
                iCs[0],
                iCs[2],
                1,
                1,
                0,
                False,
                has_avg=True,
                has_bn=False,
                has_relu=False,
            )
            residual_in = iCs[2]
        elif iCs[0] != iCs[2]:
            self.downsample = ConvBNReLU(
                iCs[0],
                iCs[2],
                1,
                1,
                0,
                False,
                has_avg=False,
                has_bn=True,
                has_relu=False,
            )
        else:
            self.downsample = None
        # self.out_dim  = max(residual_in, iCs[2])
        self.out_dim = iCs[2]","ConvBNReLU(iCs[1], iCs[2], 3, 1, 1, False, has_avg=False, has_bn=True, has_relu=False)","ConvBNReLU(*iCs[1:3], 3, 1, 1, False, has_avg=False, has_bn=True, has_relu=False)","iterable_zj[1], iterable_zj[2]",*iCs[1:3],*iCs[1:3],1
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/xautodl/models/shape_infers/InferCifarResNet_width.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/xautodl/models/shape_infers/InferCifarResNet_width.py,ResNetBasicblock,__init__$59,"def __init__(self, iCs, stride):
        super(ResNetBasicblock, self).__init__()
        assert stride == 1 or stride == 2, ""invalid stride {:}"".format(stride)
        assert isinstance(iCs, tuple) or isinstance(
            iCs, list
        ), ""invalid type of iCs : {:}"".format(iCs)
        assert len(iCs) == 3, ""invalid lengths of iCs : {:}"".format(iCs)

        self.conv_a = ConvBNReLU(
            iCs[0],
            iCs[1],
            3,
            stride,
            1,
            False,
            has_avg=False,
            has_bn=True,
            has_relu=True,
        )
        self.conv_b = ConvBNReLU(
            iCs[1], iCs[2], 3, 1, 1, False, has_avg=False, has_bn=True, has_relu=False
        )
        residual_in = iCs[0]
        if stride == 2:
            self.downsample = ConvBNReLU(
                iCs[0],
                iCs[2],
                1,
                1,
                0,
                False,
                has_avg=True,
                has_bn=False,
                has_relu=False,
            )
            residual_in = iCs[2]
        elif iCs[0] != iCs[2]:
            self.downsample = ConvBNReLU(
                iCs[0],
                iCs[2],
                1,
                1,
                0,
                False,
                has_avg=False,
                has_bn=True,
                has_relu=False,
            )
        else:
            self.downsample = None
        # self.out_dim  = max(residual_in, iCs[2])
        self.out_dim = iCs[2]","ConvBNReLU(iCs[0], iCs[2], 1, 1, 0, False, has_avg=True, has_bn=False, has_relu=False)","ConvBNReLU(*iCs[::2], 1, 1, 0, False, has_avg=True, has_bn=False, has_relu=False)","iterable_zj[0], iterable_zj[2]",*iCs[::2],*iCs[:4:2],0
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/xautodl/models/shape_infers/InferCifarResNet_width.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/xautodl/models/shape_infers/InferCifarResNet_width.py,ResNetBasicblock,__init__$59,"def __init__(self, iCs, stride):
        super(ResNetBasicblock, self).__init__()
        assert stride == 1 or stride == 2, ""invalid stride {:}"".format(stride)
        assert isinstance(iCs, tuple) or isinstance(
            iCs, list
        ), ""invalid type of iCs : {:}"".format(iCs)
        assert len(iCs) == 3, ""invalid lengths of iCs : {:}"".format(iCs)

        self.conv_a = ConvBNReLU(
            iCs[0],
            iCs[1],
            3,
            stride,
            1,
            False,
            has_avg=False,
            has_bn=True,
            has_relu=True,
        )
        self.conv_b = ConvBNReLU(
            iCs[1], iCs[2], 3, 1, 1, False, has_avg=False, has_bn=True, has_relu=False
        )
        residual_in = iCs[0]
        if stride == 2:
            self.downsample = ConvBNReLU(
                iCs[0],
                iCs[2],
                1,
                1,
                0,
                False,
                has_avg=True,
                has_bn=False,
                has_relu=False,
            )
            residual_in = iCs[2]
        elif iCs[0] != iCs[2]:
            self.downsample = ConvBNReLU(
                iCs[0],
                iCs[2],
                1,
                1,
                0,
                False,
                has_avg=False,
                has_bn=True,
                has_relu=False,
            )
        else:
            self.downsample = None
        # self.out_dim  = max(residual_in, iCs[2])
        self.out_dim = iCs[2]","ConvBNReLU(iCs[0], iCs[2], 1, 1, 0, False, has_avg=False, has_bn=True, has_relu=False)","ConvBNReLU(*iCs[::2], 1, 1, 0, False, has_avg=False, has_bn=True, has_relu=False)","iterable_zj[0], iterable_zj[2]",*iCs[::2],*iCs[:4:2],0
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/relay/op/vision/_vision.py,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/vision/_vision.py,,roi_align_shape_func$146,"def roi_align_shape_func(attrs, inputs, _):
    if attrs.layout == ""NCHW"":
        return [_roi_align_shape_func_nchw(inputs[0], inputs[1], convert(attrs.pooled_size))]
    assert attrs.layout == ""NHWC"", ""layout must be NCHW or NHWC.""
    return [_roi_align_shape_func_nhwc(inputs[0], inputs[1], convert(attrs.pooled_size))]","_roi_align_shape_func_nhwc(inputs[0], inputs[1], convert(attrs.pooled_size))","_roi_align_shape_func_nhwc(*inputs[:2], convert(attrs.pooled_size))","iterable_zj[0], iterable_zj[1]",*inputs[:2],*inputs[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/relay/op/vision/_vision.py,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/vision/_vision.py,,roi_align_shape_func$146,"def roi_align_shape_func(attrs, inputs, _):
    if attrs.layout == ""NCHW"":
        return [_roi_align_shape_func_nchw(inputs[0], inputs[1], convert(attrs.pooled_size))]
    assert attrs.layout == ""NHWC"", ""layout must be NCHW or NHWC.""
    return [_roi_align_shape_func_nhwc(inputs[0], inputs[1], convert(attrs.pooled_size))]","_roi_align_shape_func_nchw(inputs[0], inputs[1], convert(attrs.pooled_size))","_roi_align_shape_func_nchw(*inputs[:2], convert(attrs.pooled_size))","iterable_zj[0], iterable_zj[1]",*inputs[:2],*inputs[:2],1
pandapower,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/timeseries/output_writer.py,https://github.com/e2nIEE/pandapower/tree/master/pandapower/timeseries/output_writer.py,OutputWriter,log_variable$318,"def log_variable(self, table, variable, index=None, eval_function=None, eval_name=None):
        """"""
        Adds a variable to log during simulation and appends it to output_list.
        INPUT:

        **table** (str) - The DataFrame table where the variable is located as a string (e.g. ""res_bus"")

        **variable** (str) -  variable that should be logged as string (e.g. ""p_mw"")

        OPTIONAL:

        **index** (iterable, None) - Can be either one index or a list of indices, or a numpy array of indices,
        or a pandas Index, or a pandas Series (e.g. net.load.bus) for which
        the variable will be logged. If no index is given, the variable will be logged for all elements in the table

        **eval_function** (function, None) - A function to be applied on the table / variable / index combination.
        example: pd.min or pd.mean

        **eval_name** (str, None) - The name for an applied function. It *must* be unique.
                                    If the name is None the name consists of the table, variable, index and eval function
                                    example: ""max_load_p_mw_values""

        EXAMPLE:
            >>> ow.log_variable('res_bus', 'vm_pu') # add logging for bus voltage magnitudes
            >>> ow.log_variable('res_line', 'loading_percent', index=[0, 2, 5]) # add logging for line loading of lines with indices 0, 2, 5
            >>> ow.log_variable('res_line', 'loading_percent', eval_function=pd.max) # get the highest line loading only

            # Getting the cost function slope for each time step:
            >>> def cost_logging(result, n_columns=2):
            >>>      return array([result[i][0][2] for i in range(len(result))])
            >>> ow.log_variable(""pwl_cost"", ""points"", eval_function=cost_logging)

        """"""
        del_indices = list()
        append_args = set()
        append = True
        # check if new log_variable is already in log_variables. If so either append or delete
        for i, log_args in enumerate(self.log_variables):
            if len(log_args) > 4 and eval_name is not None and log_args[4] == eval_name:
                logger.warning(""eval_name '{}' already exists for table '{}' and variable '{}'. ""
                                 ""Please choose a unique eval_name. ""
                               ""I'll use the default instead."".format(eval_name, log_args[0], log_args[1]))
                eval_name = None
            if log_args[0] == table and log_args[1] == variable:
                # table and variable exist in log_variables
                if eval_function is not None or eval_name is not None:
                    append = True
                    continue
                if len(log_args) == 2 and eval_function is None:
                    # everything from table / variable is logged
                    append = False
                    continue
                if log_args[2] is not None and index is not None and eval_function is None:
                    # if index is given and an index was given before extend the index and get unique
                    log_args[2] = set(log_args[2].extend(index))
                else:
                    del_indices.append(i)
                    append_args.add((table, variable))
                    append = False

        for i in del_indices:
            del self.log_variables[i]
        for log_arg in append_args:
            self.log_variables.append(log_arg)
        if append:
            self.log_variables.append((table, variable, index, eval_function, eval_name))","""eval_name '{}' already exists for table '{}' and variable '{}'. Please choose a unique eval_name. I'll use the default instead."".format(eval_name, log_args[0], log_args[1])","""eval_name '{}' already exists for table '{}' and variable '{}'. Please choose a unique eval_name. I'll use the default instead."".format(eval_name, *log_args[:2])","iterable_zj[0], iterable_zj[1]",*log_args[:2],*log_args[:2],1
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2019/moduli.py,https://github.com/3b1b/videos/tree/master/_2019/moduli.py,TriangleModuliSpace,show_xy_rule$303,"def show_xy_rule(self):
        unit_factor = 4.0

        if hasattr(self, ""triangle""):
            triangle = self.triangle
        else:
            triangle = self.get_triangles_and_classes()[0][0]
            verts = triangle.get_vertices()
            angle = PI + angle_of_vector(verts[1] - verts[2])
            triangle.rotate(-angle)
            triangle.set_width(self.example_triangle_width)
            triangle.center()
            self.add(triangle)

        side_trackers = VGroup(*[Line() for x in range(3)])
        side_trackers.set_stroke(width=0, opacity=0)
        side_trackers.triangle = triangle

        def update_side_trackers(st):
            verts = st.triangle.get_vertices()
            st[0].put_start_and_end_on(verts[0], verts[1])
            st[1].put_start_and_end_on(verts[1], verts[2])
            st[2].put_start_and_end_on(verts[2], verts[0])

        side_trackers.add_updater(update_side_trackers)

        def get_length_labels():
            result = VGroup()
            for line in side_trackers:
                vect = normalize(line.get_vector())
                perp_vect = rotate_vector(vect, -90 * DEGREES)
                perp_vect = np.round(perp_vect, 1)
                label = DecimalNumber(line.get_length() / unit_factor)
                label.move_to(line.get_center())
                label.next_to(line.get_center(), perp_vect, buff=0.15)
                result.add(label)
            return result

        side_labels = always_redraw(get_length_labels)

        b_label, c_label, a_label = side_labels
        b_side, c_side, a_side = side_trackers

        # Rescale
        self.add(side_trackers)
        self.play(LaggedStartMap(FadeIn, side_labels, lag_ratio=0.3, run_time=1))
        self.add(side_labels)
        self.wait()
        self.play(triangle.set_width, unit_factor)
        self.play(ShowCreationThenFadeAround(c_label))
        self.wait()

        # Label x and y
        x_label = Tex(""x"")
        y_label = Tex(""y"")
        xy_labels = VGroup(x_label, y_label)
        xy_labels.scale(1.5)

        x_color = self.x1_color
        y_color = self.y1_color

        x_label[0].set_color(x_color)
        y_label[0].set_color(y_color)

        # side_labels.clear_updaters()
        for var, num, vect in zip(xy_labels, [b_label, a_label], [DR, DL]):
            buff = 0.15
            var.move_to(num, vect)
            var.brace = Brace(num, UP)
            var.brace.num = num
            var.brace.add_updater(
                lambda m: m.next_to(m.num, UP, buff=buff)
            )
            var.add_updater(
                lambda m: m.next_to(m.brace, UP, buff=buff)
            )

            var.suspend_updating()
            var.brace.suspend_updating()
            self.play(
                FadeIn(var, DOWN),
                Write(var.brace, run_time=1),
                # MoveToTarget(num)
            )
            self.wait()

        # Show plane
        to_move = VGroup(
            triangle,
            side_labels,
            x_label,
            x_label.brace,
            y_label,
            y_label.brace,
        )

        axes = Axes(
            x_min=-0.25,
            x_max=1.5,
            y_min=-0.25,
            y_max=1.5,
            axis_config={
                ""tick_frequency"": 0.25,
                ""unit_size"": 3,
            }
        )
        x_axis = axes.x_axis
        y_axis = axes.y_axis

        x_axis.add(Tex(""x"", color=x_color).next_to(x_axis, RIGHT))
        y_axis.add(Tex(""y"", color=y_color).next_to(y_axis, UP))

        for axis, vect in [(x_axis, DOWN), (y_axis, LEFT)]:
            axis.add_numbers(
                0.5, 1.0,
                direction=vect,
                num_decimal_places=1,
            )

        axes.to_corner(DR, buff=LARGE_BUFF)

        self.play(
            to_move.to_corner, UL, {""buff"": LARGE_BUFF},
            to_move.shift, MED_LARGE_BUFF * DOWN,
            Write(axes),
        )

        # Show coordinates
        coords = VGroup(b_label.copy(), a_label.copy())

        x_coord, y_coord = coords
        x_coord.add_updater(lambda m: m.set_value(b_side.get_length() / unit_factor))
        y_coord.add_updater(lambda m: m.set_value(a_side.get_length() / unit_factor))

        def get_coord_values():
            return [c.get_value() for c in coords]

        def get_ms_point():
            return axes.c2p(*get_coord_values())

        dot = always_redraw(
            lambda: triangle.copy().set_width(0.1).move_to(get_ms_point())
        )

        y_line = always_redraw(
            lambda: DashedLine(
                x_axis.n2p(x_coord.get_value()),
                get_ms_point(),
                color=y_color,
                stroke_width=1,
            )
        )
        x_line = always_redraw(
            lambda: DashedLine(
                y_axis.n2p(y_coord.get_value()),
                get_ms_point(),
                color=x_color,
                stroke_width=1,
            )
        )

        coord_label = Tex(""("", ""0.00"", "","", ""0.00"", "")"")
        cl_buff = 0
        coord_label.next_to(dot, UR, buff=cl_buff)
        for i, coord in zip([1, 3], coords):
            coord.generate_target()
            coord.target.replace(coord_label[i], dim_to_match=0)
            coord_label[i].set_opacity(0)

        self.play(
            MoveToTarget(x_coord),
            MoveToTarget(y_coord),
            FadeIn(coord_label),
            ReplacementTransform(triangle.copy().set_fill(opacity=0), dot),
        )
        coord_label.add(*coords)
        coord_label.add_updater(lambda m: m.next_to(dot, UR, buff=cl_buff))
        self.add(x_label, y_label, dot)
        self.play(
            ShowCreation(x_line),
            ShowCreation(y_line),
        )
        self.wait()

        # Adjust triangle
        tip_tracker = VectorizedPoint(triangle.get_points()[0])

        def update_triangle(tri):
            point = tip_tracker.get_location()
            tri.get_points()[0] = point
            tri.get_points()[-1] = point
            tri.make_jagged()

        triangle.add_updater(update_triangle)

        self.add(tip_tracker)
        self.play(tip_tracker.shift, 0.5 * LEFT + 1.0 * UP)
        self.play(tip_tracker.shift, 2.0 * DOWN)
        self.play(tip_tracker.shift, 1.5 * RIGHT)
        self.play(tip_tracker.shift, 1.0 * LEFT + 1.0 * UP)
        self.wait()

        # Show box
        t2c = {""x"": x_color, ""y"": y_color}
        ineq1 = Tex(""0"", ""\\le "", ""x"", ""\\le"", ""1"", tex_to_color_map=t2c)
        ineq2 = Tex(""0"", ""\\le "", ""y"", ""\\le"", ""1"", tex_to_color_map=t2c)

        ineqs = VGroup(ineq1, ineq2)
        ineqs.scale(1.5)
        ineqs.arrange(DOWN, buff=MED_LARGE_BUFF)
        ineqs.next_to(triangle, DOWN, buff=1.5)

        box = Square(
            fill_color=GREY_D,
            fill_opacity=0.75,
            stroke_color=GREY_B,
            stroke_width=2,
        )
        box.replace(Line(axes.c2p(0, 0), axes.c2p(1, 1)))
        box_outline = box.copy()
        box_outline.set_fill(opacity=0)
        box_outline.set_stroke(YELLOW, 3)

        self.add(box, axes, x_line, y_line, coord_label, dot)
        self.play(
            FadeIn(box),
            LaggedStartMap(FadeInFromDown, ineqs)
        )
        self.play(
            ShowCreationThenFadeOut(box_outline)
        )
        self.wait()

        # x >= y slice
        region = Polygon(
            axes.c2p(0, 0),
            axes.c2p(1, 0),
            axes.c2p(1, 1),
            fill_color=GREY_BROWN,
            fill_opacity=0.75,
            stroke_color=GREY_BROWN,
            stroke_width=2,
        )
        region_outline = region.copy()
        region_outline.set_fill(opacity=0)
        region_outline.set_stroke(YELLOW, 3)

        x_eq_y_line = Line(axes.c2p(0, 0), axes.c2p(1, 1))
        x_eq_y_line.set_stroke(self.x_eq_y_color, 2)
        x_eq_y_label = Tex(""x=y"", tex_to_color_map=t2c)
        x_eq_y_label.next_to(x_eq_y_line.get_end(), LEFT, MED_LARGE_BUFF)
        x_eq_y_label.shift(0.75 * DL)

        ineq = Tex(""0"", ""\\le"", ""y"", ""\\le"", ""x"", ""\\le"", ""1"")
        ineq.set_color_by_tex(""x"", x_color)
        ineq.set_color_by_tex(""y"", y_color)
        ineq.scale(1.5)
        ineq.move_to(ineqs, LEFT)

        self.add(region, axes, x_line, y_line, coord_label, dot)
        self.play(
            FadeIn(region),
            ShowCreation(x_eq_y_line),
            # FadeInFromDown(x_eq_y_label),
            Transform(ineq1[:2], ineq[:2], remover=True),
            Transform(ineq1[2:], ineq[4:], remover=True),
            Transform(ineq2[:4], ineq[:4], remover=True),
            Transform(ineq2[4:], ineq[6:], remover=True),
        )
        self.add(ineq)
        self.play(ShowCreationThenFadeOut(region_outline))
        self.wait()

        # x + y <= 1 slice
        xpy1_line = Line(axes.c2p(0, 1), axes.c2p(1, 0))
        xpy1_line.set_stroke(GREEN, 2)
        xpy1_label = Tex(""x+y=1"", tex_to_color_map=t2c)
        xpy1_label.next_to(xpy1_line.get_start(), RIGHT, MED_LARGE_BUFF)
        xpy1_label.shift(0.75 * DR)

        xpy1_ineq = Tex(""1 \\le x + y"", tex_to_color_map=t2c)
        xpy1_ineq.scale(1.5)
        xpy1_ineq.next_to(ineq, DOWN, buff=MED_LARGE_BUFF)

        ms_region = Polygon(
            axes.c2p(1, 0),
            axes.c2p(0.5, 0.5),
            axes.c2p(1, 1),
            fill_color=BLUE_E,
            fill_opacity=0.75,
            stroke_width=0,
        )
        ms_outline = ms_region.copy()
        ms_outline.set_fill(opacity=0)
        ms_outline.set_stroke(YELLOW, 2)

        tt_line = Line(DOWN, UP, color=WHITE)
        tt_line.set_height(0.25)
        tt_line.add_updater(lambda m: m.move_to(tip_tracker))

        self.play(
            ShowCreation(xpy1_line),
            # FadeIn(xpy1_label, DOWN),
            FadeIn(xpy1_ineq, UP)
        )
        self.wait()
        self.play(
            tip_tracker.set_y, triangle.get_bottom()[1] + 0.01,
            FadeIn(tt_line),
        )
        self.wait()

        self.add(ms_region, axes, x_line, y_line, coord_label, dot)
        self.play(
            FadeIn(ms_region),
            region.set_fill, GREY_D,
        )
        self.wait()

        # Move tip around
        self.play(
            tip_tracker.shift, UP + RIGHT,
            FadeOut(tt_line),
        )
        self.wait()
        self.play(tip_tracker.shift, 0.5 * DOWN + LEFT, run_time=2)
        self.wait()
        self.play(tip_tracker.shift, UP + 0.7 * LEFT, run_time=2)
        self.wait()
        equilateral_point = triangle.get_bottom() + unit_factor * 0.5 * np.sqrt(3) * UP
        self.play(
            tip_tracker.move_to,
            equilateral_point,
            run_time=2,
        )
        self.wait()

        # Label as moduli space
        ms_words = TexText(""Moduli\\\\"", ""Space"")
        ms_words.scale(1.5)
        ms_words.next_to(ms_region, RIGHT, buff=0.35)
        ms_arrow = Arrow(
            ms_words[1].get_corner(DL),
            ms_region.get_center(),
            path_arc=-90 * DEGREES,
            buff=0.1,
        )
        # ms_arrow.rotate(-10 * DEGREES)
        ms_arrow.shift(0.1 * RIGHT)
        ms_arrow.scale(0.95)

        self.play(
            FadeIn(ms_words, LEFT),
        )
        self.play(ShowCreation(ms_arrow))
        self.wait()

        # Show right triangles
        alpha = np.arcsin(0.8)
        vect = rotate_vector(0.6 * unit_factor * LEFT, -alpha)
        new_tip = triangle.get_corner(DR) + vect

        elbow = VMobject()
        elbow.start_new_path(RIGHT)
        elbow.add_line_to(UR)
        elbow.add_line_to(UP)

        elbow.rotate(3 * TAU / 4 - alpha, about_point=ORIGIN)
        elbow.scale(0.2, about_point=ORIGIN)
        elbow.shift(new_tip)

        elbow_circle = Circle()
        elbow_circle.replace(elbow)
        elbow_circle.scale(3)
        elbow_circle.move_to(new_tip)
        elbow_circle.set_stroke(self.right_color, 3)

        right_words = TexText(""Right triangle"")
        right_words.scale(1.5)
        right_words.set_color(self.right_color)
        right_words.next_to(triangle, DOWN, buff=1.5)

        ineqs = VGroup(ineq, xpy1_ineq)

        self.play(
            tip_tracker.move_to, new_tip,
            FadeOut(ms_words),
            FadeOut(ms_arrow),
        )
        self.play(
            ShowCreation(elbow),
            FadeIn(right_words, UP),
            FadeOut(ineqs, DOWN),
        )
        self.play(
            ShowCreationThenFadeOut(elbow_circle),
        )

        # Show circular arc
        pythag_eq = Tex(""x^2 + y^2"", ""="", ""1"", tex_to_color_map=t2c)
        pythag_eq.scale(1.5)
        pythag_eq.next_to(right_words, DOWN, buff=MED_LARGE_BUFF)

        arc = Arc(
            start_angle=90 * DEGREES,
            angle=-90 * DEGREES,
            color=self.right_color,
        )
        arc.replace(box)

        self.play(
            FadeIn(pythag_eq, UP),
        )
        self.add(arc, arc)
        self.play(ShowCreation(arc))
        self.wait()

        # Acute region
        arc_piece = VMobject()
        arc_piece.pointwise_become_partial(arc, 0.5, 1.0)

        acute_region = VMobject()
        acute_region.start_new_path(axes.c2p(1, 1))
        acute_region.add_line_to(arc_piece.get_start())
        acute_region.append_vectorized_mobject(arc_piece)
        acute_region.add_line_to(axes.c2p(1, 1))
        acute_region.set_fill(self.acute_color, 1)
        acute_region.set_stroke(width=0)

        obtuse_region = VMobject()
        obtuse_region.start_new_path(axes.c2p(1, 0))
        obtuse_region.add_line_to(axes.c2p(0.5, 0.5))
        obtuse_region.add_line_to(arc_piece.get_start())
        obtuse_region.append_vectorized_mobject(arc_piece)
        obtuse_region.set_fill(self.obtuse_color, 1)
        obtuse_region.set_stroke(width=0)

        acute_words = TexText(""Acute triangle"")
        acute_words.set_color(self.acute_color)
        obtuse_words = TexText(""Obtuse triangle"")
        obtuse_words.set_color(self.obtuse_color)
        for words in [acute_words, obtuse_words]:
            words.scale(1.5)
            words.move_to(right_words)

        eq = pythag_eq[-2]
        gt = Tex("">"").replace(eq)
        gt.set_color(self.acute_color)
        lt = Tex(""<"").replace(eq)
        lt.set_color(self.obtuse_color)

        self.add(acute_region, coord_label, x_line, y_line, xpy1_line, x_eq_y_line, dot)
        self.play(
            tip_tracker.shift, 0.5 * UP,
            coord_label.set_opacity, 0,
            FadeOut(elbow),
            FadeIn(acute_region),
            FadeOut(right_words, UP),
            FadeOut(eq, UP),
            FadeIn(acute_words, DOWN),
            FadeIn(gt, DOWN),
        )
        self.wait()
        self.play(tip_tracker.shift, 0.5 * RIGHT)
        self.wait()
        self.add(obtuse_region, coord_label, x_line, y_line, xpy1_line, x_eq_y_line, dot)
        self.play(
            tip_tracker.shift, 1.5 * DOWN,
            FadeIn(obtuse_region),
            FadeOut(acute_words, DOWN),
            FadeOut(gt, DOWN),
            FadeIn(obtuse_words, UP),
            FadeIn(lt, UP),
        )
        self.wait()
        self.play(tip_tracker.shift, 0.5 * LEFT)
        self.play(tip_tracker.shift, 0.5 * DOWN)
        self.play(tip_tracker.shift, 0.5 * RIGHT)
        self.play(tip_tracker.shift, 0.5 * UP)
        self.wait()

        # Ambient changes
        self.play(
            FadeOut(obtuse_words),
            FadeOut(pythag_eq[:-2]),
            FadeOut(pythag_eq[-1]),
            FadeOut(lt),
        )
        self.play(
            tip_tracker.move_to, equilateral_point + 0.25 * DL,
            path_arc=30 * DEGREES,
            run_time=8,
        )","st[0].put_start_and_end_on(verts[0], verts[1])",st[0].put_start_and_end_on(*verts[:2]),"iterable_zj[0], iterable_zj[1]",*verts[:2],*verts[:2],1
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2019/moduli.py,https://github.com/3b1b/videos/tree/master/_2019/moduli.py,TriangleModuliSpace,show_xy_rule$303,"def show_xy_rule(self):
        unit_factor = 4.0

        if hasattr(self, ""triangle""):
            triangle = self.triangle
        else:
            triangle = self.get_triangles_and_classes()[0][0]
            verts = triangle.get_vertices()
            angle = PI + angle_of_vector(verts[1] - verts[2])
            triangle.rotate(-angle)
            triangle.set_width(self.example_triangle_width)
            triangle.center()
            self.add(triangle)

        side_trackers = VGroup(*[Line() for x in range(3)])
        side_trackers.set_stroke(width=0, opacity=0)
        side_trackers.triangle = triangle

        def update_side_trackers(st):
            verts = st.triangle.get_vertices()
            st[0].put_start_and_end_on(verts[0], verts[1])
            st[1].put_start_and_end_on(verts[1], verts[2])
            st[2].put_start_and_end_on(verts[2], verts[0])

        side_trackers.add_updater(update_side_trackers)

        def get_length_labels():
            result = VGroup()
            for line in side_trackers:
                vect = normalize(line.get_vector())
                perp_vect = rotate_vector(vect, -90 * DEGREES)
                perp_vect = np.round(perp_vect, 1)
                label = DecimalNumber(line.get_length() / unit_factor)
                label.move_to(line.get_center())
                label.next_to(line.get_center(), perp_vect, buff=0.15)
                result.add(label)
            return result

        side_labels = always_redraw(get_length_labels)

        b_label, c_label, a_label = side_labels
        b_side, c_side, a_side = side_trackers

        # Rescale
        self.add(side_trackers)
        self.play(LaggedStartMap(FadeIn, side_labels, lag_ratio=0.3, run_time=1))
        self.add(side_labels)
        self.wait()
        self.play(triangle.set_width, unit_factor)
        self.play(ShowCreationThenFadeAround(c_label))
        self.wait()

        # Label x and y
        x_label = Tex(""x"")
        y_label = Tex(""y"")
        xy_labels = VGroup(x_label, y_label)
        xy_labels.scale(1.5)

        x_color = self.x1_color
        y_color = self.y1_color

        x_label[0].set_color(x_color)
        y_label[0].set_color(y_color)

        # side_labels.clear_updaters()
        for var, num, vect in zip(xy_labels, [b_label, a_label], [DR, DL]):
            buff = 0.15
            var.move_to(num, vect)
            var.brace = Brace(num, UP)
            var.brace.num = num
            var.brace.add_updater(
                lambda m: m.next_to(m.num, UP, buff=buff)
            )
            var.add_updater(
                lambda m: m.next_to(m.brace, UP, buff=buff)
            )

            var.suspend_updating()
            var.brace.suspend_updating()
            self.play(
                FadeIn(var, DOWN),
                Write(var.brace, run_time=1),
                # MoveToTarget(num)
            )
            self.wait()

        # Show plane
        to_move = VGroup(
            triangle,
            side_labels,
            x_label,
            x_label.brace,
            y_label,
            y_label.brace,
        )

        axes = Axes(
            x_min=-0.25,
            x_max=1.5,
            y_min=-0.25,
            y_max=1.5,
            axis_config={
                ""tick_frequency"": 0.25,
                ""unit_size"": 3,
            }
        )
        x_axis = axes.x_axis
        y_axis = axes.y_axis

        x_axis.add(Tex(""x"", color=x_color).next_to(x_axis, RIGHT))
        y_axis.add(Tex(""y"", color=y_color).next_to(y_axis, UP))

        for axis, vect in [(x_axis, DOWN), (y_axis, LEFT)]:
            axis.add_numbers(
                0.5, 1.0,
                direction=vect,
                num_decimal_places=1,
            )

        axes.to_corner(DR, buff=LARGE_BUFF)

        self.play(
            to_move.to_corner, UL, {""buff"": LARGE_BUFF},
            to_move.shift, MED_LARGE_BUFF * DOWN,
            Write(axes),
        )

        # Show coordinates
        coords = VGroup(b_label.copy(), a_label.copy())

        x_coord, y_coord = coords
        x_coord.add_updater(lambda m: m.set_value(b_side.get_length() / unit_factor))
        y_coord.add_updater(lambda m: m.set_value(a_side.get_length() / unit_factor))

        def get_coord_values():
            return [c.get_value() for c in coords]

        def get_ms_point():
            return axes.c2p(*get_coord_values())

        dot = always_redraw(
            lambda: triangle.copy().set_width(0.1).move_to(get_ms_point())
        )

        y_line = always_redraw(
            lambda: DashedLine(
                x_axis.n2p(x_coord.get_value()),
                get_ms_point(),
                color=y_color,
                stroke_width=1,
            )
        )
        x_line = always_redraw(
            lambda: DashedLine(
                y_axis.n2p(y_coord.get_value()),
                get_ms_point(),
                color=x_color,
                stroke_width=1,
            )
        )

        coord_label = Tex(""("", ""0.00"", "","", ""0.00"", "")"")
        cl_buff = 0
        coord_label.next_to(dot, UR, buff=cl_buff)
        for i, coord in zip([1, 3], coords):
            coord.generate_target()
            coord.target.replace(coord_label[i], dim_to_match=0)
            coord_label[i].set_opacity(0)

        self.play(
            MoveToTarget(x_coord),
            MoveToTarget(y_coord),
            FadeIn(coord_label),
            ReplacementTransform(triangle.copy().set_fill(opacity=0), dot),
        )
        coord_label.add(*coords)
        coord_label.add_updater(lambda m: m.next_to(dot, UR, buff=cl_buff))
        self.add(x_label, y_label, dot)
        self.play(
            ShowCreation(x_line),
            ShowCreation(y_line),
        )
        self.wait()

        # Adjust triangle
        tip_tracker = VectorizedPoint(triangle.get_points()[0])

        def update_triangle(tri):
            point = tip_tracker.get_location()
            tri.get_points()[0] = point
            tri.get_points()[-1] = point
            tri.make_jagged()

        triangle.add_updater(update_triangle)

        self.add(tip_tracker)
        self.play(tip_tracker.shift, 0.5 * LEFT + 1.0 * UP)
        self.play(tip_tracker.shift, 2.0 * DOWN)
        self.play(tip_tracker.shift, 1.5 * RIGHT)
        self.play(tip_tracker.shift, 1.0 * LEFT + 1.0 * UP)
        self.wait()

        # Show box
        t2c = {""x"": x_color, ""y"": y_color}
        ineq1 = Tex(""0"", ""\\le "", ""x"", ""\\le"", ""1"", tex_to_color_map=t2c)
        ineq2 = Tex(""0"", ""\\le "", ""y"", ""\\le"", ""1"", tex_to_color_map=t2c)

        ineqs = VGroup(ineq1, ineq2)
        ineqs.scale(1.5)
        ineqs.arrange(DOWN, buff=MED_LARGE_BUFF)
        ineqs.next_to(triangle, DOWN, buff=1.5)

        box = Square(
            fill_color=GREY_D,
            fill_opacity=0.75,
            stroke_color=GREY_B,
            stroke_width=2,
        )
        box.replace(Line(axes.c2p(0, 0), axes.c2p(1, 1)))
        box_outline = box.copy()
        box_outline.set_fill(opacity=0)
        box_outline.set_stroke(YELLOW, 3)

        self.add(box, axes, x_line, y_line, coord_label, dot)
        self.play(
            FadeIn(box),
            LaggedStartMap(FadeInFromDown, ineqs)
        )
        self.play(
            ShowCreationThenFadeOut(box_outline)
        )
        self.wait()

        # x >= y slice
        region = Polygon(
            axes.c2p(0, 0),
            axes.c2p(1, 0),
            axes.c2p(1, 1),
            fill_color=GREY_BROWN,
            fill_opacity=0.75,
            stroke_color=GREY_BROWN,
            stroke_width=2,
        )
        region_outline = region.copy()
        region_outline.set_fill(opacity=0)
        region_outline.set_stroke(YELLOW, 3)

        x_eq_y_line = Line(axes.c2p(0, 0), axes.c2p(1, 1))
        x_eq_y_line.set_stroke(self.x_eq_y_color, 2)
        x_eq_y_label = Tex(""x=y"", tex_to_color_map=t2c)
        x_eq_y_label.next_to(x_eq_y_line.get_end(), LEFT, MED_LARGE_BUFF)
        x_eq_y_label.shift(0.75 * DL)

        ineq = Tex(""0"", ""\\le"", ""y"", ""\\le"", ""x"", ""\\le"", ""1"")
        ineq.set_color_by_tex(""x"", x_color)
        ineq.set_color_by_tex(""y"", y_color)
        ineq.scale(1.5)
        ineq.move_to(ineqs, LEFT)

        self.add(region, axes, x_line, y_line, coord_label, dot)
        self.play(
            FadeIn(region),
            ShowCreation(x_eq_y_line),
            # FadeInFromDown(x_eq_y_label),
            Transform(ineq1[:2], ineq[:2], remover=True),
            Transform(ineq1[2:], ineq[4:], remover=True),
            Transform(ineq2[:4], ineq[:4], remover=True),
            Transform(ineq2[4:], ineq[6:], remover=True),
        )
        self.add(ineq)
        self.play(ShowCreationThenFadeOut(region_outline))
        self.wait()

        # x + y <= 1 slice
        xpy1_line = Line(axes.c2p(0, 1), axes.c2p(1, 0))
        xpy1_line.set_stroke(GREEN, 2)
        xpy1_label = Tex(""x+y=1"", tex_to_color_map=t2c)
        xpy1_label.next_to(xpy1_line.get_start(), RIGHT, MED_LARGE_BUFF)
        xpy1_label.shift(0.75 * DR)

        xpy1_ineq = Tex(""1 \\le x + y"", tex_to_color_map=t2c)
        xpy1_ineq.scale(1.5)
        xpy1_ineq.next_to(ineq, DOWN, buff=MED_LARGE_BUFF)

        ms_region = Polygon(
            axes.c2p(1, 0),
            axes.c2p(0.5, 0.5),
            axes.c2p(1, 1),
            fill_color=BLUE_E,
            fill_opacity=0.75,
            stroke_width=0,
        )
        ms_outline = ms_region.copy()
        ms_outline.set_fill(opacity=0)
        ms_outline.set_stroke(YELLOW, 2)

        tt_line = Line(DOWN, UP, color=WHITE)
        tt_line.set_height(0.25)
        tt_line.add_updater(lambda m: m.move_to(tip_tracker))

        self.play(
            ShowCreation(xpy1_line),
            # FadeIn(xpy1_label, DOWN),
            FadeIn(xpy1_ineq, UP)
        )
        self.wait()
        self.play(
            tip_tracker.set_y, triangle.get_bottom()[1] + 0.01,
            FadeIn(tt_line),
        )
        self.wait()

        self.add(ms_region, axes, x_line, y_line, coord_label, dot)
        self.play(
            FadeIn(ms_region),
            region.set_fill, GREY_D,
        )
        self.wait()

        # Move tip around
        self.play(
            tip_tracker.shift, UP + RIGHT,
            FadeOut(tt_line),
        )
        self.wait()
        self.play(tip_tracker.shift, 0.5 * DOWN + LEFT, run_time=2)
        self.wait()
        self.play(tip_tracker.shift, UP + 0.7 * LEFT, run_time=2)
        self.wait()
        equilateral_point = triangle.get_bottom() + unit_factor * 0.5 * np.sqrt(3) * UP
        self.play(
            tip_tracker.move_to,
            equilateral_point,
            run_time=2,
        )
        self.wait()

        # Label as moduli space
        ms_words = TexText(""Moduli\\\\"", ""Space"")
        ms_words.scale(1.5)
        ms_words.next_to(ms_region, RIGHT, buff=0.35)
        ms_arrow = Arrow(
            ms_words[1].get_corner(DL),
            ms_region.get_center(),
            path_arc=-90 * DEGREES,
            buff=0.1,
        )
        # ms_arrow.rotate(-10 * DEGREES)
        ms_arrow.shift(0.1 * RIGHT)
        ms_arrow.scale(0.95)

        self.play(
            FadeIn(ms_words, LEFT),
        )
        self.play(ShowCreation(ms_arrow))
        self.wait()

        # Show right triangles
        alpha = np.arcsin(0.8)
        vect = rotate_vector(0.6 * unit_factor * LEFT, -alpha)
        new_tip = triangle.get_corner(DR) + vect

        elbow = VMobject()
        elbow.start_new_path(RIGHT)
        elbow.add_line_to(UR)
        elbow.add_line_to(UP)

        elbow.rotate(3 * TAU / 4 - alpha, about_point=ORIGIN)
        elbow.scale(0.2, about_point=ORIGIN)
        elbow.shift(new_tip)

        elbow_circle = Circle()
        elbow_circle.replace(elbow)
        elbow_circle.scale(3)
        elbow_circle.move_to(new_tip)
        elbow_circle.set_stroke(self.right_color, 3)

        right_words = TexText(""Right triangle"")
        right_words.scale(1.5)
        right_words.set_color(self.right_color)
        right_words.next_to(triangle, DOWN, buff=1.5)

        ineqs = VGroup(ineq, xpy1_ineq)

        self.play(
            tip_tracker.move_to, new_tip,
            FadeOut(ms_words),
            FadeOut(ms_arrow),
        )
        self.play(
            ShowCreation(elbow),
            FadeIn(right_words, UP),
            FadeOut(ineqs, DOWN),
        )
        self.play(
            ShowCreationThenFadeOut(elbow_circle),
        )

        # Show circular arc
        pythag_eq = Tex(""x^2 + y^2"", ""="", ""1"", tex_to_color_map=t2c)
        pythag_eq.scale(1.5)
        pythag_eq.next_to(right_words, DOWN, buff=MED_LARGE_BUFF)

        arc = Arc(
            start_angle=90 * DEGREES,
            angle=-90 * DEGREES,
            color=self.right_color,
        )
        arc.replace(box)

        self.play(
            FadeIn(pythag_eq, UP),
        )
        self.add(arc, arc)
        self.play(ShowCreation(arc))
        self.wait()

        # Acute region
        arc_piece = VMobject()
        arc_piece.pointwise_become_partial(arc, 0.5, 1.0)

        acute_region = VMobject()
        acute_region.start_new_path(axes.c2p(1, 1))
        acute_region.add_line_to(arc_piece.get_start())
        acute_region.append_vectorized_mobject(arc_piece)
        acute_region.add_line_to(axes.c2p(1, 1))
        acute_region.set_fill(self.acute_color, 1)
        acute_region.set_stroke(width=0)

        obtuse_region = VMobject()
        obtuse_region.start_new_path(axes.c2p(1, 0))
        obtuse_region.add_line_to(axes.c2p(0.5, 0.5))
        obtuse_region.add_line_to(arc_piece.get_start())
        obtuse_region.append_vectorized_mobject(arc_piece)
        obtuse_region.set_fill(self.obtuse_color, 1)
        obtuse_region.set_stroke(width=0)

        acute_words = TexText(""Acute triangle"")
        acute_words.set_color(self.acute_color)
        obtuse_words = TexText(""Obtuse triangle"")
        obtuse_words.set_color(self.obtuse_color)
        for words in [acute_words, obtuse_words]:
            words.scale(1.5)
            words.move_to(right_words)

        eq = pythag_eq[-2]
        gt = Tex("">"").replace(eq)
        gt.set_color(self.acute_color)
        lt = Tex(""<"").replace(eq)
        lt.set_color(self.obtuse_color)

        self.add(acute_region, coord_label, x_line, y_line, xpy1_line, x_eq_y_line, dot)
        self.play(
            tip_tracker.shift, 0.5 * UP,
            coord_label.set_opacity, 0,
            FadeOut(elbow),
            FadeIn(acute_region),
            FadeOut(right_words, UP),
            FadeOut(eq, UP),
            FadeIn(acute_words, DOWN),
            FadeIn(gt, DOWN),
        )
        self.wait()
        self.play(tip_tracker.shift, 0.5 * RIGHT)
        self.wait()
        self.add(obtuse_region, coord_label, x_line, y_line, xpy1_line, x_eq_y_line, dot)
        self.play(
            tip_tracker.shift, 1.5 * DOWN,
            FadeIn(obtuse_region),
            FadeOut(acute_words, DOWN),
            FadeOut(gt, DOWN),
            FadeIn(obtuse_words, UP),
            FadeIn(lt, UP),
        )
        self.wait()
        self.play(tip_tracker.shift, 0.5 * LEFT)
        self.play(tip_tracker.shift, 0.5 * DOWN)
        self.play(tip_tracker.shift, 0.5 * RIGHT)
        self.play(tip_tracker.shift, 0.5 * UP)
        self.wait()

        # Ambient changes
        self.play(
            FadeOut(obtuse_words),
            FadeOut(pythag_eq[:-2]),
            FadeOut(pythag_eq[-1]),
            FadeOut(lt),
        )
        self.play(
            tip_tracker.move_to, equilateral_point + 0.25 * DL,
            path_arc=30 * DEGREES,
            run_time=8,
        )","st[1].put_start_and_end_on(verts[1], verts[2])",st[1].put_start_and_end_on(*verts[1:3]),"iterable_zj[1], iterable_zj[2]",*verts[1:3],*verts[1:3],1
ODIN,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ODIN/lib/htmlreporter.py,https://github.com/chrismaddalena/ODIN/tree/master/lib/htmlreporter.py,HTMLReporter,create_people_page$306,"def create_people_page(self):
        """"""Create the people.html page in the report directory.""""""
        with open(self.report_path + ""people.html"",""w"") as report:
            self.c.execute(""SELECT * FROM email_addresses ORDER BY email_address ASC"")
            emails = self.c.fetchall()
            self.c.execute(""SELECT * FROM employee_data ORDER BY name ASC"")
            employees = self.c.fetchall()
            self.c.execute(""SELECT * FROM twitter ORDER BY handle ASC"")
            twitter = self.c.fetchall()
            content = """"""
            <html>
            <head><link rel=""stylesheet"" href=""styles.css""></head>
            <title>Email & Social Report</title>
            <body>
            <h1>Email & Social Report</h1>
            <h2>Public Email Addresses & Related Breach Data</h2>
            <p>This table contains discovered email addresses and links to data breaches and posts from
            Have I Been Pwned's database:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Email Address</th>
            <th>Data Breach</th>
            <th>Paste</th>
            </tr>
            """"""
            for row in emails:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
            content += ""</table><p><br /></p>""
            content += """"""
            <h2>Employee Data</h2>
            <p>This table contains any data ODIN was able to collect about employees of the organization:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Employee Name</th>
            <th>Job Title</th>
            <th>LinkedIn URL</th>
            </tr>
            """"""
            for row in employees:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
            content += ""</table><p><br /></p>""
            content += """"""
            <h2>Twitter Profiles</h2>
            <p>This table contains the data collected about Twitter accounts potentally linked to the organization:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Handle</th>
            <th>Real Name</th>
            <th>Follower Count</th>
            <th>Location</th>
            <th>Description</th>
            </tr>
            """"""
            for row in twitter:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2],row[3],row[4])
            content += ""</table><p><br /></p>""
            content += """"""
            </body>
            </html>
            """"""
            report.write(content)","'<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(row[0], row[1], row[2])",'<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(*row[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*row[:3],*row[:3],1
ODIN,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ODIN/lib/htmlreporter.py,https://github.com/chrismaddalena/ODIN/tree/master/lib/htmlreporter.py,HTMLReporter,create_people_page$306,"def create_people_page(self):
        """"""Create the people.html page in the report directory.""""""
        with open(self.report_path + ""people.html"",""w"") as report:
            self.c.execute(""SELECT * FROM email_addresses ORDER BY email_address ASC"")
            emails = self.c.fetchall()
            self.c.execute(""SELECT * FROM employee_data ORDER BY name ASC"")
            employees = self.c.fetchall()
            self.c.execute(""SELECT * FROM twitter ORDER BY handle ASC"")
            twitter = self.c.fetchall()
            content = """"""
            <html>
            <head><link rel=""stylesheet"" href=""styles.css""></head>
            <title>Email & Social Report</title>
            <body>
            <h1>Email & Social Report</h1>
            <h2>Public Email Addresses & Related Breach Data</h2>
            <p>This table contains discovered email addresses and links to data breaches and posts from
            Have I Been Pwned's database:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Email Address</th>
            <th>Data Breach</th>
            <th>Paste</th>
            </tr>
            """"""
            for row in emails:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
            content += ""</table><p><br /></p>""
            content += """"""
            <h2>Employee Data</h2>
            <p>This table contains any data ODIN was able to collect about employees of the organization:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Employee Name</th>
            <th>Job Title</th>
            <th>LinkedIn URL</th>
            </tr>
            """"""
            for row in employees:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
            content += ""</table><p><br /></p>""
            content += """"""
            <h2>Twitter Profiles</h2>
            <p>This table contains the data collected about Twitter accounts potentally linked to the organization:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Handle</th>
            <th>Real Name</th>
            <th>Follower Count</th>
            <th>Location</th>
            <th>Description</th>
            </tr>
            """"""
            for row in twitter:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2],row[3],row[4])
            content += ""</table><p><br /></p>""
            content += """"""
            </body>
            </html>
            """"""
            report.write(content)","'<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(row[0], row[1], row[2])",'<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(*row[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*row[:3],*row[:3],1
ODIN,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ODIN/lib/htmlreporter.py,https://github.com/chrismaddalena/ODIN/tree/master/lib/htmlreporter.py,HTMLReporter,create_people_page$306,"def create_people_page(self):
        """"""Create the people.html page in the report directory.""""""
        with open(self.report_path + ""people.html"",""w"") as report:
            self.c.execute(""SELECT * FROM email_addresses ORDER BY email_address ASC"")
            emails = self.c.fetchall()
            self.c.execute(""SELECT * FROM employee_data ORDER BY name ASC"")
            employees = self.c.fetchall()
            self.c.execute(""SELECT * FROM twitter ORDER BY handle ASC"")
            twitter = self.c.fetchall()
            content = """"""
            <html>
            <head><link rel=""stylesheet"" href=""styles.css""></head>
            <title>Email & Social Report</title>
            <body>
            <h1>Email & Social Report</h1>
            <h2>Public Email Addresses & Related Breach Data</h2>
            <p>This table contains discovered email addresses and links to data breaches and posts from
            Have I Been Pwned's database:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Email Address</th>
            <th>Data Breach</th>
            <th>Paste</th>
            </tr>
            """"""
            for row in emails:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
            content += ""</table><p><br /></p>""
            content += """"""
            <h2>Employee Data</h2>
            <p>This table contains any data ODIN was able to collect about employees of the organization:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Employee Name</th>
            <th>Job Title</th>
            <th>LinkedIn URL</th>
            </tr>
            """"""
            for row in employees:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
            content += ""</table><p><br /></p>""
            content += """"""
            <h2>Twitter Profiles</h2>
            <p>This table contains the data collected about Twitter accounts potentally linked to the organization:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Handle</th>
            <th>Real Name</th>
            <th>Follower Count</th>
            <th>Location</th>
            <th>Description</th>
            </tr>
            """"""
            for row in twitter:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2],row[3],row[4])
            content += ""</table><p><br /></p>""
            content += """"""
            </body>
            </html>
            """"""
            report.write(content)","'<tr><td>{}</td><td>{}</td><td>{}</td><td>{}</td><td>{}</td></tr>'.format(row[0], row[1], row[2], row[3], row[4])",'<tr><td>{}</td><td>{}</td><td>{}</td><td>{}</td><td>{}</td></tr>'.format(*row[:5]),"iterable_zj[0], iterable_zj[1], iterable_zj[2], iterable_zj[3], iterable_zj[4]",*row[:5],*row[:5],1
veusz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/veusz/veusz/datasets/twod.py,https://github.com/veusz/veusz/tree/master/veusz/datasets/twod.py,Dataset2DBase,getPixelEdges$54,"def getPixelEdges(self, scalefnx=None, scalefny=None):
        """"""Return edges for x and y pixels.

        scalefnx/y: function to convert values to plotted pixel scale
                    (used to calculate edges from centres on screen)
        """"""

        def fromcentres(vals, scalefn):
            """"""Calculate edges from centres.""""""
            if scalefn:
                vals = scalefn(vals)

            if len(vals) == 0:
                e = []
            elif len(vals) == 1:
                if vals[0] != 0:
                    e = [0, vals[0]*2]
                else:
                    e = [0, 1]
            else:
                e = N.concatenate((
                    [vals[0] - 0.5*(vals[1]-vals[0])],
                    0.5*(vals[:-1] + vals[1:]),
                    [vals[-1] + 0.5*(vals[-1]-vals[-2])]
                ))
            return N.array(e)

        if self.xedge is not None:
            xg = self.xedge
            if scalefnx:
                xg = scalefnx(xg)
        elif self.xcent is not None:
            xg = fromcentres(self.xcent, scalefnx)
        else:
            xg = N.linspace(
                self.xrange[0], self.xrange[1], self.data.shape[1]+1)
            if scalefnx:
                xg = scalefnx(xg)

        if self.yedge is not None:
            yg = self.yedge
            if scalefny:
                yg = scalefny(yg)
        elif self.ycent is not None:
            yg = fromcentres(self.ycent, scalefny)
        else:
            yg = N.linspace(
                self.yrange[0], self.yrange[1], self.data.shape[0]+1)
            if scalefny:
                yg = scalefny(yg)

        return xg, yg","N.linspace(self.xrange[0], self.xrange[1], self.data.shape[1] + 1)","N.linspace(*self.xrange[:2], self.data.shape[1] + 1)","iterable_zj[0], iterable_zj[1]",*self.xrange[:2],*self.xrange[:2],1
veusz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/veusz/veusz/datasets/twod.py,https://github.com/veusz/veusz/tree/master/veusz/datasets/twod.py,Dataset2DBase,getPixelEdges$54,"def getPixelEdges(self, scalefnx=None, scalefny=None):
        """"""Return edges for x and y pixels.

        scalefnx/y: function to convert values to plotted pixel scale
                    (used to calculate edges from centres on screen)
        """"""

        def fromcentres(vals, scalefn):
            """"""Calculate edges from centres.""""""
            if scalefn:
                vals = scalefn(vals)

            if len(vals) == 0:
                e = []
            elif len(vals) == 1:
                if vals[0] != 0:
                    e = [0, vals[0]*2]
                else:
                    e = [0, 1]
            else:
                e = N.concatenate((
                    [vals[0] - 0.5*(vals[1]-vals[0])],
                    0.5*(vals[:-1] + vals[1:]),
                    [vals[-1] + 0.5*(vals[-1]-vals[-2])]
                ))
            return N.array(e)

        if self.xedge is not None:
            xg = self.xedge
            if scalefnx:
                xg = scalefnx(xg)
        elif self.xcent is not None:
            xg = fromcentres(self.xcent, scalefnx)
        else:
            xg = N.linspace(
                self.xrange[0], self.xrange[1], self.data.shape[1]+1)
            if scalefnx:
                xg = scalefnx(xg)

        if self.yedge is not None:
            yg = self.yedge
            if scalefny:
                yg = scalefny(yg)
        elif self.ycent is not None:
            yg = fromcentres(self.ycent, scalefny)
        else:
            yg = N.linspace(
                self.yrange[0], self.yrange[1], self.data.shape[0]+1)
            if scalefny:
                yg = scalefny(yg)

        return xg, yg","N.linspace(self.yrange[0], self.yrange[1], self.data.shape[0] + 1)","N.linspace(*self.yrange[:2], self.data.shape[0] + 1)","iterable_zj[0], iterable_zj[1]",*self.yrange[:2],*self.yrange[:2],1
ROMP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ROMP/romp/lib/tracking/tracking_utils/utils.py,https://github.com/Arthur151/ROMP/tree/master/romp/lib/tracking/tracking_utils/utils.py,,compute_ap$179,"def compute_ap(recall, precision):
    """""" Compute the average precision, given the recall and precision curves.
    Code originally from https://github.com/rbgirshick/py-faster-rcnn.
    # Arguments
        recall:    The recall curve (list).
        precision: The precision curve (list).
    # Returns
        The average precision as computed in py-faster-rcnn.
    """"""
    # correct AP calculation
    # first append sentinel values at the end

    mrec = np.concatenate(([0.], recall, [1.]))
    mpre = np.concatenate(([0.], precision, [0.]))

    # compute the precision envelope
    for i in range(mpre.size - 1, 0, -1):
        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

    # to calculate area under PR curve, look for points
    # where X axis (recall) changes value
    i = np.where(mrec[1:] != mrec[:-1])[0]

    # and sum (\Delta recall) * prec
    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap","np.maximum(mpre[i - 1], mpre[i])",np.maximum(*mpre[i - 1:i + 1]),"iterable_zj[i - 1], iterable_zj[i]",*mpre[i-1:i+1],*mpre[i - 1:i + 1],1
decision-transformer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/decision-transformer/atari/mingpt/model_atari.py,https://github.com/kzl/decision-transformer/tree/master/atari/mingpt/model_atari.py,GPT,forward$220,"def forward(self, states, actions, targets=None, rtgs=None, timesteps=None):
        # states: (batch, block_size, 4*84*84)
        # actions: (batch, block_size, 1)
        # targets: (batch, block_size, 1)
        # rtgs: (batch, block_size, 1)
        # timesteps: (batch, 1, 1)

        state_embeddings = self.state_encoder(states.reshape(-1, 4, 84, 84).type(torch.float32).contiguous()) # (batch * block_size, n_embd)
        state_embeddings = state_embeddings.reshape(states.shape[0], states.shape[1], self.config.n_embd) # (batch, block_size, n_embd)
        
        if actions is not None and self.model_type == 'reward_conditioned': 
            rtg_embeddings = self.ret_emb(rtgs.type(torch.float32))
            action_embeddings = self.action_embeddings(actions.type(torch.long).squeeze(-1)) # (batch, block_size, n_embd)

            token_embeddings = torch.zeros((states.shape[0], states.shape[1]*3 - int(targets is None), self.config.n_embd), dtype=torch.float32, device=state_embeddings.device)
            token_embeddings[:,::3,:] = rtg_embeddings
            token_embeddings[:,1::3,:] = state_embeddings
            token_embeddings[:,2::3,:] = action_embeddings[:,-states.shape[1] + int(targets is None):,:]
        elif actions is None and self.model_type == 'reward_conditioned': # only happens at very first timestep of evaluation
            rtg_embeddings = self.ret_emb(rtgs.type(torch.float32))

            token_embeddings = torch.zeros((states.shape[0], states.shape[1]*2, self.config.n_embd), dtype=torch.float32, device=state_embeddings.device)
            token_embeddings[:,::2,:] = rtg_embeddings # really just [:,0,:]
            token_embeddings[:,1::2,:] = state_embeddings # really just [:,1,:]
        elif actions is not None and self.model_type == 'naive':
            action_embeddings = self.action_embeddings(actions.type(torch.long).squeeze(-1)) # (batch, block_size, n_embd)

            token_embeddings = torch.zeros((states.shape[0], states.shape[1]*2 - int(targets is None), self.config.n_embd), dtype=torch.float32, device=state_embeddings.device)
            token_embeddings[:,::2,:] = state_embeddings
            token_embeddings[:,1::2,:] = action_embeddings[:,-states.shape[1] + int(targets is None):,:]
        elif actions is None and self.model_type == 'naive': # only happens at very first timestep of evaluation
            token_embeddings = state_embeddings
        else:
            raise NotImplementedError()

        batch_size = states.shape[0]
        all_global_pos_emb = torch.repeat_interleave(self.global_pos_emb, batch_size, dim=0) # batch_size, traj_length, n_embd

        position_embeddings = torch.gather(all_global_pos_emb, 1, torch.repeat_interleave(timesteps, self.config.n_embd, dim=-1)) + self.pos_emb[:, :token_embeddings.shape[1], :]

        x = self.drop(token_embeddings + position_embeddings)
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.head(x)

        if actions is not None and self.model_type == 'reward_conditioned':
            logits = logits[:, 1::3, :] # only keep predictions from state_embeddings
        elif actions is None and self.model_type == 'reward_conditioned':
            logits = logits[:, 1:, :]
        elif actions is not None and self.model_type == 'naive':
            logits = logits[:, ::2, :] # only keep predictions from state_embeddings
        elif actions is None and self.model_type == 'naive':
            logits = logits # for completeness
        else:
            raise NotImplementedError()

        # if we are given some desired targets also calculate the loss
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))

        return logits, loss","state_embeddings.reshape(states.shape[0], states.shape[1], self.config.n_embd)","state_embeddings.reshape(*states.shape[:2], self.config.n_embd)","iterable_zj[0], iterable_zj[1]",*states.shape[:2],*states.shape[:2],1
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/interpolate/_interpolate.py,https://github.com/scipy/scipy/tree/master/scipy/interpolate/_interpolate.py,NdPPoly,integrate_1d$2246,"def integrate_1d(self, a, b, axis, extrapolate=None):
        r""""""
        Compute NdPPoly representation for one dimensional definite integral

        The result is a piecewise polynomial representing the integral:

        .. math::

           p(y, z, ...) = \int_a^b dx\, p(x, y, z, ...)

        where the dimension integrated over is specified with the
        `axis` parameter.

        Parameters
        ----------
        a, b : float
            Lower and upper bound for integration.
        axis : int
            Dimension over which to compute the 1-D integrals
        extrapolate : bool, optional
            Whether to extrapolate to out-of-bounds points based on first
            and last intervals, or to return NaNs.

        Returns
        -------
        ig : NdPPoly or array-like
            Definite integral of the piecewise polynomial over [a, b].
            If the polynomial was 1D, an array is returned,
            otherwise, an NdPPoly object.

        """"""
        if extrapolate is None:
            extrapolate = self.extrapolate
        else:
            extrapolate = bool(extrapolate)

        ndim = len(self.x)
        axis = int(axis) % ndim

        # reuse 1-D integration routines
        c = self.c
        swap = list(range(c.ndim))
        swap.insert(0, swap[axis])
        del swap[axis + 1]
        swap.insert(1, swap[ndim + axis])
        del swap[ndim + axis + 1]

        c = c.transpose(swap)
        p = PPoly.construct_fast(c.reshape(c.shape[0], c.shape[1], -1),
                                 self.x[axis],
                                 extrapolate=extrapolate)
        out = p.integrate(a, b, extrapolate=extrapolate)

        # Construct result
        if ndim == 1:
            return out.reshape(c.shape[2:])
        else:
            c = out.reshape(c.shape[2:])
            x = self.x[:axis] + self.x[axis+1:]
            return self.construct_fast(c, x, extrapolate=extrapolate)","c.reshape(c.shape[0], c.shape[1], -1)","c.reshape(*c.shape[:2], -1)","iterable_zj[0], iterable_zj[1]",*c.shape[:2],*c.shape[:2],1
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/mission/foundsettlement.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/mission/foundsettlement.py,FoundSettlement,save$47,"def save(self, db):
		super().save(db)
		db(""INSERT INTO ai_mission_found_settlement(rowid, land_manager, ship, x, y, state) VALUES(?, ?, ?, ?, ?, ?)"",
			self.worldid, self.land_manager.worldid, self.ship.worldid, self.coords[0], self.coords[1], self.state.index)","db('INSERT INTO ai_mission_found_settlement(rowid, land_manager, ship, x, y, state) VALUES(?, ?, ?, ?, ?, ?)', self.worldid, self.land_manager.worldid, self.ship.worldid, self.coords[0], self.coords[1], self.state.index)","db('INSERT INTO ai_mission_found_settlement(rowid, land_manager, ship, x, y, state) VALUES(?, ?, ?, ?, ?, ?)', self.worldid, self.land_manager.worldid, self.ship.worldid, *self.coords[:2], self.state.index)","iterable_zj[0], iterable_zj[1]",*self.coords[:2],*self.coords[:2],1
GRU4Rec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GRU4Rec/gpu_ops.py,https://github.com/hidasib/GRU4Rec/tree/master//gpu_ops.py,,gpu_diag$23,"def gpu_diag(X, keepdims=False, disable_custom_op=disable_custom_op):
    if disable_custom_op:
        return T.switch(T.gt(X.shape[0], X.shape[1]), gpu_diag_tall(X, keepdims), gpu_diag_wide(X, keepdims))
    else:
        return cto.GpuExtractDiag2D(keepdims=keepdims)(X)","T.gt(X.shape[0], X.shape[1])",T.gt(*X.shape[:2]),"iterable_zj[0], iterable_zj[1]",*X.shape[:2],*X.shape[:2],1
dilation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dilation/test.py,https://github.com/fyu/dilation/tree/master//test.py,,test_bin$209,"def test_bin(options):
    label_margin = 0
    input_zoom = 8
    pad = 0
    if options.up:
        zoom = 1
    else:
        zoom = 8

    if options.gpu >= 0:
        caffe.set_mode_gpu()
        caffe.set_device(options.gpu)
        print('Using GPU ', options.gpu)
    else:
        caffe.set_mode_cpu()
        print('Using CPU')

    net = caffe.Net(options.deploy_net, options.weights, caffe.TEST)

    image_paths = [line.strip() for line in open(options.image_list, 'r')]
    bin_paths = [line.strip() for line in open(options.bin_list, 'r')]
    names = [splitext(split(p)[1])[0] for p in bin_paths]

    assert len(image_paths) == len(bin_paths)

    input_dims = net.blobs['data'].shape
    assert input_dims[0] == 1
    batch_size, num_channels, input_height, input_width = input_dims
    caffe_in = np.zeros(input_dims, dtype=np.float32)

    bin_test_image = read_array(bin_paths[0])
    bin_test_image_shape = bin_test_image.shape
    assert bin_test_image_shape[1] <= input_height and \
        bin_test_image_shape[2] <= input_width, \
        'input_size should be greater than bin image size {} x {}'.format(
            bin_test_image_shape[1], bin_test_image_shape[2])

    result_list = []

    for i in range(len(image_paths)):
        print('Predicting', bin_paths[i])
        image = cv2.imread(image_paths[i])
        image_size = image.shape
        if input_zoom != 1:
            image_rows = image_size[0] // input_zoom + \
                         (1 if image_size[0] % input_zoom != 0 else 0)
            image_cols = image_size[1] // input_zoom + \
                         (1 if image_size[1] % input_zoom != 0 else 0)
        else:
            image_rows = image_size[0]
            image_cols = image_size[1]
        image_bin = read_array(bin_paths[i])
        image_bin = image_bin[:, :image_rows, :image_cols]

        top = label_margin
        bottom = input_height - top - image_rows
        left = label_margin
        right = input_width - left - image_cols

        for j in range(num_channels):
            if pad == 1:
                caffe_in[0][j] = cv2.copyMakeBorder(
                    image_bin[j], top, bottom, left, right,
                    cv2.BORDER_REFLECT_101)
            elif pad == 0:
                caffe_in[0][j] = cv2.copyMakeBorder(
                    image_bin[j], top, bottom, left, right,
                    cv2.BORDER_CONSTANT)
        out = net.forward_all(**{net.inputs[0]: caffe_in})
        prob = out['prob'][0]
        if zoom > 1:
            prob = util.interp_map(prob, zoom, image_size[1], image_size[0])
        else:
            prob = prob[:, :image_size[0], :image_size[1]]
        prediction = np.argmax(prob.transpose([1, 2, 0]), axis=2)
        out_path = join(options.result_dir, names[i] + '.png')
        print('Writing', out_path)
        cv2.imwrite(out_path, prediction)
        result_list.append(out_path)

    print('================================')
    print('All results are generated.')
    print('================================')

    result_list_path = join(options.result_dir, 'results.txt')
    print('Writing', result_list_path)
    with open(result_list_path, 'w') as fp:
        fp.write('\n'.join(result_list))","'input_size should be greater than bin image size {} x {}'.format(bin_test_image_shape[1], bin_test_image_shape[2])",'input_size should be greater than bin image size {} x {}'.format(*bin_test_image_shape[1:3]),"iterable_zj[1], iterable_zj[2]",*bin_test_image_shape[1:3],*bin_test_image_shape[1:3],1
ROMP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ROMP/romp/lib/tracking/matching.py,https://github.com/Arthur151/ROMP/tree/master/romp/lib/tracking/matching.py,,merge_matches$11,"def merge_matches(m1, m2, shape):
    O,P,Q = shape
    m1 = np.asarray(m1)
    m2 = np.asarray(m2)

    M1 = scipy.sparse.coo_matrix((np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))
    M2 = scipy.sparse.coo_matrix((np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))

    mask = M1*M2
    match = mask.nonzero()
    match = list(zip(match[0], match[1]))
    unmatched_O = tuple(set(range(O)) - set([i for i, j in match]))
    unmatched_Q = tuple(set(range(Q)) - set([j for i, j in match]))

    return match, unmatched_O, unmatched_Q","zip(match[0], match[1])",zip(*match[:2]),"iterable_zj[0], iterable_zj[1]",*match[:2],*match[:2],1
jax,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jax/jax/_src/lax/convolution.py,https://github.com/google/jax/tree/master/jax/_src/lax/convolution.py,,_conv_general_dilated_transpose_lhs$445,"def _conv_general_dilated_transpose_lhs(
    g, rhs, *, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, feature_group_count, batch_group_count,
    lhs_shape, rhs_shape, precision, preferred_element_type):
  assert type(dimension_numbers) is ConvDimensionNumbers
  assert batch_group_count == 1 or feature_group_count == 1
  lhs_sdims, rhs_sdims, out_sdims = map(_conv_sdims, dimension_numbers)
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  t_rhs_spec = _conv_spec_transpose(rhs_spec)
  if feature_group_count > 1:
    # in addition to switching the dims in the spec, need to move the feature
    # group axis into the transposed rhs's output feature dim
    rhs = _reshape_axis_out_of(rhs_spec[0], feature_group_count, rhs)
    rhs = _reshape_axis_into(rhs_spec[0], rhs_spec[1], rhs)
  elif batch_group_count > 1:
    rhs = _reshape_axis_out_of(rhs_spec[0], batch_group_count, rhs)
    rhs = _reshape_axis_into(rhs_spec[0], rhs_spec[1], rhs)
    feature_group_count = batch_group_count
  trans_dimension_numbers = ConvDimensionNumbers(out_spec, t_rhs_spec, lhs_spec)
  padding = _conv_general_vjp_lhs_padding(
      np.take(lhs_shape, lhs_sdims), np.take(rhs_shape, rhs_sdims),
      window_strides, np.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = lax.rev(rhs, rhs_sdims)
  out = conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers,
      feature_group_count=feature_group_count,
      batch_group_count=1, precision=precision,
      preferred_element_type=preferred_element_type)
  if batch_group_count > 1:
    out = _reshape_axis_out_of(lhs_spec[1], batch_group_count, out)
    out = _reshape_axis_into(lhs_spec[1], lhs_spec[0], out)
  return out","_reshape_axis_into(rhs_spec[0], rhs_spec[1], rhs)","_reshape_axis_into(*rhs_spec[:2], rhs)","iterable_zj[0], iterable_zj[1]",*rhs_spec[:2],*rhs_spec[:2],1
jax,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jax/jax/_src/lax/convolution.py,https://github.com/google/jax/tree/master/jax/_src/lax/convolution.py,,_conv_general_dilated_transpose_lhs$445,"def _conv_general_dilated_transpose_lhs(
    g, rhs, *, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, feature_group_count, batch_group_count,
    lhs_shape, rhs_shape, precision, preferred_element_type):
  assert type(dimension_numbers) is ConvDimensionNumbers
  assert batch_group_count == 1 or feature_group_count == 1
  lhs_sdims, rhs_sdims, out_sdims = map(_conv_sdims, dimension_numbers)
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  t_rhs_spec = _conv_spec_transpose(rhs_spec)
  if feature_group_count > 1:
    # in addition to switching the dims in the spec, need to move the feature
    # group axis into the transposed rhs's output feature dim
    rhs = _reshape_axis_out_of(rhs_spec[0], feature_group_count, rhs)
    rhs = _reshape_axis_into(rhs_spec[0], rhs_spec[1], rhs)
  elif batch_group_count > 1:
    rhs = _reshape_axis_out_of(rhs_spec[0], batch_group_count, rhs)
    rhs = _reshape_axis_into(rhs_spec[0], rhs_spec[1], rhs)
    feature_group_count = batch_group_count
  trans_dimension_numbers = ConvDimensionNumbers(out_spec, t_rhs_spec, lhs_spec)
  padding = _conv_general_vjp_lhs_padding(
      np.take(lhs_shape, lhs_sdims), np.take(rhs_shape, rhs_sdims),
      window_strides, np.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = lax.rev(rhs, rhs_sdims)
  out = conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers,
      feature_group_count=feature_group_count,
      batch_group_count=1, precision=precision,
      preferred_element_type=preferred_element_type)
  if batch_group_count > 1:
    out = _reshape_axis_out_of(lhs_spec[1], batch_group_count, out)
    out = _reshape_axis_into(lhs_spec[1], lhs_spec[0], out)
  return out","_reshape_axis_into(rhs_spec[0], rhs_spec[1], rhs)","_reshape_axis_into(*rhs_spec[:2], rhs)","iterable_zj[0], iterable_zj[1]",*rhs_spec[:2],*rhs_spec[:2],1
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/lib/imageToGcode.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/imageToGcode.py,,arc_dir$1093,"def arc_dir(plane, c, p1, p2, p3):
    xc, yc = c
    x1, y1 = get_pts(plane, p1[0], p1[1], p1[2])
    x2, y2 = get_pts(plane, p2[0], p2[1], p2[2])
    x3, y3 = get_pts(plane, p3[0], p3[1], p3[2])

    theta_start = math.atan2(y1 - yc, x1 - xc)
    theta_mid = math.atan2(y2 - yc, x2 - xc)
    theta_end = math.atan2(y3 - yc, x3 - xc)

    if theta_mid < theta_start:
        theta_mid = theta_mid + 2 * math.pi
    while theta_end < theta_mid:
        theta_end = theta_end + 2 * math.pi

    return theta_end < 2 * math.pi","get_pts(plane, p1[0], p1[1], p1[2])","get_pts(plane, *p1[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*p1[:3],*p1[:3],1
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/lib/imageToGcode.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/imageToGcode.py,,arc_dir$1093,"def arc_dir(plane, c, p1, p2, p3):
    xc, yc = c
    x1, y1 = get_pts(plane, p1[0], p1[1], p1[2])
    x2, y2 = get_pts(plane, p2[0], p2[1], p2[2])
    x3, y3 = get_pts(plane, p3[0], p3[1], p3[2])

    theta_start = math.atan2(y1 - yc, x1 - xc)
    theta_mid = math.atan2(y2 - yc, x2 - xc)
    theta_end = math.atan2(y3 - yc, x3 - xc)

    if theta_mid < theta_start:
        theta_mid = theta_mid + 2 * math.pi
    while theta_end < theta_mid:
        theta_end = theta_end + 2 * math.pi

    return theta_end < 2 * math.pi","get_pts(plane, p2[0], p2[1], p2[2])","get_pts(plane, *p2[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*p2[:3],*p2[:3],1
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/lib/imageToGcode.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/imageToGcode.py,,arc_dir$1093,"def arc_dir(plane, c, p1, p2, p3):
    xc, yc = c
    x1, y1 = get_pts(plane, p1[0], p1[1], p1[2])
    x2, y2 = get_pts(plane, p2[0], p2[1], p2[2])
    x3, y3 = get_pts(plane, p3[0], p3[1], p3[2])

    theta_start = math.atan2(y1 - yc, x1 - xc)
    theta_mid = math.atan2(y2 - yc, x2 - xc)
    theta_end = math.atan2(y3 - yc, x3 - xc)

    if theta_mid < theta_start:
        theta_mid = theta_mid + 2 * math.pi
    while theta_end < theta_mid:
        theta_end = theta_end + 2 * math.pi

    return theta_end < 2 * math.pi","get_pts(plane, p3[0], p3[1], p3[2])","get_pts(plane, *p3[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*p3[:3],*p3[:3],1
GCNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GCNet/mmdet/ops/dcn/functions/deform_conv.py,https://github.com/xvjiarui/GCNet/tree/master/mmdet/ops/dcn/functions/deform_conv.py,ModulatedDeformConvFunction,backward$146,"def backward(ctx, grad_output):
        if not grad_output.is_cuda:
            raise NotImplementedError
        input, offset, mask, weight, bias = ctx.saved_tensors
        grad_input = torch.zeros_like(input)
        grad_offset = torch.zeros_like(offset)
        grad_mask = torch.zeros_like(mask)
        grad_weight = torch.zeros_like(weight)
        grad_bias = torch.zeros_like(bias)
        deform_conv_cuda.modulated_deform_conv_cuda_backward(
            input, weight, bias, ctx._bufs[0], offset, mask, ctx._bufs[1],
            grad_input, grad_weight, grad_bias, grad_offset, grad_mask,
            grad_output, weight.shape[2], weight.shape[3], ctx.stride,
            ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation,
            ctx.groups, ctx.deformable_groups, ctx.with_bias)
        if not ctx.with_bias:
            grad_bias = None

        return (grad_input, grad_offset, grad_mask, grad_weight, grad_bias,
                None, None, None, None, None)","deform_conv_cuda.modulated_deform_conv_cuda_backward(input, weight, bias, ctx._bufs[0], offset, mask, ctx._bufs[1], grad_input, grad_weight, grad_bias, grad_offset, grad_mask, grad_output, weight.shape[2], weight.shape[3], ctx.stride, ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation, ctx.groups, ctx.deformable_groups, ctx.with_bias)","deform_conv_cuda.modulated_deform_conv_cuda_backward(input, weight, bias, ctx._bufs[0], offset, mask, ctx._bufs[1], grad_input, grad_weight, grad_bias, grad_offset, grad_mask, grad_output, *weight.shape[2:4], ctx.stride, ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation, ctx.groups, ctx.deformable_groups, ctx.with_bias)","iterable_zj[2], iterable_zj[3]",*weight.shape[2:4],*weight.shape[2:4],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/unittest/test_te_schedule_bound_inference.py,https://github.com/apache/tvm/tree/master/tests/python/unittest/test_te_schedule_bound_inference.py,,test_bound_fusesplit2$172,"def test_bound_fusesplit2():
    m = te.var(""m"")
    l = tvm.runtime.convert(6)
    split = tvm.runtime.convert(3)
    A = te.placeholder((m, l), name=""A"")
    A1 = te.compute((m, l), lambda i, j: A[i, j], name=""A1"")
    A2 = te.compute((m, l), lambda i, j: A1[i, j] + 3, name=""A2"")

    s = te.create_schedule(A2.op)
    fused_axes = s[A2].fuse(A2.op.axis[0], A2.op.axis[1])
    xo, xi = s[A2].split(fused_axes, split)
    s[A1].compute_at(s[A2], xo)

    bounds = tvm.te.schedule.InferBound(s)
    assert isinstance(bounds, tvm.container.Map)
    vars = tvm.runtime.convert({xo.var: tvm.tir.const(5, ""int32"")})
    tvm.testing.assert_prim_expr_equal(
        tvm.tir.stmt_functor.substitute(bounds[A1.op.axis[0]].min, vars), 2
    )
    tvm.testing.assert_prim_expr_equal(
        tvm.tir.stmt_functor.substitute(bounds[A1.op.axis[1]].min, vars), 3
    )
    tvm.testing.assert_prim_expr_equal(
        tvm.tir.stmt_functor.substitute(bounds[A1.op.axis[0]].extent, vars), 1
    )
    tvm.testing.assert_prim_expr_equal(
        tvm.tir.stmt_functor.substitute(bounds[A1.op.axis[1]].extent, vars), 3
    )","s[A2].fuse(A2.op.axis[0], A2.op.axis[1])",s[A2].fuse(*A2.op.axis[:2]),"iterable_zj[0], iterable_zj[1]",*A2.op.axis[:2],*A2.op.axis[:2],1
PaddleSeg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSeg/EISeg/eiseg/util/qt.py,https://github.com/PaddlePaddle/PaddleSeg/tree/master/EISeg/eiseg/util/qt.py,,newIcon$16,"def newIcon(icon):
    if isinstance(icon, list) or isinstance(icon, tuple):
        pixmap = QtGui.QPixmap(100, 100)
        c = icon
        pixmap.fill(QtGui.QColor(c[0], c[1], c[2]))
        return QtGui.QIcon(pixmap)
    icons_dir = osp.join(here, ""../resource"")
    return QtGui.QIcon(osp.join("":/"", icons_dir, f""{icon}.png""))","QtGui.QColor(c[0], c[1], c[2])",QtGui.QColor(*c[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*c[:3],*c[:3],1
sfepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sfepy/sfepy/base/base.py,https://github.com/sfepy/sfepy/tree/master/sfepy/base/base.py,,update_dict_recursively$1144,"def update_dict_recursively(dst, src, tuples_too=False,
                            overwrite_by_none=True):
    """"""
    Update `dst` dictionary recursively using items in `src` dictionary.

    Parameters
    ----------
    dst : dict
        The destination dictionary.
    src : dict
        The source dictionary.
    tuples_too : bool
        If True, recurse also into dictionaries that are members of tuples.
    overwrite_by_none : bool
        If False, do not overwrite destination dictionary values by None.

    Returns
    -------
    dst : dict
        The destination dictionary.
    """"""
    def tuplezip(a):
        if isinstance(a[0], dict) and isinstance(a[1], dict):
            return update_dict_recursively(a[0], a[1], True)
        return a[1]

    for key in src:
        if key in dst:
            if isinstance(src[key], dict) and isinstance(dst[key], dict):
                dst[key] = update_dict_recursively(dst[key],
                                                   src[key], tuples_too)
                continue

            if tuples_too and isinstance(dst[key], tuple) \
                   and isinstance(src[key], tuple):
                out = map(tuplezip, zip(src[key], dst[key]))
                out = tuple(out)
                dst[key] = out[:len(dst[key])]
                continue

        if overwrite_by_none or not src[key] is None:
            dst[key] = src[key]

    return dst","update_dict_recursively(a[0], a[1], True)","update_dict_recursively(*a[:2], True)","iterable_zj[0], iterable_zj[1]",*a[:2],*a[:2],1
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/vision/models/mobilenetv1.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/vision/models/mobilenetv1.py,,_mobilenet$247,"def _mobilenet(arch, pretrained=False, **kwargs):
    model = MobileNetV1(**kwargs)
    if pretrained:
        assert (
            arch in model_urls
        ), ""{} model do not have a pretrained model now, you should set pretrained=False"".format(
            arch
        )
        weight_path = get_weights_path_from_url(
            model_urls[arch][0], model_urls[arch][1]
        )

        param = paddle.load(weight_path)
        model.load_dict(param)

    return model","get_weights_path_from_url(model_urls[arch][0], model_urls[arch][1])",get_weights_path_from_url(*model_urls[arch][:2]),"iterable_zj[0], iterable_zj[1]",*model_urls[arch][:2],*model_urls[arch][:2],1
mmdetection3d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection3d/mmdet3d/datasets/kitti_mono_dataset.py,https://github.com/open-mmlab/mmdetection3d/tree/master/mmdet3d/datasets/kitti_mono_dataset.py,KittiMonoDataset,bbox2result_kitti$271,"def bbox2result_kitti(self,
                          net_outputs,
                          class_names,
                          pklfile_prefix=None,
                          submission_prefix=None):
        """"""Convert 3D detection results to kitti format for evaluation and test
        submission.

        Args:
            net_outputs (list[np.ndarray]): List of array storing the
                inferenced bounding boxes and scores.
            class_names (list[String]): A list of class names.
            pklfile_prefix (str): The prefix of pkl file.
            submission_prefix (str): The prefix of submission file.

        Returns:
            list[dict]: A list of dictionaries with the kitti format.
        """"""
        assert len(net_outputs) == len(self.anno_infos)
        if submission_prefix is not None:
            mmcv.mkdir_or_exist(submission_prefix)

        det_annos = []
        print('\nConverting prediction to KITTI format')
        for idx, pred_dicts in enumerate(
                mmcv.track_iter_progress(net_outputs)):
            annos = []
            info = self.anno_infos[idx]
            sample_idx = info['image']['image_idx']
            image_shape = info['image']['image_shape'][:2]

            box_dict = self.convert_valid_bboxes(pred_dicts, info)
            anno = {
                'name': [],
                'truncated': [],
                'occluded': [],
                'alpha': [],
                'bbox': [],
                'dimensions': [],
                'location': [],
                'rotation_y': [],
                'score': []
            }
            if len(box_dict['bbox']) > 0:
                box_2d_preds = box_dict['bbox']
                box_preds = box_dict['box3d_camera']
                scores = box_dict['scores']
                box_preds_lidar = box_dict['box3d_lidar']
                label_preds = box_dict['label_preds']

                for box, box_lidar, bbox, score, label in zip(
                        box_preds, box_preds_lidar, box_2d_preds, scores,
                        label_preds):
                    bbox[2:] = np.minimum(bbox[2:], image_shape[::-1])
                    bbox[:2] = np.maximum(bbox[:2], [0, 0])
                    anno['name'].append(class_names[int(label)])
                    anno['truncated'].append(0.0)
                    anno['occluded'].append(0)
                    anno['alpha'].append(-np.arctan2(box[0], box[2]) + box[6])
                    anno['bbox'].append(bbox)
                    anno['dimensions'].append(box[3:6])
                    anno['location'].append(box[:3])
                    anno['rotation_y'].append(box[6])
                    anno['score'].append(score)

                anno = {k: np.stack(v) for k, v in anno.items()}
                annos.append(anno)

            else:
                anno = {
                    'name': np.array([]),
                    'truncated': np.array([]),
                    'occluded': np.array([]),
                    'alpha': np.array([]),
                    'bbox': np.zeros([0, 4]),
                    'dimensions': np.zeros([0, 3]),
                    'location': np.zeros([0, 3]),
                    'rotation_y': np.array([]),
                    'score': np.array([]),
                }
                annos.append(anno)

            if submission_prefix is not None:
                curr_file = f'{submission_prefix}/{sample_idx:06d}.txt'
                with open(curr_file, 'w') as f:
                    bbox = anno['bbox']
                    loc = anno['location']
                    dims = anno['dimensions']  # lhw -> hwl

                    for idx in range(len(bbox)):
                        print(
                            '{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} '
                            '{:.4f} {:.4f} {:.4f} '
                            '{:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(
                                anno['name'][idx], anno['alpha'][idx],
                                bbox[idx][0], bbox[idx][1], bbox[idx][2],
                                bbox[idx][3], dims[idx][1], dims[idx][2],
                                dims[idx][0], loc[idx][0], loc[idx][1],
                                loc[idx][2], anno['rotation_y'][idx],
                                anno['score'][idx]),
                            file=f)

            annos[-1]['sample_idx'] = np.array(
                [sample_idx] * len(annos[-1]['score']), dtype=np.int64)

            det_annos += annos

        if pklfile_prefix is not None:
            if not pklfile_prefix.endswith(('.pkl', '.pickle')):
                out = f'{pklfile_prefix}.pkl'
            mmcv.dump(det_annos, out)
            print('Result is saved to %s' % out)

        return det_annos","'{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(anno['name'][idx], anno['alpha'][idx], bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3], dims[idx][1], dims[idx][2], dims[idx][0], loc[idx][0], loc[idx][1], loc[idx][2], anno['rotation_y'][idx], anno['score'][idx])","'{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(anno['name'][idx], *anno['alpha'][idx:idx + 1] + bbox[idx], dims[idx][1], dims[idx][2], dims[idx][0], loc[idx][0], loc[idx][1], loc[idx][2], anno['rotation_y'][idx], anno['score'][idx])","iterable_zj[idx], bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3]",*anno['alpha'][idx:idx+1] + bbox[idx],*bbox[idx][:4],0
mmdetection3d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection3d/mmdet3d/datasets/kitti_mono_dataset.py,https://github.com/open-mmlab/mmdetection3d/tree/master/mmdet3d/datasets/kitti_mono_dataset.py,KittiMonoDataset,bbox2result_kitti$271,"def bbox2result_kitti(self,
                          net_outputs,
                          class_names,
                          pklfile_prefix=None,
                          submission_prefix=None):
        """"""Convert 3D detection results to kitti format for evaluation and test
        submission.

        Args:
            net_outputs (list[np.ndarray]): List of array storing the
                inferenced bounding boxes and scores.
            class_names (list[String]): A list of class names.
            pklfile_prefix (str): The prefix of pkl file.
            submission_prefix (str): The prefix of submission file.

        Returns:
            list[dict]: A list of dictionaries with the kitti format.
        """"""
        assert len(net_outputs) == len(self.anno_infos)
        if submission_prefix is not None:
            mmcv.mkdir_or_exist(submission_prefix)

        det_annos = []
        print('\nConverting prediction to KITTI format')
        for idx, pred_dicts in enumerate(
                mmcv.track_iter_progress(net_outputs)):
            annos = []
            info = self.anno_infos[idx]
            sample_idx = info['image']['image_idx']
            image_shape = info['image']['image_shape'][:2]

            box_dict = self.convert_valid_bboxes(pred_dicts, info)
            anno = {
                'name': [],
                'truncated': [],
                'occluded': [],
                'alpha': [],
                'bbox': [],
                'dimensions': [],
                'location': [],
                'rotation_y': [],
                'score': []
            }
            if len(box_dict['bbox']) > 0:
                box_2d_preds = box_dict['bbox']
                box_preds = box_dict['box3d_camera']
                scores = box_dict['scores']
                box_preds_lidar = box_dict['box3d_lidar']
                label_preds = box_dict['label_preds']

                for box, box_lidar, bbox, score, label in zip(
                        box_preds, box_preds_lidar, box_2d_preds, scores,
                        label_preds):
                    bbox[2:] = np.minimum(bbox[2:], image_shape[::-1])
                    bbox[:2] = np.maximum(bbox[:2], [0, 0])
                    anno['name'].append(class_names[int(label)])
                    anno['truncated'].append(0.0)
                    anno['occluded'].append(0)
                    anno['alpha'].append(-np.arctan2(box[0], box[2]) + box[6])
                    anno['bbox'].append(bbox)
                    anno['dimensions'].append(box[3:6])
                    anno['location'].append(box[:3])
                    anno['rotation_y'].append(box[6])
                    anno['score'].append(score)

                anno = {k: np.stack(v) for k, v in anno.items()}
                annos.append(anno)

            else:
                anno = {
                    'name': np.array([]),
                    'truncated': np.array([]),
                    'occluded': np.array([]),
                    'alpha': np.array([]),
                    'bbox': np.zeros([0, 4]),
                    'dimensions': np.zeros([0, 3]),
                    'location': np.zeros([0, 3]),
                    'rotation_y': np.array([]),
                    'score': np.array([]),
                }
                annos.append(anno)

            if submission_prefix is not None:
                curr_file = f'{submission_prefix}/{sample_idx:06d}.txt'
                with open(curr_file, 'w') as f:
                    bbox = anno['bbox']
                    loc = anno['location']
                    dims = anno['dimensions']  # lhw -> hwl

                    for idx in range(len(bbox)):
                        print(
                            '{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} '
                            '{:.4f} {:.4f} {:.4f} '
                            '{:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(
                                anno['name'][idx], anno['alpha'][idx],
                                bbox[idx][0], bbox[idx][1], bbox[idx][2],
                                bbox[idx][3], dims[idx][1], dims[idx][2],
                                dims[idx][0], loc[idx][0], loc[idx][1],
                                loc[idx][2], anno['rotation_y'][idx],
                                anno['score'][idx]),
                            file=f)

            annos[-1]['sample_idx'] = np.array(
                [sample_idx] * len(annos[-1]['score']), dtype=np.int64)

            det_annos += annos

        if pklfile_prefix is not None:
            if not pklfile_prefix.endswith(('.pkl', '.pickle')):
                out = f'{pklfile_prefix}.pkl'
            mmcv.dump(det_annos, out)
            print('Result is saved to %s' % out)

        return det_annos","'{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(anno['name'][idx], anno['alpha'][idx], bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3], dims[idx][1], dims[idx][2], dims[idx][0], loc[idx][0], loc[idx][1], loc[idx][2], anno['rotation_y'][idx], anno['score'][idx])","'{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(*anno['name'][idx:idx + 1] + [anno['alpha'][idx]], bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3], dims[idx][1], dims[idx][2], dims[idx][0], loc[idx][0], loc[idx][1], loc[idx][2], anno['rotation_y'][idx], anno['score'][idx])","iterable_zj[idx], anno['alpha'][idx]",*anno['name'][idx:idx+1] + [anno['alpha'][idx]],*dims[idx][1:3],0
mmdetection3d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection3d/mmdet3d/datasets/kitti_mono_dataset.py,https://github.com/open-mmlab/mmdetection3d/tree/master/mmdet3d/datasets/kitti_mono_dataset.py,KittiMonoDataset,bbox2result_kitti$271,"def bbox2result_kitti(self,
                          net_outputs,
                          class_names,
                          pklfile_prefix=None,
                          submission_prefix=None):
        """"""Convert 3D detection results to kitti format for evaluation and test
        submission.

        Args:
            net_outputs (list[np.ndarray]): List of array storing the
                inferenced bounding boxes and scores.
            class_names (list[String]): A list of class names.
            pklfile_prefix (str): The prefix of pkl file.
            submission_prefix (str): The prefix of submission file.

        Returns:
            list[dict]: A list of dictionaries with the kitti format.
        """"""
        assert len(net_outputs) == len(self.anno_infos)
        if submission_prefix is not None:
            mmcv.mkdir_or_exist(submission_prefix)

        det_annos = []
        print('\nConverting prediction to KITTI format')
        for idx, pred_dicts in enumerate(
                mmcv.track_iter_progress(net_outputs)):
            annos = []
            info = self.anno_infos[idx]
            sample_idx = info['image']['image_idx']
            image_shape = info['image']['image_shape'][:2]

            box_dict = self.convert_valid_bboxes(pred_dicts, info)
            anno = {
                'name': [],
                'truncated': [],
                'occluded': [],
                'alpha': [],
                'bbox': [],
                'dimensions': [],
                'location': [],
                'rotation_y': [],
                'score': []
            }
            if len(box_dict['bbox']) > 0:
                box_2d_preds = box_dict['bbox']
                box_preds = box_dict['box3d_camera']
                scores = box_dict['scores']
                box_preds_lidar = box_dict['box3d_lidar']
                label_preds = box_dict['label_preds']

                for box, box_lidar, bbox, score, label in zip(
                        box_preds, box_preds_lidar, box_2d_preds, scores,
                        label_preds):
                    bbox[2:] = np.minimum(bbox[2:], image_shape[::-1])
                    bbox[:2] = np.maximum(bbox[:2], [0, 0])
                    anno['name'].append(class_names[int(label)])
                    anno['truncated'].append(0.0)
                    anno['occluded'].append(0)
                    anno['alpha'].append(-np.arctan2(box[0], box[2]) + box[6])
                    anno['bbox'].append(bbox)
                    anno['dimensions'].append(box[3:6])
                    anno['location'].append(box[:3])
                    anno['rotation_y'].append(box[6])
                    anno['score'].append(score)

                anno = {k: np.stack(v) for k, v in anno.items()}
                annos.append(anno)

            else:
                anno = {
                    'name': np.array([]),
                    'truncated': np.array([]),
                    'occluded': np.array([]),
                    'alpha': np.array([]),
                    'bbox': np.zeros([0, 4]),
                    'dimensions': np.zeros([0, 3]),
                    'location': np.zeros([0, 3]),
                    'rotation_y': np.array([]),
                    'score': np.array([]),
                }
                annos.append(anno)

            if submission_prefix is not None:
                curr_file = f'{submission_prefix}/{sample_idx:06d}.txt'
                with open(curr_file, 'w') as f:
                    bbox = anno['bbox']
                    loc = anno['location']
                    dims = anno['dimensions']  # lhw -> hwl

                    for idx in range(len(bbox)):
                        print(
                            '{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} '
                            '{:.4f} {:.4f} {:.4f} '
                            '{:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(
                                anno['name'][idx], anno['alpha'][idx],
                                bbox[idx][0], bbox[idx][1], bbox[idx][2],
                                bbox[idx][3], dims[idx][1], dims[idx][2],
                                dims[idx][0], loc[idx][0], loc[idx][1],
                                loc[idx][2], anno['rotation_y'][idx],
                                anno['score'][idx]),
                            file=f)

            annos[-1]['sample_idx'] = np.array(
                [sample_idx] * len(annos[-1]['score']), dtype=np.int64)

            det_annos += annos

        if pklfile_prefix is not None:
            if not pklfile_prefix.endswith(('.pkl', '.pickle')):
                out = f'{pklfile_prefix}.pkl'
            mmcv.dump(det_annos, out)
            print('Result is saved to %s' % out)

        return det_annos","'{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(anno['name'][idx], anno['alpha'][idx], bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3], dims[idx][1], dims[idx][2], dims[idx][0], loc[idx][0], loc[idx][1], loc[idx][2], anno['rotation_y'][idx], anno['score'][idx])",Cannot refactor,"bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3]",,*loc[idx][:3],0
mmdetection3d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection3d/mmdet3d/datasets/kitti_mono_dataset.py,https://github.com/open-mmlab/mmdetection3d/tree/master/mmdet3d/datasets/kitti_mono_dataset.py,KittiMonoDataset,bbox2result_kitti$271,"def bbox2result_kitti(self,
                          net_outputs,
                          class_names,
                          pklfile_prefix=None,
                          submission_prefix=None):
        """"""Convert 3D detection results to kitti format for evaluation and test
        submission.

        Args:
            net_outputs (list[np.ndarray]): List of array storing the
                inferenced bounding boxes and scores.
            class_names (list[String]): A list of class names.
            pklfile_prefix (str): The prefix of pkl file.
            submission_prefix (str): The prefix of submission file.

        Returns:
            list[dict]: A list of dictionaries with the kitti format.
        """"""
        assert len(net_outputs) == len(self.anno_infos)
        if submission_prefix is not None:
            mmcv.mkdir_or_exist(submission_prefix)

        det_annos = []
        print('\nConverting prediction to KITTI format')
        for idx, pred_dicts in enumerate(
                mmcv.track_iter_progress(net_outputs)):
            annos = []
            info = self.anno_infos[idx]
            sample_idx = info['image']['image_idx']
            image_shape = info['image']['image_shape'][:2]

            box_dict = self.convert_valid_bboxes(pred_dicts, info)
            anno = {
                'name': [],
                'truncated': [],
                'occluded': [],
                'alpha': [],
                'bbox': [],
                'dimensions': [],
                'location': [],
                'rotation_y': [],
                'score': []
            }
            if len(box_dict['bbox']) > 0:
                box_2d_preds = box_dict['bbox']
                box_preds = box_dict['box3d_camera']
                scores = box_dict['scores']
                box_preds_lidar = box_dict['box3d_lidar']
                label_preds = box_dict['label_preds']

                for box, box_lidar, bbox, score, label in zip(
                        box_preds, box_preds_lidar, box_2d_preds, scores,
                        label_preds):
                    bbox[2:] = np.minimum(bbox[2:], image_shape[::-1])
                    bbox[:2] = np.maximum(bbox[:2], [0, 0])
                    anno['name'].append(class_names[int(label)])
                    anno['truncated'].append(0.0)
                    anno['occluded'].append(0)
                    anno['alpha'].append(-np.arctan2(box[0], box[2]) + box[6])
                    anno['bbox'].append(bbox)
                    anno['dimensions'].append(box[3:6])
                    anno['location'].append(box[:3])
                    anno['rotation_y'].append(box[6])
                    anno['score'].append(score)

                anno = {k: np.stack(v) for k, v in anno.items()}
                annos.append(anno)

            else:
                anno = {
                    'name': np.array([]),
                    'truncated': np.array([]),
                    'occluded': np.array([]),
                    'alpha': np.array([]),
                    'bbox': np.zeros([0, 4]),
                    'dimensions': np.zeros([0, 3]),
                    'location': np.zeros([0, 3]),
                    'rotation_y': np.array([]),
                    'score': np.array([]),
                }
                annos.append(anno)

            if submission_prefix is not None:
                curr_file = f'{submission_prefix}/{sample_idx:06d}.txt'
                with open(curr_file, 'w') as f:
                    bbox = anno['bbox']
                    loc = anno['location']
                    dims = anno['dimensions']  # lhw -> hwl

                    for idx in range(len(bbox)):
                        print(
                            '{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} '
                            '{:.4f} {:.4f} {:.4f} '
                            '{:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(
                                anno['name'][idx], anno['alpha'][idx],
                                bbox[idx][0], bbox[idx][1], bbox[idx][2],
                                bbox[idx][3], dims[idx][1], dims[idx][2],
                                dims[idx][0], loc[idx][0], loc[idx][1],
                                loc[idx][2], anno['rotation_y'][idx],
                                anno['score'][idx]),
                            file=f)

            annos[-1]['sample_idx'] = np.array(
                [sample_idx] * len(annos[-1]['score']), dtype=np.int64)

            det_annos += annos

        if pklfile_prefix is not None:
            if not pklfile_prefix.endswith(('.pkl', '.pickle')):
                out = f'{pklfile_prefix}.pkl'
            mmcv.dump(det_annos, out)
            print('Result is saved to %s' % out)

        return det_annos","np.arctan2(box[0], box[2])",np.arctan2(*box[::2]),"iterable_zj[0], iterable_zj[2]",*box[::2],*box[:4:2],0
networkx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/networkx/networkx/algorithms/regular.py,https://github.com/networkx/networkx/tree/master/networkx/algorithms/regular.py,,k_factor$58,"def k_factor(G, k, matching_weight=""weight""):
    """"""Compute a k-factor of G

    A k-factor of a graph is a spanning k-regular subgraph.
    A spanning k-regular subgraph of G is a subgraph that contains
    each vertex of G and a subset of the edges of G such that each
    vertex has degree k.

    Parameters
    ----------
    G : NetworkX graph
      Undirected graph

    matching_weight: string, optional (default='weight')
       Edge data key corresponding to the edge weight.
       Used for finding the max-weighted perfect matching.
       If key not found, uses 1 as weight.

    Returns
    -------
    G2 : NetworkX graph
        A k-factor of G

    References
    ----------
    .. [1] ""An algorithm for computing simple k-factors."",
       Meijer, Henk, Yurai Nez-Rodrguez, and David Rappaport,
       Information processing letters, 2009.
    """"""

    from networkx.algorithms.matching import max_weight_matching
    from networkx.algorithms.matching import is_perfect_matching

    class LargeKGadget:
        def __init__(self, k, degree, node, g):
            self.original = node
            self.g = g
            self.k = k
            self.degree = degree

            self.outer_vertices = [(node, x) for x in range(degree)]
            self.core_vertices = [(node, x + degree) for x in range(degree - k)]

        def replace_node(self):
            adj_view = self.g[self.original]
            neighbors = list(adj_view.keys())
            edge_attrs = list(adj_view.values())
            for (outer, neighbor, edge_attrs) in zip(
                self.outer_vertices, neighbors, edge_attrs
            ):
                self.g.add_edge(outer, neighbor, **edge_attrs)
            for core in self.core_vertices:
                for outer in self.outer_vertices:
                    self.g.add_edge(core, outer)
            self.g.remove_node(self.original)

        def restore_node(self):
            self.g.add_node(self.original)
            for outer in self.outer_vertices:
                adj_view = self.g[outer]
                for neighbor, edge_attrs in list(adj_view.items()):
                    if neighbor not in self.core_vertices:
                        self.g.add_edge(self.original, neighbor, **edge_attrs)
                        break
            g.remove_nodes_from(self.outer_vertices)
            g.remove_nodes_from(self.core_vertices)

    class SmallKGadget:
        def __init__(self, k, degree, node, g):
            self.original = node
            self.k = k
            self.degree = degree
            self.g = g

            self.outer_vertices = [(node, x) for x in range(degree)]
            self.inner_vertices = [(node, x + degree) for x in range(degree)]
            self.core_vertices = [(node, x + 2 * degree) for x in range(k)]

        def replace_node(self):
            adj_view = self.g[self.original]
            for (outer, inner, (neighbor, edge_attrs)) in zip(
                self.outer_vertices, self.inner_vertices, list(adj_view.items())
            ):
                self.g.add_edge(outer, inner)
                self.g.add_edge(outer, neighbor, **edge_attrs)
            for core in self.core_vertices:
                for inner in self.inner_vertices:
                    self.g.add_edge(core, inner)
            self.g.remove_node(self.original)

        def restore_node(self):
            self.g.add_node(self.original)
            for outer in self.outer_vertices:
                adj_view = self.g[outer]
                for neighbor, edge_attrs in adj_view.items():
                    if neighbor not in self.core_vertices:
                        self.g.add_edge(self.original, neighbor, **edge_attrs)
                        break
            self.g.remove_nodes_from(self.outer_vertices)
            self.g.remove_nodes_from(self.inner_vertices)
            self.g.remove_nodes_from(self.core_vertices)

    # Step 1
    if any(d < k for _, d in G.degree):
        raise nx.NetworkXUnfeasible(""Graph contains a vertex with degree less than k"")
    g = G.copy()

    # Step 2
    gadgets = []
    for node, degree in list(g.degree):
        if k < degree / 2.0:
            gadget = SmallKGadget(k, degree, node, g)
        else:
            gadget = LargeKGadget(k, degree, node, g)
        gadget.replace_node()
        gadgets.append(gadget)

    # Step 3
    matching = max_weight_matching(g, maxcardinality=True, weight=matching_weight)

    # Step 4
    if not is_perfect_matching(g, matching):
        raise nx.NetworkXUnfeasible(
            ""Cannot find k-factor because no perfect matching exists""
        )

    for edge in g.edges():
        if edge not in matching and (edge[1], edge[0]) not in matching:
            g.remove_edge(edge[0], edge[1])

    for gadget in gadgets:
        gadget.restore_node()

    return g","g.remove_edge(edge[0], edge[1])",g.remove_edge(*edge[:2]),"iterable_zj[0], iterable_zj[1]",*edge[:2],*edge[:2],1
dragonfly,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dragonfly/dragonfly/opt/unittest_mf_cp_gp_bandit.py,https://github.com/dragonfly/dragonfly/tree/master/dragonfly/opt/unittest_mf_cp_gp_bandit.py,MFCPGPBanditTestCaseDefinitions,_run_optimiser$26,"def _run_optimiser(cls, prob_funcs, domain_config_file, worker_manager, max_capital,
                     mode, *args, **kwargs):
    """""" Run the optimiser. """"""
    return gp_bandit.mf_cp_gpb_from_raw_args(prob_funcs[0], prob_funcs[1],
             domain_config_file, worker_manager=worker_manager, max_capital=max_capital,
             is_mf=True, mode=mode, *args, **kwargs)","gp_bandit.mf_cp_gpb_from_raw_args(prob_funcs[0], prob_funcs[1], domain_config_file, *args, worker_manager=worker_manager, max_capital=max_capital, is_mf=True, mode=mode, **kwargs)","gp_bandit.mf_cp_gpb_from_raw_args(*prob_funcs[:2], domain_config_file, *args, worker_manager=worker_manager, max_capital=max_capital, is_mf=True, mode=mode, **kwargs)","iterable_zj[0], iterable_zj[1]",*prob_funcs[:2],*prob_funcs[:2],1
ByteTrack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ByteTrack/tools/export_onnx.py,https://github.com/ifzhang/ByteTrack/tree/master/tools/export_onnx.py,,main$50,"def main():
    args = make_parser().parse_args()
    logger.info(""args value: {}"".format(args))
    exp = get_exp(args.exp_file, args.name)
    exp.merge(args.opts)

    if not args.experiment_name:
        args.experiment_name = exp.exp_name

    model = exp.get_model()
    if args.ckpt is None:
        file_name = os.path.join(exp.output_dir, args.experiment_name)
        ckpt_file = os.path.join(file_name, ""best_ckpt.pth.tar"")
    else:
        ckpt_file = args.ckpt

    # load the model state dict
    ckpt = torch.load(ckpt_file, map_location=""cpu"")

    model.eval()
    if ""model"" in ckpt:
        ckpt = ckpt[""model""]
    model.load_state_dict(ckpt)
    model = replace_module(model, nn.SiLU, SiLU)
    model.head.decode_in_inference = False

    logger.info(""loading checkpoint done."")
    dummy_input = torch.randn(1, 3, exp.test_size[0], exp.test_size[1])
    torch.onnx._export(
        model,
        dummy_input,
        args.output_name,
        input_names=[args.input],
        output_names=[args.output],
        opset_version=args.opset,
    )
    logger.info(""generated onnx model named {}"".format(args.output_name))

    if not args.no_onnxsim:
        import onnx

        from onnxsim import simplify

        # use onnxsimplify to reduce reduent model.
        onnx_model = onnx.load(args.output_name)
        model_simp, check = simplify(onnx_model)
        assert check, ""Simplified ONNX model could not be validated""
        onnx.save(model_simp, args.output_name)
        logger.info(""generated simplified onnx model named {}"".format(args.output_name))","torch.randn(1, 3, exp.test_size[0], exp.test_size[1])","torch.randn(1, 3, *exp.test_size[:2])","iterable_zj[0], iterable_zj[1]",*exp.test_size[:2],*exp.test_size[:2],1
tensorpack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorpack/tensorpack/dataflow/format.py,https://github.com/tensorpack/tensorpack/tree/master/tensorpack/dataflow/format.py,LMDBDataDecoder,__init__$160,"def __init__(self, lmdb_data, decoder):
        """"""
        Args:
            lmdb_data: a :class:`LMDBData` instance.
            decoder (k,v -> dp | None): a function taking k, v and returning a datapoint,
                or return None to discard.
        """"""
        def f(dp):
            return decoder(dp[0], dp[1])
        super(LMDBDataDecoder, self).__init__(lmdb_data, f)","decoder(dp[0], dp[1])",decoder(*dp[:2]),"iterable_zj[0], iterable_zj[1]",*dp[:2],*dp[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/contrib/rocblas.py,https://github.com/apache/tvm/tree/master/python/tvm/contrib/rocblas.py,,batch_matmul$53,"def batch_matmul(lhs, rhs, transa=False, transb=False):
    """"""Create an extern op that compute matrix mult of A and rhs with rocBLAS

    Parameters
    ----------
    lhs : Tensor
        The left batched matrix operand
    rhs : Tensor
        The right batched matrix operand
    transa : bool
        Whether transpose lhs
    transb : bool
        Whether transpose rhs

    Returns
    -------
    C : Tensor
        The result tensor.
    """"""
    batch_size = lhs.shape[0]
    assert batch_size == rhs.shape[0]
    n = lhs.shape[2] if transa else lhs.shape[1]
    m = rhs.shape[1] if transb else rhs.shape[2]
    return te.extern(
        (batch_size, n, m),
        [lhs, rhs],
        lambda ins, outs: tvm.tir.call_packed(
            ""tvm.contrib.rocblas.batch_matmul"", ins[0], ins[1], outs[0], transa, transb
        ),
        name=""C"",
    )","tvm.tir.call_packed('tvm.contrib.rocblas.batch_matmul', ins[0], ins[1], outs[0], transa, transb)","tvm.tir.call_packed('tvm.contrib.rocblas.batch_matmul', *ins[:2], outs[0], transa, transb)","iterable_zj[0], iterable_zj[1]",*ins[:2],*ins[:2],1
rewriting,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rewriting/utils/stylegan2/op/upfirdn2d.py,https://github.com/davidbau/rewriting/tree/master/utils/stylegan2/op/upfirdn2d.py,UpFirDn2dBackward,backward$62,"def backward(ctx, gradgrad_input):
        kernel, = ctx.saved_tensors

        gradgrad_input = gradgrad_input.reshape(-1, ctx.in_size[2], ctx.in_size[3], 1)

        gradgrad_out = upfirdn2d_op.upfirdn2d(
            gradgrad_input,
            kernel,
            ctx.up_x,
            ctx.up_y,
            ctx.down_x,
            ctx.down_y,
            ctx.pad_x0,
            ctx.pad_x1,
            ctx.pad_y0,
            ctx.pad_y1,
        )
        # gradgrad_out = gradgrad_out.view(ctx.in_size[0], ctx.out_size[0], ctx.out_size[1], ctx.in_size[3])
        gradgrad_out = gradgrad_out.view(
            ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1]
        )

        return gradgrad_out, None, None, None, None, None, None, None, None","gradgrad_input.reshape(-1, ctx.in_size[2], ctx.in_size[3], 1)","gradgrad_input.reshape(-1, *ctx.in_size[2:4], 1)","iterable_zj[2], iterable_zj[3]",*ctx.in_size[2:4],*ctx.in_size[2:4],1
rewriting,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rewriting/utils/stylegan2/op/upfirdn2d.py,https://github.com/davidbau/rewriting/tree/master/utils/stylegan2/op/upfirdn2d.py,UpFirDn2dBackward,backward$62,"def backward(ctx, gradgrad_input):
        kernel, = ctx.saved_tensors

        gradgrad_input = gradgrad_input.reshape(-1, ctx.in_size[2], ctx.in_size[3], 1)

        gradgrad_out = upfirdn2d_op.upfirdn2d(
            gradgrad_input,
            kernel,
            ctx.up_x,
            ctx.up_y,
            ctx.down_x,
            ctx.down_y,
            ctx.pad_x0,
            ctx.pad_x1,
            ctx.pad_y0,
            ctx.pad_y1,
        )
        # gradgrad_out = gradgrad_out.view(ctx.in_size[0], ctx.out_size[0], ctx.out_size[1], ctx.in_size[3])
        gradgrad_out = gradgrad_out.view(
            ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1]
        )

        return gradgrad_out, None, None, None, None, None, None, None, None","gradgrad_out.view(ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1])","gradgrad_out.view(*ctx.in_size[:2], ctx.out_size[0], ctx.out_size[1])","iterable_zj[0], iterable_zj[1]",*ctx.in_size[:2],*ctx.in_size[:2],1
rewriting,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rewriting/utils/stylegan2/op/upfirdn2d.py,https://github.com/davidbau/rewriting/tree/master/utils/stylegan2/op/upfirdn2d.py,UpFirDn2dBackward,backward$62,"def backward(ctx, gradgrad_input):
        kernel, = ctx.saved_tensors

        gradgrad_input = gradgrad_input.reshape(-1, ctx.in_size[2], ctx.in_size[3], 1)

        gradgrad_out = upfirdn2d_op.upfirdn2d(
            gradgrad_input,
            kernel,
            ctx.up_x,
            ctx.up_y,
            ctx.down_x,
            ctx.down_y,
            ctx.pad_x0,
            ctx.pad_x1,
            ctx.pad_y0,
            ctx.pad_y1,
        )
        # gradgrad_out = gradgrad_out.view(ctx.in_size[0], ctx.out_size[0], ctx.out_size[1], ctx.in_size[3])
        gradgrad_out = gradgrad_out.view(
            ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1]
        )

        return gradgrad_out, None, None, None, None, None, None, None, None","gradgrad_out.view(ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1])","gradgrad_out.view(ctx.in_size[0], ctx.in_size[1], *ctx.out_size[:2])","iterable_zj[0], iterable_zj[1]",*ctx.out_size[:2],*ctx.out_size[:2],1
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/rabbitmq_user.py,https://github.com/saltstack/salt/tree/master/salt/states/rabbitmq_user.py,,present$87,"def present(name, password=None, force=False, tags=None, perms=(), runas=None):
    """"""
    Ensure the RabbitMQ user exists.

    name
        User name
    password
        The user's password
    force
        If force is ``True``, the password will be automatically updated without extra password change check.
    tags
        Optional list of tags for the user
    perms
        A list of dicts with vhost keys and 3-tuple values
    runas
        Name of the user to run the command
    """"""
    ret = {""name"": name, ""result"": False, ""comment"": """", ""changes"": {}}

    try:
        user = __salt__[""rabbitmq.user_exists""](name, runas=runas)
    except CommandExecutionError as err:
        ret[""comment""] = ""Error: {}"".format(err)
        return ret

    passwd_reqs_update = False
    if user and password is not None:
        try:
            if not __salt__[""rabbitmq.check_password""](name, password, runas=runas):
                passwd_reqs_update = True
                log.debug(""RabbitMQ user %s password update required"", name)
        except CommandExecutionError as err:
            ret[""comment""] = ""Error: {}"".format(err)
            return ret

    if user and not any((force, perms, tags, passwd_reqs_update)):
        log.debug(
            ""RabbitMQ user '%s' exists, password is up to date and force is not set."",
            name,
        )
        ret[""comment""] = ""User '{}' is already present."".format(name)
        ret[""result""] = True
        return ret

    if not user:
        ret[""changes""].update({""user"": {""old"": """", ""new"": name}})
        if __opts__[""test""]:
            ret[""result""] = None
            ret[""comment""] = ""User '{}' is set to be created."".format(name)
            return ret

        log.debug(""RabbitMQ user '%s' doesn't exist - Creating."", name)
        try:
            __salt__[""rabbitmq.add_user""](name, password, runas=runas)
        except CommandExecutionError as err:
            ret[""comment""] = ""Error: {}"".format(err)
            return ret
    else:
        log.debug(""RabbitMQ user '%s' exists"", name)
        if force or passwd_reqs_update:
            if password is not None:
                if not __opts__[""test""]:
                    try:
                        __salt__[""rabbitmq.change_password""](
                            name, password, runas=runas
                        )
                    except CommandExecutionError as err:
                        ret[""comment""] = ""Error: {}"".format(err)
                        return ret
                ret[""changes""].update({""password"": {""old"": """", ""new"": ""Set password.""}})
            else:
                if not __opts__[""test""]:
                    log.debug(""Password for %s is not set - Clearing password."", name)
                    try:
                        __salt__[""rabbitmq.clear_password""](name, runas=runas)
                    except CommandExecutionError as err:
                        ret[""comment""] = ""Error: {}"".format(err)
                        return ret
                ret[""changes""].update(
                    {""password"": {""old"": ""Removed password."", ""new"": """"}}
                )

    if tags is not None:
        current_tags = _get_current_tags(name, runas=runas)
        if isinstance(tags, str):
            tags = tags.split()
        # Diff the tags sets. Symmetric difference operator ^ will give us
        # any element in one set, but not both
        if set(tags) ^ set(current_tags):
            if not __opts__[""test""]:
                try:
                    __salt__[""rabbitmq.set_user_tags""](name, tags, runas=runas)
                except CommandExecutionError as err:
                    ret[""comment""] = ""Error: {}"".format(err)
                    return ret
            ret[""changes""].update({""tags"": {""old"": current_tags, ""new"": tags}})
    try:
        existing_perms = __salt__[""rabbitmq.list_user_permissions""](name, runas=runas)
    except CommandExecutionError as err:
        ret[""comment""] = ""Error: {}"".format(err)
        return ret

    if _check_perms_changes(name, perms, runas=runas, existing=existing_perms):
        for vhost_perm in perms:
            for vhost, perm in vhost_perm.items():
                if not __opts__[""test""]:
                    try:
                        __salt__[""rabbitmq.set_permissions""](
                            vhost, name, perm[0], perm[1], perm[2], runas=runas
                        )
                    except CommandExecutionError as err:
                        ret[""comment""] = ""Error: {}"".format(err)
                        return ret
                new_perms = {
                    vhost: {""configure"": perm[0], ""write"": perm[1], ""read"": perm[2]}
                }
                if vhost in existing_perms:
                    if existing_perms[vhost] != new_perms[vhost]:
                        if ret[""changes""].get(""perms"") is None:
                            ret[""changes""].update({""perms"": {""old"": {}, ""new"": {}}})
                        ret[""changes""][""perms""][""old""].update(existing_perms[vhost])
                        ret[""changes""][""perms""][""new""].update(new_perms)
                else:
                    ret[""changes""].update({""perms"": {""new"": {}}})
                    ret[""changes""][""perms""][""new""].update(new_perms)

    ret[""result""] = True
    if ret[""changes""] == {}:
        ret[""comment""] = ""'{}' is already in the desired state."".format(name)
        return ret

    if __opts__[""test""]:
        ret[""result""] = None
        ret[""comment""] = ""Configuration for '{}' will change."".format(name)
        return ret

    ret[""comment""] = ""'{}' was configured."".format(name)
    return ret","__salt__['rabbitmq.set_permissions'](vhost, name, perm[0], perm[1], perm[2], runas=runas)","__salt__['rabbitmq.set_permissions'](vhost, name, *perm[:3], runas=runas)","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*perm[:3],*perm[:3],1
oio-sds,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oio-sds/oio/xcute/client.py,https://github.com/open-io/oio-sds/tree/master/oio/xcute/client.py,XcuteClient,xcute_request$86,"def xcute_request(self, method, action, params=None, **kwargs):
        """"""Make a request to the xcute service.""""""
        self._maybe_refresh_endpoint(**kwargs)
        if not params:
            params = dict()
        try:
            resp, body = self._request(method, action, params=params, **kwargs)
        except OioNetworkException as exc:
            exc_info = sys.exc_info()
            if self._refresh_delay >= 0.0:
                self.logger.info(
                    ""Refreshing xcute endpoint after error %s"", exc)
                try:
                    self._refresh_endpoint(**kwargs)
                except Exception as exc:
                    self.logger.warn(""%s"", exc)
            reraise(exc_info[0], exc_info[1], exc_info[2])
        return resp, body","reraise(exc_info[0], exc_info[1], exc_info[2])",reraise(*exc_info[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*exc_info[:3],*exc_info[:3],1
pytorchvideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorchvideo/pytorchvideo/transforms/functional.py,https://github.com/facebookresearch/pytorchvideo/tree/master/pytorchvideo/transforms/functional.py,,random_crop_with_boxes$267,"def random_crop_with_boxes(
    images: torch.Tensor, size: int, boxes: torch.Tensor
) -> Tuple[torch.Tensor, torch.Tensor]:
    """"""
    Perform random spatial crop on the given images and corresponding boxes.
    Args:
        images (tensor): images to perform random crop. The dimension is
            `channel` x `num frames` x `height` x `width`.
        size (int): the size of height and width to crop on the image.
        boxes (tensor): Corresponding boxes to images.
            Dimension is `num boxes` x 4.
    Returns:
        cropped (tensor): cropped images with dimension of
            `channel` x `num frames` x `height` x `width`.
        cropped_boxes (tensor): the cropped boxes with dimension of
            `num boxes` x 4.
    """"""
    if images.shape[2] == size and images.shape[3] == size:
        return images
    height = images.shape[2]
    width = images.shape[3]
    y_offset = 0
    if height > size:
        y_offset = int(np.random.randint(0, height - size))
    x_offset = 0
    if width > size:
        x_offset = int(np.random.randint(0, width - size))
    cropped = images[:, :, y_offset : y_offset + size, x_offset : x_offset + size]

    cropped_boxes = crop_boxes(boxes, x_offset, y_offset)
    return cropped, clip_boxes_to_image(
        cropped_boxes, cropped.shape[-2], cropped.shape[-1]
    )","clip_boxes_to_image(cropped_boxes, cropped.shape[-2], cropped.shape[-1])","clip_boxes_to_image(cropped_boxes, *cropped.shape[-2:])","iterable_zj[-2], iterable_zj[-1]",*cropped.shape[-2:],*cropped.shape[-2:0],0
RedditDownloader,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RedditDownloader/redditdownloader/processing/handlers/tumblr.py,https://github.com/shadowmoose/RedditDownloader/tree/master/redditdownloader/processing/handlers/tumblr.py,,handle$57,"def handle(task, progress):
	m = re.match(regex, task.url)
	if m is None or '.media.tumblr' in task.url:
		return False
	gr = m.groups()
	progress.set_status(""Parsing Tumblr page..."")
	# noinspection PyBroadException
	try:
		urls = get_media_urls(gr[0], gr[1])
		if not urls:
			return None
		if len(urls) > 1:
			return HandlerResponse(success=True, handler=tag, album_urls=urls)
		return http_downloader.download_binary(urls[0], task.file, progress, tag)
	except Exception as ex:
		# print('Tumblr: ERROR:', ex, task.url, file=sys.stderr, flush=True)
		return False","get_media_urls(gr[0], gr[1])",get_media_urls(*gr[:2]),"iterable_zj[0], iterable_zj[1]",*gr[:2],*gr[:2],1
numpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/lib/tests/test_stride_tricks.py,https://github.com/numpy/numpy/tree/master/numpy/lib/tests/test_stride_tricks.py,,test_same_as_ufunc$182,"def test_same_as_ufunc():
    # Check that the data layout is the same as if a ufunc did the operation.

    data = [
        [[(1,), (3,)], (3,)],
        [[(1, 3), (3, 3)], (3, 3)],
        [[(3, 1), (3, 3)], (3, 3)],
        [[(1, 3), (3, 1)], (3, 3)],
        [[(1, 1), (3, 3)], (3, 3)],
        [[(1, 1), (1, 3)], (1, 3)],
        [[(1, 1), (3, 1)], (3, 1)],
        [[(1, 0), (0, 0)], (0, 0)],
        [[(0, 1), (0, 0)], (0, 0)],
        [[(1, 0), (0, 1)], (0, 0)],
        [[(1, 1), (0, 0)], (0, 0)],
        [[(1, 1), (1, 0)], (1, 0)],
        [[(1, 1), (0, 1)], (0, 1)],
        [[(), (3,)], (3,)],
        [[(3,), (3, 3)], (3, 3)],
        [[(3,), (3, 1)], (3, 3)],
        [[(1,), (3, 3)], (3, 3)],
        [[(), (3, 3)], (3, 3)],
        [[(1, 1), (3,)], (1, 3)],
        [[(1,), (3, 1)], (3, 1)],
        [[(1,), (1, 3)], (1, 3)],
        [[(), (1, 3)], (1, 3)],
        [[(), (3, 1)], (3, 1)],
        [[(), (0,)], (0,)],
        [[(0,), (0, 0)], (0, 0)],
        [[(0,), (0, 1)], (0, 0)],
        [[(1,), (0, 0)], (0, 0)],
        [[(), (0, 0)], (0, 0)],
        [[(1, 1), (0,)], (1, 0)],
        [[(1,), (0, 1)], (0, 1)],
        [[(1,), (1, 0)], (1, 0)],
        [[(), (1, 0)], (1, 0)],
        [[(), (0, 1)], (0, 1)],
    ]
    for input_shapes, expected_shape in data:
        assert_same_as_ufunc(input_shapes[0], input_shapes[1],
                             ""Shapes: %s %s"" % (input_shapes[0], input_shapes[1]))
        # Reverse the input shapes since broadcasting should be symmetric.
        assert_same_as_ufunc(input_shapes[1], input_shapes[0])
        # Try them transposed, too.
        assert_same_as_ufunc(input_shapes[0], input_shapes[1], True)
        # ... and flipped for non-rank-0 inputs in order to test negative
        # strides.
        if () not in input_shapes:
            assert_same_as_ufunc(input_shapes[0], input_shapes[1], False, True)
            assert_same_as_ufunc(input_shapes[0], input_shapes[1], True, True)","assert_same_as_ufunc(input_shapes[0], input_shapes[1], 'Shapes: %s %s' % (input_shapes[0], input_shapes[1]))","assert_same_as_ufunc(*input_shapes[:2], 'Shapes: %s %s' % (input_shapes[0], input_shapes[1]))","iterable_zj[0], iterable_zj[1]",*input_shapes[:2],*input_shapes[:2],1
numpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/lib/tests/test_stride_tricks.py,https://github.com/numpy/numpy/tree/master/numpy/lib/tests/test_stride_tricks.py,,test_same_as_ufunc$182,"def test_same_as_ufunc():
    # Check that the data layout is the same as if a ufunc did the operation.

    data = [
        [[(1,), (3,)], (3,)],
        [[(1, 3), (3, 3)], (3, 3)],
        [[(3, 1), (3, 3)], (3, 3)],
        [[(1, 3), (3, 1)], (3, 3)],
        [[(1, 1), (3, 3)], (3, 3)],
        [[(1, 1), (1, 3)], (1, 3)],
        [[(1, 1), (3, 1)], (3, 1)],
        [[(1, 0), (0, 0)], (0, 0)],
        [[(0, 1), (0, 0)], (0, 0)],
        [[(1, 0), (0, 1)], (0, 0)],
        [[(1, 1), (0, 0)], (0, 0)],
        [[(1, 1), (1, 0)], (1, 0)],
        [[(1, 1), (0, 1)], (0, 1)],
        [[(), (3,)], (3,)],
        [[(3,), (3, 3)], (3, 3)],
        [[(3,), (3, 1)], (3, 3)],
        [[(1,), (3, 3)], (3, 3)],
        [[(), (3, 3)], (3, 3)],
        [[(1, 1), (3,)], (1, 3)],
        [[(1,), (3, 1)], (3, 1)],
        [[(1,), (1, 3)], (1, 3)],
        [[(), (1, 3)], (1, 3)],
        [[(), (3, 1)], (3, 1)],
        [[(), (0,)], (0,)],
        [[(0,), (0, 0)], (0, 0)],
        [[(0,), (0, 1)], (0, 0)],
        [[(1,), (0, 0)], (0, 0)],
        [[(), (0, 0)], (0, 0)],
        [[(1, 1), (0,)], (1, 0)],
        [[(1,), (0, 1)], (0, 1)],
        [[(1,), (1, 0)], (1, 0)],
        [[(), (1, 0)], (1, 0)],
        [[(), (0, 1)], (0, 1)],
    ]
    for input_shapes, expected_shape in data:
        assert_same_as_ufunc(input_shapes[0], input_shapes[1],
                             ""Shapes: %s %s"" % (input_shapes[0], input_shapes[1]))
        # Reverse the input shapes since broadcasting should be symmetric.
        assert_same_as_ufunc(input_shapes[1], input_shapes[0])
        # Try them transposed, too.
        assert_same_as_ufunc(input_shapes[0], input_shapes[1], True)
        # ... and flipped for non-rank-0 inputs in order to test negative
        # strides.
        if () not in input_shapes:
            assert_same_as_ufunc(input_shapes[0], input_shapes[1], False, True)
            assert_same_as_ufunc(input_shapes[0], input_shapes[1], True, True)","assert_same_as_ufunc(input_shapes[0], input_shapes[1], True)","assert_same_as_ufunc(*input_shapes[:2], True)","iterable_zj[0], iterable_zj[1]",*input_shapes[:2],*input_shapes[:2],1
numpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/lib/tests/test_stride_tricks.py,https://github.com/numpy/numpy/tree/master/numpy/lib/tests/test_stride_tricks.py,,test_same_as_ufunc$182,"def test_same_as_ufunc():
    # Check that the data layout is the same as if a ufunc did the operation.

    data = [
        [[(1,), (3,)], (3,)],
        [[(1, 3), (3, 3)], (3, 3)],
        [[(3, 1), (3, 3)], (3, 3)],
        [[(1, 3), (3, 1)], (3, 3)],
        [[(1, 1), (3, 3)], (3, 3)],
        [[(1, 1), (1, 3)], (1, 3)],
        [[(1, 1), (3, 1)], (3, 1)],
        [[(1, 0), (0, 0)], (0, 0)],
        [[(0, 1), (0, 0)], (0, 0)],
        [[(1, 0), (0, 1)], (0, 0)],
        [[(1, 1), (0, 0)], (0, 0)],
        [[(1, 1), (1, 0)], (1, 0)],
        [[(1, 1), (0, 1)], (0, 1)],
        [[(), (3,)], (3,)],
        [[(3,), (3, 3)], (3, 3)],
        [[(3,), (3, 1)], (3, 3)],
        [[(1,), (3, 3)], (3, 3)],
        [[(), (3, 3)], (3, 3)],
        [[(1, 1), (3,)], (1, 3)],
        [[(1,), (3, 1)], (3, 1)],
        [[(1,), (1, 3)], (1, 3)],
        [[(), (1, 3)], (1, 3)],
        [[(), (3, 1)], (3, 1)],
        [[(), (0,)], (0,)],
        [[(0,), (0, 0)], (0, 0)],
        [[(0,), (0, 1)], (0, 0)],
        [[(1,), (0, 0)], (0, 0)],
        [[(), (0, 0)], (0, 0)],
        [[(1, 1), (0,)], (1, 0)],
        [[(1,), (0, 1)], (0, 1)],
        [[(1,), (1, 0)], (1, 0)],
        [[(), (1, 0)], (1, 0)],
        [[(), (0, 1)], (0, 1)],
    ]
    for input_shapes, expected_shape in data:
        assert_same_as_ufunc(input_shapes[0], input_shapes[1],
                             ""Shapes: %s %s"" % (input_shapes[0], input_shapes[1]))
        # Reverse the input shapes since broadcasting should be symmetric.
        assert_same_as_ufunc(input_shapes[1], input_shapes[0])
        # Try them transposed, too.
        assert_same_as_ufunc(input_shapes[0], input_shapes[1], True)
        # ... and flipped for non-rank-0 inputs in order to test negative
        # strides.
        if () not in input_shapes:
            assert_same_as_ufunc(input_shapes[0], input_shapes[1], False, True)
            assert_same_as_ufunc(input_shapes[0], input_shapes[1], True, True)","assert_same_as_ufunc(input_shapes[0], input_shapes[1], False, True)","assert_same_as_ufunc(*input_shapes[:2], False, True)","iterable_zj[0], iterable_zj[1]",*input_shapes[:2],*input_shapes[:2],1
numpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/lib/tests/test_stride_tricks.py,https://github.com/numpy/numpy/tree/master/numpy/lib/tests/test_stride_tricks.py,,test_same_as_ufunc$182,"def test_same_as_ufunc():
    # Check that the data layout is the same as if a ufunc did the operation.

    data = [
        [[(1,), (3,)], (3,)],
        [[(1, 3), (3, 3)], (3, 3)],
        [[(3, 1), (3, 3)], (3, 3)],
        [[(1, 3), (3, 1)], (3, 3)],
        [[(1, 1), (3, 3)], (3, 3)],
        [[(1, 1), (1, 3)], (1, 3)],
        [[(1, 1), (3, 1)], (3, 1)],
        [[(1, 0), (0, 0)], (0, 0)],
        [[(0, 1), (0, 0)], (0, 0)],
        [[(1, 0), (0, 1)], (0, 0)],
        [[(1, 1), (0, 0)], (0, 0)],
        [[(1, 1), (1, 0)], (1, 0)],
        [[(1, 1), (0, 1)], (0, 1)],
        [[(), (3,)], (3,)],
        [[(3,), (3, 3)], (3, 3)],
        [[(3,), (3, 1)], (3, 3)],
        [[(1,), (3, 3)], (3, 3)],
        [[(), (3, 3)], (3, 3)],
        [[(1, 1), (3,)], (1, 3)],
        [[(1,), (3, 1)], (3, 1)],
        [[(1,), (1, 3)], (1, 3)],
        [[(), (1, 3)], (1, 3)],
        [[(), (3, 1)], (3, 1)],
        [[(), (0,)], (0,)],
        [[(0,), (0, 0)], (0, 0)],
        [[(0,), (0, 1)], (0, 0)],
        [[(1,), (0, 0)], (0, 0)],
        [[(), (0, 0)], (0, 0)],
        [[(1, 1), (0,)], (1, 0)],
        [[(1,), (0, 1)], (0, 1)],
        [[(1,), (1, 0)], (1, 0)],
        [[(), (1, 0)], (1, 0)],
        [[(), (0, 1)], (0, 1)],
    ]
    for input_shapes, expected_shape in data:
        assert_same_as_ufunc(input_shapes[0], input_shapes[1],
                             ""Shapes: %s %s"" % (input_shapes[0], input_shapes[1]))
        # Reverse the input shapes since broadcasting should be symmetric.
        assert_same_as_ufunc(input_shapes[1], input_shapes[0])
        # Try them transposed, too.
        assert_same_as_ufunc(input_shapes[0], input_shapes[1], True)
        # ... and flipped for non-rank-0 inputs in order to test negative
        # strides.
        if () not in input_shapes:
            assert_same_as_ufunc(input_shapes[0], input_shapes[1], False, True)
            assert_same_as_ufunc(input_shapes[0], input_shapes[1], True, True)","assert_same_as_ufunc(input_shapes[0], input_shapes[1], True, True)","assert_same_as_ufunc(*input_shapes[:2], True, True)","iterable_zj[0], iterable_zj[1]",*input_shapes[:2],*input_shapes[:2],1
attention-is-all-you-need-keras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/attention-is-all-you-need-keras/transformer.py,https://github.com/Lsdefine/attention-is-all-you-need-keras/tree/master//transformer.py,Decoder,__call__$196,"def __call__(self, tgt_emb, tgt_seq, src_seq, enc_output, return_att=False, active_layers=999):
		x = tgt_emb
		self_pad_mask = Lambda(lambda x:GetPadMask(x, x))(tgt_seq)
		self_sub_mask = Lambda(GetSubMask)(tgt_seq)
		self_mask = Lambda(lambda x:K.minimum(x[0], x[1]))([self_pad_mask, self_sub_mask])
		enc_mask = Lambda(lambda x:GetPadMask(x[0], x[1]))([tgt_seq, src_seq])
		if return_att: self_atts, enc_atts = [], []
		for dec_layer in self.layers[:active_layers]:
			x, self_att, enc_att = dec_layer(x, enc_output, self_mask, enc_mask)
			if return_att: 
				self_atts.append(self_att)
				enc_atts.append(enc_att)
		return (x, self_atts, enc_atts) if return_att else x","K.minimum(x[0], x[1])",K.minimum(*x[:2]),"iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
attention-is-all-you-need-keras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/attention-is-all-you-need-keras/transformer.py,https://github.com/Lsdefine/attention-is-all-you-need-keras/tree/master//transformer.py,Decoder,__call__$196,"def __call__(self, tgt_emb, tgt_seq, src_seq, enc_output, return_att=False, active_layers=999):
		x = tgt_emb
		self_pad_mask = Lambda(lambda x:GetPadMask(x, x))(tgt_seq)
		self_sub_mask = Lambda(GetSubMask)(tgt_seq)
		self_mask = Lambda(lambda x:K.minimum(x[0], x[1]))([self_pad_mask, self_sub_mask])
		enc_mask = Lambda(lambda x:GetPadMask(x[0], x[1]))([tgt_seq, src_seq])
		if return_att: self_atts, enc_atts = [], []
		for dec_layer in self.layers[:active_layers]:
			x, self_att, enc_att = dec_layer(x, enc_output, self_mask, enc_mask)
			if return_att: 
				self_atts.append(self_att)
				enc_atts.append(enc_att)
		return (x, self_atts, enc_atts) if return_att else x","GetPadMask(x[0], x[1])",GetPadMask(*x[:2]),"iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
Oscar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Oscar/oscar/utils/cider/pyciderevalcap/cider/cider_scorer.py,https://github.com/microsoft/Oscar/tree/master/oscar/utils/cider/pyciderevalcap/cider/cider_scorer.py,CiderScorer,__iadd__$95,"def __iadd__(self, other):
        '''add an instance (e.g., from another sentence).'''

        if type(other) is tuple:
            ## avoid creating new CiderScorer instances
            self.cook_append(other[0], other[1])
        else:
            self.ctest.extend(other.ctest)
            self.crefs.extend(other.crefs)

        return self","self.cook_append(other[0], other[1])",self.cook_append(*other[:2]),"iterable_zj[0], iterable_zj[1]",*other[:2],*other[:2],1
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/vision/models/squeezenet.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/vision/models/squeezenet.py,,_squeezenet$203,"def _squeezenet(arch, version, pretrained, **kwargs):
    model = SqueezeNet(version, **kwargs)
    if pretrained:
        assert (
            arch in model_urls
        ), ""{} model do not have a pretrained model now, you should set pretrained=False"".format(
            arch
        )
        weight_path = get_weights_path_from_url(
            model_urls[arch][0], model_urls[arch][1]
        )
        param = paddle.load(weight_path)
        model.set_dict(param)

    return model","get_weights_path_from_url(model_urls[arch][0], model_urls[arch][1])",get_weights_path_from_url(*model_urls[arch][:2]),"iterable_zj[0], iterable_zj[1]",*model_urls[arch][:2],*model_urls[arch][:2],1
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/sets/sets.py,https://github.com/sympy/sympy/tree/master/sympy/sets/sets.py,,imageset$2192,"def imageset(*args):
    r""""""
    Return an image of the set under transformation ``f``.

    Explanation
    ===========

    If this function cannot compute the image, it returns an
    unevaluated ImageSet object.

    .. math::
        \{ f(x) \mid x \in \mathrm{self} \}

    Examples
    ========

    >>> from sympy import S, Interval, imageset, sin, Lambda
    >>> from sympy.abc import x

    >>> imageset(x, 2*x, Interval(0, 2))
    Interval(0, 4)

    >>> imageset(lambda x: 2*x, Interval(0, 2))
    Interval(0, 4)

    >>> imageset(Lambda(x, sin(x)), Interval(-2, 1))
    ImageSet(Lambda(x, sin(x)), Interval(-2, 1))

    >>> imageset(sin, Interval(-2, 1))
    ImageSet(Lambda(x, sin(x)), Interval(-2, 1))
    >>> imageset(lambda y: x + y, Interval(-2, 1))
    ImageSet(Lambda(y, x + y), Interval(-2, 1))

    Expressions applied to the set of Integers are simplified
    to show as few negatives as possible and linear expressions
    are converted to a canonical form. If this is not desirable
    then the unevaluated ImageSet should be used.

    >>> imageset(x, -2*x + 5, S.Integers)
    ImageSet(Lambda(x, 2*x + 1), Integers)

    See Also
    ========

    sympy.sets.fancysets.ImageSet

    """"""
    from .fancysets import ImageSet
    from .setexpr import set_function

    if len(args) < 2:
        raise ValueError('imageset expects at least 2 args, got: %s' % len(args))

    if isinstance(args[0], (Symbol, tuple)) and len(args) > 2:
        f = Lambda(args[0], args[1])
        set_list = args[2:]
    else:
        f = args[0]
        set_list = args[1:]

    if isinstance(f, Lambda):
        pass
    elif callable(f):
        nargs = getattr(f, 'nargs', {})
        if nargs:
            if len(nargs) != 1:
                raise NotImplementedError(filldedent('''
                    This function can take more than 1 arg
                    but the potentially complicated set input
                    has not been analyzed at this point to
                    know its dimensions. TODO
                    '''))
            N = nargs.args[0]
            if N == 1:
                s = 'x'
            else:
                s = [Symbol('x%i' % i) for i in range(1, N + 1)]
        else:
            s = inspect.signature(f).parameters

        dexpr = _sympify(f(*[Dummy() for i in s]))
        var = tuple(uniquely_named_symbol(
            Symbol(i), dexpr) for i in s)
        f = Lambda(var, f(*var))
    else:
        raise TypeError(filldedent('''
            expecting lambda, Lambda, or FunctionClass,
            not \'%s\'.''' % func_name(f)))

    if any(not isinstance(s, Set) for s in set_list):
        name = [func_name(s) for s in set_list]
        raise ValueError(
            'arguments after mapping should be sets, not %s' % name)

    if len(set_list) == 1:
        set = set_list[0]
        try:
            # TypeError if arg count != set dimensions
            r = set_function(f, set)
            if r is None:
                raise TypeError
            if not r:
                return r
        except TypeError:
            r = ImageSet(f, set)
        if isinstance(r, ImageSet):
            f, set = r.args

        if f.variables[0] == f.expr:
            return set

        if isinstance(set, ImageSet):
            # XXX: Maybe this should just be:
            # f2 = set.lambda
            # fun = Lambda(f2.signature, f(*f2.expr))
            # return imageset(fun, *set.base_sets)
            if len(set.lamda.variables) == 1 and len(f.variables) == 1:
                x = set.lamda.variables[0]
                y = f.variables[0]
                return imageset(
                    Lambda(x, f.expr.subs(y, set.lamda.expr)), *set.base_sets)

        if r is not None:
            return r

    return ImageSet(f, *set_list)","Lambda(args[0], args[1])",Lambda(*args[:2]),"iterable_zj[0], iterable_zj[1]",*args[:2],*args[:2],1
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/detr/modeling_detr.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/detr/modeling_detr.py,DetrMHAttentionMap,forward$1855,"def forward(self, q, k, mask: Optional[Tensor] = None):
        q = self.q_linear(q)
        k = nn.functional.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)
        queries_per_head = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)
        keys_per_head = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])
        weights = torch.einsum(""bqnc,bnchw->bqnhw"", queries_per_head * self.normalize_fact, keys_per_head)

        if mask is not None:
            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)
        weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())
        weights = self.dropout(weights)
        return weights","q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)","q.view(*q.shape[:2], self.num_heads, self.hidden_dim // self.num_heads)","iterable_zj[0], iterable_zj[1]",*q.shape[:2],*q.shape[:2],1
data-validation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-validation/tensorflow_data_validation/utils/top_k_uniques_stats_util_test.py,https://github.com/tensorflow/data-validation/tree/master/tensorflow_data_validation/utils/top_k_uniques_stats_util_test.py,TopKUniquesStatsUtilTest,test_make_feature_stats_proto_topk_uniques_categorical$187,"def test_make_feature_stats_proto_topk_uniques_categorical(self):
    expected_result = text_format.Parse(
        """"""
        path {
          step: 'fa'
        }
        string_stats {
          unique: 4
          top_values {
            value: 'a'
            frequency: 4
          }
          top_values {
            value: 'c'
            frequency: 3
          }
          top_values {
            value: 'd'
            frequency: 2
          }
          rank_histogram {
            buckets {
              low_rank: 0
              high_rank: 0
              label: ""a""
              sample_count: 4.0
            }
            buckets {
              low_rank: 1
              high_rank: 1
              label: ""c""
              sample_count: 3.0
            }
          }
    }"""""", statistics_pb2.FeatureNameStatistics())

    value_counts = [('d', 2), ('c', 3), ('a', 4), ('b', 2)]
    top_k_value_count_list = [
        top_k_uniques_stats_util.FeatureValueCount(
            value_count[0], value_count[1])
        for value_count in value_counts
    ]
    result = (
        top_k_uniques_stats_util.make_feature_stats_proto_topk_uniques(
            types.FeaturePath(['fa']),
            num_top_values=3,
            frequency_threshold=1,
            num_rank_histogram_buckets=2,
            num_unique=4,
            value_count_list=top_k_value_count_list))
    test_util.assert_feature_proto_equal(self, result, expected_result)","top_k_uniques_stats_util.FeatureValueCount(value_count[0], value_count[1])",top_k_uniques_stats_util.FeatureValueCount(*value_count[:2]),"iterable_zj[0], iterable_zj[1]",*value_count[:2],*value_count[:2],1
pyGAM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyGAM/pygam/pygam.py,https://github.com/dswah/pyGAM/tree/master/pygam/pygam.py,GAM,generate_X_grid$1398,"def generate_X_grid(self, term, n=100, meshgrid=False):
        """"""create a nice grid of X data

        array is sorted by feature and uniformly spaced,
        so the marginal and joint distributions are likely wrong

        if term is >= 0, we generate n samples per feature,
        which results in n^deg samples,
        where deg is the degree of the interaction of the term

        Parameters
        ----------
        term : int,
            Which term to process.

        n : int, optional
            number of data points to create

        meshgrid : bool, optional
            Whether to return a meshgrid (useful for 3d plotting)
            or a feature matrix (useful for inference like partial predictions)

        Returns
        -------
        if meshgrid is False:
            np.array of shape (n, n_features)
            where m is the number of
            (sub)terms in the requested (tensor)term.
        else:
            tuple of len m,
            where m is the number of (sub)terms in the requested
            (tensor)term.

            each element in the tuple contains a np.ndarray of size (n)^m

        Raises
        ------
        ValueError :
            If the term requested is an intercept
            since it does not make sense to process the intercept term.
        """"""
        if not self._is_fitted:
            raise AttributeError('GAM has not been fitted. Call fit first.')

        # cant do Intercept
        if self.terms[term].isintercept:
            raise ValueError('cannot create grid for intercept term')

        # process each subterm in a TensorTerm
        if self.terms[term].istensor:
            Xs = []
            for term_ in self.terms[term]:
                Xs.append(np.linspace(term_.edge_knots_[0],
                                      term_.edge_knots_[1],
                                      num=n))

            Xs = np.meshgrid(*Xs, indexing='ij')
            if meshgrid:
                return tuple(Xs)
            else:
                return self._flatten_mesh(Xs, term=term)

        # all other Terms
        elif hasattr(self.terms[term], 'edge_knots_'):
            x = np.linspace(self.terms[term].edge_knots_[0],
                            self.terms[term].edge_knots_[1],
                            num=n)

            if meshgrid:
                return (x,)

            # fill in feature matrix with only relevant features for this term
            X = np.zeros((n, self.statistics_['m_features']))
            X[:, self.terms[term].feature] = x
            if getattr(self.terms[term], 'by', None) is not None:
                X[:, self.terms[term].by] = 1.

            return X

        # dont know what to do here
        else:
            raise TypeError('Unexpected term type: {}'.format(self.terms[term]))","np.linspace(self.terms[term].edge_knots_[0], self.terms[term].edge_knots_[1], num=n)","np.linspace(*self.terms[term].edge_knots_[:2], num=n)","iterable_zj[0], iterable_zj[1]",*self.terms[term].edge_knots_[:2],*self.terms[term].edge_knots_[:2],1
pyGAM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyGAM/pygam/pygam.py,https://github.com/dswah/pyGAM/tree/master/pygam/pygam.py,GAM,generate_X_grid$1398,"def generate_X_grid(self, term, n=100, meshgrid=False):
        """"""create a nice grid of X data

        array is sorted by feature and uniformly spaced,
        so the marginal and joint distributions are likely wrong

        if term is >= 0, we generate n samples per feature,
        which results in n^deg samples,
        where deg is the degree of the interaction of the term

        Parameters
        ----------
        term : int,
            Which term to process.

        n : int, optional
            number of data points to create

        meshgrid : bool, optional
            Whether to return a meshgrid (useful for 3d plotting)
            or a feature matrix (useful for inference like partial predictions)

        Returns
        -------
        if meshgrid is False:
            np.array of shape (n, n_features)
            where m is the number of
            (sub)terms in the requested (tensor)term.
        else:
            tuple of len m,
            where m is the number of (sub)terms in the requested
            (tensor)term.

            each element in the tuple contains a np.ndarray of size (n)^m

        Raises
        ------
        ValueError :
            If the term requested is an intercept
            since it does not make sense to process the intercept term.
        """"""
        if not self._is_fitted:
            raise AttributeError('GAM has not been fitted. Call fit first.')

        # cant do Intercept
        if self.terms[term].isintercept:
            raise ValueError('cannot create grid for intercept term')

        # process each subterm in a TensorTerm
        if self.terms[term].istensor:
            Xs = []
            for term_ in self.terms[term]:
                Xs.append(np.linspace(term_.edge_knots_[0],
                                      term_.edge_knots_[1],
                                      num=n))

            Xs = np.meshgrid(*Xs, indexing='ij')
            if meshgrid:
                return tuple(Xs)
            else:
                return self._flatten_mesh(Xs, term=term)

        # all other Terms
        elif hasattr(self.terms[term], 'edge_knots_'):
            x = np.linspace(self.terms[term].edge_knots_[0],
                            self.terms[term].edge_knots_[1],
                            num=n)

            if meshgrid:
                return (x,)

            # fill in feature matrix with only relevant features for this term
            X = np.zeros((n, self.statistics_['m_features']))
            X[:, self.terms[term].feature] = x
            if getattr(self.terms[term], 'by', None) is not None:
                X[:, self.terms[term].by] = 1.

            return X

        # dont know what to do here
        else:
            raise TypeError('Unexpected term type: {}'.format(self.terms[term]))","np.linspace(term_.edge_knots_[0], term_.edge_knots_[1], num=n)","np.linspace(*term_.edge_knots_[:2], num=n)","iterable_zj[0], iterable_zj[1]",*term_.edge_knots_[:2],*term_.edge_knots_[:2],1
PyQt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyQt/QtRemoteObjects/SyncUi/ClipboardSlave.py,https://github.com/PyQt5/PyQt/tree/master/QtRemoteObjects/SyncUi/ClipboardSlave.py,WindowSlave,__init__$27,"def __init__(self, *args, **kwargs):
        super(WindowSlave, self).__init__(*args, **kwargs)
        # 
        clipboard = QApplication.clipboard()
        clipboard.dataChanged.connect(self.on_data_changed)
        # Master
        node = QRemoteObjectNode(parent=self)
        node.connectToNode(QUrl('tcp://{}:{}'.format(sys.argv[1], sys.argv[2])))
        # WindowMaster
        self.windowMaster = node.acquireDynamic('WindowMaster')
        # 
        self.windowMaster.initialized.connect(self.onInitialized)
        #  https://doc.qt.io/qt-5/qremoteobjectreplica.html#State-enum
        self.windowMaster.stateChanged.connect(self.onStateChanged)","'tcp://{}:{}'.format(sys.argv[1], sys.argv[2])",'tcp://{}:{}'.format(*sys.argv[1:3]),"iterable_zj[1], iterable_zj[2]",*sys.argv[1:3],*sys.argv[1:3],1
mplfinance,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mplfinance/src/mplfinance/plotting.py,https://github.com/matplotlib/mplfinance/tree/master/src/mplfinance/plotting.py,,_addplot_apply_supplements$942,"def _addplot_apply_supplements(ax,apdict):
    if (apdict['ylabel'] is not None):
        ax.set_ylabel(apdict['ylabel'])
    if apdict['ylim'] is not None:
        ax.set_ylim(apdict['ylim'][0],apdict['ylim'][1])
    if apdict['title'] is not None:
        ax.set_title(apdict['title'])
    ysd = apdict['yscale']
    if isinstance(ysd,dict):
        yscale = ysd['yscale']
        del      ysd['yscale']
        ax.set_yscale(yscale,**ysd)
    elif isinstance(ysd,str):
        ax.set_yscale(ysd)","ax.set_ylim(apdict['ylim'][0], apdict['ylim'][1])",ax.set_ylim(*apdict['ylim'][:2]),"iterable_zj[0], iterable_zj[1]",*apdict['ylim'][:2],*apdict['ylim'][:2],1
YOLOv5-Lite,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/YOLOv5-Lite/models/yolo.py,https://github.com/ppogg/YOLOv5-Lite/tree/master/models/yolo.py,Model,fuse$179,"def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers
        print('Fusing layers... ')
        for m in self.model.modules():
            # print(m)
            if type(m) is RepVGGBlock:
                if hasattr(m, 'rbr_1x1'):
                    # print(m)
                    kernel, bias = m.get_equivalent_kernel_bias()
                    rbr_reparam = nn.Conv2d(in_channels=m.rbr_dense.conv.in_channels,
                                            out_channels=m.rbr_dense.conv.out_channels,
                                            kernel_size=m.rbr_dense.conv.kernel_size,
                                            stride=m.rbr_dense.conv.stride,
                                            padding=m.rbr_dense.conv.padding, dilation=m.rbr_dense.conv.dilation,
                                            groups=m.rbr_dense.conv.groups, bias=True)
                    rbr_reparam.weight.data = kernel
                    rbr_reparam.bias.data = bias
                    for para in self.parameters():
                        para.detach_()
                    m.rbr_dense = rbr_reparam
                    # m.__delattr__('rbr_dense')
                    m.__delattr__('rbr_1x1')
                    if hasattr(self, 'rbr_identity'):
                        m.__delattr__('rbr_identity')
                    if hasattr(self, 'id_tensor'):
                        m.__delattr__('id_tensor')
                    m.deploy = True
                    m.forward = m.fusevggforward  # update forward
                # continue
                # print(m)
            if type(m) is Conv and hasattr(m, 'bn'):
                # print(m)
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                delattr(m, 'bn')  # remove batchnorm
                m.forward = m.forward_fuse  # update forward

            if isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                delattr(m, 'bn')  # remove batchnorm
                m.forward = m.forward_fuse  # update forward

            if type(m) is Shuffle_Block:
                if hasattr(m, 'branch1'):
                    re_branch1 = nn.Sequential(
                        nn.Conv2d(m.branch1[0].in_channels, m.branch1[0].out_channels,
                                  kernel_size=m.branch1[0].kernel_size, stride=m.branch1[0].stride,
                                  padding=m.branch1[0].padding, groups=m.branch1[0].groups),
                        nn.Conv2d(m.branch1[2].in_channels, m.branch1[2].out_channels,
                                  kernel_size=m.branch1[2].kernel_size, stride=m.branch1[2].stride,
                                  padding=m.branch1[2].padding, bias=False),
                        nn.ReLU(inplace=True),
                    )
                    re_branch1[0] = fuse_conv_and_bn(m.branch1[0], m.branch1[1])
                    re_branch1[1] = fuse_conv_and_bn(m.branch1[2], m.branch1[3])
                    # pdb.set_trace()
                    # print(m.branch1[0])
                    m.branch1 = re_branch1
                if hasattr(m, 'branch2'):
                    re_branch2 = nn.Sequential(
                        nn.Conv2d(m.branch2[0].in_channels, m.branch2[0].out_channels,
                                  kernel_size=m.branch2[0].kernel_size, stride=m.branch2[0].stride,
                                  padding=m.branch2[0].padding, groups=m.branch2[0].groups),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(m.branch2[3].in_channels, m.branch2[3].out_channels,
                                  kernel_size=m.branch2[3].kernel_size, stride=m.branch2[3].stride,
                                  padding=m.branch2[3].padding, bias=False),
                        nn.Conv2d(m.branch2[5].in_channels, m.branch2[5].out_channels,
                                  kernel_size=m.branch2[5].kernel_size, stride=m.branch2[5].stride,
                                  padding=m.branch2[5].padding, groups=m.branch2[5].groups),
                        nn.ReLU(inplace=True),
                    )
                    re_branch2[0] = fuse_conv_and_bn(m.branch2[0], m.branch2[1])
                    re_branch2[2] = fuse_conv_and_bn(m.branch2[3], m.branch2[4])
                    re_branch2[3] = fuse_conv_and_bn(m.branch2[5], m.branch2[6])
                    # pdb.set_trace()
                    m.branch2 = re_branch2
                    # print(m.branch2)
        self.info()
        return self","fuse_conv_and_bn(m.branch1[0], m.branch1[1])",fuse_conv_and_bn(*m.branch1[:2]),"iterable_zj[0], iterable_zj[1]",*m.branch1[:2],*m.branch1[:2],1
YOLOv5-Lite,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/YOLOv5-Lite/models/yolo.py,https://github.com/ppogg/YOLOv5-Lite/tree/master/models/yolo.py,Model,fuse$179,"def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers
        print('Fusing layers... ')
        for m in self.model.modules():
            # print(m)
            if type(m) is RepVGGBlock:
                if hasattr(m, 'rbr_1x1'):
                    # print(m)
                    kernel, bias = m.get_equivalent_kernel_bias()
                    rbr_reparam = nn.Conv2d(in_channels=m.rbr_dense.conv.in_channels,
                                            out_channels=m.rbr_dense.conv.out_channels,
                                            kernel_size=m.rbr_dense.conv.kernel_size,
                                            stride=m.rbr_dense.conv.stride,
                                            padding=m.rbr_dense.conv.padding, dilation=m.rbr_dense.conv.dilation,
                                            groups=m.rbr_dense.conv.groups, bias=True)
                    rbr_reparam.weight.data = kernel
                    rbr_reparam.bias.data = bias
                    for para in self.parameters():
                        para.detach_()
                    m.rbr_dense = rbr_reparam
                    # m.__delattr__('rbr_dense')
                    m.__delattr__('rbr_1x1')
                    if hasattr(self, 'rbr_identity'):
                        m.__delattr__('rbr_identity')
                    if hasattr(self, 'id_tensor'):
                        m.__delattr__('id_tensor')
                    m.deploy = True
                    m.forward = m.fusevggforward  # update forward
                # continue
                # print(m)
            if type(m) is Conv and hasattr(m, 'bn'):
                # print(m)
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                delattr(m, 'bn')  # remove batchnorm
                m.forward = m.forward_fuse  # update forward

            if isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                delattr(m, 'bn')  # remove batchnorm
                m.forward = m.forward_fuse  # update forward

            if type(m) is Shuffle_Block:
                if hasattr(m, 'branch1'):
                    re_branch1 = nn.Sequential(
                        nn.Conv2d(m.branch1[0].in_channels, m.branch1[0].out_channels,
                                  kernel_size=m.branch1[0].kernel_size, stride=m.branch1[0].stride,
                                  padding=m.branch1[0].padding, groups=m.branch1[0].groups),
                        nn.Conv2d(m.branch1[2].in_channels, m.branch1[2].out_channels,
                                  kernel_size=m.branch1[2].kernel_size, stride=m.branch1[2].stride,
                                  padding=m.branch1[2].padding, bias=False),
                        nn.ReLU(inplace=True),
                    )
                    re_branch1[0] = fuse_conv_and_bn(m.branch1[0], m.branch1[1])
                    re_branch1[1] = fuse_conv_and_bn(m.branch1[2], m.branch1[3])
                    # pdb.set_trace()
                    # print(m.branch1[0])
                    m.branch1 = re_branch1
                if hasattr(m, 'branch2'):
                    re_branch2 = nn.Sequential(
                        nn.Conv2d(m.branch2[0].in_channels, m.branch2[0].out_channels,
                                  kernel_size=m.branch2[0].kernel_size, stride=m.branch2[0].stride,
                                  padding=m.branch2[0].padding, groups=m.branch2[0].groups),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(m.branch2[3].in_channels, m.branch2[3].out_channels,
                                  kernel_size=m.branch2[3].kernel_size, stride=m.branch2[3].stride,
                                  padding=m.branch2[3].padding, bias=False),
                        nn.Conv2d(m.branch2[5].in_channels, m.branch2[5].out_channels,
                                  kernel_size=m.branch2[5].kernel_size, stride=m.branch2[5].stride,
                                  padding=m.branch2[5].padding, groups=m.branch2[5].groups),
                        nn.ReLU(inplace=True),
                    )
                    re_branch2[0] = fuse_conv_and_bn(m.branch2[0], m.branch2[1])
                    re_branch2[2] = fuse_conv_and_bn(m.branch2[3], m.branch2[4])
                    re_branch2[3] = fuse_conv_and_bn(m.branch2[5], m.branch2[6])
                    # pdb.set_trace()
                    m.branch2 = re_branch2
                    # print(m.branch2)
        self.info()
        return self","fuse_conv_and_bn(m.branch1[2], m.branch1[3])",fuse_conv_and_bn(*m.branch1[2:4]),"iterable_zj[2], iterable_zj[3]",*m.branch1[2:4],*m.branch1[2:4],1
YOLOv5-Lite,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/YOLOv5-Lite/models/yolo.py,https://github.com/ppogg/YOLOv5-Lite/tree/master/models/yolo.py,Model,fuse$179,"def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers
        print('Fusing layers... ')
        for m in self.model.modules():
            # print(m)
            if type(m) is RepVGGBlock:
                if hasattr(m, 'rbr_1x1'):
                    # print(m)
                    kernel, bias = m.get_equivalent_kernel_bias()
                    rbr_reparam = nn.Conv2d(in_channels=m.rbr_dense.conv.in_channels,
                                            out_channels=m.rbr_dense.conv.out_channels,
                                            kernel_size=m.rbr_dense.conv.kernel_size,
                                            stride=m.rbr_dense.conv.stride,
                                            padding=m.rbr_dense.conv.padding, dilation=m.rbr_dense.conv.dilation,
                                            groups=m.rbr_dense.conv.groups, bias=True)
                    rbr_reparam.weight.data = kernel
                    rbr_reparam.bias.data = bias
                    for para in self.parameters():
                        para.detach_()
                    m.rbr_dense = rbr_reparam
                    # m.__delattr__('rbr_dense')
                    m.__delattr__('rbr_1x1')
                    if hasattr(self, 'rbr_identity'):
                        m.__delattr__('rbr_identity')
                    if hasattr(self, 'id_tensor'):
                        m.__delattr__('id_tensor')
                    m.deploy = True
                    m.forward = m.fusevggforward  # update forward
                # continue
                # print(m)
            if type(m) is Conv and hasattr(m, 'bn'):
                # print(m)
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                delattr(m, 'bn')  # remove batchnorm
                m.forward = m.forward_fuse  # update forward

            if isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                delattr(m, 'bn')  # remove batchnorm
                m.forward = m.forward_fuse  # update forward

            if type(m) is Shuffle_Block:
                if hasattr(m, 'branch1'):
                    re_branch1 = nn.Sequential(
                        nn.Conv2d(m.branch1[0].in_channels, m.branch1[0].out_channels,
                                  kernel_size=m.branch1[0].kernel_size, stride=m.branch1[0].stride,
                                  padding=m.branch1[0].padding, groups=m.branch1[0].groups),
                        nn.Conv2d(m.branch1[2].in_channels, m.branch1[2].out_channels,
                                  kernel_size=m.branch1[2].kernel_size, stride=m.branch1[2].stride,
                                  padding=m.branch1[2].padding, bias=False),
                        nn.ReLU(inplace=True),
                    )
                    re_branch1[0] = fuse_conv_and_bn(m.branch1[0], m.branch1[1])
                    re_branch1[1] = fuse_conv_and_bn(m.branch1[2], m.branch1[3])
                    # pdb.set_trace()
                    # print(m.branch1[0])
                    m.branch1 = re_branch1
                if hasattr(m, 'branch2'):
                    re_branch2 = nn.Sequential(
                        nn.Conv2d(m.branch2[0].in_channels, m.branch2[0].out_channels,
                                  kernel_size=m.branch2[0].kernel_size, stride=m.branch2[0].stride,
                                  padding=m.branch2[0].padding, groups=m.branch2[0].groups),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(m.branch2[3].in_channels, m.branch2[3].out_channels,
                                  kernel_size=m.branch2[3].kernel_size, stride=m.branch2[3].stride,
                                  padding=m.branch2[3].padding, bias=False),
                        nn.Conv2d(m.branch2[5].in_channels, m.branch2[5].out_channels,
                                  kernel_size=m.branch2[5].kernel_size, stride=m.branch2[5].stride,
                                  padding=m.branch2[5].padding, groups=m.branch2[5].groups),
                        nn.ReLU(inplace=True),
                    )
                    re_branch2[0] = fuse_conv_and_bn(m.branch2[0], m.branch2[1])
                    re_branch2[2] = fuse_conv_and_bn(m.branch2[3], m.branch2[4])
                    re_branch2[3] = fuse_conv_and_bn(m.branch2[5], m.branch2[6])
                    # pdb.set_trace()
                    m.branch2 = re_branch2
                    # print(m.branch2)
        self.info()
        return self","fuse_conv_and_bn(m.branch2[0], m.branch2[1])",fuse_conv_and_bn(*m.branch2[:2]),"iterable_zj[0], iterable_zj[1]",*m.branch2[:2],*m.branch2[:2],1
YOLOv5-Lite,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/YOLOv5-Lite/models/yolo.py,https://github.com/ppogg/YOLOv5-Lite/tree/master/models/yolo.py,Model,fuse$179,"def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers
        print('Fusing layers... ')
        for m in self.model.modules():
            # print(m)
            if type(m) is RepVGGBlock:
                if hasattr(m, 'rbr_1x1'):
                    # print(m)
                    kernel, bias = m.get_equivalent_kernel_bias()
                    rbr_reparam = nn.Conv2d(in_channels=m.rbr_dense.conv.in_channels,
                                            out_channels=m.rbr_dense.conv.out_channels,
                                            kernel_size=m.rbr_dense.conv.kernel_size,
                                            stride=m.rbr_dense.conv.stride,
                                            padding=m.rbr_dense.conv.padding, dilation=m.rbr_dense.conv.dilation,
                                            groups=m.rbr_dense.conv.groups, bias=True)
                    rbr_reparam.weight.data = kernel
                    rbr_reparam.bias.data = bias
                    for para in self.parameters():
                        para.detach_()
                    m.rbr_dense = rbr_reparam
                    # m.__delattr__('rbr_dense')
                    m.__delattr__('rbr_1x1')
                    if hasattr(self, 'rbr_identity'):
                        m.__delattr__('rbr_identity')
                    if hasattr(self, 'id_tensor'):
                        m.__delattr__('id_tensor')
                    m.deploy = True
                    m.forward = m.fusevggforward  # update forward
                # continue
                # print(m)
            if type(m) is Conv and hasattr(m, 'bn'):
                # print(m)
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                delattr(m, 'bn')  # remove batchnorm
                m.forward = m.forward_fuse  # update forward

            if isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                delattr(m, 'bn')  # remove batchnorm
                m.forward = m.forward_fuse  # update forward

            if type(m) is Shuffle_Block:
                if hasattr(m, 'branch1'):
                    re_branch1 = nn.Sequential(
                        nn.Conv2d(m.branch1[0].in_channels, m.branch1[0].out_channels,
                                  kernel_size=m.branch1[0].kernel_size, stride=m.branch1[0].stride,
                                  padding=m.branch1[0].padding, groups=m.branch1[0].groups),
                        nn.Conv2d(m.branch1[2].in_channels, m.branch1[2].out_channels,
                                  kernel_size=m.branch1[2].kernel_size, stride=m.branch1[2].stride,
                                  padding=m.branch1[2].padding, bias=False),
                        nn.ReLU(inplace=True),
                    )
                    re_branch1[0] = fuse_conv_and_bn(m.branch1[0], m.branch1[1])
                    re_branch1[1] = fuse_conv_and_bn(m.branch1[2], m.branch1[3])
                    # pdb.set_trace()
                    # print(m.branch1[0])
                    m.branch1 = re_branch1
                if hasattr(m, 'branch2'):
                    re_branch2 = nn.Sequential(
                        nn.Conv2d(m.branch2[0].in_channels, m.branch2[0].out_channels,
                                  kernel_size=m.branch2[0].kernel_size, stride=m.branch2[0].stride,
                                  padding=m.branch2[0].padding, groups=m.branch2[0].groups),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(m.branch2[3].in_channels, m.branch2[3].out_channels,
                                  kernel_size=m.branch2[3].kernel_size, stride=m.branch2[3].stride,
                                  padding=m.branch2[3].padding, bias=False),
                        nn.Conv2d(m.branch2[5].in_channels, m.branch2[5].out_channels,
                                  kernel_size=m.branch2[5].kernel_size, stride=m.branch2[5].stride,
                                  padding=m.branch2[5].padding, groups=m.branch2[5].groups),
                        nn.ReLU(inplace=True),
                    )
                    re_branch2[0] = fuse_conv_and_bn(m.branch2[0], m.branch2[1])
                    re_branch2[2] = fuse_conv_and_bn(m.branch2[3], m.branch2[4])
                    re_branch2[3] = fuse_conv_and_bn(m.branch2[5], m.branch2[6])
                    # pdb.set_trace()
                    m.branch2 = re_branch2
                    # print(m.branch2)
        self.info()
        return self","fuse_conv_and_bn(m.branch2[3], m.branch2[4])",fuse_conv_and_bn(*m.branch2[3:5]),"iterable_zj[3], iterable_zj[4]",*m.branch2[3:5],*m.branch2[3:5],1
YOLOv5-Lite,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/YOLOv5-Lite/models/yolo.py,https://github.com/ppogg/YOLOv5-Lite/tree/master/models/yolo.py,Model,fuse$179,"def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers
        print('Fusing layers... ')
        for m in self.model.modules():
            # print(m)
            if type(m) is RepVGGBlock:
                if hasattr(m, 'rbr_1x1'):
                    # print(m)
                    kernel, bias = m.get_equivalent_kernel_bias()
                    rbr_reparam = nn.Conv2d(in_channels=m.rbr_dense.conv.in_channels,
                                            out_channels=m.rbr_dense.conv.out_channels,
                                            kernel_size=m.rbr_dense.conv.kernel_size,
                                            stride=m.rbr_dense.conv.stride,
                                            padding=m.rbr_dense.conv.padding, dilation=m.rbr_dense.conv.dilation,
                                            groups=m.rbr_dense.conv.groups, bias=True)
                    rbr_reparam.weight.data = kernel
                    rbr_reparam.bias.data = bias
                    for para in self.parameters():
                        para.detach_()
                    m.rbr_dense = rbr_reparam
                    # m.__delattr__('rbr_dense')
                    m.__delattr__('rbr_1x1')
                    if hasattr(self, 'rbr_identity'):
                        m.__delattr__('rbr_identity')
                    if hasattr(self, 'id_tensor'):
                        m.__delattr__('id_tensor')
                    m.deploy = True
                    m.forward = m.fusevggforward  # update forward
                # continue
                # print(m)
            if type(m) is Conv and hasattr(m, 'bn'):
                # print(m)
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                delattr(m, 'bn')  # remove batchnorm
                m.forward = m.forward_fuse  # update forward

            if isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                delattr(m, 'bn')  # remove batchnorm
                m.forward = m.forward_fuse  # update forward

            if type(m) is Shuffle_Block:
                if hasattr(m, 'branch1'):
                    re_branch1 = nn.Sequential(
                        nn.Conv2d(m.branch1[0].in_channels, m.branch1[0].out_channels,
                                  kernel_size=m.branch1[0].kernel_size, stride=m.branch1[0].stride,
                                  padding=m.branch1[0].padding, groups=m.branch1[0].groups),
                        nn.Conv2d(m.branch1[2].in_channels, m.branch1[2].out_channels,
                                  kernel_size=m.branch1[2].kernel_size, stride=m.branch1[2].stride,
                                  padding=m.branch1[2].padding, bias=False),
                        nn.ReLU(inplace=True),
                    )
                    re_branch1[0] = fuse_conv_and_bn(m.branch1[0], m.branch1[1])
                    re_branch1[1] = fuse_conv_and_bn(m.branch1[2], m.branch1[3])
                    # pdb.set_trace()
                    # print(m.branch1[0])
                    m.branch1 = re_branch1
                if hasattr(m, 'branch2'):
                    re_branch2 = nn.Sequential(
                        nn.Conv2d(m.branch2[0].in_channels, m.branch2[0].out_channels,
                                  kernel_size=m.branch2[0].kernel_size, stride=m.branch2[0].stride,
                                  padding=m.branch2[0].padding, groups=m.branch2[0].groups),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(m.branch2[3].in_channels, m.branch2[3].out_channels,
                                  kernel_size=m.branch2[3].kernel_size, stride=m.branch2[3].stride,
                                  padding=m.branch2[3].padding, bias=False),
                        nn.Conv2d(m.branch2[5].in_channels, m.branch2[5].out_channels,
                                  kernel_size=m.branch2[5].kernel_size, stride=m.branch2[5].stride,
                                  padding=m.branch2[5].padding, groups=m.branch2[5].groups),
                        nn.ReLU(inplace=True),
                    )
                    re_branch2[0] = fuse_conv_and_bn(m.branch2[0], m.branch2[1])
                    re_branch2[2] = fuse_conv_and_bn(m.branch2[3], m.branch2[4])
                    re_branch2[3] = fuse_conv_and_bn(m.branch2[5], m.branch2[6])
                    # pdb.set_trace()
                    m.branch2 = re_branch2
                    # print(m.branch2)
        self.info()
        return self","fuse_conv_and_bn(m.branch2[5], m.branch2[6])",Cannot refactor,"iterable_zj[5], iterable_zj[6]",,*m.branch2[5:7],0
pyqtgraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyqtgraph/pyqtgraph/widgets/JoystickButton.py,https://github.com/pyqtgraph/pyqtgraph/tree/master/pyqtgraph/widgets/JoystickButton.py,JoystickButton,setState$45,"def setState(self, *xy):
        xy = list(xy)
        d = hypot(xy[0], xy[1])  # length
        nxy = [0, 0]
        for i in [0,1]:
            if xy[i] == 0:
                nxy[i] = 0
            else:
                nxy[i] = xy[i] / d
        
        if d > self.radius:
            d = self.radius
        d = (d / self.radius) ** 2
        xy = [nxy[0] * d, nxy[1] * d]
        
        w2 = self.width() / 2
        h2 = self.height() / 2
        self.spotPos = QtCore.QPoint(
            int(w2 * (1 + xy[0])),
            int(h2 * (1 - xy[1]))
        )
        self.update()
        if self.state == xy:
            return
        self.state = xy
        self.sigStateChanged.emit(self, self.state)","hypot(xy[0], xy[1])",hypot(*xy[:2]),"iterable_zj[0], iterable_zj[1]",*xy[:2],*xy[:2],1
eventlet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eventlet/tests/patcher_test.py,https://github.com/eventlet/eventlet/tree/master/tests/patcher_test.py,GreenThreadWrapper,test_ident$441,"def test_ident(self):
        self.write_to_tempfile(""newmod"", self.prologue + """"""
    print(id(t._g))
    print(t.ident)
"""""" + self.epilogue)
        output, lines = self.launch_subprocess('newmod.py')
        self.assertEqual(len(lines), 3, ""\n"".join(lines))
        self.assertEqual(lines[0], lines[1])","self.assertEqual(lines[0], lines[1])",self.assertEqual(*lines[:2]),"iterable_zj[0], iterable_zj[1]",*lines[:2],*lines[:2],1
R-Drop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/huggingface_transformer_src/src/transformers/models/transfo_xl/modeling_tf_transfo_xl_utilities.py,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/transfo_xl/modeling_tf_transfo_xl_utilities.py,TFAdaptiveSoftmaxMask,call$118,"def call(self, hidden, target, return_mean=True, training=False):
        head_logprob = 0
        if self.n_clusters == 0:
            output = self._logit(hidden, self.out_layers[0][0], self.out_layers[0][1], self.out_projs[0])
            if target is not None:
                loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target, logits=output)
            out = tf.nn.log_softmax(output, axis=-1)
        else:
            hidden_sizes = shape_list(hidden)
            out = []
            loss = tf.zeros(hidden_sizes[:2])
            for i in range(len(self.cutoffs)):
                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]
                if target is not None:
                    mask = (target >= l_idx) & (target < r_idx)
                    mask_idx = tf.where(mask)
                    cur_target = tf.boolean_mask(target, mask) - l_idx

                if self.div_val == 1:
                    cur_W = self.out_layers[0][0][l_idx:r_idx]
                    cur_b = self.out_layers[0][1][l_idx:r_idx]
                else:
                    cur_W = self.out_layers[i][0]
                    cur_b = self.out_layers[i][1]

                if i == 0:
                    cur_W = tf.concat([cur_W, self.cluster_weight], 0)
                    cur_b = tf.concat([cur_b, self.cluster_bias], 0)

                    head_logit = self._logit(hidden, cur_W, cur_b, self.out_projs[0])
                    head_logprob = tf.nn.log_softmax(head_logit)
                    out.append(head_logprob[..., : self.cutoffs[0]])
                    if target is not None:
                        cur_head_logprob = tf.boolean_mask(head_logprob, mask)
                        cur_logprob = self._gather_logprob(cur_head_logprob, cur_target)
                else:
                    tail_logit = self._logit(hidden, cur_W, cur_b, self.out_projs[i])
                    tail_logprob = tf.nn.log_softmax(tail_logit)
                    cluster_prob_idx = self.cutoffs[0] + i - 1  # No probability for the head cluster
                    logprob_i = head_logprob[..., cluster_prob_idx, None] + tail_logprob
                    out.append(logprob_i)
                    if target is not None:
                        cur_head_logprob = tf.boolean_mask(head_logprob, mask)
                        cur_tail_logprob = tf.boolean_mask(tail_logprob, mask)
                        cur_logprob = self._gather_logprob(cur_tail_logprob, cur_target)
                        cur_logprob += cur_head_logprob[:, self.cutoff_ends[1] + i - 1]
                if target is not None:
                    loss += tf.scatter_nd(mask_idx, -cur_logprob, shape_list(loss))
            out = tf.concat(out, axis=-1)

        if target is not None:
            if return_mean:
                loss = tf.reduce_mean(loss)
            # Add the training-time loss value to the layer using `self.add_loss()`.
            self.add_loss(loss)

            # Log the loss as a metric (we could log arbitrary metrics,
            # including different metrics for training and inference.
            self.add_metric(loss, name=self.name, aggregation=""mean"" if return_mean else """")

        return out","self._logit(hidden, self.out_layers[0][0], self.out_layers[0][1], self.out_projs[0])","self._logit(hidden, *self.out_layers[0][:2], self.out_projs[0])","iterable_zj[0], iterable_zj[1]",*self.out_layers[0][:2],*self.out_layers[0][:2],1
fonttools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/ttLib/tables/S__i_l_f.py,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/S__i_l_f.py,Silf,fromXML$532,"def fromXML(self, name, attrs, content, ttFont, version=2.0):
        if name == 'version':
            self.ruleVersion = float(safeEval(attrs.get('ruleVersion', ""0"")))
        if name == 'info':
            getSimple(self, attrs, *attrs_info)
        elif name == 'passindexes':
            getSimple(self, attrs, *attrs_passindexes)
        elif name == 'contexts':
            getSimple(self, attrs, *attrs_contexts)
        elif name == 'attributes':
            getSimple(self, attrs, *attrs_attributes)
        elif name == 'justifications':
            for element in content:
                if not isinstance(element, tuple): continue
                (tag, attrs, subcontent) = element
                if tag == 'justify':
                    j = _Object()
                    for k, v in attrs.items():
                        setattr(j, k, int(v))
                    self.jLevels.append(j)
        elif name == 'critFeatures':
            self.critFeatures = []
            element = content_string(content)
            self.critFeatures.extend(map(int, element.split()))
        elif name == 'scriptTags':
            self.scriptTags = []
            element = content_string(content)
            for n in element.split():
                self.scriptTags.append(n)
        elif name == 'pseudoMap':
            self.pMap = {}
            for element in content:
                if not isinstance(element, tuple): continue
                (tag, attrs, subcontent) = element
                if tag == 'pseudo':
                    k = int(attrs['unicode'], 16)
                    v = attrs['pseudo']
                self.pMap[k] = v
        elif name == 'classes':
            self.classes = Classes()
            for element in content:
                if not isinstance(element, tuple): continue
                tag, attrs, subcontent = element
                self.classes.fromXML(tag, attrs, subcontent, ttFont, version)
        elif name == 'passes':
            for element in content:
                if not isinstance(element, tuple): continue
                tag, attrs, subcontent = element
                if tag == 'pass':
                    p = Pass()
                    for e in subcontent:
                        if not isinstance(e, tuple): continue
                        p.fromXML(e[0], e[1], e[2], ttFont, version)
                    self.passes.append(p)","p.fromXML(e[0], e[1], e[2], ttFont, version)","p.fromXML(*e[:3], ttFont, version)","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*e[:3],*e[:3],1
tr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tr/tr/tr.py,https://github.com/myhub/tr/tree/master/tr/tr.py,,crnn$164,"def crnn(img, max_items=512*7000, crnn_id=1):
    buf_arr = np.zeros((max_items,), dtype=""float32"")
    shape_arr = np.zeros((8,), dtype=""int32"")
    img = c_img(img)

    assert img[3] == CV_32FC1
    assert img[1] == 32

    num = _libc.tr_crnn(
        crnn_id,
        img[0], img[1], img[2],
        c_ptr(buf_arr),
        c_ptr(shape_arr),
        max_items
    )

    buf_arr = buf_arr[:num]
    return buf_arr.reshape(shape_arr[0], shape_arr[2])","_libc.tr_crnn(crnn_id, img[0], img[1], img[2], c_ptr(buf_arr), c_ptr(shape_arr), max_items)","_libc.tr_crnn(crnn_id, *img[:3], c_ptr(buf_arr), c_ptr(shape_arr), max_items)","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*img[:3],*img[:3],1
tr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tr/tr/tr.py,https://github.com/myhub/tr/tree/master/tr/tr.py,,crnn$164,"def crnn(img, max_items=512*7000, crnn_id=1):
    buf_arr = np.zeros((max_items,), dtype=""float32"")
    shape_arr = np.zeros((8,), dtype=""int32"")
    img = c_img(img)

    assert img[3] == CV_32FC1
    assert img[1] == 32

    num = _libc.tr_crnn(
        crnn_id,
        img[0], img[1], img[2],
        c_ptr(buf_arr),
        c_ptr(shape_arr),
        max_items
    )

    buf_arr = buf_arr[:num]
    return buf_arr.reshape(shape_arr[0], shape_arr[2])","buf_arr.reshape(shape_arr[0], shape_arr[2])",buf_arr.reshape(*shape_arr[::2]),"iterable_zj[0], iterable_zj[2]",*shape_arr[::2],*shape_arr[:4:2],0
matrixprofile-ts,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/matrixprofile-ts/tests/test_matrixProfile.py,https://github.com/target/matrixprofile-ts/tree/master/tests/test_matrixProfile.py,TestClass,test_stampi_mp$134,"def test_stampi_mp(self):
        a = np.array([0.0,1.0,1.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0])
        r = stamp(a,4, sampling=1.0)
        final = np.round(stampi_update(a,4,r[0],r[1],95),2)

        mp_outcome = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.83])

        assert(np.allclose(final[0],mp_outcome))","stampi_update(a, 4, r[0], r[1], 95)","stampi_update(a, 4, *r[:2], 95)","iterable_zj[0], iterable_zj[1]",*r[:2],*r[:2],1
primerpython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/primerpython/blender_scripts/video_scenes/aggression.py,https://github.com/Helpsypoo/primerpython/tree/master/blender_scripts/video_scenes/aggression.py,HawkDove,go_to_mid_1$1941,"def go_to_mid_1(blob, start_time):
            blob.move_to(
                new_location = [
                    -2,
                    -10 / 2,
                    0
                ],
                start_time = start_time
            )
            blob.color_shift(
                color = mix_colors_hsv(COLORS_SCALED[2], COLORS_SCALED[5], 0.5),
                start_time = start_time,
                duration_time = None,
                obj = blob.ref_obj.children[0].children[0]
            )","mix_colors_hsv(COLORS_SCALED[2], COLORS_SCALED[5], 0.5)","mix_colors_hsv(*COLORS_SCALED[2], COLORS_SCALED[5] cannot be sliced using the slice operator [:]. Instead, you can use the following code to get the desired elements:, 0.5)","iterable_zj[2], iterable_zj[5]","*COLORS_SCALED[2], COLORS_SCALED[5] cannot be sliced using the slice operator [:]. Instead, you can use the following code to get the desired elements:",*COLORS_SCALED[2:8:3],0
pennylane,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pennylane/pennylane/devices/default_qubit.py,https://github.com/PennyLaneAI/pennylane/tree/master/pennylane/devices/default_qubit.py,DefaultQubit,_apply_unitary_einsum$814,"def _apply_unitary_einsum(self, state, mat, wires):
        r""""""Apply multiplication of a matrix to subsystems of the quantum state.

        This function uses einsum instead of tensordot. This approach is only
        faster for single- and two-qubit gates.

        Args:
            state (array[complex]): input state
            mat (array): matrix to multiply
            wires (Wires): target wires

        Returns:
            array[complex]: output state
        """"""
        # translate to wire labels used by device
        device_wires = self.map_wires(wires)

        dim = 2 ** len(device_wires)
        batch_size = self._get_batch_size(mat, (dim, dim), dim**2)

        # If the matrix is broadcasted, it is reshaped to have leading axis of size mat_batch_size
        shape = [2] * (len(device_wires) * 2)
        if batch_size is not None:
            shape.insert(0, batch_size)
        mat = self._cast(self._reshape(mat, shape), dtype=self.C_DTYPE)

        # Tensor indices of the quantum state
        state_indices = ABC[: self.num_wires]

        # Indices of the quantum state affected by this operation
        affected_indices = """".join(ABC_ARRAY[list(device_wires)].tolist())

        # All affected indices will be summed over, so we need the same number of new indices
        new_indices = ABC[self.num_wires : self.num_wires + len(device_wires)]

        # The new indices of the state are given by the old ones with the affected indices
        # replaced by the new_indices
        new_state_indices = functools.reduce(
            lambda old_string, idx_pair: old_string.replace(idx_pair[0], idx_pair[1]),
            zip(affected_indices, new_indices),
            state_indices,
        )

        # We now put together the indices in the notation numpy's einsum requires
        # This notation allows for the state, the matrix, or both to be broadcasted
        einsum_indices = (
            f""...{new_indices}{affected_indices},...{state_indices}->...{new_state_indices}""
        )

        return self._einsum(einsum_indices, mat, state)","old_string.replace(idx_pair[0], idx_pair[1])",old_string.replace(*idx_pair[:2]),"iterable_zj[0], iterable_zj[1]",*idx_pair[:2],*idx_pair[:2],1
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2016/wcat.py,https://github.com/3b1b/videos/tree/master/_2016/wcat.py,DefineOrderedPair,construct$890,"def construct(self):
        title = TexText(""Ordered pairs"")
        title.to_edge(UP)
        subtitle = Tex(
            ""("", ""a"", "","", ""b"", "")"", 
            ""\\ne"", 
            ""("", ""b"", "","", ""a"", "")""
        )
        labels_start = VGroup(subtitle[1], subtitle[3])
        labels_end = VGroup(subtitle[9], subtitle[7])
        subtitle.next_to(title, DOWN)
        colors = GREEN, RED
        for char, color in zip(""ab"", colors):
            subtitle.set_color_by_tex(char, color)
        self.loop.next_to(subtitle, DOWN)
        self.add(title, subtitle)

        self.add_dots_at_alphas(0.5, 0.6)
        dots = self.dots
        for dot, color, char in zip(dots, colors, ""ab""):
            dot.set_color(color)
            label = Tex(char)
            label.set_color(color)
            label.next_to(dot, RIGHT, buff = SMALL_BUFF)
            dot.label = label
        self.dots[1].label.shift(0.3*UP)
        first = TexText(""First"")
        first.next_to(self.dots[0], UP+2*LEFT, LARGE_BUFF)
        arrow = Arrow(first.get_bottom(), self.dots[0], color = GREEN)

        self.wait()
        self.play(*[
            Transform(label.copy(), dot.label)
            for label, dot in zip(labels_start, dots)
        ])
        self.remove(*self.get_mobjects_from_last_animation())
        self.add(*[d.label for d in dots])
        self.wait()
        self.play(
            Write(first),
            ShowCreation(arrow)
        )
        self.wait()","VGroup(subtitle[1], subtitle[3])",Cannot refactor,"iterable_zj[1], iterable_zj[3]",,*subtitle[1:5:2],0
sunpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sunpy/sunpy/io/tests/test_filetools.py,https://github.com/sunpy/sunpy/tree/master/sunpy/io/tests/test_filetools.py,,test_write_file_fits$129,"def test_write_file_fits(fname):
    aiapair = sunpy.io.read_file(TEST_AIA_IMAGE)[0]
    sunpy.io.write_file(fname, aiapair[0], aiapair[1],
                        overwrite=True)
    assert os.path.exists(""aia_171_image.fits"")
    outpair = sunpy.io.read_file(TEST_AIA_IMAGE)[0]
    assert np.all(np.equal(outpair[0], aiapair[0]))
    assert outpair[1] == aiapair[1]
    os.remove(""aia_171_image.fits"")","sunpy.io.write_file(fname, aiapair[0], aiapair[1], overwrite=True)","sunpy.io.write_file(fname, *aiapair[:2], overwrite=True)","iterable_zj[0], iterable_zj[1]",*aiapair[:2],*aiapair[:2],1
vision,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vision/torchvision/ops/ps_roi_align.py,https://github.com/pytorch/vision/tree/master/torchvision/ops/ps_roi_align.py,,ps_roi_align$12,"def ps_roi_align(
    input: Tensor,
    boxes: Tensor,
    output_size: int,
    spatial_scale: float = 1.0,
    sampling_ratio: int = -1,
) -> Tensor:
    """"""
    Performs Position-Sensitive Region of Interest (RoI) Align operator
    mentioned in Light-Head R-CNN.

    Args:
        input (Tensor[N, C, H, W]): The input tensor, i.e. a batch with ``N`` elements. Each element
            contains ``C`` feature maps of dimensions ``H x W``.
        boxes (Tensor[K, 5] or List[Tensor[L, 4]]): the box coordinates in (x1, y1, x2, y2)
            format where the regions will be taken from.
            The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``.
            If a single Tensor is passed, then the first column should
            contain the index of the corresponding element in the batch, i.e. a number in ``[0, N - 1]``.
            If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i
            in the batch.
        output_size (int or Tuple[int, int]): the size of the output (in bins or pixels) after the pooling
            is performed, as (height, width).
        spatial_scale (float): a scaling factor that maps the box coordinates to
            the input coordinates. For example, if your boxes are defined on the scale
            of a 224x224 image and your input is a 112x112 feature map (resulting from a 0.5x scaling of
            the original image), you'll want to set this to 0.5. Default: 1.0
        sampling_ratio (int): number of sampling points in the interpolation grid
            used to compute the output value of each pooled output bin. If > 0,
            then exactly ``sampling_ratio x sampling_ratio`` sampling points per bin are used. If
            <= 0, then an adaptive number of grid points are used (computed as
            ``ceil(roi_width / output_width)``, and likewise for height). Default: -1

    Returns:
        Tensor[K, C / (output_size[0] * output_size[1]), output_size[0], output_size[1]]: The pooled RoIs
    """"""
    if not torch.jit.is_scripting() and not torch.jit.is_tracing():
        _log_api_usage_once(ps_roi_align)
    _assert_has_ops()
    check_roi_boxes_shape(boxes)
    rois = boxes
    output_size = _pair(output_size)
    if not isinstance(rois, torch.Tensor):
        rois = convert_boxes_to_roi_format(rois)
    output, _ = torch.ops.torchvision.ps_roi_align(
        input, rois, spatial_scale, output_size[0], output_size[1], sampling_ratio
    )
    return output","torch.ops.torchvision.ps_roi_align(input, rois, spatial_scale, output_size[0], output_size[1], sampling_ratio)","torch.ops.torchvision.ps_roi_align(input, rois, spatial_scale, *output_size[:2], sampling_ratio)","iterable_zj[0], iterable_zj[1]",*output_size[:2],*output_size[:2],1
pandapower,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/test/loadflow/test_runpp.py,https://github.com/e2nIEE/pandapower/tree/master/pandapower/test/loadflow/test_runpp.py,,test_ext_grid_and_gen_at_one_bus$994,"def test_ext_grid_and_gen_at_one_bus(**kwargs):
    net = pp.create_empty_network()
    b1 = pp.create_bus(net, vn_kv=110)
    b2 = pp.create_bus(net, vn_kv=110)
    pp.create_ext_grid(net, b1, vm_pu=1.01)
    pp.create_line(net, b1, b2, 1., std_type=""305-AL1/39-ST1A 110.0"")
    pp.create_load(net, bus=b2, p_mw=3.5, q_mvar=1)

    runpp_with_consistency_checks(net, **kwargs)
    q = net.res_ext_grid.q_mvar.sum()

    ##create two gens at the slack bus
    g1 = pp.create_gen(net, b1, vm_pu=1.01, p_mw=1)
    g2 = pp.create_gen(net, b1, vm_pu=1.01, p_mw=1)
    runpp_with_consistency_checks(net, **kwargs)

    # all the reactive power previously provided by the ext_grid is now provided by the generators
    assert np.isclose(net.res_ext_grid.q_mvar.values, 0)
    assert np.isclose(net.res_gen.q_mvar.sum(), q)
    # since no Q-limits were set, reactive power is distributed equally to both generators
    assert np.isclose(net.res_gen.q_mvar.at[g1], net.res_gen.q_mvar.at[g2])

    # set reactive power limits at the generators
    net.gen[""max_q_mvar""] = [0.1, 0.01]
    net.gen[""min_q_mvar""] = [-0.1, -0.01]
    runpp_with_consistency_checks(net, **kwargs)
    # g1 now has 10 times the reactive power of g2 in accordance with the different Q ranges
    assert np.isclose(net.res_gen.q_mvar.at[g1], net.res_gen.q_mvar.at[g2] * 10)
    # all the reactive power is still provided by the generators, because Q-lims are not enforced
    assert np.allclose(net.res_ext_grid.q_mvar.values, [0])
    assert np.isclose(net.res_gen.q_mvar.sum(), q)

    # now enforce Q-lims
    runpp_with_consistency_checks(net, enforce_q_lims=True, **kwargs)
    # both generators are at there lower limit with regard to the reactive power
    assert np.allclose(net.res_gen.q_mvar.values, net.gen.max_q_mvar.values)
    # the total reactive power remains unchanged, but the rest of the power is now provided by the ext_grid
    assert np.isclose(net.res_gen.q_mvar.sum() + net.res_ext_grid.q_mvar.sum(), q)

    # second ext_grid at the slack bus
    pp.create_ext_grid(net, b1, vm_pu=1.01)
    runpp_with_consistency_checks(net, enforce_q_lims=False, **kwargs)
    # gens still have the correct active power
    assert np.allclose(net.gen.p_mw.values, net.res_gen.p_mw.values)
    # slack active power is evenly distributed to both ext_grids
    assert np.isclose(net.res_ext_grid.p_mw.values[0], net.res_ext_grid.p_mw.values[1])

    # q limits at the ext_grids are not enforced
    net.ext_grid[""max_q_mvar""] = [0.1, 0.01]
    net.ext_grid[""min_q_mvar""] = [-0.1, -0.01]
    runpp_with_consistency_checks(net, enforce_q_lims=True, **kwargs)
    assert net.res_ext_grid.q_mvar.values[0] > net.ext_grid.max_q_mvar.values[0]
    assert np.allclose(net.res_gen.q_mvar.values, net.gen.max_q_mvar.values)","np.isclose(net.res_ext_grid.p_mw.values[0], net.res_ext_grid.p_mw.values[1])",np.isclose(*net.res_ext_grid.p_mw.values[:2]),"iterable_zj[0], iterable_zj[1]",*net.res_ext_grid.p_mw.values[:2],*net.res_ext_grid.p_mw.values[:2],1
nixops,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nixops/nixops/script_defs.py,https://github.com/NixOS/nixops/tree/master/nixops/script_defs.py,,op_check$463,"def op_check(args: Namespace) -> None:  # noqa: C901
    def highlight(s: str) -> str:
        return nixops.ansi.ansi_highlight(s, outfile=sys.stdout)

    def warn(s: str) -> str:
        return nixops.ansi.ansi_warn(s, outfile=sys.stdout)

    def render_tristate(x: bool) -> str:
        if x is None:
            return ""N/A""
        elif x:
            return nixops.ansi.ansi_success(""Yes"", outfile=sys.stdout)
        else:
            return warn(""No"")

    tbl = create_table(
        ([(""Deployment"", ""l"")] if args.all else [])
        + [
            (""Name"", ""l""),
            (""Exists"", ""l""),
            (""Up"", ""l""),
            (""Reachable"", ""l""),
            (""Disks OK"", ""l""),
            (""Load avg."", ""l""),
            (""Units"", ""l""),
            (""Notes"", ""l""),
        ]
    )

    machines: List[nixops.backends.GenericMachineState] = []
    resources: List[nixops.resources.GenericResourceState] = []

    def check(depl: nixops.deployment.Deployment) -> None:
        for m in depl.active_resources.values():
            if not nixops.deployment.should_do(
                m, args.include or [], args.exclude or []
            ):
                continue
            if isinstance(m, nixops.backends.MachineState):
                machines.append(m)
            else:
                resources.append(m)

    # TODO: writable=False?
    # Historically, nixops check was allowed to write to the state file.
    # With remote state however, this requires an exclusive lock, which may
    # not be the best choice.
    with one_or_all(args, writable=True, activityDescription=""nixops check"") as depls:
        for depl in depls:
            check(depl)

        ResourceStatus = Tuple[
            str,
            Union[
                nixops.backends.GenericMachineState,
                nixops.resources.GenericResourceState,
            ],
            List[str],
            int,
        ]

        # Check all machines in parallel.
        def worker(m: nixops.backends.GenericMachineState) -> ResourceStatus:
            res = m.check()

            unit_lines: List[str] = []
            if res.failed_units:
                unit_lines.append(
                    ""\n"".join(
                        [warn(""{0} [failed]"".format(x)) for x in res.failed_units]
                    )
                )
            if res.in_progress_units:
                unit_lines.append(
                    ""\n"".join(
                        [
                            highlight(""{0} [running]"".format(x))
                            for x in res.in_progress_units
                        ]
                    )
                )

            row = ([m.depl.name or m.depl.uuid] if args.all else []) + [
                m.name,
                render_tristate(res.exists),
                render_tristate(res.is_up),
                render_tristate(res.is_reachable),
                render_tristate(res.disks_ok),
                ""{0} {1} {2}"".format(res.load[0], res.load[1], res.load[2])
                if res.load is not None
                else """",
                ""\n"".join(unit_lines),
                ""\n"".join([warn(x) for x in res.messages]),
            ]
            status = 0
            if res.exists is False:
                status |= 1
            if res.is_up is False:
                status |= 2
            if res.is_reachable is False:
                status |= 4
            if res.disks_ok is False:
                status |= 8
            if res.failed_units is not None and res.failed_units != []:
                status |= 16
            return (m.depl.name or m.depl.uuid, m, row, status)

        resources_tbl = create_table(
            ([(""Deployment"", ""l"")] if args.all else [])
            + [(""Name"", ""l""), (""Exists"", ""l"")]
        )

        def resource_worker(
            r: nixops.resources.GenericResourceState,
        ) -> Optional[ResourceStatus]:
            if not nixops.deployment.is_machine(r):
                r.check()
                exist = True if r.state == nixops.resources.ResourceState.UP else False
                row = ([r.depl.name or r.depl.uuid] if args.all else []) + [
                    r.name,
                    render_tristate(exist),
                ]
                return (r.depl.name or r.depl.uuid, r, row, 0)
            return None

        results = run_tasks(nr_workers=len(machines), tasks=machines, worker_fun=worker)
        resources_results = run_tasks(
            nr_workers=len(resources), tasks=resources, worker_fun=resource_worker
        )

        # Sort the rows by deployment/machine.
        status = 0
        for res in sorted(
            [res for res in results if res is not None],
            key=lambda res: machine_to_key(res[0], res[1].name, res[1].get_type()),
        ):
            tbl.add_row(res[2])
            status |= res[3]
        print(nixops.ansi.ansi_success(""Machines state:""))
        print(tbl)

        for res in sorted(
            [res for res in resources_results if res is not None],
            key=lambda res: machine_to_key(res[0], res[1].name, res[1].get_type()),
        ):
            resources_tbl.add_row(res[2])
            status |= res[3]
        print(nixops.ansi.ansi_success(""Non machines resources state:""))
        print(resources_tbl)

        sys.exit(status)","'{0} {1} {2}'.format(res.load[0], res.load[1], res.load[2])",'{0} {1} {2}'.format(*res.load[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*res.load[:3],*res.load[:3],1
adversarial-robustness-toolbox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adversarial-robustness-toolbox/art/utils.py,https://github.com/Trusted-AI/adversarial-robustness-toolbox/tree/master/art/utils.py,,clip_and_round$1213,"def clip_and_round(x: np.ndarray, clip_values: Optional[""CLIP_VALUES_TYPE""], round_samples: float) -> np.ndarray:
    """"""
    Rounds the input to the correct level of granularity.
    Useful to ensure data passed to classifier can be represented
    in the correct domain, e.g., [0, 255] integers verses [0,1]
    or [0, 255] floating points.

    :param x: Sample input with shape as expected by the model.
    :param clip_values: Tuple of the form `(min, max)` representing the minimum and maximum values allowed
           for features, or `None` if no clipping should be performed.
    :param round_samples: The resolution of the input domain to round the data to, e.g., 1.0, or 1/255. Set to 0 to
           disable.
    """"""
    if round_samples == 0.0:
        return x
    if clip_values is not None:
        np.clip(x, clip_values[0], clip_values[1], out=x)
    x = np.around(x / round_samples) * round_samples
    return x","np.clip(x, clip_values[0], clip_values[1], out=x)","np.clip(x, *clip_values[:2], out=x)","iterable_zj[0], iterable_zj[1]",*clip_values[:2],*clip_values[:2],1
hydrus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydrus/hydrus/client/gui/search/ClientGUISearchPanels.py,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/gui/search/ClientGUISearchPanels.py,EditFavouriteSearchesPanel,_AddNewFavouriteSearch$211,"def _AddNewFavouriteSearch( self, search_row = None ):
        
        existing_folders_to_names = self._GetExistingFoldersToNames()
        
        existing_names = { name for name in itertools.chain.from_iterable( existing_folders_to_names.values() ) }
        
        if search_row is None:
            
            foldername = None
            name = 'new favourite search'
            
            default_local_file_service_key = HG.client_controller.services_manager.GetDefaultLocalFileServiceKey()
            
            location_search_context = ClientSearch.LocationSearchContext( current_service_keys = [ default_local_file_service_key ] )
            
            file_search_context = ClientSearch.FileSearchContext( location_search_context = location_search_context )
            
            synchronised = True
            media_sort = None
            media_collect = None
            
        else:
            
            ( foldername, name, file_search_context, synchronised, media_sort, media_collect ) = search_row
            
        
        name = HydrusData.GetNonDupeName( name, existing_names )
        
        with ClientGUITopLevelWindowsPanels.DialogEdit( self, 'edit favourite search' ) as dlg:
            
            panel = EditFavouriteSearchPanel( dlg, existing_folders_to_names, foldername, name, file_search_context, synchronised, media_sort, media_collect )
            
            dlg.SetPanel( panel )
            
            if dlg.exec() == QW.QDialog.Accepted:
                
                row = panel.GetValue()
                
                self._DeleteRow( row[0], row[1] )
                
                self._favourite_searches.AddDatas( ( row, ) )
                
                self._favourite_searches.Sort()","self._DeleteRow(row[0], row[1])",self._DeleteRow(*row[:2]),"iterable_zj[0], iterable_zj[1]",*row[:2],*row[:2],1
alfred,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alfred/alfred/modules/data/yolo2voc.py,https://github.com/jinfagang/alfred/tree/master/alfred/modules/data/yolo2voc.py,,if_main_my$133,"if __name__ == ""__main__"":
    yolo2voc(sys.argv[1], sys.argv[2], None)","yolo2voc(sys.argv[1], sys.argv[2], None)","yolo2voc(*sys.argv[1:3], None)","iterable_zj[1], iterable_zj[2]",*sys.argv[1:3],*sys.argv[1:3],1
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/returners/django_return.py,https://github.com/saltstack/salt/tree/master/salt/returners/django_return.py,,returner$57,"def returner(ret):
    """"""
    Signal a Django server that a return is available
    """"""
    signaled = dispatch.Signal(providing_args=[""ret""]).send(sender=""returner"", ret=ret)

    for signal in signaled:
        log.debug(
            ""Django returner function 'returner' signaled %s which responded with %s"",
            signal[0],
            signal[1],
        )","log.debug(""Django returner function 'returner' signaled %s which responded with %s"", signal[0], signal[1])","log.debug(""Django returner function 'returner' signaled %s which responded with %s"", *signal[:2])","iterable_zj[0], iterable_zj[1]",*signal[:2],*signal[:2],1
qutip,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutip/qutip/fastsparse.py,https://github.com/qutip/qutip/tree/master/qutip/fastsparse.py,fast_csr_matrix,_binopt$42,"def _binopt(self, other, op):
        """"""
        Do the binary operation fn to two sparse matrices using
        fast_csr_matrix only when other is also a fast_csr_matrix.
        """"""
        # e.g. csr_plus_csr, csr_minus_csr, etc.
        if not isinstance(other, fast_csr_matrix):
            other = csr_matrix(other)
        # e.g. csr_plus_csr, csr_minus_csr, etc.
        fn = getattr(_sparsetools, self.format + op + self.format)

        maxnnz = self.nnz + other.nnz
        idx_dtype = get_index_dtype((self.indptr, self.indices,
                                     other.indptr, other.indices),
                                    maxval=maxnnz)
        indptr = np.empty(self.indptr.shape, dtype=idx_dtype)
        indices = np.empty(maxnnz, dtype=idx_dtype)

        bool_ops = ['_ne_', '_lt_', '_gt_', '_le_', '_ge_']
        if op in bool_ops:
            data = np.empty(maxnnz, dtype=np.bool_)
        else:
            data = np.empty(maxnnz, dtype=upcast(self.dtype, other.dtype))

        fn(self.shape[0], self.shape[1],
           np.asarray(self.indptr, dtype=idx_dtype),
           np.asarray(self.indices, dtype=idx_dtype),
           self.data,
           np.asarray(other.indptr, dtype=idx_dtype),
           np.asarray(other.indices, dtype=idx_dtype),
           other.data,
           indptr, indices, data)

        actual_nnz = indptr[-1]
        indices = indices[:actual_nnz]
        data = data[:actual_nnz]
        if actual_nnz < maxnnz // 2:
            # too much waste, trim arrays
            indices = indices.copy()
            data = data.copy()
        if isinstance(other, fast_csr_matrix) and (not op in bool_ops):
            A = fast_csr_matrix((data, indices, indptr), dtype=data.dtype, shape=self.shape)
        else:
            A = csr_matrix((data, indices, indptr), dtype=data.dtype, shape=self.shape)
        return A","fn(self.shape[0], self.shape[1], np.asarray(self.indptr, dtype=idx_dtype), np.asarray(self.indices, dtype=idx_dtype), self.data, np.asarray(other.indptr, dtype=idx_dtype), np.asarray(other.indices, dtype=idx_dtype), other.data, indptr, indices, data)","fn(*self.shape[:2], np.asarray(self.indptr, dtype=idx_dtype), np.asarray(self.indices, dtype=idx_dtype), self.data, np.asarray(other.indptr, dtype=idx_dtype), np.asarray(other.indices, dtype=idx_dtype), other.data, indptr, indices, data)","iterable_zj[0], iterable_zj[1]",*self.shape[:2],*self.shape[:2],1
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/ppdet/modeling/backbones/hardnet.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/ppdet/modeling/backbones/hardnet.py,HarDNet,__init__$147,"def __init__(self, depth_wise=False, return_idx=[1, 3, 8, 13], arch=85):
        super(HarDNet, self).__init__()
        assert arch in [39, 68, 85], ""HarDNet-{} not support."".format(arch)
        if arch == 85:
            first_ch = [48, 96]
            second_kernel = 3
            ch_list = [192, 256, 320, 480, 720]
            grmul = 1.7
            gr = [24, 24, 28, 36, 48]
            n_layers = [8, 16, 16, 16, 16]
        elif arch == 68:
            first_ch = [32, 64]
            second_kernel = 3
            ch_list = [128, 256, 320, 640]
            grmul = 1.7
            gr = [14, 16, 20, 40]
            n_layers = [8, 16, 16, 16]

        self.return_idx = return_idx
        self._out_channels = [96, 214, 458, 784]

        avg_pool = True
        if depth_wise:
            second_kernel = 1
            avg_pool = False

        blks = len(n_layers)
        self.base = nn.LayerList([])

        # First Layer: Standard Conv3x3, Stride=2
        self.base.append(
            ConvLayer(
                in_channels=3,
                out_channels=first_ch[0],
                kernel_size=3,
                stride=2,
                bias_attr=False))

        # Second Layer
        self.base.append(
            ConvLayer(
                first_ch[0], first_ch[1], kernel_size=second_kernel))

        # Avgpooling or DWConv3x3 downsampling
        if avg_pool:
            self.base.append(nn.AvgPool2D(kernel_size=3, stride=2, padding=1))
        else:
            self.base.append(DWConvLayer(first_ch[1], first_ch[1], stride=2))

        # Build all HarDNet blocks
        ch = first_ch[1]
        for i in range(blks):
            blk = HarDBlock(ch, gr[i], grmul, n_layers[i], dwconv=depth_wise)
            ch = blk.out_channels
            self.base.append(blk)

            if i != blks - 1:
                self.base.append(ConvLayer(ch, ch_list[i], kernel_size=1))
            ch = ch_list[i]
            if i == 0:
                self.base.append(
                    nn.AvgPool2D(
                        kernel_size=2, stride=2, ceil_mode=True))
            elif i != blks - 1 and i != 1 and i != 3:
                self.base.append(nn.AvgPool2D(kernel_size=2, stride=2))","ConvLayer(first_ch[0], first_ch[1], kernel_size=second_kernel)","ConvLayer(*first_ch[:2], kernel_size=second_kernel)","iterable_zj[0], iterable_zj[1]",*first_ch[:2],*first_ch[:2],1
Poco,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Poco/poco/drivers/windows/windowsui_poco.py,https://github.com/AirtestProject/Poco/tree/master/poco/drivers/windows/windowsui_poco.py,WindowsPoco,double_click$72,"def double_click(self, pos):
        return self.agent.input.double_click(pos[0], pos[1])","self.agent.input.double_click(pos[0], pos[1])",self.agent.input.double_click(*pos[:2]),"iterable_zj[0], iterable_zj[1]",*pos[:2],*pos[:2],1
cogdl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cogdl/cogdl/models/emb/node2vec.py,https://github.com/THUDM/cogdl/tree/master/cogdl/models/emb/node2vec.py,Node2vec,_preprocess_transition_probs$159,"def _preprocess_transition_probs(self):
        # Preprocessing of transition probabilities for guiding the random walks.
        G = self.G
        is_directed = nx.is_directed(self.G)

        print(len(list(G.nodes())))
        print(len(list(G.edges())))

        s = time.time()
        alias_nodes = {}
        for node in G.nodes():
            unnormalized_probs = [G[node][nbr][""weight""] for nbr in G.neighbors(node)]
            norm_const = sum(unnormalized_probs)
            normalized_probs = [float(u_prob) / norm_const for u_prob in unnormalized_probs]
            alias_nodes[node] = alias_setup(normalized_probs)

        t = time.time()
        print(""alias_nodes"", t - s)

        alias_edges = {}
        s = time.time()

        if is_directed:
            for edge in G.edges():
                alias_edges[edge] = self._get_alias_edge(edge[0], edge[1])
        else:
            for edge in G.edges():
                alias_edges[edge] = self._get_alias_edge(edge[0], edge[1])
                alias_edges[(edge[1], edge[0])] = self._get_alias_edge(edge[1], edge[0])

        t = time.time()
        print(""alias_edges"", t - s)

        self.alias_nodes = alias_nodes
        self.alias_edges = alias_edges

        return","self._get_alias_edge(edge[0], edge[1])",self._get_alias_edge(*edge[:2]),"iterable_zj[0], iterable_zj[1]",*edge[:2],*edge[:2],1
cogdl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cogdl/cogdl/models/emb/node2vec.py,https://github.com/THUDM/cogdl/tree/master/cogdl/models/emb/node2vec.py,Node2vec,_preprocess_transition_probs$159,"def _preprocess_transition_probs(self):
        # Preprocessing of transition probabilities for guiding the random walks.
        G = self.G
        is_directed = nx.is_directed(self.G)

        print(len(list(G.nodes())))
        print(len(list(G.edges())))

        s = time.time()
        alias_nodes = {}
        for node in G.nodes():
            unnormalized_probs = [G[node][nbr][""weight""] for nbr in G.neighbors(node)]
            norm_const = sum(unnormalized_probs)
            normalized_probs = [float(u_prob) / norm_const for u_prob in unnormalized_probs]
            alias_nodes[node] = alias_setup(normalized_probs)

        t = time.time()
        print(""alias_nodes"", t - s)

        alias_edges = {}
        s = time.time()

        if is_directed:
            for edge in G.edges():
                alias_edges[edge] = self._get_alias_edge(edge[0], edge[1])
        else:
            for edge in G.edges():
                alias_edges[edge] = self._get_alias_edge(edge[0], edge[1])
                alias_edges[(edge[1], edge[0])] = self._get_alias_edge(edge[1], edge[0])

        t = time.time()
        print(""alias_edges"", t - s)

        self.alias_nodes = alias_nodes
        self.alias_edges = alias_edges

        return","self._get_alias_edge(edge[0], edge[1])",self._get_alias_edge(*edge[:2]),"iterable_zj[0], iterable_zj[1]",*edge[:2],*edge[:2],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_bip_mapping.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_bip_mapping.py,TestBIPMapping,test_multi_cregs$180,"def test_multi_cregs(self):
        """"""Test for multiple ClassicalRegisters.""""""

        #                        
        # qr_0:  X M
        #              
        # qr_1:  X  H M
        #                
        # qr_2:  X M
        #                       
        # qr_3:  X M
        #                              
        #  c: 2/
        #                               0    1  
        #                                       
        #  d: 2/
        #                                  0     1
        qr = QuantumRegister(4, ""qr"")
        cr1 = ClassicalRegister(2, ""c"")
        cr2 = ClassicalRegister(2, ""d"")
        circuit = QuantumCircuit(qr, cr1, cr2)
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])
        circuit.cx(qr[1], qr[2])
        circuit.h(qr[1])
        circuit.cx(qr[1], qr[0])
        circuit.barrier(qr)
        circuit.measure(qr[0], cr1[0])
        circuit.measure(qr[1], cr2[0])
        circuit.measure(qr[2], cr1[1])
        circuit.measure(qr[3], cr2[1])

        coupling = CouplingMap([[0, 1], [0, 2], [2, 3]])  # linear [1, 0, 2, 3]
        property_set = {}
        actual = BIPMapping(coupling, objective=""depth"")(circuit, property_set)
        self.assertEqual(5, actual.depth())

        CheckMap(coupling)(actual, property_set)
        self.assertTrue(property_set[""is_swap_mapped""])","circuit.cx(qr[0], qr[1])",circuit.cx(*qr[:2]),"iterable_zj[0], iterable_zj[1]",*qr[:2],*qr[:2],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_bip_mapping.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_bip_mapping.py,TestBIPMapping,test_multi_cregs$180,"def test_multi_cregs(self):
        """"""Test for multiple ClassicalRegisters.""""""

        #                        
        # qr_0:  X M
        #              
        # qr_1:  X  H M
        #                
        # qr_2:  X M
        #                       
        # qr_3:  X M
        #                              
        #  c: 2/
        #                               0    1  
        #                                       
        #  d: 2/
        #                                  0     1
        qr = QuantumRegister(4, ""qr"")
        cr1 = ClassicalRegister(2, ""c"")
        cr2 = ClassicalRegister(2, ""d"")
        circuit = QuantumCircuit(qr, cr1, cr2)
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])
        circuit.cx(qr[1], qr[2])
        circuit.h(qr[1])
        circuit.cx(qr[1], qr[0])
        circuit.barrier(qr)
        circuit.measure(qr[0], cr1[0])
        circuit.measure(qr[1], cr2[0])
        circuit.measure(qr[2], cr1[1])
        circuit.measure(qr[3], cr2[1])

        coupling = CouplingMap([[0, 1], [0, 2], [2, 3]])  # linear [1, 0, 2, 3]
        property_set = {}
        actual = BIPMapping(coupling, objective=""depth"")(circuit, property_set)
        self.assertEqual(5, actual.depth())

        CheckMap(coupling)(actual, property_set)
        self.assertTrue(property_set[""is_swap_mapped""])","circuit.cx(qr[2], qr[3])",Cannot refactor,"iterable_zj[2], iterable_zj[3]",,*qr[2:4],0
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_bip_mapping.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_bip_mapping.py,TestBIPMapping,test_multi_cregs$180,"def test_multi_cregs(self):
        """"""Test for multiple ClassicalRegisters.""""""

        #                        
        # qr_0:  X M
        #              
        # qr_1:  X  H M
        #                
        # qr_2:  X M
        #                       
        # qr_3:  X M
        #                              
        #  c: 2/
        #                               0    1  
        #                                       
        #  d: 2/
        #                                  0     1
        qr = QuantumRegister(4, ""qr"")
        cr1 = ClassicalRegister(2, ""c"")
        cr2 = ClassicalRegister(2, ""d"")
        circuit = QuantumCircuit(qr, cr1, cr2)
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])
        circuit.cx(qr[1], qr[2])
        circuit.h(qr[1])
        circuit.cx(qr[1], qr[0])
        circuit.barrier(qr)
        circuit.measure(qr[0], cr1[0])
        circuit.measure(qr[1], cr2[0])
        circuit.measure(qr[2], cr1[1])
        circuit.measure(qr[3], cr2[1])

        coupling = CouplingMap([[0, 1], [0, 2], [2, 3]])  # linear [1, 0, 2, 3]
        property_set = {}
        actual = BIPMapping(coupling, objective=""depth"")(circuit, property_set)
        self.assertEqual(5, actual.depth())

        CheckMap(coupling)(actual, property_set)
        self.assertTrue(property_set[""is_swap_mapped""])","circuit.cx(qr[1], qr[2])",circuit.cx(*qr[1:3]),"iterable_zj[1], iterable_zj[2]",*qr[1:3],*qr[1:3],1
few-shot-vid2vid,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/few-shot-vid2vid/models/networks/flownet2_pytorch/models.py,https://github.com/NVlabs/few-shot-vid2vid/tree/master/models/networks/flownet2_pytorch/models.py,FlowNet2,init_deconv_bilinear$101,"def init_deconv_bilinear(self, weight):
        f_shape = weight.size()
        heigh, width = f_shape[-2], f_shape[-1]
        f = np.ceil(width/2.0)
        c = (2 * f - 1 - f % 2) / (2.0 * f)
        bilinear = np.zeros([heigh, width])
        for x in range(width):
            for y in range(heigh):
                value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))
                bilinear[x, y] = value
        min_dim = min(f_shape[0], f_shape[1])
        weight.data.fill_(0.)
        for i in range(min_dim):
            weight.data[i,i,:,:] = torch.from_numpy(bilinear)
        return","min(f_shape[0], f_shape[1])",min(*f_shape[:2]),"iterable_zj[0], iterable_zj[1]",*f_shape[:2],*f_shape[:2],1
ImageAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ImageAI/imageai/Detection/Custom/generator.py,https://github.com/OlafenwaMoses/ImageAI/tree/master/imageai/Detection/Custom/generator.py,BatchGenerator,__init__$9,"def __init__(self, 
        instances, 
        anchors,   
        labels,        
        downsample=32, # ratio between network input's size and network output's size, 32 for YOLOv3
        max_box_per_image=30,
        batch_size=1,
        min_net_size=320,
        max_net_size=608,    
        shuffle=True, 
        jitter=True, 
        norm=None
    ):
        self.instances          = instances
        self.batch_size         = batch_size
        self.labels             = labels
        self.downsample         = downsample
        self.max_box_per_image  = max_box_per_image
        self.min_net_size       = (min_net_size//self.downsample)*self.downsample
        self.max_net_size       = (max_net_size//self.downsample)*self.downsample
        self.shuffle            = shuffle
        self.jitter             = jitter
        self.norm               = norm
        self.anchors            = [BoundBox(0, 0, anchors[2*i], anchors[2*i+1]) for i in range(len(anchors)//2)]
        self.net_h              = 416  
        self.net_w              = 416

        if shuffle: np.random.shuffle(self.instances)","BoundBox(0, 0, anchors[2 * i], anchors[2 * i + 1])","BoundBox(0, 0, *anchors[::2])","iterable_zj[2 * i], iterable_zj[2 * i + 1]",*anchors[::2],*anchors[2 * i:2 * i + 2],0
multi-agent-emergence-environments,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/multi-agent-emergence-environments/mae_envs/wrappers/food.py,https://github.com/openai/multi-agent-emergence-environments/tree/master/mae_envs/wrappers/food.py,FoodHealthWrapper,reset$49,"def reset(self):
        obs = self.env.reset()
        sim = self.unwrapped.sim

        # Reset obs/action space to match
        self.curr_n_food = self.metadata['curr_n_food']

        self.food_site_ids = np.array([sim.model.site_name2id(f'food{i}')
                                       for i in range(self.curr_n_food)])
        # Reset food healths
        self.food_healths = np.ones((self.curr_n_food, 1)) * self.max_food_health
        self.eat_per_food = np.zeros((self.curr_n_food, 1))

        # Reset food size
        self.respawn_counters = np.zeros((self.curr_n_food,))

        self.curr_reward_scale = np.random.uniform(self.reward_scale[0], self.reward_scale[1])

        return self.observation(obs)","np.random.uniform(self.reward_scale[0], self.reward_scale[1])",np.random.uniform(*self.reward_scale[:2]),"iterable_zj[0], iterable_zj[1]",*self.reward_scale[:2],*self.reward_scale[:2],1
matplotlib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/matplotlib/lib/matplotlib/tests/test_dates.py,https://github.com/matplotlib/matplotlib/tree/master/lib/matplotlib/tests/test_dates.py,,test_yearlocator_pytz$1054,"def test_yearlocator_pytz():
    pytz = pytest.importorskip(""pytz"")

    tz = pytz.timezone('America/New_York')
    x = [tz.localize(datetime.datetime(2010, 1, 1))
         + datetime.timedelta(i) for i in range(2000)]
    locator = mdates.AutoDateLocator(interval_multiples=True, tz=tz)
    locator.create_dummy_axis()
    locator.axis.set_view_interval(mdates.date2num(x[0])-1.0,
                                   mdates.date2num(x[-1])+1.0)
    t = np.array([733408.208333, 733773.208333, 734138.208333,
                  734503.208333, 734869.208333, 735234.208333, 735599.208333])
    # convert to new epoch from old...
    t = t + mdates.date2num(np.datetime64('0000-12-31'))
    np.testing.assert_allclose(t, locator())
    expected = ['2009-01-01 00:00:00-05:00',
                '2010-01-01 00:00:00-05:00', '2011-01-01 00:00:00-05:00',
                '2012-01-01 00:00:00-05:00', '2013-01-01 00:00:00-05:00',
                '2014-01-01 00:00:00-05:00', '2015-01-01 00:00:00-05:00']
    st = list(map(str, mdates.num2date(locator(), tz=tz)))
    assert st == expected
    assert np.allclose(locator.tick_values(x[0], x[1]), np.array(
        [14610.20833333, 14610.33333333, 14610.45833333, 14610.58333333,
         14610.70833333, 14610.83333333, 14610.95833333, 14611.08333333,
         14611.20833333]))
    assert np.allclose(locator.get_locator(x[1], x[0]).tick_values(x[0], x[1]),
                       np.array(
        [14610.20833333, 14610.33333333, 14610.45833333, 14610.58333333,
         14610.70833333, 14610.83333333, 14610.95833333, 14611.08333333,
         14611.20833333]))","locator.tick_values(x[0], x[1])",locator.tick_values(*x[:2]),"iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
matplotlib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/matplotlib/lib/matplotlib/tests/test_dates.py,https://github.com/matplotlib/matplotlib/tree/master/lib/matplotlib/tests/test_dates.py,,test_yearlocator_pytz$1054,"def test_yearlocator_pytz():
    pytz = pytest.importorskip(""pytz"")

    tz = pytz.timezone('America/New_York')
    x = [tz.localize(datetime.datetime(2010, 1, 1))
         + datetime.timedelta(i) for i in range(2000)]
    locator = mdates.AutoDateLocator(interval_multiples=True, tz=tz)
    locator.create_dummy_axis()
    locator.axis.set_view_interval(mdates.date2num(x[0])-1.0,
                                   mdates.date2num(x[-1])+1.0)
    t = np.array([733408.208333, 733773.208333, 734138.208333,
                  734503.208333, 734869.208333, 735234.208333, 735599.208333])
    # convert to new epoch from old...
    t = t + mdates.date2num(np.datetime64('0000-12-31'))
    np.testing.assert_allclose(t, locator())
    expected = ['2009-01-01 00:00:00-05:00',
                '2010-01-01 00:00:00-05:00', '2011-01-01 00:00:00-05:00',
                '2012-01-01 00:00:00-05:00', '2013-01-01 00:00:00-05:00',
                '2014-01-01 00:00:00-05:00', '2015-01-01 00:00:00-05:00']
    st = list(map(str, mdates.num2date(locator(), tz=tz)))
    assert st == expected
    assert np.allclose(locator.tick_values(x[0], x[1]), np.array(
        [14610.20833333, 14610.33333333, 14610.45833333, 14610.58333333,
         14610.70833333, 14610.83333333, 14610.95833333, 14611.08333333,
         14611.20833333]))
    assert np.allclose(locator.get_locator(x[1], x[0]).tick_values(x[0], x[1]),
                       np.array(
        [14610.20833333, 14610.33333333, 14610.45833333, 14610.58333333,
         14610.70833333, 14610.83333333, 14610.95833333, 14611.08333333,
         14611.20833333]))","locator.get_locator(x[1], x[0]).tick_values(x[0], x[1])","locator.get_locator(x[1], x[0]).tick_values(*x[:2])","iterable_zj[0], iterable_zj[1]",*x[:2],*x[:2],1
pyshp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyshp/shapefile.py,https://github.com/GeospatialPython/pyshp/tree/master//shapefile.py,Writer,__shpRecord$2115,"def __shpRecord(self, s):
        f = self.__getFileObj(self.shp)
        offset = f.tell()
        # Record number, Content length place holder
        self.shpNum += 1
        f.write(pack("">2i"", self.shpNum, 0))
        start = f.tell()
        # Shape Type
        if self.shapeType is None and s.shapeType != NULL:
            self.shapeType = s.shapeType
        if s.shapeType != NULL and s.shapeType != self.shapeType:
            raise Exception(""The shape's type (%s) must match the type of the shapefile (%s)."" % (s.shapeType, self.shapeType))
        f.write(pack(""<i"", s.shapeType))

        # For point just update bbox of the whole shapefile
        if s.shapeType in (1,11,21):
            self.__bbox(s)
        # All shape types capable of having a bounding box
        if s.shapeType in (3,5,8,13,15,18,23,25,28,31):
            try:
                f.write(pack(""<4d"", *self.__bbox(s)))
            except error:
                raise ShapefileException(""Failed to write bounding box for record %s. Expected floats."" % self.shpNum)
        # Shape types with parts
        if s.shapeType in (3,5,13,15,23,25,31):
            # Number of parts
            f.write(pack(""<i"", len(s.parts)))
        # Shape types with multiple points per record
        if s.shapeType in (3,5,8,13,15,18,23,25,28,31):
            # Number of points
            f.write(pack(""<i"", len(s.points)))
        # Write part indexes
        if s.shapeType in (3,5,13,15,23,25,31):
            for p in s.parts:
                f.write(pack(""<i"", p))
        # Part types for Multipatch (31)
        if s.shapeType == 31:
            for pt in s.partTypes:
                f.write(pack(""<i"", pt))
        # Write points for multiple-point records
        if s.shapeType in (3,5,8,13,15,18,23,25,28,31):
            try:
                [f.write(pack(""<2d"", *p[:2])) for p in s.points]
            except error:
                raise ShapefileException(""Failed to write points for record %s. Expected floats."" % self.shpNum)
        # Write z extremes and values
        # Note: missing z values are autoset to 0, but not sure if this is ideal.
        if s.shapeType in (13,15,18,31):
            try:
                f.write(pack(""<2d"", *self.__zbox(s)))
            except error:
                raise ShapefileException(""Failed to write elevation extremes for record %s. Expected floats."" % self.shpNum)
            try:
                if hasattr(s,""z""):
                    # if z values are stored in attribute
                    f.write(pack(""<%sd"" % len(s.z), *s.z))
                else:
                    # if z values are stored as 3rd dimension
                    [f.write(pack(""<d"", p[2] if len(p) > 2 else 0)) for p in s.points]  
            except error:
                raise ShapefileException(""Failed to write elevation values for record %s. Expected floats."" % self.shpNum)
        # Write m extremes and values
        # When reading a file, pyshp converts NODATA m values to None, so here we make sure to convert them back to NODATA
        # Note: missing m values are autoset to NODATA.
        if s.shapeType in (13,15,18,23,25,28,31):
            try:
                f.write(pack(""<2d"", *self.__mbox(s)))
            except error:
                raise ShapefileException(""Failed to write measure extremes for record %s. Expected floats"" % self.shpNum)
            try:
                if hasattr(s,""m""): 
                    # if m values are stored in attribute
                    f.write(pack(""<%sd"" % len(s.m), *[m if m is not None else NODATA for m in s.m]))
                else:
                    # if m values are stored as 3rd/4th dimension
                    # 0-index position of m value is 3 if z type (x,y,z,m), or 2 if m type (x,y,m)
                    mpos = 3 if s.shapeType in (13,15,18,31) else 2
                    [f.write(pack(""<d"", p[mpos] if len(p) > mpos and p[mpos] is not None else NODATA)) for p in s.points]
            except error:
                raise ShapefileException(""Failed to write measure values for record %s. Expected floats"" % self.shpNum)
        # Write a single point
        if s.shapeType in (1,11,21):
            try:
                f.write(pack(""<2d"", s.points[0][0], s.points[0][1]))
            except error:
                raise ShapefileException(""Failed to write point for record %s. Expected floats."" % self.shpNum)
        # Write a single Z value
        # Note: missing z values are autoset to 0, but not sure if this is ideal.
        if s.shapeType == 11:
            # update the global z box
            self.__zbox(s)
            # then write value
            if hasattr(s, ""z""):
                # if z values are stored in attribute
                try:
                    if not s.z:
                        s.z = (0,)
                    f.write(pack(""<d"", s.z[0]))
                except error:
                    raise ShapefileException(""Failed to write elevation value for record %s. Expected floats."" % self.shpNum)
            else:
                # if z values are stored as 3rd dimension
                try:
                    if len(s.points[0]) < 3:
                        s.points[0].append(0)
                    f.write(pack(""<d"", s.points[0][2]))
                except error:
                    raise ShapefileException(""Failed to write elevation value for record %s. Expected floats."" % self.shpNum)
        # Write a single M value
        # Note: missing m values are autoset to NODATA.
        if s.shapeType in (11,21):
            # update the global m box
            self.__mbox(s)
            # then write value
            if hasattr(s, ""m""):
                # if m values are stored in attribute
                try:
                    if not s.m or s.m[0] is None:
                        s.m = (NODATA,) 
                    f.write(pack(""<1d"", s.m[0]))
                except error:
                    raise ShapefileException(""Failed to write measure value for record %s. Expected floats."" % self.shpNum)
            else:
                # if m values are stored as 3rd/4th dimension
                # 0-index position of m value is 3 if z type (x,y,z,m), or 2 if m type (x,y,m)
                try:
                    mpos = 3 if s.shapeType == 11 else 2
                    if len(s.points[0]) < mpos+1:
                        s.points[0].append(NODATA)
                    elif s.points[0][mpos] is None:
                        s.points[0][mpos] = NODATA
                    f.write(pack(""<1d"", s.points[0][mpos]))
                except error:
                    raise ShapefileException(""Failed to write measure value for record %s. Expected floats."" % self.shpNum)
        # Finalize record length as 16-bit words
        finish = f.tell()
        length = (finish - start) // 2
        # start - 4 bytes is the content length field
        f.seek(start-4)
        f.write(pack("">i"", length))
        f.seek(finish)
        return offset,length","pack('<2d', s.points[0][0], s.points[0][1])","pack('<2d', *s.points[0][:2])","iterable_zj[0], iterable_zj[1]",*s.points[0][:2],*s.points[0][:2],1
petl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/petl/petl/transform/reshape.py,https://github.com/petl-developers/petl/tree/master/petl/transform/reshape.py,UnflattenView,__init__$648,"def __init__(self, *args, **kwargs):
        if len(args) == 2:
            self.input = args[0]
            self.period = args[1]
        elif len(args) == 3:
            self.input = values(args[0], args[1])
            self.period = args[2]
        else:
            assert False, 'invalid arguments'
        self.missing = kwargs.get('missing', None)","values(args[0], args[1])",values(*args[:2]),"iterable_zj[0], iterable_zj[1]",*args[:2],*args[:2],1
gyroflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gyroflow/gyro_integrator.py,https://github.com/ElvinC/gyroflow/tree/master//gyro_integrator.py,EulerIntegrator,get_interpolated_stab_transform$654,"def get_interpolated_stab_transform(self,smooth, start=0, interval=1/29.97):
        time_list, smoothed_orientation = self.get_stabilize_transform(smooth)

        time = start

        out_times = []
        slerped_rotations = []

        while time < 0:
            slerped_rotations.append(smoothed_orientation[0])
            out_times.append(time)
            time += interval

        while time_list[0] >= time:
            slerped_rotations.append(smoothed_orientation[0])
            out_times.append(time)
            time += interval


        for i in range(len(time_list)-1):
            if time_list[i] <= time < time_list[i+1]:

                # interpolate between two quaternions
                weight = (time - time_list[i])/(time_list[i+1]-time_list[i])
                slerped_rotations.append(quat.slerp(smoothed_orientation[i],smoothed_orientation[i+1],[weight]))
                out_times.append(time)

                time += interval

        return (out_times, slerped_rotations)","quat.slerp(smoothed_orientation[i], smoothed_orientation[i + 1], [weight])","quat.slerp(*smoothed_orientation[i:i + 2], [weight])","iterable_zj[i], iterable_zj[i + 1]",*smoothed_orientation[i:i+2],*smoothed_orientation[i:i + 2],1
captcha_trainer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/captcha_trainer/middleware/random_captcha.py,https://github.com/kerlomz/captcha_trainer/tree/master/middleware/random_captcha.py,RandomCaptcha,set_text$189,"def set_text(self, __image: ImageDraw, img_width, img_height):

        if img_width >= 150:
            font_size = random.choice(range(self.font_size[0], self.font_size[1]))
        else:
            font_size = random.choice(range(self.font_size[0], int((self.font_size[0] + self.font_size[1])/2)))

        font_num = random.choice(range(self.fonts_num[0], self.fonts_num[1]))
        max_width = int(img_width / font_num)
        max_height = int(img_height)
        font_type = random.choice(self.fonts_list)
        try:
            font = ImageFont.truetype(font_type, font_size)
        except OSError:
            del self.fonts_list[self.fonts_list.index(font_type)]
            raise Exception(""{} opened fail"")
        labels = []
        for idx in range(font_num):
            fw = range(int(max_width - font_size))
            if len(fw) > 0:
                x = max_width * idx + random.choice(fw)
            else:
                x = max_width * idx
            y = random.choice(range(int(max_height - font_size)))
            f = random.choice(self.sample)
            labels.append(f)
            __image.text((x, y), f, font=font,
                         fill=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))
        return labels, font_type","range(self.fonts_num[0], self.fonts_num[1])",range(*self.fonts_num[:2]),"iterable_zj[0], iterable_zj[1]",*self.fonts_num[:2],*self.fonts_num[:2],1
captcha_trainer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/captcha_trainer/middleware/random_captcha.py,https://github.com/kerlomz/captcha_trainer/tree/master/middleware/random_captcha.py,RandomCaptcha,set_text$189,"def set_text(self, __image: ImageDraw, img_width, img_height):

        if img_width >= 150:
            font_size = random.choice(range(self.font_size[0], self.font_size[1]))
        else:
            font_size = random.choice(range(self.font_size[0], int((self.font_size[0] + self.font_size[1])/2)))

        font_num = random.choice(range(self.fonts_num[0], self.fonts_num[1]))
        max_width = int(img_width / font_num)
        max_height = int(img_height)
        font_type = random.choice(self.fonts_list)
        try:
            font = ImageFont.truetype(font_type, font_size)
        except OSError:
            del self.fonts_list[self.fonts_list.index(font_type)]
            raise Exception(""{} opened fail"")
        labels = []
        for idx in range(font_num):
            fw = range(int(max_width - font_size))
            if len(fw) > 0:
                x = max_width * idx + random.choice(fw)
            else:
                x = max_width * idx
            y = random.choice(range(int(max_height - font_size)))
            f = random.choice(self.sample)
            labels.append(f)
            __image.text((x, y), f, font=font,
                         fill=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))
        return labels, font_type","range(self.font_size[0], self.font_size[1])",range(*self.font_size[:2]),"iterable_zj[0], iterable_zj[1]",*self.font_size[:2],*self.font_size[:2],1
BiSeNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BiSeNet/old/diss/evaluate.py,https://github.com/CoinCheung/BiSeNet/tree/master/old/diss/evaluate.py,MscEval,pad_tensor$44,"def pad_tensor(self, inten, size):
        N, C, H, W = inten.size()
        outten = torch.zeros(N, C, size[0], size[1]).cuda()
        outten.requires_grad = False
        margin_h, margin_w = size[0]-H, size[1]-W
        hst, hed = margin_h//2, margin_h//2+H
        wst, wed = margin_w//2, margin_w//2+W
        outten[:, :, hst:hed, wst:wed] = inten
        return outten, [hst, hed, wst, wed]","torch.zeros(N, C, size[0], size[1])","torch.zeros(N, C, *size[:2])","iterable_zj[0], iterable_zj[1]",*size[:2],*size[:2],1
person-blocker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/person-blocker/visualize.py,https://github.com/minimaxir/person-blocker/tree/master//visualize.py,,display_top_masks$225,"def display_top_masks(image, mask, class_ids, class_names, limit=4):
    """"""Display the given image and the top few class masks.""""""
    to_display = []
    titles = []
    to_display.append(image)
    titles.append(""H x W={}x{}"".format(image.shape[0], image.shape[1]))
    # Pick top prominent classes in this image
    unique_class_ids = np.unique(class_ids)
    mask_area = [np.sum(mask[:, :, np.where(class_ids == i)[0]])
                 for i in unique_class_ids]
    top_ids = [v[0] for v in sorted(zip(unique_class_ids, mask_area),
                                    key=lambda r: r[1], reverse=True) if v[1] > 0]
    # Generate images and titles
    for i in range(limit):
        class_id = top_ids[i] if i < len(top_ids) else -1
        # Pull masks of instances belonging to the same class.
        m = mask[:, :, np.where(class_ids == class_id)[0]]
        m = np.sum(m * np.arange(1, m.shape[-1] + 1), -1)
        to_display.append(m)
        titles.append(class_names[class_id] if class_id != -1 else ""-"")
    display_images(to_display, titles=titles, cols=limit + 1, cmap=""Blues_r"")","'H x W={}x{}'.format(image.shape[0], image.shape[1])",'H x W={}x{}'.format(*image.shape[:2]),"iterable_zj[0], iterable_zj[1]",*image.shape[:2],*image.shape[:2],1
audio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/audio/torchaudio/datasets/speechcommands.py,https://github.com/pytorch/audio/tree/master/torchaudio/datasets/speechcommands.py,SPEECHCOMMANDS,__getitem__$158,"def __getitem__(self, n: int) -> Tuple[Tensor, int, str, str, int]:
        """"""Load the n-th sample from the dataset.

        Args:
            n (int): The index of the sample to be loaded

        Returns:
            Tuple of the following items;

            Tensor:
                Waveform
            int:
                Sample rate
            str:
                Label
            str:
                Speaker ID
            int:
                Utterance number
        """"""
        metadata = self.get_metadata(n)
        waveform = _load_waveform(self._archive, metadata[0], metadata[1])
        return (waveform,) + metadata[1:]","_load_waveform(self._archive, metadata[0], metadata[1])","_load_waveform(self._archive, *metadata[:2])","iterable_zj[0], iterable_zj[1]",*metadata[:2],*metadata[:2],1
pennylane,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pennylane/tests/transforms/test_specs.py,https://github.com/PennyLaneAI/pennylane/tree/master/tests/transforms/test_specs.py,TestSpecsTransform,circuit$74,"def circuit(x, y, add_RY=True):
            qml.RX(x[0], wires=0)
            qml.Toffoli(wires=(0, 1, 2))
            qml.CRY(x[1], wires=(0, 1))
            qml.Rot(x[2], x[3], y, wires=2)
            if add_RY:
                qml.RY(x[4], wires=1)
            return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliX(1))","qml.Rot(x[2], x[3], y, wires=2)","qml.Rot(*x[2:4], y, wires=2)","iterable_zj[2], iterable_zj[3]",*x[2:4],*x[2:4],1
pytorch-toolbelt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch-toolbelt/pytorch_toolbelt/inference/tiles.py,https://github.com/BloodAxe/pytorch-toolbelt/tree/master/pytorch_toolbelt/inference/tiles.py,ImageSlicer,_pyramid$292,"def _pyramid(self, tile_size):
        w, _, _ = compute_pyramid_patch_weight_loss(tile_size[0], tile_size[1])
        return w","compute_pyramid_patch_weight_loss(tile_size[0], tile_size[1])",compute_pyramid_patch_weight_loss(*tile_size[:2]),"iterable_zj[0], iterable_zj[1]",*tile_size[:2],*tile_size[:2],1
anycost-gan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anycost-gan/cuda_op/op_native.py,https://github.com/mit-han-lab/anycost-gan/tree/master/cuda_op/op_native.py,,upfirdn2d$29,"def upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):
    out = upfirdn2d_native(input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1])
    return out","upfirdn2d_native(input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1])","upfirdn2d_native(input, kernel, up, up, down, down, *pad[:2], pad[0], pad[1])","iterable_zj[0], iterable_zj[1]",*pad[:2],*pad[:2],1
anycost-gan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anycost-gan/cuda_op/op_native.py,https://github.com/mit-han-lab/anycost-gan/tree/master/cuda_op/op_native.py,,upfirdn2d$29,"def upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):
    out = upfirdn2d_native(input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1])
    return out","upfirdn2d_native(input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1])","upfirdn2d_native(input, kernel, up, up, down, down, pad[0], *(pad[0:2][::-1] or pad[0:2][::-1]), pad[1])","iterable_zj[1], iterable_zj[0]",*pad[0:2][::-1] or pad[0:2][::-1],*pad[:2],0
mayavi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mayavi/mayavi/tools/modules.py,https://github.com/enthought/mayavi/tree/master/mayavi/tools/modules.py,VolumeFactory,_vmin_changed$615,"def _vmin_changed(self):
        vmin = self.vmin
        vmax = self.vmax
        range_min, range_max = self._target.current_range
        if vmin is None:
            vmin = range_min
        if vmax is None:
            vmax = range_max

        # Change the opacity function
        from tvtk.util.ctf import PiecewiseFunction, save_ctfs

        otf = PiecewiseFunction()
        if range_min < vmin:
            otf.add_point(range_min, 0.)
        if range_max > vmax:
            otf.add_point(range_max, 0.2)
        otf.add_point(vmin, 0.)
        otf.add_point(vmax, 0.2)
        self._target._otf = otf
        self._target._volume_property.set_scalar_opacity(otf)
        if self.color is None and \
           ((self.vmin is not None) or (self.vmax is not None)):
            # FIXME: We don't use 'rescale_ctfs' because it screws up the
            # nodes, this is because, the values are actually scaled between
            # the specified vmin/vmax and NOT the full range of values
            # specified in the CTF or in the volume object.
            if self.__last_vrange:
                last_min, last_max = self.__last_vrange
            else:
                last_min, last_max = range_min, range_max

            def _rescale_value(x):
                nx = (x - last_min) / (last_max - last_min)
                return vmin + nx * (vmax - vmin)

            # For some reason on older versions of VTK (< 8.1 at least),
            # The range trait is not updated correctly when the rgb points
            # are added, this causes problems so we explicitly update them.
            self._target._ctf.update_traits()
            scale_min, scale_max = self._target._ctf.range

            def _rescale_node(x):
                nx = (x - scale_min) / (scale_max - scale_min)
                return range_min + nx * (range_max - range_min)

            if hasattr(self._target._ctf, 'nodes'):
                rgb = list()
                for value in self._target._ctf.nodes:
                    r, g, b = \
                            self._target._ctf.get_color(value)
                    rgb.append((_rescale_node(value), r, g, b))
            else:
                rgb = save_ctfs(self._target.volume_property)['rgb']

            from tvtk.util.ctf import ColorTransferFunction
            ctf = ColorTransferFunction()
            try:
                ctf.range = (range_min, range_max)
            except Exception:
                # VTK versions < 5.2 don't seem to need this.
                pass
            rgb.sort()
            v = rgb[0]
            ctf.add_rgb_point(range_min, v[1], v[2], v[3])
            for v in rgb:
                ctf.add_rgb_point(_rescale_value(v[0]), v[1], v[2], v[3])
            ctf.add_rgb_point(range_max, v[1], v[2], v[3])

            self._target._ctf = ctf
            self._target._volume_property.set_color(ctf)
            self.__last_vrange = vmin, vmax

        self._target.update_ctf = True","ctf.add_rgb_point(range_min, v[1], v[2], v[3])","ctf.add_rgb_point(range_min, *v[1:4])","iterable_zj[1], iterable_zj[2], iterable_zj[3]",*v[1:4],*v[1:4],1
mayavi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mayavi/mayavi/tools/modules.py,https://github.com/enthought/mayavi/tree/master/mayavi/tools/modules.py,VolumeFactory,_vmin_changed$615,"def _vmin_changed(self):
        vmin = self.vmin
        vmax = self.vmax
        range_min, range_max = self._target.current_range
        if vmin is None:
            vmin = range_min
        if vmax is None:
            vmax = range_max

        # Change the opacity function
        from tvtk.util.ctf import PiecewiseFunction, save_ctfs

        otf = PiecewiseFunction()
        if range_min < vmin:
            otf.add_point(range_min, 0.)
        if range_max > vmax:
            otf.add_point(range_max, 0.2)
        otf.add_point(vmin, 0.)
        otf.add_point(vmax, 0.2)
        self._target._otf = otf
        self._target._volume_property.set_scalar_opacity(otf)
        if self.color is None and \
           ((self.vmin is not None) or (self.vmax is not None)):
            # FIXME: We don't use 'rescale_ctfs' because it screws up the
            # nodes, this is because, the values are actually scaled between
            # the specified vmin/vmax and NOT the full range of values
            # specified in the CTF or in the volume object.
            if self.__last_vrange:
                last_min, last_max = self.__last_vrange
            else:
                last_min, last_max = range_min, range_max

            def _rescale_value(x):
                nx = (x - last_min) / (last_max - last_min)
                return vmin + nx * (vmax - vmin)

            # For some reason on older versions of VTK (< 8.1 at least),
            # The range trait is not updated correctly when the rgb points
            # are added, this causes problems so we explicitly update them.
            self._target._ctf.update_traits()
            scale_min, scale_max = self._target._ctf.range

            def _rescale_node(x):
                nx = (x - scale_min) / (scale_max - scale_min)
                return range_min + nx * (range_max - range_min)

            if hasattr(self._target._ctf, 'nodes'):
                rgb = list()
                for value in self._target._ctf.nodes:
                    r, g, b = \
                            self._target._ctf.get_color(value)
                    rgb.append((_rescale_node(value), r, g, b))
            else:
                rgb = save_ctfs(self._target.volume_property)['rgb']

            from tvtk.util.ctf import ColorTransferFunction
            ctf = ColorTransferFunction()
            try:
                ctf.range = (range_min, range_max)
            except Exception:
                # VTK versions < 5.2 don't seem to need this.
                pass
            rgb.sort()
            v = rgb[0]
            ctf.add_rgb_point(range_min, v[1], v[2], v[3])
            for v in rgb:
                ctf.add_rgb_point(_rescale_value(v[0]), v[1], v[2], v[3])
            ctf.add_rgb_point(range_max, v[1], v[2], v[3])

            self._target._ctf = ctf
            self._target._volume_property.set_color(ctf)
            self.__last_vrange = vmin, vmax

        self._target.update_ctf = True","ctf.add_rgb_point(range_max, v[1], v[2], v[3])","ctf.add_rgb_point(range_max, *v[1:4])","iterable_zj[1], iterable_zj[2], iterable_zj[3]",*v[1:4],*v[1:4],1
mayavi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mayavi/mayavi/tools/modules.py,https://github.com/enthought/mayavi/tree/master/mayavi/tools/modules.py,VolumeFactory,_vmin_changed$615,"def _vmin_changed(self):
        vmin = self.vmin
        vmax = self.vmax
        range_min, range_max = self._target.current_range
        if vmin is None:
            vmin = range_min
        if vmax is None:
            vmax = range_max

        # Change the opacity function
        from tvtk.util.ctf import PiecewiseFunction, save_ctfs

        otf = PiecewiseFunction()
        if range_min < vmin:
            otf.add_point(range_min, 0.)
        if range_max > vmax:
            otf.add_point(range_max, 0.2)
        otf.add_point(vmin, 0.)
        otf.add_point(vmax, 0.2)
        self._target._otf = otf
        self._target._volume_property.set_scalar_opacity(otf)
        if self.color is None and \
           ((self.vmin is not None) or (self.vmax is not None)):
            # FIXME: We don't use 'rescale_ctfs' because it screws up the
            # nodes, this is because, the values are actually scaled between
            # the specified vmin/vmax and NOT the full range of values
            # specified in the CTF or in the volume object.
            if self.__last_vrange:
                last_min, last_max = self.__last_vrange
            else:
                last_min, last_max = range_min, range_max

            def _rescale_value(x):
                nx = (x - last_min) / (last_max - last_min)
                return vmin + nx * (vmax - vmin)

            # For some reason on older versions of VTK (< 8.1 at least),
            # The range trait is not updated correctly when the rgb points
            # are added, this causes problems so we explicitly update them.
            self._target._ctf.update_traits()
            scale_min, scale_max = self._target._ctf.range

            def _rescale_node(x):
                nx = (x - scale_min) / (scale_max - scale_min)
                return range_min + nx * (range_max - range_min)

            if hasattr(self._target._ctf, 'nodes'):
                rgb = list()
                for value in self._target._ctf.nodes:
                    r, g, b = \
                            self._target._ctf.get_color(value)
                    rgb.append((_rescale_node(value), r, g, b))
            else:
                rgb = save_ctfs(self._target.volume_property)['rgb']

            from tvtk.util.ctf import ColorTransferFunction
            ctf = ColorTransferFunction()
            try:
                ctf.range = (range_min, range_max)
            except Exception:
                # VTK versions < 5.2 don't seem to need this.
                pass
            rgb.sort()
            v = rgb[0]
            ctf.add_rgb_point(range_min, v[1], v[2], v[3])
            for v in rgb:
                ctf.add_rgb_point(_rescale_value(v[0]), v[1], v[2], v[3])
            ctf.add_rgb_point(range_max, v[1], v[2], v[3])

            self._target._ctf = ctf
            self._target._volume_property.set_color(ctf)
            self.__last_vrange = vmin, vmax

        self._target.update_ctf = True","ctf.add_rgb_point(_rescale_value(v[0]), v[1], v[2], v[3])","ctf.add_rgb_point(_rescale_value(v[0]), *v[1:4])","iterable_zj[1], iterable_zj[2], iterable_zj[3]",*v[1:4],*v[1:4],1
d2l-tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/d2l-tvm/d2ltvm/d2ltvm.py,https://github.com/d2l-ai/d2l-tvm/tree/master/d2ltvm/d2ltvm.py,,bench_pooling_tvm$570,"def bench_pooling_tvm(func, sizes, target):
    """"""Benchmark pooling in TVM

    func : the scheduling method
    sizes : the data size list, each of which is a (channel, input_hw, kernel_hw) triplet
    target : the TVM target, e.g. llvm or cuda
    """"""
    def workload(nrepeats):
        timer = mod.time_evaluator(mod.entry_name, ctx=ctx, number=nrepeats)
        return timer(data, out_max).mean * nrepeats
    times = []
    for size in sizes:
        sch, args = func(size)
        mod = tvm.build(sch, args, target)
        ctx = tvm.context(target, 0)
        data, _, out_max = d2ltvm.get_conv_data(size[0], size[0], size[1], size[2], 1, 1,
                                                lambda x: tvm.nd.array(x, ctx=ctx))
        times.append(d2ltvm.bench_workload(workload))
    return np.array(times)","d2ltvm.get_conv_data(size[0], size[0], size[1], size[2], 1, 1, lambda x: tvm.nd.array(x, ctx=ctx))","d2ltvm.get_conv_data(*(size[:3] or [size[0], size[0], size[1], size[2]]), 1, 1, lambda x: tvm.nd.array(x, ctx=ctx))","iterable_zj[0], iterable_zj[0], iterable_zj[1], iterable_zj[2]","*size[:3] or [size[0], size[0], size[1], size[2]]",*size[:3],0
dask-ml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dask-ml/tests/model_selection/test_split.py,https://github.com/dask/dask-ml/tree/master/tests/model_selection/test_split.py,,test_blockwise_shufflesplit_rng$54,"def test_blockwise_shufflesplit_rng():
    # Regression test for issue #380
    n_splits = 2
    splitter = dask_ml.model_selection.ShuffleSplit(n_splits=n_splits, random_state=0)
    gen = splitter.split(dX)

    train_indices = []
    test_indices = []
    for train_idx, test_idx in gen:
        train_indices.append(train_idx)
        test_indices.append(test_idx)

    assert not np.array_equal(train_indices[0], train_indices[1])
    assert not np.array_equal(test_indices[0], test_indices[1])

    # Test that splitting is reproducible
    n_splits = 2
    split1 = dask_ml.model_selection.ShuffleSplit(n_splits=n_splits, random_state=0)
    split2 = dask_ml.model_selection.ShuffleSplit(n_splits=n_splits, random_state=0)

    for (train_1, test_1), (train_2, test_2) in zip(split1.split(dX), split2.split(dX)):
        da.utils.assert_eq(train_1, train_2)
        da.utils.assert_eq(test_1, test_2)","np.array_equal(train_indices[0], train_indices[1])",np.array_equal(*train_indices[:2]),"iterable_zj[0], iterable_zj[1]",*train_indices[:2],*train_indices[:2],1
dask-ml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dask-ml/tests/model_selection/test_split.py,https://github.com/dask/dask-ml/tree/master/tests/model_selection/test_split.py,,test_blockwise_shufflesplit_rng$54,"def test_blockwise_shufflesplit_rng():
    # Regression test for issue #380
    n_splits = 2
    splitter = dask_ml.model_selection.ShuffleSplit(n_splits=n_splits, random_state=0)
    gen = splitter.split(dX)

    train_indices = []
    test_indices = []
    for train_idx, test_idx in gen:
        train_indices.append(train_idx)
        test_indices.append(test_idx)

    assert not np.array_equal(train_indices[0], train_indices[1])
    assert not np.array_equal(test_indices[0], test_indices[1])

    # Test that splitting is reproducible
    n_splits = 2
    split1 = dask_ml.model_selection.ShuffleSplit(n_splits=n_splits, random_state=0)
    split2 = dask_ml.model_selection.ShuffleSplit(n_splits=n_splits, random_state=0)

    for (train_1, test_1), (train_2, test_2) in zip(split1.split(dX), split2.split(dX)):
        da.utils.assert_eq(train_1, train_2)
        da.utils.assert_eq(test_1, test_2)","np.array_equal(test_indices[0], test_indices[1])",np.array_equal(*test_indices[:2]),"iterable_zj[0], iterable_zj[1]",*test_indices[:2],*test_indices[:2],1
imgaug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgaug/test/augmenters/test_blur.py,https://github.com/aleju/imgaug/tree/master/test/augmenters/test_blur.py,TestMotionBlur,test_simple_parameters_angle_is_list$1588,"def test_simple_parameters_angle_is_list(self):
        # random angle
        aug = iaa.MotionBlur(k=3, angle=[0, 90], direction=0.0)
        matrix_func = aug.matrix
        matrices = [
            matrix_func(
                np.zeros((128, 128, 3), dtype=np.uint8),
                3,
                iarandom.RNG(i)
            ) for i in range(50)
        ]
        expected1 = np.float32([
            [0, 1.0/3, 0],
            [0, 1.0/3, 0],
            [0, 1.0/3, 0]
        ])
        expected2 = np.float32([
            [0, 0, 0],
            [1.0/3, 1.0/3, 1.0/3],
            [0, 0, 0],
        ])
        nb_seen = [0, 0]
        for matrices_image in matrices:
            assert np.allclose(matrices_image[0], matrices_image[1])
            assert np.allclose(matrices_image[1], matrices_image[2])
            for matrix_channel in matrices_image:
                if np.allclose(matrix_channel, expected1):
                    nb_seen[0] += 1
                elif np.allclose(matrix_channel, expected2):
                    nb_seen[1] += 1
        assert nb_seen[0] > 0
        assert nb_seen[1] > 0","np.allclose(matrices_image[0], matrices_image[1])",np.allclose(*matrices_image[:2]),"iterable_zj[0], iterable_zj[1]",*matrices_image[:2],*matrices_image[:2],1
imgaug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgaug/test/augmenters/test_blur.py,https://github.com/aleju/imgaug/tree/master/test/augmenters/test_blur.py,TestMotionBlur,test_simple_parameters_angle_is_list$1588,"def test_simple_parameters_angle_is_list(self):
        # random angle
        aug = iaa.MotionBlur(k=3, angle=[0, 90], direction=0.0)
        matrix_func = aug.matrix
        matrices = [
            matrix_func(
                np.zeros((128, 128, 3), dtype=np.uint8),
                3,
                iarandom.RNG(i)
            ) for i in range(50)
        ]
        expected1 = np.float32([
            [0, 1.0/3, 0],
            [0, 1.0/3, 0],
            [0, 1.0/3, 0]
        ])
        expected2 = np.float32([
            [0, 0, 0],
            [1.0/3, 1.0/3, 1.0/3],
            [0, 0, 0],
        ])
        nb_seen = [0, 0]
        for matrices_image in matrices:
            assert np.allclose(matrices_image[0], matrices_image[1])
            assert np.allclose(matrices_image[1], matrices_image[2])
            for matrix_channel in matrices_image:
                if np.allclose(matrix_channel, expected1):
                    nb_seen[0] += 1
                elif np.allclose(matrix_channel, expected2):
                    nb_seen[1] += 1
        assert nb_seen[0] > 0
        assert nb_seen[1] > 0","np.allclose(matrices_image[1], matrices_image[2])",np.allclose(*matrices_image[1:3]),"iterable_zj[1], iterable_zj[2]",*matrices_image[1:3],*matrices_image[1:3],1
quay,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quay/data/buildlogs.py,https://github.com/quay/quay/tree/master/data/buildlogs.py,BuildLogs,init_app$149,"def init_app(self, app):
        buildlogs_config = app.config.get(""BUILDLOGS_REDIS"")
        if not buildlogs_config:
            # This is the old key name.
            buildlogs_config = {""host"": app.config.get(""BUILDLOGS_REDIS_HOSTNAME"")}

        buildlogs_options = app.config.get(""BUILDLOGS_OPTIONS"", [])
        buildlogs_import = app.config.get(""BUILDLOGS_MODULE_AND_CLASS"", None)

        if buildlogs_import is None:
            klass = RedisBuildLogs
        else:
            klass = import_class(buildlogs_import[0], buildlogs_import[1])

        buildlogs = klass(buildlogs_config, *buildlogs_options)

        # register extension with app
        app.extensions = getattr(app, ""extensions"", {})
        app.extensions[""buildlogs""] = buildlogs
        return buildlogs","import_class(buildlogs_import[0], buildlogs_import[1])",import_class(*buildlogs_import[:2]),"iterable_zj[0], iterable_zj[1]",*buildlogs_import[:2],*buildlogs_import[:2],1
PaddleX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex/cv/transforms/visualize.py,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex/cv/transforms/visualize.py,,det_compose$103,"def det_compose(im,
                im_info=None,
                label_info=None,
                transforms=None,
                vdl_writer=None,
                step=0,
                labels=[],
                catid2color=None):
    def decode_image(im_file, im_info, label_info):
        if im_info is None:
            im_info = dict()
        if isinstance(im_file, np.ndarray):
            if len(im_file.shape) != 3:
                raise Exception(
                    ""im should be 3-dimensions, but now is {}-dimensions"".
                    format(len(im_file.shape)))
            im = im_file
        else:
            try:
                im = cv2.imread(im_file).astype('float32')
            except:
                raise TypeError('Can\'t read The image file {}!'.format(
                    im_file))
        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
        # make default im_info with [h, w, 1]
        im_info['im_resize_info'] = np.array(
            [im.shape[0], im.shape[1], 1.], dtype=np.float32)
        im_info['image_shape'] = np.array([im.shape[0],
                                           im.shape[1]]).astype('int32')
        use_mixup = False
        for t in transforms:
            if type(t).__name__ == 'MixupImage':
                use_mixup = True
            if not use_mixup:
                if 'mixup' in im_info:
                    del im_info['mixup']
        # decode mixup image
        if 'mixup' in im_info:
            im_info['mixup'] = \
              decode_image(im_info['mixup'][0],
                           im_info['mixup'][1],
                           im_info['mixup'][2])
        if label_info is None:
            return (im, im_info)
        else:
            return (im, im_info, label_info)

    outputs = decode_image(im, im_info, label_info)
    im = outputs[0]
    im_info = outputs[1]
    if len(outputs) == 3:
        label_info = outputs[2]
    if vdl_writer is not None:
        vdl_writer.add_image(
            tag='0. OriginalImage/' + str(step), img=im, step=0)
    op_id = 1
    bboxes = label_info['gt_bbox']
    transforms = [None] + transforms
    for op in transforms:
        if im is None:
            return None
        if isinstance(op, DetTransform) or op is None:
            if vdl_writer is not None and hasattr(op, 'prob'):
                op.prob = 1.0
            if op is not None:
                outputs = op(im, im_info, label_info)
            else:
                outputs = (im, im_info, label_info)
            im = outputs[0]
            vdl_im = im
            if vdl_writer is not None:
                if isinstance(op,
                              pdx.cv.transforms.det_transforms.ResizeByShort):
                    scale = outputs[1]['im_resize_info'][2]
                    bboxes = bboxes * scale
                elif isinstance(op, pdx.cv.transforms.det_transforms.Resize):
                    h = outputs[1]['image_shape'][0]
                    w = outputs[1]['image_shape'][1]
                    target_size = op.target_size
                    if isinstance(target_size, int):
                        h_scale = float(target_size) / h
                        w_scale = float(target_size) / w
                    else:
                        h_scale = float(target_size[0]) / h
                        w_scale = float(target_size[1]) / w
                    bboxes[:, 0] = bboxes[:, 0] * w_scale
                    bboxes[:, 1] = bboxes[:, 1] * h_scale
                    bboxes[:, 2] = bboxes[:, 2] * w_scale
                    bboxes[:, 3] = bboxes[:, 3] * h_scale
                else:
                    bboxes = outputs[2]['gt_bbox']
                if not isinstance(op, (
                        pdx.cv.transforms.det_transforms.RandomHorizontalFlip,
                        pdx.cv.transforms.det_transforms.Padding)):
                    for i in range(bboxes.shape[0]):
                        bbox = bboxes[i]
                        cname = labels[outputs[2]['gt_class'][i][0] - 1]
                        vdl_im = _draw_rectangle_and_cname(
                            vdl_im,
                            int(bbox[0]),
                            int(bbox[1]),
                            int(bbox[2]),
                            int(bbox[3]), cname,
                            catid2color[outputs[2]['gt_class'][i][0] - 1])
                if isinstance(op, pdx.cv.transforms.det_transforms.Normalize):
                    continue
        else:
            im = execute_imgaug(op, im)
            if label_info is not None:
                outputs = (im, im_info, label_info)
            else:
                outputs = (im, im_info)
            vdl_im = im
        if vdl_writer is not None:
            tag = str(op_id) + '. ' + op.__class__.__name__ + '/' + str(step)
            if op is None:
                tag = str(op_id) + '. OriginalImageWithGTBox/' + str(step)
            vdl_writer.add_image(tag=tag, img=vdl_im, step=0)
        op_id += 1","decode_image(im_info['mixup'][0], im_info['mixup'][1], im_info['mixup'][2])",decode_image(*im_info['mixup'][:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*im_info['mixup'][:3],*im_info['mixup'][:3],1
DetectoRS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DetectoRS/mmdet/ops/dcn/deform_pool.py,https://github.com/joe-siyuan-qiao/DetectoRS/tree/master/mmdet/ops/dcn/deform_pool.py,DeformRoIPoolingPack,forward$149,"def forward(self, data, rois):
        assert data.size(1) == self.out_channels
        n = rois.shape[0]
        if n == 0:
            return data.new_empty(n, self.out_channels, self.out_size[0],
                                  self.out_size[1])
        if self.no_trans:
            offset = data.new_empty(0)
            return deform_roi_pooling(data, rois, offset, self.spatial_scale,
                                      self.out_size, self.out_channels,
                                      self.no_trans, self.group_size,
                                      self.part_size, self.sample_per_part,
                                      self.trans_std)
        else:
            offset = data.new_empty(0)
            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,
                                   self.out_size, self.out_channels, True,
                                   self.group_size, self.part_size,
                                   self.sample_per_part, self.trans_std)
            offset = self.offset_fc(x.view(n, -1))
            offset = offset.view(n, 2, self.out_size[0], self.out_size[1])
            return deform_roi_pooling(data, rois, offset, self.spatial_scale,
                                      self.out_size, self.out_channels,
                                      self.no_trans, self.group_size,
                                      self.part_size, self.sample_per_part,
                                      self.trans_std)","data.new_empty(n, self.out_channels, self.out_size[0], self.out_size[1])","data.new_empty(n, self.out_channels, *self.out_size[:2])","iterable_zj[0], iterable_zj[1]",*self.out_size[:2],*self.out_size[:2],1
DetectoRS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DetectoRS/mmdet/ops/dcn/deform_pool.py,https://github.com/joe-siyuan-qiao/DetectoRS/tree/master/mmdet/ops/dcn/deform_pool.py,DeformRoIPoolingPack,forward$149,"def forward(self, data, rois):
        assert data.size(1) == self.out_channels
        n = rois.shape[0]
        if n == 0:
            return data.new_empty(n, self.out_channels, self.out_size[0],
                                  self.out_size[1])
        if self.no_trans:
            offset = data.new_empty(0)
            return deform_roi_pooling(data, rois, offset, self.spatial_scale,
                                      self.out_size, self.out_channels,
                                      self.no_trans, self.group_size,
                                      self.part_size, self.sample_per_part,
                                      self.trans_std)
        else:
            offset = data.new_empty(0)
            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,
                                   self.out_size, self.out_channels, True,
                                   self.group_size, self.part_size,
                                   self.sample_per_part, self.trans_std)
            offset = self.offset_fc(x.view(n, -1))
            offset = offset.view(n, 2, self.out_size[0], self.out_size[1])
            return deform_roi_pooling(data, rois, offset, self.spatial_scale,
                                      self.out_size, self.out_channels,
                                      self.no_trans, self.group_size,
                                      self.part_size, self.sample_per_part,
                                      self.trans_std)","offset.view(n, 2, self.out_size[0], self.out_size[1])","offset.view(n, 2, *self.out_size[:2])","iterable_zj[0], iterable_zj[1]",*self.out_size[:2],*self.out_size[:2],1
beancount,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/beancount/beancount/query/query_parser.py,https://github.com/beancount/beancount/tree/master/beancount/query/query_parser.py,SelectParser,p_expression_match$542,"def p_expression_match(self, p):
        ""expression : expression TILDE expression""
        p[0] = Match(p[1], p[3])","Match(p[1], p[3])",Match(*p[1:4:2]),"iterable_zj[1], iterable_zj[3]",*p[1:4:2],*p[1:5:2],0
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2019/clacks/solution2/mirror_scenes.py,https://github.com/3b1b/videos/tree/master/_2019/clacks/solution2/mirror_scenes.py,MirrorAndWiresOverlay,add_wires$813,"def add_wires(self):
        ul_corner = TOP + LEFT_SIDE
        points = self.wire_points = [
            ul_corner + np.array([
                (x / self.max_x_pixel) * FRAME_HEIGHT,
                (-y / self.max_y_pixel) * FRAME_HEIGHT,
                0
            ])
            for x, y in self.wire_pixel_points
        ]
        wires = self.wires = VGroup(
            Line(points[0], points[1]),
            Line(points[1], points[2]),
            Line(points[1], points[3]),
            Line(points[1], points[4]),
        )
        wires.set_stroke(RED, 4)
        self.dl_wire, self.dr_wire, self.ul_wire, self.ur_wire = wires

        self.trajectory = VMobject()
        self.trajectory.set_points_as_corners(points[:3])
        self.ghost_trajectory = VMobject()
        self.ghost_trajectory.set_points_as_corners([*points[:2], points[4]])
        VGroup(self.trajectory, self.ghost_trajectory).match_style(
            self.wires
        )","Line(points[0], points[1])",Line(*points[:2]),"iterable_zj[0], iterable_zj[1]",*points[:2],*points[:2],1
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2019/clacks/solution2/mirror_scenes.py,https://github.com/3b1b/videos/tree/master/_2019/clacks/solution2/mirror_scenes.py,MirrorAndWiresOverlay,add_wires$813,"def add_wires(self):
        ul_corner = TOP + LEFT_SIDE
        points = self.wire_points = [
            ul_corner + np.array([
                (x / self.max_x_pixel) * FRAME_HEIGHT,
                (-y / self.max_y_pixel) * FRAME_HEIGHT,
                0
            ])
            for x, y in self.wire_pixel_points
        ]
        wires = self.wires = VGroup(
            Line(points[0], points[1]),
            Line(points[1], points[2]),
            Line(points[1], points[3]),
            Line(points[1], points[4]),
        )
        wires.set_stroke(RED, 4)
        self.dl_wire, self.dr_wire, self.ul_wire, self.ur_wire = wires

        self.trajectory = VMobject()
        self.trajectory.set_points_as_corners(points[:3])
        self.ghost_trajectory = VMobject()
        self.ghost_trajectory.set_points_as_corners([*points[:2], points[4]])
        VGroup(self.trajectory, self.ghost_trajectory).match_style(
            self.wires
        )","Line(points[1], points[2])",Line(*points[1:3]),"iterable_zj[1], iterable_zj[2]",*points[1:3],*points[1:3],1
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2019/clacks/solution2/mirror_scenes.py,https://github.com/3b1b/videos/tree/master/_2019/clacks/solution2/mirror_scenes.py,MirrorAndWiresOverlay,add_wires$813,"def add_wires(self):
        ul_corner = TOP + LEFT_SIDE
        points = self.wire_points = [
            ul_corner + np.array([
                (x / self.max_x_pixel) * FRAME_HEIGHT,
                (-y / self.max_y_pixel) * FRAME_HEIGHT,
                0
            ])
            for x, y in self.wire_pixel_points
        ]
        wires = self.wires = VGroup(
            Line(points[0], points[1]),
            Line(points[1], points[2]),
            Line(points[1], points[3]),
            Line(points[1], points[4]),
        )
        wires.set_stroke(RED, 4)
        self.dl_wire, self.dr_wire, self.ul_wire, self.ur_wire = wires

        self.trajectory = VMobject()
        self.trajectory.set_points_as_corners(points[:3])
        self.ghost_trajectory = VMobject()
        self.ghost_trajectory.set_points_as_corners([*points[:2], points[4]])
        VGroup(self.trajectory, self.ghost_trajectory).match_style(
            self.wires
        )","Line(points[1], points[3])",Line(*points[1:4:2]),"iterable_zj[1], iterable_zj[3]",*points[1:4:2],*points[1:5:2],0
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2019/clacks/solution2/mirror_scenes.py,https://github.com/3b1b/videos/tree/master/_2019/clacks/solution2/mirror_scenes.py,MirrorAndWiresOverlay,add_wires$813,"def add_wires(self):
        ul_corner = TOP + LEFT_SIDE
        points = self.wire_points = [
            ul_corner + np.array([
                (x / self.max_x_pixel) * FRAME_HEIGHT,
                (-y / self.max_y_pixel) * FRAME_HEIGHT,
                0
            ])
            for x, y in self.wire_pixel_points
        ]
        wires = self.wires = VGroup(
            Line(points[0], points[1]),
            Line(points[1], points[2]),
            Line(points[1], points[3]),
            Line(points[1], points[4]),
        )
        wires.set_stroke(RED, 4)
        self.dl_wire, self.dr_wire, self.ul_wire, self.ur_wire = wires

        self.trajectory = VMobject()
        self.trajectory.set_points_as_corners(points[:3])
        self.ghost_trajectory = VMobject()
        self.ghost_trajectory.set_points_as_corners([*points[:2], points[4]])
        VGroup(self.trajectory, self.ghost_trajectory).match_style(
            self.wires
        )","Line(points[1], points[4])",Line(*points[1:5:3]),"iterable_zj[1], iterable_zj[4]",*points[1:5:3],*points[1:7:3],0
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/interpolate/_interpolate.py,https://github.com/scipy/scipy/tree/master/scipy/interpolate/_interpolate.py,PPoly,_evaluate$1019,"def _evaluate(self, x, nu, extrapolate, out):
        _ppoly.evaluate(self.c.reshape(self.c.shape[0], self.c.shape[1], -1),
                        self.x, x, nu, bool(extrapolate), out)","self.c.reshape(self.c.shape[0], self.c.shape[1], -1)","self.c.reshape(*self.c.shape[:2], -1)","iterable_zj[0], iterable_zj[1]",*self.c.shape[:2],*self.c.shape[:2],1
kamene,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,,__make_table$1030,"def __make_table(yfmtfunc, fmtfunc, endline, items, fxyz, sortx=None, sorty=None, seplinefunc=None):
    vx = {}
    vy = {}
    vz = {}
    vxf = {}
    vyf = {}
    max_length = 0
    for record in items:
        xx,yy,zz = map(str, fxyz(record[0], record[1]))
        max_length = max(len(yy),max_length)
        vx[xx] = max(vx.get(xx,0), len(xx), len(zz))
        vy[yy] = None
        vz[(xx,yy)] = zz

    vxk = list(vx.keys())
    vyk = list(vy.keys())
    if sortx:
        vxk.sort(sortx)
    else:
        try:
            vxk.sort(key = lambda x: atol(x))
        except:
            vxk.sort()
    if sorty:
        vyk.sort(sorty)
    else:
        try:
            vyk.sort(key = lambda x: atol(x))
        except:
            vyk.sort()


    if seplinefunc:
        sepline = seplinefunc(max_length, [vx[x] for x in vxk])
        print(sepline)

    fmt = yfmtfunc(max_length)
    print(fmt % """", end = "" "")
    for x in vxk:
        vxf[x] = fmtfunc(vx[x])
        print(vxf[x] % x, end = "" "")
    print(endline)
    if seplinefunc:
        print(sepline)
    for y in vyk:
        print(fmt % y, end = "" "")
        for x in vxk:
            print(vxf[x] % vz.get((x,y), ""-""), end = "" "")
        print(endline)
    if seplinefunc:
        print(sepline)","fxyz(record[0], record[1])",fxyz(*record[:2]),"iterable_zj[0], iterable_zj[1]",*record[:2],*record[:2],1
trezor-firmware,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/trezor-firmware/core/src/trezor/loop.py,https://github.com/trezor/trezor-firmware/tree/master/core/src/trezor/loop.py,,run$111,"def run() -> None:
    """"""
    Loop forever, stepping through scheduled tasks and awaiting I/O events
    in between.  Use `schedule` first to add a coroutine to the task queue.
    Tasks yield back to the scheduler on any I/O, usually by calling `await` on
    a `Syscall`.
    """"""
    task_entry = [0, 0, 0]  # deadline, task, value
    msg_entry = [0, 0]  # iface | flags, value
    while _queue or _paused:
        if __debug__:
            # process synthetic events
            if synthetic_events:
                iface, event = synthetic_events[0]
                msg_tasks = _paused.pop(iface, ())
                if msg_tasks:
                    synthetic_events.pop(0)
                    for task in msg_tasks:
                        _step(task, event)

                    # XXX: we assume that synthetic events are rare. If there is a lot of them,
                    # this degrades to ""while synthetic_events"" and would ignore all real ones.
                    continue

        # compute the maximum amount of time we can wait for a message
        if _queue:
            delay = utime.ticks_diff(_queue.peektime(), utime.ticks_ms())
        else:
            delay = 1000  # wait for 1 sec maximum if queue is empty

        if io.poll(_paused, msg_entry, delay):
            # message received, run tasks paused on the interface
            msg_tasks = _paused.pop(msg_entry[0], ())
            for task in msg_tasks:
                _step(task, msg_entry[1])
        else:
            # timeout occurred, run the first scheduled task
            if _queue:
                _queue.pop(task_entry)
                _step(task_entry[1], task_entry[2])","_step(task_entry[1], task_entry[2])",_step(*task_entry[1:3]),"iterable_zj[1], iterable_zj[2]",*task_entry[1:3],*task_entry[1:3],1
pennylane,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pennylane/tests/devices/test_default_qubit_torch.py,https://github.com/PennyLaneAI/pennylane/tree/master/tests/devices/test_default_qubit_torch.py,TestPassthruIntegration,circuit$2207,"def circuit(x, weights, w):
            """"""In this example, a mixture of scalar
            arguments, array arguments, and keyword arguments are used.""""""
            qml.QubitStateVector(input_state, wires=w)
            operation(x, weights[0], weights[1], wires=w)
            return qml.expval(qml.PauliX(w))","operation(x, weights[0], weights[1], wires=w)","operation(x, *weights[:2], wires=w)","iterable_zj[0], iterable_zj[1]",*weights[:2],*weights[:2],1
freeipa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/plugins/trust.py,https://github.com/freeipa/freeipa/tree/master/ipaserver/plugins/trust.py,trustdomain_disable,execute$1952,"def execute(self, *keys, **options):
        ldap = self.api.Backend.ldap2
        verify_samba_component_presence(ldap, self.api)

        if keys[0].lower() == keys[1].lower():
            raise errors.ValidationError(
                name='domain',
                error=_(""cannot disable root domain of the trust, ""
                        ""use trust-del to delete the trust itself"")
            )
        try:
            trust_dn = self.obj.get_dn(keys[0], trust_type=u'ad')
            trust_entry = ldap.get_entry(trust_dn)
        except errors.NotFound:
            raise self.api.Object[self.obj.parent_object].handle_not_found(
                keys[0])

        dn = self.obj.get_dn(keys[0], keys[1], trust_type=u'ad')
        try:
            entry = ldap.get_entry(dn)
            sid = entry.single_value.get('ipanttrusteddomainsid', None)
            if sid not in trust_entry['ipantsidblacklistincoming']:
                trust_entry['ipantsidblacklistincoming'].append(sid)
                ldap.update_entry(trust_entry)
            else:
                raise errors.AlreadyInactive()
        except errors.NotFound:
            raise self.obj.handle_not_found(*keys)

        return dict(
            result=True,
            value=pkey_to_value(keys[1], options),
        )","self.obj.get_dn(keys[0], keys[1], trust_type=u'ad')","self.obj.get_dn(*keys[:2], trust_type=u'ad')","iterable_zj[0], iterable_zj[1]",*keys[:2],*keys[:2],1
cve-search,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cve-search/lib/content_handlers.py,https://github.com/cve-search/cve-search/tree/master/lib/content_handlers.py,CapecHandler,endElement$195,"def endElement(self, name):
        if name == ""Description"" and not self.Attack_step_tag:
            self.Summary.append(self.Summary_ch.rstrip())
            if self.Summary_ch != """":
                self.Summary_ch = """"
            self.Description_tag = False

        if name == ""Entry_ID"":
            self.entry_id = False

        if name == ""Entry_Name"":
            self.entry_name = False

            entry_id = self.entry_id_ch.rstrip()

            cut_entry = entry_id.split(""."")

            url = """"

            if self.taxonomy_name == ""ATTACK"":
                if len(cut_entry) == 1:
                    # no subtechnique use plain entry_id
                    url = ""https://attack.mitre.org/techniques/T{}"".format(entry_id)
                else:
                    # attack with subtechniques use cut_entry
                    url = ""https://attack.mitre.org/techniques/T{}/{}"".format(
                        cut_entry[0], cut_entry[1]
                    )

            elif self.taxonomy_name == ""WASC"":

                if ""/"" in self.entry_name_ch:
                    url = ""http://projects.webappsec.org/{}"".format(
                        self.entry_name_ch.replace(""/"", "" and "").replace("" "", ""-"")
                    )
                else:
                    url = ""http://projects.webappsec.org/{}"".format(
                        self.entry_name_ch.replace("" "", ""-"")
                    )

            elif self.taxonomy_name == ""OWASP Attacks"":
                entry_id = ""Link""

                url = ""https://owasp.org/www-community/attacks/{}"".format(
                    self.entry_name_ch.replace("" "", ""_"")
                )

            self.taxonomy_mapping[self.taxonomy_name][
                self.entry_id_ch.rstrip().replace(""."", ""_"")
            ] = {
                ""Entry_ID"": entry_id,
                ""Entry_Name"": self.entry_name_ch.rstrip(),
                ""URL"": url,
            }

            if self.entry_id_ch != """":
                self.entry_id_ch = """"

            if self.entry_name_ch != """":
                self.entry_name_ch = """"

        if name == ""Taxonomy_Mappings"":
            self.Taxonomy_Mappings = False

        if name == ""Taxonomy_Mapping"":
            self.Taxonomy_Mapping = False

        if name == ""Step"":
            self.step_name = self.Step_ch.rstrip()
            self.Step = False

        if name == ""Phase"":
            self.Phase = False

        if name == ""Description"" and self.Attack_step_tag:
            self.Attack_Description = False

            self.execution_flow[self.step_name] = {
                ""Phase"": self.Phase_ch.rstrip(),
                ""Description"": self.Attack_Description_ch.rstrip(),
                ""Techniques"": [],
            }

            if self.Step_ch != """":
                self.Step_ch = """"

            if self.Phase_ch != """":
                self.Phase_ch = """"

            if self.Attack_Description_ch != """":
                self.Attack_Description_ch = """"

        if name == ""Technique"" and self.Attack_step_tag:
            if self.Technique_ch != """":
                self.execution_flow[self.step_name][""Techniques""].append(
                    self.Technique_ch.rstrip()
                )
                self.Technique_ch = """"
            self.Technique = False

        if name == ""Attack_Step"":
            self.Attack_step_tag = False

        if name == ""Execution_Flow"":
            self.Execution_Flow = False

        if name == ""Prerequisite"":
            if self.Prerequisite_ch != """":
                self.Prerequisite.append(self.Prerequisite_ch.rstrip())
            self.Prerequisite_tag = False
        if name == ""Mitigation"":
            if self.Mitigation_ch != """":
                self.Solution_or_Mitigation.append(self.Mitigation_ch.rstrip())
                self.Mitigation_ch = """"
            self.Mitigation_tag = False

        if name == ""Prerequisites"":
            self.Prerequisites_tag = False
        if name == ""Mitigations"":
            self.Mitigations_tag = False
        if name == ""Related_Weaknesses"":
            self.Related_Weaknesses_tag = False

        if name == ""Related_Attack_Patterns"":
            self.Related_Attack_Patterns = False

        if name == ""Likelihood_Of_Attack"":
            self.Likelihood_Of_Attack = False
            self.loa = self.Likelihood_Of_Attack_ch.rstrip()
            self.Likelihood_Of_Attack_ch = """"

        if name == ""Typical_Severity"":
            self.Typical_Severity = False
            self.ts = self.Typical_Severity_ch.rstrip()
            self.Typical_Severity_ch = """"

        if name == ""Attack_Pattern"":
            if not self.name.startswith(""DEPRECATED""):
                self.capec.append(
                    {
                        ""name"": self.name,
                        ""id"": self.id,
                        ""summary"": ""\n"".join(self.Summary),
                        ""prerequisites"": "" "".join(self.Prerequisite),
                        ""solutions"": "" "".join(self.Solution_or_Mitigation),
                        ""related_capecs"": sorted(self.Related_AttackPatterns),
                        ""related_weakness"": sorted(self.Related_Weakness),
                        ""taxonomy"": dict(self.taxonomy_mapping),
                        ""execution_flow"": dict(self.execution_flow),
                        ""loa"": self.loa,
                        ""typical_severity"": self.ts,
                    }
                )
            self.Summary = []
            self.Prerequisite = []
            self.Solution_or_Mitigation = []
            self.Related_Weakness = []
            self.Related_AttackPatterns = []
            self.techniques = []

            self.taxonomy_mapping = defaultdict(dict)

            self.execution_flow = defaultdict(dict)

            self.Attack_Pattern_tag = False
        if name == ""Attack_Patterns"":
            self.Attack_Patterns_tag = False
        if name == ""Attack_Pattern_Catalog"":
            self.Attack_Pattern_Catalog_tag = False","'https://attack.mitre.org/techniques/T{}/{}'.format(cut_entry[0], cut_entry[1])",'https://attack.mitre.org/techniques/T{}/{}'.format(*cut_entry[:2]),"iterable_zj[0], iterable_zj[1]",*cut_entry[:2],*cut_entry[:2],1
bumblebee-status,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bumblebee-status/bumblebee_status/modules/core/load.py,https://github.com/tobi-wan-kenobi/bumblebee-status/tree/master/bumblebee_status/modules/core/load.py,Module,load$35,"def load(self, widget):
        return ""{:.02f}/{:.02f}/{:.02f}"".format(
            self._load[0], self._load[1], self._load[2]
        )","'{:.02f}/{:.02f}/{:.02f}'.format(self._load[0], self._load[1], self._load[2])",'{:.02f}/{:.02f}/{:.02f}'.format(*self._load[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*self._load[:3],*self._load[:3],1
holoviews,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/holoviews/holoviews/core/ndmapping.py,https://github.com/holoviz/holoviews/tree/master/holoviews/core/ndmapping.py,MultiDimensionalMapping,__init__$99,"def __init__(self, initial_items=None, kdims=None, **params):
        if isinstance(initial_items, MultiDimensionalMapping):
            params = dict(util.get_param_values(initial_items), **dict(params))
        if kdims is not None:
            params['kdims'] = kdims
        super().__init__(OrderedDict(), **dict(params))
        if type(initial_items) is dict and not self.sort:
            raise ValueError('If sort=False the data must define a fixed '
                             'ordering, please supply a list of items or '
                             'an OrderedDict, not a regular dictionary.')

        self._next_ind = 0
        self._check_key_type = True

        if initial_items is None: initial_items = []
        if isinstance(initial_items, tuple):
            self._add_item(initial_items[0], initial_items[1])
        elif not self._check_items:
            if isinstance(initial_items, dict):
                initial_items = initial_items.items()
            elif isinstance(initial_items, MultiDimensionalMapping):
                initial_items = initial_items.data.items()
            self.data = OrderedDict((k if isinstance(k, tuple) else (k,), v)
                                    for k, v in initial_items)
            if self.sort:
                self._resort()
        elif initial_items is not None:
            self.update(OrderedDict(initial_items))","self._add_item(initial_items[0], initial_items[1])",self._add_item(*initial_items[:2]),"iterable_zj[0], iterable_zj[1]",*initial_items[:2],*initial_items[:2],1
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,findSEH$6013,"def findSEH(modulecriteria={},criteria={}):
	""""""
	Performs a search for pointers to gain code execution in a SEH overwrite exploit

	Arguments:
	modulecriteria - dictionary with criteria modules need to comply with.
	                 Default settings are : ignore aslr, rebase and safeseh protected modules
	criteria - dictionary with criteria the pointers need to comply with.

	Return:
	Dictionary (pointers)
	""""""
	type = """"
	if ""rop"" in criteria:
		type = ""rop""
	search = getSearchSequences(""seh"",0,type) 
	
	found_opcodes = {}
	all_opcodes = {}
		
	modulestosearch = getModulesToQuery(modulecriteria)
	if not silent:
		dbg.log(""[+] Querying %d modules"" % len(modulestosearch))
	
	starttime = datetime.datetime.now()
	for thismodule in modulestosearch:
		if not silent:
			dbg.log(""    - Querying module %s"" % thismodule)
		dbg.updateLog()
		#search
		found_opcodes = searchInModule(search,thismodule,criteria)
		#merge results
		all_opcodes = mergeOpcodes(all_opcodes,found_opcodes)
	#search outside modules
	if ""all"" in criteria:
		if ""accesslevel"" in criteria:
			if criteria[""accesslevel""].find(""R"") == -1:
				if not silent:
					dbg.log(""[+] Setting pointer access level criteria to 'R', to increase search results"")
				criteria[""accesslevel""] = ""R""
				if not silent:
					dbg.log(""    New pointer access level : %s"" % criteria[""accesslevel""])
		if criteria[""all""]:
			rangestosearch = getRangesOutsideModules()
			if not silent:
				dbg.log(""[+] Querying memory outside modules"")
			for thisrange in rangestosearch:
				if not silent:
					dbg.log(""    - Querying 0x%08x - 0x%08x"" % (thisrange[0],thisrange[1]))
				found_opcodes = searchInRange(search, thisrange[0], thisrange[1],criteria)
				all_opcodes = mergeOpcodes(all_opcodes,found_opcodes)
			if not silent:
				dbg.log(""    - Search complete, processing results"")
			dbg.updateLog()
	return all_opcodes","searchInRange(search, thisrange[0], thisrange[1], criteria)","searchInRange(search, *thisrange[:2], criteria)","iterable_zj[0], iterable_zj[1]",*thisrange[:2],*thisrange[:2],1
electricitymap-contrib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electricitymap-contrib/parsers/statnett.py,https://github.com/tmrowco/electricitymap-contrib/tree/master/parsers/statnett.py,,_fetch_exchanges_from_sorted_bidding_zones$238,"def _fetch_exchanges_from_sorted_bidding_zones(sorted_bidding_zones, session=None, target_datetime=None):
    zones = sorted_bidding_zones.split('->')
    return fetch_exchange_by_bidding_zone(zones[0], zones[1], session, target_datetime)","fetch_exchange_by_bidding_zone(zones[0], zones[1], session, target_datetime)","fetch_exchange_by_bidding_zone(*zones[:2], session, target_datetime)","iterable_zj[0], iterable_zj[1]",*zones[:2],*zones[:2],1
anchore-engine,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anchore-engine/anchore_engine/util/rpm.py,https://github.com/anchore/anchore-engine/tree/master/anchore_engine/util/rpm.py,,if_main_my$248,"if __name__ == ""__main__"":
    import sys

    print((compare_versions(sys.argv[1], sys.argv[2])))","compare_versions(sys.argv[1], sys.argv[2])",compare_versions(*sys.argv[1:3]),"iterable_zj[1], iterable_zj[2]",*sys.argv[1:3],*sys.argv[1:3],1
crocodilehunter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/crocodilehunter/src/webui.py,https://github.com/EFForg/crocodilehunter/tree/master/src/webui.py,Webui,enodeb_sightings$44,"def enodeb_sightings(self):
        trilat_pts = []
        enodebs = []
        known_towers = [kt.to_dict() for kt in self.watchdog.get_known_towers().all()]
        towers = self.watchdog.get_unique_enodebs()
        for t in towers:
            self.watchdog.get_max_column_by_enodeb
            sightings = self.watchdog.get_sightings_for_enodeb(t)

            trilat = self.watchdog.trilaterate_enodeb_location(sightings)
            enodebs.append({
                ""trilat"": list(trilat),
                ""enodeb_id"": t.enodeb_id,
                ""plmn"": t.plmn(),
                ""closest_tower"": self.watchdog.closest_known_tower(trilat[0], trilat[1]),
                ""unique_cells"": self.watchdog.get_cells_count_for_enodebid(t),
                ""sightings"": sightings.count(),
                ""max_suspiciousness"": self.watchdog.get_suspicious_percentage_by_enodeb(t),
                ""first_seen"": str(self.watchdog.get_min_column_by_enodeb(t, 'timestamp')),
                ""last_seen"": str(self.watchdog.get_max_column_by_enodeb(t, 'timestamp'))

            })
        return render_template('index.html', name=self.watchdog.project_name,
                                known_towers = json.dumps(known_towers),
                                key = 'enodeb_id',
                                enodebs=json.dumps(enodebs))","self.watchdog.closest_known_tower(trilat[0], trilat[1])",self.watchdog.closest_known_tower(*trilat[:2]),"iterable_zj[0], iterable_zj[1]",*trilat[:2],*trilat[:2],1
Gooey,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Gooey/gooey/gui/three_to_four.py,https://github.com/chriskiehl/Gooey/tree/master/gooey/gui/three_to_four.py,,bitmapFromBufferRGBA$51,"def bitmapFromBufferRGBA(im, rgba):
    if isLatestVersion:
        return wx.Bitmap.FromBufferRGBA(im.size[0], im.size[1], rgba)
    else:
        return wx.BitmapFromBufferRGBA(im.size[0], im.size[1], rgba)","wx.Bitmap.FromBufferRGBA(im.size[0], im.size[1], rgba)","wx.Bitmap.FromBufferRGBA(*im.size[:2], rgba)","iterable_zj[0], iterable_zj[1]",*im.size[:2],*im.size[:2],1
Gooey,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Gooey/gooey/gui/three_to_four.py,https://github.com/chriskiehl/Gooey/tree/master/gooey/gui/three_to_four.py,,bitmapFromBufferRGBA$51,"def bitmapFromBufferRGBA(im, rgba):
    if isLatestVersion:
        return wx.Bitmap.FromBufferRGBA(im.size[0], im.size[1], rgba)
    else:
        return wx.BitmapFromBufferRGBA(im.size[0], im.size[1], rgba)","wx.BitmapFromBufferRGBA(im.size[0], im.size[1], rgba)","wx.BitmapFromBufferRGBA(*im.size[:2], rgba)","iterable_zj[0], iterable_zj[1]",*im.size[:2],*im.size[:2],1
opentelemetry-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opentelemetry-python/shim/opentelemetry-opentracing-shim/tests/testbed/test_active_span_replacement/test_asyncio.py,https://github.com/open-telemetry/opentelemetry-python/tree/master/shim/opentelemetry-opentracing-shim/tests/testbed/test_active_span_replacement/test_asyncio.py,TestAsyncio,test_main$13,"def test_main(self):
        # Start an isolated task and query for its result -and finish it-
        # in another task/thread
        span = self.tracer.start_span(""initial"")
        self.submit_another_task(span)

        stop_loop_when(
            self.loop,
            lambda: len(self.tracer.finished_spans()) >= 3,
            timeout=5.0,
        )
        self.loop.run_forever()

        spans = self.tracer.finished_spans()
        self.assertEqual(len(spans), 3)
        self.assertNamesEqual(spans, [""initial"", ""subtask"", ""task""])

        # task/subtask are part of the same trace,
        # and subtask is a child of task
        self.assertSameTrace(spans[1], spans[2])
        self.assertIsChildOf(spans[1], spans[2])

        # initial task is not related in any way to those two tasks
        self.assertNotSameTrace(spans[0], spans[1])
        self.assertEqual(spans[0].parent, None)","self.assertSameTrace(spans[1], spans[2])",self.assertSameTrace(*spans[1:3]),"iterable_zj[1], iterable_zj[2]",*spans[1:3],*spans[1:3],1
opentelemetry-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opentelemetry-python/shim/opentelemetry-opentracing-shim/tests/testbed/test_active_span_replacement/test_asyncio.py,https://github.com/open-telemetry/opentelemetry-python/tree/master/shim/opentelemetry-opentracing-shim/tests/testbed/test_active_span_replacement/test_asyncio.py,TestAsyncio,test_main$13,"def test_main(self):
        # Start an isolated task and query for its result -and finish it-
        # in another task/thread
        span = self.tracer.start_span(""initial"")
        self.submit_another_task(span)

        stop_loop_when(
            self.loop,
            lambda: len(self.tracer.finished_spans()) >= 3,
            timeout=5.0,
        )
        self.loop.run_forever()

        spans = self.tracer.finished_spans()
        self.assertEqual(len(spans), 3)
        self.assertNamesEqual(spans, [""initial"", ""subtask"", ""task""])

        # task/subtask are part of the same trace,
        # and subtask is a child of task
        self.assertSameTrace(spans[1], spans[2])
        self.assertIsChildOf(spans[1], spans[2])

        # initial task is not related in any way to those two tasks
        self.assertNotSameTrace(spans[0], spans[1])
        self.assertEqual(spans[0].parent, None)","self.assertIsChildOf(spans[1], spans[2])",self.assertIsChildOf(*spans[1:3]),"iterable_zj[1], iterable_zj[2]",*spans[1:3],*spans[1:3],1
opentelemetry-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opentelemetry-python/shim/opentelemetry-opentracing-shim/tests/testbed/test_active_span_replacement/test_asyncio.py,https://github.com/open-telemetry/opentelemetry-python/tree/master/shim/opentelemetry-opentracing-shim/tests/testbed/test_active_span_replacement/test_asyncio.py,TestAsyncio,test_main$13,"def test_main(self):
        # Start an isolated task and query for its result -and finish it-
        # in another task/thread
        span = self.tracer.start_span(""initial"")
        self.submit_another_task(span)

        stop_loop_when(
            self.loop,
            lambda: len(self.tracer.finished_spans()) >= 3,
            timeout=5.0,
        )
        self.loop.run_forever()

        spans = self.tracer.finished_spans()
        self.assertEqual(len(spans), 3)
        self.assertNamesEqual(spans, [""initial"", ""subtask"", ""task""])

        # task/subtask are part of the same trace,
        # and subtask is a child of task
        self.assertSameTrace(spans[1], spans[2])
        self.assertIsChildOf(spans[1], spans[2])

        # initial task is not related in any way to those two tasks
        self.assertNotSameTrace(spans[0], spans[1])
        self.assertEqual(spans[0].parent, None)","self.assertNotSameTrace(spans[0], spans[1])",self.assertNotSameTrace(*spans[:2]),"iterable_zj[0], iterable_zj[1]",*spans[:2],*spans[:2],1
open_model_zoo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open_model_zoo/demos/multi_camera_multi_target_tracking_demo/python/multi_camera_multi_target_tracking_demo.py,https://github.com/openvinotoolkit/open_model_zoo/tree/master/demos/multi_camera_multi_target_tracking_demo/python/multi_camera_multi_target_tracking_demo.py,,check_detectors$46,"def check_detectors(args):
    detectors = {
        '--m_detector': args.m_detector,
        '--m_segmentation': args.m_segmentation,
        '--detections': args.detections
    }
    non_empty_detectors = [(det, value) for det, value in detectors.items() if value]
    det_number = len(non_empty_detectors)
    if det_number == 0:
        log.error('No detector specified, please specify one of the following parameters: '
                  '\'--m_detector\', \'--m_segmentation\' or \'--detections\'')
    elif det_number > 1:
        det_string = ''.join('\n\t{}={}'.format(det[0], det[1]) for det in non_empty_detectors)
        log.error('Only one detector expected but got {}, please specify one of them:{}'
                  .format(len(non_empty_detectors), det_string))
    return det_number","'\n\t{}={}'.format(det[0], det[1])",'\n\t{}={}'.format(*det[:2]),"iterable_zj[0], iterable_zj[1]",*det[:2],*det[:2],1
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/livestatus/api/python/livestatus.py,https://github.com/tribe29/checkmk/tree/master/livestatus/api/python/livestatus.py,Helpers,query_row_assoc$223,"def query_row_assoc(self, query: ""QueryTypes"") -> Dict[str, Any]:
        """"""Issues a query that returns one line of data and returns the elements
        of that line as a dictionary from column names to values""""""
        normalized_query = Query(query) if not isinstance(query, Query) else query

        r = self.query(normalized_query, ""ColumnHeaders: on\n"")[0:2]
        return dict(zip(r[0], r[1]))","zip(r[0], r[1])",zip(*r[:2]),"iterable_zj[0], iterable_zj[1]",*r[:2],*r[:2],1
ssd.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ssd.pytorch/eval.py,https://github.com/amdegroot/ssd.pytorch/tree/master//eval.py,,voc_ap$194,"def voc_ap(rec, prec, use_07_metric=True):
    """""" ap = voc_ap(rec, prec, [use_07_metric])
    Compute VOC AP given precision and recall.
    If use_07_metric is true, uses the
    VOC 07 11 point method (default:True).
    """"""
    if use_07_metric:
        # 11 point metric
        ap = 0.
        for t in np.arange(0., 1.1, 0.1):
            if np.sum(rec >= t) == 0:
                p = 0
            else:
                p = np.max(prec[rec >= t])
            ap = ap + p / 11.
    else:
        # correct AP calculation
        # first append sentinel values at the end
        mrec = np.concatenate(([0.], rec, [1.]))
        mpre = np.concatenate(([0.], prec, [0.]))

        # compute the precision envelope
        for i in range(mpre.size - 1, 0, -1):
            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

        # to calculate area under PR curve, look for points
        # where X axis (recall) changes value
        i = np.where(mrec[1:] != mrec[:-1])[0]

        # and sum (\Delta recall) * prec
        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap","np.maximum(mpre[i - 1], mpre[i])",np.maximum(*mpre[i - 1:i + 1]),"iterable_zj[i - 1], iterable_zj[i]",*mpre[i-1:i+1],*mpre[i - 1:i + 1],1
fairseq,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairseq/examples/speech_recognition/kaldi/kaldi_initializer.py,https://github.com/pytorch/fairseq/tree/master/examples/speech_recognition/kaldi/kaldi_initializer.py,,create_lexicon$58,"def create_lexicon(
    cfg: KaldiInitializerConfig,
    fst_dir: Path,
    unique_label: str,
    in_units_file: Path,
    out_words_file: Path,
) -> (Path, Path):

    disambig_in_units_file = fst_dir / f""kaldi_dict.{cfg.in_labels}_disambig.txt""
    lexicon_file = fst_dir / f""kaldi_lexicon.{unique_label}.txt""
    disambig_lexicon_file = fst_dir / f""kaldi_lexicon.{unique_label}_disambig.txt""
    if (
        not lexicon_file.exists()
        or not disambig_lexicon_file.exists()
        or not disambig_in_units_file.exists()
    ):
        logger.info(f""Creating {lexicon_file} (in units file: {in_units_file})"")

        assert cfg.wav2letter_lexicon is not None or cfg.in_labels == cfg.out_labels

        if cfg.wav2letter_lexicon is not None:
            lm_words = set()
            with open(out_words_file, ""r"") as lm_dict_f:
                for line in lm_dict_f:
                    lm_words.add(line.split()[0])

            num_skipped = 0
            total = 0
            with open(cfg.wav2letter_lexicon, ""r"") as w2l_lex_f, open(
                lexicon_file, ""w""
            ) as out_f:
                for line in w2l_lex_f:
                    items = line.rstrip().split(""\t"")
                    assert len(items) == 2, items
                    if items[0] in lm_words:
                        print(items[0], items[1], file=out_f)
                    else:
                        num_skipped += 1
                        logger.debug(
                            f""Skipping word {items[0]} as it was not found in LM""
                        )
                    total += 1
            if num_skipped > 0:
                logger.warning(
                    f""Skipped {num_skipped} out of {total} words as they were not found in LM""
                )
        else:
            with open(in_units_file, ""r"") as in_f, open(lexicon_file, ""w"") as out_f:
                for line in in_f:
                    symb = line.split()[0]
                    if symb != ""<eps>"" and symb != ""<ctc_blank>"" and symb != ""<SIL>"":
                        print(symb, symb, file=out_f)

        lex_disambig_path = (
            Path(cfg.kaldi_root) / ""egs/wsj/s5/utils/add_lex_disambig.pl""
        )
        res = subprocess.run(
            [lex_disambig_path, lexicon_file, disambig_lexicon_file],
            check=True,
            capture_output=True,
        )
        ndisambig = int(res.stdout)
        disamib_path = Path(cfg.kaldi_root) / ""egs/wsj/s5/utils/add_disambig.pl""
        res = subprocess.run(
            [disamib_path, ""--include-zero"", in_units_file, str(ndisambig)],
            check=True,
            capture_output=True,
        )
        with open(disambig_in_units_file, ""wb"") as f:
            f.write(res.stdout)

    return disambig_lexicon_file, disambig_in_units_file","print(items[0], items[1], file=out_f)","print(*items[:2], file=out_f)","iterable_zj[0], iterable_zj[1]",*items[:2],*items[:2],1
sfepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sfepy/sfepy/discrete/fem/mesh.py,https://github.com/sfepy/sfepy/tree/master/sfepy/discrete/fem/mesh.py,Mesh,create_conn_graph$490,"def create_conn_graph(self, verbose=True):
        """"""
        Create a graph of mesh connectivity.

        Returns
        -------
        graph : csr_matrix
            The mesh connectivity graph as a SciPy CSR matrix.
        """"""
        from sfepy.discrete.common.extmods.cmesh import create_mesh_graph

        shape = (self.n_nod, self.n_nod)
        output('graph shape:', shape, verbose=verbose)
        if nm.prod(shape) == 0:
            output('no graph (zero size)!', verbose=verbose)
            return None

        output('assembling mesh graph...', verbose=verbose)
        timer = Timer(start=True)

        conn = self.get_conn(self.descs[0])
        nnz, prow, icol = create_mesh_graph(shape[0], shape[1],
                                            1, [conn], [conn])
        output('...done in %.2f s' % timer.stop(), verbose=verbose)
        output('graph nonzeros: %d (%.2e%% fill)' \
               % (nnz, 100.0 * float(nnz) / nm.prod(shape)), verbose=verbose)

        data = nm.ones((nnz,), dtype=bool)
        graph = sp.csr_matrix((data, icol, prow), shape)

        return graph","create_mesh_graph(shape[0], shape[1], 1, [conn], [conn])","create_mesh_graph(*shape[:2], 1, [conn], [conn])","iterable_zj[0], iterable_zj[1]",*shape[:2],*shape[:2],1
X-StereoLab,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/X-StereoLab/disparity/layers/roi_pool.py,https://github.com/meteorshowers/X-StereoLab/tree/master/disparity/layers/roi_pool.py,_ROIPool,backward$25,"def backward(ctx, grad_output):
        input, rois, argmax = ctx.saved_tensors
        output_size = ctx.output_size
        spatial_scale = ctx.spatial_scale
        bs, ch, h, w = ctx.input_shape
        grad_input = _C.roi_pool_backward(
            grad_output,
            input,
            rois,
            argmax,
            spatial_scale,
            output_size[0],
            output_size[1],
            bs,
            ch,
            h,
            w,
        )
        return grad_input, None, None, None","_C.roi_pool_backward(grad_output, input, rois, argmax, spatial_scale, output_size[0], output_size[1], bs, ch, h, w)","_C.roi_pool_backward(grad_output, input, rois, argmax, spatial_scale, *output_size[:2], bs, ch, h, w)","iterable_zj[0], iterable_zj[1]",*output_size[:2],*output_size[:2],1
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/ppdet/modeling/mot/matching/jde_matching.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/ppdet/modeling/mot/matching/jde_matching.py,,merge_matches$36,"def merge_matches(m1, m2, shape):
    O, P, Q = shape
    m1 = np.asarray(m1)
    m2 = np.asarray(m2)

    M1 = scipy.sparse.coo_matrix(
        (np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))
    M2 = scipy.sparse.coo_matrix(
        (np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))

    mask = M1 * M2
    match = mask.nonzero()
    match = list(zip(match[0], match[1]))
    unmatched_O = tuple(set(range(O)) - set([i for i, j in match]))
    unmatched_Q = tuple(set(range(Q)) - set([j for i, j in match]))

    return match, unmatched_O, unmatched_Q","zip(match[0], match[1])",zip(*match[:2]),"iterable_zj[0], iterable_zj[1]",*match[:2],*match[:2],1
Pointnet2_PyTorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pointnet2_PyTorch/pointnet2_ops_lib/pointnet2_ops/pointnet2_modules.py,https://github.com/erikwijmans/Pointnet2_PyTorch/tree/master/pointnet2_ops_lib/pointnet2_ops/pointnet2_modules.py,,build_shared_mlp$9,"def build_shared_mlp(mlp_spec: List[int], bn: bool = True):
    layers = []
    for i in range(1, len(mlp_spec)):
        layers.append(
            nn.Conv2d(mlp_spec[i - 1], mlp_spec[i], kernel_size=1, bias=not bn)
        )
        if bn:
            layers.append(nn.BatchNorm2d(mlp_spec[i]))
        layers.append(nn.ReLU(True))

    return nn.Sequential(*layers)","nn.Conv2d(mlp_spec[i - 1], mlp_spec[i], kernel_size=1, bias=not bn)","nn.Conv2d(*mlp_spec[i - 1:i + 1], kernel_size=1, bias=not bn)","iterable_zj[i - 1], iterable_zj[i]",*mlp_spec[i-1:i+1],*mlp_spec[i - 1:i + 1],1
AIDungeon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AIDungeon/story/utils.py,https://github.com/Latitude-Archives/AIDungeon/tree/master/story/utils.py,,second_to_first_person$284,"def second_to_first_person(text):
    text = "" "" + text
    text = standardize_punctuation(text)
    for pair in second_to_first_mappings:
        variations = mapping_variation_pairs(pair)
        for variation in variations:
            text = replace_outside_quotes(text, variation[0], variation[1])

    return capitalize_first_letters(text[1:])","replace_outside_quotes(text, variation[0], variation[1])","replace_outside_quotes(text, *variation[:2])","iterable_zj[0], iterable_zj[1]",*variation[:2],*variation[:2],1
Discord-Selfbot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Discord-Selfbot/cogs/emoji.py,https://github.com/appu1232/Discord-Selfbot/tree/master/cogs/emoji.py,Emoji,find_emoji$15,"def find_emoji(self, msg):
        msg = re.sub(""<a?:(.+):([0-9]+)>"", ""\\2"", msg)
        color_modifiers = [""1f3fb"", ""1f3fc"", ""1f3fd"", ""1f44c"", ""1f3fe"", ""1f3ff""]  # These color modifiers aren't in Twemoji
        
        name = None

        for guild in self.bot.guilds:
            for emoji in guild.emojis:
                if msg.strip().lower() in emoji.name.lower():
                    name = emoji.name + ("".gif"" if emoji.animated else "".png"")
                    url = emoji.url
                    id = emoji.id
                    guild_name = guild.name
                if msg.strip() in (str(emoji.id), emoji.name):
                    name = emoji.name + ("".gif"" if emoji.animated else "".png"")
                    url = emoji.url
                    return name, url, emoji.id, guild.name
        if name:
            return name, url, id, guild_name

        # Here we check for a stock emoji before returning a failure
        codepoint_regex = re.compile('([\d#])?\\\\[xuU]0*([a-f\d]*)')
        unicode_raw = msg.encode('unicode-escape').decode('ascii')
        codepoints = codepoint_regex.findall(unicode_raw)
        if codepoints == []:
            return """", """", """", """"

        if len(codepoints) > 1 and codepoints[1][1] in color_modifiers:
            codepoints.pop(1)

        if codepoints[0][0] == '#':
            emoji_code = '23-20e3'
        elif codepoints[0][0] == '':
            codepoints = [x[1] for x in codepoints]
            emoji_code = '-'.join(codepoints)
        else:
            emoji_code = ""3{}-{}"".format(codepoints[0][0], codepoints[0][1])
        url = ""https://raw.githubusercontent.com/astronautlevel2/twemoji/gh-pages/128x128/{}.png"".format(emoji_code)
        name = ""emoji.png""
        return name, url, ""N/A"", ""Official""","'3{}-{}'.format(codepoints[0][0], codepoints[0][1])",'3{}-{}'.format(*codepoints[0][:2]),"iterable_zj[0], iterable_zj[1]",*codepoints[0][:2],*codepoints[0][:2],1
open_lth,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open_lth/training/metric_logger.py,https://github.com/facebookresearch/open_lth/tree/master/training/metric_logger.py,MetricLogger,__str__$18,"def __str__(self):
        return '\n'.join(['{},{},{}'.format(k[0], k[1], v) for k, v in self.log.items()])","'{},{},{}'.format(k[0], k[1], v)","'{},{},{}'.format(*k[:2], v)","iterable_zj[0], iterable_zj[1]",*k[:2],*k[:2],1
django-filter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-filter/tests/test_filters.py,https://github.com/carltongibson/django-filter/tree/master/tests/test_filters.py,MultipleChoiceFilterTests,test_filter_conjoined_true$450,"def test_filter_conjoined_true(self):
        """"""Tests that a filter with `conjoined=True` returns objects that
        have all the values included in `value`. For example filter
        users that have all of this books.

        """"""
        book_kwargs = {'price': 1, 'average_rating': 1}
        books = []
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))

        user1 = User.objects.create()
        user2 = User.objects.create()
        user3 = User.objects.create()
        user4 = User.objects.create()
        user5 = User.objects.create()

        user1.favorite_books.add(books[0], books[1])
        user2.favorite_books.add(books[0], books[1], books[2])
        user3.favorite_books.add(books[1], books[2])
        user4.favorite_books.add(books[2], books[3])
        user5.favorite_books.add(books[4], books[5])

        filter_list = (
            ((books[0].pk, books[0].pk),  # values
             [1, 2]),  # list of user.pk that have `value` books
            ((books[1].pk, books[1].pk),
             [1, 2, 3]),
            ((books[2].pk, books[2].pk),
             [2, 3, 4]),
            ((books[3].pk, books[3].pk),
             [4, ]),
            ((books[4].pk, books[4].pk),
             [5, ]),
            ((books[0].pk, books[1].pk),
             [1, 2]),
            ((books[0].pk, books[2].pk),
             [2, ]),
            ((books[1].pk, books[2].pk),
             [2, 3]),
            ((books[2].pk, books[3].pk),
             [4, ]),
            ((books[4].pk, books[5].pk),
             [5, ]),
            ((books[3].pk, books[4].pk),
             []),
        )
        users = User.objects.all()

        for item in filter_list:
            f = MultipleChoiceFilter(field_name='favorite_books__pk', conjoined=True)
            queryset = f.filter(users, item[0])
            expected_pks = [c[0] for c in queryset.values_list('pk')]
            self.assertListEqual(
                expected_pks,
                item[1],
                'Lists Differ: {0} != {1} for case {2}'.format(
                    expected_pks, item[1], item[0]))","user1.favorite_books.add(books[0], books[1])",user1.favorite_books.add(*books[:2]),"iterable_zj[0], iterable_zj[1]",*books[:2],*books[:2],1
django-filter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-filter/tests/test_filters.py,https://github.com/carltongibson/django-filter/tree/master/tests/test_filters.py,MultipleChoiceFilterTests,test_filter_conjoined_true$450,"def test_filter_conjoined_true(self):
        """"""Tests that a filter with `conjoined=True` returns objects that
        have all the values included in `value`. For example filter
        users that have all of this books.

        """"""
        book_kwargs = {'price': 1, 'average_rating': 1}
        books = []
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))

        user1 = User.objects.create()
        user2 = User.objects.create()
        user3 = User.objects.create()
        user4 = User.objects.create()
        user5 = User.objects.create()

        user1.favorite_books.add(books[0], books[1])
        user2.favorite_books.add(books[0], books[1], books[2])
        user3.favorite_books.add(books[1], books[2])
        user4.favorite_books.add(books[2], books[3])
        user5.favorite_books.add(books[4], books[5])

        filter_list = (
            ((books[0].pk, books[0].pk),  # values
             [1, 2]),  # list of user.pk that have `value` books
            ((books[1].pk, books[1].pk),
             [1, 2, 3]),
            ((books[2].pk, books[2].pk),
             [2, 3, 4]),
            ((books[3].pk, books[3].pk),
             [4, ]),
            ((books[4].pk, books[4].pk),
             [5, ]),
            ((books[0].pk, books[1].pk),
             [1, 2]),
            ((books[0].pk, books[2].pk),
             [2, ]),
            ((books[1].pk, books[2].pk),
             [2, 3]),
            ((books[2].pk, books[3].pk),
             [4, ]),
            ((books[4].pk, books[5].pk),
             [5, ]),
            ((books[3].pk, books[4].pk),
             []),
        )
        users = User.objects.all()

        for item in filter_list:
            f = MultipleChoiceFilter(field_name='favorite_books__pk', conjoined=True)
            queryset = f.filter(users, item[0])
            expected_pks = [c[0] for c in queryset.values_list('pk')]
            self.assertListEqual(
                expected_pks,
                item[1],
                'Lists Differ: {0} != {1} for case {2}'.format(
                    expected_pks, item[1], item[0]))","user2.favorite_books.add(books[0], books[1], books[2])",user2.favorite_books.add(*books[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*books[:3],*books[:3],1
django-filter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-filter/tests/test_filters.py,https://github.com/carltongibson/django-filter/tree/master/tests/test_filters.py,MultipleChoiceFilterTests,test_filter_conjoined_true$450,"def test_filter_conjoined_true(self):
        """"""Tests that a filter with `conjoined=True` returns objects that
        have all the values included in `value`. For example filter
        users that have all of this books.

        """"""
        book_kwargs = {'price': 1, 'average_rating': 1}
        books = []
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))

        user1 = User.objects.create()
        user2 = User.objects.create()
        user3 = User.objects.create()
        user4 = User.objects.create()
        user5 = User.objects.create()

        user1.favorite_books.add(books[0], books[1])
        user2.favorite_books.add(books[0], books[1], books[2])
        user3.favorite_books.add(books[1], books[2])
        user4.favorite_books.add(books[2], books[3])
        user5.favorite_books.add(books[4], books[5])

        filter_list = (
            ((books[0].pk, books[0].pk),  # values
             [1, 2]),  # list of user.pk that have `value` books
            ((books[1].pk, books[1].pk),
             [1, 2, 3]),
            ((books[2].pk, books[2].pk),
             [2, 3, 4]),
            ((books[3].pk, books[3].pk),
             [4, ]),
            ((books[4].pk, books[4].pk),
             [5, ]),
            ((books[0].pk, books[1].pk),
             [1, 2]),
            ((books[0].pk, books[2].pk),
             [2, ]),
            ((books[1].pk, books[2].pk),
             [2, 3]),
            ((books[2].pk, books[3].pk),
             [4, ]),
            ((books[4].pk, books[5].pk),
             [5, ]),
            ((books[3].pk, books[4].pk),
             []),
        )
        users = User.objects.all()

        for item in filter_list:
            f = MultipleChoiceFilter(field_name='favorite_books__pk', conjoined=True)
            queryset = f.filter(users, item[0])
            expected_pks = [c[0] for c in queryset.values_list('pk')]
            self.assertListEqual(
                expected_pks,
                item[1],
                'Lists Differ: {0} != {1} for case {2}'.format(
                    expected_pks, item[1], item[0]))","user3.favorite_books.add(books[1], books[2])",user3.favorite_books.add(*books[1:3]),"iterable_zj[1], iterable_zj[2]",*books[1:3],*books[1:3],1
django-filter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-filter/tests/test_filters.py,https://github.com/carltongibson/django-filter/tree/master/tests/test_filters.py,MultipleChoiceFilterTests,test_filter_conjoined_true$450,"def test_filter_conjoined_true(self):
        """"""Tests that a filter with `conjoined=True` returns objects that
        have all the values included in `value`. For example filter
        users that have all of this books.

        """"""
        book_kwargs = {'price': 1, 'average_rating': 1}
        books = []
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))

        user1 = User.objects.create()
        user2 = User.objects.create()
        user3 = User.objects.create()
        user4 = User.objects.create()
        user5 = User.objects.create()

        user1.favorite_books.add(books[0], books[1])
        user2.favorite_books.add(books[0], books[1], books[2])
        user3.favorite_books.add(books[1], books[2])
        user4.favorite_books.add(books[2], books[3])
        user5.favorite_books.add(books[4], books[5])

        filter_list = (
            ((books[0].pk, books[0].pk),  # values
             [1, 2]),  # list of user.pk that have `value` books
            ((books[1].pk, books[1].pk),
             [1, 2, 3]),
            ((books[2].pk, books[2].pk),
             [2, 3, 4]),
            ((books[3].pk, books[3].pk),
             [4, ]),
            ((books[4].pk, books[4].pk),
             [5, ]),
            ((books[0].pk, books[1].pk),
             [1, 2]),
            ((books[0].pk, books[2].pk),
             [2, ]),
            ((books[1].pk, books[2].pk),
             [2, 3]),
            ((books[2].pk, books[3].pk),
             [4, ]),
            ((books[4].pk, books[5].pk),
             [5, ]),
            ((books[3].pk, books[4].pk),
             []),
        )
        users = User.objects.all()

        for item in filter_list:
            f = MultipleChoiceFilter(field_name='favorite_books__pk', conjoined=True)
            queryset = f.filter(users, item[0])
            expected_pks = [c[0] for c in queryset.values_list('pk')]
            self.assertListEqual(
                expected_pks,
                item[1],
                'Lists Differ: {0} != {1} for case {2}'.format(
                    expected_pks, item[1], item[0]))","user4.favorite_books.add(books[2], books[3])",user4.favorite_books.add(*books[2:4]),"iterable_zj[2], iterable_zj[3]",*books[2:4],*books[2:4],1
django-filter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-filter/tests/test_filters.py,https://github.com/carltongibson/django-filter/tree/master/tests/test_filters.py,MultipleChoiceFilterTests,test_filter_conjoined_true$450,"def test_filter_conjoined_true(self):
        """"""Tests that a filter with `conjoined=True` returns objects that
        have all the values included in `value`. For example filter
        users that have all of this books.

        """"""
        book_kwargs = {'price': 1, 'average_rating': 1}
        books = []
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))
        books.append(Book.objects.create(**book_kwargs))

        user1 = User.objects.create()
        user2 = User.objects.create()
        user3 = User.objects.create()
        user4 = User.objects.create()
        user5 = User.objects.create()

        user1.favorite_books.add(books[0], books[1])
        user2.favorite_books.add(books[0], books[1], books[2])
        user3.favorite_books.add(books[1], books[2])
        user4.favorite_books.add(books[2], books[3])
        user5.favorite_books.add(books[4], books[5])

        filter_list = (
            ((books[0].pk, books[0].pk),  # values
             [1, 2]),  # list of user.pk that have `value` books
            ((books[1].pk, books[1].pk),
             [1, 2, 3]),
            ((books[2].pk, books[2].pk),
             [2, 3, 4]),
            ((books[3].pk, books[3].pk),
             [4, ]),
            ((books[4].pk, books[4].pk),
             [5, ]),
            ((books[0].pk, books[1].pk),
             [1, 2]),
            ((books[0].pk, books[2].pk),
             [2, ]),
            ((books[1].pk, books[2].pk),
             [2, 3]),
            ((books[2].pk, books[3].pk),
             [4, ]),
            ((books[4].pk, books[5].pk),
             [5, ]),
            ((books[3].pk, books[4].pk),
             []),
        )
        users = User.objects.all()

        for item in filter_list:
            f = MultipleChoiceFilter(field_name='favorite_books__pk', conjoined=True)
            queryset = f.filter(users, item[0])
            expected_pks = [c[0] for c in queryset.values_list('pk')]
            self.assertListEqual(
                expected_pks,
                item[1],
                'Lists Differ: {0} != {1} for case {2}'.format(
                    expected_pks, item[1], item[0]))","user5.favorite_books.add(books[4], books[5])",user5.favorite_books.add(*books[4:6]),"iterable_zj[4], iterable_zj[5]",*books[4:6],*books[4:6],1
nimfa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nimfa/nimfa/methods/factorization/psmf.py,https://github.com/mims-harvard/nimfa/tree/master/nimfa/methods/factorization/psmf.py,Psmf,_update_sigma$344,"def _update_sigma(self):
        """"""Compute E-step and update sigma.""""""
        self.cross_terms = np.zeros((self.V.shape[0], self.rank, self.N))
        for cc in range(self.rank):
            t_c1 = np.tile(self.sigma[:, cc, :].reshape((self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1))
            t_c2 = np.tile(np.dot(self.zeta[cc, :], self.zeta.T), (self.V.shape[0], 1))
            t_c3 = np.tile((self.lamb * np.tile(self.lamb[:, cc].reshape((self.lamb.shape[0], 1)), (1, self.rank)) * t_c2).reshape(
                t_c2.shape[0], t_c2.shape[1], 1), (1, 1, self.N))
            self.cross_terms += t_c1 * t_c3
        self.sigma = np.zeros(self.sigma.shape)
        for t in range(self.V.shape[1]):
            t_s1 = np.tile(self.__arr(self.V[:, t]), (1, self.rank)) - self.lamb * np.tile(
                self.zeta[:, t].T, (self.V.shape[0], 1))
            t_s2 = t_s1 ** 2 + self.lamb ** 2 * \
                np.tile(self.phi.T, (self.V.shape[0], 1))
            self.sigma -= 0.5 * \
                np.tile((t_s2 / np.tile(self.psi, (1, self.rank))).reshape(
                    t_s2.shape[0], t_s2.shape[1], 1), (1, 1, self.N))
        for n in range(self.N):
            for nn in range(self.N):
                if nn != n:
                    t_s1 = (1e-50 + self.rho[:, max(n, nn):self.N]).sum(
                        axis=1) / (1e-50 + self.rho[:, n:self.N]).sum(axis=1)
                    self.sigma[:, :, n] -= np.tile(t_s1.reshape(self.psi.shape) / self.psi, (1, self.rank)) * self.cross_terms[:,:, nn]        
        self.sigma = np.exp(self.sigma - np.tile(np.amax(self.sigma, 1).reshape(
            (self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1)))
        self.sigma /= np.tile(self.sigma.sum(axis=1).reshape(
            (self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1))
        self.cross_terms = self._cross_terms()
        self.s = np.argmax(self.sigma, axis=1)
        self.s = self.s.transpose([0, 1])","(self.lamb * np.tile(self.lamb[:, cc].reshape((self.lamb.shape[0], 1)), (1, self.rank)) * t_c2).reshape(t_c2.shape[0], t_c2.shape[1], 1)","(self.lamb * np.tile(self.lamb[:, cc].reshape((self.lamb.shape[0], 1)), (1, self.rank)) * t_c2).reshape(*t_c2.shape[:2], 1)","iterable_zj[0], iterable_zj[1]",*t_c2.shape[:2],*t_c2.shape[:2],1
nimfa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nimfa/nimfa/methods/factorization/psmf.py,https://github.com/mims-harvard/nimfa/tree/master/nimfa/methods/factorization/psmf.py,Psmf,_update_sigma$344,"def _update_sigma(self):
        """"""Compute E-step and update sigma.""""""
        self.cross_terms = np.zeros((self.V.shape[0], self.rank, self.N))
        for cc in range(self.rank):
            t_c1 = np.tile(self.sigma[:, cc, :].reshape((self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1))
            t_c2 = np.tile(np.dot(self.zeta[cc, :], self.zeta.T), (self.V.shape[0], 1))
            t_c3 = np.tile((self.lamb * np.tile(self.lamb[:, cc].reshape((self.lamb.shape[0], 1)), (1, self.rank)) * t_c2).reshape(
                t_c2.shape[0], t_c2.shape[1], 1), (1, 1, self.N))
            self.cross_terms += t_c1 * t_c3
        self.sigma = np.zeros(self.sigma.shape)
        for t in range(self.V.shape[1]):
            t_s1 = np.tile(self.__arr(self.V[:, t]), (1, self.rank)) - self.lamb * np.tile(
                self.zeta[:, t].T, (self.V.shape[0], 1))
            t_s2 = t_s1 ** 2 + self.lamb ** 2 * \
                np.tile(self.phi.T, (self.V.shape[0], 1))
            self.sigma -= 0.5 * \
                np.tile((t_s2 / np.tile(self.psi, (1, self.rank))).reshape(
                    t_s2.shape[0], t_s2.shape[1], 1), (1, 1, self.N))
        for n in range(self.N):
            for nn in range(self.N):
                if nn != n:
                    t_s1 = (1e-50 + self.rho[:, max(n, nn):self.N]).sum(
                        axis=1) / (1e-50 + self.rho[:, n:self.N]).sum(axis=1)
                    self.sigma[:, :, n] -= np.tile(t_s1.reshape(self.psi.shape) / self.psi, (1, self.rank)) * self.cross_terms[:,:, nn]        
        self.sigma = np.exp(self.sigma - np.tile(np.amax(self.sigma, 1).reshape(
            (self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1)))
        self.sigma /= np.tile(self.sigma.sum(axis=1).reshape(
            (self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1))
        self.cross_terms = self._cross_terms()
        self.s = np.argmax(self.sigma, axis=1)
        self.s = self.s.transpose([0, 1])","(t_s2 / np.tile(self.psi, (1, self.rank))).reshape(t_s2.shape[0], t_s2.shape[1], 1)","(t_s2 / np.tile(self.psi, (1, self.rank))).reshape(*t_s2.shape[:2], 1)","iterable_zj[0], iterable_zj[1]",*t_s2.shape[:2],*t_s2.shape[:2],1
gym-malware,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gym-malware/train_agent_chainer.py,https://github.com/endgameinc/gym-malware/tree/master//train_agent_chainer.py,QFunction,__init__$26,"def __init__(self, obs_size, n_actions, n_hidden_channels=[1024,256]):
        super(QFunction,self).__init__()
        net = []
        inpdim = obs_size
        for i,n_hid in enumerate(n_hidden_channels):
            net += [ ('l{}'.format(i), L.Linear( inpdim, n_hid ) ) ]
            net += [ ('norm{}'.format(i), L.BatchNormalization( n_hid ) ) ]
            net += [ ('_act{}'.format(i), F.relu ) ]
            inpdim = n_hid

        net += [('output', L.Linear( inpdim, n_actions) )]

        with self.init_scope():
            for n in net:
                if not n[0].startswith('_'):
                    setattr(self, n[0], n[1])

        self.forward = net","setattr(self, n[0], n[1])","setattr(self, *n[:2])","iterable_zj[0], iterable_zj[1]",*n[:2],*n[:2],1
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/widgets/optionwidgets.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/widgets/optionwidgets.py,MySlider,apply_pic$711,"def apply_pic(self, picn, x, y, length, resize=None, only_text=None, source_val=None):
        
        if resize:
            txt = '<html><img src=""{0}"" width=""{2}""><p>{1}</p><html>'.format(picn, length, 100)
        else:
            txt = '<html><img src=""{}""><p>{}</p><html>'.format(picn, length)
        if ui.live_preview_style == 'tooltip':
            try:
                if self.final_url != ui.final_playing_url and not resize:
                    self.final_url = ui.final_playing_url
                    img = Image.open(picn)
                    self.half_size = int(img.size[0]/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\n change dimensions \n')
            except Exception as err:
                ui.logger.error(err)
            if self.tooltip is None:
                self.setToolTip('')
                self.setToolTip(txt)
            else:
                if ui.fullscreen_video or ui.force_fs:
                    y_cord = self.parent.y() + self.parent.maximumHeight() - 25
                else:
                    y_cord = self.parent.y() + self.parent.maximumHeight() + 25
                x_cord = self.parent.x() + x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                if source_val and source_val == ""progressbar"":
                    offset = 25
                else:
                    offset = 0
                print(offset)
                point = QtCore.QPoint(x_cord, y_cord - offset)
                rect = QtCore.QRect(self.parent.x(), self.parent.y() - offset, self.parent.width(), self.parent.height())
                #print(self.parent.x(), self.parent.y(), self.parent.width(), self.parent.height())
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip.showText(point, txt, self, rect, 3000)
        elif ui.live_preview_style == 'widget':
            try:
                if self.check_dimension_again and not resize:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = False
                if self.final_url != ui.final_playing_url:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.final_url = ui.final_playing_url
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = True
            except Exception as err:
                ui.logger.error(err)
            if os.path.isfile(picn):
                if not only_text:
                    self.pic.setPixmap(QtGui.QPixmap(picn, ""1""))
                x_cord = self.parent.x()+x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit - self.tooltip_widget.width()
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                y_cord = self.parent.y() - self.tooltip_widget.height() + ui.player_opt.height()
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip_widget.setGeometry(x_cord, y_cord, 128, 128)
                    self.tooltip_widget.show()
                    self.txt.setText(length)","QtCore.QSize(img.size[0], img.size[1])",QtCore.QSize(*img.size[:2]),"iterable_zj[0], iterable_zj[1]",*img.size[:2],*img.size[:2],1
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/widgets/optionwidgets.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/widgets/optionwidgets.py,MySlider,apply_pic$711,"def apply_pic(self, picn, x, y, length, resize=None, only_text=None, source_val=None):
        
        if resize:
            txt = '<html><img src=""{0}"" width=""{2}""><p>{1}</p><html>'.format(picn, length, 100)
        else:
            txt = '<html><img src=""{}""><p>{}</p><html>'.format(picn, length)
        if ui.live_preview_style == 'tooltip':
            try:
                if self.final_url != ui.final_playing_url and not resize:
                    self.final_url = ui.final_playing_url
                    img = Image.open(picn)
                    self.half_size = int(img.size[0]/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\n change dimensions \n')
            except Exception as err:
                ui.logger.error(err)
            if self.tooltip is None:
                self.setToolTip('')
                self.setToolTip(txt)
            else:
                if ui.fullscreen_video or ui.force_fs:
                    y_cord = self.parent.y() + self.parent.maximumHeight() - 25
                else:
                    y_cord = self.parent.y() + self.parent.maximumHeight() + 25
                x_cord = self.parent.x() + x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                if source_val and source_val == ""progressbar"":
                    offset = 25
                else:
                    offset = 0
                print(offset)
                point = QtCore.QPoint(x_cord, y_cord - offset)
                rect = QtCore.QRect(self.parent.x(), self.parent.y() - offset, self.parent.width(), self.parent.height())
                #print(self.parent.x(), self.parent.y(), self.parent.width(), self.parent.height())
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip.showText(point, txt, self, rect, 3000)
        elif ui.live_preview_style == 'widget':
            try:
                if self.check_dimension_again and not resize:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = False
                if self.final_url != ui.final_playing_url:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.final_url = ui.final_playing_url
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = True
            except Exception as err:
                ui.logger.error(err)
            if os.path.isfile(picn):
                if not only_text:
                    self.pic.setPixmap(QtGui.QPixmap(picn, ""1""))
                x_cord = self.parent.x()+x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit - self.tooltip_widget.width()
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                y_cord = self.parent.y() - self.tooltip_widget.height() + ui.player_opt.height()
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip_widget.setGeometry(x_cord, y_cord, 128, 128)
                    self.tooltip_widget.show()
                    self.txt.setText(length)","QtCore.QSize(img.size[0], img.size[1])",QtCore.QSize(*img.size[:2]),"iterable_zj[0], iterable_zj[1]",*img.size[:2],*img.size[:2],1
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/widgets/optionwidgets.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/widgets/optionwidgets.py,MySlider,apply_pic$711,"def apply_pic(self, picn, x, y, length, resize=None, only_text=None, source_val=None):
        
        if resize:
            txt = '<html><img src=""{0}"" width=""{2}""><p>{1}</p><html>'.format(picn, length, 100)
        else:
            txt = '<html><img src=""{}""><p>{}</p><html>'.format(picn, length)
        if ui.live_preview_style == 'tooltip':
            try:
                if self.final_url != ui.final_playing_url and not resize:
                    self.final_url = ui.final_playing_url
                    img = Image.open(picn)
                    self.half_size = int(img.size[0]/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\n change dimensions \n')
            except Exception as err:
                ui.logger.error(err)
            if self.tooltip is None:
                self.setToolTip('')
                self.setToolTip(txt)
            else:
                if ui.fullscreen_video or ui.force_fs:
                    y_cord = self.parent.y() + self.parent.maximumHeight() - 25
                else:
                    y_cord = self.parent.y() + self.parent.maximumHeight() + 25
                x_cord = self.parent.x() + x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                if source_val and source_val == ""progressbar"":
                    offset = 25
                else:
                    offset = 0
                print(offset)
                point = QtCore.QPoint(x_cord, y_cord - offset)
                rect = QtCore.QRect(self.parent.x(), self.parent.y() - offset, self.parent.width(), self.parent.height())
                #print(self.parent.x(), self.parent.y(), self.parent.width(), self.parent.height())
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip.showText(point, txt, self, rect, 3000)
        elif ui.live_preview_style == 'widget':
            try:
                if self.check_dimension_again and not resize:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = False
                if self.final_url != ui.final_playing_url:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.final_url = ui.final_playing_url
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = True
            except Exception as err:
                ui.logger.error(err)
            if os.path.isfile(picn):
                if not only_text:
                    self.pic.setPixmap(QtGui.QPixmap(picn, ""1""))
                x_cord = self.parent.x()+x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit - self.tooltip_widget.width()
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                y_cord = self.parent.y() - self.tooltip_widget.height() + ui.player_opt.height()
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip_widget.setGeometry(x_cord, y_cord, 128, 128)
                    self.tooltip_widget.show()
                    self.txt.setText(length)","QtCore.QSize(img.size[0], img.size[1])",QtCore.QSize(*img.size[:2]),"iterable_zj[0], iterable_zj[1]",*img.size[:2],*img.size[:2],1
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/widgets/optionwidgets.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/widgets/optionwidgets.py,MySlider,apply_pic$711,"def apply_pic(self, picn, x, y, length, resize=None, only_text=None, source_val=None):
        
        if resize:
            txt = '<html><img src=""{0}"" width=""{2}""><p>{1}</p><html>'.format(picn, length, 100)
        else:
            txt = '<html><img src=""{}""><p>{}</p><html>'.format(picn, length)
        if ui.live_preview_style == 'tooltip':
            try:
                if self.final_url != ui.final_playing_url and not resize:
                    self.final_url = ui.final_playing_url
                    img = Image.open(picn)
                    self.half_size = int(img.size[0]/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\n change dimensions \n')
            except Exception as err:
                ui.logger.error(err)
            if self.tooltip is None:
                self.setToolTip('')
                self.setToolTip(txt)
            else:
                if ui.fullscreen_video or ui.force_fs:
                    y_cord = self.parent.y() + self.parent.maximumHeight() - 25
                else:
                    y_cord = self.parent.y() + self.parent.maximumHeight() + 25
                x_cord = self.parent.x() + x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                if source_val and source_val == ""progressbar"":
                    offset = 25
                else:
                    offset = 0
                print(offset)
                point = QtCore.QPoint(x_cord, y_cord - offset)
                rect = QtCore.QRect(self.parent.x(), self.parent.y() - offset, self.parent.width(), self.parent.height())
                #print(self.parent.x(), self.parent.y(), self.parent.width(), self.parent.height())
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip.showText(point, txt, self, rect, 3000)
        elif ui.live_preview_style == 'widget':
            try:
                if self.check_dimension_again and not resize:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = False
                if self.final_url != ui.final_playing_url:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.final_url = ui.final_playing_url
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = True
            except Exception as err:
                ui.logger.error(err)
            if os.path.isfile(picn):
                if not only_text:
                    self.pic.setPixmap(QtGui.QPixmap(picn, ""1""))
                x_cord = self.parent.x()+x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit - self.tooltip_widget.width()
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                y_cord = self.parent.y() - self.tooltip_widget.height() + ui.player_opt.height()
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip_widget.setGeometry(x_cord, y_cord, 128, 128)
                    self.tooltip_widget.show()
                    self.txt.setText(length)","QtCore.QSize(img.size[0], img.size[1])",QtCore.QSize(*img.size[:2]),"iterable_zj[0], iterable_zj[1]",*img.size[:2],*img.size[:2],1
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/world/building/buildable.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/world/building/buildable.py,BuildableSingleEverywhere,check_build$329,"def check_build(cls, session, point, rotation=45, check_settlement=True, ship=None, issuer=None):
		# for non-quadratic buildings, we have to switch width and height depending on the rotation
		if rotation in [45, 225]:
			position = Rect.init_from_topleft_and_size(point.x, point.y, cls.size[0], cls.size[1])
		else:
			position = Rect.init_from_topleft_and_size(point.x, point.y, cls.size[1], cls.size[0])

		buildable = True
		tearset = []
		return _BuildPosition(position, rotation, tearset, buildable)","Rect.init_from_topleft_and_size(point.x, point.y, cls.size[0], cls.size[1])","Rect.init_from_topleft_and_size(point.x, point.y, *cls.size[:2])","iterable_zj[0], iterable_zj[1]",*cls.size[:2],*cls.size[:2],1
Python-UIAutomation-for-Windows,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Python-UIAutomation-for-Windows/uiautomation/uiautomation.py,https://github.com/yinkaisheng/Python-UIAutomation-for-Windows/tree/master/uiautomation/uiautomation.py,Control,MiddleClick$6546,"def MiddleClick(self, x: int = None, y: int = None, ratioX: float = 0.5, ratioY: float = 0.5, simulateMove: bool = True, waitTime: float = OPERATION_WAIT_TIME) -> None:
        """"""
        x: int, if < 0, middle click self.BoundingRectangle.right + x, if not None, ignore ratioX.
        y: int, if < 0, middle click self.BoundingRectangle.bottom + y, if not None, ignore ratioY.
        ratioX: float.
        ratioY: float.
        simulateMove: bool, if True, first move cursor to control smoothly.
        waitTime: float.

        MiddleClick(), MiddleClick(ratioX=0.5, ratioY=0.5): middle click center.
        MiddleClick(10, 10): middle click left+10, top+10.
        MiddleClick(-10, -10): middle click right-10, bottom-10.
        """"""
        point = self.MoveCursorToInnerPos(x, y, ratioX, ratioY, simulateMove)
        if point:
            MiddleClick(point[0], point[1], waitTime)","MiddleClick(point[0], point[1], waitTime)","MiddleClick(*point[:2], waitTime)","iterable_zj[0], iterable_zj[1]",*point[:2],*point[:2],1
beancount,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/beancount/beancount/utils/date_utils_test.py,https://github.com/beancount/beancount/tree/master/beancount/utils/date_utils_test.py,TestDateUtils,test_parse_date_liberally$21,"def test_parse_date_liberally(self):
        const_date = datetime.date(2014, 12, 7)
        test_cases = (
            ('12/7/2014',),
            ('7-Dec-2014',),
            ('7/12/2014', {'parserinfo': dateutil.parser.parserinfo(dayfirst=True)}),
            ('12/7', {'default': datetime.datetime(2014, 1, 1)}),
            ('7.12.2014', {'dayfirst': True}),
            ('14 12 7', {'yearfirst': True}),
            ('Transaction of 7th December 2014', {'fuzzy': True}),
        )
        for case in test_cases:
            if len(case) == 2:
                parse_date = date_utils.parse_date_liberally(case[0], case[1])
            else:
                parse_date = date_utils.parse_date_liberally(case[0])
            self.assertEqual(const_date, parse_date)","date_utils.parse_date_liberally(case[0], case[1])",date_utils.parse_date_liberally(*case[:2]),"iterable_zj[0], iterable_zj[1]",*case[:2],*case[:2],1
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/visualization/test_circuit_text_drawer.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/visualization/test_circuit_text_drawer.py,TestTextDrawerGatesInCircuit,test_text_crz$467,"def test_text_crz(self):
        """"""crz drawing.""""""
        expected = ""\n"".join(
            [
                ""                   "",
                ""q_0: |0> Rz(/2) "",
                ""        "",
                ""q_1: |0> Rz(/2) "",
                ""                  "",
                ""q_2: |0>"",
                ""                              "",
            ]
        )
        qr = QuantumRegister(3, ""q"")
        circuit = QuantumCircuit(qr)
        circuit.crz(pi / 2, qr[0], qr[1])
        circuit.crz(pi / 2, qr[2], qr[0])
        self.assertEqual(str(_text_circuit_drawer(circuit)), expected)","circuit.crz(pi / 2, qr[0], qr[1])","circuit.crz(pi / 2, *qr[:2])","iterable_zj[0], iterable_zj[1]",*qr[:2],*qr[:2],1
OpenPCDet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenPCDet/pcdet/datasets/augmentor/augmentor_utils.py,https://github.com/open-mmlab/OpenPCDet/tree/master/pcdet/datasets/augmentor/augmentor_utils.py,,global_scaling$74,"def global_scaling(gt_boxes, points, scale_range, return_scale=False):
    """"""
    Args:
        gt_boxes: (N, 7), [x, y, z, dx, dy, dz, heading]
        points: (M, 3 + C),
        scale_range: [min, max]
    Returns:
    """"""
    if scale_range[1] - scale_range[0] < 1e-3:
        return gt_boxes, points
    noise_scale = np.random.uniform(scale_range[0], scale_range[1])
    points[:, :3] *= noise_scale
    gt_boxes[:, :6] *= noise_scale
    if gt_boxes.shape[1] > 7:
        gt_boxes[:, 7:] *= noise_scale
        
    if return_scale:
        return gt_boxes, points, noise_scale
    return gt_boxes, points","np.random.uniform(scale_range[0], scale_range[1])",np.random.uniform(*scale_range[:2]),"iterable_zj[0], iterable_zj[1]",*scale_range[:2],*scale_range[:2],1
saleor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/saleor/saleor/graphql/product/types/channels.py,https://github.com/saleor/saleor/tree/master/saleor/graphql/product/types/channels.py,ProductChannelListing,calculate_margin_with_variants$172,"def calculate_margin_with_variants(variants):
            def calculate_margin_with_channel(channel):
                def calculate_margin_with_channel_listings(variant_channel_listings):
                    variant_channel_listings = list(
                        filter(None, variant_channel_listings)
                    )
                    if not variant_channel_listings:
                        return None

                    has_variants = True if len(variant_ids_channel_slug) > 0 else False
                    _purchase_cost, margin = get_product_costs_data(
                        variant_channel_listings, has_variants, root.currency
                    )
                    return Margin(margin[0], margin[1])

                variant_ids_channel_slug = [
                    (variant.id, channel.slug) for variant in variants
                ]
                return (
                    VariantChannelListingByVariantIdAndChannelSlugLoader(info.context)
                    .load_many(variant_ids_channel_slug)
                    .then(calculate_margin_with_channel_listings)
                )

            return channel.then(calculate_margin_with_channel)","Margin(margin[0], margin[1])",Margin(*margin[:2]),"iterable_zj[0], iterable_zj[1]",*margin[:2],*margin[:2],1
spotify-ripper,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spotify-ripper/spotify_ripper/ripper.py,https://github.com/hbashton/spotify-ripper/tree/master/spotify_ripper/ripper.py,Ripper,run$178,"def run(self):
        args = self.args

        # start event loop
        self.event_loop.start()

        # wait for main thread to login
        self.ripper_continue.wait()
        if self.abort.is_set():
            return

        # list of spotify URIs
        uris = args.uri

        def get_tracks_from_uri(uri):
            self.current_playlist = None
            self.current_album = None
            self.current_chart = None

            if isinstance(uri, list):
                return uri
            else:
                if (uri.startswith(""spotify:artist:"") and
                        (args.artist_album_type is not None or
                         args.artist_album_market is not None)):
                    album_uris = self.web.get_albums_with_filter(uri)
                    return itertools.chain(
                        *[self.load_link(album_uri) for
                          album_uri in album_uris])
                elif uri.startswith(""spotify:charts:""):
                    charts = self.web.get_charts(uri)
                    if charts is not None:
                        self.current_chart = charts
                        chart_uris = charts[""tracks""]
                        return itertools.chain(
                            *[self.load_link(chart_uri) for
                              chart_uri in chart_uris])
                    else:
                        return iter([])
                else:
                    return self.load_link(uri)

        # calculate total size and time
        all_tracks = []
        for uri in uris:
            tracks = list(get_tracks_from_uri(uri))

            # TODO: remove dependency on current_album, ...
            for idx, track in enumerate(tracks):

                # ignore local tracks
                if track.is_local:
                    continue

                audio_file = self.format_track_path(idx, track)
                all_tracks.append((track, audio_file))

        self.progress.calc_total(all_tracks)

        if self.progress.total_size > 0:
            print(
                ""Total Download Size: "" +
                format_size(self.progress.total_size))

        # create track iterator
        for uri in uris:
            if self.abort.is_set():
                break

            tracks = list(get_tracks_from_uri(uri))

            if args.playlist_sync and self.current_playlist:
                self.sync = Sync(args, self)
                self.sync.sync_playlist(self.current_playlist)

            # ripping loop
            for idx, track in enumerate(tracks):
                try:
                    self.check_stop_time()
                    self.skip.clear()

                    if self.abort.is_set():
                        break

                    print('Loading track...')
                    track.load()
                    if track.availability != 1 or track.is_local:
                        print(
                            Fore.RED + 'Track is not available, '
                                       'skipping...' + Fore.RESET)
                        self.post.log_failure(track)
                        continue

                    self.audio_file = self.format_track_path(idx, track)

                    if not args.overwrite and path_exists(self.audio_file):
                        if is_partial(self.audio_file, track):
                            print(""Overwriting partial file"")
                        else:
                            print(
                                Fore.YELLOW + ""Skipping "" +
                                track.link.uri + Fore.RESET)
                            print(Fore.CYAN + self.audio_file + Fore.RESET)
                            self.post.queue_remove_from_playlist(idx)
                            continue

                    self.session.player.load(track)
                    self.prepare_rip(idx, track)
                    self.session.player.play()

                    timeout_count = 0
                    while not self.end_of_track.is_set() or \
                            not self.rip_queue.empty():
                        try:
                            if self.abort.is_set() or self.skip.is_set():
                                break

                            rip_item = self.rip_queue.get(timeout=1)

                            if self.abort.is_set() or self.skip.is_set():
                                break

                            self.rip(self.session, rip_item[0],
                                     rip_item[1], rip_item[2])
                        except queue.Empty:
                            timeout_count += 1
                            if timeout_count > 60:
                                raise spotify.Error(""Timeout while ""
                                                    ""ripping track"")

                    if self.skip.is_set():
                        extra_line = """" if self.play_token_resume.is_set() \
                                        else ""\n""
                        print(extra_line + Fore.YELLOW +
                            ""User skipped track... "" + Fore.RESET)
                        self.session.player.play(False)
                        self.post.clean_up_partial()
                        self.post.log_failure(track)
                        self.end_of_track.clear()
                        self.progress.end_track(show_end=False)
                        self.ripping.clear()
                        continue

                    if self.abort.is_set():
                        self.session.player.play(False)
                        self.end_of_track.set()
                        self.post.clean_up_partial()
                        self.post.log_failure(track)
                        break

                    self.end_of_track.clear()

                    self.finish_rip(track)

                    # update id3v2 with metadata and embed front cover image
                    set_metadata_tags(args, self.audio_file, idx, track, self)

                    # make a note of the index and remove all the
                    # tracks from the playlist when everything is done
                    self.post.queue_remove_from_playlist(idx)

                except (spotify.Error, Exception) as e:
                    if isinstance(e, Exception):
                        print(Fore.RED + ""Spotify error detected"" + Fore.RESET)
                    print(str(e))
                    print(""Skipping to next track..."")
                    self.session.player.play(False)
                    self.post.clean_up_partial()
                    self.post.log_failure(track)
                    continue

            # create playlist m3u file if needed
            self.post.create_playlist_m3u(tracks)

            # create playlist wpl file if needed
            self.post.create_playlist_wpl(tracks)

            # actually removing the tracks from playlist
            self.post.remove_tracks_from_playlist()

            # remove libspotify's offline storage cache
            self.post.remove_offline_cache()

        # logout, we are done
        self.post.end_failure_log()
        self.post.print_summary()
        self.logout()
        self.stop_event_loop()
        self.finished.set()","self.rip(self.session, rip_item[0], rip_item[1], rip_item[2])","self.rip(self.session, *rip_item[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*rip_item[:3],*rip_item[:3],1
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/examples/advanced/pyglet_plotting.py,https://github.com/sympy/sympy/tree/master/examples/advanced/pyglet_plotting.py,,draw$173,"def draw():
                    """"""
                    Iterate through the calculated
                    vectors and draw them.
                    """"""
                    glBegin(GL_LINES)
                    for u in Gvl:
                        for v in u:
                            point = [[v[0][0], v[0][1], v[0][2]],
                                     [v[0][0] + v[1][0], v[0][1] + v[1][1], v[0][2] + v[1][2]]]
                            draw_arrow(point[0], point[1])
                    glEnd()","draw_arrow(point[0], point[1])",draw_arrow(*point[:2]),"iterable_zj[0], iterable_zj[1]",*point[:2],*point[:2],1
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/modeling/core.py,https://github.com/astropy/astropy/tree/master/astropy/modeling/core.py,,binary_operation$4229,"def binary_operation(binoperator, left, right):
    """"""
    Perform binary operation. Operands may be matching tuples of operands.
    """"""
    if isinstance(left, tuple) and isinstance(right, tuple):
        return tuple(binoperator(item[0], item[1]) for item in zip(left, right))
    return binoperator(left, right)","binoperator(item[0], item[1])",binoperator(*item[:2]),"iterable_zj[0], iterable_zj[1]",*item[:2],*item[:2],1
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/container/test_sharder.py,https://github.com/openstack/swift/tree/master/test/unit/container/test_sharder.py,TestSharder,test_misplaced_objects_deleted_and_updated$4320,"def test_misplaced_objects_deleted_and_updated(self):
        # setup
        broker = self._make_broker()
        broker.enable_sharding(next(self.ts_iter))

        shard_bounds = (('', 'here'), ('here', ''))
        root_shard_ranges = self._make_shard_ranges(
            shard_bounds, state=ShardRange.ACTIVE)
        expected_shard_dbs = []
        for sr in root_shard_ranges:
            db_hash = hash_path(sr.account, sr.container)
            expected_shard_dbs.append(
                os.path.join(self.tempdir, 'sda', 'containers', '0',
                             db_hash[-3:], db_hash, db_hash + '.db'))
        broker.merge_shard_ranges(root_shard_ranges)
        self.assertTrue(broker.set_sharding_state())

        ts_older_internal = self.ts_encoded()  # used later
        # put deleted objects into source
        objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(objects, broker.db_file)  # sanity check
        # pretend we cleaved all ranges - sharded state
        self.assertTrue(broker.set_sharded_state())

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        expected_stats = {'attempted': 1, 'success': 1, 'failure': 0,
                          'found': 1, 'placed': 2, 'unplaced': 0}
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check new misplaced objects were moved
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source db with older undeleted versions of same objects
        old_objects = [
            ['b', ts_older_internal, 2, 'text/plain', 'etag_b', 0, 0],
            ['x', ts_older_internal, 4, 'text/plain', 'etag_x', 0, 0]
        ]
        for obj in old_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(old_objects, broker.db_file)  # sanity check
        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check older misplaced objects were not merged to shard brokers
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # the destination shard dbs for misplaced objects may already exist so
        # check they are updated correctly when overwriting objects
        # update source db with newer deleted versions of same objects
        new_objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in new_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(new_objects, broker.db_file)  # sanity check
        shard_broker = ContainerBroker(
            expected_shard_dbs[0], account=root_shard_ranges[0].account,
            container=root_shard_ranges[0].container)
        # update one shard container with even newer version of object
        timestamps = [next(self.ts_iter) for i in range(7)]
        ts_newer = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[5])
        newer_object = ('b', ts_newer, 10, 'text/plain', 'etag_b', 0, 0)
        shard_broker.put_object(*newer_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check only the newer misplaced object was moved
        self._check_objects([newer_object], expected_shard_dbs[0])
        self._check_objects(new_objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has newer data
        # but older content-type and metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[4])
        update_object = ('b', ts_update, 20, 'text/ignored', 'etag_newer', 0,
                         0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[5])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # and content-type but newer metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/ignored', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # but newer content-type and metadata
        ts_update = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/newer', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/newer', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)","encode_timestamps(timestamps[1], timestamps[3], timestamps[5])",encode_timestamps(*timestamps[1:6:2]),"iterable_zj[1], iterable_zj[3], iterable_zj[5]",*timestamps[1:6:2],*timestamps[1:7:2],0
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/container/test_sharder.py,https://github.com/openstack/swift/tree/master/test/unit/container/test_sharder.py,TestSharder,test_misplaced_objects_deleted_and_updated$4320,"def test_misplaced_objects_deleted_and_updated(self):
        # setup
        broker = self._make_broker()
        broker.enable_sharding(next(self.ts_iter))

        shard_bounds = (('', 'here'), ('here', ''))
        root_shard_ranges = self._make_shard_ranges(
            shard_bounds, state=ShardRange.ACTIVE)
        expected_shard_dbs = []
        for sr in root_shard_ranges:
            db_hash = hash_path(sr.account, sr.container)
            expected_shard_dbs.append(
                os.path.join(self.tempdir, 'sda', 'containers', '0',
                             db_hash[-3:], db_hash, db_hash + '.db'))
        broker.merge_shard_ranges(root_shard_ranges)
        self.assertTrue(broker.set_sharding_state())

        ts_older_internal = self.ts_encoded()  # used later
        # put deleted objects into source
        objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(objects, broker.db_file)  # sanity check
        # pretend we cleaved all ranges - sharded state
        self.assertTrue(broker.set_sharded_state())

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        expected_stats = {'attempted': 1, 'success': 1, 'failure': 0,
                          'found': 1, 'placed': 2, 'unplaced': 0}
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check new misplaced objects were moved
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source db with older undeleted versions of same objects
        old_objects = [
            ['b', ts_older_internal, 2, 'text/plain', 'etag_b', 0, 0],
            ['x', ts_older_internal, 4, 'text/plain', 'etag_x', 0, 0]
        ]
        for obj in old_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(old_objects, broker.db_file)  # sanity check
        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check older misplaced objects were not merged to shard brokers
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # the destination shard dbs for misplaced objects may already exist so
        # check they are updated correctly when overwriting objects
        # update source db with newer deleted versions of same objects
        new_objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in new_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(new_objects, broker.db_file)  # sanity check
        shard_broker = ContainerBroker(
            expected_shard_dbs[0], account=root_shard_ranges[0].account,
            container=root_shard_ranges[0].container)
        # update one shard container with even newer version of object
        timestamps = [next(self.ts_iter) for i in range(7)]
        ts_newer = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[5])
        newer_object = ('b', ts_newer, 10, 'text/plain', 'etag_b', 0, 0)
        shard_broker.put_object(*newer_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check only the newer misplaced object was moved
        self._check_objects([newer_object], expected_shard_dbs[0])
        self._check_objects(new_objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has newer data
        # but older content-type and metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[4])
        update_object = ('b', ts_update, 20, 'text/ignored', 'etag_newer', 0,
                         0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[5])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # and content-type but newer metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/ignored', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # but newer content-type and metadata
        ts_update = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/newer', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/newer', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)","encode_timestamps(timestamps[2], timestamps[3], timestamps[4])",encode_timestamps(*timestamps[2:5]),"iterable_zj[2], iterable_zj[3], iterable_zj[4]",*timestamps[2:5],*timestamps[2:5],1
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/container/test_sharder.py,https://github.com/openstack/swift/tree/master/test/unit/container/test_sharder.py,TestSharder,test_misplaced_objects_deleted_and_updated$4320,"def test_misplaced_objects_deleted_and_updated(self):
        # setup
        broker = self._make_broker()
        broker.enable_sharding(next(self.ts_iter))

        shard_bounds = (('', 'here'), ('here', ''))
        root_shard_ranges = self._make_shard_ranges(
            shard_bounds, state=ShardRange.ACTIVE)
        expected_shard_dbs = []
        for sr in root_shard_ranges:
            db_hash = hash_path(sr.account, sr.container)
            expected_shard_dbs.append(
                os.path.join(self.tempdir, 'sda', 'containers', '0',
                             db_hash[-3:], db_hash, db_hash + '.db'))
        broker.merge_shard_ranges(root_shard_ranges)
        self.assertTrue(broker.set_sharding_state())

        ts_older_internal = self.ts_encoded()  # used later
        # put deleted objects into source
        objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(objects, broker.db_file)  # sanity check
        # pretend we cleaved all ranges - sharded state
        self.assertTrue(broker.set_sharded_state())

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        expected_stats = {'attempted': 1, 'success': 1, 'failure': 0,
                          'found': 1, 'placed': 2, 'unplaced': 0}
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check new misplaced objects were moved
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source db with older undeleted versions of same objects
        old_objects = [
            ['b', ts_older_internal, 2, 'text/plain', 'etag_b', 0, 0],
            ['x', ts_older_internal, 4, 'text/plain', 'etag_x', 0, 0]
        ]
        for obj in old_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(old_objects, broker.db_file)  # sanity check
        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check older misplaced objects were not merged to shard brokers
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # the destination shard dbs for misplaced objects may already exist so
        # check they are updated correctly when overwriting objects
        # update source db with newer deleted versions of same objects
        new_objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in new_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(new_objects, broker.db_file)  # sanity check
        shard_broker = ContainerBroker(
            expected_shard_dbs[0], account=root_shard_ranges[0].account,
            container=root_shard_ranges[0].container)
        # update one shard container with even newer version of object
        timestamps = [next(self.ts_iter) for i in range(7)]
        ts_newer = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[5])
        newer_object = ('b', ts_newer, 10, 'text/plain', 'etag_b', 0, 0)
        shard_broker.put_object(*newer_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check only the newer misplaced object was moved
        self._check_objects([newer_object], expected_shard_dbs[0])
        self._check_objects(new_objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has newer data
        # but older content-type and metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[4])
        update_object = ('b', ts_update, 20, 'text/ignored', 'etag_newer', 0,
                         0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[5])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # and content-type but newer metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/ignored', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # but newer content-type and metadata
        ts_update = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/newer', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/newer', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)","encode_timestamps(timestamps[2], timestamps[6], timestamps[6])","encode_timestamps(*timestamps[2], timestamps[6] cannot be sliced using the slice operator [:]. Instead, you can use indexing to directly access these elements as timestamps[2] and timestamps[6]., timestamps[6])","iterable_zj[2], iterable_zj[6]","*timestamps[2], timestamps[6] cannot be sliced using the slice operator [:]. Instead, you can use indexing to directly access these elements as timestamps[2] and timestamps[6].",*timestamps[2:10:4],0
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/container/test_sharder.py,https://github.com/openstack/swift/tree/master/test/unit/container/test_sharder.py,TestSharder,test_misplaced_objects_deleted_and_updated$4320,"def test_misplaced_objects_deleted_and_updated(self):
        # setup
        broker = self._make_broker()
        broker.enable_sharding(next(self.ts_iter))

        shard_bounds = (('', 'here'), ('here', ''))
        root_shard_ranges = self._make_shard_ranges(
            shard_bounds, state=ShardRange.ACTIVE)
        expected_shard_dbs = []
        for sr in root_shard_ranges:
            db_hash = hash_path(sr.account, sr.container)
            expected_shard_dbs.append(
                os.path.join(self.tempdir, 'sda', 'containers', '0',
                             db_hash[-3:], db_hash, db_hash + '.db'))
        broker.merge_shard_ranges(root_shard_ranges)
        self.assertTrue(broker.set_sharding_state())

        ts_older_internal = self.ts_encoded()  # used later
        # put deleted objects into source
        objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(objects, broker.db_file)  # sanity check
        # pretend we cleaved all ranges - sharded state
        self.assertTrue(broker.set_sharded_state())

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        expected_stats = {'attempted': 1, 'success': 1, 'failure': 0,
                          'found': 1, 'placed': 2, 'unplaced': 0}
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check new misplaced objects were moved
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source db with older undeleted versions of same objects
        old_objects = [
            ['b', ts_older_internal, 2, 'text/plain', 'etag_b', 0, 0],
            ['x', ts_older_internal, 4, 'text/plain', 'etag_x', 0, 0]
        ]
        for obj in old_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(old_objects, broker.db_file)  # sanity check
        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check older misplaced objects were not merged to shard brokers
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # the destination shard dbs for misplaced objects may already exist so
        # check they are updated correctly when overwriting objects
        # update source db with newer deleted versions of same objects
        new_objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in new_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(new_objects, broker.db_file)  # sanity check
        shard_broker = ContainerBroker(
            expected_shard_dbs[0], account=root_shard_ranges[0].account,
            container=root_shard_ranges[0].container)
        # update one shard container with even newer version of object
        timestamps = [next(self.ts_iter) for i in range(7)]
        ts_newer = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[5])
        newer_object = ('b', ts_newer, 10, 'text/plain', 'etag_b', 0, 0)
        shard_broker.put_object(*newer_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check only the newer misplaced object was moved
        self._check_objects([newer_object], expected_shard_dbs[0])
        self._check_objects(new_objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has newer data
        # but older content-type and metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[4])
        update_object = ('b', ts_update, 20, 'text/ignored', 'etag_newer', 0,
                         0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[5])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # and content-type but newer metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/ignored', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # but newer content-type and metadata
        ts_update = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/newer', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/newer', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)","encode_timestamps(timestamps[2], timestamps[6], timestamps[6])",Cannot refactor,"iterable_zj[6], iterable_zj[6]",,*timestamps[2:10:4],0
TensorNetwork,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorNetwork/tensornetwork/backends/pytorch/pytorch_tensornetwork_test.py,https://github.com/google/TensorNetwork/tree/master/tensornetwork/backends/pytorch/pytorch_tensornetwork_test.py,,test_dynamic_network_sizes_flatten_trace$78,"def test_dynamic_network_sizes_flatten_trace():

  def f(x, n):
    x_slice = x[..., :n]
    n1 = Node(x_slice, backend=""pytorch"")
    connect(n1[0], n1[2])
    connect(n1[1], n1[3])
    return contract(flatten_edges_between(n1, n1)).get_tensor()

  x = torch.ones((3, 4, 3, 4, 5))
  np.testing.assert_allclose(f(x, 2), np.ones((2,)) * 12)
  np.testing.assert_allclose(f(x, 3), np.ones((3,)) * 12)","connect(n1[0], n1[2])",connect(*n1[::2]),"iterable_zj[0], iterable_zj[2]",*n1[::2],*n1[:4:2],0
TensorNetwork,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorNetwork/tensornetwork/backends/pytorch/pytorch_tensornetwork_test.py,https://github.com/google/TensorNetwork/tree/master/tensornetwork/backends/pytorch/pytorch_tensornetwork_test.py,,test_dynamic_network_sizes_flatten_trace$78,"def test_dynamic_network_sizes_flatten_trace():

  def f(x, n):
    x_slice = x[..., :n]
    n1 = Node(x_slice, backend=""pytorch"")
    connect(n1[0], n1[2])
    connect(n1[1], n1[3])
    return contract(flatten_edges_between(n1, n1)).get_tensor()

  x = torch.ones((3, 4, 3, 4, 5))
  np.testing.assert_allclose(f(x, 2), np.ones((2,)) * 12)
  np.testing.assert_allclose(f(x, 3), np.ones((3,)) * 12)","connect(n1[1], n1[3])",connect(*n1[1:4:2]),"iterable_zj[1], iterable_zj[3]",*n1[1:4:2],*n1[1:5:2],0
OpenPCDet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenPCDet/pcdet/datasets/augmentor/augmentor_utils.py,https://github.com/open-mmlab/OpenPCDet/tree/master/pcdet/datasets/augmentor/augmentor_utils.py,,global_scaling_with_roi_boxes$94,"def global_scaling_with_roi_boxes(gt_boxes, roi_boxes, points, scale_range, return_scale=False):
    """"""
    Args:
        gt_boxes: (N, 7), [x, y, z, dx, dy, dz, heading]
        points: (M, 3 + C),
        scale_range: [min, max]
    Returns:
    """"""
    if scale_range[1] - scale_range[0] < 1e-3:
        return gt_boxes, points
    noise_scale = np.random.uniform(scale_range[0], scale_range[1])
    points[:, :3] *= noise_scale
    gt_boxes[:, :6] *= noise_scale
    roi_boxes[:,:, [0,1,2,3,4,5,7,8]] *= noise_scale
    if return_scale:
        return gt_boxes,roi_boxes, points, noise_scale
    return gt_boxes, roi_boxes, points","np.random.uniform(scale_range[0], scale_range[1])",np.random.uniform(*scale_range[:2]),"iterable_zj[0], iterable_zj[1]",*scale_range[:2],*scale_range[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/frontend/pytorch/test_forward.py,https://github.com/apache/tvm/tree/master/tests/python/frontend/pytorch/test_forward.py,,test_forward_take$3431,"def test_forward_take():
    """"""test_forward_take""""""
    torch.set_grad_enabled(False)

    class Take1(Module):
        def forward(self, *args):
            indices = torch.tensor([[0, 0], [1, 0]])
            if torch.cuda.is_available():
                indices = indices.cuda()
            return torch.take(args[0], indices)

    class Take2(Module):
        def forward(self, *args):
            return torch.take(args[0], args[1])

    input_data = torch.tensor([[1, 2], [3, 4]])
    verify_model(Take1().float().eval(), input_data=input_data)
    indices = torch.tensor([[0, 0], [1, 0]])
    verify_model(Take2().float().eval(), input_data=[input_data, indices])
    indices = torch.tensor([0, -1])
    verify_model(Take2().float().eval(), input_data=[input_data, indices])","torch.take(args[0], args[1])",torch.take(*args[:2]),"iterable_zj[0], iterable_zj[1]",*args[:2],*args[:2],1
Malt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Malt/Malt/GL/Texture.py,https://github.com/bnpr/Malt/tree/master/Malt/GL/Texture.py,Texture,__init__$9,"def __init__(self, resolution, internal_format=GL_RGB32F, data_format = None, data = NULL, 
        wrap=GL_CLAMP_TO_EDGE, min_filter=GL_LINEAR, mag_filter=GL_LINEAR, pixel_format=None, 
        build_mipmaps = False, anisotropy = False):
        
        self.resolution = resolution
        self.internal_format = internal_format
        self.format = pixel_format or internal_format_to_format(internal_format)
        self.data_format = data_format or internal_format_to_data_format(internal_format)
        self.channel_count = format_channels(self.format)
        self.channel_size = data_format_size(self.data_format)

        self.texture = gl_buffer(GL_INT, 1)
        glGenTextures(1, self.texture)

        glBindTexture(GL_TEXTURE_2D, self.texture[0])
        glTexImage2D(GL_TEXTURE_2D, 0, self.internal_format, resolution[0], resolution[1], 
            0, self.format, self.data_format, data)

        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, wrap)
        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, wrap)
        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, min_filter)
        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, mag_filter)
        
        if build_mipmaps:
            glGenerateMipmap(GL_TEXTURE_2D)
        if anisotropy:
            level = glGetFloatv(GL_MAX_TEXTURE_MAX_ANISOTROPY)
            glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAX_ANISOTROPY, level)

        glBindTexture(GL_TEXTURE_2D, 0)","glTexImage2D(GL_TEXTURE_2D, 0, self.internal_format, resolution[0], resolution[1], 0, self.format, self.data_format, data)","glTexImage2D(GL_TEXTURE_2D, 0, self.internal_format, *resolution[:2], 0, self.format, self.data_format, data)","iterable_zj[0], iterable_zj[1]",*resolution[:2],*resolution[:2],1
Lihang,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Lihang/CH15/unit_test.py,https://github.com/SmirkCao/Lihang/tree/master/CH15/unit_test.py,TestSVDMethods,test_e_15_6$234,"def test_e_15_6(self):
        A = np.array([[1, 0, 0, 0],
                      [0, 0, 0, 4],
                      [0, 3, 0, 0],
                      [0, 0, 0, 0],
                      [2, 0, 0, 0]])
        u, s, vh = np.linalg.svd(A)
        print(u)
        print(s)
        print(vh)

        print(u[:, 0], u[:, 1], vh[0], vh[1])
        print(np.dot(u[:, 0].reshape(-1, 1), vh[0].reshape(1, -1)))
        print(np.dot(u[:, 1].reshape(-1, 1), vh[1].reshape(1, -1)))
        # 
        print(40*""*""+""ATA Eig Vector""+40*""*"")
        print(np.linalg.eigh(np.dot(A.T, A)))
        print(40*""*""+""AAT Eig Vector""+40*""*"")
        print(np.linalg.eigh(np.dot(A, A.T)))","print(u[:, 0], u[:, 1], vh[0], vh[1])",Cannot refactor,"iterable_zj[:, 0], iterable_zj[:, 1]",,*vh[:2],0
sofgan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sofgan/train.py,https://github.com/apchenstu/sofgan/tree/master//train.py,,train$28,"def train(args, loader, generator, discriminator, g_optim, d_optim, g_ema, device):
    loader = sample_data(loader)

    start_iter = args.start_iter // get_world_size() // args.batch
    pbar = range(args.iter // get_world_size() // args.batch)

    if get_rank() == 0:
        pbar = tqdm(pbar, initial=start_iter, dynamic_ncols=True, smoothing=0.01)

    mean_path_length = 0

    seg_loss = torch.tensor(0.0, device=device)
    r1_loss = torch.tensor(0.0, device=device)
    path_loss = torch.tensor(0.0, device=device)
    path_lengths = torch.tensor(0.0, device=device)
    mean_path_length_avg, seg_loss_val, shift_loss_val = 0, 0, 0
    loss_dict = {}

    if args.distributed:
        g_module = generator.module
        d_module = discriminator.module
    else:
        g_module = generator
        d_module = discriminator

    accum = 0.5 ** (32 / (10 * 1000))


    sample_condition_img, sample_conditions, condition_img_color = random_condition_img(args.n_sample)

    if get_rank() == 0:
        os.makedirs(f'sample', exist_ok=True)
        os.makedirs(f'sample/{args.name}', exist_ok=True)
        os.makedirs(f'ckpts/{args.name}', exist_ok=True)
        if args.with_tensorboard:
            os.makedirs(f'tensorboard/{args.name}', exist_ok=True)
            writer = SummaryWriter(f'tensorboard/{args.name}')

    for idx in pbar:
        i = idx + start_iter

        if i > args.iter:
            print('Done!')
            break

        real_img, condition_img = next(loader)
        real_img = real_img.to(device)


        if args.condition_path is not None:
            condition_img = condition_img.to(device)
        else:
            condition_img = None


        requires_grad(generator, False)
        requires_grad(discriminator, True)

        noise = mixing_noise(args.batch, args.latent, args.mixing, device)
        fake_img, _, _, _ = generator(noise, condition_img=condition_img)
        if args.with_rgbs:
            condition_img_encoder = F.interpolate(condition_img, size=args.resolution, mode='nearest')
            real_img = torch.cat((real_img, condition_img_encoder), dim=1)
        fake_pred, _ = discriminator(fake_img)
        real_pred, real_pred_feat = discriminator(real_img)
        d_loss = d_logistic_loss(real_pred, fake_pred)

        loss_dict['d'] = d_loss
        loss_dict['real_score'] = real_pred.mean()
        loss_dict['fake_score'] = fake_pred.mean()

        discriminator.zero_grad()
        d_loss.backward()
        d_optim.step()

        d_regularize = i % args.d_reg_every == 0

        if d_regularize:
            real_img.requires_grad = True
            real_pred, _ = discriminator(real_img)
            r1_loss = d_r1_loss(real_pred, real_img)

            discriminator.zero_grad()
            (args.r1 / 2 * r1_loss * args.d_reg_every + 0 * real_pred[0]).backward()
            d_optim.step()

        loss_dict['r1'] = r1_loss

        requires_grad(generator, True)
        requires_grad(discriminator, False)

        noise = mixing_noise(args.batch, args.latent, args.mixing, device)
        fake_img, _, _,parsing_feature = generator(noise, condition_img=condition_img)

        fake_pred, fake_pred_feat = discriminator(fake_img)
        g_loss = g_nonsaturating_loss(fake_pred)
        loss_dict['g'] = g_loss

        loss_dict['seg'] = seg_loss
        loss_dict['shift_loss'] = seg_loss

        loss = g_loss
        generator.zero_grad()
        loss.backward()
        g_optim.step()

        requires_grad(generator, True)
        requires_grad(discriminator, False)
        g_regularize = i % args.g_reg_every == 0
        if g_regularize:
            path_batch_size = max(1, args.batch // args.path_batch_shrink)
            noise = mixing_noise(
                path_batch_size, args.latent, args.mixing, device
            )
            if args.condition_path is not None:
                condition_img = condition_img[range(path_batch_size)]
                condition_img.requires_grad = True


            fake_img, latents, _, _ = generator(noise, return_latents=True, condition_img=condition_img)

            path_loss, mean_path_length, path_lengths, isNaN = g_path_regularize(fake_img, latents, mean_path_length)

            generator.zero_grad()
            weighted_path_loss = args.g_reg_every * args.path_regularize  * path_loss

            if args.path_batch_shrink:
                weighted_path_loss += 0 * fake_img[0, 0, 0, 0]

            weighted_path_loss.backward()
            if not isNaN:
                g_optim.step()
            mean_path_length_avg = (
                    reduce_sum(mean_path_length).item() / get_world_size()
            )

        loss_dict['path'] = path_loss
        loss_dict['path_length'] = path_lengths.mean()

        accumulate(g_ema, g_module, accum)
        loss_reduced = reduce_loss_dict(loss_dict)

        d_loss_val = loss_reduced['d'].mean().item()
        g_loss_val = loss_reduced['g'].mean().item()
        r1_val = loss_reduced['r1'].mean().item()
        path_length_val = loss_reduced['path_length'].mean().item()

        if args.condition_path is not None and (0 == i % args.g_reg_every):
            seg_loss_val = loss_reduced['seg'].mean().item()
            shift_loss_val = loss_reduced['shift_loss'].mean().item()

        if get_rank() == 0:
            pbar.set_description(
                (
                    f'mean path: {mean_path_length_avg:.4f}'
                )
            )

            if args.with_tensorboard:
                writer.add_scalar('Loss/Generator', g_loss_val, i)
                writer.add_scalar('Loss/Discriminator', d_loss_val, i)
                writer.add_scalar('Loss/R1', r1_val, i)
                writer.add_scalar('Loss/Path Length', path_length_val, i)
                writer.add_scalar('Loss/mean path', mean_path_length_avg, i)


                if args.condition_path is not None:
                    writer.add_scalar('Loss/seg_img', seg_loss_val, i)
                    writer.add_scalar('Loss/shift_loss', shift_loss_val, i)

            steps = get_world_size() * args.batch * (1 + i)
            if steps % 100000 < get_world_size() * args.batch or (steps<1000 and steps%500==get_world_size() * args.batch):
                with torch.no_grad():
                    g_ema.eval()
                    samples,featuresMaps,parsing_features = [],[],[]
                    small_batch = args.n_sample // args.batch
                    if 0 != args.n_sample % args.batch:
                        small_batch += 1

                    # only condition change
                    rows = int(args.n_sample ** 0.5)
                    if args.condition_path is not None:
                        sample_z = mixing_noise(rows, args.latent, args.mixing, device)
                        sample_z = sample_z.unsqueeze(1).repeat(1,rows,1,1).view(args.n_sample,sample_z.shape[1],sample_z.shape[2])
                    else:
                        sample_z = mixing_noise(args.n_sample, args.latent, args.mixing, device)


                    for k in range(small_batch):

                        start,end = k* args.batch,(k+1)* args.batch
                        if k == small_batch-1:
                            end = sample_z.shape[0]

                        if args.condition_path is not None:
                            sample_condition_img_sub = sample_condition_img[start:end]
                            sample_condition_img_sub = random_affine(sample_condition_img_sub.clone(), Scale=0.0).to(device)
                        else:
                            sample_condition_img_sub = None

                        sample, _, _, _ = g_ema(sample_z[start:end], condition_img=sample_condition_img_sub)
                        samples.append(sample.cpu().detach())

                    samples = torch.cat(samples,dim=0)

                    nrow = int(args.n_sample ** 0.5)
                    c,h,w = samples.shape[-3:]
                    samples = samples.reshape(nrow,nrow,c,h,w).transpose(1,0).reshape(-1,c,h,w)
                    utils.save_image(
                        samples,
                        f'sample/{args.name}/{str(steps).zfill(6)}.png',
                        nrow=int(args.n_sample ** 0.5),
                        normalize=True,
                        range=(-1, 1),
                    )
                    if 0 ==i:
                        c, h, w = condition_img_color.shape[-3:]
                        condition_img_color = condition_img_color.reshape(nrow, nrow, c, h, w).transpose(1, 0).reshape(-1, c, h, w)
                        utils.save_image(
                            condition_img_color,
                            f'sample/{args.name}/seg_vis.png',
                            nrow=nrow,
                            normalize=True,
                            range=(-1, 1),
                        )


            if (steps+get_world_size()*args.batch) % 100000 < get_world_size()*args.batch and steps != args.start_iter:
                torch.save(
                    {
                        'g': g_module.state_dict(),
                        'd': d_module.state_dict(),
                        'g_ema': g_ema.state_dict(),
                        # 'g_optim': g_optim.state_dict(),
                        # 'd_optim': d_optim.state_dict(),
                    },
                    f'ckpts/{args.name}/{str(steps).zfill(6)}.pt',
                )","sample_z.unsqueeze(1).repeat(1, rows, 1, 1).view(args.n_sample, sample_z.shape[1], sample_z.shape[2])","sample_z.unsqueeze(1).repeat(1, rows, 1, 1).view(args.n_sample, *sample_z.shape[1:3])","iterable_zj[1], iterable_zj[2]",*sample_z.shape[1:3],*sample_z.shape[1:3],1
ezdxf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/render/dim_base.py,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/render/dim_base.py,Geometry,add_line$849,"def add_line(
        self,
        start: Vec2,
        end: Vec2,
        dxfattribs,
        remove_hidden_lines=False,
    ) -> None:
        """"""Add a LINE entity to the geometry layout. Removes parts of the line
        hidden by dimension text if `remove_hidden_lines` is True.

        Args:
            start: start point of line
            end: end point of line
            dxfattribs: additional or overridden DXF attributes
            remove_hidden_lines: removes parts of the line hidden by dimension
                text if ``True``

        """"""

        def add_line_to_block(start, end):
            # LINE is handled like an OCS entity !?
            self.layout.add_line(
                to_ocs(Vec3(start)).vec2,
                to_ocs(Vec3(end)).vec2,
                dxfattribs=dxfattribs,
            )

        def order(a: Vec2, b: Vec2) -> tuple[Vec2, Vec2]:
            if (start - a).magnitude < (start - b).magnitude:
                return a, b
            else:
                return b, a

        to_ocs = self.ucs.to_ocs
        if remove_hidden_lines and self.has_text_box:
            text_box = self._text_box
            start_inside = int(text_box.is_inside(start))
            end_inside = int(text_box.is_inside(end))
            inside = start_inside + end_inside
            if inside == 2:  # start and end inside text_box
                return  # do not draw line
            elif inside == 1:  # one point inside text_box or on a border line
                intersection_points = text_box.intersect(
                    ConstructionLine(start, end)
                )
                if len(intersection_points) == 1:
                    # one point inside one point outside -> one intersection point
                    p1 = intersection_points[0]
                else:
                    # second point on a text box border line
                    p1, _ = order(*intersection_points)
                p2 = start if end_inside else end
                add_line_to_block(p1, p2)
                return
            else:
                intersection_points = text_box.intersect(
                    ConstructionLine(start, end)
                )
                if len(intersection_points) == 2:
                    # sort intersection points by distance to start point
                    p1, p2 = order(
                        intersection_points[0], intersection_points[1]
                    )
                    # line[start-p1] - gap - line[p2-end]
                    add_line_to_block(start, p1)
                    add_line_to_block(p2, end)
                    return
                # else: fall through
        add_line_to_block(start, end)","order(intersection_points[0], intersection_points[1])",order(*intersection_points[:2]),"iterable_zj[0], iterable_zj[1]",*intersection_points[:2],*intersection_points[:2],1
torchdrug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torchdrug/torchdrug/models/schnet.py,https://github.com/DeepGraphLearning/torchdrug/tree/master/torchdrug/models/schnet.py,SchNet,__init__$30,"def __init__(self, input_dim, hidden_dims, edge_input_dim=None, cutoff=5, num_gaussian=100, short_cut=True,
                 batch_norm=False, activation=""shifted_softplus"", concat_hidden=False):
        super(SchNet, self).__init__()

        if not isinstance(hidden_dims, Sequence):
            hidden_dims = [hidden_dims]
        self.input_dim = input_dim
        self.output_dim = hidden_dims[-1] * (len(hidden_dims) if concat_hidden else 1)
        self.dims = [input_dim] + list(hidden_dims)
        self.short_cut = short_cut
        self.concat_hidden = concat_hidden

        self.layers = nn.ModuleList()
        for i in range(len(self.dims) - 1):
            self.layers.append(layers.ContinuousFilterConv(self.dims[i], self.dims[i + 1], edge_input_dim, None, cutoff,
                                                           num_gaussian, batch_norm, activation))

        self.readout = layers.SumReadout()","layers.ContinuousFilterConv(self.dims[i], self.dims[i + 1], edge_input_dim, None, cutoff, num_gaussian, batch_norm, activation)","layers.ContinuousFilterConv(*self.dims[i:i + 2], edge_input_dim, None, cutoff, num_gaussian, batch_norm, activation)","iterable_zj[i], iterable_zj[i + 1]",*self.dims[i:i+2],*self.dims[i:i + 2],1
flexx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flexx/flexxamples/demos/colab_painting.py,https://github.com/flexxui/flexx/tree/master/flexxamples/demos/colab_painting.py,ColabPaintingView,add_paint_to_canvas$106,"def add_paint_to_canvas(self, pos, color):
        """""" Actually draw a dot on the canvas.
        """"""
        self._ctx.globalAlpha = 0.8
        self._ctx.beginPath()
        self._ctx.fillStyle = color
        self._ctx.arc(pos[0], pos[1], 5, 0, 6.2831)
        self._ctx.fill()","self._ctx.arc(pos[0], pos[1], 5, 0, 6.2831)","self._ctx.arc(*pos[:2], 5, 0, 6.2831)","iterable_zj[0], iterable_zj[1]",*pos[:2],*pos[:2],1
CrackMapExec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CrackMapExec/cme/protocols/mssql.py,https://github.com/byt3bl33d3r/CrackMapExec/tree/master/cme/protocols/mssql.py,mssql,get_file$368,"def get_file(self):
        self.logger.info('Copy {} to {}'.format(self.args.get_file[0], self.args.get_file[1]))
        try:
            exec_method = MSSQLEXEC(self.conn)
            exec_method.get_file(self.args.get_file[0], self.args.get_file[1])
            self.logger.success('File {} was transferred to {}'.format(self.args.get_file[0], self.args.get_file[1]))
        except Exception as e:
            self.logger.error('Error reading file {}: {}'.format(self.args.get_file[0], e))","'Copy {} to {}'.format(self.args.get_file[0], self.args.get_file[1])",'Copy {} to {}'.format(*self.args.get_file[:2]),"iterable_zj[0], iterable_zj[1]",*self.args.get_file[:2],*self.args.get_file[:2],1
CrackMapExec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CrackMapExec/cme/protocols/mssql.py,https://github.com/byt3bl33d3r/CrackMapExec/tree/master/cme/protocols/mssql.py,mssql,get_file$368,"def get_file(self):
        self.logger.info('Copy {} to {}'.format(self.args.get_file[0], self.args.get_file[1]))
        try:
            exec_method = MSSQLEXEC(self.conn)
            exec_method.get_file(self.args.get_file[0], self.args.get_file[1])
            self.logger.success('File {} was transferred to {}'.format(self.args.get_file[0], self.args.get_file[1]))
        except Exception as e:
            self.logger.error('Error reading file {}: {}'.format(self.args.get_file[0], e))","exec_method.get_file(self.args.get_file[0], self.args.get_file[1])",exec_method.get_file(*self.args.get_file[:2]),"iterable_zj[0], iterable_zj[1]",*self.args.get_file[:2],*self.args.get_file[:2],1
CrackMapExec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CrackMapExec/cme/protocols/mssql.py,https://github.com/byt3bl33d3r/CrackMapExec/tree/master/cme/protocols/mssql.py,mssql,get_file$368,"def get_file(self):
        self.logger.info('Copy {} to {}'.format(self.args.get_file[0], self.args.get_file[1]))
        try:
            exec_method = MSSQLEXEC(self.conn)
            exec_method.get_file(self.args.get_file[0], self.args.get_file[1])
            self.logger.success('File {} was transferred to {}'.format(self.args.get_file[0], self.args.get_file[1]))
        except Exception as e:
            self.logger.error('Error reading file {}: {}'.format(self.args.get_file[0], e))","'File {} was transferred to {}'.format(self.args.get_file[0], self.args.get_file[1])",'File {} was transferred to {}'.format(*self.args.get_file[:2]),"iterable_zj[0], iterable_zj[1]",*self.args.get_file[:2],*self.args.get_file[:2],1
hypertools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hypertools/tests/test_align.py,https://github.com/ContextLab/hypertools/tree/master/tests/test_align.py,,test_align_geo$47,"def test_align_geo():
    aligned = align(geo)
    assert np.allclose(aligned[0], aligned[1])","np.allclose(aligned[0], aligned[1])",np.allclose(*aligned[:2]),"iterable_zj[0], iterable_zj[1]",*aligned[:2],*aligned[:2],1
great_expectations,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/great_expectations/tests/data_context/fixtures/post_init_project_v0.8.0_A/data/bob-ross/cluster-paintings.py,https://github.com/great-expectations/great_expectations/tree/master/tests/data_context/fixtures/post_init_project_v0.8.0_A/data/bob-ross/cluster-paintings.py,,main$16,"def main():
    # load data into vectors of 1s and 0s for each tag
    with open(""elements-by-episode.csv"") as csvfile:
        reader = csv.reader(csvfile)
        reader.next()  # skip header
        data = []
        for row in reader:
            data.append(
                map(lambda x: int(x), row[2:])
            )  # exclude EPISODE and TITLE columns

    # convert to numpy matrix
    matrix = np.array(data)

    # remove columns that have been tagged less than 5 times
    columns_to_remove = []
    for col in range(np.shape(matrix)[1]):
        if sum(matrix[:, col]) <= 5:
            columns_to_remove.append(col)
    matrix = np.delete(matrix, columns_to_remove, axis=1)

    # normalize according to stddev
    whitened = whiten(matrix)
    output = kmeans(whitened, 10)

    print(""episode"", ""distance"", ""cluster"")

    # determine distance between each of 403 vectors and each centroid, find closest neighbor
    for i, v in enumerate(whitened):

        # distance between centroid 0 and feature vector
        distance = math.sqrt(sum((v - output[0][0]) ** 2))

        # group is the centroid it is closest to so far, set initially to centroid 0
        group = 0
        closest_match = (distance, group)

        # test the vector i against the 10 centroids, find nearest neighbor
        for x in range(0, 10):
            dist_x = math.sqrt(sum((v - output[0][x]) ** 2))
            if dist_x < closest_match[0]:
                closest_match = (dist_x, x)

        print(i + 1, closest_match[0], closest_match[1])","print(i + 1, closest_match[0], closest_match[1])","print(i + 1, *closest_match[:2])","iterable_zj[0], iterable_zj[1]",*closest_match[:2],*closest_match[:2],1
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2016/eola/chapter6.py,https://github.com/3b1b/videos/tree/master/_2016/eola/chapter6.py,ShowAdditivityProperty,construct$1801,"def construct(self):
        v = Vector([2, -1])
        w = Vector([1, 2])
        v.set_color(YELLOW)
        w.set_color(MAROON_B)
        sum_vect = Vector(v.get_end()+w.get_end(), color = PINK)
        form = Tex(
            ""A("",
            ""\\vec{\\textbf{v}}"",
            ""+"",
            ""\\vec{\\textbf{w}}"",
            "")"",
            ""=A"",
            ""\\vec{\\textbf{v}}"",
            ""+A"",
            ""\\vec{\\textbf{w}}"",
        )
        form.to_corner(UP+RIGHT)
        VMobject(form[1], form[6]).set_color(YELLOW)
        VMobject(form[3], form[8]).set_color(MAROON_B)
        initial_sum = VMobject(*form[1:4])
        transformer = VMobject(form[0], form[4])
        final_sum = VMobject(*form[5:])
        form_rect = BackgroundRectangle(form)

        self.add(form_rect)
        self.add_vector(v, animate = True)
        self.add_vector(w, animate = True)
        w_copy = w.copy()
        self.play(w_copy.shift, v.get_end())
        self.add_vector(sum_vect, animate = True)
        self.play(
            Write(initial_sum),
            FadeOut(w_copy)
        )
        self.add_foreground_mobject(form_rect, initial_sum)
        self.apply_transposed_matrix(
            self.t_matrix,
            added_anims = [Write(transformer)]
        )
        self.wait()
        self.play(w.copy().shift, v.get_end())
        self.play(Write(final_sum))
        self.wait()","VMobject(form[0], form[4])",VMobject(*form[0:5:4]),"iterable_zj[0], iterable_zj[4]",*form[0:5:4],*form[:8:4],0
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2016/eola/chapter6.py,https://github.com/3b1b/videos/tree/master/_2016/eola/chapter6.py,ShowAdditivityProperty,construct$1801,"def construct(self):
        v = Vector([2, -1])
        w = Vector([1, 2])
        v.set_color(YELLOW)
        w.set_color(MAROON_B)
        sum_vect = Vector(v.get_end()+w.get_end(), color = PINK)
        form = Tex(
            ""A("",
            ""\\vec{\\textbf{v}}"",
            ""+"",
            ""\\vec{\\textbf{w}}"",
            "")"",
            ""=A"",
            ""\\vec{\\textbf{v}}"",
            ""+A"",
            ""\\vec{\\textbf{w}}"",
        )
        form.to_corner(UP+RIGHT)
        VMobject(form[1], form[6]).set_color(YELLOW)
        VMobject(form[3], form[8]).set_color(MAROON_B)
        initial_sum = VMobject(*form[1:4])
        transformer = VMobject(form[0], form[4])
        final_sum = VMobject(*form[5:])
        form_rect = BackgroundRectangle(form)

        self.add(form_rect)
        self.add_vector(v, animate = True)
        self.add_vector(w, animate = True)
        w_copy = w.copy()
        self.play(w_copy.shift, v.get_end())
        self.add_vector(sum_vect, animate = True)
        self.play(
            Write(initial_sum),
            FadeOut(w_copy)
        )
        self.add_foreground_mobject(form_rect, initial_sum)
        self.apply_transposed_matrix(
            self.t_matrix,
            added_anims = [Write(transformer)]
        )
        self.wait()
        self.play(w.copy().shift, v.get_end())
        self.play(Write(final_sum))
        self.wait()","VMobject(form[1], form[6])","VMobject(*form[1], form[6] can be obtained using the slice operator as form[1:7:5])","iterable_zj[1], iterable_zj[6]","*form[1], form[6] can be obtained using the slice operator as form[1:7:5]",*form[1:11:5],0
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2016/eola/chapter6.py,https://github.com/3b1b/videos/tree/master/_2016/eola/chapter6.py,ShowAdditivityProperty,construct$1801,"def construct(self):
        v = Vector([2, -1])
        w = Vector([1, 2])
        v.set_color(YELLOW)
        w.set_color(MAROON_B)
        sum_vect = Vector(v.get_end()+w.get_end(), color = PINK)
        form = Tex(
            ""A("",
            ""\\vec{\\textbf{v}}"",
            ""+"",
            ""\\vec{\\textbf{w}}"",
            "")"",
            ""=A"",
            ""\\vec{\\textbf{v}}"",
            ""+A"",
            ""\\vec{\\textbf{w}}"",
        )
        form.to_corner(UP+RIGHT)
        VMobject(form[1], form[6]).set_color(YELLOW)
        VMobject(form[3], form[8]).set_color(MAROON_B)
        initial_sum = VMobject(*form[1:4])
        transformer = VMobject(form[0], form[4])
        final_sum = VMobject(*form[5:])
        form_rect = BackgroundRectangle(form)

        self.add(form_rect)
        self.add_vector(v, animate = True)
        self.add_vector(w, animate = True)
        w_copy = w.copy()
        self.play(w_copy.shift, v.get_end())
        self.add_vector(sum_vect, animate = True)
        self.play(
            Write(initial_sum),
            FadeOut(w_copy)
        )
        self.add_foreground_mobject(form_rect, initial_sum)
        self.apply_transposed_matrix(
            self.t_matrix,
            added_anims = [Write(transformer)]
        )
        self.wait()
        self.play(w.copy().shift, v.get_end())
        self.play(Write(final_sum))
        self.wait()","VMobject(form[3], form[8])","VMobject(*form[3], form[8] cannot be sliced using the slice operator [:] as they are not consecutive elements. Instead, we can use the indexing operator [] to directly access these elements as form[3] and form[8].)","iterable_zj[3], iterable_zj[8]","*form[3], form[8] cannot be sliced using the slice operator [:] as they are not consecutive elements. Instead, we can use the indexing operator [] to directly access these elements as form[3] and form[8].",*form[3:13:5],0
adversarial-robustness-toolbox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adversarial-robustness-toolbox/examples/get_started_fasterrcnn.py,https://github.com/Trusted-AI/adversarial-robustness-toolbox/tree/master/examples/get_started_fasterrcnn.py,,plot_image_with_boxes$154,"def plot_image_with_boxes(img, boxes, pred_cls):
    text_size = 5
    text_th = 5
    rect_th = 6

    for i in range(len(boxes)):
        # Draw Rectangle with the coordinates
        cv2.rectangle(img, boxes[i][0], boxes[i][1], color=(0, 255, 0), thickness=rect_th)

        # Write the prediction class
        cv2.putText(img, pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0, 255, 0), thickness=text_th)

    plt.axis(""off"")
    plt.imshow(img.astype(np.uint8), interpolation=""nearest"")
    plt.show()","cv2.rectangle(img, boxes[i][0], boxes[i][1], color=(0, 255, 0), thickness=rect_th)","cv2.rectangle(img, *boxes[i][:2], color=(0, 255, 0), thickness=rect_th)","iterable_zj[0], iterable_zj[1]",*boxes[i][:2],*boxes[i][:2],1
HanLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HanLP/hanlp/transform/conll_tf.py,https://github.com/hankcs/HanLP/tree/master/hanlp/transform/conll_tf.py,CoNLL_SDP_Transform,batched_inputs_to_batches$734,"def batched_inputs_to_batches(self, corpus, indices, shuffle):
        use_pos = self.use_pos
        raw_batch = [[], [], [], []] if use_pos else [[], [], []]
        max_len = len(max([corpus[i] for i in indices], key=len))
        for idx in indices:
            arc = np.zeros((max_len, max_len), dtype=np.bool)
            rel = np.zeros((max_len, max_len), dtype=np.int64)
            for b in raw_batch[:2]:
                b.append([])
            for m, cells in enumerate(corpus[idx]):
                if use_pos:
                    for b, c, v in zip(raw_batch, cells,
                                       [self.form_vocab, self.cpos_vocab]):
                        b[-1].append(v.get_idx_without_add(c))
                else:
                    for b, c, v in zip(raw_batch, cells,
                                       [self.form_vocab]):
                        b[-1].append(v.get_idx_without_add(c))
                for n, r in zip(cells[-2], cells[-1]):
                    arc[m, n] = True
                    rid = self.rel_vocab.get_idx_without_add(r)
                    if rid is None:
                        logger.warning(f'Relation OOV: {r} not exists in train')
                        continue
                    rel[m, n] = rid
            raw_batch[-2].append(arc)
            raw_batch[-1].append(rel)
        batch = []
        for b, v in zip(raw_batch, [self.form_vocab, self.cpos_vocab]):
            b = tf.keras.preprocessing.sequence.pad_sequences(b, padding='post',
                                                              value=v.safe_pad_token_idx,
                                                              dtype='int64')
            batch.append(b)
        batch += raw_batch[2:]
        assert len(batch) == 4
        yield (batch[0], batch[1]), (batch[2], batch[3])","zip(cells[-2], cells[-1])",zip(*cells[-2:]),"iterable_zj[-2], iterable_zj[-1]",*cells[-2:],*cells[-2:0],0
DeepLogo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepLogo/logo_detection.py,https://github.com/satojkovic/DeepLogo/tree/master//logo_detection.py,,run_inference_for_single_image$42,"def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[1], image.shape[2])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: image})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.int64)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict","utils_ops.reframe_box_masks_to_image_masks(detection_masks, detection_boxes, image.shape[1], image.shape[2])","utils_ops.reframe_box_masks_to_image_masks(detection_masks, detection_boxes, *image.shape[1:3])","iterable_zj[1], iterable_zj[2]",*image.shape[1:3],*image.shape[1:3],1
TensorNetwork,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorNetwork/tensornetwork/block_sparse/linalg_test.py,https://github.com/google/TensorNetwork/tree/master/tensornetwork/block_sparse/linalg_test.py,,test_eye$509,"def test_eye(dtype, num_charges, D):
  charge = BaseCharge(
      np.random.randint(-5, 6, (D, num_charges), dtype=np.int16),
      charge_types=[U1Charge] * num_charges)
  flow = False
  index = Index(charge, flow)
  A = eye(index, dtype=dtype)
  blocks, _, shapes = _find_diagonal_sparse_blocks(A.flat_charges, A.flat_flows,
                                                   1)
  for n, block in enumerate(blocks):
    t = np.reshape(A.data[block], shapes[:, n])
    np.testing.assert_almost_equal(t, np.eye(t.shape[0], t.shape[1]))","np.eye(t.shape[0], t.shape[1])",np.eye(*t.shape[:2]),"iterable_zj[0], iterable_zj[1]",*t.shape[:2],*t.shape[:2],1
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/core/bbox/box_np_ops.py,https://github.com/poodarchu/Det3D/tree/master/det3d/core/bbox/box_np_ops.py,,create_anchors_3d_range$733,"def create_anchors_3d_range(
    feature_size,
    anchor_range,
    sizes=[1.6, 3.9, 1.56],
    rotations=[0, np.pi / 2],
    velocities=None,
    dtype=np.float32,
):
    """"""
    Args:
        feature_size: list [D, H, W](zyx)
        sizes: [N, 3] list of list or array, size of anchors, xyz
        rotations: len(stride) num Reference
        velocities: ref velo along x and y axis.

    Returns:
        anchors: [*feature_size, num_sizes, num_rots, 9] tensor.
    """"""
    anchor_range = np.array(anchor_range, dtype)
    stride = (anchor_range[3] - anchor_range[0]) / feature_size[2]

    z_centers = np.linspace(
        anchor_range[2], anchor_range[5], feature_size[0], dtype=dtype
    )
    y_centers = (
        np.linspace(
            anchor_range[1],
            anchor_range[4],
            feature_size[1],
            endpoint=False,
            dtype=dtype,
        )
        + stride / 2
    )
    x_centers = (
        np.linspace(
            anchor_range[0],
            anchor_range[3],
            feature_size[2],
            endpoint=False,
            dtype=dtype,
        )
        + stride / 2
    )
    rotations = np.array(rotations, dtype=dtype)
    sizes = np.reshape(np.array(sizes, dtype=dtype), [-1, 3])

    if velocities is not None:
        velocities = np.array(velocities, dtype=dtype).reshape([-1, 2])
        combines = np.hstack([sizes, velocities]).reshape([-1, 5])
    else:
        combines = sizes

    rets = np.meshgrid(x_centers, y_centers, z_centers, rotations, indexing=""ij"")

    tile_shape = [1] * 5
    tile_shape[-2] = int(sizes.shape[0])
    for i in range(len(rets)):
        rets[i] = np.tile(rets[i][..., np.newaxis, :], tile_shape)
        rets[i] = rets[i][..., np.newaxis]  # for concat
    # sizes = np.reshape(sizes, [1, 1, 1, -1, 1, 3])
    combines = np.reshape(combines, [1, 1, 1, -1, 1, combines.shape[-1]])
    tile_size_shape = list(rets[0].shape)
    tile_size_shape[3] = 1
    # sizes = np.tile(sizes, tile_size_shape)
    combines = np.tile(combines, tile_size_shape)

    # rets.insert(3, sizes)
    rets.insert(3, combines)

    ret = np.concatenate(rets, axis=-1)

    return np.transpose(ret, [2, 1, 0, 3, 4, 5])","np.linspace(anchor_range[2], anchor_range[5], feature_size[0], dtype=dtype)","np.linspace(*anchor_range[2], anchor_range[5] cannot be sliced using the slice operator [:]. Instead, you can use the indexing operator [] to directly access these elements as anchor_range[2] and anchor_range[5]., feature_size[0], dtype=dtype)","iterable_zj[2], iterable_zj[5]","*anchor_range[2], anchor_range[5] cannot be sliced using the slice operator [:]. Instead, you can use the indexing operator [] to directly access these elements as anchor_range[2] and anchor_range[5].",*anchor_range[2:8:3],0
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/core/bbox/box_np_ops.py,https://github.com/poodarchu/Det3D/tree/master/det3d/core/bbox/box_np_ops.py,,create_anchors_3d_range$733,"def create_anchors_3d_range(
    feature_size,
    anchor_range,
    sizes=[1.6, 3.9, 1.56],
    rotations=[0, np.pi / 2],
    velocities=None,
    dtype=np.float32,
):
    """"""
    Args:
        feature_size: list [D, H, W](zyx)
        sizes: [N, 3] list of list or array, size of anchors, xyz
        rotations: len(stride) num Reference
        velocities: ref velo along x and y axis.

    Returns:
        anchors: [*feature_size, num_sizes, num_rots, 9] tensor.
    """"""
    anchor_range = np.array(anchor_range, dtype)
    stride = (anchor_range[3] - anchor_range[0]) / feature_size[2]

    z_centers = np.linspace(
        anchor_range[2], anchor_range[5], feature_size[0], dtype=dtype
    )
    y_centers = (
        np.linspace(
            anchor_range[1],
            anchor_range[4],
            feature_size[1],
            endpoint=False,
            dtype=dtype,
        )
        + stride / 2
    )
    x_centers = (
        np.linspace(
            anchor_range[0],
            anchor_range[3],
            feature_size[2],
            endpoint=False,
            dtype=dtype,
        )
        + stride / 2
    )
    rotations = np.array(rotations, dtype=dtype)
    sizes = np.reshape(np.array(sizes, dtype=dtype), [-1, 3])

    if velocities is not None:
        velocities = np.array(velocities, dtype=dtype).reshape([-1, 2])
        combines = np.hstack([sizes, velocities]).reshape([-1, 5])
    else:
        combines = sizes

    rets = np.meshgrid(x_centers, y_centers, z_centers, rotations, indexing=""ij"")

    tile_shape = [1] * 5
    tile_shape[-2] = int(sizes.shape[0])
    for i in range(len(rets)):
        rets[i] = np.tile(rets[i][..., np.newaxis, :], tile_shape)
        rets[i] = rets[i][..., np.newaxis]  # for concat
    # sizes = np.reshape(sizes, [1, 1, 1, -1, 1, 3])
    combines = np.reshape(combines, [1, 1, 1, -1, 1, combines.shape[-1]])
    tile_size_shape = list(rets[0].shape)
    tile_size_shape[3] = 1
    # sizes = np.tile(sizes, tile_size_shape)
    combines = np.tile(combines, tile_size_shape)

    # rets.insert(3, sizes)
    rets.insert(3, combines)

    ret = np.concatenate(rets, axis=-1)

    return np.transpose(ret, [2, 1, 0, 3, 4, 5])","np.linspace(anchor_range[1], anchor_range[4], feature_size[1], endpoint=False, dtype=dtype)","np.linspace(*anchor_range[1:5:3], feature_size[1], endpoint=False, dtype=dtype)","iterable_zj[1], iterable_zj[4]",*anchor_range[1:5:3],*anchor_range[1:7:3],0
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/core/bbox/box_np_ops.py,https://github.com/poodarchu/Det3D/tree/master/det3d/core/bbox/box_np_ops.py,,create_anchors_3d_range$733,"def create_anchors_3d_range(
    feature_size,
    anchor_range,
    sizes=[1.6, 3.9, 1.56],
    rotations=[0, np.pi / 2],
    velocities=None,
    dtype=np.float32,
):
    """"""
    Args:
        feature_size: list [D, H, W](zyx)
        sizes: [N, 3] list of list or array, size of anchors, xyz
        rotations: len(stride) num Reference
        velocities: ref velo along x and y axis.

    Returns:
        anchors: [*feature_size, num_sizes, num_rots, 9] tensor.
    """"""
    anchor_range = np.array(anchor_range, dtype)
    stride = (anchor_range[3] - anchor_range[0]) / feature_size[2]

    z_centers = np.linspace(
        anchor_range[2], anchor_range[5], feature_size[0], dtype=dtype
    )
    y_centers = (
        np.linspace(
            anchor_range[1],
            anchor_range[4],
            feature_size[1],
            endpoint=False,
            dtype=dtype,
        )
        + stride / 2
    )
    x_centers = (
        np.linspace(
            anchor_range[0],
            anchor_range[3],
            feature_size[2],
            endpoint=False,
            dtype=dtype,
        )
        + stride / 2
    )
    rotations = np.array(rotations, dtype=dtype)
    sizes = np.reshape(np.array(sizes, dtype=dtype), [-1, 3])

    if velocities is not None:
        velocities = np.array(velocities, dtype=dtype).reshape([-1, 2])
        combines = np.hstack([sizes, velocities]).reshape([-1, 5])
    else:
        combines = sizes

    rets = np.meshgrid(x_centers, y_centers, z_centers, rotations, indexing=""ij"")

    tile_shape = [1] * 5
    tile_shape[-2] = int(sizes.shape[0])
    for i in range(len(rets)):
        rets[i] = np.tile(rets[i][..., np.newaxis, :], tile_shape)
        rets[i] = rets[i][..., np.newaxis]  # for concat
    # sizes = np.reshape(sizes, [1, 1, 1, -1, 1, 3])
    combines = np.reshape(combines, [1, 1, 1, -1, 1, combines.shape[-1]])
    tile_size_shape = list(rets[0].shape)
    tile_size_shape[3] = 1
    # sizes = np.tile(sizes, tile_size_shape)
    combines = np.tile(combines, tile_size_shape)

    # rets.insert(3, sizes)
    rets.insert(3, combines)

    ret = np.concatenate(rets, axis=-1)

    return np.transpose(ret, [2, 1, 0, 3, 4, 5])","np.linspace(anchor_range[0], anchor_range[3], feature_size[2], endpoint=False, dtype=dtype)","np.linspace(*anchor_range[0], anchor_range[3], feature_size[2], endpoint=False, dtype=dtype)","iterable_zj[0], iterable_zj[3]","*anchor_range[0], anchor_range[3]",*anchor_range[:6:3],0
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/relay/op/nn/_nn.py,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/nn/_nn.py,,convert_conv2d_backward_weight$1095,"def convert_conv2d_backward_weight(attrs, inputs, _, desired_layouts):
    """"""Convert Layout pass registration for conv2d_backward_weight op.
    Note that `desired_layouts` must be a pair [`data_layout`, `kernel_layouts`],
    where `kernel_layouts` affects the output of this op (since the output of this op
    is the weight gradient). The layout of the output gradient (the second input to this op)
    is assumed to be the same as `data_layout`.
    Parameters
    ----------
    attrs : tvm.ir.Attrs
        Attributes of current op
    inputs : list of tvm.relay.Expr
        The args of the Relay expr to be legalized
    tinfos : list of types
        List of input and output types
    desired_layouts : list of layout strings
        List of layouts defining our desired
        layout for the data and kernel inputs respectively.
    Returns
    -------
    result : tvm.relay.Expr
        The transformed expr
    """"""
    new_attrs = dict(attrs)
    assert len(desired_layouts) == 2, ""A desired layout is expected for both of data and gradient.""
    desired_data_layout, desired_kernel_layout = map(str, desired_layouts)
    assert desired_data_layout != ""default"", ""Data layout cannot be default""
    new_attrs[""grad_layout""] = desired_data_layout
    new_attrs[""data_layout""] = desired_data_layout
    new_attrs[""kernel_layout""] = desired_kernel_layout
    new_attrs.pop(""out_layout"")
    return relay.nn.conv2d_backward_weight(inputs[0], inputs[1], **new_attrs)","relay.nn.conv2d_backward_weight(inputs[0], inputs[1], **new_attrs)","relay.nn.conv2d_backward_weight(*inputs[:2], **new_attrs)","iterable_zj[0], iterable_zj[1]",*inputs[:2],*inputs[:2],1
TerminalView,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TerminalView/sublime_terminal_buffer.py,https://github.com/Wramberg/TerminalView/tree/master//sublime_terminal_buffer.py,TerminalViewUpdate,_update_cursor$339,"def _update_cursor(self):
        cursor_pos = self._sub_buffer.terminal_emulator().cursor()
        last_cursor_pos = self.view.settings().get(""terminal_view_last_cursor_pos"")
        if last_cursor_pos and last_cursor_pos[0] == cursor_pos[0] and last_cursor_pos[1] == cursor_pos[1]:
            return

        tp = self.view.text_point(cursor_pos[0], cursor_pos[1])
        self.view.sel().clear()
        self.view.sel().add(sublime.Region(tp, tp))
        self.view.settings().set(""terminal_view_last_cursor_pos"", cursor_pos)","self.view.text_point(cursor_pos[0], cursor_pos[1])",self.view.text_point(*cursor_pos[:2]),"iterable_zj[0], iterable_zj[1]",*cursor_pos[:2],*cursor_pos[:2],1
RenderPipeline,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RenderPipeline/rpcore/common_resources.py,https://github.com/tobspr/RenderPipeline/tree/master/rpcore/common_resources.py,CommonResources,update$173,"def update(self):
        """""" Updates the commonly used resources, mostly the shader inputs """"""
        update = self._input_ubo.update_input

        # Get the current transform matrix of the camera
        view_mat = Globals.render.get_transform(self._showbase.cam).get_mat()

        # Compute the view matrix, but with a z-up coordinate system
        zup_conversion = Mat4.convert_mat(CS_zup_right, CS_yup_right)
        update(""view_mat_z_up"", view_mat * zup_conversion)

        # Compute the view matrix without the camera rotation
        view_mat_billboard = Mat4(view_mat)
        view_mat_billboard.set_row(0, Vec3(1, 0, 0))
        view_mat_billboard.set_row(1, Vec3(0, 1, 0))
        view_mat_billboard.set_row(2, Vec3(0, 0, 1))
        update(""view_mat_billboard"", view_mat_billboard)

        update(""camera_pos"", self._showbase.camera.get_pos(Globals.render))

        # Compute last view projection mat
        curr_vp = self._input_ubo.get_input(""view_proj_mat_no_jitter"")
        update(""last_view_proj_mat_no_jitter"", curr_vp)
        curr_vp = Mat4(curr_vp)
        curr_vp.invert_in_place()
        curr_inv_vp = curr_vp
        update(""last_inv_view_proj_mat_no_jitter"", curr_inv_vp)

        proj_mat = Mat4(self._showbase.camLens.get_projection_mat())

        # Set the projection matrix as an input, but convert it to the correct
        # coordinate system before.
        proj_mat_zup = Mat4.convert_mat(CS_yup_right, CS_zup_right) * proj_mat
        update(""proj_mat"", proj_mat_zup)

        # Set the inverse projection matrix
        update(""inv_proj_mat"", invert(proj_mat_zup))

        # Remove jitter and set the new view projection mat
        proj_mat.set_cell(1, 0, 0.0)
        proj_mat.set_cell(1, 1, 0.0)
        update(""view_proj_mat_no_jitter"", view_mat * proj_mat)

        # Store the frame delta
        update(""frame_delta"", Globals.clock.get_dt())
        update(""smooth_frame_delta"", 1.0 / max(1e-5, Globals.clock.get_average_frame_rate()))
        update(""frame_time"", Globals.clock.get_frame_time())

        # Store the current film offset, we use this to compute the pixel-perfect
        # velocity, which is otherwise not possible. Usually this is always 0
        # except when SMAA and reprojection is enabled
        update(""current_film_offset"", self._showbase.camLens.get_film_offset())
        update(""frame_index"", Globals.clock.get_frame_count())

        # Compute frustum corners in the order BL, BR, TL, TR
        ws_frustum_directions = Mat4()
        vs_frustum_directions = Mat4()
        inv_proj_mat = Globals.base.camLens.get_projection_mat_inv()
        view_mat_inv = Mat4(view_mat)
        view_mat_inv.invert_in_place()

        for i, point in enumerate(((-1, -1), (1, -1), (-1, 1), (1, 1))):
            result = inv_proj_mat.xform(Vec4(point[0], point[1], 1.0, 1.0))
            vs_dir = (zup_conversion.xform(result)).xyz.normalized()
            vs_frustum_directions.set_row(i, Vec4(vs_dir, 1))
            ws_dir = view_mat_inv.xform(Vec4(result.xyz, 0))
            ws_frustum_directions.set_row(i, ws_dir)

        update(""vs_frustum_directions"", vs_frustum_directions)
        update(""ws_frustum_directions"", ws_frustum_directions)

        update(""screen_size"", Globals.resolution)
        update(""native_screen_size"", Globals.native_resolution)
        update(""lc_tile_count"", self._pipeline.light_mgr.num_tiles)","Vec4(point[0], point[1], 1.0, 1.0)","Vec4(*point[:2], 1.0, 1.0)","iterable_zj[0], iterable_zj[1]",*point[:2],*point[:2],1
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/circuits/gates/fermionic_simulation.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/circuits/gates/fermionic_simulation.py,QuarticFermionicSimulationGate,_eigen_components$741,"def _eigen_components(self):
        # projector onto subspace spanned by basis states with
        # Hamming weight != 2
        zero_component = np.diag(
            [int(bin(i).count('1') != 2) for i in range(16)])

        state_pairs = (('0110', '1001'), ('0101', '1010'), ('0011', '1100'))

        plus_minus_components = tuple(
            (-abs(weight) * sign / np.pi,
             state_swap_eigen_component(state_pair[0], state_pair[1], sign,
                                        np.angle(weight)))
            for weight, state_pair in zip(self.weights, state_pairs)
            for sign in (-1, 1))

        return ((0, zero_component),) + plus_minus_components","state_swap_eigen_component(state_pair[0], state_pair[1], sign, np.angle(weight))","state_swap_eigen_component(*state_pair[:2], sign, np.angle(weight))","iterable_zj[0], iterable_zj[1]",*state_pair[:2],*state_pair[:2],1
openpilot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openpilot/selfdrive/controls/radard.py,https://github.com/commaai/openpilot/tree/master/selfdrive/controls/radard.py,RadarD,update$107,"def update(self, sm, rr, enable_lead):
    self.current_time = 1e-9*max(sm.logMonoTime.values())

    if sm.updated['carState']:
      self.v_ego = sm['carState'].vEgo
      self.v_ego_hist.append(self.v_ego)
    if sm.updated['modelV2']:
      self.ready = True

    ar_pts = {}
    for pt in rr.points:
      ar_pts[pt.trackId] = [pt.dRel, pt.yRel, pt.vRel, pt.measured]

    # *** remove missing points from meta data ***
    for ids in list(self.tracks.keys()):
      if ids not in ar_pts:
        self.tracks.pop(ids, None)

    # *** compute the tracks ***
    for ids in ar_pts:
      rpt = ar_pts[ids]

      # align v_ego by a fixed time to align it with the radar measurement
      v_lead = rpt[2] + self.v_ego_hist[0]

      # create the track if it doesn't exist or it's a new track
      if ids not in self.tracks:
        self.tracks[ids] = Track(v_lead, self.kalman_params)
      self.tracks[ids].update(rpt[0], rpt[1], rpt[2], v_lead, rpt[3])

    idens = list(sorted(self.tracks.keys()))
    track_pts = list([self.tracks[iden].get_key_for_cluster() for iden in idens])

    # If we have multiple points, cluster them
    if len(track_pts) > 1:
      cluster_idxs = cluster_points_centroid(track_pts, 2.5)
      clusters = [None] * (max(cluster_idxs) + 1)

      for idx in range(len(track_pts)):
        cluster_i = cluster_idxs[idx]
        if clusters[cluster_i] is None:
          clusters[cluster_i] = Cluster()
        clusters[cluster_i].add(self.tracks[idens[idx]])
    elif len(track_pts) == 1:
      # FIXME: cluster_point_centroid hangs forever if len(track_pts) == 1
      cluster_idxs = [0]
      clusters = [Cluster()]
      clusters[0].add(self.tracks[idens[0]])
    else:
      clusters = []

    # if a new point, reset accel to the rest of the cluster
    for idx in range(len(track_pts)):
      if self.tracks[idens[idx]].cnt <= 1:
        aLeadK = clusters[cluster_idxs[idx]].aLeadK
        aLeadTau = clusters[cluster_idxs[idx]].aLeadTau
        self.tracks[idens[idx]].reset_a_lead(aLeadK, aLeadTau)

    # *** publish radarState ***
    dat = messaging.new_message('radarState')
    dat.valid = sm.all_alive_and_valid() and len(rr.errors) == 0
    radarState = dat.radarState
    radarState.mdMonoTime = sm.logMonoTime['modelV2']
    radarState.canMonoTimes = list(rr.canMonoTimes)
    radarState.radarErrors = list(rr.errors)
    radarState.carStateMonoTime = sm.logMonoTime['carState']

    if enable_lead:
      if len(sm['modelV2'].leadsV3) > 1:
        radarState.leadOne = get_lead(self.v_ego, self.ready, clusters, sm['modelV2'].leadsV3[0], low_speed_override=True)
        radarState.leadTwo = get_lead(self.v_ego, self.ready, clusters, sm['modelV2'].leadsV3[1], low_speed_override=False)
    return dat","self.tracks[ids].update(rpt[0], rpt[1], rpt[2], v_lead, rpt[3])","self.tracks[ids].update(*rpt[:3], v_lead, rpt[3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*rpt[:3],*rpt[:3],1
httplib2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/httplib2/python2/httplib2/socks.py,https://github.com/httplib2/httplib2/tree/master/python2/httplib2/socks.py,socksocket,connect$469,"def connect(self, destpair):
        """"""connect(self, despair)
        Connects to the specified destination through a proxy.
        destpar - A tuple of the IP/DNS address and the port number.
        (identical to socket's connect).
        To select the proxy server use setproxy().
        """"""
        # Do a minimal input check first
        if (
            (not type(destpair) in (list, tuple))
            or (len(destpair) < 2)
            or (not isinstance(destpair[0], basestring))
            or (type(destpair[1]) != int)
        ):
            raise GeneralProxyError((5, _generalerrors[5]))
        if self.__proxy[0] == PROXY_TYPE_SOCKS5:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks5(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks4(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatehttp(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP_NO_TUNNEL:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            if destpair[1] == 443:
                self.__negotiatehttp(destpair[0], destpair[1])
            else:
                self.__httptunnel = False
        elif self.__proxy[0] == None:
            _orgsocket.connect(self, (destpair[0], destpair[1]))
        else:
            raise GeneralProxyError((4, _generalerrors[4]))","self.__negotiatesocks5(destpair[0], destpair[1])",self.__negotiatesocks5(*destpair[:2]),"iterable_zj[0], iterable_zj[1]",*destpair[:2],*destpair[:2],1
httplib2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/httplib2/python2/httplib2/socks.py,https://github.com/httplib2/httplib2/tree/master/python2/httplib2/socks.py,socksocket,connect$469,"def connect(self, destpair):
        """"""connect(self, despair)
        Connects to the specified destination through a proxy.
        destpar - A tuple of the IP/DNS address and the port number.
        (identical to socket's connect).
        To select the proxy server use setproxy().
        """"""
        # Do a minimal input check first
        if (
            (not type(destpair) in (list, tuple))
            or (len(destpair) < 2)
            or (not isinstance(destpair[0], basestring))
            or (type(destpair[1]) != int)
        ):
            raise GeneralProxyError((5, _generalerrors[5]))
        if self.__proxy[0] == PROXY_TYPE_SOCKS5:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks5(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks4(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatehttp(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP_NO_TUNNEL:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            if destpair[1] == 443:
                self.__negotiatehttp(destpair[0], destpair[1])
            else:
                self.__httptunnel = False
        elif self.__proxy[0] == None:
            _orgsocket.connect(self, (destpair[0], destpair[1]))
        else:
            raise GeneralProxyError((4, _generalerrors[4]))","self.__negotiatesocks4(destpair[0], destpair[1])",self.__negotiatesocks4(*destpair[:2]),"iterable_zj[0], iterable_zj[1]",*destpair[:2],*destpair[:2],1
httplib2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/httplib2/python2/httplib2/socks.py,https://github.com/httplib2/httplib2/tree/master/python2/httplib2/socks.py,socksocket,connect$469,"def connect(self, destpair):
        """"""connect(self, despair)
        Connects to the specified destination through a proxy.
        destpar - A tuple of the IP/DNS address and the port number.
        (identical to socket's connect).
        To select the proxy server use setproxy().
        """"""
        # Do a minimal input check first
        if (
            (not type(destpair) in (list, tuple))
            or (len(destpair) < 2)
            or (not isinstance(destpair[0], basestring))
            or (type(destpair[1]) != int)
        ):
            raise GeneralProxyError((5, _generalerrors[5]))
        if self.__proxy[0] == PROXY_TYPE_SOCKS5:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks5(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks4(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatehttp(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP_NO_TUNNEL:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            if destpair[1] == 443:
                self.__negotiatehttp(destpair[0], destpair[1])
            else:
                self.__httptunnel = False
        elif self.__proxy[0] == None:
            _orgsocket.connect(self, (destpair[0], destpair[1]))
        else:
            raise GeneralProxyError((4, _generalerrors[4]))","self.__negotiatehttp(destpair[0], destpair[1])",self.__negotiatehttp(*destpair[:2]),"iterable_zj[0], iterable_zj[1]",*destpair[:2],*destpair[:2],1
httplib2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/httplib2/python2/httplib2/socks.py,https://github.com/httplib2/httplib2/tree/master/python2/httplib2/socks.py,socksocket,connect$469,"def connect(self, destpair):
        """"""connect(self, despair)
        Connects to the specified destination through a proxy.
        destpar - A tuple of the IP/DNS address and the port number.
        (identical to socket's connect).
        To select the proxy server use setproxy().
        """"""
        # Do a minimal input check first
        if (
            (not type(destpair) in (list, tuple))
            or (len(destpair) < 2)
            or (not isinstance(destpair[0], basestring))
            or (type(destpair[1]) != int)
        ):
            raise GeneralProxyError((5, _generalerrors[5]))
        if self.__proxy[0] == PROXY_TYPE_SOCKS5:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks5(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks4(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatehttp(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP_NO_TUNNEL:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            if destpair[1] == 443:
                self.__negotiatehttp(destpair[0], destpair[1])
            else:
                self.__httptunnel = False
        elif self.__proxy[0] == None:
            _orgsocket.connect(self, (destpair[0], destpair[1]))
        else:
            raise GeneralProxyError((4, _generalerrors[4]))","self.__negotiatehttp(destpair[0], destpair[1])",self.__negotiatehttp(*destpair[:2]),"iterable_zj[0], iterable_zj[1]",*destpair[:2],*destpair[:2],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/topi/vision/nms_util.py,https://github.com/apache/tvm/tree/master/python/tvm/topi/vision/nms_util.py,,run_all_class_nms$238,"def run_all_class_nms(
    boxes,
    sorted_scores,
    sorted_indices,
    valid_count,
    max_output_size_per_class,
    iou_threshold,
    nms_loop,
    return_scores=False,
):
    """"""The core all class NMS routine

    Parameters
    ----------
    boxes : tvm.te.Tensor
        3-D tensor with shape (batch_size, num_boxes, 4)

    sorted_scores: tvm.te.Tensor
        2-D tensor with shape (batch_size * num_classes, num_boxes)
        One of the outputs from argsort

    sorted_indices: tvm.te.Tensor
        2-D tensor with shape (batch_size * num_classes, num_boxes)
        The other output from argsort

    valid_count: tvm.te.Tensor
        1-D tensor with shape (batch_size * num_classes,), representing
        the number of boxes whose score is above score_threshold, per batch and class

    max_output_boxes_per_class : int or tvm.te.Tensor, optional
        The maxinum number of output selected boxes per class

    iou_threshold : float or tvm.te.Tensor, optionaIl
        IoU test threshold

    nms_loop : function
        A core NMS loop, see its usage in vision/nms.py and cuda/nms.py

    return_scores : bool, optional
        Whether or not to return selected scores, needed by the tensorflow output format.

    Returns
    -------
    out : a list of tvm.te.Tensor
        The output is three tensors, the first and second are indices and scores of size
        (batch_size * num_class, num_boxes), and the third is a tensor
        num_selected_boxes of shape (batch_size * num_class,) representing the total number of
        selected boxes per batch and class. If return_scores is False, the second output is
        None.
    """"""
    batch, num_boxes, _ = boxes.shape
    batch_class = sorted_scores.shape[0]
    num_class = batch_class // batch

    if return_scores is False:
        selected_indices, num_detections = te.extern(
            [(batch_class, num_boxes), (1, batch_class)],
            [boxes, sorted_scores, sorted_indices, valid_count],
            lambda ins, outs: _all_class_nms_ir(
                ins[0],  # boxes
                ins[1],  # sorted_scores
                ins[2],  # sorted_indices
                ins[3],  # valid_count
                batch_class,
                num_class,
                num_boxes,
                iou_threshold,
                max_output_size_per_class,
                outs[0],  # box_indices
                None,  # scores
                outs[1],  # num_selected_boxes
                nms_loop,
            ),
            dtype=[""int32"", ""int32""],
            name=""all_class_nms"",
            tag=""all_class_nms"",
        )
        return selected_indices, None, num_detections

    return te.extern(
        [(batch_class, num_boxes), (batch_class, num_boxes), (1, batch_class)],
        [boxes, sorted_scores, sorted_indices, valid_count],
        lambda ins, outs: _all_class_nms_ir(
            ins[0],  # boxes
            ins[1],  # sorted_scores
            ins[2],  # sorted_indices
            ins[3],  # valid_count
            batch_class,
            num_class,
            num_boxes,
            iou_threshold,
            max_output_size_per_class,
            outs[0],  # box_indices
            outs[1],  # selected scores
            outs[2],  # num_selected_boxes
            nms_loop,
        ),
        dtype=[""int32"", ""float32"", ""int32""],
        name=""all_class_nms"",
        tag=""all_class_nms"",
    )","_all_class_nms_ir(ins[0], ins[1], ins[2], ins[3], batch_class, num_class, num_boxes, iou_threshold, max_output_size_per_class, outs[0], outs[1], outs[2], nms_loop)","_all_class_nms_ir(*ins[:4], batch_class, num_class, num_boxes, iou_threshold, max_output_size_per_class, outs[0], outs[1], outs[2], nms_loop)","iterable_zj[0], iterable_zj[1], iterable_zj[2], iterable_zj[3]",*ins[:4],*ins[:4],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/topi/vision/nms_util.py,https://github.com/apache/tvm/tree/master/python/tvm/topi/vision/nms_util.py,,run_all_class_nms$238,"def run_all_class_nms(
    boxes,
    sorted_scores,
    sorted_indices,
    valid_count,
    max_output_size_per_class,
    iou_threshold,
    nms_loop,
    return_scores=False,
):
    """"""The core all class NMS routine

    Parameters
    ----------
    boxes : tvm.te.Tensor
        3-D tensor with shape (batch_size, num_boxes, 4)

    sorted_scores: tvm.te.Tensor
        2-D tensor with shape (batch_size * num_classes, num_boxes)
        One of the outputs from argsort

    sorted_indices: tvm.te.Tensor
        2-D tensor with shape (batch_size * num_classes, num_boxes)
        The other output from argsort

    valid_count: tvm.te.Tensor
        1-D tensor with shape (batch_size * num_classes,), representing
        the number of boxes whose score is above score_threshold, per batch and class

    max_output_boxes_per_class : int or tvm.te.Tensor, optional
        The maxinum number of output selected boxes per class

    iou_threshold : float or tvm.te.Tensor, optionaIl
        IoU test threshold

    nms_loop : function
        A core NMS loop, see its usage in vision/nms.py and cuda/nms.py

    return_scores : bool, optional
        Whether or not to return selected scores, needed by the tensorflow output format.

    Returns
    -------
    out : a list of tvm.te.Tensor
        The output is three tensors, the first and second are indices and scores of size
        (batch_size * num_class, num_boxes), and the third is a tensor
        num_selected_boxes of shape (batch_size * num_class,) representing the total number of
        selected boxes per batch and class. If return_scores is False, the second output is
        None.
    """"""
    batch, num_boxes, _ = boxes.shape
    batch_class = sorted_scores.shape[0]
    num_class = batch_class // batch

    if return_scores is False:
        selected_indices, num_detections = te.extern(
            [(batch_class, num_boxes), (1, batch_class)],
            [boxes, sorted_scores, sorted_indices, valid_count],
            lambda ins, outs: _all_class_nms_ir(
                ins[0],  # boxes
                ins[1],  # sorted_scores
                ins[2],  # sorted_indices
                ins[3],  # valid_count
                batch_class,
                num_class,
                num_boxes,
                iou_threshold,
                max_output_size_per_class,
                outs[0],  # box_indices
                None,  # scores
                outs[1],  # num_selected_boxes
                nms_loop,
            ),
            dtype=[""int32"", ""int32""],
            name=""all_class_nms"",
            tag=""all_class_nms"",
        )
        return selected_indices, None, num_detections

    return te.extern(
        [(batch_class, num_boxes), (batch_class, num_boxes), (1, batch_class)],
        [boxes, sorted_scores, sorted_indices, valid_count],
        lambda ins, outs: _all_class_nms_ir(
            ins[0],  # boxes
            ins[1],  # sorted_scores
            ins[2],  # sorted_indices
            ins[3],  # valid_count
            batch_class,
            num_class,
            num_boxes,
            iou_threshold,
            max_output_size_per_class,
            outs[0],  # box_indices
            outs[1],  # selected scores
            outs[2],  # num_selected_boxes
            nms_loop,
        ),
        dtype=[""int32"", ""float32"", ""int32""],
        name=""all_class_nms"",
        tag=""all_class_nms"",
    )","_all_class_nms_ir(ins[0], ins[1], ins[2], ins[3], batch_class, num_class, num_boxes, iou_threshold, max_output_size_per_class, outs[0], outs[1], outs[2], nms_loop)","_all_class_nms_ir(ins[0], ins[1], ins[2], ins[3], batch_class, num_class, num_boxes, iou_threshold, max_output_size_per_class, *outs[:3], nms_loop)","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*outs[:3],*outs[:3],1
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/topi/vision/nms_util.py,https://github.com/apache/tvm/tree/master/python/tvm/topi/vision/nms_util.py,,run_all_class_nms$238,"def run_all_class_nms(
    boxes,
    sorted_scores,
    sorted_indices,
    valid_count,
    max_output_size_per_class,
    iou_threshold,
    nms_loop,
    return_scores=False,
):
    """"""The core all class NMS routine

    Parameters
    ----------
    boxes : tvm.te.Tensor
        3-D tensor with shape (batch_size, num_boxes, 4)

    sorted_scores: tvm.te.Tensor
        2-D tensor with shape (batch_size * num_classes, num_boxes)
        One of the outputs from argsort

    sorted_indices: tvm.te.Tensor
        2-D tensor with shape (batch_size * num_classes, num_boxes)
        The other output from argsort

    valid_count: tvm.te.Tensor
        1-D tensor with shape (batch_size * num_classes,), representing
        the number of boxes whose score is above score_threshold, per batch and class

    max_output_boxes_per_class : int or tvm.te.Tensor, optional
        The maxinum number of output selected boxes per class

    iou_threshold : float or tvm.te.Tensor, optionaIl
        IoU test threshold

    nms_loop : function
        A core NMS loop, see its usage in vision/nms.py and cuda/nms.py

    return_scores : bool, optional
        Whether or not to return selected scores, needed by the tensorflow output format.

    Returns
    -------
    out : a list of tvm.te.Tensor
        The output is three tensors, the first and second are indices and scores of size
        (batch_size * num_class, num_boxes), and the third is a tensor
        num_selected_boxes of shape (batch_size * num_class,) representing the total number of
        selected boxes per batch and class. If return_scores is False, the second output is
        None.
    """"""
    batch, num_boxes, _ = boxes.shape
    batch_class = sorted_scores.shape[0]
    num_class = batch_class // batch

    if return_scores is False:
        selected_indices, num_detections = te.extern(
            [(batch_class, num_boxes), (1, batch_class)],
            [boxes, sorted_scores, sorted_indices, valid_count],
            lambda ins, outs: _all_class_nms_ir(
                ins[0],  # boxes
                ins[1],  # sorted_scores
                ins[2],  # sorted_indices
                ins[3],  # valid_count
                batch_class,
                num_class,
                num_boxes,
                iou_threshold,
                max_output_size_per_class,
                outs[0],  # box_indices
                None,  # scores
                outs[1],  # num_selected_boxes
                nms_loop,
            ),
            dtype=[""int32"", ""int32""],
            name=""all_class_nms"",
            tag=""all_class_nms"",
        )
        return selected_indices, None, num_detections

    return te.extern(
        [(batch_class, num_boxes), (batch_class, num_boxes), (1, batch_class)],
        [boxes, sorted_scores, sorted_indices, valid_count],
        lambda ins, outs: _all_class_nms_ir(
            ins[0],  # boxes
            ins[1],  # sorted_scores
            ins[2],  # sorted_indices
            ins[3],  # valid_count
            batch_class,
            num_class,
            num_boxes,
            iou_threshold,
            max_output_size_per_class,
            outs[0],  # box_indices
            outs[1],  # selected scores
            outs[2],  # num_selected_boxes
            nms_loop,
        ),
        dtype=[""int32"", ""float32"", ""int32""],
        name=""all_class_nms"",
        tag=""all_class_nms"",
    )","_all_class_nms_ir(ins[0], ins[1], ins[2], ins[3], batch_class, num_class, num_boxes, iou_threshold, max_output_size_per_class, outs[0], None, outs[1], nms_loop)","_all_class_nms_ir(*ins[:4], batch_class, num_class, num_boxes, iou_threshold, max_output_size_per_class, outs[0], None, outs[1], nms_loop)","iterable_zj[0], iterable_zj[1], iterable_zj[2], iterable_zj[3]",*ins[:4],*ins[:4],1
atomic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/atomic/Atomic/syscontainers.py,https://github.com/projectatomic/atomic/tree/master/Atomic/syscontainers.py,SystemContainers,delete_image$1791,"def delete_image(self, image):
        """"""
        Deletes a specific image.

        :param image: The name of the image to delete.
        :type image: str
        """"""
        repo = self._get_ostree_repo()
        if not repo:
            return
        imgs = self._resolve_image(repo, image, allow_multiple=True)
        if not imgs:
            return
        for imagebranch, _ in imgs:
            ref = OSTree.parse_refspec(imagebranch)
            repo.set_ref_immediate(ref[1], ref[2], None)","repo.set_ref_immediate(ref[1], ref[2], None)","repo.set_ref_immediate(*ref[1:3], None)","iterable_zj[1], iterable_zj[2]",*ref[1:3],*ref[1:3],1
TensorNetwork,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorNetwork/tensornetwork/tests/tensornetwork_symmetric_test.py,https://github.com/google/TensorNetwork/tree/master/tensornetwork/tests/tensornetwork_symmetric_test.py,,test_flatten_trace_consistent_tensor$272,"def test_flatten_trace_consistent_tensor(dtype, num_charges):
  a_val = get_random_symmetric((5, 5, 5, 5, 5),
                               [False, False, True, True, True],
                               num_charges,
                               dtype=dtype)
  a = tn.Node(a_val, backend='symmetric')
  e1 = tn.connect(a[0], a[4])
  e2 = tn.connect(a[3], a[2])
  tn.flatten_edges([e2, e1])
  tn.check_correct({a})
  # Check expected values.
  a_final = np.reshape(
      np.transpose(a_val.todense(), (1, 2, 0, 3, 4)), (5, 25, 25))
  np.testing.assert_allclose(a.tensor.todense(), a_final)","tn.connect(a[0], a[4])",tn.connect(*a[0:5:4]),"iterable_zj[0], iterable_zj[4]",*a[0:5:4],*a[:8:4],0
UNITER,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UNITER/data/pretrain_vcr.py,https://github.com/ChenRocks/UNITER/tree/master/data/pretrain_vcr.py,MrfrDatasetForVCR,__getitem__$155,"def __getitem__(self, i):
        example = super().__getitem__(i)
        # text input
        input_ids, txt_type_ids = self._get_input_ids(example, mask=False)
        input_ids, txt_type_ids = self.combine_txt_inputs(
            input_ids, txt_type_ids)

        # image input features
        img_feat, img_pos_feat, num_bb = self._get_img_feat(
            example['img_fname'][0], example['img_fname'][1])
        img_mask = _get_img_mask(self.mask_prob, num_bb)
        img_mask_tgt = _get_img_tgt_mask(img_mask, len(input_ids))

        attn_masks = torch.ones(len(input_ids) + num_bb, dtype=torch.long)

        return (input_ids, txt_type_ids, img_feat, img_pos_feat,
                attn_masks, img_mask, img_mask_tgt)","self._get_img_feat(example['img_fname'][0], example['img_fname'][1])",self._get_img_feat(*example['img_fname'][:2]),"iterable_zj[0], iterable_zj[1]",*example['img_fname'][:2],*example['img_fname'][:2],1
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_deformable_psroi_pooling.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_deformable_psroi_pooling.py,TestDeformablePSROIPoolOp,calc_deformable_psroi_pooling$349,"def calc_deformable_psroi_pooling(self):
        output_shape = (
            self.rois_num,
            self.output_channels,
            self.pooled_height,
            self.pooled_width,
        )
        self.out = np.zeros(output_shape)
        self.trans = np.random.rand(
            self.rois_num, 2, self.part_size[0], self.part_size[1]
        ).astype('float32')
        self.top_count = np.random.random((output_shape)).astype('float32')
        count = (
            self.rois_num
            * self.output_channels
            * self.pooled_height
            * self.pooled_width
        )
        for index in range(count):
            p_w = int(index % self.pooled_width)
            p_h = int(index / self.pooled_width % self.pooled_height)
            ctop = int(
                index
                / self.pooled_width
                / self.pooled_height
                % self.output_channels
            )
            n_out = int(
                index
                / self.pooled_width
                / self.pooled_height
                / self.output_channels
            )
            roi = self.rois[n_out]
            roi_batch_id = int(roi[0])
            roi_start_w = int(np.round(roi[1])) * self.spatial_scale - 0.5
            roi_start_h = int(np.round(roi[2])) * self.spatial_scale - 0.5
            roi_end_w = int(np.round(roi[3] + 1)) * self.spatial_scale - 0.5
            roi_end_h = int(np.round(roi[4] + 1)) * self.spatial_scale - 0.5
            roi_width = max(roi_end_w - roi_start_w, 0.1)
            roi_height = max(roi_end_h - roi_start_h, 0.1)
            bin_size_h = float(roi_height) / float(self.pooled_height)
            bin_size_w = float(roi_width) / float(self.pooled_width)
            sub_bin_size_h = bin_size_h / self.sample_per_part
            sub_bin_size_w = bin_size_w / self.sample_per_part
            part_h = int(np.floor(p_h) / self.pooled_height * self.part_size[0])
            part_w = int(np.floor(p_w) / self.pooled_width * self.part_size[1])
            if self.no_trans:
                trans_x = 0
                trans_y = 0
            else:
                trans_x = self.trans[n_out][0][part_h][part_w] * self.trans_std
                trans_y = self.trans[n_out][1][part_h][part_w] * self.trans_std
            wstart = p_w * bin_size_w + roi_start_w
            wstart = wstart + trans_x * roi_width
            hstart = p_h * bin_size_h + roi_start_h
            hstart = hstart + trans_y * roi_height
            sum = 0
            num_sample = 0
            g_w = np.floor(p_w * self.group_size[0] / self.pooled_height)
            g_h = np.floor(p_h * self.group_size[1] / self.pooled_width)
            g_w = min(max(g_w, 0), self.group_size[0] - 1)
            g_h = min(max(g_h, 0), self.group_size[1] - 1)
            input_i = self.input[roi_batch_id]
            for i_w in range(self.sample_per_part):
                for i_h in range(self.sample_per_part):
                    w_sample = wstart + i_w * sub_bin_size_w
                    h_sample = hstart + i_h * sub_bin_size_h
                    if (
                        w_sample < -0.5
                        or w_sample > self.width - 0.5
                        or h_sample < -0.5
                        or h_sample > self.height - 0.5
                    ):
                        continue
                    w_sample = min(max(w_sample, 0.0), self.width - 1.0)
                    h_sample = min(max(h_sample, 0.0), self.height - 1.0)
                    c_sample = int(
                        (ctop * self.group_size[0] + g_h) * self.group_size[1]
                        + g_w
                    )
                    val = self.dmc_bilinear(
                        input_i[c_sample], h_sample, w_sample
                    )
                    sum = sum + val
                    num_sample = num_sample + 1
            if num_sample == 0:
                self.out[n_out][ctop][p_h][p_w] = 0
            else:
                self.out[n_out][ctop][p_h][p_w] = sum / num_sample
            self.top_count[n_out][ctop][p_h][p_w] = num_sample","np.random.rand(self.rois_num, 2, self.part_size[0], self.part_size[1])","np.random.rand(self.rois_num, 2, *self.part_size[:2])","iterable_zj[0], iterable_zj[1]",*self.part_size[:2],*self.part_size[:2],1
ThinkMatch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ThinkMatch/models/NGM/model_v2.py,https://github.com/Thinklab-SJTU/ThinkMatch/tree/master/models/NGM/model_v2.py,Net,forward$68,"def forward(
        self,
        data_dict,
    ):
        images = data_dict['images']
        points = data_dict['Ps']
        n_points = data_dict['ns']
        graphs = data_dict['pyg_graphs']
        batch_size = data_dict['batch_size']
        num_graphs = len(images)

        global_list = []
        orig_graph_list = []
        for image, p, n_p, graph in zip(images, points, n_points, graphs):
            # extract feature
            nodes = self.node_layers(image)
            edges = self.edge_layers(nodes)

            global_list.append(self.final_layers(edges).reshape((nodes.shape[0], -1)))
            nodes = normalize_over_channels(nodes)
            edges = normalize_over_channels(edges)

            # arrange features
            U = concat_features(feature_align(nodes, p, n_p, self.rescale), n_p)
            F = concat_features(feature_align(edges, p, n_p, self.rescale), n_p)
            node_features = torch.cat((U, F), dim=1)
            graph.x = node_features

            graph = self.message_pass_node_features(graph)
            orig_graph = self.build_edge_features_from_node_features(graph)
            orig_graph_list.append(orig_graph)

        global_weights_list = [
            torch.cat([global_src, global_tgt], axis=-1) for global_src, global_tgt in lexico_iter(global_list)
        ]

        global_weights_list = [normalize_over_channels(g) for g in global_weights_list]

        unary_affs_list = [
            self.vertex_affinity([item.x for item in g_1], [item.x for item in g_2], global_weights)
            for (g_1, g_2), global_weights in zip(lexico_iter(orig_graph_list), global_weights_list)
        ]

        quadratic_affs_list = [
            self.edge_affinity([item.edge_attr for item in g_1], [item.edge_attr for item in g_2], global_weights)
            for (g_1, g_2), global_weights in zip(lexico_iter(orig_graph_list), global_weights_list)
        ]

        quadratic_affs_list = [[0.5 * x for x in quadratic_affs] for quadratic_affs in quadratic_affs_list]

        s_list, mgm_s_list, x_list, mgm_x_list, indices = [], [], [], [], []

        for unary_affs, quadratic_affs, (idx1, idx2) in zip(unary_affs_list, quadratic_affs_list, lexico_iter(range(num_graphs))):
            kro_G, kro_H = data_dict['KGHs'] if num_graphs == 2 else data_dict['KGHs']['{},{}'.format(idx1, idx2)]
            Kp = torch.stack(pad_tensor(unary_affs), dim=0)
            Ke = torch.stack(pad_tensor(quadratic_affs), dim=0)
            K = construct_aff_mat(Ke, Kp, kro_G, kro_H)
            if num_graphs == 2: data_dict['aff_mat'] = K

            if cfg.NGM.FIRST_ORDER:
                emb = Kp.transpose(1, 2).contiguous().view(Kp.shape[0], -1, 1)
            else:
                emb = torch.ones(K.shape[0], K.shape[1], 1, device=K.device)

            if cfg.NGM.POSITIVE_EDGES:
                A = (K > 0).to(K.dtype)
            else:
                A = (K != 0).to(K.dtype)

            emb_K = K.unsqueeze(-1)

            # NGM qap solver
            for i in range(self.gnn_layer):
                gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))
                emb_K, emb = gnn_layer(A, emb_K, emb, n_points[idx1], n_points[idx2])

            v = self.classifier(emb)
            s = v.view(v.shape[0], points[idx2].shape[1], -1).transpose(1, 2)

            ss = self.sinkhorn(s, n_points[idx1], n_points[idx2], dummy_row=True)
            x = hungarian(ss, n_points[idx1], n_points[idx2])
            s_list.append(ss)
            x_list.append(x)
            indices.append((idx1, idx2))

        if num_graphs > 2:
            joint_indices = torch.cat((torch.cumsum(torch.stack([torch.max(np) for np in n_points]), dim=0), torch.zeros((1,), dtype=torch.long, device=K.device)))
            joint_S = torch.zeros(batch_size, torch.max(joint_indices), torch.max(joint_indices), device=K.device)
            for idx in range(num_graphs):
                for b in range(batch_size):
                    start = joint_indices[idx-1]
                    joint_S[b, start:start+n_points[idx][b], start:start+n_points[idx][b]] += torch.eye(n_points[idx][b], device=K.device)

            for (idx1, idx2), s in zip(indices, s_list):
                if idx1 > idx2:
                    joint_S[:, joint_indices[idx2-1]:joint_indices[idx2], joint_indices[idx1-1]:joint_indices[idx1]] += s.transpose(1, 2)
                else:
                    joint_S[:, joint_indices[idx1-1]:joint_indices[idx1], joint_indices[idx2-1]:joint_indices[idx2]] += s

            matching_s = []
            for b in range(batch_size):
                e, v = torch.symeig(joint_S[b], eigenvectors=True)
                diff = e[-self.univ_size:-1] - e[-self.univ_size+1:]
                if self.training and torch.min(torch.abs(diff)) <= 1e-4:
                    matching_s.append(joint_S[b])
                else:
                    matching_s.append(num_graphs * torch.mm(v[:, -self.univ_size:], v[:, -self.univ_size:].transpose(0, 1)))

            matching_s = torch.stack(matching_s, dim=0)

            for idx1, idx2 in indices:
                s = matching_s[:, joint_indices[idx1-1]:joint_indices[idx1], joint_indices[idx2-1]:joint_indices[idx2]]
                s = self.sinkhorn_mgm(torch.log(torch.relu(s)), n_points[idx1], n_points[idx2]) # only perform row/col norm, do not perform exp
                x = hungarian(s, n_points[idx1], n_points[idx2])

                mgm_s_list.append(s)
                mgm_x_list.append(x)

        if cfg.PROBLEM.TYPE == '2GM':
            data_dict.update({
                'ds_mat': s_list[0],
                'perm_mat': x_list[0]
            })
        elif cfg.PROBLEM.TYPE == 'MGM':
            data_dict.update({
                'ds_mat_list': mgm_s_list,
                'perm_mat_list': mgm_x_list,
                'graph_indices': indices,
            })

        return data_dict","torch.ones(K.shape[0], K.shape[1], 1, device=K.device)","torch.ones(*K.shape[:2], 1, device=K.device)","iterable_zj[0], iterable_zj[1]",*K.shape[:2],*K.shape[:2],1
Reinforcement-learning-with-tensorflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Reinforcement-learning-with-tensorflow/contents/10_A3C/A3C_continuous_action.py,https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/10_A3C/A3C_continuous_action.py,ACNet,__init__$45,"def __init__(self, scope, globalAC=None):

        if scope == GLOBAL_NET_SCOPE:   # get global network
            with tf.variable_scope(scope):
                self.s = tf.placeholder(tf.float32, [None, N_S], 'S')
                self.a_params, self.c_params = self._build_net(scope)[-2:]
        else:   # local net, calculate losses
            with tf.variable_scope(scope):
                self.s = tf.placeholder(tf.float32, [None, N_S], 'S')
                self.a_his = tf.placeholder(tf.float32, [None, N_A], 'A')
                self.v_target = tf.placeholder(tf.float32, [None, 1], 'Vtarget')

                mu, sigma, self.v, self.a_params, self.c_params = self._build_net(scope)

                td = tf.subtract(self.v_target, self.v, name='TD_error')
                with tf.name_scope('c_loss'):
                    self.c_loss = tf.reduce_mean(tf.square(td))

                with tf.name_scope('wrap_a_out'):
                    mu, sigma = mu * A_BOUND[1], sigma + 1e-4

                normal_dist = tf.distributions.Normal(mu, sigma)

                with tf.name_scope('a_loss'):
                    log_prob = normal_dist.log_prob(self.a_his)
                    exp_v = log_prob * tf.stop_gradient(td)
                    entropy = normal_dist.entropy()  # encourage exploration
                    self.exp_v = ENTROPY_BETA * entropy + exp_v
                    self.a_loss = tf.reduce_mean(-self.exp_v)

                with tf.name_scope('choose_a'):  # use local params to choose action
                    self.A = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=[0, 1]), A_BOUND[0], A_BOUND[1])
                with tf.name_scope('local_grad'):
                    self.a_grads = tf.gradients(self.a_loss, self.a_params)
                    self.c_grads = tf.gradients(self.c_loss, self.c_params)

            with tf.name_scope('sync'):
                with tf.name_scope('pull'):
                    self.pull_a_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.a_params, globalAC.a_params)]
                    self.pull_c_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.c_params, globalAC.c_params)]
                with tf.name_scope('push'):
                    self.update_a_op = OPT_A.apply_gradients(zip(self.a_grads, globalAC.a_params))
                    self.update_c_op = OPT_C.apply_gradients(zip(self.c_grads, globalAC.c_params))","tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=[0, 1]), A_BOUND[0], A_BOUND[1])","tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=[0, 1]), *A_BOUND[:2])","iterable_zj[0], iterable_zj[1]",*A_BOUND[:2],*A_BOUND[:2],1
openrazer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openrazer/daemon/openrazer_daemon/hardware/device_base.py,https://github.com/openrazer/openrazer/tree/master/daemon/openrazer_daemon/hardware/device_base.py,RazerDevice,get_device_mode$888,"def get_device_mode(self):
        """"""
        Get device mode

        :return: String of device mode and arg separated by colon, e.g. 0:0 or 3:0
        :rtype: str
        """"""
        device_mode_path = os.path.join(self._device_path, 'device_mode')
        with open(device_mode_path, 'rb') as mode_file:
            count = 0
            mode = mode_file.read().strip()
            while len(mode) == 0:
                if count >= 3:
                    break
                mode = mode_file.read().strip()

                count += 1
                time.sleep(0.1)

            return ""{0}:{1}"".format(mode[0], mode[1])","'{0}:{1}'.format(mode[0], mode[1])",'{0}:{1}'.format(*mode[:2]),"iterable_zj[0], iterable_zj[1]",*mode[:2],*mode[:2],1
PyTorch-YOLOv3,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyTorch-YOLOv3/pytorchyolo/utils/utils.py,https://github.com/eriklindernoren/PyTorch-YOLOv3/tree/master/pytorchyolo/utils/utils.py,,compute_ap$157,"def compute_ap(recall, precision):
    """""" Compute the average precision, given the recall and precision curves.
    Code originally from https://github.com/rbgirshick/py-faster-rcnn.

    # Arguments
        recall:    The recall curve (list).
        precision: The precision curve (list).
    # Returns
        The average precision as computed in py-faster-rcnn.
    """"""
    # correct AP calculation
    # first append sentinel values at the end
    mrec = np.concatenate(([0.0], recall, [1.0]))
    mpre = np.concatenate(([0.0], precision, [0.0]))

    # compute the precision envelope
    for i in range(mpre.size - 1, 0, -1):
        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

    # to calculate area under PR curve, look for points
    # where X axis (recall) changes value
    i = np.where(mrec[1:] != mrec[:-1])[0]

    # and sum (\Delta recall) * prec
    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap","np.maximum(mpre[i - 1], mpre[i])",np.maximum(*mpre[i - 1:i + 1]),"iterable_zj[i - 1], iterable_zj[i]",*mpre[i-1:i+1],*mpre[i - 1:i + 1],1
dplython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dplython/dplython/later.py,https://github.com/dodger487/dplython/tree/master/dplython/later.py,SliceStep,format_operation$218,"def format_operation(self, obj, args, kwargs):
    return ""{0}[{1}]"".format(self.format_exp(obj),
                             self.format_arg(slice(args[0], args[1])))","slice(args[0], args[1])",slice(*args[:2]),"iterable_zj[0], iterable_zj[1]",*args[:2],*args[:2],1
feincms,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/feincms/feincms/templatetags/fragment_tags.py,https://github.com/feincms/feincms/tree/master/feincms/templatetags/fragment_tags.py,,get_fragment$79,"def get_fragment(parser, token):
    """"""
    Fetches the content of a fragment.

    Either::

        {% get_fragment request ""title"" %}

    or::

        {% get_fragment request ""title"" as title %}
    """"""

    fragments = token.contents.split()

    if len(fragments) == 3:
        return GetFragmentNode(fragments[1], fragments[2])
    elif len(fragments) == 5 and fragments[3] == ""as"":
        return GetFragmentNode(fragments[1], fragments[2], fragments[4])
    raise template.TemplateSyntaxError(
        ""Invalid syntax for get_fragment: %s"" % token.contents
    )","GetFragmentNode(fragments[1], fragments[2])",GetFragmentNode(*fragments[1:3]),"iterable_zj[1], iterable_zj[2]",*fragments[1:3],*fragments[1:3],1
feincms,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/feincms/feincms/templatetags/fragment_tags.py,https://github.com/feincms/feincms/tree/master/feincms/templatetags/fragment_tags.py,,get_fragment$79,"def get_fragment(parser, token):
    """"""
    Fetches the content of a fragment.

    Either::

        {% get_fragment request ""title"" %}

    or::

        {% get_fragment request ""title"" as title %}
    """"""

    fragments = token.contents.split()

    if len(fragments) == 3:
        return GetFragmentNode(fragments[1], fragments[2])
    elif len(fragments) == 5 and fragments[3] == ""as"":
        return GetFragmentNode(fragments[1], fragments[2], fragments[4])
    raise template.TemplateSyntaxError(
        ""Invalid syntax for get_fragment: %s"" % token.contents
    )","GetFragmentNode(fragments[1], fragments[2], fragments[4])",GetFragmentNode(*fragments[1:5:2]),"iterable_zj[1], iterable_zj[2], iterable_zj[4]",*fragments[1:5:2],*fragments[1:3],0
OWOD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OWOD/detectron2/modeling/test_time_augmentation.py,https://github.com/JosephKJ/OWOD/tree/master/detectron2/modeling/test_time_augmentation.py,DatasetMapperTTA,__call__$42,"def __call__(self, dataset_dict):
        """"""
        Args:
            dict: a dict in standard model input format. See tutorials for details.

        Returns:
            list[dict]:
                a list of dicts, which contain augmented version of the input image.
                The total number of dicts is ``len(min_sizes) * (2 if flip else 1)``.
                Each dict has field ""transforms"" which is a TransformList,
                containing the transforms that are used to generate this image.
        """"""
        numpy_image = dataset_dict[""image""].permute(1, 2, 0).numpy()
        shape = numpy_image.shape
        orig_shape = (dataset_dict[""height""], dataset_dict[""width""])
        if shape[:2] != orig_shape:
            # It transforms the ""original"" image in the dataset to the input image
            pre_tfm = ResizeTransform(orig_shape[0], orig_shape[1], shape[0], shape[1])
        else:
            pre_tfm = NoOpTransform()

        # Create all combinations of augmentations to use
        aug_candidates = []  # each element is a list[Augmentation]
        for min_size in self.min_sizes:
            resize = ResizeShortestEdge(min_size, self.max_size)
            aug_candidates.append([resize])  # resize only
            if self.flip:
                flip = RandomFlip(prob=1.0)
                aug_candidates.append([resize, flip])  # resize + flip

        # Apply all the augmentations
        ret = []
        for aug in aug_candidates:
            new_image, tfms = apply_augmentations(aug, np.copy(numpy_image))
            torch_image = torch.from_numpy(np.ascontiguousarray(new_image.transpose(2, 0, 1)))

            dic = copy.deepcopy(dataset_dict)
            dic[""transforms""] = pre_tfm + tfms
            dic[""image""] = torch_image
            ret.append(dic)
        return ret","ResizeTransform(orig_shape[0], orig_shape[1], shape[0], shape[1])","ResizeTransform(*orig_shape[:2], shape[0], shape[1])","iterable_zj[0], iterable_zj[1]",*orig_shape[:2],*orig_shape[:2],1
OWOD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OWOD/detectron2/modeling/test_time_augmentation.py,https://github.com/JosephKJ/OWOD/tree/master/detectron2/modeling/test_time_augmentation.py,DatasetMapperTTA,__call__$42,"def __call__(self, dataset_dict):
        """"""
        Args:
            dict: a dict in standard model input format. See tutorials for details.

        Returns:
            list[dict]:
                a list of dicts, which contain augmented version of the input image.
                The total number of dicts is ``len(min_sizes) * (2 if flip else 1)``.
                Each dict has field ""transforms"" which is a TransformList,
                containing the transforms that are used to generate this image.
        """"""
        numpy_image = dataset_dict[""image""].permute(1, 2, 0).numpy()
        shape = numpy_image.shape
        orig_shape = (dataset_dict[""height""], dataset_dict[""width""])
        if shape[:2] != orig_shape:
            # It transforms the ""original"" image in the dataset to the input image
            pre_tfm = ResizeTransform(orig_shape[0], orig_shape[1], shape[0], shape[1])
        else:
            pre_tfm = NoOpTransform()

        # Create all combinations of augmentations to use
        aug_candidates = []  # each element is a list[Augmentation]
        for min_size in self.min_sizes:
            resize = ResizeShortestEdge(min_size, self.max_size)
            aug_candidates.append([resize])  # resize only
            if self.flip:
                flip = RandomFlip(prob=1.0)
                aug_candidates.append([resize, flip])  # resize + flip

        # Apply all the augmentations
        ret = []
        for aug in aug_candidates:
            new_image, tfms = apply_augmentations(aug, np.copy(numpy_image))
            torch_image = torch.from_numpy(np.ascontiguousarray(new_image.transpose(2, 0, 1)))

            dic = copy.deepcopy(dataset_dict)
            dic[""transforms""] = pre_tfm + tfms
            dic[""image""] = torch_image
            ret.append(dic)
        return ret","ResizeTransform(orig_shape[0], orig_shape[1], shape[0], shape[1])","ResizeTransform(orig_shape[0], orig_shape[1], *shape[:2])","iterable_zj[0], iterable_zj[1]",*shape[:2],*shape[:2],1
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/signal/_signaltools.py,https://github.com/scipy/scipy/tree/master/scipy/signal/_signaltools.py,,detrend$3408,"def detrend(data, axis=-1, type='linear', bp=0, overwrite_data=False):
    """"""
    Remove linear trend along axis from data.

    Parameters
    ----------
    data : array_like
        The input data.
    axis : int, optional
        The axis along which to detrend the data. By default this is the
        last axis (-1).
    type : {'linear', 'constant'}, optional
        The type of detrending. If ``type == 'linear'`` (default),
        the result of a linear least-squares fit to `data` is subtracted
        from `data`.
        If ``type == 'constant'``, only the mean of `data` is subtracted.
    bp : array_like of ints, optional
        A sequence of break points. If given, an individual linear fit is
        performed for each part of `data` between two break points.
        Break points are specified as indices into `data`. This parameter
        only has an effect when ``type == 'linear'``.
    overwrite_data : bool, optional
        If True, perform in place detrending and avoid a copy. Default is False

    Returns
    -------
    ret : ndarray
        The detrended input data.

    Examples
    --------
    >>> from scipy import signal
    >>> from numpy.random import default_rng
    >>> rng = default_rng()
    >>> npoints = 1000
    >>> noise = rng.standard_normal(npoints)
    >>> x = 3 + 2*np.linspace(0, 1, npoints) + noise
    >>> (signal.detrend(x) - noise).max()
    0.06  # random

    """"""
    if type not in ['linear', 'l', 'constant', 'c']:
        raise ValueError(""Trend type must be 'linear' or 'constant'."")
    data = np.asarray(data)
    dtype = data.dtype.char
    if dtype not in 'dfDF':
        dtype = 'd'
    if type in ['constant', 'c']:
        ret = data - np.mean(data, axis, keepdims=True)
        return ret
    else:
        dshape = data.shape
        N = dshape[axis]
        bp = np.sort(np.unique(np.r_[0, bp, N]))
        if np.any(bp > N):
            raise ValueError(""Breakpoints must be less than length ""
                             ""of data along given axis."")
        Nreg = len(bp) - 1
        # Restructure data so that axis is along first dimension and
        #  all other dimensions are collapsed into second dimension
        rnk = len(dshape)
        if axis < 0:
            axis = axis + rnk
        newdims = np.r_[axis, 0:axis, axis + 1:rnk]
        newdata = np.reshape(np.transpose(data, tuple(newdims)),
                             (N, _prod(dshape) // N))
        if not overwrite_data:
            newdata = newdata.copy()  # make sure we have a copy
        if newdata.dtype.char not in 'dfDF':
            newdata = newdata.astype(dtype)
        # Find leastsq fit and remove it for each piece
        for m in range(Nreg):
            Npts = bp[m + 1] - bp[m]
            A = np.ones((Npts, 2), dtype)
            A[:, 0] = np.cast[dtype](np.arange(1, Npts + 1) * 1.0 / Npts)
            sl = slice(bp[m], bp[m + 1])
            coef, resids, rank, s = linalg.lstsq(A, newdata[sl])
            newdata[sl] = newdata[sl] - np.dot(A, coef)
        # Put data back in original shape.
        tdshape = np.take(dshape, newdims, 0)
        ret = np.reshape(newdata, tuple(tdshape))
        vals = list(range(1, rnk))
        olddims = vals[:axis] + [0] + vals[axis:]
        ret = np.transpose(ret, tuple(olddims))
        return ret","slice(bp[m], bp[m + 1])",slice(*bp[m:m + 2]),"iterable_zj[m], iterable_zj[m + 1]",*bp[m:m+2],*bp[m:m + 2],1
vega,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/networks/mindspore/faster_rcnn/roi_align.py,https://github.com/huawei-noah/vega/tree/master/vega/networks/mindspore/faster_rcnn/roi_align.py,ROIAlign,__init__$37,"def __init__(self,
                 out_size_h,
                 out_size_w,
                 spatial_scale,
                 sample_num=0):
        super(ROIAlign, self).__init__()

        self.out_size = (out_size_h, out_size_w)
        self.spatial_scale = float(spatial_scale)
        self.sample_num = int(sample_num)
        self.align_op = P.ROIAlign(self.out_size[0], self.out_size[1],
                                   self.spatial_scale, self.sample_num)","P.ROIAlign(self.out_size[0], self.out_size[1], self.spatial_scale, self.sample_num)","P.ROIAlign(*self.out_size[:2], self.spatial_scale, self.sample_num)","iterable_zj[0], iterable_zj[1]",*self.out_size[:2],*self.out_size[:2],1
centerNet-deep-sort,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/centerNet-deep-sort/CenterNet/src/lib/datasets/sample/ctdet.py,https://github.com/kimyoon-young/centerNet-deep-sort/tree/master/CenterNet/src/lib/datasets/sample/ctdet.py,CTDetDataset,__getitem__$29,"def __getitem__(self, index):
    img_id = self.images[index]
    file_name = self.coco.loadImgs(ids=[img_id])[0]['file_name']
    img_path = os.path.join(self.img_dir, file_name)
    ann_ids = self.coco.getAnnIds(imgIds=[img_id])
    anns = self.coco.loadAnns(ids=ann_ids)
    num_objs = min(len(anns), self.max_objs)

    img = cv2.imread(img_path)

    height, width = img.shape[0], img.shape[1]
    c = np.array([img.shape[1] / 2., img.shape[0] / 2.], dtype=np.float32)
    if self.opt.keep_res:
      input_h = (height | self.opt.pad) + 1
      input_w = (width | self.opt.pad) + 1
      s = np.array([input_w, input_h], dtype=np.float32)
    else:
      s = max(img.shape[0], img.shape[1]) * 1.0
      input_h, input_w = self.opt.input_h, self.opt.input_w
    
    flipped = False
    if self.split == 'train':
      if not self.opt.not_rand_crop:
        s = s * np.random.choice(np.arange(0.6, 1.4, 0.1))
        w_border = self._get_border(128, img.shape[1])
        h_border = self._get_border(128, img.shape[0])
        c[0] = np.random.randint(low=w_border, high=img.shape[1] - w_border)
        c[1] = np.random.randint(low=h_border, high=img.shape[0] - h_border)
      else:
        sf = self.opt.scale
        cf = self.opt.shift
        c[0] += s * np.clip(np.random.randn()*cf, -2*cf, 2*cf)
        c[1] += s * np.clip(np.random.randn()*cf, -2*cf, 2*cf)
        s = s * np.clip(np.random.randn()*sf + 1, 1 - sf, 1 + sf)
      
      if np.random.random() < self.opt.flip:
        flipped = True
        img = img[:, ::-1, :]
        c[0] =  width - c[0] - 1
        

    trans_input = get_affine_transform(
      c, s, 0, [input_w, input_h])
    inp = cv2.warpAffine(img, trans_input, 
                         (input_w, input_h),
                         flags=cv2.INTER_LINEAR)
    inp = (inp.astype(np.float32) / 255.)
    if self.split == 'train' and not self.opt.no_color_aug:
      color_aug(self._data_rng, inp, self._eig_val, self._eig_vec)
    inp = (inp - self.mean) / self.std
    inp = inp.transpose(2, 0, 1)

    output_h = input_h // self.opt.down_ratio
    output_w = input_w // self.opt.down_ratio
    num_classes = self.num_classes
    trans_output = get_affine_transform(c, s, 0, [output_w, output_h])

    hm = np.zeros((num_classes, output_h, output_w), dtype=np.float32)
    wh = np.zeros((self.max_objs, 2), dtype=np.float32)
    dense_wh = np.zeros((2, output_h, output_w), dtype=np.float32)
    reg = np.zeros((self.max_objs, 2), dtype=np.float32)
    ind = np.zeros((self.max_objs), dtype=np.int64)
    reg_mask = np.zeros((self.max_objs), dtype=np.uint8)
    cat_spec_wh = np.zeros((self.max_objs, num_classes * 2), dtype=np.float32)
    cat_spec_mask = np.zeros((self.max_objs, num_classes * 2), dtype=np.uint8)
    
    draw_gaussian = draw_msra_gaussian if self.opt.mse_loss else \
                    draw_umich_gaussian

    gt_det = []
    for k in range(num_objs):
      ann = anns[k]
      bbox = self._coco_box_to_bbox(ann['bbox'])
      cls_id = int(self.cat_ids[ann['category_id']])
      if flipped:
        bbox[[0, 2]] = width - bbox[[2, 0]] - 1
      bbox[:2] = affine_transform(bbox[:2], trans_output)
      bbox[2:] = affine_transform(bbox[2:], trans_output)
      bbox[[0, 2]] = np.clip(bbox[[0, 2]], 0, output_w - 1)
      bbox[[1, 3]] = np.clip(bbox[[1, 3]], 0, output_h - 1)
      h, w = bbox[3] - bbox[1], bbox[2] - bbox[0]
      if h > 0 and w > 0:
        radius = gaussian_radius((math.ceil(h), math.ceil(w)))
        radius = max(0, int(radius))
        radius = self.opt.hm_gauss if self.opt.mse_loss else radius
        ct = np.array(
          [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)
        ct_int = ct.astype(np.int32)
        draw_gaussian(hm[cls_id], ct_int, radius)
        wh[k] = 1. * w, 1. * h
        ind[k] = ct_int[1] * output_w + ct_int[0]
        reg[k] = ct - ct_int
        reg_mask[k] = 1
        cat_spec_wh[k, cls_id * 2: cls_id * 2 + 2] = wh[k]
        cat_spec_mask[k, cls_id * 2: cls_id * 2 + 2] = 1
        if self.opt.dense_wh:
          draw_dense_reg(dense_wh, hm.max(axis=0), ct_int, wh[k], radius)
        gt_det.append([ct[0] - w / 2, ct[1] - h / 2, 
                       ct[0] + w / 2, ct[1] + h / 2, 1, cls_id])
    
    ret = {'input': inp, 'hm': hm, 'reg_mask': reg_mask, 'ind': ind, 'wh': wh}
    if self.opt.dense_wh:
      hm_a = hm.max(axis=0, keepdims=True)
      dense_wh_mask = np.concatenate([hm_a, hm_a], axis=0)
      ret.update({'dense_wh': dense_wh, 'dense_wh_mask': dense_wh_mask})
      del ret['wh']
    elif self.opt.cat_spec_wh:
      ret.update({'cat_spec_wh': cat_spec_wh, 'cat_spec_mask': cat_spec_mask})
      del ret['wh']
    if self.opt.reg_offset:
      ret.update({'reg': reg})
    if self.opt.debug > 0 or not self.split == 'train':
      gt_det = np.array(gt_det, dtype=np.float32) if len(gt_det) > 0 else \
               np.zeros((1, 6), dtype=np.float32)
      meta = {'c': c, 's': s, 'gt_det': gt_det, 'img_id': img_id}
      ret['meta'] = meta
    return ret","max(img.shape[0], img.shape[1])",max(*img.shape[:2]),"iterable_zj[0], iterable_zj[1]",*img.shape[:2],*img.shape[:2],1
convnet-drawer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/convnet-drawer/convnet_drawer.py,https://github.com/yu4u/convnet-drawer/tree/master//convnet_drawer.py,Deconv2D,get_description$258,"def get_description(self):
        return [""deconv{}x{}, {}"".format(self.kernel_size[0], self.kernel_size[1], self.filters),
                ""stride {}"".format(self.strides)]","'deconv{}x{}, {}'.format(self.kernel_size[0], self.kernel_size[1], self.filters)","'deconv{}x{}, {}'.format(*self.kernel_size[:2], self.filters)","iterable_zj[0], iterable_zj[1]",*self.kernel_size[:2],*self.kernel_size[:2],1
DECA,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DECA/decalib/utils/lossfunc.py,https://github.com/YadiraF/DECA/tree/master/decalib/utils/lossfunc.py,VGGLoss,mrf_loss$590,"def mrf_loss(self, gen, tar):
        meanT = torch.mean(tar, 1, keepdim=True)
        gen_feats, tar_feats = gen - meanT, tar - meanT

        gen_feats_norm = torch.norm(gen_feats, p=2, dim=1, keepdim=True)
        tar_feats_norm = torch.norm(tar_feats, p=2, dim=1, keepdim=True)

        gen_normalized = gen_feats / gen_feats_norm
        tar_normalized = tar_feats / tar_feats_norm

        cosine_dist_l = []
        BatchSize = tar.size(0)

        for i in range(BatchSize):
            tar_feat_i = tar_normalized[i:i+1, :, :, :]
            gen_feat_i = gen_normalized[i:i+1, :, :, :]
            patches_OIHW = self.patch_extraction(tar_feat_i)

            cosine_dist_i = F.conv2d(gen_feat_i, patches_OIHW)
            cosine_dist_l.append(cosine_dist_i)
        cosine_dist = torch.cat(cosine_dist_l, dim=0)
        cosine_dist_zero_2_one = - (cosine_dist - 1) / 2
        relative_dist = self.compute_relative_distances(cosine_dist_zero_2_one)
        rela_dist = self.exp_norm_relative_dist(relative_dist)
        dims_div_mrf = rela_dist.size()
        k_max_nc = torch.max(rela_dist.view(dims_div_mrf[0], dims_div_mrf[1], -1), dim=2)[0]
        div_mrf = torch.mean(k_max_nc, dim=1)
        div_mrf_sum = -torch.log(div_mrf)
        div_mrf_sum = torch.sum(div_mrf_sum)
        return div_mrf_sum","rela_dist.view(dims_div_mrf[0], dims_div_mrf[1], -1)","rela_dist.view(*dims_div_mrf[:2], -1)","iterable_zj[0], iterable_zj[1]",*dims_div_mrf[:2],*dims_div_mrf[:2],1
NOFOUND
External-Attention-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/External-Attention-pytorch/model/attention/BAM.py,https://github.com/xmu-xiaoma666/External-Attention-pytorch/tree/master/model/attention/BAM.py,ChannelAttention,__init__$11,"def __init__(self,channel,reduction=16,num_layers=3):
        super().__init__()
        self.avgpool=nn.AdaptiveAvgPool2d(1)
        gate_channels=[channel]
        gate_channels+=[channel//reduction]*num_layers
        gate_channels+=[channel]


        self.ca=nn.Sequential()
        self.ca.add_module('flatten',Flatten())
        for i in range(len(gate_channels)-2):
            self.ca.add_module('fc%d'%i,nn.Linear(gate_channels[i],gate_channels[i+1]))
            self.ca.add_module('bn%d'%i,nn.BatchNorm1d(gate_channels[i+1]))
            self.ca.add_module('relu%d'%i,nn.ReLU())
        self.ca.add_module('last_fc',nn.Linear(gate_channels[-2],gate_channels[-1]))","nn.Linear(gate_channels[i], gate_channels[i + 1])",*gate_channels[i:i + 2],0
pynndescent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pynndescent/pynndescent/pynndescent_.py,https://github.com/lmcinnes/pynndescent/tree/master/pynndescent/pynndescent_.py,,degree_prune_internal$465,"def degree_prune_internal(indptr, data, max_degree=20):
    for i in numba.prange(indptr.shape[0] - 1):
        row_data = data[indptr[i] : indptr[i + 1]]
        if row_data.shape[0] > max_degree:
            cut_value = np.sort(row_data)[max_degree]
            for j in range(indptr[i], indptr[i + 1]):
                if data[j] > cut_value:
                    data[j] = 0.0

    return","range(indptr[i], indptr[i + 1])",*indptr[i:i + 2],0
Ordered-Neurons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Ordered-Neurons/ON_LSTM.py,https://github.com/yikangshen/Ordered-Neurons/tree/master//ON_LSTM.py,ONLSTMStack,__init__$122,"def __init__(self, layer_sizes, chunk_size, dropout=0., dropconnect=0.):
        super(ONLSTMStack, self).__init__()
        self.cells = nn.ModuleList([ONLSTMCell(layer_sizes[i],
                                               layer_sizes[i+1],
                                               chunk_size,
                                               dropconnect=dropconnect)
                                    for i in range(len(layer_sizes) - 1)])
        self.lockdrop = LockedDropout()
        self.dropout = dropout
        self.sizes = layer_sizes","ONLSTMCell(layer_sizes[i], layer_sizes[i + 1], chunk_size, dropconnect=dropconnect)",*layer_sizes[i:i + 2],0
cvpods,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cvpods/cvpods/modeling/meta_arch/yolov3.py,https://github.com/Megvii-BaseDetection/cvpods/tree/master/cvpods/modeling/meta_arch/yolov3.py,YOLOv3,_make_embedding$92,"def _make_embedding(self, filters_list, in_filters, out_filter):
        m = nn.ModuleList([
            self._make_cbl(in_filters, filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3),
            self._make_cbl(filters_list[1], filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3),
            self._make_cbl(filters_list[1], filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3)])
        m.add_module(""conv_out"", nn.Conv2d(filters_list[1], out_filter, kernel_size=1,
                                           stride=1, padding=0, bias=True))
        return m","self._make_cbl(filters_list[0], filters_list[1], 3)",*filters_list[:2],0
cvpods,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cvpods/cvpods/modeling/meta_arch/yolov3.py,https://github.com/Megvii-BaseDetection/cvpods/tree/master/cvpods/modeling/meta_arch/yolov3.py,YOLOv3,_make_embedding$92,"def _make_embedding(self, filters_list, in_filters, out_filter):
        m = nn.ModuleList([
            self._make_cbl(in_filters, filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3),
            self._make_cbl(filters_list[1], filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3),
            self._make_cbl(filters_list[1], filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3)])
        m.add_module(""conv_out"", nn.Conv2d(filters_list[1], out_filter, kernel_size=1,
                                           stride=1, padding=0, bias=True))
        return m","self._make_cbl(filters_list[0], filters_list[1], 3)",*filters_list[:2],0
cvpods,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cvpods/cvpods/modeling/meta_arch/yolov3.py,https://github.com/Megvii-BaseDetection/cvpods/tree/master/cvpods/modeling/meta_arch/yolov3.py,YOLOv3,_make_embedding$92,"def _make_embedding(self, filters_list, in_filters, out_filter):
        m = nn.ModuleList([
            self._make_cbl(in_filters, filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3),
            self._make_cbl(filters_list[1], filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3),
            self._make_cbl(filters_list[1], filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3)])
        m.add_module(""conv_out"", nn.Conv2d(filters_list[1], out_filter, kernel_size=1,
                                           stride=1, padding=0, bias=True))
        return m","self._make_cbl(filters_list[0], filters_list[1], 3)",*filters_list[:2],0
cornac,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cornac/cornac/models/bivaecf/bivae.py,https://github.com/PreferredAI/cornac/tree/master/cornac/models/bivaecf/bivae.py,BiVAE,__init__$36,"def __init__(
        self,
        k,
        user_encoder_structure,
        item_encoder_structure,
        act_fn,
        likelihood,
        cap_priors,
        feature_dim,
        batch_size,
    ):
        super(BiVAE, self).__init__()

        self.mu_theta = torch.zeros((item_encoder_structure[0], k))  # n_users*k
        self.mu_beta = torch.zeros((user_encoder_structure[0], k))  # n_items*k

        self.theta = torch.randn(item_encoder_structure[0], k) * 0.01
        self.beta = torch.randn(user_encoder_structure[0], k) * 0.01
        torch.nn.init.kaiming_uniform_(self.theta, a=np.sqrt(5))

        self.likelihood = likelihood
        self.act_fn = ACT.get(act_fn, None)
        if self.act_fn is None:
            raise ValueError(""Supported act_fn: {}"".format(ACT.keys()))

        self.cap_priors = cap_priors
        if self.cap_priors.get(""user"", False):
            self.user_prior_encoder = nn.Linear(feature_dim.get(""user""), k)
        if self.cap_priors.get(""item"", False):
            self.item_prior_encoder = nn.Linear(feature_dim.get(""item""), k)

        # User Encoder
        self.user_encoder = nn.Sequential()
        for i in range(len(user_encoder_structure) - 1):
            self.user_encoder.add_module(
                ""fc{}"".format(i),
                nn.Linear(user_encoder_structure[i], user_encoder_structure[i + 1]),
            )
            self.user_encoder.add_module(""act{}"".format(i), self.act_fn)
        self.user_mu = nn.Linear(user_encoder_structure[-1], k)  # mu
        self.user_std = nn.Linear(user_encoder_structure[-1], k)

        # Item Encoder
        self.item_encoder = nn.Sequential()
        for i in range(len(item_encoder_structure) - 1):
            self.item_encoder.add_module(
                ""fc{}"".format(i),
                nn.Linear(item_encoder_structure[i], item_encoder_structure[i + 1]),
            )
            self.item_encoder.add_module(""act{}"".format(i), self.act_fn)
        self.item_mu = nn.Linear(item_encoder_structure[-1], k)  # mu
        self.item_std = nn.Linear(item_encoder_structure[-1], k)","nn.Linear(user_encoder_structure[i], user_encoder_structure[i + 1])",*user_encoder_structure[i:i + 2],0
cornac,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cornac/cornac/models/bivaecf/bivae.py,https://github.com/PreferredAI/cornac/tree/master/cornac/models/bivaecf/bivae.py,BiVAE,__init__$36,"def __init__(
        self,
        k,
        user_encoder_structure,
        item_encoder_structure,
        act_fn,
        likelihood,
        cap_priors,
        feature_dim,
        batch_size,
    ):
        super(BiVAE, self).__init__()

        self.mu_theta = torch.zeros((item_encoder_structure[0], k))  # n_users*k
        self.mu_beta = torch.zeros((user_encoder_structure[0], k))  # n_items*k

        self.theta = torch.randn(item_encoder_structure[0], k) * 0.01
        self.beta = torch.randn(user_encoder_structure[0], k) * 0.01
        torch.nn.init.kaiming_uniform_(self.theta, a=np.sqrt(5))

        self.likelihood = likelihood
        self.act_fn = ACT.get(act_fn, None)
        if self.act_fn is None:
            raise ValueError(""Supported act_fn: {}"".format(ACT.keys()))

        self.cap_priors = cap_priors
        if self.cap_priors.get(""user"", False):
            self.user_prior_encoder = nn.Linear(feature_dim.get(""user""), k)
        if self.cap_priors.get(""item"", False):
            self.item_prior_encoder = nn.Linear(feature_dim.get(""item""), k)

        # User Encoder
        self.user_encoder = nn.Sequential()
        for i in range(len(user_encoder_structure) - 1):
            self.user_encoder.add_module(
                ""fc{}"".format(i),
                nn.Linear(user_encoder_structure[i], user_encoder_structure[i + 1]),
            )
            self.user_encoder.add_module(""act{}"".format(i), self.act_fn)
        self.user_mu = nn.Linear(user_encoder_structure[-1], k)  # mu
        self.user_std = nn.Linear(user_encoder_structure[-1], k)

        # Item Encoder
        self.item_encoder = nn.Sequential()
        for i in range(len(item_encoder_structure) - 1):
            self.item_encoder.add_module(
                ""fc{}"".format(i),
                nn.Linear(item_encoder_structure[i], item_encoder_structure[i + 1]),
            )
            self.item_encoder.add_module(""act{}"".format(i), self.act_fn)
        self.item_mu = nn.Linear(item_encoder_structure[-1], k)  # mu
        self.item_std = nn.Linear(item_encoder_structure[-1], k)","nn.Linear(item_encoder_structure[i], item_encoder_structure[i + 1])",*item_encoder_structure[i:i + 2],0
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/qiskit/circuit/library/generalized_gates/diagonal.py,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/circuit/library/generalized_gates/diagonal.py,Diagonal,__init__$76,"def __init__(self, diag: Union[List, np.array]) -> None:
        """"""Create a new Diagonal circuit.

        Args:
            diag: list of the 2^k diagonal entries (for a diagonal gate on k qubits).

        Raises:
            CircuitError: if the list of the diagonal entries or the qubit list is in bad format;
                if the number of diagonal entries is not 2^k, where k denotes the number of qubits
        """"""
        if not isinstance(diag, (list, np.ndarray)):
            raise CircuitError(""Diagonal entries must be in a list or numpy array."")
        num_qubits = np.log2(len(diag))
        if num_qubits < 1 or not num_qubits.is_integer():
            raise CircuitError(""The number of diagonal entries is not a positive power of 2."")
        if not np.allclose(np.abs(diag), 1, atol=_EPS):
            raise CircuitError(""A diagonal element does not have absolute value one."")

        num_qubits = int(num_qubits)

        circuit = QuantumCircuit(num_qubits, name=""Diagonal"")

        # Since the diagonal is a unitary, all its entries have absolute value
        # one and the diagonal is fully specified by the phases of its entries.
        diag_phases = [cmath.phase(z) for z in diag]
        n = len(diag)
        while n >= 2:
            angles_rz = []
            for i in range(0, n, 2):
                diag_phases[i // 2], rz_angle = _extract_rz(diag_phases[i], diag_phases[i + 1])
                angles_rz.append(rz_angle)
            num_act_qubits = int(np.log2(n))
            ctrl_qubits = list(range(num_qubits - num_act_qubits + 1, num_qubits))
            target_qubit = num_qubits - num_act_qubits
            circuit.ucrz(angles_rz, ctrl_qubits, target_qubit)
            n //= 2
        circuit.global_phase += diag_phases[0]

        super().__init__(num_qubits, name=""Diagonal"")
        self.append(circuit.to_gate(), self.qubits)","_extract_rz(diag_phases[i], diag_phases[i + 1])",*diag_phases[i:i + 2],0
DSB2017,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DSB2017/training/classifier/net_detector_3.py,https://github.com/lfz/DSB2017/tree/master/training/classifier/net_detector_3.py,Net,forward$103,"def forward(self, x, coord):
        #x = (x-128.)/128.
        out = self.preBlock(x)#16
        out_pool,indices0 = self.maxpool1(out)
        out1 = self.forw1(out_pool)#32
        out1_pool,indices1 = self.maxpool2(out1)
        out2 = self.forw2(out1_pool)#64
        #out2 = self.drop(out2)
        out2_pool,indices2 = self.maxpool3(out2)
        out3 = self.forw3(out2_pool)#96
        out3_pool,indices3 = self.maxpool4(out3)
        out4 = self.forw4(out3_pool)#96
        #out4 = self.drop(out4)
        
        rev3 = self.path1(out4)
        comb3 = self.back3(torch.cat((rev3, out3), 1))#96+96
        #comb3 = self.drop(comb3)
        rev2 = self.path2(comb3)
        
        feat = self.back2(torch.cat((rev2, out2,coord), 1))#64+64
        comb2 = self.drop(feat)
        out = self.output(comb2)
        size = out.size()
        out = out.view(out.size(0), out.size(1), -1)
        #out = out.transpose(1, 4).transpose(1, 2).transpose(2, 3).contiguous()
        out = out.transpose(1, 2).contiguous().view(size[0], size[2], size[3], size[4], len(config['anchors']), 5)
        #out = out.view(-1, 5)
        return feat,out","out.transpose(1, 2).contiguous().view(size[0], size[2], size[3], size[4], len(config['anchors']), 5)",*size[3:5],0
tf-cpn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-cpn/models/COCO.res101.384x288.CPN/dataset.py,https://github.com/chenyilun95/tf-cpn/tree/master/models/COCO.res101.384x288.CPN/dataset.py,,Preprocessing$162,"def Preprocessing(d, stage='train'):
    height, width = cfg.data_shape
    imgs = []
    labels = []
    valids = []
    if cfg.use_seg:
        segms = []

    vis = False
    img = cv2.imread(os.path.join(cfg.img_path, d['imgpath']))
    #hack(multiprocessing data provider)
    while img is None:
        print('read none image')
        time.sleep(np.random.rand() * 5)
        img = cv2.imread(os.path.join(cfg.img_path, d['imgpath']))
    add = max(img.shape[0], img.shape[1])
    bimg = cv2.copyMakeBorder(img, add, add, add, add, borderType=cv2.BORDER_CONSTANT,
                              value=cfg.pixel_means.reshape(-1))

    bbox = np.array(d['bbox']).reshape(4, ).astype(np.float32)
    bbox[:2] += add

    if 'joints' in d:
        joints = np.array(d['joints']).reshape(cfg.nr_skeleton, 3).astype(np.float32)
        joints[:, :2] += add
        inds = np.where(joints[:, -1] == 0)
        joints[inds, :2] = -1000000

    crop_width = bbox[2] * (1 + cfg.imgExtXBorder * 2)
    crop_height = bbox[3] * (1 + cfg.imgExtYBorder * 2)
    objcenter = np.array([bbox[0] + bbox[2] / 2., bbox[1] + bbox[3] / 2.])

    if stage == 'train':
        crop_width = crop_width * (1 + 0.25)
        crop_height = crop_height * (1 + 0.25)

    if crop_height / height > crop_width / width:
        crop_size = crop_height
        min_shape = height
    else:
        crop_size = crop_width
        min_shape = width
    crop_size = min(crop_size, objcenter[0] / width * min_shape * 2. - 1.)
    crop_size = min(crop_size, (bimg.shape[1] - objcenter[0]) / width * min_shape * 2. - 1)
    crop_size = min(crop_size, objcenter[1] / height * min_shape * 2. - 1.)
    crop_size = min(crop_size, (bimg.shape[0] - objcenter[1]) / height * min_shape * 2. - 1)

    min_x = int(objcenter[0] - crop_size / 2. / min_shape * width)
    max_x = int(objcenter[0] + crop_size / 2. / min_shape * width)
    min_y = int(objcenter[1] - crop_size / 2. / min_shape * height)
    max_y = int(objcenter[1] + crop_size / 2. / min_shape * height)

    x_ratio = float(width) / (max_x - min_x)
    y_ratio = float(height) / (max_y - min_y)

    if 'joints' in d:
        joints[:, 0] = joints[:, 0] - min_x
        joints[:, 1] = joints[:, 1] - min_y

        joints[:, 0] *= x_ratio
        joints[:, 1] *= y_ratio
        label = joints[:, :2].copy()
        valid = joints[:, 2].copy()

    img = cv2.resize(bimg[min_y:max_y, min_x:max_x, :], (width, height))

    if stage != 'train':
        details = np.asarray([min_x - add, min_y - add, max_x - add, max_y - add])

    if cfg.use_seg is True and 'segmentation' in d:
        seg = get_seg(ori_img.shape[0], ori_img.shape[1], d['segmentation'])
        add = max(seg.shape[0], seg.shape[1])
        bimg = cv2.copyMakeBorder(seg, add, add, add, add, borderType=cv2.BORDER_CONSTANT, value=(0, 0, 0))
        seg = cv2.resize(bimg[min_y:max_y, min_x:max_x], (width, height))
        segms.append(seg)

    if vis:
        tmpimg = img.copy()
        from utils.visualize import draw_skeleton
        draw_skeleton(tmpimg, label.astype(int))
        cv2.imwrite('vis.jpg', tmpimg)
        from IPython import embed; embed()

    img = img - cfg.pixel_means
    if cfg.pixel_norm:
        img = img / 255.
    img = img.transpose(2, 0, 1)
    imgs.append(img)
    if 'joints' in d:
        labels.append(label.reshape(-1))
        valids.append(valid.reshape(-1))

    if stage == 'train':
        imgs, labels, valids = data_augmentation(imgs, labels, valids)
        heatmaps15 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                        gaussian_kernel=cfg.gk15)
        heatmaps11 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                        gaussian_kernel=cfg.gk11)
        heatmaps9 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                       gaussian_kernel=cfg.gk9)
        heatmaps7 = joints_heatmap_gen(imgs, labels, cfg.output_shape, cfg.data_shape, return_valid=False,
                                       gaussian_kernel=cfg.gk7)

        return [imgs.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps15.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps11.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps9.astype(np.float32).transpose(0, 2, 3, 1),
                heatmaps7.astype(np.float32).transpose(0, 2, 3, 1),
                valids.astype(np.float32)]
    else:
        return [np.asarray(imgs).astype(np.float32), details]","get_seg(ori_img.shape[0], ori_img.shape[1], d['segmentation'])",*ori_img.shape[:2],0
GaitSet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GaitSet/work/OUMVLP_network/gaitset.py,https://github.com/AbnerHqC/GaitSet/tree/master/work/OUMVLP_network/gaitset.py,SetNet,__init__$2,"def __init__(self, hidden_dim):
        super(SetNet, self).__init__()
        self.hidden_dim = hidden_dim
        self.batch_frame = None
        
        _in_channels = 1
        _channels = [64,128,256]
        self.set_layer1 = SetBlock(BasicConv2d(_in_channels, _channels[0], 5, padding=2))
        self.set_layer2 = SetBlock(BasicConv2d(_channels[0], _channels[0], 3, padding=1), True)
        self.set_layer3 = SetBlock(BasicConv2d(_channels[0], _channels[1], 3, padding=1))
        self.set_layer4 = SetBlock(BasicConv2d(_channels[1], _channels[1], 3, padding=1), True)
        self.set_layer5 = SetBlock(BasicConv2d(_channels[1], _channels[2], 3, padding=1))
        self.set_layer6 = SetBlock(BasicConv2d(_channels[2], _channels[2], 3, padding=1))
        
        self.gl_layer1 = BasicConv2d(_channels[0], _channels[1], 3, padding=1)
        self.gl_layer2 = BasicConv2d(_channels[1], _channels[1], 3, padding=1)
        self.gl_layer3 = BasicConv2d(_channels[1], _channels[2], 3, padding=1)
        self.gl_layer4 = BasicConv2d(_channels[2], _channels[2], 3, padding=1)
        self.gl_pooling = nn.MaxPool2d(2)
        
        self.gl_hpm = HPM(_channels[-1], hidden_dim)
        self.x_hpm = HPM(_channels[-1], hidden_dim)
        
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.Conv1d)):
                nn.init.xavier_uniform(m.weight.data)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform(m.weight.data)
                nn.init.constant(m.bias.data, 0.0)
            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):
                nn.init.normal(m.weight.data, 1.0, 0.02)
                nn.init.constant(m.bias.data, 0.0)","BasicConv2d(_channels[0], _channels[1], 3, padding=1)",*_channels[:2],0
GaitSet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GaitSet/work/OUMVLP_network/gaitset.py,https://github.com/AbnerHqC/GaitSet/tree/master/work/OUMVLP_network/gaitset.py,SetNet,__init__$2,"def __init__(self, hidden_dim):
        super(SetNet, self).__init__()
        self.hidden_dim = hidden_dim
        self.batch_frame = None
        
        _in_channels = 1
        _channels = [64,128,256]
        self.set_layer1 = SetBlock(BasicConv2d(_in_channels, _channels[0], 5, padding=2))
        self.set_layer2 = SetBlock(BasicConv2d(_channels[0], _channels[0], 3, padding=1), True)
        self.set_layer3 = SetBlock(BasicConv2d(_channels[0], _channels[1], 3, padding=1))
        self.set_layer4 = SetBlock(BasicConv2d(_channels[1], _channels[1], 3, padding=1), True)
        self.set_layer5 = SetBlock(BasicConv2d(_channels[1], _channels[2], 3, padding=1))
        self.set_layer6 = SetBlock(BasicConv2d(_channels[2], _channels[2], 3, padding=1))
        
        self.gl_layer1 = BasicConv2d(_channels[0], _channels[1], 3, padding=1)
        self.gl_layer2 = BasicConv2d(_channels[1], _channels[1], 3, padding=1)
        self.gl_layer3 = BasicConv2d(_channels[1], _channels[2], 3, padding=1)
        self.gl_layer4 = BasicConv2d(_channels[2], _channels[2], 3, padding=1)
        self.gl_pooling = nn.MaxPool2d(2)
        
        self.gl_hpm = HPM(_channels[-1], hidden_dim)
        self.x_hpm = HPM(_channels[-1], hidden_dim)
        
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.Conv1d)):
                nn.init.xavier_uniform(m.weight.data)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform(m.weight.data)
                nn.init.constant(m.bias.data, 0.0)
            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):
                nn.init.normal(m.weight.data, 1.0, 0.02)
                nn.init.constant(m.bias.data, 0.0)","BasicConv2d(_channels[1], _channels[2], 3, padding=1)",*_channels[1:3],0
GaitSet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GaitSet/work/OUMVLP_network/gaitset.py,https://github.com/AbnerHqC/GaitSet/tree/master/work/OUMVLP_network/gaitset.py,SetNet,__init__$2,"def __init__(self, hidden_dim):
        super(SetNet, self).__init__()
        self.hidden_dim = hidden_dim
        self.batch_frame = None
        
        _in_channels = 1
        _channels = [64,128,256]
        self.set_layer1 = SetBlock(BasicConv2d(_in_channels, _channels[0], 5, padding=2))
        self.set_layer2 = SetBlock(BasicConv2d(_channels[0], _channels[0], 3, padding=1), True)
        self.set_layer3 = SetBlock(BasicConv2d(_channels[0], _channels[1], 3, padding=1))
        self.set_layer4 = SetBlock(BasicConv2d(_channels[1], _channels[1], 3, padding=1), True)
        self.set_layer5 = SetBlock(BasicConv2d(_channels[1], _channels[2], 3, padding=1))
        self.set_layer6 = SetBlock(BasicConv2d(_channels[2], _channels[2], 3, padding=1))
        
        self.gl_layer1 = BasicConv2d(_channels[0], _channels[1], 3, padding=1)
        self.gl_layer2 = BasicConv2d(_channels[1], _channels[1], 3, padding=1)
        self.gl_layer3 = BasicConv2d(_channels[1], _channels[2], 3, padding=1)
        self.gl_layer4 = BasicConv2d(_channels[2], _channels[2], 3, padding=1)
        self.gl_pooling = nn.MaxPool2d(2)
        
        self.gl_hpm = HPM(_channels[-1], hidden_dim)
        self.x_hpm = HPM(_channels[-1], hidden_dim)
        
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.Conv1d)):
                nn.init.xavier_uniform(m.weight.data)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform(m.weight.data)
                nn.init.constant(m.bias.data, 0.0)
            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):
                nn.init.normal(m.weight.data, 1.0, 0.02)
                nn.init.constant(m.bias.data, 0.0)","BasicConv2d(_channels[0], _channels[1], 3, padding=1)",*_channels[:2],0
GaitSet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GaitSet/work/OUMVLP_network/gaitset.py,https://github.com/AbnerHqC/GaitSet/tree/master/work/OUMVLP_network/gaitset.py,SetNet,__init__$2,"def __init__(self, hidden_dim):
        super(SetNet, self).__init__()
        self.hidden_dim = hidden_dim
        self.batch_frame = None
        
        _in_channels = 1
        _channels = [64,128,256]
        self.set_layer1 = SetBlock(BasicConv2d(_in_channels, _channels[0], 5, padding=2))
        self.set_layer2 = SetBlock(BasicConv2d(_channels[0], _channels[0], 3, padding=1), True)
        self.set_layer3 = SetBlock(BasicConv2d(_channels[0], _channels[1], 3, padding=1))
        self.set_layer4 = SetBlock(BasicConv2d(_channels[1], _channels[1], 3, padding=1), True)
        self.set_layer5 = SetBlock(BasicConv2d(_channels[1], _channels[2], 3, padding=1))
        self.set_layer6 = SetBlock(BasicConv2d(_channels[2], _channels[2], 3, padding=1))
        
        self.gl_layer1 = BasicConv2d(_channels[0], _channels[1], 3, padding=1)
        self.gl_layer2 = BasicConv2d(_channels[1], _channels[1], 3, padding=1)
        self.gl_layer3 = BasicConv2d(_channels[1], _channels[2], 3, padding=1)
        self.gl_layer4 = BasicConv2d(_channels[2], _channels[2], 3, padding=1)
        self.gl_pooling = nn.MaxPool2d(2)
        
        self.gl_hpm = HPM(_channels[-1], hidden_dim)
        self.x_hpm = HPM(_channels[-1], hidden_dim)
        
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.Conv1d)):
                nn.init.xavier_uniform(m.weight.data)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform(m.weight.data)
                nn.init.constant(m.bias.data, 0.0)
            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):
                nn.init.normal(m.weight.data, 1.0, 0.02)
                nn.init.constant(m.bias.data, 0.0)","BasicConv2d(_channels[1], _channels[2], 3, padding=1)",*_channels[1:3],0
apex,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/apex/tests/L0/run_mlp/test_mlp.py,https://github.com/NVIDIA/apex/tree/master/tests/L0/run_mlp/test_mlp.py,TestMLP,test_no_bias$54,"def test_no_bias(self):
        for use_activation in ['none', 'relu', 'sigmoid']:
            mlp = MLP(mlp_sizes, bias=False, activation=use_activation).cuda()

            mlp_layers = []
            for i in range(mlp.num_layers):
                linear = nn.Linear(mlp_sizes[i], mlp_sizes[i + 1], bias=False)
                mlp.weights[i].data.copy_(linear.weight)
                mlp_layers.append(linear)
                if use_activation == 'relu':
                    mlp_layers.append(nn.ReLU(inplace=True))
                if use_activation == 'sigmoid':
                    mlp_layers.append(nn.Sigmoid())

            ref_mlp = nn.Sequential(*mlp_layers).cuda()

            test_input = torch.empty(batch_size, mlp_sizes[0], device=""cuda"").uniform_(-1., 1.).requires_grad_()
            ref_input = test_input.clone().detach().requires_grad_()
            mlp_out = mlp(test_input)
            ref_out = ref_mlp(ref_input)
            np.testing.assert_allclose(
                mlp_out.detach().cpu().numpy(),
                ref_out.detach().cpu().numpy(),
                atol=1e-7, rtol=1e-5)

            # Use mean value as scalar loss. Multiply 10 to make it big enough not zero out
            mlp_out.mean().mul(10.).backward()
            ref_out.mean().mul(10.).backward()
            np.testing.assert_allclose(
                test_input.grad.detach().cpu().numpy(),
                ref_input.grad.detach().cpu().numpy(),
                atol=0, rtol=100)
            np.testing.assert_allclose(
                mlp.weights[0].grad.detach().cpu().numpy(),
                ref_mlp[0].weight.grad.detach().cpu().numpy(),
                atol=1e-7, rtol=100)","nn.Linear(mlp_sizes[i], mlp_sizes[i + 1], bias=False)",*mlp_sizes[i:i + 2],0
rewriting,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rewriting/baselines/neural_best_buddies/algorithms/feature_metric.py,https://github.com/davidbau/rewriting/tree/master/baselines/neural_best_buddies/algorithms/feature_metric.py,,identity_map$57,"def identity_map(size):
    idnty_map = torch.Tensor(size[0],2,size[2],size[3])
    idnty_map[0,0,:,:].copy_(torch.arange(0,size[2]).repeat(size[3],1).transpose(0,1))
    idnty_map[0,1,:,:].copy_(torch.arange(0,size[3]).repeat(size[2],1))
    return idnty_map","torch.Tensor(size[0], 2, size[2], size[3])",*size[2:4],0
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/geo/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/geo/geometry.py,Geometry,add_box$280,"def add_box(
        self,
        x0: float,
        x1: float,
        y0: float,
        y1: float,
        z0: float,
        z1: float,
        mesh_size: float | None = None,
        with_volume: bool = True,
        holes=None,
    ):
        if holes is None:
            holes = []

        if holes:
            assert with_volume

        # Define corner points.
        p = [
            self.add_point([x1, y1, z1], mesh_size=mesh_size),
            self.add_point([x1, y1, z0], mesh_size=mesh_size),
            self.add_point([x1, y0, z1], mesh_size=mesh_size),
            self.add_point([x1, y0, z0], mesh_size=mesh_size),
            self.add_point([x0, y1, z1], mesh_size=mesh_size),
            self.add_point([x0, y1, z0], mesh_size=mesh_size),
            self.add_point([x0, y0, z1], mesh_size=mesh_size),
            self.add_point([x0, y0, z0], mesh_size=mesh_size),
        ]
        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[0], p[2]),
            self.add_line(p[0], p[4]),
            self.add_line(p[1], p[3]),
            self.add_line(p[1], p[5]),
            self.add_line(p[2], p[3]),
            self.add_line(p[2], p[6]),
            self.add_line(p[3], p[7]),
            self.add_line(p[4], p[5]),
            self.add_line(p[4], p[6]),
            self.add_line(p[5], p[7]),
            self.add_line(p[6], p[7]),
        ]

        # Define the six line loops.
        ll = [
            self.add_curve_loop([e[0], e[3], -e[5], -e[1]]),
            self.add_curve_loop([e[0], e[4], -e[8], -e[2]]),
            self.add_curve_loop([e[1], e[6], -e[9], -e[2]]),
            self.add_curve_loop([e[3], e[7], -e[10], -e[4]]),
            self.add_curve_loop([e[5], e[7], -e[11], -e[6]]),
            self.add_curve_loop([e[8], e[10], -e[11], -e[9]]),
        ]

        # Create a surface for each line loop.
        s = [self.add_surface(l) for l in ll]
        # Create the surface loop.
        surface_loop = self.add_surface_loop(s)

        # Create volume
        vol = self.add_volume(surface_loop, holes) if with_volume else None

        class Box:
            def __init__(
                self, x0, x1, y0, y1, z0, z1, surface_loop, volume, mesh_size=None
            ):
                self.x0 = x0
                self.x1 = x1
                self.y0 = y0
                self.y1 = y1
                self.z0 = z0
                self.z1 = z1
                self.mesh_size = mesh_size
                self.surface_loop = surface_loop
                self.volume = volume

        return Box(x0, x1, y0, y1, z0, z1, surface_loop, vol, mesh_size=mesh_size)","self.add_line(p[2], p[6])",*p[2:10:4],0
LightNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LightNet/modules/residual.py,https://github.com/linksense/LightNet/tree/master/modules/residual.py,IdentityResidualBlock,__init__$8,"def __init__(self, in_channels, channels, stride=1, dilation=1, groups=1, norm_act=ABN, is_se=False, dropout=None):
        """"""Configurable identity-mapping residual block

        Parameters
        ----------
        in_channels : int
            Number of input channels.
        channels : list of int
            Number of channels in the internal feature maps. Can either have two or three elements: if three construct
            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then
            `3 x 3` then `1 x 1` convolutions.
        stride : int
            Stride of the first `3 x 3` convolution
        dilation : int
            Dilation to apply to the `3 x 3` convolutions.
        groups : int
            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with
            bottleneck blocks.
        norm_act : callable
            Function to create normalization / activation Module.
        dropout: callable
            Function to create Dropout Module.
        """"""
        super(IdentityResidualBlock, self).__init__()

        # Check parameters for inconsistencies
        if len(channels) != 2 and len(channels) != 3:
            raise ValueError(""channels must contain either two or three values"")
        if len(channels) == 2 and groups != 1:
            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")

        is_bottleneck = len(channels) == 3
        need_proj_conv = stride != 1 or in_channels != channels[-1]

        self.bn1 = norm_act(in_channels)
        if not is_bottleneck:
            layers = [
                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,
                                    dilation=dilation)),
                (""bn2"", norm_act(channels[0])),
                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                    dilation=dilation))
            ]
            if dropout is not None:
                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]
        else:
            if not is_se:
                layers = [
                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),
                    (""bn2"", norm_act(channels[0])),
                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                        groups=groups, dilation=dilation)),
                    (""bn3"", norm_act(channels[1])),
                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))
                ]
            else:
                layers = [
                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),
                    (""bn2"", norm_act(channels[0])),
                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                        groups=groups, dilation=dilation)),
                    (""bn3"", norm_act(channels[1])),
                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False)),
                    (""se_block"", SEBlock(channels[2], 16))
                ]
            if dropout is not None:
                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]
        self.convs = nn.Sequential(OrderedDict(layers))

        if need_proj_conv:
            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)","nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False, dilation=dilation)",*channels[:2],0
LightNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LightNet/modules/residual.py,https://github.com/linksense/LightNet/tree/master/modules/residual.py,IdentityResidualBlock,__init__$8,"def __init__(self, in_channels, channels, stride=1, dilation=1, groups=1, norm_act=ABN, is_se=False, dropout=None):
        """"""Configurable identity-mapping residual block

        Parameters
        ----------
        in_channels : int
            Number of input channels.
        channels : list of int
            Number of channels in the internal feature maps. Can either have two or three elements: if three construct
            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then
            `3 x 3` then `1 x 1` convolutions.
        stride : int
            Stride of the first `3 x 3` convolution
        dilation : int
            Dilation to apply to the `3 x 3` convolutions.
        groups : int
            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with
            bottleneck blocks.
        norm_act : callable
            Function to create normalization / activation Module.
        dropout: callable
            Function to create Dropout Module.
        """"""
        super(IdentityResidualBlock, self).__init__()

        # Check parameters for inconsistencies
        if len(channels) != 2 and len(channels) != 3:
            raise ValueError(""channels must contain either two or three values"")
        if len(channels) == 2 and groups != 1:
            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")

        is_bottleneck = len(channels) == 3
        need_proj_conv = stride != 1 or in_channels != channels[-1]

        self.bn1 = norm_act(in_channels)
        if not is_bottleneck:
            layers = [
                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,
                                    dilation=dilation)),
                (""bn2"", norm_act(channels[0])),
                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                    dilation=dilation))
            ]
            if dropout is not None:
                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]
        else:
            if not is_se:
                layers = [
                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),
                    (""bn2"", norm_act(channels[0])),
                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                        groups=groups, dilation=dilation)),
                    (""bn3"", norm_act(channels[1])),
                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))
                ]
            else:
                layers = [
                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),
                    (""bn2"", norm_act(channels[0])),
                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                        groups=groups, dilation=dilation)),
                    (""bn3"", norm_act(channels[1])),
                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False)),
                    (""se_block"", SEBlock(channels[2], 16))
                ]
            if dropout is not None:
                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]
        self.convs = nn.Sequential(OrderedDict(layers))

        if need_proj_conv:
            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)","nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False, groups=groups, dilation=dilation)",*channels[:2],0
LightNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LightNet/modules/residual.py,https://github.com/linksense/LightNet/tree/master/modules/residual.py,IdentityResidualBlock,__init__$8,"def __init__(self, in_channels, channels, stride=1, dilation=1, groups=1, norm_act=ABN, is_se=False, dropout=None):
        """"""Configurable identity-mapping residual block

        Parameters
        ----------
        in_channels : int
            Number of input channels.
        channels : list of int
            Number of channels in the internal feature maps. Can either have two or three elements: if three construct
            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then
            `3 x 3` then `1 x 1` convolutions.
        stride : int
            Stride of the first `3 x 3` convolution
        dilation : int
            Dilation to apply to the `3 x 3` convolutions.
        groups : int
            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with
            bottleneck blocks.
        norm_act : callable
            Function to create normalization / activation Module.
        dropout: callable
            Function to create Dropout Module.
        """"""
        super(IdentityResidualBlock, self).__init__()

        # Check parameters for inconsistencies
        if len(channels) != 2 and len(channels) != 3:
            raise ValueError(""channels must contain either two or three values"")
        if len(channels) == 2 and groups != 1:
            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")

        is_bottleneck = len(channels) == 3
        need_proj_conv = stride != 1 or in_channels != channels[-1]

        self.bn1 = norm_act(in_channels)
        if not is_bottleneck:
            layers = [
                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,
                                    dilation=dilation)),
                (""bn2"", norm_act(channels[0])),
                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                    dilation=dilation))
            ]
            if dropout is not None:
                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]
        else:
            if not is_se:
                layers = [
                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),
                    (""bn2"", norm_act(channels[0])),
                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                        groups=groups, dilation=dilation)),
                    (""bn3"", norm_act(channels[1])),
                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))
                ]
            else:
                layers = [
                    (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),
                    (""bn2"", norm_act(channels[0])),
                    (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,
                                        groups=groups, dilation=dilation)),
                    (""bn3"", norm_act(channels[1])),
                    (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False)),
                    (""se_block"", SEBlock(channels[2], 16))
                ]
            if dropout is not None:
                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]
        self.convs = nn.Sequential(OrderedDict(layers))

        if need_proj_conv:
            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)","nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False, groups=groups, dilation=dilation)",*channels[:2],0
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/ops/pointnet2/pytorch_utils.py,https://github.com/poodarchu/Det3D/tree/master/det3d/ops/pointnet2/pytorch_utils.py,SharedMLP,__init__$14,"def __init__(
        self,
        args: List[int],
        *,
        bn: bool = False,
        activation=nn.ReLU(inplace=True),
        preact: bool = False,
        first: bool = False,
        name: str = """"
    ):
        super().__init__()

        for i in range(len(args) - 1):
            self.add_module(
                name + ""layer{}"".format(i),
                Conv2d(
                    args[i],
                    args[i + 1],
                    bn=(not first or not preact or (i != 0)) and bn,
                    activation=activation
                    if (not first or not preact or (i != 0))
                    else None,
                    preact=preact,
                ),
            )","Conv2d(args[i], args[i + 1], bn=(not first or not preact or i != 0) and bn, activation=activation if not first or not preact or i != 0 else None, preact=preact)",*args[i:i + 2],0
pyGAM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyGAM/gen_imgs.py,https://github.com/dswah/pyGAM/tree/master//gen_imgs.py,,chicago_tensor$269,"def chicago_tensor():
    """"""
    chicago tensor
    """"""
    X, y = chicago()
    gam = PoissonGAM(s(0, n_splines=200) + te(3, 1) + s(2)).fit(X, y)

    XX = gam.generate_X_grid(term=1, meshgrid=True)
    Z = gam.partial_dependence(term=1, meshgrid=True)

    fig = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_surface(XX[0], XX[1], Z, cmap='viridis')
    fig.tight_layout()

    plt.savefig('imgs/pygam_chicago_tensor.png', dpi=300)","ax.plot_surface(XX[0], XX[1], Z, cmap='viridis')",*XX[:2],0
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/detr/modeling_detr.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/detr/modeling_detr.py,DetrMHAttentionMap,forward$1855,"def forward(self, q, k, mask: Optional[Tensor] = None):
        q = self.q_linear(q)
        k = nn.functional.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)
        queries_per_head = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)
        keys_per_head = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])
        weights = torch.einsum(""bqnc,bnchw->bqnhw"", queries_per_head * self.normalize_fact, keys_per_head)

        if mask is not None:
            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)
        weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())
        weights = self.dropout(weights)
        return weights","k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])",*k.shape[-2:0],0
vid2vid,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vid2vid/models/vid2vid_model_G.py,https://github.com/NVIDIA/vid2vid/tree/master/models/vid2vid_model_G.py,Vid2VidModelG,get_face_features$290,"def get_face_features(self, real_image, inst):                
        feat_map = self.netE.forward(real_image, inst)            
        #if self.opt.use_encoded_image:
        #    return feat_map
        
        load_name = 'checkpoints/edge2face_single/features.npy'
        features = np.load(load_name, encoding='latin1').item()                        
        inst_np = inst.cpu().numpy().astype(int)

        # find nearest neighbor in the training dataset
        num_images = features[6].shape[0]
        feat_map = feat_map.data.cpu().numpy()
        feat_ori = torch.FloatTensor(7, self.opt.feat_num, 1) # feature map for test img (for each facial part)
        feat_ref = torch.FloatTensor(7, self.opt.feat_num, num_images) # feature map for training imgs
        for label in np.unique(inst_np):
            idx = (inst == int(label)).nonzero() 
            for k in range(self.opt.feat_num): 
                feat_ori[label,k] = float(feat_map[idx[0,0], idx[0,1] + k, idx[0,2], idx[0,3]])
                for m in range(num_images):
                    feat_ref[label,k,m] = features[label][m,k]                
        cluster_idx = self.dists_min(feat_ori.expand_as(feat_ref).cuda(), feat_ref.cuda(), num=1)

        # construct new feature map from nearest neighbors
        feat_map = self.Tensor(inst.size()[0], self.opt.feat_num, inst.size()[2], inst.size()[3])
        for label in np.unique(inst_np):
            feat = features[label][:,:-1]                                                    
            idx = (inst == int(label)).nonzero()                
            for k in range(self.opt.feat_num):                    
                feat_map[idx[:,0], idx[:,1] + k, idx[:,2], idx[:,3]] = feat[min(cluster_idx, feat.shape[0]-1), k]
        
        return Variable(feat_map)","self.Tensor(inst.size()[0], self.opt.feat_num, inst.size()[2], inst.size()[3])",*inst.size()[2:4],0
tensorlayer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorlayer/tensorlayer/prepro.py,https://github.com/tensorlayer/tensorlayer/tree/master/tensorlayer/prepro.py,,obj_box_zoom$3137,"def obj_box_zoom(
    im, classes=None, coords=None, zoom_range=(0.9, 1.1), row_index=0, col_index=1, channel_index=2,
    fill_mode='nearest', cval=0., order=1, is_rescale=False, is_center=False, is_random=False, thresh_wh=0.02,
    thresh_wh2=12.
):
    """"""Zoom in and out of a single image, randomly or non-randomly, and compute the new bounding box coordinates.
    Objects outside the cropped image will be removed.

    Parameters
    -----------
    im : numpy.array
        An image with dimension of [row, col, channel] (default).
    classes : list of int or None
        Class IDs.
    coords : list of list of 4 int/float or None
        Coordinates [[x, y, w, h], [x, y, w, h], ...].
    zoom_range row_index col_index channel_index is_random fill_mode cval and order : see ``tl.prepro.zoom``.
    is_rescale : boolean
        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.
    is_center : boolean
        Set to True, if the x and y of coordinates are the centroid. (i.e. darknet format). Default is False.
    thresh_wh : float
        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.
    thresh_wh2 : float
        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.

    Returns
    -------
    numpy.array
        A processed image
    list of int
        A list of classes
    list of list of 4 numbers
        A list of new bounding boxes.

    """"""
    if classes is None:
        classes = []
    if coords is None:
        coords = []

    if len(zoom_range) != 2:
        raise Exception('zoom_range should be a tuple or list of two floats. ' 'Received arg: ', zoom_range)
    if is_random:
        if zoom_range[0] == 1 and zoom_range[1] == 1:
            zx, zy = 1, 1
            tl.logging.info("" random_zoom : not zoom in/out"")
        else:
            zx, zy = np.random.uniform(zoom_range[0], zoom_range[1], 2)
    else:
        zx, zy = zoom_range
    # tl.logging.info(zx, zy)
    zoom_matrix = np.array([[zx, 0, 0], [0, zy, 0], [0, 0, 1]])

    h, w = im.shape[row_index], im.shape[col_index]
    transform_matrix = transform_matrix_offset_center(zoom_matrix, h, w)
    im_new = affine_transform(im, transform_matrix, channel_index, fill_mode, cval, order)

    # modified from obj_box_crop
    def _get_coord(coord):
        """"""Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,
        before getting the new coordinates.
        Boxes outsides the cropped image will be removed.

        """"""
        if is_center:
            coord = obj_box_coord_centroid_to_upleft(coord)

        # ======= pixel unit format and upleft, w, h ==========
        x = (coord[0] - im.shape[1] / 2) / zy + im.shape[1] / 2  # only change this
        y = (coord[1] - im.shape[0] / 2) / zx + im.shape[0] / 2  # only change this
        w = coord[2] / zy  # only change this
        h = coord[3] / zx  # only change thisS

        if x < 0:
            if x + w <= 0:
                return None
            w = w + x
            x = 0
        elif x > im_new.shape[1]:  # object outside the cropped image
            return None

        if y < 0:
            if y + h <= 0:
                return None
            h = h + y
            y = 0
        elif y > im_new.shape[0]:  # object outside the cropped image
            return None

        if (x is not None) and (x + w > im_new.shape[1]):  # box outside the cropped image
            w = im_new.shape[1] - x

        if (y is not None) and (y + h > im_new.shape[0]):  # box outside the cropped image
            h = im_new.shape[0] - y

        if (w / (h + 1.) > thresh_wh2) or (h / (w + 1.) > thresh_wh2):  # object shape strange: too narrow
            # tl.logging.info('xx', w, h)
            return None

        if (w / (im_new.shape[1] * 1.) < thresh_wh) or (h / (im_new.shape[0] * 1.) <
                                                        thresh_wh):  # object shape strange: too narrow
            # tl.logging.info('yy', w, im_new.shape[1], h, im_new.shape[0])
            return None

        coord = [x, y, w, h]

        # convert back if input format is center.
        if is_center:
            coord = obj_box_coord_upleft_to_centroid(coord)

        return coord

    coords_new = list()
    classes_new = list()
    for i, _ in enumerate(coords):
        coord = coords[i]

        if len(coord) != 4:
            raise AssertionError(""coordinate should be 4 values : [x, y, w, h]"")

        if is_rescale:
            # for scaled coord, upscaled before process and scale back in the end.
            coord = obj_box_coord_scale_to_pixelunit(coord, im.shape)
            coord = _get_coord(coord)
            if coord is not None:
                coord = obj_box_coord_rescale(coord, im_new.shape)
                coords_new.append(coord)
                classes_new.append(classes[i])
        else:
            coord = _get_coord(coord)
            if coord is not None:
                coords_new.append(coord)
                classes_new.append(classes[i])
    return im_new, classes_new, coords_new","np.random.uniform(zoom_range[0], zoom_range[1], 2)",*zoom_range[:2],0
kornia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kornia/kornia/geometry/boxes.py,https://github.com/kornia/kornia/tree/master/kornia/geometry/boxes.py,Boxes3D,to_tensor$499,"def to_tensor(self, mode: str = ""xyzxyz"") -> torch.Tensor:
        r""""""Cast :class:`Boxes3D` to a tensor. ``mode`` controls which 3D boxes format should be use to represent boxes
        in the tensor.

        Args:
            mode: The format in which the boxes are provided.

                * 'xyzxyz': boxes are assumed to be in the format ``xmin, ymin, zmin, xmax, ymax, zmax`` where
                  ``width = xmax - xmin``, ``height = ymax - ymin`` and ``depth = zmax - zmin``.
                * 'xyzxyz_plus': similar to 'xyzxyz' mode but where box width, length and depth are defined as
                   ``width = xmax - xmin + 1``, ``height = ymax - ymin + 1`` and ``depth = zmax - zmin + 1``.
                * 'xyzwhd': boxes are assumed to be in the format ``xmin, ymin, zmin, width, height, depth`` where
                  ``width = xmax - xmin``, ``height = ymax - ymin`` and ``depth = zmax - zmin``.
                * 'vertices': boxes are defined by their vertices points in the following ``clockwise`` order:
                  *front-top-left, front-top-right, front-bottom-right, front-bottom-left, back-top-left,
                  back-top-right, back-bottom-right,  back-bottom-left*. Vertices coordinates are in (x,y, z) order.
                  Finally, box width, height and depth are defined as ``width = xmax - xmin``, ``height = ymax - ymin``
                  and ``depth = zmax - zmin``.
                * 'vertices_plus': similar to 'vertices' mode but where box width, length and depth are defined as
                  ``width = xmax - xmin + 1`` and ``height = ymax - ymin + 1``.

        Returns:
            3D Boxes tensor in the ``mode`` format. The shape depends with the ``mode`` value:

                * 'vertices' or 'verticies_plus': :math:`(N, 8, 3)` or :math:`(B, N, 8, 3)`.
                * Any other value: :math:`(N, 6)` or :math:`(B, N, 6)`.

        Note:
            It is currently non-differentiable due to a bug. See github issue
            `#1304 <https://github.com/kornia/kornia/issues/1396>`_.

        Examples:
            >>> boxes_xyzxyz = torch.as_tensor([[0, 3, 6, 1, 4, 8], [5, 1, 3, 8, 4, 9]])
            >>> boxes = Boxes3D.from_tensor(boxes_xyzxyz, mode='xyzxyz')
            >>> assert (boxes.to_tensor(mode='xyzxyz') == boxes_xyzxyz).all()
        """"""
        if self._boxes.requires_grad:
            raise RuntimeError(""Boxes3D.to_tensor doesn't support computing gradients since they aren't accurate. ""
                               ""Please, create boxes from tensors with `requires_grad=False`. ""
                               ""This is a known bug. Help is needed to fix it. For more information, ""
                               ""see https://github.com/kornia/kornia/issues/1396."")

        batched_boxes = self._boxes if self._is_batch else self._boxes.unsqueeze(0)

        # Create boxes in xyzxyz_plus format.
        boxes = torch.stack([batched_boxes.amin(dim=-2), batched_boxes.amax(dim=-2)], dim=-2).view(
            batched_boxes.shape[0], batched_boxes.shape[1], 6
        )

        mode = mode.lower()
        if mode in (""xyzxyz"", ""xyzxyz_plus""):
            pass
        elif mode in (""xyzwhd"", ""vertices"", ""vertices_plus""):
            width = boxes[..., 3] - boxes[..., 0] + 1
            height = boxes[..., 4] - boxes[..., 1] + 1
            depth = boxes[..., 5] - boxes[..., 2] + 1
            boxes[..., 3] = width
            boxes[..., 4] = height
            boxes[..., 5] = depth
        else:
            raise ValueError(f""Unknown mode {mode}"")

        if mode in (""xyzxyz"", ""vertices""):
            offset = torch.as_tensor([0, 0, 0, 1, 1, 1], device=boxes.device, dtype=boxes.dtype)
            boxes = boxes + offset

        if mode.startswith('vertices'):
            xmin, ymin, zmin = boxes[..., 0], boxes[..., 1], boxes[..., 2]
            width, height, depth = boxes[..., 3], boxes[..., 4], boxes[..., 5]

            boxes = _boxes3d_to_polygons3d(xmin, ymin, zmin, width, height, depth)

        boxes = boxes if self._is_batch else boxes.squeeze(0)
        return boxes","torch.stack([batched_boxes.amin(dim=-2), batched_boxes.amax(dim=-2)], dim=-2).view(batched_boxes.shape[0], batched_boxes.shape[1], 6)",*batched_boxes.shape[:2],0
OpenPCDet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenPCDet/pcdet/ops/pointnet2/pointnet2_stack/pointnet2_modules.py,https://github.com/open-mmlab/OpenPCDet/tree/master/pcdet/ops/pointnet2/pointnet2_stack/pointnet2_modules.py,VectorPoolLocalInterpolateModule,__init__$161,"def __init__(self, mlp, num_voxels, max_neighbour_distance, nsample, neighbor_type, use_xyz=True,
                 neighbour_distance_multiplier=1.0, xyz_encoding_type='concat'):
        """"""
        Args:
            mlp:
            num_voxels:
            max_neighbour_distance:
            neighbor_type: 1: ball, others: cube
            nsample: find all (-1), find limited number(>0)
            use_xyz:
            neighbour_distance_multiplier:
            xyz_encoding_type:
        """"""
        super().__init__()
        self.num_voxels = num_voxels  # [num_grid_x, num_grid_y, num_grid_z]: number of grids in each local area centered at new_xyz
        self.num_total_grids = self.num_voxels[0] * self.num_voxels[1] * self.num_voxels[2]
        self.max_neighbour_distance = max_neighbour_distance
        self.neighbor_distance_multiplier = neighbour_distance_multiplier
        self.nsample = nsample
        self.neighbor_type = neighbor_type
        self.use_xyz = use_xyz
        self.xyz_encoding_type = xyz_encoding_type

        if mlp is not None:
            if self.use_xyz:
                mlp[0] += 9 if self.xyz_encoding_type == 'concat' else 0
            shared_mlps = []
            for k in range(len(mlp) - 1):
                shared_mlps.extend([
                    nn.Conv2d(mlp[k], mlp[k + 1], kernel_size=1, bias=False),
                    nn.BatchNorm2d(mlp[k + 1]),
                    nn.ReLU()
                ])
            self.mlp = nn.Sequential(*shared_mlps)
        else:
            self.mlp = None

        self.num_avg_length_of_neighbor_idxs = 1000","nn.Conv2d(mlp[k], mlp[k + 1], kernel_size=1, bias=False)",*mlp[k:k + 2],0
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/container/test_sharder.py,https://github.com/openstack/swift/tree/master/test/unit/container/test_sharder.py,TestSharder,test_misplaced_objects_deleted_and_updated$4320,"def test_misplaced_objects_deleted_and_updated(self):
        # setup
        broker = self._make_broker()
        broker.enable_sharding(next(self.ts_iter))

        shard_bounds = (('', 'here'), ('here', ''))
        root_shard_ranges = self._make_shard_ranges(
            shard_bounds, state=ShardRange.ACTIVE)
        expected_shard_dbs = []
        for sr in root_shard_ranges:
            db_hash = hash_path(sr.account, sr.container)
            expected_shard_dbs.append(
                os.path.join(self.tempdir, 'sda', 'containers', '0',
                             db_hash[-3:], db_hash, db_hash + '.db'))
        broker.merge_shard_ranges(root_shard_ranges)
        self.assertTrue(broker.set_sharding_state())

        ts_older_internal = self.ts_encoded()  # used later
        # put deleted objects into source
        objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(objects, broker.db_file)  # sanity check
        # pretend we cleaved all ranges - sharded state
        self.assertTrue(broker.set_sharded_state())

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        expected_stats = {'attempted': 1, 'success': 1, 'failure': 0,
                          'found': 1, 'placed': 2, 'unplaced': 0}
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check new misplaced objects were moved
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source db with older undeleted versions of same objects
        old_objects = [
            ['b', ts_older_internal, 2, 'text/plain', 'etag_b', 0, 0],
            ['x', ts_older_internal, 4, 'text/plain', 'etag_x', 0, 0]
        ]
        for obj in old_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(old_objects, broker.db_file)  # sanity check
        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check older misplaced objects were not merged to shard brokers
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # the destination shard dbs for misplaced objects may already exist so
        # check they are updated correctly when overwriting objects
        # update source db with newer deleted versions of same objects
        new_objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in new_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(new_objects, broker.db_file)  # sanity check
        shard_broker = ContainerBroker(
            expected_shard_dbs[0], account=root_shard_ranges[0].account,
            container=root_shard_ranges[0].container)
        # update one shard container with even newer version of object
        timestamps = [next(self.ts_iter) for i in range(7)]
        ts_newer = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[5])
        newer_object = ('b', ts_newer, 10, 'text/plain', 'etag_b', 0, 0)
        shard_broker.put_object(*newer_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check only the newer misplaced object was moved
        self._check_objects([newer_object], expected_shard_dbs[0])
        self._check_objects(new_objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has newer data
        # but older content-type and metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[4])
        update_object = ('b', ts_update, 20, 'text/ignored', 'etag_newer', 0,
                         0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[5])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # and content-type but newer metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/ignored', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # but newer content-type and metadata
        ts_update = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/newer', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/newer', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)","encode_timestamps(timestamps[2], timestamps[3], timestamps[5])",*timestamps[2:4],0
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/container/test_sharder.py,https://github.com/openstack/swift/tree/master/test/unit/container/test_sharder.py,TestSharder,test_misplaced_objects_deleted_and_updated$4320,"def test_misplaced_objects_deleted_and_updated(self):
        # setup
        broker = self._make_broker()
        broker.enable_sharding(next(self.ts_iter))

        shard_bounds = (('', 'here'), ('here', ''))
        root_shard_ranges = self._make_shard_ranges(
            shard_bounds, state=ShardRange.ACTIVE)
        expected_shard_dbs = []
        for sr in root_shard_ranges:
            db_hash = hash_path(sr.account, sr.container)
            expected_shard_dbs.append(
                os.path.join(self.tempdir, 'sda', 'containers', '0',
                             db_hash[-3:], db_hash, db_hash + '.db'))
        broker.merge_shard_ranges(root_shard_ranges)
        self.assertTrue(broker.set_sharding_state())

        ts_older_internal = self.ts_encoded()  # used later
        # put deleted objects into source
        objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(objects, broker.db_file)  # sanity check
        # pretend we cleaved all ranges - sharded state
        self.assertTrue(broker.set_sharded_state())

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        expected_stats = {'attempted': 1, 'success': 1, 'failure': 0,
                          'found': 1, 'placed': 2, 'unplaced': 0}
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check new misplaced objects were moved
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source db with older undeleted versions of same objects
        old_objects = [
            ['b', ts_older_internal, 2, 'text/plain', 'etag_b', 0, 0],
            ['x', ts_older_internal, 4, 'text/plain', 'etag_x', 0, 0]
        ]
        for obj in old_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(old_objects, broker.db_file)  # sanity check
        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check older misplaced objects were not merged to shard brokers
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # the destination shard dbs for misplaced objects may already exist so
        # check they are updated correctly when overwriting objects
        # update source db with newer deleted versions of same objects
        new_objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in new_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(new_objects, broker.db_file)  # sanity check
        shard_broker = ContainerBroker(
            expected_shard_dbs[0], account=root_shard_ranges[0].account,
            container=root_shard_ranges[0].container)
        # update one shard container with even newer version of object
        timestamps = [next(self.ts_iter) for i in range(7)]
        ts_newer = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[5])
        newer_object = ('b', ts_newer, 10, 'text/plain', 'etag_b', 0, 0)
        shard_broker.put_object(*newer_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check only the newer misplaced object was moved
        self._check_objects([newer_object], expected_shard_dbs[0])
        self._check_objects(new_objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has newer data
        # but older content-type and metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[4])
        update_object = ('b', ts_update, 20, 'text/ignored', 'etag_newer', 0,
                         0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[5])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # and content-type but newer metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/ignored', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # but newer content-type and metadata
        ts_update = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/newer', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/newer', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)","encode_timestamps(timestamps[1], timestamps[3], timestamps[6])",*timestamps[1:5:2],0
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/container/test_sharder.py,https://github.com/openstack/swift/tree/master/test/unit/container/test_sharder.py,TestSharder,test_misplaced_objects_deleted_and_updated$4320,"def test_misplaced_objects_deleted_and_updated(self):
        # setup
        broker = self._make_broker()
        broker.enable_sharding(next(self.ts_iter))

        shard_bounds = (('', 'here'), ('here', ''))
        root_shard_ranges = self._make_shard_ranges(
            shard_bounds, state=ShardRange.ACTIVE)
        expected_shard_dbs = []
        for sr in root_shard_ranges:
            db_hash = hash_path(sr.account, sr.container)
            expected_shard_dbs.append(
                os.path.join(self.tempdir, 'sda', 'containers', '0',
                             db_hash[-3:], db_hash, db_hash + '.db'))
        broker.merge_shard_ranges(root_shard_ranges)
        self.assertTrue(broker.set_sharding_state())

        ts_older_internal = self.ts_encoded()  # used later
        # put deleted objects into source
        objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(objects, broker.db_file)  # sanity check
        # pretend we cleaved all ranges - sharded state
        self.assertTrue(broker.set_sharded_state())

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        expected_stats = {'attempted': 1, 'success': 1, 'failure': 0,
                          'found': 1, 'placed': 2, 'unplaced': 0}
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check new misplaced objects were moved
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source db with older undeleted versions of same objects
        old_objects = [
            ['b', ts_older_internal, 2, 'text/plain', 'etag_b', 0, 0],
            ['x', ts_older_internal, 4, 'text/plain', 'etag_x', 0, 0]
        ]
        for obj in old_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(old_objects, broker.db_file)  # sanity check
        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check older misplaced objects were not merged to shard brokers
        self._check_objects(objects[:1], expected_shard_dbs[0])
        self._check_objects(objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # the destination shard dbs for misplaced objects may already exist so
        # check they are updated correctly when overwriting objects
        # update source db with newer deleted versions of same objects
        new_objects = [
            ['b', self.ts_encoded(), 0, '', '', 1, 0],
            ['x', self.ts_encoded(), 0, '', '', 1, 0]
        ]
        for obj in new_objects:
            broker.put_object(*obj)
        broker.get_info()
        self._check_objects(new_objects, broker.db_file)  # sanity check
        shard_broker = ContainerBroker(
            expected_shard_dbs[0], account=root_shard_ranges[0].account,
            container=root_shard_ranges[0].container)
        # update one shard container with even newer version of object
        timestamps = [next(self.ts_iter) for i in range(7)]
        ts_newer = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[5])
        newer_object = ('b', ts_newer, 10, 'text/plain', 'etag_b', 0, 0)
        shard_broker.put_object(*newer_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        sharder._replicate_object.assert_has_calls(
            [mock.call(0, db, 0) for db in (expected_shard_dbs[0],
                                            expected_shard_dbs[1])],
            any_order=True
        )
        self._assert_stats(expected_stats, sharder, 'misplaced')
        self.assertEqual(
            1, sharder.logger.get_stats_counts()['misplaced_found'])

        # check only the newer misplaced object was moved
        self._check_objects([newer_object], expected_shard_dbs[0])
        self._check_objects(new_objects[1:], expected_shard_dbs[1])
        # ... and removed from the source db
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has newer data
        # but older content-type and metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[4])
        update_object = ('b', ts_update, 20, 'text/ignored', 'etag_newer', 0,
                         0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[5])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # and content-type but newer metadata relative to shard object
        ts_update = encode_timestamps(
            timestamps[1], timestamps[3], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/ignored', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[3], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/plain', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)

        # update source with a version of 'b' that has older data
        # but newer content-type and metadata
        ts_update = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        update_object = ('b', ts_update, 999, 'text/newer', 'etag_b', 0, 0)
        broker.put_object(*update_object)

        with self._mock_sharder() as sharder:
            sharder._move_misplaced_objects(broker)

        ts_expected = encode_timestamps(
            timestamps[2], timestamps[6], timestamps[6])
        expected = ('b', ts_expected, 20, 'text/newer', 'etag_newer', 0, 0)
        self._check_objects([expected], expected_shard_dbs[0])
        self._check_objects([], broker.db_file)","encode_timestamps(timestamps[2], timestamps[3], timestamps[6])",*timestamps[2:4],0
modin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/modin/modin/core/execution/ray/implementations/cudf_on_ray/dataframe/dataframe.py,https://github.com/modin-project/modin/tree/master/modin/core/execution/ray/implementations/cudf_on_ray/dataframe/dataframe.py,cuDFOnRayDataframe,synchronize_labels$52,"def synchronize_labels(self, axis=None):
        """"""
        Synchronize labels by applying the index object (Index or Columns) to the partitions eagerly.

        Parameters
        ----------
        axis : {0, 1, None}, default: None
            The axis to apply to. If None, it applies to both axes.
        """"""
        ErrorMessage.catch_bugs_and_request_email(
            axis is not None and axis not in [0, 1]
        )

        cum_row_lengths = np.cumsum([0] + self._row_lengths)
        cum_col_widths = np.cumsum([0] + self._column_widths)

        def apply_idx_objs(df, idx, cols, axis):
            # cudf does not support set_axis. It only supports rename with 1-to-1 mapping.
            # Therefore, we need to create the dictionary that have the relationship between
            # current index and new ones.
            idx = {df.index[i]: idx[i] for i in range(len(idx))}
            cols = {df.index[i]: cols[i] for i in range(len(cols))}

            if axis == 0:
                return df.rename(index=idx)
            elif axis == 1:
                return df.rename(columns=cols)
            else:
                return df.rename(index=idx, columns=cols)

        keys = np.array(
            [
                [
                    self._partitions[i][j].apply(
                        apply_idx_objs,
                        idx=self.index[
                            slice(cum_row_lengths[i], cum_row_lengths[i + 1])
                        ],
                        cols=self.columns[
                            slice(cum_col_widths[j], cum_col_widths[j + 1])
                        ],
                        axis=axis,
                    )
                    for j in range(len(self._partitions[i]))
                ]
                for i in range(len(self._partitions))
            ]
        )

        self._partitions = np.array(
            [
                [
                    cuDFOnRayDataframePartition(
                        self._partitions[i][j].get_gpu_manager(),
                        keys[i][j],
                        self._partitions[i][j]._length_cache,
                        self._partitions[i][j]._width_cache,
                    )
                    for j in range(len(keys[i]))
                ]
                for i in range(len(keys))
            ]
        )","slice(cum_row_lengths[i], cum_row_lengths[i + 1])",*cum_row_lengths[i:i + 2],0
modin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/modin/modin/core/execution/ray/implementations/cudf_on_ray/dataframe/dataframe.py,https://github.com/modin-project/modin/tree/master/modin/core/execution/ray/implementations/cudf_on_ray/dataframe/dataframe.py,cuDFOnRayDataframe,synchronize_labels$52,"def synchronize_labels(self, axis=None):
        """"""
        Synchronize labels by applying the index object (Index or Columns) to the partitions eagerly.

        Parameters
        ----------
        axis : {0, 1, None}, default: None
            The axis to apply to. If None, it applies to both axes.
        """"""
        ErrorMessage.catch_bugs_and_request_email(
            axis is not None and axis not in [0, 1]
        )

        cum_row_lengths = np.cumsum([0] + self._row_lengths)
        cum_col_widths = np.cumsum([0] + self._column_widths)

        def apply_idx_objs(df, idx, cols, axis):
            # cudf does not support set_axis. It only supports rename with 1-to-1 mapping.
            # Therefore, we need to create the dictionary that have the relationship between
            # current index and new ones.
            idx = {df.index[i]: idx[i] for i in range(len(idx))}
            cols = {df.index[i]: cols[i] for i in range(len(cols))}

            if axis == 0:
                return df.rename(index=idx)
            elif axis == 1:
                return df.rename(columns=cols)
            else:
                return df.rename(index=idx, columns=cols)

        keys = np.array(
            [
                [
                    self._partitions[i][j].apply(
                        apply_idx_objs,
                        idx=self.index[
                            slice(cum_row_lengths[i], cum_row_lengths[i + 1])
                        ],
                        cols=self.columns[
                            slice(cum_col_widths[j], cum_col_widths[j + 1])
                        ],
                        axis=axis,
                    )
                    for j in range(len(self._partitions[i]))
                ]
                for i in range(len(self._partitions))
            ]
        )

        self._partitions = np.array(
            [
                [
                    cuDFOnRayDataframePartition(
                        self._partitions[i][j].get_gpu_manager(),
                        keys[i][j],
                        self._partitions[i][j]._length_cache,
                        self._partitions[i][j]._width_cache,
                    )
                    for j in range(len(keys[i]))
                ]
                for i in range(len(keys))
            ]
        )","slice(cum_col_widths[j], cum_col_widths[j + 1])",*cum_col_widths[j:j + 2],0
numpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/core/tests/test_function_base.py,https://github.com/numpy/numpy/tree/master/numpy/core/tests/test_function_base.py,TestGeomspace,test_start_stop_array_scalar$204,"def test_start_stop_array_scalar(self):
        lim1 = array([120, 100], dtype=""int8"")
        lim2 = array([-120, -100], dtype=""int8"")
        lim3 = array([1200, 1000], dtype=""uint16"")
        t1 = geomspace(lim1[0], lim1[1], 5)
        t2 = geomspace(lim2[0], lim2[1], 5)
        t3 = geomspace(lim3[0], lim3[1], 5)
        t4 = geomspace(120.0, 100.0, 5)
        t5 = geomspace(-120.0, -100.0, 5)
        t6 = geomspace(1200.0, 1000.0, 5)

        # t3 uses float32, t6 uses float64
        assert_allclose(t1, t4, rtol=1e-2)
        assert_allclose(t2, t5, rtol=1e-2)
        assert_allclose(t3, t6, rtol=1e-5)","geomspace(lim1[0], lim1[1], 5)",*lim1[:2],0
numpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/core/tests/test_function_base.py,https://github.com/numpy/numpy/tree/master/numpy/core/tests/test_function_base.py,TestGeomspace,test_start_stop_array_scalar$204,"def test_start_stop_array_scalar(self):
        lim1 = array([120, 100], dtype=""int8"")
        lim2 = array([-120, -100], dtype=""int8"")
        lim3 = array([1200, 1000], dtype=""uint16"")
        t1 = geomspace(lim1[0], lim1[1], 5)
        t2 = geomspace(lim2[0], lim2[1], 5)
        t3 = geomspace(lim3[0], lim3[1], 5)
        t4 = geomspace(120.0, 100.0, 5)
        t5 = geomspace(-120.0, -100.0, 5)
        t6 = geomspace(1200.0, 1000.0, 5)

        # t3 uses float32, t6 uses float64
        assert_allclose(t1, t4, rtol=1e-2)
        assert_allclose(t2, t5, rtol=1e-2)
        assert_allclose(t3, t6, rtol=1e-5)","geomspace(lim2[0], lim2[1], 5)",*lim2[:2],0
numpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/core/tests/test_function_base.py,https://github.com/numpy/numpy/tree/master/numpy/core/tests/test_function_base.py,TestGeomspace,test_start_stop_array_scalar$204,"def test_start_stop_array_scalar(self):
        lim1 = array([120, 100], dtype=""int8"")
        lim2 = array([-120, -100], dtype=""int8"")
        lim3 = array([1200, 1000], dtype=""uint16"")
        t1 = geomspace(lim1[0], lim1[1], 5)
        t2 = geomspace(lim2[0], lim2[1], 5)
        t3 = geomspace(lim3[0], lim3[1], 5)
        t4 = geomspace(120.0, 100.0, 5)
        t5 = geomspace(-120.0, -100.0, 5)
        t6 = geomspace(1200.0, 1000.0, 5)

        # t3 uses float32, t6 uses float64
        assert_allclose(t1, t4, rtol=1e-2)
        assert_allclose(t2, t5, rtol=1e-2)
        assert_allclose(t3, t6, rtol=1e-5)","geomspace(lim3[0], lim3[1], 5)",*lim3[:2],0
mmdetection3d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection3d/mmdet3d/ops/paconv/paconv.py,https://github.com/open-mmlab/mmdetection3d/tree/master/mmdet3d/ops/paconv/paconv.py,ScoreNet,__init__$40,"def __init__(self,
                 mlp_channels,
                 last_bn=False,
                 score_norm='softmax',
                 temp_factor=1.0,
                 norm_cfg=dict(type='BN2d'),
                 bias='auto'):
        super(ScoreNet, self).__init__()

        assert score_norm in ['softmax', 'sigmoid', 'identity'], \
            f'unsupported score_norm function {score_norm}'

        self.score_norm = score_norm
        self.temp_factor = temp_factor

        self.mlps = nn.Sequential()
        for i in range(len(mlp_channels) - 2):
            self.mlps.add_module(
                f'layer{i}',
                ConvModule(
                    mlp_channels[i],
                    mlp_channels[i + 1],
                    kernel_size=(1, 1),
                    stride=(1, 1),
                    conv_cfg=dict(type='Conv2d'),
                    norm_cfg=norm_cfg,
                    bias=bias))

        # for the last mlp that outputs scores, no relu and possibly no bn
        i = len(mlp_channels) - 2
        self.mlps.add_module(
            f'layer{i}',
            ConvModule(
                mlp_channels[i],
                mlp_channels[i + 1],
                kernel_size=(1, 1),
                stride=(1, 1),
                conv_cfg=dict(type='Conv2d'),
                norm_cfg=norm_cfg if last_bn else None,
                act_cfg=None,
                bias=bias))","ConvModule(mlp_channels[i], mlp_channels[i + 1], kernel_size=(1, 1), stride=(1, 1), conv_cfg=dict(type='Conv2d'), norm_cfg=norm_cfg if last_bn else None, act_cfg=None, bias=bias)",*mlp_channels[i:i + 2],0
mmdetection3d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection3d/mmdet3d/ops/paconv/paconv.py,https://github.com/open-mmlab/mmdetection3d/tree/master/mmdet3d/ops/paconv/paconv.py,ScoreNet,__init__$40,"def __init__(self,
                 mlp_channels,
                 last_bn=False,
                 score_norm='softmax',
                 temp_factor=1.0,
                 norm_cfg=dict(type='BN2d'),
                 bias='auto'):
        super(ScoreNet, self).__init__()

        assert score_norm in ['softmax', 'sigmoid', 'identity'], \
            f'unsupported score_norm function {score_norm}'

        self.score_norm = score_norm
        self.temp_factor = temp_factor

        self.mlps = nn.Sequential()
        for i in range(len(mlp_channels) - 2):
            self.mlps.add_module(
                f'layer{i}',
                ConvModule(
                    mlp_channels[i],
                    mlp_channels[i + 1],
                    kernel_size=(1, 1),
                    stride=(1, 1),
                    conv_cfg=dict(type='Conv2d'),
                    norm_cfg=norm_cfg,
                    bias=bias))

        # for the last mlp that outputs scores, no relu and possibly no bn
        i = len(mlp_channels) - 2
        self.mlps.add_module(
            f'layer{i}',
            ConvModule(
                mlp_channels[i],
                mlp_channels[i + 1],
                kernel_size=(1, 1),
                stride=(1, 1),
                conv_cfg=dict(type='Conv2d'),
                norm_cfg=norm_cfg if last_bn else None,
                act_cfg=None,
                bias=bias))","ConvModule(mlp_channels[i], mlp_channels[i + 1], kernel_size=(1, 1), stride=(1, 1), conv_cfg=dict(type='Conv2d'), norm_cfg=norm_cfg, bias=bias)",*mlp_channels[i:i + 2],0
