repo_name,file_path,file_html,class_name,me_name,me_code,old_code,chatGPT_code,element_str,slice_str,truth_code,,
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/modeling/backbones/mobilenet.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/modeling/backbones/mobilenet.py,MobileNet,__call__$159,"def __call__(self, input):
        scale = self.conv_group_scale

        blocks = []
        # input 1/1
        out = self._conv_norm(
            input, 3, int(32 * scale), 2, 1, name=self.prefix_name + ""conv1"")
        # 1/2
        out = self.depthwise_separable(
            out, 32, 64, 32, 1, scale, name=self.prefix_name + ""conv2_1"")
        out = self.depthwise_separable(
            out, 64, 128, 64, 2, scale, name=self.prefix_name + ""conv2_2"")
        # 1/4
        out = self.depthwise_separable(
            out, 128, 128, 128, 1, scale, name=self.prefix_name + ""conv3_1"")
        out = self.depthwise_separable(
            out, 128, 256, 128, 2, scale, name=self.prefix_name + ""conv3_2"")
        # 1/8
        blocks.append(out)
        out = self.depthwise_separable(
            out, 256, 256, 256, 1, scale, name=self.prefix_name + ""conv4_1"")
        out = self.depthwise_separable(
            out, 256, 512, 256, 2, scale, name=self.prefix_name + ""conv4_2"")
        # 1/16
        blocks.append(out)
        for i in range(5):
            out = self.depthwise_separable(
                out,
                512,
                512,
                512,
                1,
                scale,
                name=self.prefix_name + ""conv5_"" + str(i + 1))
        module11 = out

        out = self.depthwise_separable(
            out, 512, 1024, 512, 2, scale, name=self.prefix_name + ""conv5_6"")
        # 1/32
        out = self.depthwise_separable(
            out, 1024, 1024, 1024, 1, scale, name=self.prefix_name + ""conv6"")
        module13 = out
        blocks.append(out)
        if not self.with_extra_blocks:
            return blocks

        num_filters = self.extra_block_filters
        module14 = self._extra_block(module13, num_filters[0][0],
                                     num_filters[0][1], 1, 2,
                                     self.prefix_name + ""conv7_1"")
        module15 = self._extra_block(module14, num_filters[1][0],
                                     num_filters[1][1], 1, 2,
                                     self.prefix_name + ""conv7_2"")
        module16 = self._extra_block(module15, num_filters[2][0],
                                     num_filters[2][1], 1, 2,
                                     self.prefix_name + ""conv7_3"")
        module17 = self._extra_block(module16, num_filters[3][0],
                                     num_filters[3][1], 1, 2,
                                     self.prefix_name + ""conv7_4"")
        return module11, module13, module14, module15, module16, module17","self._extra_block(module13, num_filters[0][0], num_filters[0][1], 1, 2, self.prefix_name + 'conv7_1')","self._extra_block(module13, *num_filters[0][:2], 1, 2, self.prefix_name + 'conv7_1')","iterable_zj[0], iterable_zj[1]",*num_filters[0][:2],*num_filters[0][:2],1,
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/modeling/backbones/mobilenet.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/modeling/backbones/mobilenet.py,MobileNet,__call__$159,"def __call__(self, input):
        scale = self.conv_group_scale

        blocks = []
        # input 1/1
        out = self._conv_norm(
            input, 3, int(32 * scale), 2, 1, name=self.prefix_name + ""conv1"")
        # 1/2
        out = self.depthwise_separable(
            out, 32, 64, 32, 1, scale, name=self.prefix_name + ""conv2_1"")
        out = self.depthwise_separable(
            out, 64, 128, 64, 2, scale, name=self.prefix_name + ""conv2_2"")
        # 1/4
        out = self.depthwise_separable(
            out, 128, 128, 128, 1, scale, name=self.prefix_name + ""conv3_1"")
        out = self.depthwise_separable(
            out, 128, 256, 128, 2, scale, name=self.prefix_name + ""conv3_2"")
        # 1/8
        blocks.append(out)
        out = self.depthwise_separable(
            out, 256, 256, 256, 1, scale, name=self.prefix_name + ""conv4_1"")
        out = self.depthwise_separable(
            out, 256, 512, 256, 2, scale, name=self.prefix_name + ""conv4_2"")
        # 1/16
        blocks.append(out)
        for i in range(5):
            out = self.depthwise_separable(
                out,
                512,
                512,
                512,
                1,
                scale,
                name=self.prefix_name + ""conv5_"" + str(i + 1))
        module11 = out

        out = self.depthwise_separable(
            out, 512, 1024, 512, 2, scale, name=self.prefix_name + ""conv5_6"")
        # 1/32
        out = self.depthwise_separable(
            out, 1024, 1024, 1024, 1, scale, name=self.prefix_name + ""conv6"")
        module13 = out
        blocks.append(out)
        if not self.with_extra_blocks:
            return blocks

        num_filters = self.extra_block_filters
        module14 = self._extra_block(module13, num_filters[0][0],
                                     num_filters[0][1], 1, 2,
                                     self.prefix_name + ""conv7_1"")
        module15 = self._extra_block(module14, num_filters[1][0],
                                     num_filters[1][1], 1, 2,
                                     self.prefix_name + ""conv7_2"")
        module16 = self._extra_block(module15, num_filters[2][0],
                                     num_filters[2][1], 1, 2,
                                     self.prefix_name + ""conv7_3"")
        module17 = self._extra_block(module16, num_filters[3][0],
                                     num_filters[3][1], 1, 2,
                                     self.prefix_name + ""conv7_4"")
        return module11, module13, module14, module15, module16, module17","self._extra_block(module14, num_filters[1][0], num_filters[1][1], 1, 2, self.prefix_name + 'conv7_2')","self._extra_block(module14, *num_filters[1][:2], 1, 2, self.prefix_name + 'conv7_2')","iterable_zj[0], iterable_zj[1]",*num_filters[1][:2],*num_filters[1][:2],1,
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/modeling/backbones/mobilenet.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/modeling/backbones/mobilenet.py,MobileNet,__call__$159,"def __call__(self, input):
        scale = self.conv_group_scale

        blocks = []
        # input 1/1
        out = self._conv_norm(
            input, 3, int(32 * scale), 2, 1, name=self.prefix_name + ""conv1"")
        # 1/2
        out = self.depthwise_separable(
            out, 32, 64, 32, 1, scale, name=self.prefix_name + ""conv2_1"")
        out = self.depthwise_separable(
            out, 64, 128, 64, 2, scale, name=self.prefix_name + ""conv2_2"")
        # 1/4
        out = self.depthwise_separable(
            out, 128, 128, 128, 1, scale, name=self.prefix_name + ""conv3_1"")
        out = self.depthwise_separable(
            out, 128, 256, 128, 2, scale, name=self.prefix_name + ""conv3_2"")
        # 1/8
        blocks.append(out)
        out = self.depthwise_separable(
            out, 256, 256, 256, 1, scale, name=self.prefix_name + ""conv4_1"")
        out = self.depthwise_separable(
            out, 256, 512, 256, 2, scale, name=self.prefix_name + ""conv4_2"")
        # 1/16
        blocks.append(out)
        for i in range(5):
            out = self.depthwise_separable(
                out,
                512,
                512,
                512,
                1,
                scale,
                name=self.prefix_name + ""conv5_"" + str(i + 1))
        module11 = out

        out = self.depthwise_separable(
            out, 512, 1024, 512, 2, scale, name=self.prefix_name + ""conv5_6"")
        # 1/32
        out = self.depthwise_separable(
            out, 1024, 1024, 1024, 1, scale, name=self.prefix_name + ""conv6"")
        module13 = out
        blocks.append(out)
        if not self.with_extra_blocks:
            return blocks

        num_filters = self.extra_block_filters
        module14 = self._extra_block(module13, num_filters[0][0],
                                     num_filters[0][1], 1, 2,
                                     self.prefix_name + ""conv7_1"")
        module15 = self._extra_block(module14, num_filters[1][0],
                                     num_filters[1][1], 1, 2,
                                     self.prefix_name + ""conv7_2"")
        module16 = self._extra_block(module15, num_filters[2][0],
                                     num_filters[2][1], 1, 2,
                                     self.prefix_name + ""conv7_3"")
        module17 = self._extra_block(module16, num_filters[3][0],
                                     num_filters[3][1], 1, 2,
                                     self.prefix_name + ""conv7_4"")
        return module11, module13, module14, module15, module16, module17","self._extra_block(module15, num_filters[2][0], num_filters[2][1], 1, 2, self.prefix_name + 'conv7_3')","self._extra_block(module15, *num_filters[2][:2], 1, 2, self.prefix_name + 'conv7_3')","iterable_zj[0], iterable_zj[1]",*num_filters[2][:2],*num_filters[2][:2],1,
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/modeling/backbones/mobilenet.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/modeling/backbones/mobilenet.py,MobileNet,__call__$159,"def __call__(self, input):
        scale = self.conv_group_scale

        blocks = []
        # input 1/1
        out = self._conv_norm(
            input, 3, int(32 * scale), 2, 1, name=self.prefix_name + ""conv1"")
        # 1/2
        out = self.depthwise_separable(
            out, 32, 64, 32, 1, scale, name=self.prefix_name + ""conv2_1"")
        out = self.depthwise_separable(
            out, 64, 128, 64, 2, scale, name=self.prefix_name + ""conv2_2"")
        # 1/4
        out = self.depthwise_separable(
            out, 128, 128, 128, 1, scale, name=self.prefix_name + ""conv3_1"")
        out = self.depthwise_separable(
            out, 128, 256, 128, 2, scale, name=self.prefix_name + ""conv3_2"")
        # 1/8
        blocks.append(out)
        out = self.depthwise_separable(
            out, 256, 256, 256, 1, scale, name=self.prefix_name + ""conv4_1"")
        out = self.depthwise_separable(
            out, 256, 512, 256, 2, scale, name=self.prefix_name + ""conv4_2"")
        # 1/16
        blocks.append(out)
        for i in range(5):
            out = self.depthwise_separable(
                out,
                512,
                512,
                512,
                1,
                scale,
                name=self.prefix_name + ""conv5_"" + str(i + 1))
        module11 = out

        out = self.depthwise_separable(
            out, 512, 1024, 512, 2, scale, name=self.prefix_name + ""conv5_6"")
        # 1/32
        out = self.depthwise_separable(
            out, 1024, 1024, 1024, 1, scale, name=self.prefix_name + ""conv6"")
        module13 = out
        blocks.append(out)
        if not self.with_extra_blocks:
            return blocks

        num_filters = self.extra_block_filters
        module14 = self._extra_block(module13, num_filters[0][0],
                                     num_filters[0][1], 1, 2,
                                     self.prefix_name + ""conv7_1"")
        module15 = self._extra_block(module14, num_filters[1][0],
                                     num_filters[1][1], 1, 2,
                                     self.prefix_name + ""conv7_2"")
        module16 = self._extra_block(module15, num_filters[2][0],
                                     num_filters[2][1], 1, 2,
                                     self.prefix_name + ""conv7_3"")
        module17 = self._extra_block(module16, num_filters[3][0],
                                     num_filters[3][1], 1, 2,
                                     self.prefix_name + ""conv7_4"")
        return module11, module13, module14, module15, module16, module17","self._extra_block(module16, num_filters[3][0], num_filters[3][1], 1, 2, self.prefix_name + 'conv7_4')","self._extra_block(module16, *num_filters[3][:2], 1, 2, self.prefix_name + 'conv7_4')","iterable_zj[0], iterable_zj[1]",*num_filters[3][:2],*num_filters[3][:2],1,
openpilot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openpilot/selfdrive/controls/radard.py,https://github.com/commaai/openpilot/tree/master/selfdrive/controls/radard.py,RadarD,update$107,"def update(self, sm, rr, enable_lead):
    self.current_time = 1e-9*max(sm.logMonoTime.values())

    if sm.updated['carState']:
      self.v_ego = sm['carState'].vEgo
      self.v_ego_hist.append(self.v_ego)
    if sm.updated['modelV2']:
      self.ready = True

    ar_pts = {}
    for pt in rr.points:
      ar_pts[pt.trackId] = [pt.dRel, pt.yRel, pt.vRel, pt.measured]

    # *** remove missing points from meta data ***
    for ids in list(self.tracks.keys()):
      if ids not in ar_pts:
        self.tracks.pop(ids, None)

    # *** compute the tracks ***
    for ids in ar_pts:
      rpt = ar_pts[ids]

      # align v_ego by a fixed time to align it with the radar measurement
      v_lead = rpt[2] + self.v_ego_hist[0]

      # create the track if it doesn't exist or it's a new track
      if ids not in self.tracks:
        self.tracks[ids] = Track(v_lead, self.kalman_params)
      self.tracks[ids].update(rpt[0], rpt[1], rpt[2], v_lead, rpt[3])

    idens = list(sorted(self.tracks.keys()))
    track_pts = list([self.tracks[iden].get_key_for_cluster() for iden in idens])

    # If we have multiple points, cluster them
    if len(track_pts) > 1:
      cluster_idxs = cluster_points_centroid(track_pts, 2.5)
      clusters = [None] * (max(cluster_idxs) + 1)

      for idx in range(len(track_pts)):
        cluster_i = cluster_idxs[idx]
        if clusters[cluster_i] is None:
          clusters[cluster_i] = Cluster()
        clusters[cluster_i].add(self.tracks[idens[idx]])
    elif len(track_pts) == 1:
      # FIXME: cluster_point_centroid hangs forever if len(track_pts) == 1
      cluster_idxs = [0]
      clusters = [Cluster()]
      clusters[0].add(self.tracks[idens[0]])
    else:
      clusters = []

    # if a new point, reset accel to the rest of the cluster
    for idx in range(len(track_pts)):
      if self.tracks[idens[idx]].cnt <= 1:
        aLeadK = clusters[cluster_idxs[idx]].aLeadK
        aLeadTau = clusters[cluster_idxs[idx]].aLeadTau
        self.tracks[idens[idx]].reset_a_lead(aLeadK, aLeadTau)

    # *** publish radarState ***
    dat = messaging.new_message('radarState')
    dat.valid = sm.all_alive_and_valid() and len(rr.errors) == 0
    radarState = dat.radarState
    radarState.mdMonoTime = sm.logMonoTime['modelV2']
    radarState.canMonoTimes = list(rr.canMonoTimes)
    radarState.radarErrors = list(rr.errors)
    radarState.carStateMonoTime = sm.logMonoTime['carState']

    if enable_lead:
      if len(sm['modelV2'].leadsV3) > 1:
        radarState.leadOne = get_lead(self.v_ego, self.ready, clusters, sm['modelV2'].leadsV3[0], low_speed_override=True)
        radarState.leadTwo = get_lead(self.v_ego, self.ready, clusters, sm['modelV2'].leadsV3[1], low_speed_override=False)
    return dat","self.tracks[ids].update(rpt[0], rpt[1], rpt[2], v_lead, rpt[3])","self.tracks[ids].update(*rpt[:3], v_lead, rpt[3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*rpt[:3],*rpt[:3],1,
dragonfly,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dragonfly/dragonfly/opt/unittest_mf_cp_gp_bandit.py,https://github.com/dragonfly/dragonfly/tree/master/dragonfly/opt/unittest_mf_cp_gp_bandit.py,MFCPGPBanditTestCaseDefinitions,_run_optimiser$26,"def _run_optimiser(cls, prob_funcs, domain_config_file, worker_manager, max_capital,
                     mode, *args, **kwargs):
    """""" Run the optimiser. """"""
    return gp_bandit.mf_cp_gpb_from_raw_args(prob_funcs[0], prob_funcs[1],
             domain_config_file, worker_manager=worker_manager, max_capital=max_capital,
             is_mf=True, mode=mode, *args, **kwargs)","gp_bandit.mf_cp_gpb_from_raw_args(prob_funcs[0], prob_funcs[1], domain_config_file, *args, worker_manager=worker_manager, max_capital=max_capital, is_mf=True, mode=mode, **kwargs)","gp_bandit.mf_cp_gpb_from_raw_args(*prob_funcs[:2], domain_config_file, *args, worker_manager=worker_manager, max_capital=max_capital, is_mf=True, mode=mode, **kwargs)","iterable_zj[0], iterable_zj[1]",*prob_funcs[:2],*prob_funcs[:2],1,
opytimizer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opytimizer/tests/opytimizer/optimizers/swarm/test_kh.py,https://github.com/gugarosa/opytimizer/tree/master/tests/opytimizer/optimizers/swarm/test_kh.py,,test_kh_best_beta$289,"def test_kh_best_beta():
    search_space = search.SearchSpace(n_agents=5, n_variables=2,
                                      lower_bound=[0, 0], upper_bound=[10, 10])

    new_kh = kh.KH()
    new_kh.compile(search_space)

    beta = new_kh._best_beta(
        search_space.agents[0], search_space.agents[-1], search_space.agents[0])

    assert beta.shape == (2, 1)","new_kh._best_beta(search_space.agents[0], search_space.agents[-1], search_space.agents[0])",new_kh._best_beta(*search_space.agents[0::len(search_space.agents) - 1]),"iterable_zj[0], iterable_zj[-1], iterable_zj[0]",*search_space.agents[0::len(search_space.agents)-1],*search_space.agents[-1:1],0,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_lookahead_swap.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_lookahead_swap.py,TestLookaheadSwap,test_lookahead_swap_hang_in_min_case$225,"def test_lookahead_swap_hang_in_min_case(self):
        """"""Verify LookaheadSwap does not stall in minimal case.""""""
        # ref: https://github.com/Qiskit/qiskit-terra/issues/2171

        qr = QuantumRegister(14, ""q"")
        qc = QuantumCircuit(qr)
        qc.cx(qr[0], qr[13])
        qc.cx(qr[1], qr[13])
        qc.cx(qr[1], qr[0])
        qc.cx(qr[13], qr[1])
        dag = circuit_to_dag(qc)

        cmap = CouplingMap(FakeMelbourne().configuration().coupling_map)

        out = LookaheadSwap(cmap, search_depth=4, search_width=4).run(dag)

        self.assertIsInstance(out, DAGCircuit)","qc.cx(qr[0], qr[13])","qc.cx(*qr[0], qr[13])","iterable_zj[0], iterable_zj[13]","*qr[0], qr[13]",*qr[:26:13],0,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_lookahead_swap.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_lookahead_swap.py,TestLookaheadSwap,test_lookahead_swap_hang_in_min_case$225,"def test_lookahead_swap_hang_in_min_case(self):
        """"""Verify LookaheadSwap does not stall in minimal case.""""""
        # ref: https://github.com/Qiskit/qiskit-terra/issues/2171

        qr = QuantumRegister(14, ""q"")
        qc = QuantumCircuit(qr)
        qc.cx(qr[0], qr[13])
        qc.cx(qr[1], qr[13])
        qc.cx(qr[1], qr[0])
        qc.cx(qr[13], qr[1])
        dag = circuit_to_dag(qc)

        cmap = CouplingMap(FakeMelbourne().configuration().coupling_map)

        out = LookaheadSwap(cmap, search_depth=4, search_width=4).run(dag)

        self.assertIsInstance(out, DAGCircuit)","qc.cx(qr[1], qr[13])",qc.cx(*qr[1:14:12]),"iterable_zj[1], iterable_zj[13]",*qr[1:14:12],*qr[1:25:12],0,1
open_model_zoo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open_model_zoo/demos/multi_camera_multi_target_tracking_demo/python/multi_camera_multi_target_tracking_demo.py,https://github.com/openvinotoolkit/open_model_zoo/tree/master/demos/multi_camera_multi_target_tracking_demo/python/multi_camera_multi_target_tracking_demo.py,,check_detectors$46,"def check_detectors(args):
    detectors = {
        '--m_detector': args.m_detector,
        '--m_segmentation': args.m_segmentation,
        '--detections': args.detections
    }
    non_empty_detectors = [(det, value) for det, value in detectors.items() if value]
    det_number = len(non_empty_detectors)
    if det_number == 0:
        log.error('No detector specified, please specify one of the following parameters: '
                  '\'--m_detector\', \'--m_segmentation\' or \'--detections\'')
    elif det_number > 1:
        det_string = ''.join('\n\t{}={}'.format(det[0], det[1]) for det in non_empty_detectors)
        log.error('Only one detector expected but got {}, please specify one of them:{}'
                  .format(len(non_empty_detectors), det_string))
    return det_number","'\n\t{}={}'.format(det[0], det[1])",'\n\t{}={}'.format(*det[:2]),"iterable_zj[0], iterable_zj[1]",*det[:2],*det[:2],1,
kornia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kornia/kornia/geometry/boxes.py,https://github.com/kornia/kornia/tree/master/kornia/geometry/boxes.py,Boxes3D,to_tensor$499,"def to_tensor(self, mode: str = ""xyzxyz"") -> torch.Tensor:
        r""""""Cast :class:`Boxes3D` to a tensor. ``mode`` controls which 3D boxes format should be use to represent boxes
        in the tensor.

        Args:
            mode: The format in which the boxes are provided.

                * 'xyzxyz': boxes are assumed to be in the format ``xmin, ymin, zmin, xmax, ymax, zmax`` where
                  ``width = xmax - xmin``, ``height = ymax - ymin`` and ``depth = zmax - zmin``.
                * 'xyzxyz_plus': similar to 'xyzxyz' mode but where box width, length and depth are defined as
                   ``width = xmax - xmin + 1``, ``height = ymax - ymin + 1`` and ``depth = zmax - zmin + 1``.
                * 'xyzwhd': boxes are assumed to be in the format ``xmin, ymin, zmin, width, height, depth`` where
                  ``width = xmax - xmin``, ``height = ymax - ymin`` and ``depth = zmax - zmin``.
                * 'vertices': boxes are defined by their vertices points in the following ``clockwise`` order:
                  *front-top-left, front-top-right, front-bottom-right, front-bottom-left, back-top-left,
                  back-top-right, back-bottom-right,  back-bottom-left*. Vertices coordinates are in (x,y, z) order.
                  Finally, box width, height and depth are defined as ``width = xmax - xmin``, ``height = ymax - ymin``
                  and ``depth = zmax - zmin``.
                * 'vertices_plus': similar to 'vertices' mode but where box width, length and depth are defined as
                  ``width = xmax - xmin + 1`` and ``height = ymax - ymin + 1``.

        Returns:
            3D Boxes tensor in the ``mode`` format. The shape depends with the ``mode`` value:

                * 'vertices' or 'verticies_plus': :math:`(N, 8, 3)` or :math:`(B, N, 8, 3)`.
                * Any other value: :math:`(N, 6)` or :math:`(B, N, 6)`.

        Note:
            It is currently non-differentiable due to a bug. See github issue
            `#1304 <https://github.com/kornia/kornia/issues/1396>`_.

        Examples:
            >>> boxes_xyzxyz = torch.as_tensor([[0, 3, 6, 1, 4, 8], [5, 1, 3, 8, 4, 9]])
            >>> boxes = Boxes3D.from_tensor(boxes_xyzxyz, mode='xyzxyz')
            >>> assert (boxes.to_tensor(mode='xyzxyz') == boxes_xyzxyz).all()
        """"""
        if self._boxes.requires_grad:
            raise RuntimeError(""Boxes3D.to_tensor doesn't support computing gradients since they aren't accurate. ""
                               ""Please, create boxes from tensors with `requires_grad=False`. ""
                               ""This is a known bug. Help is needed to fix it. For more information, ""
                               ""see https://github.com/kornia/kornia/issues/1396."")

        batched_boxes = self._boxes if self._is_batch else self._boxes.unsqueeze(0)

        # Create boxes in xyzxyz_plus format.
        boxes = torch.stack([batched_boxes.amin(dim=-2), batched_boxes.amax(dim=-2)], dim=-2).view(
            batched_boxes.shape[0], batched_boxes.shape[1], 6
        )

        mode = mode.lower()
        if mode in (""xyzxyz"", ""xyzxyz_plus""):
            pass
        elif mode in (""xyzwhd"", ""vertices"", ""vertices_plus""):
            width = boxes[..., 3] - boxes[..., 0] + 1
            height = boxes[..., 4] - boxes[..., 1] + 1
            depth = boxes[..., 5] - boxes[..., 2] + 1
            boxes[..., 3] = width
            boxes[..., 4] = height
            boxes[..., 5] = depth
        else:
            raise ValueError(f""Unknown mode {mode}"")

        if mode in (""xyzxyz"", ""vertices""):
            offset = torch.as_tensor([0, 0, 0, 1, 1, 1], device=boxes.device, dtype=boxes.dtype)
            boxes = boxes + offset

        if mode.startswith('vertices'):
            xmin, ymin, zmin = boxes[..., 0], boxes[..., 1], boxes[..., 2]
            width, height, depth = boxes[..., 3], boxes[..., 4], boxes[..., 5]

            boxes = _boxes3d_to_polygons3d(xmin, ymin, zmin, width, height, depth)

        boxes = boxes if self._is_batch else boxes.squeeze(0)
        return boxes","torch.stack([batched_boxes.amin(dim=-2), batched_boxes.amax(dim=-2)], dim=-2).view(batched_boxes.shape[0], batched_boxes.shape[1], 6)","torch.stack([batched_boxes.amin(dim=-2), batched_boxes.amax(dim=-2)], dim=-2).view(*batched_boxes.shape[:2], 6)","iterable_zj[0], iterable_zj[1]",*batched_boxes.shape[:2],*batched_boxes.shape[:2],1,
byob,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/byob/byob/core/util.py,https://github.com/malwaredllc/byob/tree/master/byob/core/util.py,,ftp$512,"def ftp(source, host=None, user=None, password=None, filetype=None):
    """"""
    Upload file/data to FTP server

    `Required`
    :param str source:    data or readable file-like object
    :param str host:      FTP server hostname
    :param str user:      FTP account username
    :param str password:  FTP account password

    `Optional`
    :param str filetype:  target file type (default: .txt)

    """"""
    import os
    import time
    import ftplib

    try:
        from StringIO import StringIO  # Python 2
    except ImportError:
        from io import StringIO        # Python 3

    if host and user and password:
        path  = ''
        local = time.ctime().split()
        if os.path.isfile(str(source)):
            path   = source
            source = open(path, 'rb')
        elif hasattr(source, 'seek'):
            source.seek(0)
        else:
            source = StringIO(source)
        try:
            ftp = ftplib.FTP(host=host, user=user, password=password)
        except:
            return ""Upload failed - remote FTP server authorization error""
        addr = public_ip()
        if 'tmp' not in ftp.nlst():
            ftp.mkd('/tmp')
        if addr not in ftp.nlst('/tmp'):
            ftp.mkd('/tmp/{}'.format(addr))
        if path:
            path = '/tmp/{}/{}'.format(addr, os.path.basename(path))
        else:
            filetype = '.' + str(filetype) if not str(filetype).startswith('.') else str(filetype)
            path = '/tmp/{}/{}'.format(addr, '{}-{}_{}{}'.format(local[1], local[2], local[3], filetype))
        stor = ftp.storbinary('STOR ' + path, source)
        return path
    else:
        log('missing one or more required arguments: host, user, password')","'{}-{}_{}{}'.format(local[1], local[2], local[3], filetype)","'{}-{}_{}{}'.format(*local[1:4], filetype)","iterable_zj[1], iterable_zj[2], iterable_zj[3]",*local[1:4],*local[1:4],1,
nimfa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nimfa/nimfa/methods/factorization/psmf.py,https://github.com/mims-harvard/nimfa/tree/master/nimfa/methods/factorization/psmf.py,Psmf,_update_sigma$344,"def _update_sigma(self):
        """"""Compute E-step and update sigma.""""""
        self.cross_terms = np.zeros((self.V.shape[0], self.rank, self.N))
        for cc in range(self.rank):
            t_c1 = np.tile(self.sigma[:, cc, :].reshape((self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1))
            t_c2 = np.tile(np.dot(self.zeta[cc, :], self.zeta.T), (self.V.shape[0], 1))
            t_c3 = np.tile((self.lamb * np.tile(self.lamb[:, cc].reshape((self.lamb.shape[0], 1)), (1, self.rank)) * t_c2).reshape(
                t_c2.shape[0], t_c2.shape[1], 1), (1, 1, self.N))
            self.cross_terms += t_c1 * t_c3
        self.sigma = np.zeros(self.sigma.shape)
        for t in range(self.V.shape[1]):
            t_s1 = np.tile(self.__arr(self.V[:, t]), (1, self.rank)) - self.lamb * np.tile(
                self.zeta[:, t].T, (self.V.shape[0], 1))
            t_s2 = t_s1 ** 2 + self.lamb ** 2 * \
                np.tile(self.phi.T, (self.V.shape[0], 1))
            self.sigma -= 0.5 * \
                np.tile((t_s2 / np.tile(self.psi, (1, self.rank))).reshape(
                    t_s2.shape[0], t_s2.shape[1], 1), (1, 1, self.N))
        for n in range(self.N):
            for nn in range(self.N):
                if nn != n:
                    t_s1 = (1e-50 + self.rho[:, max(n, nn):self.N]).sum(
                        axis=1) / (1e-50 + self.rho[:, n:self.N]).sum(axis=1)
                    self.sigma[:, :, n] -= np.tile(t_s1.reshape(self.psi.shape) / self.psi, (1, self.rank)) * self.cross_terms[:,:, nn]        
        self.sigma = np.exp(self.sigma - np.tile(np.amax(self.sigma, 1).reshape(
            (self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1)))
        self.sigma /= np.tile(self.sigma.sum(axis=1).reshape(
            (self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1))
        self.cross_terms = self._cross_terms()
        self.s = np.argmax(self.sigma, axis=1)
        self.s = self.s.transpose([0, 1])","(self.lamb * np.tile(self.lamb[:, cc].reshape((self.lamb.shape[0], 1)), (1, self.rank)) * t_c2).reshape(t_c2.shape[0], t_c2.shape[1], 1)","(self.lamb * np.tile(self.lamb[:, cc].reshape((self.lamb.shape[0], 1)), (1, self.rank)) * t_c2).reshape(*t_c2.shape[:2], 1)","iterable_zj[0], iterable_zj[1]",*t_c2.shape[:2],*t_c2.shape[:2],1,
nimfa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nimfa/nimfa/methods/factorization/psmf.py,https://github.com/mims-harvard/nimfa/tree/master/nimfa/methods/factorization/psmf.py,Psmf,_update_sigma$344,"def _update_sigma(self):
        """"""Compute E-step and update sigma.""""""
        self.cross_terms = np.zeros((self.V.shape[0], self.rank, self.N))
        for cc in range(self.rank):
            t_c1 = np.tile(self.sigma[:, cc, :].reshape((self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1))
            t_c2 = np.tile(np.dot(self.zeta[cc, :], self.zeta.T), (self.V.shape[0], 1))
            t_c3 = np.tile((self.lamb * np.tile(self.lamb[:, cc].reshape((self.lamb.shape[0], 1)), (1, self.rank)) * t_c2).reshape(
                t_c2.shape[0], t_c2.shape[1], 1), (1, 1, self.N))
            self.cross_terms += t_c1 * t_c3
        self.sigma = np.zeros(self.sigma.shape)
        for t in range(self.V.shape[1]):
            t_s1 = np.tile(self.__arr(self.V[:, t]), (1, self.rank)) - self.lamb * np.tile(
                self.zeta[:, t].T, (self.V.shape[0], 1))
            t_s2 = t_s1 ** 2 + self.lamb ** 2 * \
                np.tile(self.phi.T, (self.V.shape[0], 1))
            self.sigma -= 0.5 * \
                np.tile((t_s2 / np.tile(self.psi, (1, self.rank))).reshape(
                    t_s2.shape[0], t_s2.shape[1], 1), (1, 1, self.N))
        for n in range(self.N):
            for nn in range(self.N):
                if nn != n:
                    t_s1 = (1e-50 + self.rho[:, max(n, nn):self.N]).sum(
                        axis=1) / (1e-50 + self.rho[:, n:self.N]).sum(axis=1)
                    self.sigma[:, :, n] -= np.tile(t_s1.reshape(self.psi.shape) / self.psi, (1, self.rank)) * self.cross_terms[:,:, nn]        
        self.sigma = np.exp(self.sigma - np.tile(np.amax(self.sigma, 1).reshape(
            (self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1)))
        self.sigma /= np.tile(self.sigma.sum(axis=1).reshape(
            (self.sigma.shape[0], 1, self.sigma.shape[2])), (1, self.rank, 1))
        self.cross_terms = self._cross_terms()
        self.s = np.argmax(self.sigma, axis=1)
        self.s = self.s.transpose([0, 1])","(t_s2 / np.tile(self.psi, (1, self.rank))).reshape(t_s2.shape[0], t_s2.shape[1], 1)","(t_s2 / np.tile(self.psi, (1, self.rank))).reshape(*t_s2.shape[:2], 1)","iterable_zj[0], iterable_zj[1]",*t_s2.shape[:2],*t_s2.shape[:2],1,
gaphor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gaphor/gaphor/diagram/presentation.py,https://github.com/gaphor/gaphor/tree/master/gaphor/diagram/presentation.py,LinePresentation,draw$253,"def draw(self, context):
        def draw_line_end(end_handle, second_handle, draw):
            pos, p1 = end_handle.pos, second_handle.pos
            angle = atan2(p1.y - pos.y, p1.x - pos.x)
            cr.save()
            try:
                cr.translate(*pos)
                cr.rotate(angle)
                draw(context)
            finally:
                cr.restore()

        style = merge_styles(context.style, self.style)
        context = replace(context, style=style)

        self.update_shape_bounds(context)
        cr = context.cairo

        handles = self._handles
        draw_line_end(handles[0], handles[1], self.draw_head)

        for h in self._handles[1:-1]:
            cr.line_to(*h.pos)

        draw_line_end(handles[-1], handles[-2], self.draw_tail)

        stroke(context)

        for shape, rect in (
            (self.shape_head, self._shape_head_rect),
            (self.shape_middle, self._shape_middle_rect),
            (self.shape_tail, self._shape_tail_rect),
        ):
            if shape:
                shape.draw(context, rect)","draw_line_end(handles[0], handles[1], self.draw_head)","draw_line_end(*handles[:2], self.draw_head)","iterable_zj[0], iterable_zj[1]",*handles[:2],*handles[:2],1,
pyglet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglet/tools/wraptypes/cparser.py,https://github.com/pyglet/pyglet/tree/master/tools/wraptypes/cparser.py,,p_shift_expression$521,"def p_shift_expression(p):
    '''shift_expression : additive_expression
                        | shift_expression LEFT_OP additive_expression
                        | shift_expression RIGHT_OP additive_expression
    '''
    if len(p) == 2:
        p[0] = p[1]
    else:
        p[0] = BinaryExpressionNode({
            '<<': operator.lshift,
            '>>': operator.rshift}[p[2]], p[2], p[1], p[3])","BinaryExpressionNode({'<<': operator.lshift, '>>': operator.rshift}[p[2]], p[2], p[1], p[3])",Cannot refactor,"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*p[:3],*p[1:5:2],0,
spotify-ripper,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spotify-ripper/spotify_ripper/ripper.py,https://github.com/hbashton/spotify-ripper/tree/master/spotify_ripper/ripper.py,Ripper,run$178,"def run(self):
        args = self.args

        # start event loop
        self.event_loop.start()

        # wait for main thread to login
        self.ripper_continue.wait()
        if self.abort.is_set():
            return

        # list of spotify URIs
        uris = args.uri

        def get_tracks_from_uri(uri):
            self.current_playlist = None
            self.current_album = None
            self.current_chart = None

            if isinstance(uri, list):
                return uri
            else:
                if (uri.startswith(""spotify:artist:"") and
                        (args.artist_album_type is not None or
                         args.artist_album_market is not None)):
                    album_uris = self.web.get_albums_with_filter(uri)
                    return itertools.chain(
                        *[self.load_link(album_uri) for
                          album_uri in album_uris])
                elif uri.startswith(""spotify:charts:""):
                    charts = self.web.get_charts(uri)
                    if charts is not None:
                        self.current_chart = charts
                        chart_uris = charts[""tracks""]
                        return itertools.chain(
                            *[self.load_link(chart_uri) for
                              chart_uri in chart_uris])
                    else:
                        return iter([])
                else:
                    return self.load_link(uri)

        # calculate total size and time
        all_tracks = []
        for uri in uris:
            tracks = list(get_tracks_from_uri(uri))

            # TODO: remove dependency on current_album, ...
            for idx, track in enumerate(tracks):

                # ignore local tracks
                if track.is_local:
                    continue

                audio_file = self.format_track_path(idx, track)
                all_tracks.append((track, audio_file))

        self.progress.calc_total(all_tracks)

        if self.progress.total_size > 0:
            print(
                ""Total Download Size: "" +
                format_size(self.progress.total_size))

        # create track iterator
        for uri in uris:
            if self.abort.is_set():
                break

            tracks = list(get_tracks_from_uri(uri))

            if args.playlist_sync and self.current_playlist:
                self.sync = Sync(args, self)
                self.sync.sync_playlist(self.current_playlist)

            # ripping loop
            for idx, track in enumerate(tracks):
                try:
                    self.check_stop_time()
                    self.skip.clear()

                    if self.abort.is_set():
                        break

                    print('Loading track...')
                    track.load()
                    if track.availability != 1 or track.is_local:
                        print(
                            Fore.RED + 'Track is not available, '
                                       'skipping...' + Fore.RESET)
                        self.post.log_failure(track)
                        continue

                    self.audio_file = self.format_track_path(idx, track)

                    if not args.overwrite and path_exists(self.audio_file):
                        if is_partial(self.audio_file, track):
                            print(""Overwriting partial file"")
                        else:
                            print(
                                Fore.YELLOW + ""Skipping "" +
                                track.link.uri + Fore.RESET)
                            print(Fore.CYAN + self.audio_file + Fore.RESET)
                            self.post.queue_remove_from_playlist(idx)
                            continue

                    self.session.player.load(track)
                    self.prepare_rip(idx, track)
                    self.session.player.play()

                    timeout_count = 0
                    while not self.end_of_track.is_set() or \
                            not self.rip_queue.empty():
                        try:
                            if self.abort.is_set() or self.skip.is_set():
                                break

                            rip_item = self.rip_queue.get(timeout=1)

                            if self.abort.is_set() or self.skip.is_set():
                                break

                            self.rip(self.session, rip_item[0],
                                     rip_item[1], rip_item[2])
                        except queue.Empty:
                            timeout_count += 1
                            if timeout_count > 60:
                                raise spotify.Error(""Timeout while ""
                                                    ""ripping track"")

                    if self.skip.is_set():
                        extra_line = """" if self.play_token_resume.is_set() \
                                        else ""\n""
                        print(extra_line + Fore.YELLOW +
                            ""User skipped track... "" + Fore.RESET)
                        self.session.player.play(False)
                        self.post.clean_up_partial()
                        self.post.log_failure(track)
                        self.end_of_track.clear()
                        self.progress.end_track(show_end=False)
                        self.ripping.clear()
                        continue

                    if self.abort.is_set():
                        self.session.player.play(False)
                        self.end_of_track.set()
                        self.post.clean_up_partial()
                        self.post.log_failure(track)
                        break

                    self.end_of_track.clear()

                    self.finish_rip(track)

                    # update id3v2 with metadata and embed front cover image
                    set_metadata_tags(args, self.audio_file, idx, track, self)

                    # make a note of the index and remove all the
                    # tracks from the playlist when everything is done
                    self.post.queue_remove_from_playlist(idx)

                except (spotify.Error, Exception) as e:
                    if isinstance(e, Exception):
                        print(Fore.RED + ""Spotify error detected"" + Fore.RESET)
                    print(str(e))
                    print(""Skipping to next track..."")
                    self.session.player.play(False)
                    self.post.clean_up_partial()
                    self.post.log_failure(track)
                    continue

            # create playlist m3u file if needed
            self.post.create_playlist_m3u(tracks)

            # create playlist wpl file if needed
            self.post.create_playlist_wpl(tracks)

            # actually removing the tracks from playlist
            self.post.remove_tracks_from_playlist()

            # remove libspotify's offline storage cache
            self.post.remove_offline_cache()

        # logout, we are done
        self.post.end_failure_log()
        self.post.print_summary()
        self.logout()
        self.stop_event_loop()
        self.finished.set()","self.rip(self.session, rip_item[0], rip_item[1], rip_item[2])","self.rip(self.session, *rip_item[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*rip_item[:3],*rip_item[:3],1,
PhiFlow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/torch/_torch_backend.py,https://github.com/tum-pbs/PhiFlow/tree/master/phi/torch/_torch_backend.py,TorchBackend,sparse_tensor$585,"def sparse_tensor(self, indices, values, shape):
        indices_ = self.to_int64(indices)
        values_ = self.to_float(values)
        if not self.is_available(values_):
            # the output of torch.sparse_coo_tensor is considered constant
            @torch.jit.script
            def sparse_coo_tensor(values, indices, cols: int, rows: int, dtype: torch.dtype) -> torch.sparse.Tensor:
                size = torch.Size([cols, rows])
                return torch.sparse_coo_tensor(indices, values, size=size, dtype=dtype)
            result = sparse_coo_tensor(values_, indices_, shape[0], shape[1], to_torch_dtype(self.float_type))
        else:
            result = torch.sparse_coo_tensor(indices_, values_, shape, dtype=to_torch_dtype(self.float_type))
        return result","sparse_coo_tensor(values_, indices_, shape[0], shape[1], to_torch_dtype(self.float_type))","sparse_coo_tensor(values_, indices_, *shape[:2], to_torch_dtype(self.float_type))","iterable_zj[0], iterable_zj[1]",*shape[:2],*shape[:2],1,
TerminalView,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TerminalView/sublime_terminal_buffer.py,https://github.com/Wramberg/TerminalView/tree/master//sublime_terminal_buffer.py,TerminalViewUpdate,_update_cursor$339,"def _update_cursor(self):
        cursor_pos = self._sub_buffer.terminal_emulator().cursor()
        last_cursor_pos = self.view.settings().get(""terminal_view_last_cursor_pos"")
        if last_cursor_pos and last_cursor_pos[0] == cursor_pos[0] and last_cursor_pos[1] == cursor_pos[1]:
            return

        tp = self.view.text_point(cursor_pos[0], cursor_pos[1])
        self.view.sel().clear()
        self.view.sel().add(sublime.Region(tp, tp))
        self.view.settings().set(""terminal_view_last_cursor_pos"", cursor_pos)","self.view.text_point(cursor_pos[0], cursor_pos[1])",self.view.text_point(*cursor_pos[:2]),"iterable_zj[0], iterable_zj[1]",*cursor_pos[:2],*cursor_pos[:2],1,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_bip_mapping.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_bip_mapping.py,TestBIPMapping,test_multi_cregs$180,"def test_multi_cregs(self):
        """"""Test for multiple ClassicalRegisters.""""""

        #                      鈹屸攢鈹鈹鈹 鈻 鈹屸攢鈹
        # qr_0: 鈹鈹鈻犫攢鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹 X 鈹溾攢鈻戔攢鈹M鈹溾攢鈹鈹鈹鈹鈹鈹鈹鈹
        #       鈹屸攢鈹粹攢鈹     鈹屸攢鈹鈹鈹愨敂鈹鈹鈹鈹 鈻 鈹斺暐鈹樷攲鈹鈹
        # qr_1: 鈹 X 鈹溾攢鈹鈻犫攢鈹鈹 H 鈹溾攢鈹鈻犫攢鈹鈹鈻戔攢鈹鈺鈹鈹M鈹溾攢鈹鈹鈹鈹鈹
        #       鈹斺攢鈹鈹鈹樷攲鈹鈹粹攢鈹愨敂鈹鈹鈹鈹      鈻  鈺 鈹斺暐鈹樷攲鈹鈹
        # qr_2: 鈹鈹鈻犫攢鈹鈹 X 鈹溾攢鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈻戔攢鈹鈺鈹鈹鈺鈹鈹M鈹溾攢鈹鈹
        #       鈹屸攢鈹粹攢鈹愨敂鈹鈹鈹鈹           鈻  鈺  鈺 鈹斺暐鈹樷攲鈹鈹
        # qr_3: 鈹 X 鈹溾攢鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈻戔攢鈹鈺鈹鈹鈺鈹鈹鈺鈹鈹M鈹
        #       鈹斺攢鈹鈹鈹                鈻  鈺  鈺  鈺 鈹斺暐鈹
        #  c: 2/鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺┾晲鈺愨暚鈺愨晲鈺┾晲鈺愨暚鈺
        #                               0  鈺  1  鈺
        #                                  鈺     鈺
        #  d: 2/鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨暕鈺愨晲鈺愨晲鈺愨暕鈺
        #                                  0     1
        qr = QuantumRegister(4, ""qr"")
        cr1 = ClassicalRegister(2, ""c"")
        cr2 = ClassicalRegister(2, ""d"")
        circuit = QuantumCircuit(qr, cr1, cr2)
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])
        circuit.cx(qr[1], qr[2])
        circuit.h(qr[1])
        circuit.cx(qr[1], qr[0])
        circuit.barrier(qr)
        circuit.measure(qr[0], cr1[0])
        circuit.measure(qr[1], cr2[0])
        circuit.measure(qr[2], cr1[1])
        circuit.measure(qr[3], cr2[1])

        coupling = CouplingMap([[0, 1], [0, 2], [2, 3]])  # linear [1, 0, 2, 3]
        property_set = {}
        actual = BIPMapping(coupling, objective=""depth"")(circuit, property_set)
        self.assertEqual(5, actual.depth())

        CheckMap(coupling)(actual, property_set)
        self.assertTrue(property_set[""is_swap_mapped""])","circuit.cx(qr[0], qr[1])",circuit.cx(*qr[:2]),"iterable_zj[0], iterable_zj[1]",*qr[:2],*qr[:2],1,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_bip_mapping.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_bip_mapping.py,TestBIPMapping,test_multi_cregs$180,"def test_multi_cregs(self):
        """"""Test for multiple ClassicalRegisters.""""""

        #                      鈹屸攢鈹鈹鈹 鈻 鈹屸攢鈹
        # qr_0: 鈹鈹鈻犫攢鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹 X 鈹溾攢鈻戔攢鈹M鈹溾攢鈹鈹鈹鈹鈹鈹鈹鈹
        #       鈹屸攢鈹粹攢鈹     鈹屸攢鈹鈹鈹愨敂鈹鈹鈹鈹 鈻 鈹斺暐鈹樷攲鈹鈹
        # qr_1: 鈹 X 鈹溾攢鈹鈻犫攢鈹鈹 H 鈹溾攢鈹鈻犫攢鈹鈹鈻戔攢鈹鈺鈹鈹M鈹溾攢鈹鈹鈹鈹鈹
        #       鈹斺攢鈹鈹鈹樷攲鈹鈹粹攢鈹愨敂鈹鈹鈹鈹      鈻  鈺 鈹斺暐鈹樷攲鈹鈹
        # qr_2: 鈹鈹鈻犫攢鈹鈹 X 鈹溾攢鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈻戔攢鈹鈺鈹鈹鈺鈹鈹M鈹溾攢鈹鈹
        #       鈹屸攢鈹粹攢鈹愨敂鈹鈹鈹鈹           鈻  鈺  鈺 鈹斺暐鈹樷攲鈹鈹
        # qr_3: 鈹 X 鈹溾攢鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈻戔攢鈹鈺鈹鈹鈺鈹鈹鈺鈹鈹M鈹
        #       鈹斺攢鈹鈹鈹                鈻  鈺  鈺  鈺 鈹斺暐鈹
        #  c: 2/鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺┾晲鈺愨暚鈺愨晲鈺┾晲鈺愨暚鈺
        #                               0  鈺  1  鈺
        #                                  鈺     鈺
        #  d: 2/鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨暕鈺愨晲鈺愨晲鈺愨暕鈺
        #                                  0     1
        qr = QuantumRegister(4, ""qr"")
        cr1 = ClassicalRegister(2, ""c"")
        cr2 = ClassicalRegister(2, ""d"")
        circuit = QuantumCircuit(qr, cr1, cr2)
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])
        circuit.cx(qr[1], qr[2])
        circuit.h(qr[1])
        circuit.cx(qr[1], qr[0])
        circuit.barrier(qr)
        circuit.measure(qr[0], cr1[0])
        circuit.measure(qr[1], cr2[0])
        circuit.measure(qr[2], cr1[1])
        circuit.measure(qr[3], cr2[1])

        coupling = CouplingMap([[0, 1], [0, 2], [2, 3]])  # linear [1, 0, 2, 3]
        property_set = {}
        actual = BIPMapping(coupling, objective=""depth"")(circuit, property_set)
        self.assertEqual(5, actual.depth())

        CheckMap(coupling)(actual, property_set)
        self.assertTrue(property_set[""is_swap_mapped""])","circuit.cx(qr[1], qr[2])",circuit.cx(*qr[1:3]),"iterable_zj[1], iterable_zj[2]",*qr[1:3],*qr[1:3],1,
ThinkMatch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ThinkMatch/models/NGM/model_v2.py,https://github.com/Thinklab-SJTU/ThinkMatch/tree/master/models/NGM/model_v2.py,Net,forward$68,"def forward(
        self,
        data_dict,
    ):
        images = data_dict['images']
        points = data_dict['Ps']
        n_points = data_dict['ns']
        graphs = data_dict['pyg_graphs']
        batch_size = data_dict['batch_size']
        num_graphs = len(images)

        global_list = []
        orig_graph_list = []
        for image, p, n_p, graph in zip(images, points, n_points, graphs):
            # extract feature
            nodes = self.node_layers(image)
            edges = self.edge_layers(nodes)

            global_list.append(self.final_layers(edges).reshape((nodes.shape[0], -1)))
            nodes = normalize_over_channels(nodes)
            edges = normalize_over_channels(edges)

            # arrange features
            U = concat_features(feature_align(nodes, p, n_p, self.rescale), n_p)
            F = concat_features(feature_align(edges, p, n_p, self.rescale), n_p)
            node_features = torch.cat((U, F), dim=1)
            graph.x = node_features

            graph = self.message_pass_node_features(graph)
            orig_graph = self.build_edge_features_from_node_features(graph)
            orig_graph_list.append(orig_graph)

        global_weights_list = [
            torch.cat([global_src, global_tgt], axis=-1) for global_src, global_tgt in lexico_iter(global_list)
        ]

        global_weights_list = [normalize_over_channels(g) for g in global_weights_list]

        unary_affs_list = [
            self.vertex_affinity([item.x for item in g_1], [item.x for item in g_2], global_weights)
            for (g_1, g_2), global_weights in zip(lexico_iter(orig_graph_list), global_weights_list)
        ]

        quadratic_affs_list = [
            self.edge_affinity([item.edge_attr for item in g_1], [item.edge_attr for item in g_2], global_weights)
            for (g_1, g_2), global_weights in zip(lexico_iter(orig_graph_list), global_weights_list)
        ]

        quadratic_affs_list = [[0.5 * x for x in quadratic_affs] for quadratic_affs in quadratic_affs_list]

        s_list, mgm_s_list, x_list, mgm_x_list, indices = [], [], [], [], []

        for unary_affs, quadratic_affs, (idx1, idx2) in zip(unary_affs_list, quadratic_affs_list, lexico_iter(range(num_graphs))):
            kro_G, kro_H = data_dict['KGHs'] if num_graphs == 2 else data_dict['KGHs']['{},{}'.format(idx1, idx2)]
            Kp = torch.stack(pad_tensor(unary_affs), dim=0)
            Ke = torch.stack(pad_tensor(quadratic_affs), dim=0)
            K = construct_aff_mat(Ke, Kp, kro_G, kro_H)
            if num_graphs == 2: data_dict['aff_mat'] = K

            if cfg.NGM.FIRST_ORDER:
                emb = Kp.transpose(1, 2).contiguous().view(Kp.shape[0], -1, 1)
            else:
                emb = torch.ones(K.shape[0], K.shape[1], 1, device=K.device)

            if cfg.NGM.POSITIVE_EDGES:
                A = (K > 0).to(K.dtype)
            else:
                A = (K != 0).to(K.dtype)

            emb_K = K.unsqueeze(-1)

            # NGM qap solver
            for i in range(self.gnn_layer):
                gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))
                emb_K, emb = gnn_layer(A, emb_K, emb, n_points[idx1], n_points[idx2])

            v = self.classifier(emb)
            s = v.view(v.shape[0], points[idx2].shape[1], -1).transpose(1, 2)

            ss = self.sinkhorn(s, n_points[idx1], n_points[idx2], dummy_row=True)
            x = hungarian(ss, n_points[idx1], n_points[idx2])
            s_list.append(ss)
            x_list.append(x)
            indices.append((idx1, idx2))

        if num_graphs > 2:
            joint_indices = torch.cat((torch.cumsum(torch.stack([torch.max(np) for np in n_points]), dim=0), torch.zeros((1,), dtype=torch.long, device=K.device)))
            joint_S = torch.zeros(batch_size, torch.max(joint_indices), torch.max(joint_indices), device=K.device)
            for idx in range(num_graphs):
                for b in range(batch_size):
                    start = joint_indices[idx-1]
                    joint_S[b, start:start+n_points[idx][b], start:start+n_points[idx][b]] += torch.eye(n_points[idx][b], device=K.device)

            for (idx1, idx2), s in zip(indices, s_list):
                if idx1 > idx2:
                    joint_S[:, joint_indices[idx2-1]:joint_indices[idx2], joint_indices[idx1-1]:joint_indices[idx1]] += s.transpose(1, 2)
                else:
                    joint_S[:, joint_indices[idx1-1]:joint_indices[idx1], joint_indices[idx2-1]:joint_indices[idx2]] += s

            matching_s = []
            for b in range(batch_size):
                e, v = torch.symeig(joint_S[b], eigenvectors=True)
                diff = e[-self.univ_size:-1] - e[-self.univ_size+1:]
                if self.training and torch.min(torch.abs(diff)) <= 1e-4:
                    matching_s.append(joint_S[b])
                else:
                    matching_s.append(num_graphs * torch.mm(v[:, -self.univ_size:], v[:, -self.univ_size:].transpose(0, 1)))

            matching_s = torch.stack(matching_s, dim=0)

            for idx1, idx2 in indices:
                s = matching_s[:, joint_indices[idx1-1]:joint_indices[idx1], joint_indices[idx2-1]:joint_indices[idx2]]
                s = self.sinkhorn_mgm(torch.log(torch.relu(s)), n_points[idx1], n_points[idx2]) # only perform row/col norm, do not perform exp
                x = hungarian(s, n_points[idx1], n_points[idx2])

                mgm_s_list.append(s)
                mgm_x_list.append(x)

        if cfg.PROBLEM.TYPE == '2GM':
            data_dict.update({
                'ds_mat': s_list[0],
                'perm_mat': x_list[0]
            })
        elif cfg.PROBLEM.TYPE == 'MGM':
            data_dict.update({
                'ds_mat_list': mgm_s_list,
                'perm_mat_list': mgm_x_list,
                'graph_indices': indices,
            })

        return data_dict","torch.ones(K.shape[0], K.shape[1], 1, device=K.device)","torch.ones(*K.shape[:2], 1, device=K.device)","iterable_zj[0], iterable_zj[1]",*K.shape[:2],*K.shape[:2],1,
Poco,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Poco/poco/drivers/windows/windowsui_poco.py,https://github.com/AirtestProject/Poco/tree/master/poco/drivers/windows/windowsui_poco.py,WindowsPoco,double_click$72,"def double_click(self, pos):
        return self.agent.input.double_click(pos[0], pos[1])","self.agent.input.double_click(pos[0], pos[1])",self.agent.input.double_click(*pos[:2]),"iterable_zj[0], iterable_zj[1]",*pos[:2],*pos[:2],1,
ezdxf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/render/mesh.py,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/render/mesh.py,,estimate_face_normals_direction$147,"def estimate_face_normals_direction(
    vertices: Sequence[Vec3], faces: Sequence[Face]
) -> float:
    """"""Returns the estimated face-normals direction as ``float`` value
    in the range [-1.0, 1.0] for a closed surface.

    This heuristic works well for simple convex hulls but struggles with
    more complex structures like a torus (doughnut).

    A counter-clockwise (ccw) vertex arrangement is assumed but a
    clockwise (cw) arrangement works too but the values are reversed.

    The closer the value to 1.0 (-1.0 for cw) the more likely all normals
    pointing outwards from the surface.

    The closer the value to -1.0 (1.0 for cw) the more likely all normals
    pointing inwards from the surface.

    """"""
    n_vertices = len(vertices)
    if n_vertices == 0:
        return 0.0

    mesh_centroid = Vec3.sum(vertices) / n_vertices
    count = 0
    direction_sum = 0.0
    for face in faces:
        if len(face) < 3:
            continue
        try:
            face_vertices = tuple(vertices[i] for i in face)
        except IndexError:
            continue
        face_centroid = Vec3.sum(face_vertices) / len(face)
        try:
            face_normal = normal_vector_3p(
                face_vertices[0], face_vertices[1], face_vertices[2]
            )
        except ZeroDivisionError:
            continue
        try:
            outward_vec = (face_centroid - mesh_centroid).normalize()
        except ZeroDivisionError:
            continue
        direction_sum += face_normal.dot(outward_vec)
        count += 1
    if count > 0:
        return direction_sum / count
    return 0.0","normal_vector_3p(face_vertices[0], face_vertices[1], face_vertices[2])",normal_vector_3p(*face_vertices[:3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*face_vertices[:3],*face_vertices[:3],1,
godot-blender-exporter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/godot-blender-exporter/io_scene_godot/converters/animation/serializer.py,https://github.com/godotengine/godot-blender-exporter/tree/master/io_scene_godot/converters/animation/serializer.py,AnimationResource,add_obj_xform_track$529,"def add_obj_xform_track(self, node_type, track_path,
                            xform_frames_list, frame_range,
                            parent_mat_inverse=mathutils.Matrix.Identity(4)):
        """"""Add a object transform track to AnimationResource""""""
        track = TransformTrack(
            track_path,
            frames_iter=range(frame_range[0], frame_range[1]),
            values_iter=xform_frames_list,
        )
        track.set_parent_inverse(parent_mat_inverse)
        if node_type in (""SpotLight"", ""DirectionalLight"",
                         ""Camera"", ""CollisionShape""):
            track.is_directional = True

        self.add_track(track)","range(frame_range[0], frame_range[1])",range(*frame_range[:2]),"iterable_zj[0], iterable_zj[1]",*frame_range[:2],*frame_range[:2],1,
OpenPCDet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenPCDet/pcdet/datasets/augmentor/augmentor_utils.py,https://github.com/open-mmlab/OpenPCDet/tree/master/pcdet/datasets/augmentor/augmentor_utils.py,,global_frustum_dropout_right$270,"def global_frustum_dropout_right(gt_boxes, points, intensity_range):
    """"""
    Args:
        gt_boxes: (N, 7), [x, y, z, dx, dy, dz, heading, [vx], [vy]],
        points: (M, 3 + C),
        intensity: [min, max]
    Returns:
    """"""
    intensity = np.random.uniform(intensity_range[0], intensity_range[1])
    
    threshold = np.min(points[:, 1]) + intensity * (np.max(points[:, 1]) - np.min(points[:, 1]))
    points = points[points[:, 1] > threshold]
    gt_boxes = gt_boxes[gt_boxes[:, 1] > threshold]
    
    return gt_boxes, points","np.random.uniform(intensity_range[0], intensity_range[1])",np.random.uniform(*intensity_range[:2]),"iterable_zj[0], iterable_zj[1]",*intensity_range[:2],*intensity_range[:2],1,
graph-rcnn.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/graph-rcnn.pytorch/lib/scene_parser/rcnn/layers/dcn/deform_conv_func.py,https://github.com/jwyang/graph-rcnn.pytorch/tree/master/lib/scene_parser/rcnn/layers/dcn/deform_conv_func.py,ModulatedDeformConvFunction,forward$153,"def forward(
        ctx,
        input,
        offset,
        mask,
        weight,
        bias=None,
        stride=1,
        padding=0,
        dilation=1,
        groups=1,
        deformable_groups=1
    ):
        ctx.stride = stride
        ctx.padding = padding
        ctx.dilation = dilation
        ctx.groups = groups
        ctx.deformable_groups = deformable_groups
        ctx.with_bias = bias is not None
        if not ctx.with_bias:
            bias = input.new_empty(1)  # fake tensor
        if not input.is_cuda:
            raise NotImplementedError
        if weight.requires_grad or mask.requires_grad or offset.requires_grad \
                or input.requires_grad:
            ctx.save_for_backward(input, offset, mask, weight, bias)
        output = input.new_empty(
            ModulatedDeformConvFunction._infer_shape(ctx, input, weight))
        ctx._bufs = [input.new_empty(0), input.new_empty(0)]
        _C.modulated_deform_conv_forward(
            input,
            weight,
            bias,
            ctx._bufs[0],
            offset,
            mask,
            output,
            ctx._bufs[1],
            weight.shape[2],
            weight.shape[3],
            ctx.stride,
            ctx.stride,
            ctx.padding,
            ctx.padding,
            ctx.dilation,
            ctx.dilation,
            ctx.groups,
            ctx.deformable_groups,
            ctx.with_bias
        )
        return output","_C.modulated_deform_conv_forward(input, weight, bias, ctx._bufs[0], offset, mask, output, ctx._bufs[1], weight.shape[2], weight.shape[3], ctx.stride, ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation, ctx.groups, ctx.deformable_groups, ctx.with_bias)","_C.modulated_deform_conv_forward(input, weight, bias, ctx._bufs[0], offset, mask, output, ctx._bufs[1], *weight.shape[2:4], ctx.stride, ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation, ctx.groups, ctx.deformable_groups, ctx.with_bias)","iterable_zj[2], iterable_zj[3]",*weight.shape[2:4],*weight.shape[2:4],1,
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/widgets/optionwidgets.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/widgets/optionwidgets.py,MySlider,apply_pic$711,"def apply_pic(self, picn, x, y, length, resize=None, only_text=None, source_val=None):
        
        if resize:
            txt = '<html><img src=""{0}"" width=""{2}""><p>{1}</p><html>'.format(picn, length, 100)
        else:
            txt = '<html><img src=""{}""><p>{}</p><html>'.format(picn, length)
        if ui.live_preview_style == 'tooltip':
            try:
                if self.final_url != ui.final_playing_url and not resize:
                    self.final_url = ui.final_playing_url
                    img = Image.open(picn)
                    self.half_size = int(img.size[0]/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\n change dimensions \n')
            except Exception as err:
                ui.logger.error(err)
            if self.tooltip is None:
                self.setToolTip('')
                self.setToolTip(txt)
            else:
                if ui.fullscreen_video or ui.force_fs:
                    y_cord = self.parent.y() + self.parent.maximumHeight() - 25
                else:
                    y_cord = self.parent.y() + self.parent.maximumHeight() + 25
                x_cord = self.parent.x() + x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                if source_val and source_val == ""progressbar"":
                    offset = 25
                else:
                    offset = 0
                print(offset)
                point = QtCore.QPoint(x_cord, y_cord - offset)
                rect = QtCore.QRect(self.parent.x(), self.parent.y() - offset, self.parent.width(), self.parent.height())
                #print(self.parent.x(), self.parent.y(), self.parent.width(), self.parent.height())
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip.showText(point, txt, self, rect, 3000)
        elif ui.live_preview_style == 'widget':
            try:
                if self.check_dimension_again and not resize:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = False
                if self.final_url != ui.final_playing_url:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.final_url = ui.final_playing_url
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = True
            except Exception as err:
                ui.logger.error(err)
            if os.path.isfile(picn):
                if not only_text:
                    self.pic.setPixmap(QtGui.QPixmap(picn, ""1""))
                x_cord = self.parent.x()+x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit - self.tooltip_widget.width()
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                y_cord = self.parent.y() - self.tooltip_widget.height() + ui.player_opt.height()
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip_widget.setGeometry(x_cord, y_cord, 128, 128)
                    self.tooltip_widget.show()
                    self.txt.setText(length)","QtCore.QSize(img.size[0], img.size[1])",QtCore.QSize(*img.size[:2]),"iterable_zj[0], iterable_zj[1]",*img.size[:2],*img.size[:2],1,
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/widgets/optionwidgets.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/widgets/optionwidgets.py,MySlider,apply_pic$711,"def apply_pic(self, picn, x, y, length, resize=None, only_text=None, source_val=None):
        
        if resize:
            txt = '<html><img src=""{0}"" width=""{2}""><p>{1}</p><html>'.format(picn, length, 100)
        else:
            txt = '<html><img src=""{}""><p>{}</p><html>'.format(picn, length)
        if ui.live_preview_style == 'tooltip':
            try:
                if self.final_url != ui.final_playing_url and not resize:
                    self.final_url = ui.final_playing_url
                    img = Image.open(picn)
                    self.half_size = int(img.size[0]/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\n change dimensions \n')
            except Exception as err:
                ui.logger.error(err)
            if self.tooltip is None:
                self.setToolTip('')
                self.setToolTip(txt)
            else:
                if ui.fullscreen_video or ui.force_fs:
                    y_cord = self.parent.y() + self.parent.maximumHeight() - 25
                else:
                    y_cord = self.parent.y() + self.parent.maximumHeight() + 25
                x_cord = self.parent.x() + x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                if source_val and source_val == ""progressbar"":
                    offset = 25
                else:
                    offset = 0
                print(offset)
                point = QtCore.QPoint(x_cord, y_cord - offset)
                rect = QtCore.QRect(self.parent.x(), self.parent.y() - offset, self.parent.width(), self.parent.height())
                #print(self.parent.x(), self.parent.y(), self.parent.width(), self.parent.height())
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip.showText(point, txt, self, rect, 3000)
        elif ui.live_preview_style == 'widget':
            try:
                if self.check_dimension_again and not resize:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = False
                if self.final_url != ui.final_playing_url:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.final_url = ui.final_playing_url
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = True
            except Exception as err:
                ui.logger.error(err)
            if os.path.isfile(picn):
                if not only_text:
                    self.pic.setPixmap(QtGui.QPixmap(picn, ""1""))
                x_cord = self.parent.x()+x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit - self.tooltip_widget.width()
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                y_cord = self.parent.y() - self.tooltip_widget.height() + ui.player_opt.height()
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip_widget.setGeometry(x_cord, y_cord, 128, 128)
                    self.tooltip_widget.show()
                    self.txt.setText(length)","QtCore.QSize(img.size[0], img.size[1])",QtCore.QSize(*img.size[:2]),"iterable_zj[0], iterable_zj[1]",*img.size[:2],*img.size[:2],1,
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/widgets/optionwidgets.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/widgets/optionwidgets.py,MySlider,apply_pic$711,"def apply_pic(self, picn, x, y, length, resize=None, only_text=None, source_val=None):
        
        if resize:
            txt = '<html><img src=""{0}"" width=""{2}""><p>{1}</p><html>'.format(picn, length, 100)
        else:
            txt = '<html><img src=""{}""><p>{}</p><html>'.format(picn, length)
        if ui.live_preview_style == 'tooltip':
            try:
                if self.final_url != ui.final_playing_url and not resize:
                    self.final_url = ui.final_playing_url
                    img = Image.open(picn)
                    self.half_size = int(img.size[0]/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\n change dimensions \n')
            except Exception as err:
                ui.logger.error(err)
            if self.tooltip is None:
                self.setToolTip('')
                self.setToolTip(txt)
            else:
                if ui.fullscreen_video or ui.force_fs:
                    y_cord = self.parent.y() + self.parent.maximumHeight() - 25
                else:
                    y_cord = self.parent.y() + self.parent.maximumHeight() + 25
                x_cord = self.parent.x() + x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                if source_val and source_val == ""progressbar"":
                    offset = 25
                else:
                    offset = 0
                print(offset)
                point = QtCore.QPoint(x_cord, y_cord - offset)
                rect = QtCore.QRect(self.parent.x(), self.parent.y() - offset, self.parent.width(), self.parent.height())
                #print(self.parent.x(), self.parent.y(), self.parent.width(), self.parent.height())
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip.showText(point, txt, self, rect, 3000)
        elif ui.live_preview_style == 'widget':
            try:
                if self.check_dimension_again and not resize:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = False
                if self.final_url != ui.final_playing_url:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.final_url = ui.final_playing_url
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = True
            except Exception as err:
                ui.logger.error(err)
            if os.path.isfile(picn):
                if not only_text:
                    self.pic.setPixmap(QtGui.QPixmap(picn, ""1""))
                x_cord = self.parent.x()+x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit - self.tooltip_widget.width()
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                y_cord = self.parent.y() - self.tooltip_widget.height() + ui.player_opt.height()
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip_widget.setGeometry(x_cord, y_cord, 128, 128)
                    self.tooltip_widget.show()
                    self.txt.setText(length)","QtCore.QSize(img.size[0], img.size[1])",QtCore.QSize(*img.size[:2]),"iterable_zj[0], iterable_zj[1]",*img.size[:2],*img.size[:2],1,
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/widgets/optionwidgets.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/widgets/optionwidgets.py,MySlider,apply_pic$711,"def apply_pic(self, picn, x, y, length, resize=None, only_text=None, source_val=None):
        
        if resize:
            txt = '<html><img src=""{0}"" width=""{2}""><p>{1}</p><html>'.format(picn, length, 100)
        else:
            txt = '<html><img src=""{}""><p>{}</p><html>'.format(picn, length)
        if ui.live_preview_style == 'tooltip':
            try:
                if self.final_url != ui.final_playing_url and not resize:
                    self.final_url = ui.final_playing_url
                    img = Image.open(picn)
                    self.half_size = int(img.size[0]/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\n change dimensions \n')
            except Exception as err:
                ui.logger.error(err)
            if self.tooltip is None:
                self.setToolTip('')
                self.setToolTip(txt)
            else:
                if ui.fullscreen_video or ui.force_fs:
                    y_cord = self.parent.y() + self.parent.maximumHeight() - 25
                else:
                    y_cord = self.parent.y() + self.parent.maximumHeight() + 25
                x_cord = self.parent.x() + x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                if source_val and source_val == ""progressbar"":
                    offset = 25
                else:
                    offset = 0
                print(offset)
                point = QtCore.QPoint(x_cord, y_cord - offset)
                rect = QtCore.QRect(self.parent.x(), self.parent.y() - offset, self.parent.width(), self.parent.height())
                #print(self.parent.x(), self.parent.y(), self.parent.width(), self.parent.height())
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip.showText(point, txt, self, rect, 3000)
        elif ui.live_preview_style == 'widget':
            try:
                if self.check_dimension_again and not resize:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = False
                if self.final_url != ui.final_playing_url:
                    img = Image.open(picn)
                    self.pic.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.pic.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]))
                    self.tooltip_widget.setMaximumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.tooltip_widget.setMinimumSize(QtCore.QSize(img.size[0], img.size[1]+30))
                    self.txt.setMaximumSize(QtCore.QSize(img.size[0], 30))
                    self.txt.setMinimumSize(QtCore.QSize(img.size[0], 30))
                    self.final_url = ui.final_playing_url
                    self.half_size = int(self.tooltip_widget.width()/2)
                    self.upper_limit = self.parent.x() + self.parent.width()
                    self.lower_limit = self.parent.x()
                    ui.logger.debug('\nchange dimensions\n')
                    self.check_dimension_again = True
            except Exception as err:
                ui.logger.error(err)
            if os.path.isfile(picn):
                if not only_text:
                    self.pic.setPixmap(QtGui.QPixmap(picn, ""1""))
                x_cord = self.parent.x()+x
                if x_cord + self.half_size > self.upper_limit:
                    x_cord = self.upper_limit - self.tooltip_widget.width()
                elif x_cord - self.half_size < self.lower_limit:
                    x_cord = self.parent.x()
                else:
                    x_cord = x_cord - self.half_size
                y_cord = self.parent.y() - self.tooltip_widget.height() + ui.player_opt.height()
                if (not self.preview_pending or resize or ui.live_preview == 'slow') and self.enter:
                    self.tooltip_widget.setGeometry(x_cord, y_cord, 128, 128)
                    self.tooltip_widget.show()
                    self.txt.setText(length)","QtCore.QSize(img.size[0], img.size[1])",QtCore.QSize(*img.size[:2]),"iterable_zj[0], iterable_zj[1]",*img.size[:2],*img.size[:2],1,
airflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/tests/providers/postgres/hooks/test_postgres.py,https://github.com/apache/airflow/tree/master/tests/providers/postgres/hooks/test_postgres.py,TestPostgresHook,test_insert_rows_replace$289,"def test_insert_rows_replace(self):
        table = ""table""
        rows = [
            (
                1,
                ""hello"",
            ),
            (
                2,
                ""world"",
            ),
        ]
        fields = (""id"", ""value"")

        self.db_hook.insert_rows(table, rows, fields, replace=True, replace_index=fields[0])

        assert self.conn.close.call_count == 1
        assert self.cur.close.call_count == 1

        commit_count = 2  # The first and last commit
        assert commit_count == self.conn.commit.call_count

        sql = (
            ""INSERT INTO {0} ({1}, {2}) VALUES (%s,%s) ""
            ""ON CONFLICT ({1}) DO UPDATE SET {2} = excluded.{2}"".format(table, fields[0], fields[1])
        )
        for row in rows:
            self.cur.execute.assert_any_call(sql, row)","'INSERT INTO {0} ({1}, {2}) VALUES (%s,%s) ON CONFLICT ({1}) DO UPDATE SET {2} = excluded.{2}'.format(table, fields[0], fields[1])","'INSERT INTO {0} ({1}, {2}) VALUES (%s,%s) ON CONFLICT ({1}) DO UPDATE SET {2} = excluded.{2}'.format(table, *fields[:2])","iterable_zj[0], iterable_zj[1]",*fields[:2],*fields[:2],1,
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->a', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1,
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->ab', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1,
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->bc', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1,
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->cd', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1,
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1,
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1,
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1,
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/utils/tests/test_random.py,https://github.com/bayespy/bayespy/tree/master/bayespy/utils/tests/test_random.py,TestAlphaBetaRecursion,test$102,"def test(self):
        """"""
        Test the results of alpha-beta recursion for Markov chains
        """"""

        np.seterr(divide='ignore')

        # Deterministic oscillator
        p0 = np.array([1.0, 0.0])
        P = np.array(3*[[[0.0, 1.0],
                         [1.0, 0.0]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]],
                              [[0.0, 0.0],
                               [1.0, 0.0]],
                              [[0.0, 1.0],
                               [0.0, 0.0]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Maximum randomness
        p0 = np.array([0.5, 0.5])
        P = np.array(3*[[[0.5, 0.5],
                         [0.5, 0.5]]])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Unnormalized probabilities
        p0 = np.array([2, 2])
        P = np.array([ [[4, 4],
                        [4, 4]],
                       [[8, 8],
                        [8, 8]],
                       [[20, 20],
                        [20, 20]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [0.5, 0.5])
        self.assertAllClose(zz,
                            [ [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]],
                              [[0.25, 0.25],
                               [0.25, 0.25]] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")
        p0 = np.array([2, 6])
        P = np.array([ [[0, 3],
                        [4, 1]],
                       [[3, 5],
                        [6, 4]],
                       [[9, 2],
                        [8, 1]] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        y0 = np.einsum('a,ab,bc,cd->a', p0, P[0], P[1], P[2])
        y1 = np.einsum('a,ab,bc,cd->ab', p0, P[0], P[1], P[2])
        y2 = np.einsum('a,ab,bc,cd->bc', p0, P[0], P[1], P[2])
        y3 = np.einsum('a,ab,bc,cd->cd', p0, P[0], P[1], P[2])
        self.assertAllClose(z0,
                            y0 / np.sum(y0))
        self.assertAllClose(zz,
                            [ y1 / np.sum(y1),
                              y2 / np.sum(y2),
                              y3 / np.sum(y3) ])
        self.assertAllClose(g,
                            -np.log(np.einsum('a,ab,bc,cd->',
                                              p0, P[0], P[1], P[2])),
                            msg=""Cumulant generating function incorrect"")

        # Test plates
        p0 = np.array([ [1.0, 0.0],
                        [0.5, 0.5] ])
        P = np.array([ [ [[0.0, 1.0],
                          [1.0, 0.0]] ],
                       [ [[0.5, 0.5],
                          [0.5, 0.5]] ] ])
        (z0, zz, g) = random.alpha_beta_recursion(np.log(p0),
                                                  np.log(P))
        self.assertAllClose(z0,
                            [[1.0, 0.0],
                             [0.5, 0.5]])
        self.assertAllClose(zz,
                            [ [ [[0.0, 1.0],
                                 [0.0, 0.0]] ],
                              [ [[0.25, 0.25],
                                 [0.25, 0.25]] ] ])
        self.assertAllClose(g,
                            -np.log(np.einsum('...a,...ab->...',
                                              p0, P[...,0,:,:])),
                            msg=""Cumulant generating function incorrect"")

        # Test overflow
        logp0 = np.array([1e5, -np.inf])
        logP = np.array([[[-np.inf, 1e5],
                          [-np.inf, 1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test underflow
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array([[[-np.inf, -1e5],
                          [-np.inf, -1e5]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertAllClose(z0,
                            [1.0, 0])
        self.assertAllClose(zz,
                            [ [[0.0, 1.0],
                               [0.0, 0.0]] ])
        ## self.assertAllClose(g,
        ##                     -np.log(np.einsum('a,ab,bc,cd->',
        ##                                       p0, P[0], P[1], P[2])))

        # Test stability of the algorithm
        logp0 = np.array([-1e5, -np.inf])
        logP = np.array(10*[[[-np.inf, 1e5],
                             [1e0, -np.inf]]])
        (z0, zz, g) = random.alpha_beta_recursion(logp0,
                                                  logP)
        self.assertTrue(np.all(~np.isnan(z0)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(zz)),
                        msg=""Nans in results, algorithm not stable"")
        self.assertTrue(np.all(~np.isnan(g)),
                        msg=""Nans in results, algorithm not stable"")

        pass","np.einsum('a,ab,bc,cd->', p0, P[0], P[1], P[2])","np.einsum('a,ab,bc,cd->', p0, *P[:3])","iterable_zj[0], iterable_zj[1], iterable_zj[2]",*P[:3],*P[:3],1,
pynndescent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pynndescent/pynndescent/pynndescent_.py,https://github.com/lmcinnes/pynndescent/tree/master/pynndescent/pynndescent_.py,NNDescent,_init_search_function$1190,"def _init_search_function(self):

        if self.verbose:
            print(ts(), ""Building and compiling search function"")

        if self.tree_init:
            tree_hyperplanes = self._search_forest[0].hyperplanes
            tree_offsets = self._search_forest[0].offsets
            tree_indices = self._search_forest[0].indices
            tree_children = self._search_forest[0].children

            @numba.njit(
                [
                    numba.types.Array(numba.types.int32, 1, ""C"", readonly=True)(
                        numba.types.Array(numba.types.float32, 1, ""C"", readonly=True),
                        numba.types.Array(numba.types.int64, 1, ""C"", readonly=False),
                    )
                ],
                locals={""node"": numba.types.uint32, ""side"": numba.types.boolean},
            )
            def tree_search_closure(point, rng_state):
                node = 0
                while tree_children[node, 0] > 0:
                    side = select_side(
                        tree_hyperplanes[node], tree_offsets[node], point, rng_state
                    )
                    if side == 0:
                        node = tree_children[node, 0]
                    else:
                        node = tree_children[node, 1]

                return -tree_children[node]

            self._tree_search = tree_search_closure
        else:

            @numba.njit()
            def tree_search_closure(point, rng_state):
                return (0, 0)

            self._tree_search = tree_search_closure
            tree_indices = np.zeros(1, dtype=np.int64)

        alternative_dot = pynnd_dist.alternative_dot
        alternative_cosine = pynnd_dist.alternative_cosine

        data = self._raw_data
        indptr = self._search_graph.indptr
        indices = self._search_graph.indices
        dist = self._distance_func
        n_neighbors = self.n_neighbors
        parallel_search = self.parallel_batch_queries

        @numba.njit(
            fastmath=True,
            locals={
                ""current_query"": numba.types.float32[::1],
                ""i"": numba.types.uint32,
                ""j"": numba.types.uint32,
                ""heap_priorities"": numba.types.float32[::1],
                ""heap_indices"": numba.types.int32[::1],
                ""candidate"": numba.types.int32,
                ""vertex"": numba.types.int32,
                ""d"": numba.types.float32,
                ""d_vertex"": numba.types.float32,
                ""visited"": numba.types.uint8[::1],
                ""indices"": numba.types.int32[::1],
                ""indptr"": numba.types.int32[::1],
                ""data"": numba.types.float32[:, ::1],
                ""heap_size"": numba.types.int16,
                ""distance_scale"": numba.types.float32,
                ""distance_bound"": numba.types.float32,
                ""seed_scale"": numba.types.float32,
            },
            parallel=self.parallel_batch_queries,
        )
        def search_closure(query_points, k, epsilon, visited, rng_state):

            result = make_heap(query_points.shape[0], k)
            distance_scale = 1.0 + epsilon
            internal_rng_state = np.copy(rng_state)

            for i in numba.prange(query_points.shape[0]):
                # Avoid races on visited if parallel
                if parallel_search:
                    visited_nodes = np.zeros_like(visited)
                else:
                    visited_nodes = visited
                    visited_nodes[:] = 0

                if dist == alternative_dot or dist == alternative_cosine:
                    norm = np.sqrt((query_points[i] ** 2).sum())
                    if norm > 0.0:
                        current_query = query_points[i] / norm
                    else:
                        continue
                else:
                    current_query = query_points[i]

                heap_priorities = result[1][i]
                heap_indices = result[0][i]
                seed_set = [(np.float32(np.inf), np.int32(-1)) for j in range(0)]
                # heapq.heapify(seed_set)

                ############ Init ################
                index_bounds = tree_search_closure(current_query, internal_rng_state)
                candidate_indices = tree_indices[index_bounds[0] : index_bounds[1]]

                n_initial_points = candidate_indices.shape[0]
                n_random_samples = min(k, n_neighbors) - n_initial_points

                for j in range(n_initial_points):
                    candidate = candidate_indices[j]
                    d = np.float32(dist(data[candidate], current_query))
                    # indices are guaranteed different
                    simple_heap_push(heap_priorities, heap_indices, d, candidate)
                    heapq.heappush(seed_set, (d, candidate))
                    mark_visited(visited_nodes, candidate)

                if n_random_samples > 0:
                    for j in range(n_random_samples):
                        candidate = np.int32(
                            np.abs(tau_rand_int(internal_rng_state)) % data.shape[0]
                        )
                        if has_been_visited(visited_nodes, candidate) == 0:
                            d = np.float32(dist(data[candidate], current_query))
                            simple_heap_push(
                                heap_priorities, heap_indices, d, candidate
                            )
                            heapq.heappush(seed_set, (d, candidate))
                            mark_visited(visited_nodes, candidate)

                ############ Search ##############
                distance_bound = distance_scale * heap_priorities[0]

                # Find smallest seed point
                d_vertex, vertex = heapq.heappop(seed_set)

                while d_vertex < distance_bound:

                    for j in range(indptr[vertex], indptr[vertex + 1]):

                        candidate = indices[j]

                        if has_been_visited(visited_nodes, candidate) == 0:
                            mark_visited(visited_nodes, candidate)

                            d = np.float32(dist(data[candidate], current_query))

                            if d < distance_bound:
                                simple_heap_push(
                                    heap_priorities, heap_indices, d, candidate
                                )
                                heapq.heappush(seed_set, (d, candidate))
                                # Update bound
                                distance_bound = distance_scale * heap_priorities[0]

                    # find new smallest seed point
                    if len(seed_set) == 0:
                        break
                    else:
                        d_vertex, vertex = heapq.heappop(seed_set)

            return result

        self._search_function = search_closure
        if hasattr(deheap_sort, ""py_func""):
            self._deheap_function = numba.njit(parallel=self.parallel_batch_queries)(
                deheap_sort.py_func
            )
        else:
            self._deheap_function = deheap_sort

        # Force compilation of the search function (hardcoded k, epsilon)
        query_data = self._raw_data[:1]
        inds, dists, _ = self._search_function(
            query_data, 5, 0.0, self._visited, self.search_rng_state
        )
        _ = self._deheap_function(inds, dists)","range(indptr[vertex], indptr[vertex + 1])",range(*indptr[vertex:vertex + 2]),"iterable_zj[vertex], iterable_zj[vertex + 1]",*indptr[vertex:vertex+2],*indptr[vertex:vertex + 2],1,
External-Attention-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/External-Attention-pytorch/model/attention/BAM.py,https://github.com/xmu-xiaoma666/External-Attention-pytorch/tree/master/model/attention/BAM.py,ChannelAttention,__init__$11,"def __init__(self,channel,reduction=16,num_layers=3):
        super().__init__()
        self.avgpool=nn.AdaptiveAvgPool2d(1)
        gate_channels=[channel]
        gate_channels+=[channel//reduction]*num_layers
        gate_channels+=[channel]


        self.ca=nn.Sequential()
        self.ca.add_module('flatten',Flatten())
        for i in range(len(gate_channels)-2):
            self.ca.add_module('fc%d'%i,nn.Linear(gate_channels[i],gate_channels[i+1]))
            self.ca.add_module('bn%d'%i,nn.BatchNorm1d(gate_channels[i+1]))
            self.ca.add_module('relu%d'%i,nn.ReLU())
        self.ca.add_module('last_fc',nn.Linear(gate_channels[-2],gate_channels[-1]))","nn.Linear(gate_channels[-2], gate_channels[-1])",nn.Linear(*gate_channels[-2:]),"iterable_zj[-2], iterable_zj[-1]",*gate_channels[-2:],*gate_channels[-2:0],0,1
External-Attention-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/External-Attention-pytorch/model/attention/BAM.py,https://github.com/xmu-xiaoma666/External-Attention-pytorch/tree/master/model/attention/BAM.py,ChannelAttention,__init__$11,"def __init__(self,channel,reduction=16,num_layers=3):
        super().__init__()
        self.avgpool=nn.AdaptiveAvgPool2d(1)
        gate_channels=[channel]
        gate_channels+=[channel//reduction]*num_layers
        gate_channels+=[channel]


        self.ca=nn.Sequential()
        self.ca.add_module('flatten',Flatten())
        for i in range(len(gate_channels)-2):
            self.ca.add_module('fc%d'%i,nn.Linear(gate_channels[i],gate_channels[i+1]))
            self.ca.add_module('bn%d'%i,nn.BatchNorm1d(gate_channels[i+1]))
            self.ca.add_module('relu%d'%i,nn.ReLU())
        self.ca.add_module('last_fc',nn.Linear(gate_channels[-2],gate_channels[-1]))","nn.Linear(gate_channels[i], gate_channels[i + 1])",Cannot refactor,"The only sequence that meets the condition is: iterable_zj[i], iterable_zj[i+1]",,*gate_channels[i:i + 2],0,
Pointnet2_PyTorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pointnet2_PyTorch/pointnet2_ops_lib/pointnet2_ops/pointnet2_modules.py,https://github.com/erikwijmans/Pointnet2_PyTorch/tree/master/pointnet2_ops_lib/pointnet2_ops/pointnet2_modules.py,,build_shared_mlp$9,"def build_shared_mlp(mlp_spec: List[int], bn: bool = True):
    layers = []
    for i in range(1, len(mlp_spec)):
        layers.append(
            nn.Conv2d(mlp_spec[i - 1], mlp_spec[i], kernel_size=1, bias=not bn)
        )
        if bn:
            layers.append(nn.BatchNorm2d(mlp_spec[i]))
        layers.append(nn.ReLU(True))

    return nn.Sequential(*layers)","nn.Conv2d(mlp_spec[i - 1], mlp_spec[i], kernel_size=1, bias=not bn)",Cannot refactor,"iterable_zj[i-1], iterable_zj[i]",*mlp_spec[i-1:i+1],*mlp_spec[i - 1:i + 1],0,
mmf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmf/mmf/modules/layers.py,https://github.com/facebookresearch/mmf/tree/master/mmf/modules/layers.py,AttnPool2d,forward$776,"def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(
            2, 0, 1
        )  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC
        x, _ = nn.functional.multi_head_attention_forward(
            query=x,
            key=x,
            value=x,
            embed_dim_to_check=x.shape[-1],
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat(
                [self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]
            ),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False,
        )

        return x[0]","x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3])",x.reshape(*x.shape[:3] * x.shape[3]),"iterable_zj[0], iterable_zj[1], iterable_zj[2]",*x.shape[:3],*x.shape[:2],0,
httplib2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/httplib2/python2/httplib2/socks.py,https://github.com/httplib2/httplib2/tree/master/python2/httplib2/socks.py,socksocket,connect$469,"def connect(self, destpair):
        """"""connect(self, despair)
        Connects to the specified destination through a proxy.
        destpar - A tuple of the IP/DNS address and the port number.
        (identical to socket's connect).
        To select the proxy server use setproxy().
        """"""
        # Do a minimal input check first
        if (
            (not type(destpair) in (list, tuple))
            or (len(destpair) < 2)
            or (not isinstance(destpair[0], basestring))
            or (type(destpair[1]) != int)
        ):
            raise GeneralProxyError((5, _generalerrors[5]))
        if self.__proxy[0] == PROXY_TYPE_SOCKS5:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks5(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks4(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatehttp(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP_NO_TUNNEL:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            if destpair[1] == 443:
                self.__negotiatehttp(destpair[0], destpair[1])
            else:
                self.__httptunnel = False
        elif self.__proxy[0] == None:
            _orgsocket.connect(self, (destpair[0], destpair[1]))
        else:
            raise GeneralProxyError((4, _generalerrors[4]))","self.__negotiatesocks5(destpair[0], destpair[1])",self.__negotiatesocks5(*destpair[:2]),"iterable_zj[0], iterable_zj[1]",*destpair[:2],*destpair[:2],1,
httplib2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/httplib2/python2/httplib2/socks.py,https://github.com/httplib2/httplib2/tree/master/python2/httplib2/socks.py,socksocket,connect$469,"def connect(self, destpair):
        """"""connect(self, despair)
        Connects to the specified destination through a proxy.
        destpar - A tuple of the IP/DNS address and the port number.
        (identical to socket's connect).
        To select the proxy server use setproxy().
        """"""
        # Do a minimal input check first
        if (
            (not type(destpair) in (list, tuple))
            or (len(destpair) < 2)
            or (not isinstance(destpair[0], basestring))
            or (type(destpair[1]) != int)
        ):
            raise GeneralProxyError((5, _generalerrors[5]))
        if self.__proxy[0] == PROXY_TYPE_SOCKS5:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks5(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks4(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatehttp(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP_NO_TUNNEL:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            if destpair[1] == 443:
                self.__negotiatehttp(destpair[0], destpair[1])
            else:
                self.__httptunnel = False
        elif self.__proxy[0] == None:
            _orgsocket.connect(self, (destpair[0], destpair[1]))
        else:
            raise GeneralProxyError((4, _generalerrors[4]))","self.__negotiatesocks4(destpair[0], destpair[1])",self.__negotiatesocks4(*destpair[:2]),"iterable_zj[0], iterable_zj[1]",*destpair[:2],*destpair[:2],1,
httplib2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/httplib2/python2/httplib2/socks.py,https://github.com/httplib2/httplib2/tree/master/python2/httplib2/socks.py,socksocket,connect$469,"def connect(self, destpair):
        """"""connect(self, despair)
        Connects to the specified destination through a proxy.
        destpar - A tuple of the IP/DNS address and the port number.
        (identical to socket's connect).
        To select the proxy server use setproxy().
        """"""
        # Do a minimal input check first
        if (
            (not type(destpair) in (list, tuple))
            or (len(destpair) < 2)
            or (not isinstance(destpair[0], basestring))
            or (type(destpair[1]) != int)
        ):
            raise GeneralProxyError((5, _generalerrors[5]))
        if self.__proxy[0] == PROXY_TYPE_SOCKS5:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks5(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks4(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatehttp(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP_NO_TUNNEL:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            if destpair[1] == 443:
                self.__negotiatehttp(destpair[0], destpair[1])
            else:
                self.__httptunnel = False
        elif self.__proxy[0] == None:
            _orgsocket.connect(self, (destpair[0], destpair[1]))
        else:
            raise GeneralProxyError((4, _generalerrors[4]))","self.__negotiatehttp(destpair[0], destpair[1])",self.__negotiatehttp(*destpair[:2]),"iterable_zj[0], iterable_zj[1]",*destpair[:2],*destpair[:2],1,
httplib2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/httplib2/python2/httplib2/socks.py,https://github.com/httplib2/httplib2/tree/master/python2/httplib2/socks.py,socksocket,connect$469,"def connect(self, destpair):
        """"""connect(self, despair)
        Connects to the specified destination through a proxy.
        destpar - A tuple of the IP/DNS address and the port number.
        (identical to socket's connect).
        To select the proxy server use setproxy().
        """"""
        # Do a minimal input check first
        if (
            (not type(destpair) in (list, tuple))
            or (len(destpair) < 2)
            or (not isinstance(destpair[0], basestring))
            or (type(destpair[1]) != int)
        ):
            raise GeneralProxyError((5, _generalerrors[5]))
        if self.__proxy[0] == PROXY_TYPE_SOCKS5:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks5(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 1080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatesocks4(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            self.__negotiatehttp(destpair[0], destpair[1])
        elif self.__proxy[0] == PROXY_TYPE_HTTP_NO_TUNNEL:
            if self.__proxy[2] != None:
                portnum = self.__proxy[2]
            else:
                portnum = 8080
            _orgsocket.connect(self, (self.__proxy[1], portnum))
            if destpair[1] == 443:
                self.__negotiatehttp(destpair[0], destpair[1])
            else:
                self.__httptunnel = False
        elif self.__proxy[0] == None:
            _orgsocket.connect(self, (destpair[0], destpair[1]))
        else:
            raise GeneralProxyError((4, _generalerrors[4]))","self.__negotiatehttp(destpair[0], destpair[1])",self.__negotiatehttp(*destpair[:2]),"iterable_zj[0], iterable_zj[1]",*destpair[:2],*destpair[:2],1,
sclack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sclack/sclack/loading.py,https://github.com/haskellcamargo/sclack/tree/master/sclack/loading.py,SlackBot,__init__$95,"def __init__(self):
        super(SlackBot, self).__init__([
            urwid.Text([
                (urwid.AttrSpec(pair[1], pair[2]), pair[0]) for pair in row
            ], align='center')
            for row in self._matrix
        ])","urwid.AttrSpec(pair[1], pair[2])",urwid.AttrSpec(*pair[1:3]),"iterable_zj[1], iterable_zj[2]",*pair[1:3],*pair[1:3],1,
NOFOUND,,,,,,,,,,,,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/transpiler/test_bip_mapping.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_bip_mapping.py,TestBIPMapping,test_multi_cregs$180,"def test_multi_cregs(self):
        """"""Test for multiple ClassicalRegisters.""""""

        #                      鈹屸攢鈹鈹鈹 鈻 鈹屸攢鈹
        # qr_0: 鈹鈹鈻犫攢鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹 X 鈹溾攢鈻戔攢鈹M鈹溾攢鈹鈹鈹鈹鈹鈹鈹鈹
        #       鈹屸攢鈹粹攢鈹     鈹屸攢鈹鈹鈹愨敂鈹鈹鈹鈹 鈻 鈹斺暐鈹樷攲鈹鈹
        # qr_1: 鈹 X 鈹溾攢鈹鈻犫攢鈹鈹 H 鈹溾攢鈹鈻犫攢鈹鈹鈻戔攢鈹鈺鈹鈹M鈹溾攢鈹鈹鈹鈹鈹
        #       鈹斺攢鈹鈹鈹樷攲鈹鈹粹攢鈹愨敂鈹鈹鈹鈹      鈻  鈺 鈹斺暐鈹樷攲鈹鈹
        # qr_2: 鈹鈹鈻犫攢鈹鈹 X 鈹溾攢鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈻戔攢鈹鈺鈹鈹鈺鈹鈹M鈹溾攢鈹鈹
        #       鈹屸攢鈹粹攢鈹愨敂鈹鈹鈹鈹           鈻  鈺  鈺 鈹斺暐鈹樷攲鈹鈹
        # qr_3: 鈹 X 鈹溾攢鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈹鈻戔攢鈹鈺鈹鈹鈺鈹鈹鈺鈹鈹M鈹
        #       鈹斺攢鈹鈹鈹                鈻  鈺  鈺  鈺 鈹斺暐鈹
        #  c: 2/鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺┾晲鈺愨暚鈺愨晲鈺┾晲鈺愨暚鈺
        #                               0  鈺  1  鈺
        #                                  鈺     鈺
        #  d: 2/鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨晲鈺愨暕鈺愨晲鈺愨晲鈺愨暕鈺
        #                                  0     1
        qr = QuantumRegister(4, ""qr"")
        cr1 = ClassicalRegister(2, ""c"")
        cr2 = ClassicalRegister(2, ""d"")
        circuit = QuantumCircuit(qr, cr1, cr2)
        circuit.cx(qr[0], qr[1])
        circuit.cx(qr[2], qr[3])
        circuit.cx(qr[1], qr[2])
        circuit.h(qr[1])
        circuit.cx(qr[1], qr[0])
        circuit.barrier(qr)
        circuit.measure(qr[0], cr1[0])
        circuit.measure(qr[1], cr2[0])
        circuit.measure(qr[2], cr1[1])
        circuit.measure(qr[3], cr2[1])

        coupling = CouplingMap([[0, 1], [0, 2], [2, 3]])  # linear [1, 0, 2, 3]
        property_set = {}
        actual = BIPMapping(coupling, objective=""depth"")(circuit, property_set)
        self.assertEqual(5, actual.depth())

        CheckMap(coupling)(actual, property_set)
        self.assertTrue(property_set[""is_swap_mapped""])","circuit.cx(qr[2], qr[3])",*qr[2:4],0,,,,
