repo_name,file_path,file_html,class_name,me_name,me_code,old_code,chatGPT_code,element_str,slice_str,truth_code,
Plex-Meta-Manager,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Plex-Meta-Manager/modules/sonarr.py,https://github.com/meisnate12/Plex-Meta-Manager/tree/master/modules/sonarr.py,Sonarr,remove_all_with_tags$234,"def remove_all_with_tags(self, tags):
        lower_tags = [_t.lower() for _t in tags]
        remove_items = []
        for series in self.api.all_series():
            tag_strs = [_t.label.lower() for _t in series.tags]
            remove = True
            for tag in lower_tags:
                if tag not in tag_strs:
                    remove = False
                    break
            if remove:
                remove_items.append(series)
        if remove_items:
            self.api.delete_multiple_series(remove_items)","for tag in lower_tags:
    if tag not in tag_strs:
        remove = False
        break
if remove:
    remove_items.append(series)","for tag in lower_tags:
    if tag not in tag_strs:
        break
else:
    remove_items.append(series)","for tag in lower_tags:
    if tag not in tag_strs:
        break
else:
    remove_items.append(series)",1,"for tag in lower_tags:
    if tag not in tag_strs:
        remove = False
        break
if remove:
    remove_items.append(series)","break statement is executed:None
break statement is not executed:zejun1"
blueman,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blueman/blueman/plugins/applet/NetUsage.py,https://github.com/blueman-project/blueman/tree/master/blueman/plugins/applet/NetUsage.py,Dialog,__init__$103,"def __init__(self, plugin: ""NetUsage""):
        if not Dialog.running:
            Dialog.running = True
        else:
            return
        self.plugin = plugin
        builder = Builder(""net-usage.ui"")

        self.dialog = builder.get_widget(""dialog"", Gtk.Dialog)
        self.dialog.connect(""response"", self.on_response)
        cr1 = Gtk.CellRendererText()
        cr1.props.ellipsize = Pango.EllipsizeMode.END

        self._handlerids: List[int] = []
        self._handlerids.append(plugin.connect(""monitor-added"", self.monitor_added))
        self._handlerids.append(plugin.connect(""monitor-removed"", self.monitor_removed))
        self._handlerids.append(plugin.connect(""stats"", self.on_stats))

        cr2 = Gtk.CellRendererText()
        cr2.props.sensitive = False
        cr2.props.style = Pango.Style.ITALIC

        self.liststore = Gtk.ListStore(str, str, str, object)

        self.e_ul = builder.get_widget(""e_ul"", Gtk.Entry)
        self.e_dl = builder.get_widget(""e_dl"", Gtk.Entry)
        self.e_total = builder.get_widget(""e_total"", Gtk.Entry)

        self.l_started = builder.get_widget(""l_started"", Gtk.Label)
        self.l_duration = builder.get_widget(""l_duration"", Gtk.Label)

        self.b_reset = builder.get_widget(""b_reset"", Gtk.Button)
        self.b_reset.connect(""clicked"", self.on_reset)

        self.cb_device = builder.get_widget(""cb_device"", Gtk.ComboBox)
        self.cb_device.props.model = self.liststore
        self.cb_device.connect(""changed"", self.on_selection_changed)

        self.cb_device.pack_start(cr1, True)
        self.cb_device.add_attribute(cr1, 'markup', 1)

        self.cb_device.pack_start(cr2, False)
        self.cb_device.add_attribute(cr2, 'markup', 2)

        general_config = Gio.Settings(schema_id=""org.blueman.general"")

        added = False
        for d in general_config[""netusage-dev-list""]:
            for m in plugin.monitors:
                if d == m.device[""Address""]:
                    titer = self.liststore.append(
                        [d, self.get_caption(m.device.display_name, m.device[""Address""]),
                         _(""Connected:"") + "" "" + m.interface, m])
                    if self.cb_device.get_active() == -1:
                        self.cb_device.set_active_iter(titer)
                    added = True
                    break
            if not added:
                name = d
                if self.plugin.parent.Manager:
                    device = self.plugin.parent.Manager.find_device(d)
                    if device is None:
                        pass
                    else:
                        name = self.get_caption(device.display_name, device[""Address""])

                self.liststore.append([d, name, _(""Not Connected""), None])
            added = False
        if len(self.liststore) > 0:
            if self.cb_device.get_active() == -1:
                self.cb_device.set_active(0)
        else:
            msg = _(""No usage statistics are available yet. Try establishing a connection first and ""
                    ""then check this page."")
            d = Gtk.MessageDialog(parent=self.dialog, modal=True, type=Gtk.MessageType.INFO,
                                  buttons=Gtk.ButtonsType.CLOSE, text=msg)
            d.props.icon_name = ""blueman""
            d.run()
            d.destroy()
            self.on_response(None, None)
            return

        self.dialog.show()","for m in plugin.monitors:
    if d == m.device['Address']:
        titer = self.liststore.append([d, self.get_caption(m.device.display_name, m.device['Address']), _('Connected:') + ' ' + m.interface, m])
        if self.cb_device.get_active() == -1:
            self.cb_device.set_active_iter(titer)
        added = True
        break
if not added:
    name = d
    if self.plugin.parent.Manager:
        device = self.plugin.parent.Manager.find_device(d)
        if device is None:
            pass
        else:
            name = self.get_caption(device.display_name, device['Address'])
    self.liststore.append([d, name, _('Not Connected'), None])","for m in plugin.monitors:
    if d == m.device['Address']:
        titer = self.liststore.append([d, self.get_caption(m.device.display_name, m.device['Address']), _('Connected:') + ' ' + m.interface, m])
        if self.cb_device.get_active() == -1:
            self.cb_device.set_active_iter(titer)
        added = True
        break
else:
    name = d
    if self.plugin.parent.Manager:
        device = self.plugin.parent.Manager.find_device(d)
        if device is None:
            pass
        else:
            name = self.get_caption(device.display_name, device['Address'])
    self.liststore.append([d, name, _('Not Connected'), None])","for m in plugin.monitors:
    if d == m.device['Address']:
        titer = self.liststore.append([d, self.get_caption(m.device.display_name, m.device['Address']), _('Connected:') + ' ' + m.interface, m])
        if self.cb_device.get_active() == -1:
            self.cb_device.set_active_iter(titer)
        added = True
        break
else:
    name = d
    if self.plugin.parent.Manager:
        device = self.plugin.parent.Manager.find_device(d)
        if device is None:
            pass
        else:
            name = self.get_caption(device.display_name, device['Address'])
    self.liststore.append([d, name, _('Not Connected'), None])",1,"for m in plugin.monitors:
    if d == m.device['Address']:
        titer = self.liststore.append([d, self.get_caption(m.device.display_name, m.device['Address']), _('Connected:') + ' ' + m.interface, m])
        if self.cb_device.get_active() == -1:
            self.cb_device.set_active_iter(titer)
        added = True
        break
if not added:
    name = d
    if self.plugin.parent.Manager:
        device = self.plugin.parent.Manager.find_device(d)
        if device is None:
            pass
        else:
            name = self.get_caption(device.display_name, device['Address'])
    self.liststore.append([d, name, _('Not Connected'), None])","break statement is executed:None
break statement is not executed:zejun1"
MB-Lab,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MB-Lab/humanoid.py,https://github.com/animate1978/MB-Lab/tree/master//humanoid.py,HumanCategory,get_modifier_tiny_name$142,"def get_modifier_tiny_name(self, sub_categories=[], exclude_in_others=[]):
        # Return the short name minus the beginning
        # of its name corresponding to sub_category
        # The key is subcategory name.
        # The value is [tiny, short, full]
        # Method used only for expressions editor for now
        # exclude_in_others means that the modifiers' name that are in this
        # list can't be put in ""other"" category
        if len(sub_categories) > 0:
            tiny = {'other': []}
            triple = []
            done = False
            false_others = False
            sub_categories = sorted(sub_categories, reverse = True)
            for modif in self.modifiers:
                for sub in sub_categories:
                    if not sub in tiny:
                        tiny[sub] = []
                    if modif.short_name.startswith(sub):
                        triple = [modif.short_name.lstrip(sub), modif.short_name, modif.name]
                        tiny[sub].append(triple)
                        done = True
                        break
                if done:
                    done = False
                else:
                    for fo in exclude_in_others:
                        if modif.short_name.startswith(fo) or modif.short_name.startswith(""ID""):
                            false_others = True
                    if false_others:
                        false_others = False
                    else:
                        tiny['other'].append([modif.short_name, modif.short_name, modif.name])
            return tiny
        return {}","for sub in sub_categories:
    if not sub in tiny:
        tiny[sub] = []
    if modif.short_name.startswith(sub):
        triple = [modif.short_name.lstrip(sub), modif.short_name, modif.name]
        tiny[sub].append(triple)
        done = True
        break
if done:
    done = False
else:
    for fo in exclude_in_others:
        if modif.short_name.startswith(fo) or modif.short_name.startswith('ID'):
            false_others = True
    if false_others:
        false_others = False
    else:
        tiny['other'].append([modif.short_name, modif.short_name, modif.name])","for sub in sub_categories:
    if not sub in tiny:
        tiny[sub] = []
    if modif.short_name.startswith(sub):
        triple = [modif.short_name.lstrip(sub), modif.short_name, modif.name]
        tiny[sub].append(triple)
        done = True
        done = False
        break
else:
    for fo in exclude_in_others:
        if modif.short_name.startswith(fo) or modif.short_name.startswith('ID'):
            false_others = True
    if false_others:
        false_others = False
    else:
        tiny['other'].append([modif.short_name, modif.short_name, modif.name])","for sub in sub_categories:
    if not sub in tiny:
        tiny[sub] = []
    if modif.short_name.startswith(sub):
        triple = [modif.short_name.lstrip(sub), modif.short_name, modif.name]
        tiny[sub].append(triple)
        done = True
        done = False
        break
else:
    for fo in exclude_in_others:
        if modif.short_name.startswith(fo) or modif.short_name.startswith('ID'):
            false_others = True
    if false_others:
        false_others = False
    else:
        tiny['other'].append([modif.short_name, modif.short_name, modif.name])",1,"for sub in sub_categories:
    if not sub in tiny:
        tiny[sub] = []
    if modif.short_name.startswith(sub):
        triple = [modif.short_name.lstrip(sub), modif.short_name, modif.name]
        tiny[sub].append(triple)
        done = True
        break
if done:
    done = False
else:
    for fo in exclude_in_others:
        if modif.short_name.startswith(fo) or modif.short_name.startswith('ID'):
            false_others = True
    if false_others:
        false_others = False
    else:
        tiny['other'].append([modif.short_name, modif.short_name, modif.name])","break statement is executed:zejun1
break statement is not executed:None"
UNetPlusPlus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UNetPlusPlus/pytorch/nnunet/training/network_training/network_trainer.py,https://github.com/MrGiovanni/UNetPlusPlus/tree/master/pytorch/nnunet/training/network_training/network_trainer.py,NetworkTrainer,load_pretrained_weights$316,"def load_pretrained_weights(self,fname):                                    
        saved_model = torch.load(fname)                                         
        pretrained_dict = saved_model['state_dict']                             
        model_dict = self.network.state_dict()                                  
        fine_tune = True
        for key, _ in model_dict.items():
           if ('conv_blocks' in key):
               if (key in pretrained_dict) and (model_dict[key].shape == pretrained_dict[key].shape):
                   continue
               else:
                   fine_tune = False
                   break
        # filter unnecessary keys
        if fine_tune:
            pretrained_dict = {k: v for k, v in pretrained_dict.items() if
                           (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}
            # 2. overwrite entries in the existing state dict                       
            model_dict.update(pretrained_dict)
            # print(model_dict)                                                     
            print(""############################################### Loading pre-trained Models Genesis from "",fname)
            print(""Below is the list of overlapping blocks in pre-trained Models Genesis and nnUNet architecture:"")
            for key, _ in pretrained_dict.items():
                print(key)
            print(""############################################### Done"")
            for key, _ in model_dict.items():
                print(key)
            self.network.load_state_dict(model_dict)
        else:
            print('############################################### Training from scratch')","for (key, _) in model_dict.items():
    if 'conv_blocks' in key:
        if key in pretrained_dict and model_dict[key].shape == pretrained_dict[key].shape:
            continue
        else:
            fine_tune = False
            break
if fine_tune:
    pretrained_dict = {k: v for (k, v) in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}
    model_dict.update(pretrained_dict)
    print('############################################### Loading pre-trained Models Genesis from ', fname)
    print('Below is the list of overlapping blocks in pre-trained Models Genesis and nnUNet architecture:')
    for (key, _) in pretrained_dict.items():
        print(key)
    print('############################################### Done')
    for (key, _) in model_dict.items():
        print(key)
    self.network.load_state_dict(model_dict)
else:
    print('############################################### Training from scratch')","for (key, _) in model_dict.items():
    if 'conv_blocks' in key:
        if key in pretrained_dict and model_dict[key].shape == pretrained_dict[key].shape:
            continue
        else:
            print('############################################### Training from scratch')
            break
else:
    pretrained_dict = {k: v for (k, v) in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}
    model_dict.update(pretrained_dict)
    print('############################################### Loading pre-trained Models Genesis from ', fname)
    print('Below is the list of overlapping blocks in pre-trained Models Genesis and nnUNet architecture:')
    for (key, _) in pretrained_dict.items():
        print(key)
    print('############################################### Done')
    for (key, _) in model_dict.items():
        print(key)
    self.network.load_state_dict(model_dict)","for (key, _) in model_dict.items():
    if 'conv_blocks' in key:
        if key in pretrained_dict and model_dict[key].shape == pretrained_dict[key].shape:
            print('############################################### Training from scratch')
            continue
        else:
            break
else:
    pretrained_dict = {k: v for (k, v) in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}
    model_dict.update(pretrained_dict)
    print('############################################### Loading pre-trained Models Genesis from ', fname)
    print('Below is the list of overlapping blocks in pre-trained Models Genesis and nnUNet architecture:')
    for (key, _) in pretrained_dict.items():
        print(key)
    print('############################################### Done')
    for (key, _) in model_dict.items():
        print(key)
    self.network.load_state_dict(model_dict)",0,"for (key, _) in model_dict.items():
    if 'conv_blocks' in key:
        if key in pretrained_dict and model_dict[key].shape == pretrained_dict[key].shape:
            continue
        else:
            fine_tune = False
            break
if fine_tune:
    pretrained_dict = {k: v for (k, v) in pretrained_dict.items() if k in model_dict and model_dict[k].shape == pretrained_dict[k].shape}
    model_dict.update(pretrained_dict)
    print('############################################### Loading pre-trained Models Genesis from ', fname)
    print('Below is the list of overlapping blocks in pre-trained Models Genesis and nnUNet architecture:')
    for (key, _) in pretrained_dict.items():
        print(key)
    print('############################################### Done')
    for (key, _) in model_dict.items():
        print(key)
    self.network.load_state_dict(model_dict)
else:
    print('############################################### Training from scratch')","break statement is executed:None
break statement is not executed:zejun1"
Axelrod,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Axelrod/axelrod/_strategy_utils.py,https://github.com/Axelrod-Python/Axelrod/tree/master/axelrod/_strategy_utils.py,,detect_cycle$13,"def detect_cycle(history, min_size=1, max_size=12, offset=0):
    """"""Detects cycles in the sequence history.

    Mainly used by hunter strategies.

    Parameters
    ----------
    history: sequence of C and D
        The sequence to look for cycles within
    min_size: int, 1
        The minimum length of the cycle
    max_size: int, 12
        The maximum length of the cycle
    offset: int, 0
        The amount of history to skip initially

    Returns
    -------
    Tuple of C and D
        The cycle detected in the input history
    """"""
    history_tail = history[offset:]
    new_max_size = min(len(history_tail) // 2, max_size)
    for i in range(min_size, new_max_size + 1):
        has_cycle = True
        cycle = tuple(history_tail[:i])
        for j, elem in enumerate(history_tail):
            if elem != cycle[j % len(cycle)]:
                has_cycle = False
                break
        if has_cycle:
            return cycle
    return None","for (j, elem) in enumerate(history_tail):
    if elem != cycle[j % len(cycle)]:
        has_cycle = False
        break
if has_cycle:
    return cycle","for (j, elem) in enumerate(history_tail):
    if elem != cycle[j % len(cycle)]:
        break
else:
    return cycle","for (j, elem) in enumerate(history_tail):
    if elem != cycle[j % len(cycle)]:
        break
else:
    return cycle",1,"for (j, elem) in enumerate(history_tail):
    if elem != cycle[j % len(cycle)]:
        has_cycle = False
        break
if has_cycle:
    return cycle","break statement is executed:None
break statement is not executed:zejun1"
freeipa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/install/ipa_cacert_manage.py,https://github.com/freeipa/freeipa/tree/master/ipaserver/install/ipa_cacert_manage.py,CACertManage,_delete_by_nickname$483,"def _delete_by_nickname(self, nicknames, options):
        conn = api.Backend.ldap2

        ca_certs = certstore.get_ca_certs_nss(api.Backend.ldap2,
                                              api.env.basedn,
                                              api.env.realm,
                                              False)

        ipa_ca_nickname = get_ca_nickname(api.env.realm)

        for nickname in nicknames:
            found = False
            for _ca_cert, ca_nickname, _ca_trust_flags in ca_certs:
                if ca_nickname == nickname:
                    if ca_nickname == ipa_ca_nickname:
                        raise admintool.ScriptError(
                            'The IPA CA cannot be removed with this tool'
                        )
                    else:
                        found = True
                        break

            if not found:
                raise admintool.ScriptError(
                    'Unknown CA \'{}\''.format(nickname)
                )

        with certs.NSSDatabase() as tmpdb:
            tmpdb.create_db()
            for ca_cert, ca_nickname, ca_trust_flags in ca_certs:
                tmpdb.add_cert(ca_cert, ca_nickname, ca_trust_flags)
            loaded = tmpdb.list_certs()
            logger.debug(""loaded raw certs '%s'"", loaded)

            for nickname in nicknames:
                tmpdb.delete_cert(nickname)

            for ca_nickname, _trust_flags in loaded:
                if ca_nickname in nicknames:
                    continue
                if ipa_ca_nickname in nicknames:
                    raise admintool.ScriptError(
                        ""The IPA CA cannot be removed"")
                logger.debug(""Verifying %s"", ca_nickname)
                try:
                    tmpdb.verify_ca_cert_validity(ca_nickname)
                except ValueError as e:
                    msg = ""Verifying \'%s\' failed. Removing part of the "" \
                          ""chain? %s"" % (nickname, e)
                    if options.force:
                        print(msg)
                        continue
                    raise admintool.ScriptError(msg)
                else:
                    logger.debug(""Verified %s"", ca_nickname)

        for _ca_cert, ca_nickname, _ca_trust_flags in ca_certs:
            if ca_nickname in nicknames:
                container_dn = DN(('cn', 'certificates'), ('cn', 'ipa'),
                                  ('cn', 'etc'), api.env.basedn)
                dn = DN(('cn', nickname), container_dn)
                logger.debug(""Deleting %s"", ca_nickname)
                conn.delete_entry(dn)
                return","for (_ca_cert, ca_nickname, _ca_trust_flags) in ca_certs:
    if ca_nickname == nickname:
        if ca_nickname == ipa_ca_nickname:
            raise admintool.ScriptError('The IPA CA cannot be removed with this tool')
        else:
            found = True
            break
if not found:
    raise admintool.ScriptError(""Unknown CA '{}'"".format(nickname))","for (_ca_cert, ca_nickname, _ca_trust_flags) in ca_certs:
    if ca_nickname == nickname:
        if ca_nickname == ipa_ca_nickname:
            raise admintool.ScriptError('The IPA CA cannot be removed with this tool')
        else:
            break
else:
    raise admintool.ScriptError(""Unknown CA '{}'"".format(nickname))","for (_ca_cert, ca_nickname, _ca_trust_flags) in ca_certs:
    if ca_nickname == nickname:
        if ca_nickname == ipa_ca_nickname:
            raise admintool.ScriptError('The IPA CA cannot be removed with this tool')
        else:
            break
else:
    raise admintool.ScriptError(""Unknown CA '{}'"".format(nickname))",1,"for (_ca_cert, ca_nickname, _ca_trust_flags) in ca_certs:
    if ca_nickname == nickname:
        if ca_nickname == ipa_ca_nickname:
            raise admintool.ScriptError('The IPA CA cannot be removed with this tool')
        else:
            found = True
            break
if not found:
    raise admintool.ScriptError(""Unknown CA '{}'"".format(nickname))","break statement is executed:None
break statement is not executed:zejun1"
dm_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dm_control/dm_control/composer/initializers/prop_initializer.py,https://github.com/deepmind/dm_control/tree/master/dm_control/composer/initializers/prop_initializer.py,PropPlacer,__call__$159,"def __call__(self, physics, random_state, ignore_contacts_with_entities=None):
    """"""Sets initial prop poses.

    Args:
      physics: An `mjcf.Physics` instance.
      random_state: a `np.random.RandomState` instance.
      ignore_contacts_with_entities: a list of `composer.Entity` instances
        to ignore when detecting collisions. This can be used to ignore props
        that are not being placed by this initializer, but are known to be
        colliding in the current state of the simulation (for example if they
        are going to be placed by a different initializer that will be called
        subsequently).

    Raises:
      RuntimeError: If `ignore_collisions == False` and a non-colliding prop
        pose could not be found within `max_attempts_per_prop`.
    """"""
    if ignore_contacts_with_entities is None:
      ignore_contacts_with_entities = []
    # Temporarily disable contacts for all geoms that belong to props which
    # haven't yet been placed in order to free up space in the contact buffer.
    cached_contact_params = self._disable_and_cache_contact_parameters(
        physics, self._props + ignore_contacts_with_entities)

    try:
      physics.forward()
    except control.PhysicsError as cause:
      effect = control.PhysicsError(
          'Despite disabling contact for all props in this initializer, '
          '`physics.forward()` resulted in a `PhysicsError`')
      raise effect from cause

    def place_props():
      for prop in self._props:
        # Restore the original contact parameters for all geoms in the prop
        # we're about to place, so that we can detect if the new pose results in
        # collisions.
        self._restore_contact_parameters(physics, prop, cached_contact_params)

        success = False
        initial_position, initial_quaternion = prop.get_pose(physics)
        next_position, next_quaternion = initial_position, initial_quaternion
        for _ in range(self._max_attempts_per_prop):
          next_position = variation.evaluate(self._position,
                                             initial_value=initial_position,
                                             current_value=next_position,
                                             random_state=random_state)
          next_quaternion = variation.evaluate(self._quaternion,
                                               initial_value=initial_quaternion,
                                               current_value=next_quaternion,
                                               random_state=random_state)
          prop.set_pose(physics, next_position, next_quaternion)
          try:
            # If this pose results in collisions then there's a chance we'll
            # encounter a PhysicsError error here due to a full contact buffer,
            # in which case reject this pose and sample another.
            physics.forward()
          except control.PhysicsError:
            continue

          if (self._ignore_collisions
              or not self._has_collisions_with_prop(physics, prop)):
            success = True
            break

        if not success:
          raise RuntimeError(_REJECTION_SAMPLING_FAILED.format(
              model_name=prop.mjcf_model.model,
              max_attempts=self._max_attempts_per_prop))

      for prop in ignore_contacts_with_entities:
        self._restore_contact_parameters(physics, prop, cached_contact_params)

    # Place the props and settle the physics. If settling was requested and it
    # it fails, re-place the props.
    def place_and_settle():
      for _ in range(self._max_settle_physics_attempts):
        place_props()

        # Step physics and check prop states.
        original_time = physics.data.time
        props_isolator = utils.JointStaticIsolator(physics, self._prop_joints)
        prop_joints_mj = physics.bind(self._prop_joints)
        while physics.data.time - original_time < self._max_settle_physics_time:
          with props_isolator:
            physics.step()
          max_qvel = np.max(np.abs(prop_joints_mj.qvel))
          max_qacc = np.max(np.abs(prop_joints_mj.qacc))
          if (max_qvel < self._max_qvel_tol) and (
              max_qacc < self._max_qacc_tol) and (
                  physics.data.time - original_time
                  ) > self._min_settle_physics_time:
            return True
        physics.data.time = original_time

      if self._raise_exception_on_settle_failure:
        raise RuntimeError(
            _SETTLING_PHYSICS_FAILED.format(
                max_attempts=self._max_settle_physics_attempts,
                max_time=self._max_settle_physics_time,
                max_qvel=max_qvel,
                max_qacc=max_qacc,
            ))
      else:
        log_str = _SETTLING_PHYSICS_FAILED.format(
            max_attempts='%s',
            max_time='%s',
            max_qvel='%s',
            max_qacc='%s',
        )
        logging.warning(log_str, self._max_settle_physics_attempts,
                        self._max_settle_physics_time, max_qvel, max_qacc)

      return False

    if self._settle_physics:
      place_and_settle()
    else:
      place_props()","for _ in range(self._max_attempts_per_prop):
    next_position = variation.evaluate(self._position, initial_value=initial_position, current_value=next_position, random_state=random_state)
    next_quaternion = variation.evaluate(self._quaternion, initial_value=initial_quaternion, current_value=next_quaternion, random_state=random_state)
    prop.set_pose(physics, next_position, next_quaternion)
    try:
        physics.forward()
    except control.PhysicsError:
        continue
    if self._ignore_collisions or not self._has_collisions_with_prop(physics, prop):
        success = True
        break
if not success:
    raise RuntimeError(_REJECTION_SAMPLING_FAILED.format(model_name=prop.mjcf_model.model, max_attempts=self._max_attempts_per_prop))","for _ in range(self._max_attempts_per_prop):
    next_position = variation.evaluate(self._position, initial_value=initial_position, current_value=next_position, random_state=random_state)
    next_quaternion = variation.evaluate(self._quaternion, initial_value=initial_quaternion, current_value=next_quaternion, random_state=random_state)
    prop.set_pose(physics, next_position, next_quaternion)
    try:
        physics.forward()
    except control.PhysicsError:
        continue
    if self._ignore_collisions or not self._has_collisions_with_prop(physics, prop):
        break
else:
    raise RuntimeError(_REJECTION_SAMPLING_FAILED.format(model_name=prop.mjcf_model.model, max_attempts=self._max_attempts_per_prop))","for _ in range(self._max_attempts_per_prop):
    next_position = variation.evaluate(self._position, initial_value=initial_position, current_value=next_position, random_state=random_state)
    next_quaternion = variation.evaluate(self._quaternion, initial_value=initial_quaternion, current_value=next_quaternion, random_state=random_state)
    prop.set_pose(physics, next_position, next_quaternion)
    try:
        physics.forward()
    except control.PhysicsError:
        continue
    if self._ignore_collisions or not self._has_collisions_with_prop(physics, prop):
        break
else:
    raise RuntimeError(_REJECTION_SAMPLING_FAILED.format(model_name=prop.mjcf_model.model, max_attempts=self._max_attempts_per_prop))",1,"for _ in range(self._max_attempts_per_prop):
    next_position = variation.evaluate(self._position, initial_value=initial_position, current_value=next_position, random_state=random_state)
    next_quaternion = variation.evaluate(self._quaternion, initial_value=initial_quaternion, current_value=next_quaternion, random_state=random_state)
    prop.set_pose(physics, next_position, next_quaternion)
    try:
        physics.forward()
    except control.PhysicsError:
        continue
    if self._ignore_collisions or not self._has_collisions_with_prop(physics, prop):
        success = True
        break
if not success:
    raise RuntimeError(_REJECTION_SAMPLING_FAILED.format(model_name=prop.mjcf_model.model, max_attempts=self._max_attempts_per_prop))","break statement is executed:None
break statement is not executed:zejun1"
samsungctl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/samsungctl/samsungctl/__main__.py,https://github.com/Ape/samsungctl/tree/master/samsungctl/__main__.py,,_read_config$16,"def _read_config():
    config = collections.defaultdict(lambda: None, {
        ""name"": ""samsungctl"",
        ""description"": ""PC"",
        ""id"": """",
        ""method"": ""legacy"",
        ""timeout"": 0,
    })

    file_loaded = False
    directories = []

    xdg_config = os.getenv(""XDG_CONFIG_HOME"")
    if xdg_config:
        directories.append(xdg_config)

    directories.append(os.path.join(os.getenv(""HOME""), "".config""))
    directories.append(""/etc"")

    for directory in directories:
        path = os.path.join(directory, ""samsungctl.conf"")
        try:
            config_file = open(path)
        except IOError as e:
            if e.errno == errno.ENOENT:
                continue
            else:
                raise
        else:
            file_loaded = True
            break

    if not file_loaded:
        return config

    with config_file:
        try:
            config_json = json.load(config_file)
        except ValueError as e:
            message = ""Warning: Could not parse the configuration file.\n  %s""
            logging.warning(message, e)
            return config

        config.update(config_json)

    return config","for directory in directories:
    path = os.path.join(directory, 'samsungctl.conf')
    try:
        config_file = open(path)
    except IOError as e:
        if e.errno == errno.ENOENT:
            continue
        else:
            raise
    else:
        file_loaded = True
        break
if not file_loaded:
    return config","for directory in directories:
    path = os.path.join(directory, 'samsungctl.conf')
    try:
        config_file = open(path)
    except IOError as e:
        if e.errno == errno.ENOENT:
            continue
        else:
            raise
    else:
        break
else:
    return config","for directory in directories:
    path = os.path.join(directory, 'samsungctl.conf')
    try:
        config_file = open(path)
    except IOError as e:
        if e.errno == errno.ENOENT:
            continue
        else:
            raise
    else:
        break
else:
    return config",1,"for directory in directories:
    path = os.path.join(directory, 'samsungctl.conf')
    try:
        config_file = open(path)
    except IOError as e:
        if e.errno == errno.ENOENT:
            continue
        else:
            raise
    else:
        file_loaded = True
        break
if not file_loaded:
    return config","break statement is executed:None
break statement is not executed:zejun1"
ansible-modules-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-core/cloud/openstack/os_router.py,https://github.com/ansible/ansible-modules-core/tree/master/cloud/openstack/os_router.py,,_needs_update$182,"def _needs_update(cloud, module, router, network, internal_subnet_ids):
    """"""Decide if the given router needs an update.
    """"""
    if router['admin_state_up'] != module.params['admin_state_up']:
        return True
    if router['external_gateway_info']:
        if router['external_gateway_info'].get('enable_snat', True) != module.params['enable_snat']:
            return True
    if network:
        if not router['external_gateway_info']:
            return True
        elif router['external_gateway_info']['network_id'] != network['id']:
            return True

    # check external interfaces
    if module.params['external_fixed_ips']:
        for new_iface in module.params['external_fixed_ips']:
            subnet = cloud.get_subnet(new_iface['subnet'])
            exists = False

            # compare the requested interface with existing, looking for an existing match
            for existing_iface in router['external_gateway_info']['external_fixed_ips']:
                if existing_iface['subnet_id'] == subnet['id']:
                    if 'ip' in new_iface:
                        if existing_iface['ip_address'] == new_iface['ip']:
                            # both subnet id and ip address match
                            exists = True
                            break
                    else:
                        # only the subnet was given, so ip doesn't matter
                        exists = True
                        break

            # this interface isn't present on the existing router
            if not exists:
                return True

    # check internal interfaces
    if module.params['interfaces']:
        existing_subnet_ids = []
        for port in cloud.list_router_interfaces(router, 'internal'):
            if 'fixed_ips' in port:
                for fixed_ip in port['fixed_ips']:
                    existing_subnet_ids.append(fixed_ip['subnet_id'])

        if set(internal_subnet_ids) != set(existing_subnet_ids):
            return True

    return False","for existing_iface in router['external_gateway_info']['external_fixed_ips']:
    if existing_iface['subnet_id'] == subnet['id']:
        if 'ip' in new_iface:
            if existing_iface['ip_address'] == new_iface['ip']:
                exists = True
                break
        else:
            exists = True
            break
if not exists:
    return True","for existing_iface in router['external_gateway_info']['external_fixed_ips']:
    if existing_iface['subnet_id'] == subnet['id']:
        if 'ip' in new_iface:
            if existing_iface['ip_address'] == new_iface['ip']:
                break
        else:
            break
else:
    return True","for existing_iface in router['external_gateway_info']['external_fixed_ips']:
    if existing_iface['subnet_id'] == subnet['id']:
        if 'ip' in new_iface:
            if existing_iface['ip_address'] == new_iface['ip']:
                break
        else:
            break
else:
    return True",1,"for existing_iface in router['external_gateway_info']['external_fixed_ips']:
    if existing_iface['subnet_id'] == subnet['id']:
        if 'ip' in new_iface:
            if existing_iface['ip_address'] == new_iface['ip']:
                exists = True
                break
        else:
            exists = True
            break
if not exists:
    return True","break statement is executed:None
break statement is not executed:zejun1"
TauonMusicBox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TauonMusicBox/t_modules/t_main.py,https://github.com/Taiko2k/TauonMusicBox/tree/master/t_modules/t_main.py,,get_lyric_fire$15350,"def get_lyric_fire(track_object, silent=False):
    lyrics_ren.lyrics_position = 0

    if not prefs.lyrics_enables:
        if not silent:
            show_message(_(""There are no lyric sources enabled.""),
                         ""See 'lyrics settings' under 'functions' tab in settings."", mode='info')
        return

    t = lyrics_fetch_timer.get()
    print(""Lyric rate limit timer is: "" + str(t) + "" / -60"")
    if t < -40:
        print(""Lets try again later"")
        if not silent:
            show_message(_(""Let's be polite and try later.""))

            if t < -65:
                show_message(""Stop requesting lyrics AAAAAA."", mode='error')

        # If the user keeps pressing, lets mess with them haha
        lyrics_fetch_timer.force_set(t - 5)

        return 'later'

    if t > 0:
        lyrics_fetch_timer.set()
        t = 0

    lyrics_fetch_timer.force_set(t - 10)

    if not silent:
        show_message(_(""Searching...""))

    s_artist = track_object.artist
    s_title = track_object.title

    if s_artist in prefs.lyrics_subs:
        s_artist = prefs.lyrics_subs[s_artist]
    if s_title in prefs.lyrics_subs:
        s_title = prefs.lyrics_subs[s_title]

    console.print(f""Searching for lyrics: {s_artist} - {s_title}"", level=1)

    found = False
    for name in prefs.lyrics_enables:

        if name in lyric_sources.keys():
            func = lyric_sources[name]

            try:
                lyrics = func(s_artist, s_title)
                if lyrics:
                    console.print(f""Found lyrics from {name}"", level=1)
                    track_object.lyrics = lyrics
                    found = True
                    break
            except Exception as e:
                console.print(str(e))

            if not found:
                console.print(f""Could not find lyrics from source {name}"", level=1)

    if not found:
        if not silent:
            show_message(_(""No lyrics for this track were found""))
    else:
        gui.message_box = False
        if not gui.showcase_mode:
            prefs.show_lyrics_side = True
        gui.update += 1
        lyrics_ren.lyrics_position = 0
        pctl.notify_change()","for name in prefs.lyrics_enables:
    if name in lyric_sources.keys():
        func = lyric_sources[name]
        try:
            lyrics = func(s_artist, s_title)
            if lyrics:
                console.print(f'Found lyrics from {name}', level=1)
                track_object.lyrics = lyrics
                found = True
                break
        except Exception as e:
            console.print(str(e))
        if not found:
            console.print(f'Could not find lyrics from source {name}', level=1)
if not found:
    if not silent:
        show_message(_('No lyrics for this track were found'))
else:
    gui.message_box = False
    if not gui.showcase_mode:
        prefs.show_lyrics_side = True
    gui.update += 1
    lyrics_ren.lyrics_position = 0
    pctl.notify_change()","for name in prefs.lyrics_enables:
    if name in lyric_sources.keys():
        func = lyric_sources[name]
        try:
            lyrics = func(s_artist, s_title)
            if lyrics:
                console.print(f'Found lyrics from {name}', level=1)
                track_object.lyrics = lyrics
                gui.message_box = False
                if not gui.showcase_mode:
                    prefs.show_lyrics_side = True
                gui.update += 1
                lyrics_ren.lyrics_position = 0
                pctl.notify_change()
                break
        except Exception as e:
            console.print(str(e))
        if not found:
            console.print(f'Could not find lyrics from source {name}', level=1)
else:
    if not silent:
        show_message(_('No lyrics for this track were found'))","for name in prefs.lyrics_enables:
    if name in lyric_sources.keys():
        func = lyric_sources[name]
        try:
            lyrics = func(s_artist, s_title)
            if lyrics:
                console.print(f'Found lyrics from {name}', level=1)
                track_object.lyrics = lyrics
                found = True
                gui.message_box = False
                if not gui.showcase_mode:
                    prefs.show_lyrics_side = True
                gui.update += 1
                lyrics_ren.lyrics_position = 0
                pctl.notify_change()
                break
        except Exception as e:
            console.print(str(e))
        if not found:
            console.print(f'Could not find lyrics from source {name}', level=1)
else:
    if not silent:
        show_message(_('No lyrics for this track were found'))",0,"for name in prefs.lyrics_enables:
    if name in lyric_sources.keys():
        func = lyric_sources[name]
        try:
            lyrics = func(s_artist, s_title)
            if lyrics:
                console.print(f'Found lyrics from {name}', level=1)
                track_object.lyrics = lyrics
                found = True
                break
        except Exception as e:
            console.print(str(e))
        if not found:
            console.print(f'Could not find lyrics from source {name}', level=1)
if not found:
    if not silent:
        show_message(_('No lyrics for this track were found'))
else:
    gui.message_box = False
    if not gui.showcase_mode:
        prefs.show_lyrics_side = True
    gui.update += 1
    lyrics_ren.lyrics_position = 0
    pctl.notify_change()","break statement is executed:None
break statement is not executed:zejun1"
OpenWPM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenWPM/openwpm/browser_manager.py,https://github.com/openwpm/OpenWPM/tree/master/openwpm/browser_manager.py,BrowserManager,_start_extension$657,"def _start_extension(self, browser_profile_path: Path) -> ClientSocket:
        """"""Start up the extension
        Blocks until the extension has fully started up
        """"""
        assert self.browser_params.browser_id is not None
        self.logger.debug(
            ""BROWSER %i: Looking for extension port information ""
            ""in %s"" % (self.browser_params.browser_id, browser_profile_path)
        )
        elapsed = 0.0
        port = None
        ep_filename = browser_profile_path / ""extension_port.txt""
        while elapsed < 5:
            try:
                with open(ep_filename, ""rt"") as f:
                    port = int(f.read().strip())
                    break
            except IOError as e:
                if e.errno != errno.ENOENT:
                    raise
            time.sleep(0.1)
            elapsed += 0.1
        if port is None:
            # try one last time, allowing all exceptions to propagate
            with open(ep_filename, ""rt"") as f:
                port = int(f.read().strip())

        ep_filename.unlink()
        self.logger.debug(
            ""BROWSER %i: Connecting to extension on port %i""
            % (self.browser_params.browser_id, port)
        )
        extension_socket = ClientSocket(serialization=""json"")
        extension_socket.connect(""127.0.0.1"", int(port))

        success_filename = browser_profile_path / ""OPENWPM_STARTUP_SUCCESS.txt""
        startup_successful = False
        while elapsed < 10:
            if success_filename.exists():
                startup_successful = True
                break
            time.sleep(0.1)
            elapsed += 0.1

        if not startup_successful:
            self.logger.error(
                ""BROWSER %i: Failed to complete extension startup in time"",
                self.browser_params.browser_id,
            )
            raise BrowserConfigError(""The extension did not boot up in time"")
        success_filename.unlink()
        return extension_socket","while elapsed < 5:
    try:
        with open(ep_filename, 'rt') as f:
            port = int(f.read().strip())
            break
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
    time.sleep(0.1)
    elapsed += 0.1
if port is None:
    with open(ep_filename, 'rt') as f:
        port = int(f.read().strip())","while elapsed < 5:
    try:
        with open(ep_filename, 'rt') as f:
            port = int(f.read().strip())
            break
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
    time.sleep(0.1)
    elapsed += 0.1
else:
    with open(ep_filename, 'rt') as f:
        port = int(f.read().strip())",Cannot refactor,-1,"while elapsed < 5:
    try:
        with open(ep_filename, 'rt') as f:
            port = int(f.read().strip())
            break
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
    time.sleep(0.1)
    elapsed += 0.1
if port is None:
    with open(ep_filename, 'rt') as f:
        port = int(f.read().strip())","break statement is executed:None
break statement is not executed:zejun1"
OpenWPM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenWPM/openwpm/browser_manager.py,https://github.com/openwpm/OpenWPM/tree/master/openwpm/browser_manager.py,BrowserManager,_start_extension$657,"def _start_extension(self, browser_profile_path: Path) -> ClientSocket:
        """"""Start up the extension
        Blocks until the extension has fully started up
        """"""
        assert self.browser_params.browser_id is not None
        self.logger.debug(
            ""BROWSER %i: Looking for extension port information ""
            ""in %s"" % (self.browser_params.browser_id, browser_profile_path)
        )
        elapsed = 0.0
        port = None
        ep_filename = browser_profile_path / ""extension_port.txt""
        while elapsed < 5:
            try:
                with open(ep_filename, ""rt"") as f:
                    port = int(f.read().strip())
                    break
            except IOError as e:
                if e.errno != errno.ENOENT:
                    raise
            time.sleep(0.1)
            elapsed += 0.1
        if port is None:
            # try one last time, allowing all exceptions to propagate
            with open(ep_filename, ""rt"") as f:
                port = int(f.read().strip())

        ep_filename.unlink()
        self.logger.debug(
            ""BROWSER %i: Connecting to extension on port %i""
            % (self.browser_params.browser_id, port)
        )
        extension_socket = ClientSocket(serialization=""json"")
        extension_socket.connect(""127.0.0.1"", int(port))

        success_filename = browser_profile_path / ""OPENWPM_STARTUP_SUCCESS.txt""
        startup_successful = False
        while elapsed < 10:
            if success_filename.exists():
                startup_successful = True
                break
            time.sleep(0.1)
            elapsed += 0.1

        if not startup_successful:
            self.logger.error(
                ""BROWSER %i: Failed to complete extension startup in time"",
                self.browser_params.browser_id,
            )
            raise BrowserConfigError(""The extension did not boot up in time"")
        success_filename.unlink()
        return extension_socket","while elapsed < 10:
    if success_filename.exists():
        startup_successful = True
        break
    time.sleep(0.1)
    elapsed += 0.1
if not startup_successful:
    self.logger.error('BROWSER %i: Failed to complete extension startup in time', self.browser_params.browser_id)
    raise BrowserConfigError('The extension did not boot up in time')","while elapsed < 10:
    if success_filename.exists():
        break
    time.sleep(0.1)
    elapsed += 0.1
else:
    self.logger.error('BROWSER %i: Failed to complete extension startup in time', self.browser_params.browser_id)
    raise BrowserConfigError('The extension did not boot up in time')","while elapsed < 10:
    if success_filename.exists():
        break
    time.sleep(0.1)
    elapsed += 0.1
else:
    self.logger.error('BROWSER %i: Failed to complete extension startup in time', self.browser_params.browser_id)
    raise BrowserConfigError('The extension did not boot up in time')",1,"while elapsed < 10:
    if success_filename.exists():
        startup_successful = True
        break
    time.sleep(0.1)
    elapsed += 0.1
if not startup_successful:
    self.logger.error('BROWSER %i: Failed to complete extension startup in time', self.browser_params.browser_id)
    raise BrowserConfigError('The extension did not boot up in time')","break statement is executed:None
break statement is not executed:zejun1"
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/core/doctype/doctype/doctype.py,https://github.com/frappe/frappe/tree/master/frappe/core/doctype/doctype/doctype.py,,check_level_zero_is_set$1555,"def check_level_zero_is_set(d):
		if cint(d.permlevel) > 0 and d.role != ""All"":
			has_zero_perm = False
			for p in permissions:
				if p.role == d.role and (p.permlevel or 0) == 0 and p != d:
					has_zero_perm = True
					break

			if not has_zero_perm:
				frappe.throw(
					_(""{0}: Permission at level 0 must be set before higher levels are set"").format(get_txt(d))
				)

			for invalid in (""create"", ""submit"", ""cancel"", ""amend""):
				if d.get(invalid):
					d.set(invalid, 0)","for p in permissions:
    if p.role == d.role and (p.permlevel or 0) == 0 and (p != d):
        has_zero_perm = True
        break
if not has_zero_perm:
    frappe.throw(_('{0}: Permission at level 0 must be set before higher levels are set').format(get_txt(d)))","for p in permissions:
    if p.role == d.role and (p.permlevel or 0) == 0 and (p != d):
        break
else:
    frappe.throw(_('{0}: Permission at level 0 must be set before higher levels are set').format(get_txt(d)))","for p in permissions:
    if p.role == d.role and (p.permlevel or 0) == 0 and (p != d):
        break
else:
    frappe.throw(_('{0}: Permission at level 0 must be set before higher levels are set').format(get_txt(d)))",1,"for p in permissions:
    if p.role == d.role and (p.permlevel or 0) == 0 and (p != d):
        has_zero_perm = True
        break
if not has_zero_perm:
    frappe.throw(_('{0}: Permission at level 0 must be set before higher levels are set').format(get_txt(d)))","break statement is executed:None
break statement is not executed:zejun1"
keystone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/catalog/core.py,https://github.com/openstack/keystone/tree/master/keystone/catalog/core.py,Manager,get_endpoints_filtered_by_endpoint_group$279,"def get_endpoints_filtered_by_endpoint_group(self, endpoint_group_id):
        endpoints = self.list_endpoints()
        filters = self.get_endpoint_group(endpoint_group_id)['filters']
        filtered_endpoints = []

        for endpoint in endpoints:
            is_candidate = True
            for key, value in filters.items():
                if endpoint[key] != value:
                    is_candidate = False
                    break
            if is_candidate:
                filtered_endpoints.append(endpoint)
        return filtered_endpoints","for (key, value) in filters.items():
    if endpoint[key] != value:
        is_candidate = False
        break
if is_candidate:
    filtered_endpoints.append(endpoint)","for (key, value) in filters.items():
    if endpoint[key] != value:
        break
else:
    filtered_endpoints.append(endpoint)","for (key, value) in filters.items():
    if endpoint[key] != value:
        break
else:
    filtered_endpoints.append(endpoint)",1,"for (key, value) in filters.items():
    if endpoint[key] != value:
        is_candidate = False
        break
if is_candidate:
    filtered_endpoints.append(endpoint)","break statement is executed:None
break statement is not executed:zejun1"
shuup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/core/models/_product_shops.py,https://github.com/shuup/shuup/tree/master/shuup/core/models/_product_shops.py,ShopProduct,get_orderability_errors_for_variable_variation_parent$511,"def get_orderability_errors_for_variable_variation_parent(self, supplier, customer):
        from shuup.core.models import ProductVariationResult

        sellable = False
        for combo in self.product.get_all_available_combinations():
            res = ProductVariationResult.resolve(self.product, combo[""variable_to_value""])
            if not res:
                continue
            try:
                child_shop_product = res.get_shop_instance(self.shop)
            except ShopProduct.DoesNotExist:
                continue

            if child_shop_product.is_orderable(
                supplier=supplier,
                customer=customer,
                quantity=child_shop_product.minimum_purchase_quantity,
                allow_cache=False,
            ):
                sellable = True
                break
        if not sellable:
            yield ValidationError(_(""Product has no sellable children.""), code=""no_sellable_children"")","for combo in self.product.get_all_available_combinations():
    res = ProductVariationResult.resolve(self.product, combo['variable_to_value'])
    if not res:
        continue
    try:
        child_shop_product = res.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        sellable = True
        break
if not sellable:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')","for combo in self.product.get_all_available_combinations():
    res = ProductVariationResult.resolve(self.product, combo['variable_to_value'])
    if not res:
        continue
    try:
        child_shop_product = res.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        break
else:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')","for combo in self.product.get_all_available_combinations():
    res = ProductVariationResult.resolve(self.product, combo['variable_to_value'])
    if not res:
        continue
    try:
        child_shop_product = res.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        break
else:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')",1,"for combo in self.product.get_all_available_combinations():
    res = ProductVariationResult.resolve(self.product, combo['variable_to_value'])
    if not res:
        continue
    try:
        child_shop_product = res.get_shop_instance(self.shop)
    except ShopProduct.DoesNotExist:
        continue
    if child_shop_product.is_orderable(supplier=supplier, customer=customer, quantity=child_shop_product.minimum_purchase_quantity, allow_cache=False):
        sellable = True
        break
if not sellable:
    yield ValidationError(_('Product has no sellable children.'), code='no_sellable_children')","break statement is executed:None
break statement is not executed:zejun1"
policy_sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/policy_sentry/policy_sentry/shared/awsdocs.py,https://github.com/salesforce/policy_sentry/tree/master/policy_sentry/shared/awsdocs.py,,header_matches$30,"def header_matches(string, table):
    """"""checks if the string is found in the table header""""""
    headers = [chomp(str(x)).lower() for x in table.find_all(""th"")]
    match_found = False
    for header in headers:
        if string in header:
            match_found = True
            break
    if not match_found:
        return False
    return True","for header in headers:
    if string in header:
        match_found = True
        break
if not match_found:
    return False","for header in headers:
    if string in header:
        break
else:
    return False","for header in headers:
    if string in header:
        break
else:
    return False",1,"for header in headers:
    if string in header:
        match_found = True
        break
if not match_found:
    return False","break statement is executed:None
break statement is not executed:zejun1"
pyradio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyradio/pyradio/browser.py,https://github.com/coderholic/pyradio/tree/master/pyradio/browser.py,RadioBrowserSearchWindow,_focus_next$2650,"def _focus_next(self):
        # logger.error('DE focus next ==========================')
        new_focus = self._focus + 1
        if new_focus == len(self._widgets):
            new_focus = 0
        # logger.error('DE new_focus = {}'.format(new_focus))
        focus_ok = False
        for i in range(new_focus, len(self._widgets)):
            if self._widgets[i].enabled:
                new_focus = i
                focus_ok = True
                # logger.error('DE 1 new_focus = {}'.format(new_focus))
                break
        if not focus_ok:
            for i in range(0, new_focus + 1):
                if self._widgets[i].enabled:
                    new_focus = i
                    focus_ok = True
                    # logger.error('DE 2 new_focus = {}'.format(new_focus))
                    break
        # logger.error('DE new_focus = {}'.format(new_focus))
        # logger.error('DE end focus next ==========================')
        self._apply_new_focus(new_focus)","for i in range(new_focus, len(self._widgets)):
    if self._widgets[i].enabled:
        new_focus = i
        focus_ok = True
        break
if not focus_ok:
    for i in range(0, new_focus + 1):
        if self._widgets[i].enabled:
            new_focus = i
            focus_ok = True
            break","for i in range(new_focus, len(self._widgets)):
    if self._widgets[i].enabled:
        new_focus = i
        focus_ok = True
        break
else:
    for i in range(0, new_focus + 1):
        if self._widgets[i].enabled:
            new_focus = i
            focus_ok = True
            break","for i in range(new_focus, len(self._widgets)):
    if self._widgets[i].enabled:
        new_focus = i
        focus_ok = True
        break
else:
    for i in range(0, new_focus + 1):
        if self._widgets[i].enabled:
            new_focus = i
            focus_ok = True
            break",1,"for i in range(new_focus, len(self._widgets)):
    if self._widgets[i].enabled:
        new_focus = i
        focus_ok = True
        break
if not focus_ok:
    for i in range(0, new_focus + 1):
        if self._widgets[i].enabled:
            new_focus = i
            focus_ok = True
            break","break statement is executed:None
break statement is not executed:zejun1"
hamster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hamster/waflib/Tools/fc_scan.py,https://github.com/projecthamster/hamster/tree/master/waflib/Tools/fc_scan.py,fortran_parser,tryfind_header$103,"def tryfind_header(self, filename):
		""""""
		Adds an include file to the list of nodes to process

		:param filename: file name
		:type filename: string
		""""""
		found = None
		for n in self.incpaths:
			found = n.find_resource(filename)
			if found:
				self.nodes.append(found)
				self.waiting.append(found)
				break
		if not found:
			if not filename in self.names:
				self.names.append(filename)","for n in self.incpaths:
    found = n.find_resource(filename)
    if found:
        self.nodes.append(found)
        self.waiting.append(found)
        break
if not found:
    if not filename in self.names:
        self.names.append(filename)","for n in self.incpaths:
    found = n.find_resource(filename)
    if found:
        self.nodes.append(found)
        self.waiting.append(found)
        break
else:
    if not filename in self.names:
        self.names.append(filename)","for n in self.incpaths:
    found = n.find_resource(filename)
    if found:
        self.nodes.append(found)
        self.waiting.append(found)
        break
else:
    if not filename in self.names:
        self.names.append(filename)",1,"for n in self.incpaths:
    found = n.find_resource(filename)
    if found:
        self.nodes.append(found)
        self.waiting.append(found)
        break
if not found:
    if not filename in self.names:
        self.names.append(filename)","break statement is executed:None
break statement is not executed:zejun1"
NeuralBabyTalk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NeuralBabyTalk/misc/dataloader_coco.py,https://github.com/jiasenlu/NeuralBabyTalk/tree/master/misc/dataloader_coco.py,DataLoader,__getitem__$189,"def __getitem__(self, index):
        ix = self.split_ix[index]

        # load image here.
        image_id = self.info['images'][ix]['id']
        file_path = self.info['images'][ix]['file_path']

        proposal_item =copy.deepcopy(self.dataloader_hdf[ix])
        num_proposal = int(proposal_item['dets_num'])
        num_nms = int(proposal_item['nms_num'])
        proposals = proposal_item['dets_labels']
        proposals = proposals.squeeze()[:num_nms, :]

        coco_split = file_path.split('/')[0]
        # get the ground truth bounding box.
        if coco_split == 'train2014':
            coco = self.coco_train
        else:
            coco = self.coco_val

        bbox_ann_ids = coco.getAnnIds(imgIds=image_id)
        bbox_ann = [{'label': self.ctol[i['category_id']], 'bbox': i['bbox']} for i in coco.loadAnns(bbox_ann_ids)]

        gt_bboxs = np.zeros((len(bbox_ann), 5))
        for i, bbox in enumerate(bbox_ann):
            gt_bboxs[i, :4] = bbox['bbox']
            gt_bboxs[i, 4] = bbox['label']

        # convert from x,y,w,h to x_min, y_min, x_max, y_max
        gt_bboxs[:,2] = gt_bboxs[:,2] + gt_bboxs[:,0]
        gt_bboxs[:,3] = gt_bboxs[:,3] + gt_bboxs[:,1]

        # load the image.
        img = Image.open(os.path.join(self.opt.image_path, file_path)).convert('RGB')

        width, height = img.size
        # resize the image.
        img = self.Resize(img)

        if self.split == 'train':
            # resize the gt_bboxs and proposals.
            proposals = utils.resize_bbox(proposals, width, height, self.opt.image_size, self.opt.image_size)
            gt_bboxs = utils.resize_bbox(gt_bboxs, width, height, self.opt.image_size, self.opt.image_size)
        else:
            proposals = utils.resize_bbox(proposals, width, height, self.opt.image_crop_size, self.opt.image_crop_size)
            gt_bboxs = utils.resize_bbox(gt_bboxs, width, height, self.opt.image_crop_size, self.opt.image_crop_size)

        # crop the image and the bounding box.
        img, proposals, gt_bboxs = self.RandomCropWithBbox(img, proposals, gt_bboxs)

        gt_x = (gt_bboxs[:,2]-gt_bboxs[:,0]+1)
        gt_y = (gt_bboxs[:,3]-gt_bboxs[:,1]+1)
        gt_area_nonzero = (((gt_x != 1) & (gt_y != 1)))

        gt_bboxs = gt_bboxs[gt_area_nonzero]
        captions = self.caption_file[ix]

        # given the bbox_ann, and caption, this function determine which word belongs to the detection.
        det_indicator = self.get_det_word(gt_bboxs, captions)

        # fetch the captions
        ncap = len(captions) # number of captions available for this image
        assert ncap > 0, 'an image does not have any label. this can be handled but right now isn\'t'

        # convert caption into sequence label.
        cap_seq = np.zeros([ncap, self.seq_length, 5])
        for i, caption in enumerate(captions):
            j = 0
            k = 0
            while j < len(caption) and j < self.seq_length:
                is_det = False
                for n in range(2, 0, -1):
                    if det_indicator[n][i][j][0] != 0:
                        cap_seq[i,k,0] = det_indicator[n][i][j][0] + self.vocab_size
                        cap_seq[i,k,1] = det_indicator[n][i][j][1]
                        cap_seq[i,k,2] = det_indicator[n][i][j][2]
                        cap_seq[i,k,3] = self.wtoi[caption[j]]
                        cap_seq[i,k,4] = self.wtoi[caption[j]]

                        is_det = True
                        j += n # skip the ngram.
                        break
                if is_det == False:
                    cap_seq[i,k,0] = self.wtoi[caption[j]]
                    cap_seq[i,k,4] = cap_seq[i,k,0]
                    j += 1
                k += 1

        # get the mask of the ground truth bounding box. The data shape is
        # num_caption x num_box x num_seq
        box_mask = np.ones((len(captions), gt_bboxs.shape[0], self.seq_length))
        for i in range(len(captions)):
            for j in range(self.seq_length):
                if cap_seq[i,j,0] > self.vocab_size:
                    box_mask[i,:,j] = ((gt_bboxs[:,4] == (cap_seq[i,j,0]-self.vocab_size)) == 0)

        # get the batch version of the seq and box_mask.
        if ncap < self.seq_per_img:
            seq_batch = np.zeros([self.seq_per_img, self.seq_length, 4])
            mask_batch = np.zeros([self.seq_per_img, gt_bboxs.shape[0], self.seq_length])
            # we need to subsample (with replacement)
            for q in range(self.seq_per_img):
                ixl = random.randint(0,ncap)
                seq_batch[q,:] = cap_seq[ixl,:,:4]
                mask_batch[q,:]=box_mask[ixl]
        else:
            ixl = random.randint(0, ncap - self.seq_per_img)
            seq_batch = cap_seq[ixl:ixl+self.seq_per_img,:,:4]
            mask_batch = box_mask[ixl:ixl+self.seq_per_img]

        input_seq = np.zeros([self.seq_per_img, self.seq_length+1, 4])
        input_seq[:,1:] = seq_batch

        gt_seq = np.zeros([10, self.seq_length])
        gt_seq[:ncap,:] = cap_seq[:,:,4]

        # if self.split == 'train':
            # augment the proposal with the gt bounding box.
            # this is just to make sure there exist proposals which labels to 1.
            # gt_bboxs_tmp = np.concatenate((gt_bboxs, np.ones((gt_bboxs.shape[0],1))), axis=1)
            # proposals = np.concatenate((gt_bboxs_tmp, proposals), axis=0)
        # flag = False
        # for cap in captions:
        #     if 'bus' in cap:
        #         flag = True
        # if flag:
        #     img_show = np.array(img)
        #     img_show2 = copy.deepcopy(img_show)
        #     import cv2
        #     for i in range(gt_bboxs.shape[0]):
        #         class_name = self.itoc[int(gt_bboxs[i, 4])]
        #         bbox = tuple(int(np.round(x)) for x in gt_bboxs[i, :4])
        #         cv2.rectangle(img_show, bbox[0:2], bbox[2:4], (0, 204, 0), 2)
        #         cv2.putText(img_show, '%s: %.3f' % (class_name, 1), (bbox[0], bbox[1] + 15), cv2.FONT_HERSHEY_PLAIN,
        #                     1.0, (0, 0, 255), thickness=1)
        #     cv2.imwrite('gt_boxes.jpg', img_show)

        #     for i in range(proposals.shape[0]):
        #         bbox = tuple(int(np.round(x)) for x in proposals[i, :4])
        #         score =  proposals[i, 5]
        #         class_name = self.itoc[int(proposals[i, 4])]
        #         cv2.rectangle(img_show2, bbox[0:2], bbox[2:4], (0, 204, 0), 2)

        #         cv2.putText(img_show2, '%s: %.3f' % (class_name, score), (bbox[0], bbox[1] + 15), cv2.FONT_HERSHEY_PLAIN,
        #                     1.0, (0, 0, 255), thickness=1)
        #     cv2.imwrite('proposals.jpg', img_show2)

        #     pdb.set_trace()
        # padding the proposals and gt_bboxs
        pad_proposals = np.zeros((self.max_proposal, 6))
        pad_gt_bboxs = np.zeros((self.max_gt_box, 5))
        pad_box_mask = np.ones((self.seq_per_img, self.max_gt_box, self.seq_length+1))

        if self.opt.det_oracle == False:
            num_pps = min(proposals.shape[0], self.max_proposal)
            num_box = min(gt_bboxs.shape[0], self.max_gt_box)

            pad_proposals[:num_pps] = proposals[:num_pps]
            pad_gt_bboxs[:num_box] = gt_bboxs[:num_box]
            pad_box_mask[:,:num_box,1:] = mask_batch[:,:num_box,:]
        else:
            num_pps = min(gt_bboxs.shape[0], self.max_proposal)
            pad_proposals[:num_pps] = np.concatenate((gt_bboxs[:num_pps], np.ones([num_pps,1])),axis=1)
            num_box = min(gt_bboxs.shape[0], self.max_gt_box)
            pad_gt_bboxs[:num_box] = gt_bboxs[:num_box]
            pad_box_mask[:,:num_box,1:] = mask_batch[:,:num_box,:]

        input_seq = torch.from_numpy(input_seq).long()
        gt_seq = torch.from_numpy(gt_seq).long()
        pad_proposals = torch.from_numpy(pad_proposals).float()
        pad_box_mask = torch.from_numpy(pad_box_mask).byte()
        pad_gt_bboxs = torch.from_numpy(pad_gt_bboxs).float()
        num = torch.FloatTensor([ncap, num_pps, num_box])

        if self.opt.cnn_backend == 'vgg16':
            img = np.array(img, dtype='float32')
            img = img[:,:,::-1].copy() # RGB --> BGR
            img -= self.vgg_pixel_mean
            img = torch.from_numpy(img)
            img = img.permute(2, 0, 1).contiguous()
        else:
            img = self.ToTensor(img)
            img = self.res_Normalize(img)

        return img, input_seq, gt_seq, num, pad_proposals, pad_gt_bboxs, pad_box_mask, image_id","for n in range(2, 0, -1):
    if det_indicator[n][i][j][0] != 0:
        cap_seq[i, k, 0] = det_indicator[n][i][j][0] + self.vocab_size
        cap_seq[i, k, 1] = det_indicator[n][i][j][1]
        cap_seq[i, k, 2] = det_indicator[n][i][j][2]
        cap_seq[i, k, 3] = self.wtoi[caption[j]]
        cap_seq[i, k, 4] = self.wtoi[caption[j]]
        is_det = True
        j += n
        break
if is_det == False:
    cap_seq[i, k, 0] = self.wtoi[caption[j]]
    cap_seq[i, k, 4] = cap_seq[i, k, 0]
    j += 1","for n in range(2, 0, -1):
    if det_indicator[n][i][j][0] != 0:
        cap_seq[i, k, 0] = det_indicator[n][i][j][0] + self.vocab_size
        cap_seq[i, k, 1] = det_indicator[n][i][j][1]
        cap_seq[i, k, 2] = det_indicator[n][i][j][2]
        cap_seq[i, k, 3] = self.wtoi[caption[j]]
        cap_seq[i, k, 4] = self.wtoi[caption[j]]
        j += n
        break
else:
    cap_seq[i, k, 0] = self.wtoi[caption[j]]
    cap_seq[i, k, 4] = cap_seq[i, k, 0]
    j += 1","for n in range(2, 0, -1):
    if det_indicator[n][i][j][0] != 0:
        cap_seq[i, k, 0] = det_indicator[n][i][j][0] + self.vocab_size
        cap_seq[i, k, 1] = det_indicator[n][i][j][1]
        cap_seq[i, k, 2] = det_indicator[n][i][j][2]
        cap_seq[i, k, 3] = self.wtoi[caption[j]]
        cap_seq[i, k, 4] = self.wtoi[caption[j]]
        j += n
        break
else:
    cap_seq[i, k, 0] = self.wtoi[caption[j]]
    cap_seq[i, k, 4] = cap_seq[i, k, 0]
    j += 1",1,"for n in range(2, 0, -1):
    if det_indicator[n][i][j][0] != 0:
        cap_seq[i, k, 0] = det_indicator[n][i][j][0] + self.vocab_size
        cap_seq[i, k, 1] = det_indicator[n][i][j][1]
        cap_seq[i, k, 2] = det_indicator[n][i][j][2]
        cap_seq[i, k, 3] = self.wtoi[caption[j]]
        cap_seq[i, k, 4] = self.wtoi[caption[j]]
        is_det = True
        j += n
        break
if is_det == False:
    cap_seq[i, k, 0] = self.wtoi[caption[j]]
    cap_seq[i, k, 4] = cap_seq[i, k, 0]
    j += 1","break statement is executed:None
break statement is not executed:zejun1"
BlenderProc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BlenderProc/blenderproc/python/material/MaterialLoaderUtility.py,https://github.com/DLR-RM/BlenderProc/tree/master/blenderproc/python/material/MaterialLoaderUtility.py,,change_to_texture_less_render$473,"def change_to_texture_less_render(use_alpha_channel):
    """""" Changes the materials, which do not contain a emission shader to a white slightly glossy texture

    :param use_alpha_channel: If true, the alpha channel stored in .png textures is used.
    """"""
    new_mat = bpy.data.materials.new(name=""TextureLess"")
    new_mat.use_nodes = True
    nodes = new_mat.node_tree.nodes

    principled_bsdf = Utility.get_the_one_node_with_type(nodes, ""BsdfPrincipled"")

    # setting the color values for the shader
    principled_bsdf.inputs['Specular'].default_value = 0.65  # specular
    principled_bsdf.inputs['Roughness'].default_value = 0.2  # roughness

    for used_object in [obj for obj in bpy.context.scene.objects if hasattr(obj.data, 'materials')]:
        # replace all materials with the new texture less material
        for slot in used_object.material_slots:
            emission_shader = False
            # check if the material contains an emission shader:
            for node in slot.material.node_tree.nodes:
                # check if one of the shader nodes is a Emission Shader
                if 'Emission' in node.bl_idname:
                    emission_shader = True
                    break
            # only replace materials, which do not contain any emission shader
            if not emission_shader:
                if use_alpha_channel:
                    slot.material = add_alpha_texture_node(slot.material, new_mat)
                else:
                    slot.material = new_mat","for node in slot.material.node_tree.nodes:
    if 'Emission' in node.bl_idname:
        emission_shader = True
        break
if not emission_shader:
    if use_alpha_channel:
        slot.material = add_alpha_texture_node(slot.material, new_mat)
    else:
        slot.material = new_mat","for node in slot.material.node_tree.nodes:
    if 'Emission' in node.bl_idname:
        break
else:
    if use_alpha_channel:
        slot.material = add_alpha_texture_node(slot.material, new_mat)
    else:
        slot.material = new_mat","for node in slot.material.node_tree.nodes:
    if 'Emission' in node.bl_idname:
        break
else:
    if use_alpha_channel:
        slot.material = add_alpha_texture_node(slot.material, new_mat)
    else:
        slot.material = new_mat",1,"for node in slot.material.node_tree.nodes:
    if 'Emission' in node.bl_idname:
        emission_shader = True
        break
if not emission_shader:
    if use_alpha_channel:
        slot.material = add_alpha_texture_node(slot.material, new_mat)
    else:
        slot.material = new_mat","break statement is executed:None
break statement is not executed:zejun1"
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/model/document.py,https://github.com/frappe/frappe/tree/master/frappe/model/document.py,Document,apply_fieldlevel_read_permissions$653,"def apply_fieldlevel_read_permissions(self):
		""""""Remove values the user is not allowed to read (called when loading in desk)""""""

		if frappe.session.user == ""Administrator"":
			return

		has_higher_permlevel = False

		all_fields = self.meta.fields.copy()
		for table_field in self.meta.get_table_fields():
			all_fields += frappe.get_meta(table_field.options).fields or []

		for df in all_fields:
			if df.permlevel > 0:
				has_higher_permlevel = True
				break

		if not has_higher_permlevel:
			return

		has_access_to = self.get_permlevel_access(""read"")

		for df in self.meta.fields:
			if df.permlevel and not df.permlevel in has_access_to:
				self.set(df.fieldname, None)

		for table_field in self.meta.get_table_fields():
			for df in frappe.get_meta(table_field.options).fields or []:
				if df.permlevel and not df.permlevel in has_access_to:
					for child in self.get(table_field.fieldname) or []:
						child.set(df.fieldname, None)","for df in all_fields:
    if df.permlevel > 0:
        has_higher_permlevel = True
        break
if not has_higher_permlevel:
    return","for df in all_fields:
    if df.permlevel > 0:
        break
else:
    return","for df in all_fields:
    if df.permlevel > 0:
        break
else:
    return",1,"for df in all_fields:
    if df.permlevel > 0:
        has_higher_permlevel = True
        break
if not has_higher_permlevel:
    return","break statement is executed:None
break statement is not executed:zejun1"
commix,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/commix/src/core/injections/blind/techniques/time_based/tb_injector.py,https://github.com/commixproject/commix/tree/master/src/core/injections/blind/techniques/time_based/tb_injector.py,,false_positive_check$355,"def false_positive_check(separator, TAG, cmd, whitespace, prefix, suffix, timesec, http_request_method, url, vuln_parameter, randvcalc, alter_shell, how_long, url_time_response, false_positive_warning):

  if settings.TARGET_OS == ""win"":
    previous_cmd = cmd
    if alter_shell:
      cmd = settings.WIN_PYTHON_INTERPRETER + "" -c \""import os; print len(os.popen('cmd /c "" + cmd + ""').read().strip())\""""
    else: 
      cmd = ""powershell.exe -InputFormat none write-host ([string](cmd /c "" + cmd + "")).trim().length""

  found_chars = False
  checks.check_for_false_positive_result(false_positive_warning)

  # Varying the sleep time.
  if false_positive_warning:
    timesec = timesec + random.randint(3, 5)

  # Checking the output length of the used payload.
  if settings.VERBOSITY_LEVEL == 0: 
    sys.stdout.write(""."")
  for output_length in range(1, 3):
    if settings.VERBOSITY_LEVEL == 0: 
      sys.stdout.write(""."")
    # Execute shell commands on vulnerable host.
    if alter_shell:
      payload = tb_payloads.cmd_execution_alter_shell(separator, cmd, output_length, timesec, http_request_method)
    else:
      payload = tb_payloads.cmd_execution(separator, cmd, output_length, timesec, http_request_method)
    
    # Fix prefixes / suffixes
    payload = parameters.prefixes(payload, prefix)
    payload = parameters.suffixes(payload, suffix)

    # Whitespace fixation
    payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)

    # Perform payload modification
    payload = checks.perform_payload_modification(payload)

    # Check if defined ""--verbose"" option.
    if settings.VERBOSITY_LEVEL != 0:
      payload_msg = payload.replace(""\n"", ""\\n"") 
      print(settings.print_payload(payload_msg))

    # Check if defined cookie with ""INJECT_HERE"" tag
    if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
      how_long = cookie_injection_test(url, vuln_parameter, payload)

    # Check if defined user-agent with ""INJECT_HERE"" tag
    elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
      how_long = user_agent_injection_test(url, vuln_parameter, payload)

    # Check if defined referer with ""INJECT_HERE"" tag
    elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
      how_long = referer_injection_test(url, vuln_parameter, payload)

    # Check if defined host with ""INJECT_HERE"" tag
    elif menu.options.host and settings.INJECT_TAG in menu.options.host:
      how_long = host_injection_test(url, vuln_parameter, payload)

    # Check if defined custom header with ""INJECT_HERE"" tag
    elif settings.CUSTOM_HEADER_INJECTION:
      how_long = custom_header_injection_test(url, vuln_parameter, payload)

    else:  
      how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)

    if (how_long >= settings.FOUND_HOW_LONG) and (how_long - timesec >= settings.FOUND_DIFF):
      found_chars = True
      break

  if found_chars == True :
    if settings.TARGET_OS == ""win"":
      cmd = previous_cmd
    num_of_chars = output_length + 1
    check_start = 0
    check_end = 0
    check_start = time.time()
    
    output = []
    percent = 0
    sys.stdout.flush()

    is_valid = False
    for num_of_chars in range(1, int(num_of_chars)):
      for ascii_char in range(1, 20):
        if settings.VERBOSITY_LEVEL == 0:
          sys.stdout.write(""."")
        if alter_shell:
          # Get the execution output, of shell execution.
          payload = tb_payloads.fp_result_alter_shell(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
        else:
          # Get the execution output, of shell execution.
          payload = tb_payloads.fp_result(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
          
        # Fix prefixes / suffixes
        payload = parameters.prefixes(payload, prefix)
        payload = parameters.suffixes(payload, suffix)

        # Whitespace fixation
        payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)

        # Perform payload modification
        payload = checks.perform_payload_modification(payload)

        # Check if defined ""--verbose"" option.
        if settings.VERBOSITY_LEVEL != 0:
          payload_msg = payload.replace(""\n"", ""\\n"") 
          print(settings.print_payload(payload_msg))

        # Check if defined cookie with ""INJECT_HERE"" tag
        if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
          how_long = cookie_injection_test(url, vuln_parameter, payload)

        # Check if defined user-agent with ""INJECT_HERE"" tag
        elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
          how_long = user_agent_injection_test(url, vuln_parameter, payload)

        # Check if defined referer with ""INJECT_HERE"" tag
        elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
          how_long = referer_injection_test(url, vuln_parameter, payload)

        # Check if defined host with ""INJECT_HERE"" tag
        elif menu.options.host and settings.INJECT_TAG in menu.options.host:
          how_long = host_injection_test(url, vuln_parameter, payload)

        # Check if defined custom header with ""INJECT_HERE"" tag
        elif settings.CUSTOM_HEADER_INJECTION:
          how_long = custom_header_injection_test(url, vuln_parameter, payload)

        else:    
          how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)

        if (how_long >= settings.FOUND_HOW_LONG) and (how_long - timesec >= settings.FOUND_DIFF):
          output.append(ascii_char)
          is_valid = True
          break
          
      if is_valid:
          break

    check_end  = time.time()
    check_how_long = int(check_end - check_start)
    output = """".join(str(p) for p in output)

    if str(output) == str(randvcalc):
      if settings.VERBOSITY_LEVEL == 0: 
        sys.stdout.write("" (done)"")
      return how_long, output

  else:
    checks.unexploitable_point()","for output_length in range(1, 3):
    if settings.VERBOSITY_LEVEL == 0:
        sys.stdout.write('.')
    if alter_shell:
        payload = tb_payloads.cmd_execution_alter_shell(separator, cmd, output_length, timesec, http_request_method)
    else:
        payload = tb_payloads.cmd_execution(separator, cmd, output_length, timesec, http_request_method)
    payload = parameters.prefixes(payload, prefix)
    payload = parameters.suffixes(payload, suffix)
    payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
    payload = checks.perform_payload_modification(payload)
    if settings.VERBOSITY_LEVEL != 0:
        payload_msg = payload.replace('\n', '\\n')
        print(settings.print_payload(payload_msg))
    if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
        how_long = cookie_injection_test(url, vuln_parameter, payload)
    elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
        how_long = user_agent_injection_test(url, vuln_parameter, payload)
    elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
        how_long = referer_injection_test(url, vuln_parameter, payload)
    elif menu.options.host and settings.INJECT_TAG in menu.options.host:
        how_long = host_injection_test(url, vuln_parameter, payload)
    elif settings.CUSTOM_HEADER_INJECTION:
        how_long = custom_header_injection_test(url, vuln_parameter, payload)
    else:
        how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
    if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
        found_chars = True
        break
if found_chars == True:
    if settings.TARGET_OS == 'win':
        cmd = previous_cmd
    num_of_chars = output_length + 1
    check_start = 0
    check_end = 0
    check_start = time.time()
    output = []
    percent = 0
    sys.stdout.flush()
    is_valid = False
    for num_of_chars in range(1, int(num_of_chars)):
        for ascii_char in range(1, 20):
            if settings.VERBOSITY_LEVEL == 0:
                sys.stdout.write('.')
            if alter_shell:
                payload = tb_payloads.fp_result_alter_shell(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
            else:
                payload = tb_payloads.fp_result(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
            payload = parameters.prefixes(payload, prefix)
            payload = parameters.suffixes(payload, suffix)
            payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
            payload = checks.perform_payload_modification(payload)
            if settings.VERBOSITY_LEVEL != 0:
                payload_msg = payload.replace('\n', '\\n')
                print(settings.print_payload(payload_msg))
            if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
                how_long = cookie_injection_test(url, vuln_parameter, payload)
            elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
                how_long = user_agent_injection_test(url, vuln_parameter, payload)
            elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
                how_long = referer_injection_test(url, vuln_parameter, payload)
            elif menu.options.host and settings.INJECT_TAG in menu.options.host:
                how_long = host_injection_test(url, vuln_parameter, payload)
            elif settings.CUSTOM_HEADER_INJECTION:
                how_long = custom_header_injection_test(url, vuln_parameter, payload)
            else:
                how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
            if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
                output.append(ascii_char)
                is_valid = True
                break
        if is_valid:
            break
    check_end = time.time()
    check_how_long = int(check_end - check_start)
    output = ''.join((str(p) for p in output))
    if str(output) == str(randvcalc):
        if settings.VERBOSITY_LEVEL == 0:
            sys.stdout.write(' (done)')
        return (how_long, output)
else:
    checks.unexploitable_point()","for output_length in range(1, 3):
    if settings.VERBOSITY_LEVEL == 0:
        sys.stdout.write('.')
    if alter_shell:
        payload = tb_payloads.cmd_execution_alter_shell(separator, cmd, output_length, timesec, http_request_method)
    else:
        payload = tb_payloads.cmd_execution(separator, cmd, output_length, timesec, http_request_method)
    payload = parameters.prefixes(payload, prefix)
    payload = parameters.suffixes(payload, suffix)
    payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
    payload = checks.perform_payload_modification(payload)
    if settings.VERBOSITY_LEVEL != 0:
        payload_msg = payload.replace('\n', '\\n')
        print(settings.print_payload(payload_msg))
    if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
        how_long = cookie_injection_test(url, vuln_parameter, payload)
    elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
        how_long = user_agent_injection_test(url, vuln_parameter, payload)
    elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
        how_long = referer_injection_test(url, vuln_parameter, payload)
    elif menu.options.host and settings.INJECT_TAG in menu.options.host:
        how_long = host_injection_test(url, vuln_parameter, payload)
    elif settings.CUSTOM_HEADER_INJECTION:
        how_long = custom_header_injection_test(url, vuln_parameter, payload)
    else:
        how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
    if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
        if settings.TARGET_OS == 'win':
            cmd = previous_cmd
        num_of_chars = output_length + 1
        check_start = 0
        check_end = 0
        check_start = time.time()
        output = []
        percent = 0
        sys.stdout.flush()
        is_valid = False
        for num_of_chars in range(1, int(num_of_chars)):
            for ascii_char in range(1, 20):
                if settings.VERBOSITY_LEVEL == 0:
                    sys.stdout.write('.')
                if alter_shell:
                    payload = tb_payloads.fp_result_alter_shell(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
                else:
                    payload = tb_payloads.fp_result(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
                payload = parameters.prefixes(payload, prefix)
                payload = parameters.suffixes(payload, suffix)
                payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
                payload = checks.perform_payload_modification(payload)
                if settings.VERBOSITY_LEVEL != 0:
                    payload_msg = payload.replace('\n', '\\n')
                    print(settings.print_payload(payload_msg))
                if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
                    how_long = cookie_injection_test(url, vuln_parameter, payload)
                elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
                    how_long = user_agent_injection_test(url, vuln_parameter, payload)
                elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
                    how_long = referer_injection_test(url, vuln_parameter, payload)
                elif menu.options.host and settings.INJECT_TAG in menu.options.host:
                    how_long = host_injection_test(url, vuln_parameter, payload)
                elif settings.CUSTOM_HEADER_INJECTION:
                    how_long = custom_header_injection_test(url, vuln_parameter, payload)
                else:
                    how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
                if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
                    output.append(ascii_char)
                    is_valid = True
                    break
            if is_valid:
                break
        check_end = time.time()
        check_how_long = int(check_end - check_start)
        output = ''.join((str(p) for p in output))
        if str(output) == str(randvcalc):
            if settings.VERBOSITY_LEVEL == 0:
                sys.stdout.write(' (done)')
            return (how_long, output)
        break
else:
    checks.unexploitable_point()","for output_length in range(1, 3):
    if settings.VERBOSITY_LEVEL == 0:
        sys.stdout.write('.')
    if alter_shell:
        payload = tb_payloads.cmd_execution_alter_shell(separator, cmd, output_length, timesec, http_request_method)
    else:
        payload = tb_payloads.cmd_execution(separator, cmd, output_length, timesec, http_request_method)
    payload = parameters.prefixes(payload, prefix)
    payload = parameters.suffixes(payload, suffix)
    payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
    payload = checks.perform_payload_modification(payload)
    if settings.VERBOSITY_LEVEL != 0:
        payload_msg = payload.replace('\n', '\\n')
        print(settings.print_payload(payload_msg))
    if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
        how_long = cookie_injection_test(url, vuln_parameter, payload)
    elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
        how_long = user_agent_injection_test(url, vuln_parameter, payload)
    elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
        how_long = referer_injection_test(url, vuln_parameter, payload)
    elif menu.options.host and settings.INJECT_TAG in menu.options.host:
        how_long = host_injection_test(url, vuln_parameter, payload)
    elif settings.CUSTOM_HEADER_INJECTION:
        how_long = custom_header_injection_test(url, vuln_parameter, payload)
    else:
        how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
    if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
        if settings.TARGET_OS == 'win':
            cmd = previous_cmd
        num_of_chars = output_length + 1
        check_start = 0
        check_end = 0
        check_start = time.time()
        output = []
        percent = 0
        sys.stdout.flush()
        is_valid = False
        for num_of_chars in range(1, int(num_of_chars)):
            for ascii_char in range(1, 20):
                if settings.VERBOSITY_LEVEL == 0:
                    sys.stdout.write('.')
                if alter_shell:
                    payload = tb_payloads.fp_result_alter_shell(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
                else:
                    payload = tb_payloads.fp_result(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
                payload = parameters.prefixes(payload, prefix)
                payload = parameters.suffixes(payload, suffix)
                payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
                payload = checks.perform_payload_modification(payload)
                if settings.VERBOSITY_LEVEL != 0:
                    payload_msg = payload.replace('\n', '\\n')
                    print(settings.print_payload(payload_msg))
                if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
                    how_long = cookie_injection_test(url, vuln_parameter, payload)
                elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
                    how_long = user_agent_injection_test(url, vuln_parameter, payload)
                elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
                    how_long = referer_injection_test(url, vuln_parameter, payload)
                elif menu.options.host and settings.INJECT_TAG in menu.options.host:
                    how_long = host_injection_test(url, vuln_parameter, payload)
                elif settings.CUSTOM_HEADER_INJECTION:
                    how_long = custom_header_injection_test(url, vuln_parameter, payload)
                else:
                    how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
                if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
                    output.append(ascii_char)
                    is_valid = True
                    break
            if is_valid:
                break
        check_end = time.time()
        check_how_long = int(check_end - check_start)
        output = ''.join((str(p) for p in output))
        if str(output) == str(randvcalc):
            if settings.VERBOSITY_LEVEL == 0:
                sys.stdout.write(' (done)')
            return (how_long, output)
        break
else:
    checks.unexploitable_point()",1,"for output_length in range(1, 3):
    if settings.VERBOSITY_LEVEL == 0:
        sys.stdout.write('.')
    if alter_shell:
        payload = tb_payloads.cmd_execution_alter_shell(separator, cmd, output_length, timesec, http_request_method)
    else:
        payload = tb_payloads.cmd_execution(separator, cmd, output_length, timesec, http_request_method)
    payload = parameters.prefixes(payload, prefix)
    payload = parameters.suffixes(payload, suffix)
    payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
    payload = checks.perform_payload_modification(payload)
    if settings.VERBOSITY_LEVEL != 0:
        payload_msg = payload.replace('\n', '\\n')
        print(settings.print_payload(payload_msg))
    if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
        how_long = cookie_injection_test(url, vuln_parameter, payload)
    elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
        how_long = user_agent_injection_test(url, vuln_parameter, payload)
    elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
        how_long = referer_injection_test(url, vuln_parameter, payload)
    elif menu.options.host and settings.INJECT_TAG in menu.options.host:
        how_long = host_injection_test(url, vuln_parameter, payload)
    elif settings.CUSTOM_HEADER_INJECTION:
        how_long = custom_header_injection_test(url, vuln_parameter, payload)
    else:
        how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
    if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
        found_chars = True
        break
if found_chars == True:
    if settings.TARGET_OS == 'win':
        cmd = previous_cmd
    num_of_chars = output_length + 1
    check_start = 0
    check_end = 0
    check_start = time.time()
    output = []
    percent = 0
    sys.stdout.flush()
    is_valid = False
    for num_of_chars in range(1, int(num_of_chars)):
        for ascii_char in range(1, 20):
            if settings.VERBOSITY_LEVEL == 0:
                sys.stdout.write('.')
            if alter_shell:
                payload = tb_payloads.fp_result_alter_shell(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
            else:
                payload = tb_payloads.fp_result(separator, cmd, num_of_chars, ascii_char, timesec, http_request_method)
            payload = parameters.prefixes(payload, prefix)
            payload = parameters.suffixes(payload, suffix)
            payload = payload.replace(settings.SINGLE_WHITESPACE, whitespace)
            payload = checks.perform_payload_modification(payload)
            if settings.VERBOSITY_LEVEL != 0:
                payload_msg = payload.replace('\n', '\\n')
                print(settings.print_payload(payload_msg))
            if menu.options.cookie and settings.INJECT_TAG in menu.options.cookie:
                how_long = cookie_injection_test(url, vuln_parameter, payload)
            elif menu.options.agent and settings.INJECT_TAG in menu.options.agent:
                how_long = user_agent_injection_test(url, vuln_parameter, payload)
            elif menu.options.referer and settings.INJECT_TAG in menu.options.referer:
                how_long = referer_injection_test(url, vuln_parameter, payload)
            elif menu.options.host and settings.INJECT_TAG in menu.options.host:
                how_long = host_injection_test(url, vuln_parameter, payload)
            elif settings.CUSTOM_HEADER_INJECTION:
                how_long = custom_header_injection_test(url, vuln_parameter, payload)
            else:
                how_long = examine_requests(payload, vuln_parameter, http_request_method, url, timesec, url_time_response)
            if how_long >= settings.FOUND_HOW_LONG and how_long - timesec >= settings.FOUND_DIFF:
                output.append(ascii_char)
                is_valid = True
                break
        if is_valid:
            break
    check_end = time.time()
    check_how_long = int(check_end - check_start)
    output = ''.join((str(p) for p in output))
    if str(output) == str(randvcalc):
        if settings.VERBOSITY_LEVEL == 0:
            sys.stdout.write(' (done)')
        return (how_long, output)
else:
    checks.unexploitable_point()","break statement is executed:zejun1
break statement is not executed:None"
pyopencl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyopencl/pyopencl/cache.py,https://github.com/inducer/pyopencl/tree/master/pyopencl/cache.py,,_inner$172,"def _inner(src):
        for match in C_INCLUDE_RE.finditer(src):
            included = match.group(1)

            found = False
            for ipath in include_path:
                included_file_name = realpath(join(ipath, included))

                if included_file_name not in result:
                    try:
                        src_file = open(included_file_name)
                    except OSError:
                        continue

                    try:
                        included_src = src_file.read()
                    finally:
                        src_file.close()

                    # jrevent infinite recursion if some header file appears to
                    # include itself
                    result[included_file_name] = None

                    checksum = new_hash()
                    update_checksum(checksum, included_src)
                    _inner(included_src)

                    result[included_file_name] = (
                            os.stat(included_file_name).st_mtime,
                            checksum.hexdigest(),
                            )

                    found = True
                    break  # stop searching the include path

            if not found:
                pass","for ipath in include_path:
    included_file_name = realpath(join(ipath, included))
    if included_file_name not in result:
        try:
            src_file = open(included_file_name)
        except OSError:
            continue
        try:
            included_src = src_file.read()
        finally:
            src_file.close()
        result[included_file_name] = None
        checksum = new_hash()
        update_checksum(checksum, included_src)
        _inner(included_src)
        result[included_file_name] = (os.stat(included_file_name).st_mtime, checksum.hexdigest())
        found = True
        break
if not found:
    pass","for ipath in include_path:
    included_file_name = realpath(join(ipath, included))
    if included_file_name not in result:
        try:
            src_file = open(included_file_name)
        except OSError:
            continue
        try:
            included_src = src_file.read()
        finally:
            src_file.close()
        result[included_file_name] = None
        checksum = new_hash()
        update_checksum(checksum, included_src)
        _inner(included_src)
        result[included_file_name] = (os.stat(included_file_name).st_mtime, checksum.hexdigest())
        break
else:
    pass","for ipath in include_path:
    included_file_name = realpath(join(ipath, included))
    if included_file_name not in result:
        try:
            src_file = open(included_file_name)
        except OSError:
            continue
        try:
            included_src = src_file.read()
        finally:
            src_file.close()
        result[included_file_name] = None
        checksum = new_hash()
        update_checksum(checksum, included_src)
        _inner(included_src)
        result[included_file_name] = (os.stat(included_file_name).st_mtime, checksum.hexdigest())
        break
else:
    pass",1,"for ipath in include_path:
    included_file_name = realpath(join(ipath, included))
    if included_file_name not in result:
        try:
            src_file = open(included_file_name)
        except OSError:
            continue
        try:
            included_src = src_file.read()
        finally:
            src_file.close()
        result[included_file_name] = None
        checksum = new_hash()
        update_checksum(checksum, included_src)
        _inner(included_src)
        result[included_file_name] = (os.stat(included_file_name).st_mtime, checksum.hexdigest())
        found = True
        break
if not found:
    pass","break statement is executed:None
break statement is not executed:zejun1"
labelImg,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/labelImg/libs/create_ml_io.py,https://github.com/tzutalin/labelImg/tree/master/libs/create_ml_io.py,CreateMLWriter,write$25,"def write(self):
        if os.path.isfile(self.output_file):
            with open(self.output_file, ""r"") as file:
                input_data = file.read()
                output_dict = json.loads(input_data)
        else:
            output_dict = []

        output_image_dict = {
            ""image"": self.filename,
            ""annotations"": []
        }

        for shape in self.shapes:
            points = shape[""points""]

            x1 = points[0][0]
            y1 = points[0][1]
            x2 = points[1][0]
            y2 = points[2][1]

            height, width, x, y = self.calculate_coordinates(x1, x2, y1, y2)

            shape_dict = {
                ""label"": shape[""label""],
                ""coordinates"": {
                    ""x"": x,
                    ""y"": y,
                    ""width"": width,
                    ""height"": height
                }
            }
            output_image_dict[""annotations""].append(shape_dict)

        # check if image already in output
        exists = False
        for i in range(0, len(output_dict)):
            if output_dict[i][""image""] == output_image_dict[""image""]:
                exists = True
                output_dict[i] = output_image_dict
                break

        if not exists:
            output_dict.append(output_image_dict)

        Path(self.output_file).write_text(json.dumps(output_dict), ENCODE_METHOD)","for i in range(0, len(output_dict)):
    if output_dict[i]['image'] == output_image_dict['image']:
        exists = True
        output_dict[i] = output_image_dict
        break
if not exists:
    output_dict.append(output_image_dict)","for i in range(0, len(output_dict)):
    if output_dict[i]['image'] == output_image_dict['image']:
        output_dict[i] = output_image_dict
        break
else:
    output_dict.append(output_image_dict)","for i in range(0, len(output_dict)):
    if output_dict[i]['image'] == output_image_dict['image']:
        output_dict[i] = output_image_dict
        break
else:
    output_dict.append(output_image_dict)",1,"for i in range(0, len(output_dict)):
    if output_dict[i]['image'] == output_image_dict['image']:
        exists = True
        output_dict[i] = output_image_dict
        break
if not exists:
    output_dict.append(output_image_dict)","break statement is executed:None
break statement is not executed:zejun1"
allennlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/allennlp/allennlp/modules/transformer/transformer_module.py,https://github.com/allenai/allennlp/tree/master/allennlp/modules/transformer/transformer_module.py,TransformerModule,_get_relevant_submodule_state$90,"def _get_relevant_submodule_state(
        cls,
        state_dict: StateDictType,
        relevant_module: Optional[Union[str, List[str]]] = None,
    ) -> StateDictType:
        """"""
        Returns the relevant part of the `state_dict`.
        """"""
        relevant_modules: Optional[List[str]] = None
        if relevant_module:
            relevant_modules = (
                [relevant_module] if isinstance(relevant_module, str) else relevant_module
            )
        elif isinstance(cls._pretrained_relevant_module, str):
            relevant_modules = [cls._pretrained_relevant_module]
        elif isinstance(cls._pretrained_relevant_module, list):
            relevant_modules = cls._pretrained_relevant_module

        if relevant_modules:
            found = False
            for module_name in relevant_modules:
                relevant_keys = set(
                    [key for key in state_dict.keys() if key.startswith(module_name + ""."")]
                )
                if relevant_keys:
                    # Only keep elements of state dict that correspond to the relevant module.
                    state_dict = {
                        key.replace(module_name + ""."", """", 1): value
                        for key, value in state_dict.items()
                        if key in relevant_keys
                    }
                    found = True
                    break

            if not found:
                warnings.warn(
                    f""{relevant_modules} was not found at top level of state_dict!"", UserWarning
                )

        return state_dict","for module_name in relevant_modules:
    relevant_keys = set([key for key in state_dict.keys() if key.startswith(module_name + '.')])
    if relevant_keys:
        state_dict = {key.replace(module_name + '.', '', 1): value for (key, value) in state_dict.items() if key in relevant_keys}
        found = True
        break
if not found:
    warnings.warn(f'{relevant_modules} was not found at top level of state_dict!', UserWarning)","for module_name in relevant_modules:
    relevant_keys = set([key for key in state_dict.keys() if key.startswith(module_name + '.')])
    if relevant_keys:
        state_dict = {key.replace(module_name + '.', '', 1): value for (key, value) in state_dict.items() if key in relevant_keys}
        break
else:
    warnings.warn(f'{relevant_modules} was not found at top level of state_dict!', UserWarning)","for module_name in relevant_modules:
    relevant_keys = set([key for key in state_dict.keys() if key.startswith(module_name + '.')])
    if relevant_keys:
        state_dict = {key.replace(module_name + '.', '', 1): value for (key, value) in state_dict.items() if key in relevant_keys}
        break
else:
    warnings.warn(f'{relevant_modules} was not found at top level of state_dict!', UserWarning)",1,"for module_name in relevant_modules:
    relevant_keys = set([key for key in state_dict.keys() if key.startswith(module_name + '.')])
    if relevant_keys:
        state_dict = {key.replace(module_name + '.', '', 1): value for (key, value) in state_dict.items() if key in relevant_keys}
        found = True
        break
if not found:
    warnings.warn(f'{relevant_modules} was not found at top level of state_dict!', UserWarning)","break statement is executed:None
break statement is not executed:zejun1"
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/mobilebert/tokenization_mobilebert.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/mobilebert/tokenization_mobilebert.py,WordpieceTokenizer,tokenize$458,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/mobilebert/tokenization_mobilebert.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/mobilebert/tokenization_mobilebert.py,WordpieceTokenizer,tokenize$458,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/tvdb_async/tvdb.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/tvdb_async/tvdb.py,TVDB,process_search$278,"def process_search(self, *args):
        search_dict = {}
        result = args[-1]
        srch = args[1]
        eps = args[2]
        exact_found = False
        soup = BeautifulSoup(result.html, 'html.parser')
        info_list = []
        lang = None
        title_text = None
        title_link = None
        tvdb_id = None
        status = None
        original_network = None
        start_date = None
        end_date = None
        min_val = None
        min_dict = {'final':None}
        min_index = 0
        for i, tr in enumerate(soup.findAll('tr')):
            for j, td in enumerate(tr.findAll('td')):
                if j == 0:
                    lang = td.text
                elif j == 1:
                    if 'href' in str(td):
                        txt = td.text
                        if txt.lower() == srch.lower():
                            exact_found = True
                        else:
                            dist = self.levenshtein(txt.lower(), srch.lower())
                            if not min_val:
                                min_val = dist
                                min_index = i
                            else:
                                if min_val > dist:
                                    min_val = dist
                                    min_index = i
                        href = td.find('a')['href']
                        title_text = txt
                        title_link = self.base_url + href
                elif j == 2:
                    tvdb_id = td.text
                elif j == 3:
                    status = td.text
                elif j == 4:
                    original_network = td.text
                elif j == 5:
                    start_date = td.text
                elif j == 6:
                    end_date = td.text
            arg_list = [
                title_text, title_link, tvdb_id, status,
                original_network, start_date, end_date, lang
                ]
            if exact_found:
                search_dict.clear()
                search_dict.update({0:arg_list.copy()})
                break
            elif title_link is None:
                pass
            else:
                search_dict.update({i:arg_list.copy()})
                if min_index == i:
                    min_dict.clear()
                    min_dict = {i:arg_list.copy()}
        if exact_found:
            return search_dict, self.search_and_grab
        else:
            return min_dict, self.search_and_grab","for (i, tr) in enumerate(soup.findAll('tr')):
    for (j, td) in enumerate(tr.findAll('td')):
        if j == 0:
            lang = td.text
        elif j == 1:
            if 'href' in str(td):
                txt = td.text
                if txt.lower() == srch.lower():
                    exact_found = True
                else:
                    dist = self.levenshtein(txt.lower(), srch.lower())
                    if not min_val:
                        min_val = dist
                        min_index = i
                    elif min_val > dist:
                        min_val = dist
                        min_index = i
                href = td.find('a')['href']
                title_text = txt
                title_link = self.base_url + href
        elif j == 2:
            tvdb_id = td.text
        elif j == 3:
            status = td.text
        elif j == 4:
            original_network = td.text
        elif j == 5:
            start_date = td.text
        elif j == 6:
            end_date = td.text
    arg_list = [title_text, title_link, tvdb_id, status, original_network, start_date, end_date, lang]
    if exact_found:
        search_dict.clear()
        search_dict.update({0: arg_list.copy()})
        break
    elif title_link is None:
        pass
    else:
        search_dict.update({i: arg_list.copy()})
        if min_index == i:
            min_dict.clear()
            min_dict = {i: arg_list.copy()}
if exact_found:
    return (search_dict, self.search_and_grab)
else:
    return (min_dict, self.search_and_grab)","for (i, tr) in enumerate(soup.findAll('tr')):
    for (j, td) in enumerate(tr.findAll('td')):
        if j == 0:
            lang = td.text
        elif j == 1:
            if 'href' in str(td):
                txt = td.text
                if txt.lower() == srch.lower():
                    exact_found = True
                else:
                    dist = self.levenshtein(txt.lower(), srch.lower())
                    if not min_val:
                        min_val = dist
                        min_index = i
                    elif min_val > dist:
                        min_val = dist
                        min_index = i
                href = td.find('a')['href']
                title_text = txt
                title_link = self.base_url + href
        elif j == 2:
            tvdb_id = td.text
        elif j == 3:
            status = td.text
        elif j == 4:
            original_network = td.text
        elif j == 5:
            start_date = td.text
        elif j == 6:
            end_date = td.text
    arg_list = [title_text, title_link, tvdb_id, status, original_network, start_date, end_date, lang]
    if exact_found:
        search_dict.clear()
        search_dict.update({0: arg_list.copy()})
        return (min_dict, self.search_and_grab)
        break
    elif title_link is None:
        pass
    else:
        search_dict.update({i: arg_list.copy()})
        if min_index == i:
            min_dict.clear()
            min_dict = {i: arg_list.copy()}
else:
    return (search_dict, self.search_and_grab)","for (i, tr) in enumerate(soup.findAll('tr')):
    for (j, td) in enumerate(tr.findAll('td')):
        if j == 0:
            lang = td.text
        elif j == 1:
            if 'href' in str(td):
                txt = td.text
                if txt.lower() == srch.lower():
                    exact_found = True
                else:
                    dist = self.levenshtein(txt.lower(), srch.lower())
                    if not min_val:
                        min_val = dist
                        min_index = i
                    elif min_val > dist:
                        min_val = dist
                        min_index = i
                href = td.find('a')['href']
                title_text = txt
                title_link = self.base_url + href
        elif j == 2:
            tvdb_id = td.text
        elif j == 3:
            status = td.text
        elif j == 4:
            original_network = td.text
        elif j == 5:
            start_date = td.text
        elif j == 6:
            end_date = td.text
    arg_list = [title_text, title_link, tvdb_id, status, original_network, start_date, end_date, lang]
    if exact_found:
        search_dict.clear()
        search_dict.update({0: arg_list.copy()})
        return (search_dict, self.search_and_grab)
        break
    elif title_link is None:
        pass
    else:
        search_dict.update({i: arg_list.copy()})
        if min_index == i:
            min_dict.clear()
            min_dict = {i: arg_list.copy()}
else:
    return (min_dict, self.search_and_grab)",0,"for (i, tr) in enumerate(soup.findAll('tr')):
    for (j, td) in enumerate(tr.findAll('td')):
        if j == 0:
            lang = td.text
        elif j == 1:
            if 'href' in str(td):
                txt = td.text
                if txt.lower() == srch.lower():
                    exact_found = True
                else:
                    dist = self.levenshtein(txt.lower(), srch.lower())
                    if not min_val:
                        min_val = dist
                        min_index = i
                    elif min_val > dist:
                        min_val = dist
                        min_index = i
                href = td.find('a')['href']
                title_text = txt
                title_link = self.base_url + href
        elif j == 2:
            tvdb_id = td.text
        elif j == 3:
            status = td.text
        elif j == 4:
            original_network = td.text
        elif j == 5:
            start_date = td.text
        elif j == 6:
            end_date = td.text
    arg_list = [title_text, title_link, tvdb_id, status, original_network, start_date, end_date, lang]
    if exact_found:
        search_dict.clear()
        search_dict.update({0: arg_list.copy()})
        break
    elif title_link is None:
        pass
    else:
        search_dict.update({i: arg_list.copy()})
        if min_index == i:
            min_dict.clear()
            min_dict = {i: arg_list.copy()}
if exact_found:
    return (search_dict, self.search_and_grab)
else:
    return (min_dict, self.search_and_grab)","break statement is executed:None
break statement is not executed:zejun1"
anchore-engine,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anchore-engine/anchore_engine/services/analyzer/analysis.py,https://github.com/anchore/anchore-engine/tree/master/anchore_engine/services/analyzer/analysis.py,,import_to_policy_engine$195,"def import_to_policy_engine(account: str, image_id: str, image_digest: str):
    """"""
    Import the given image into the policy engine

    :param account:
    :param image_id:
    :param image_digest:
    :return:
    """"""
    if image_id is None:
        raise ValueError(""image_id must not be None"")

    pe_client = internal_client_for(PolicyEngineClient, account)

    try:
        logger.debug(
            ""clearing any existing image record in policy engine: {} / {} / {}"".format(
                account, image_id, image_digest
            )
        )
        rc = pe_client.delete_image(user_id=account, image_id=image_id)
    except Exception as err:
        logger.warn(""exception on pre-delete - exception: "" + str(err))

    client_success = False
    last_exception = None

    # TODO: rework this wait logic using 'retrying.retry()' decorator on this whole function to allow new client on each call
    for retry_wait in [1, 3, 5, 0]:
        try:
            logger.info(
                ""loading image into policy engine: account={} image_id={} image_digest={}"".format(
                    account, image_id, image_digest
                )
            )
            image_analysis_fetch_url = build_catalog_url(account, image_digest)
            logger.debug(
                ""policy engine request catalog content url: "" + image_analysis_fetch_url
            )
            resp = pe_client.ingress_image(account, image_id, image_analysis_fetch_url)
            logger.debug(""policy engine image add response: "" + str(resp))
            client_success = True
            break
            # TODO: add a vuln eval and policy eval (with active policy), here to prime any caches since this isn't a highly latency sensitive code section
        except Exception as e:
            logger.warn(""attempt failed, will retry - exception: {}"".format(e))
            last_exception = e
            time.sleep(retry_wait)

    if not client_success:
        raise last_exception

    return True","for retry_wait in [1, 3, 5, 0]:
    try:
        logger.info('loading image into policy engine: account={} image_id={} image_digest={}'.format(account, image_id, image_digest))
        image_analysis_fetch_url = build_catalog_url(account, image_digest)
        logger.debug('policy engine request catalog content url: ' + image_analysis_fetch_url)
        resp = pe_client.ingress_image(account, image_id, image_analysis_fetch_url)
        logger.debug('policy engine image add response: ' + str(resp))
        client_success = True
        break
    except Exception as e:
        logger.warn('attempt failed, will retry - exception: {}'.format(e))
        last_exception = e
        time.sleep(retry_wait)
if not client_success:
    raise last_exception","for retry_wait in [1, 3, 5, 0]:
    try:
        logger.info('loading image into policy engine: account={} image_id={} image_digest={}'.format(account, image_id, image_digest))
        image_analysis_fetch_url = build_catalog_url(account, image_digest)
        logger.debug('policy engine request catalog content url: ' + image_analysis_fetch_url)
        resp = pe_client.ingress_image(account, image_id, image_analysis_fetch_url)
        logger.debug('policy engine image add response: ' + str(resp))
        break
    except Exception as e:
        logger.warn('attempt failed, will retry - exception: {}'.format(e))
        last_exception = e
        time.sleep(retry_wait)
else:
    raise last_exception","for retry_wait in [1, 3, 5, 0]:
    try:
        logger.info('loading image into policy engine: account={} image_id={} image_digest={}'.format(account, image_id, image_digest))
        image_analysis_fetch_url = build_catalog_url(account, image_digest)
        logger.debug('policy engine request catalog content url: ' + image_analysis_fetch_url)
        resp = pe_client.ingress_image(account, image_id, image_analysis_fetch_url)
        logger.debug('policy engine image add response: ' + str(resp))
        break
    except Exception as e:
        logger.warn('attempt failed, will retry - exception: {}'.format(e))
        last_exception = e
        time.sleep(retry_wait)
else:
    raise last_exception",1,"for retry_wait in [1, 3, 5, 0]:
    try:
        logger.info('loading image into policy engine: account={} image_id={} image_digest={}'.format(account, image_id, image_digest))
        image_analysis_fetch_url = build_catalog_url(account, image_digest)
        logger.debug('policy engine request catalog content url: ' + image_analysis_fetch_url)
        resp = pe_client.ingress_image(account, image_id, image_analysis_fetch_url)
        logger.debug('policy engine image add response: ' + str(resp))
        client_success = True
        break
    except Exception as e:
        logger.warn('attempt failed, will retry - exception: {}'.format(e))
        last_exception = e
        time.sleep(retry_wait)
if not client_success:
    raise last_exception","break statement is executed:None
break statement is not executed:zejun1"
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/utils/dateutils.py,https://github.com/frappe/frappe/tree/master/frappe/utils/dateutils.py,,parse_date$48,"def parse_date(date):
	""""""tries to parse given date to system's format i.e. yyyy-mm-dd. returns a string""""""
	parsed_date = None

	if "" "" in date:
		# as date-timestamp, remove the time part
		date = date.split("" "")[0]

	# why the sorting? checking should be done in a predictable order
	check_formats = [None] + sorted(
		list(dateformats), reverse=not get_user_date_format().startswith(""dd"")
	)

	for f in check_formats:
		try:
			parsed_date = user_to_str(date, f)
			if parsed_date:
				break
		except ValueError:
			pass

	if not parsed_date:
		raise Exception(
			""""""Cannot understand date - '%s'.
			Try formatting it like your default format - '%s'""""""
			% (date, get_user_date_format())
		)

	return parsed_date","for f in check_formats:
    try:
        parsed_date = user_to_str(date, f)
        if parsed_date:
            break
    except ValueError:
        pass
if not parsed_date:
    raise Exception(""Cannot understand date - '%s'.\n\t\t\tTry formatting it like your default format - '%s'"" % (date, get_user_date_format()))","for f in check_formats:
    try:
        parsed_date = user_to_str(date, f)
        if parsed_date:
            break
    except ValueError:
        pass
else:
    raise Exception(""Cannot understand date - '%s'.\n\t\t\tTry formatting it like your default format - '%s'"" % (date, get_user_date_format()))","for f in check_formats:
    try:
        parsed_date = user_to_str(date, f)
        if parsed_date:
            break
    except ValueError:
        pass
else:
    raise Exception(""Cannot understand date - '%s'.\n\t\t\tTry formatting it like your default format - '%s'"" % (date, get_user_date_format()))",1,"for f in check_formats:
    try:
        parsed_date = user_to_str(date, f)
        if parsed_date:
            break
    except ValueError:
        pass
if not parsed_date:
    raise Exception(""Cannot understand date - '%s'.\n\t\t\tTry formatting it like your default format - '%s'"" % (date, get_user_date_format()))","break statement is executed:None
break statement is not executed:zejun1"
lit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lit/lit_nlp/components/minimal_targeted_counterfactuals.py,https://github.com/PAIR-code/lit/tree/master/lit_nlp/components/minimal_targeted_counterfactuals.py,TabularMTC,_sort_and_filter_examples$624,"def _sort_and_filter_examples(self, examples: List[JsonDict],
                                ref_example: JsonDict, fields: List[Text],
                                dataset: lit_dataset.Dataset,
                                dataset_name: Text) -> List[JsonDict]:
    # Keep only those examples which field values are different from the
    # reference example.
    filtered_examples = []
    for example in examples:
      should_keep = True
      for field in fields:
        if example[field] == ref_example[field]:
          should_keep = False
          break
      if should_keep:
        filtered_examples.append(example)

    if not filtered_examples:
      return []

    # Deduplicate examples.
    dedup_hashes = set()
    dedup_examples = []
    for example in filtered_examples:
      h = self._create_hash(example, fields)
      if h not in dedup_hashes:
        dedup_examples.append(example)
        dedup_hashes.add(h)

    if len(dedup_examples) > MAX_EXAMPLES_PER_COMBINATION:
      dedup_examples = dedup_examples[:MAX_EXAMPLES_PER_COMBINATION]

    # Calculate distances with respect to the reference example taking into
    # consideration only the given fields.
    distances = []  # type: List[float]
    for example in dedup_examples:
      distance, _ = self._calculate_L1_distance(
          example_1=example,
          example_2=ref_example,
          dataset=dataset,
          dataset_name=dataset_name,
          field_names=fields)
      distances.append(distance)

    # Sort the filtered examples based on the distances.
    sorted_tuples = list(
        zip(*sorted(zip(dedup_examples, distances), key=lambda e: e[1])))[0]
    return list(sorted_tuples)","for field in fields:
    if example[field] == ref_example[field]:
        should_keep = False
        break
if should_keep:
    filtered_examples.append(example)","for field in fields:
    if example[field] == ref_example[field]:
        break
else:
    filtered_examples.append(example)","for field in fields:
    if example[field] == ref_example[field]:
        break
else:
    filtered_examples.append(example)",1,"for field in fields:
    if example[field] == ref_example[field]:
        should_keep = False
        break
if should_keep:
    filtered_examples.append(example)","break statement is executed:None
break statement is not executed:zejun1"
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/codestyle/docstring_checker.py,https://github.com/PaddlePaddle/Paddle/tree/master/tools/codestyle/docstring_checker.py,DocstringChecker,with_returns$299,"def with_returns(self, node, doc):
        """"""with_returns checks if docstring comments what are returned .
        Args:
            node (astroid.node): the node is visiting.
            doc (Docstring): Docstring object.
        Returns:
            True if successful otherwise False.
        """"""

        if node.name.startswith(""__"") or node.name.startswith(""_""):
            return True
        find = False
        for t in node.body:
            if not isinstance(t, astroid.Return):
                continue

            find = True
            break

        if not find:
            return True

        if len(doc.get_returns()) == 0:
            self.add_message('W9007', node=node, line=node.fromlineno)
            return False

        return True","for t in node.body:
    if not isinstance(t, astroid.Return):
        continue
    find = True
    break
if not find:
    return True","for t in node.body:
    if not isinstance(t, astroid.Return):
        continue
    break
else:
    return True","for t in node.body:
    if not isinstance(t, astroid.Return):
        continue
    break
else:
    return True",1,"for t in node.body:
    if not isinstance(t, astroid.Return):
        continue
    find = True
    break
if not find:
    return True","break statement is executed:None
break statement is not executed:zejun1"
bertviz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/transformers_neuron_view/tokenization_bert.py,https://github.com/jessevig/bertviz/tree/master/bertviz/transformers_neuron_view/tokenization_bert.py,WordpieceTokenizer,tokenize$363,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
bertviz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/transformers_neuron_view/tokenization_bert.py,https://github.com/jessevig/bertviz/tree/master/bertviz/transformers_neuron_view/tokenization_bert.py,WordpieceTokenizer,tokenize$363,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
CellProfiler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CellProfiler/tests/test_nowx.py,https://github.com/CellProfiler/CellProfiler/tree/master/tests/test_nowx.py,TestNoWX,test_01_06_run_pipeline$78,"def test_01_06_run_pipeline(self):
        import cellprofiler_core.pipeline as cpp
        import cellprofiler_core.module as cpm

        def callback(caller, event):
            self.assertFalse(isinstance(event, (cpp.event.LoadException,
                                                cpp.event.RunException)))
        pipeline = cpp.Pipeline()
        pipeline.add_listener(callback)
        fly_pipe = get_test_resources_directory(""../ExampleFlyURL.cppipe"")
        pipeline.load(fly_pipe)
        while True:
            removed_something = False
            for module in reversed(pipeline.modules()):
                self.assertTrue(isinstance(module, cpm.Module))
                if module.module_name in (""SaveImages"",
                                          ""CalculateStatistics"",
                                          ""ExportToSpreadsheet"",
                                          ""ExportToDatabase""):
                    pipeline.remove_module(module.module_num)
                    removed_something = True
                    break
            if not removed_something:
                break
        for module in pipeline.modules():
            module.show_window = False
        m = pipeline.run(image_set_end=1)","for module in reversed(pipeline.modules()):
    self.assertTrue(isinstance(module, cpm.Module))
    if module.module_name in ('SaveImages', 'CalculateStatistics', 'ExportToSpreadsheet', 'ExportToDatabase'):
        pipeline.remove_module(module.module_num)
        removed_something = True
        break
if not removed_something:
    break","for module in reversed(pipeline.modules()):
    self.assertTrue(isinstance(module, cpm.Module))
    if module.module_name in ('SaveImages', 'CalculateStatistics', 'ExportToSpreadsheet', 'ExportToDatabase'):
        pipeline.remove_module(module.module_num)
        break
else:
    break","for module in reversed(pipeline.modules()):
    self.assertTrue(isinstance(module, cpm.Module))
    if module.module_name in ('SaveImages', 'CalculateStatistics', 'ExportToSpreadsheet', 'ExportToDatabase'):
        pipeline.remove_module(module.module_num)
        break
else:
    break",1,"for module in reversed(pipeline.modules()):
    self.assertTrue(isinstance(module, cpm.Module))
    if module.module_name in ('SaveImages', 'CalculateStatistics', 'ExportToSpreadsheet', 'ExportToDatabase'):
        pipeline.remove_module(module.module_num)
        removed_something = True
        break
if not removed_something:
    break","break statement is executed:None
break statement is not executed:zejun1"
roberta_zh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/roberta_zh/create_pretraining_data.py,https://github.com/brightmart/roberta_zh/tree/master//create_pretraining_data.py,,get_new_segment$250,"def get_new_segment(segment): #   ####
    """"""
    : ask(""#"")
    :param segment: 
    :return: 
    """"""
    seq_cws = jieba.lcut("""".join(segment))
    seq_cws_dict = {x: 1 for x in seq_cws}
    new_segment = []
    i = 0
    while i < len(segment):
        if len(re.findall('[\u4E00-\u9FA5]', segment[i]))==0: # 
            new_segment.append(segment[i])
            i += 1
            continue

        has_add = False
        for length in range(3,0,-1):
            if i+length>len(segment):
                continue
            if ''.join(segment[i:i+length]) in seq_cws_dict:
                new_segment.append(segment[i])
                for l in range(1, length):
                    new_segment.append('##' + segment[i+l])
                i += length
                has_add = True
                break
        if not has_add:
            new_segment.append(segment[i])
            i += 1
    return new_segment","for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        has_add = True
        break
if not has_add:
    new_segment.append(segment[i])
    i += 1","for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        break
else:
    new_segment.append(segment[i])
    i += 1","for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        break
else:
    new_segment.append(segment[i])
    i += 1",1,"for length in range(3, 0, -1):
    if i + length > len(segment):
        continue
    if ''.join(segment[i:i + length]) in seq_cws_dict:
        new_segment.append(segment[i])
        for l in range(1, length):
            new_segment.append('##' + segment[i + l])
        i += length
        has_add = True
        break
if not has_add:
    new_segment.append(segment[i])
    i += 1","break statement is executed:None
break statement is not executed:zejun1"
AIGames,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AIGames/AIPacman/Algorithm_1/Algorithm_1_v1/gameAPI/game.py,https://github.com/CharlesPikachu/AIGames/tree/master/AIPacman/Algorithm_1/Algorithm_1_v1/gameAPI/game.py,GamePacmanAgent,runGame$147,"def runGame(self):
		clock = pygame.time.Clock()
		is_win = False
		while True:
			for event in pygame.event.get():
				if event.type == pygame.QUIT:
					sys.exit(-1)
					pygame.quit()
			pressed_keys = pygame.key.get_pressed()
			if pressed_keys[pygame.K_UP]:
				self.pacman_sprites.update([0, -1], self.wall_sprites, None)
			elif pressed_keys[pygame.K_DOWN]:
				self.pacman_sprites.update([0, 1], self.wall_sprites, None)
			elif pressed_keys[pygame.K_LEFT]:
				self.pacman_sprites.update([-1, 0], self.wall_sprites, None)
			elif pressed_keys[pygame.K_RIGHT]:
				self.pacman_sprites.update([1, 0], self.wall_sprites, None)
			for pacman in self.pacman_sprites:
				food_eaten = pygame.sprite.spritecollide(pacman, self.food_sprites, True)
				capsule_eaten = pygame.sprite.spritecollide(pacman, self.capsule_sprites, True)
			nonscared_ghost_sprites = pygame.sprite.Group()
			dead_ghost_sprites = pygame.sprite.Group()
			for ghost in self.ghost_sprites:
				if ghost.is_scared:
					if pygame.sprite.spritecollide(ghost, self.pacman_sprites, False):
						self.score += 6
						dead_ghost_sprites.add(ghost)
				else:
					nonscared_ghost_sprites.add(ghost)
			for ghost in dead_ghost_sprites:
				ghost.reset()
			self.score += len(food_eaten) * 2
			self.score += len(capsule_eaten) * 3
			if len(capsule_eaten) > 0:
				for ghost in self.ghost_sprites:
					ghost.is_scared = True
			self.ghost_sprites.update(self.wall_sprites, None, self.config.ghost_action_method, self.pacman_sprites)
			self.screen.fill(self.config.BLACK)
			self.wall_sprites.draw(self.screen)
			self.food_sprites.draw(self.screen)
			self.capsule_sprites.draw(self.screen)
			self.pacman_sprites.draw(self.screen)
			self.ghost_sprites.draw(self.screen)
			# show the score
			text = self.font.render('SCORE: %s' % self.score, True, self.config.WHITE)
			self.screen.blit(text, (2, 2))
			# judge whether game over
			if len(self.food_sprites) == 0 and len(self.capsule_sprites) == 0:
				is_win = True
				break
			if pygame.sprite.groupcollide(self.pacman_sprites, nonscared_ghost_sprites, False, False):
				is_win = False
				break
			pygame.display.flip()
			clock.tick(10)
		if is_win:
			self.__showText(msg='You won!', position=(self.screen_width//2-50, int(self.screen_height/2.5)))
		else:
			self.__showText(msg='Game Over!', position=(self.screen_width//2-80, int(self.screen_height/2.5)))","while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            sys.exit(-1)
            pygame.quit()
    pressed_keys = pygame.key.get_pressed()
    if pressed_keys[pygame.K_UP]:
        self.pacman_sprites.update([0, -1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_DOWN]:
        self.pacman_sprites.update([0, 1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_LEFT]:
        self.pacman_sprites.update([-1, 0], self.wall_sprites, None)
    elif pressed_keys[pygame.K_RIGHT]:
        self.pacman_sprites.update([1, 0], self.wall_sprites, None)
    for pacman in self.pacman_sprites:
        food_eaten = pygame.sprite.spritecollide(pacman, self.food_sprites, True)
        capsule_eaten = pygame.sprite.spritecollide(pacman, self.capsule_sprites, True)
    nonscared_ghost_sprites = pygame.sprite.Group()
    dead_ghost_sprites = pygame.sprite.Group()
    for ghost in self.ghost_sprites:
        if ghost.is_scared:
            if pygame.sprite.spritecollide(ghost, self.pacman_sprites, False):
                self.score += 6
                dead_ghost_sprites.add(ghost)
        else:
            nonscared_ghost_sprites.add(ghost)
    for ghost in dead_ghost_sprites:
        ghost.reset()
    self.score += len(food_eaten) * 2
    self.score += len(capsule_eaten) * 3
    if len(capsule_eaten) > 0:
        for ghost in self.ghost_sprites:
            ghost.is_scared = True
    self.ghost_sprites.update(self.wall_sprites, None, self.config.ghost_action_method, self.pacman_sprites)
    self.screen.fill(self.config.BLACK)
    self.wall_sprites.draw(self.screen)
    self.food_sprites.draw(self.screen)
    self.capsule_sprites.draw(self.screen)
    self.pacman_sprites.draw(self.screen)
    self.ghost_sprites.draw(self.screen)
    text = self.font.render('SCORE: %s' % self.score, True, self.config.WHITE)
    self.screen.blit(text, (2, 2))
    if len(self.food_sprites) == 0 and len(self.capsule_sprites) == 0:
        is_win = True
        break
    if pygame.sprite.groupcollide(self.pacman_sprites, nonscared_ghost_sprites, False, False):
        is_win = False
        break
    pygame.display.flip()
    clock.tick(10)
if is_win:
    self.__showText(msg='You won!', position=(self.screen_width // 2 - 50, int(self.screen_height / 2.5)))
else:
    self.__showText(msg='Game Over!', position=(self.screen_width // 2 - 80, int(self.screen_height / 2.5)))","while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            sys.exit(-1)
            pygame.quit()
    pressed_keys = pygame.key.get_pressed()
    if pressed_keys[pygame.K_UP]:
        self.pacman_sprites.update([0, -1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_DOWN]:
        self.pacman_sprites.update([0, 1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_LEFT]:
        self.pacman_sprites.update([-1, 0], self.wall_sprites, None)
    elif pressed_keys[pygame.K_RIGHT]:
        self.pacman_sprites.update([1, 0], self.wall_sprites, None)
    for pacman in self.pacman_sprites:
        food_eaten = pygame.sprite.spritecollide(pacman, self.food_sprites, True)
        capsule_eaten = pygame.sprite.spritecollide(pacman, self.capsule_sprites, True)
    nonscared_ghost_sprites = pygame.sprite.Group()
    dead_ghost_sprites = pygame.sprite.Group()
    for ghost in self.ghost_sprites:
        if ghost.is_scared:
            if pygame.sprite.spritecollide(ghost, self.pacman_sprites, False):
                self.score += 6
                dead_ghost_sprites.add(ghost)
        else:
            nonscared_ghost_sprites.add(ghost)
    for ghost in dead_ghost_sprites:
        ghost.reset()
    self.score += len(food_eaten) * 2
    self.score += len(capsule_eaten) * 3
    if len(capsule_eaten) > 0:
        for ghost in self.ghost_sprites:
            ghost.is_scared = True
    self.ghost_sprites.update(self.wall_sprites, None, self.config.ghost_action_method, self.pacman_sprites)
    self.screen.fill(self.config.BLACK)
    self.wall_sprites.draw(self.screen)
    self.food_sprites.draw(self.screen)
    self.capsule_sprites.draw(self.screen)
    self.pacman_sprites.draw(self.screen)
    self.ghost_sprites.draw(self.screen)
    text = self.font.render('SCORE: %s' % self.score, True, self.config.WHITE)
    self.screen.blit(text, (2, 2))
    if len(self.food_sprites) == 0 and len(self.capsule_sprites) == 0:
        is_win = True
        self.__showText(msg='You won!', position=(self.screen_width // 2 - 50, int(self.screen_height / 2.5)))
        break
    if pygame.sprite.groupcollide(self.pacman_sprites, nonscared_ghost_sprites, False, False):
        is_win = False
        self.__showText(msg='You won!', position=(self.screen_width // 2 - 50, int(self.screen_height / 2.5)))
        break
    pygame.display.flip()
    clock.tick(10)
else:
    self.__showText(msg='Game Over!', position=(self.screen_width // 2 - 80, int(self.screen_height / 2.5)))","while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            sys.exit(-1)
            pygame.quit()
    pressed_keys = pygame.key.get_pressed()
    if pressed_keys[pygame.K_UP]:
        self.pacman_sprites.update([0, -1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_DOWN]:
        self.pacman_sprites.update([0, 1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_LEFT]:
        self.pacman_sprites.update([-1, 0], self.wall_sprites, None)
    elif pressed_keys[pygame.K_RIGHT]:
        self.pacman_sprites.update([1, 0], self.wall_sprites, None)
    for pacman in self.pacman_sprites:
        food_eaten = pygame.sprite.spritecollide(pacman, self.food_sprites, True)
        capsule_eaten = pygame.sprite.spritecollide(pacman, self.capsule_sprites, True)
    nonscared_ghost_sprites = pygame.sprite.Group()
    dead_ghost_sprites = pygame.sprite.Group()
    for ghost in self.ghost_sprites:
        if ghost.is_scared:
            if pygame.sprite.spritecollide(ghost, self.pacman_sprites, False):
                self.score += 6
                dead_ghost_sprites.add(ghost)
        else:
            nonscared_ghost_sprites.add(ghost)
    for ghost in dead_ghost_sprites:
        ghost.reset()
    self.score += len(food_eaten) * 2
    self.score += len(capsule_eaten) * 3
    if len(capsule_eaten) > 0:
        for ghost in self.ghost_sprites:
            ghost.is_scared = True
    self.ghost_sprites.update(self.wall_sprites, None, self.config.ghost_action_method, self.pacman_sprites)
    self.screen.fill(self.config.BLACK)
    self.wall_sprites.draw(self.screen)
    self.food_sprites.draw(self.screen)
    self.capsule_sprites.draw(self.screen)
    self.pacman_sprites.draw(self.screen)
    self.ghost_sprites.draw(self.screen)
    text = self.font.render('SCORE: %s' % self.score, True, self.config.WHITE)
    self.screen.blit(text, (2, 2))
    if len(self.food_sprites) == 0 and len(self.capsule_sprites) == 0:
        self.__showText(msg='You won!', position=(self.screen_width // 2 - 50, int(self.screen_height / 2.5)))
        break
    if pygame.sprite.groupcollide(self.pacman_sprites, nonscared_ghost_sprites, False, False):
        self.__showText(msg='Game Over!', position=(self.screen_width // 2 - 80, int(self.screen_height / 2.5)))
        break
    pygame.display.flip()
    clock.tick(10)
else:
    self.__showText(msg='You won!', position=(self.screen_width // 2 - 50, int(self.screen_height / 2.5)))",0,"while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            sys.exit(-1)
            pygame.quit()
    pressed_keys = pygame.key.get_pressed()
    if pressed_keys[pygame.K_UP]:
        self.pacman_sprites.update([0, -1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_DOWN]:
        self.pacman_sprites.update([0, 1], self.wall_sprites, None)
    elif pressed_keys[pygame.K_LEFT]:
        self.pacman_sprites.update([-1, 0], self.wall_sprites, None)
    elif pressed_keys[pygame.K_RIGHT]:
        self.pacman_sprites.update([1, 0], self.wall_sprites, None)
    for pacman in self.pacman_sprites:
        food_eaten = pygame.sprite.spritecollide(pacman, self.food_sprites, True)
        capsule_eaten = pygame.sprite.spritecollide(pacman, self.capsule_sprites, True)
    nonscared_ghost_sprites = pygame.sprite.Group()
    dead_ghost_sprites = pygame.sprite.Group()
    for ghost in self.ghost_sprites:
        if ghost.is_scared:
            if pygame.sprite.spritecollide(ghost, self.pacman_sprites, False):
                self.score += 6
                dead_ghost_sprites.add(ghost)
        else:
            nonscared_ghost_sprites.add(ghost)
    for ghost in dead_ghost_sprites:
        ghost.reset()
    self.score += len(food_eaten) * 2
    self.score += len(capsule_eaten) * 3
    if len(capsule_eaten) > 0:
        for ghost in self.ghost_sprites:
            ghost.is_scared = True
    self.ghost_sprites.update(self.wall_sprites, None, self.config.ghost_action_method, self.pacman_sprites)
    self.screen.fill(self.config.BLACK)
    self.wall_sprites.draw(self.screen)
    self.food_sprites.draw(self.screen)
    self.capsule_sprites.draw(self.screen)
    self.pacman_sprites.draw(self.screen)
    self.ghost_sprites.draw(self.screen)
    text = self.font.render('SCORE: %s' % self.score, True, self.config.WHITE)
    self.screen.blit(text, (2, 2))
    if len(self.food_sprites) == 0 and len(self.capsule_sprites) == 0:
        is_win = True
        break
    if pygame.sprite.groupcollide(self.pacman_sprites, nonscared_ghost_sprites, False, False):
        is_win = False
        break
    pygame.display.flip()
    clock.tick(10)
if is_win:
    self.__showText(msg='You won!', position=(self.screen_width // 2 - 50, int(self.screen_height / 2.5)))
else:
    self.__showText(msg='Game Over!', position=(self.screen_width // 2 - 80, int(self.screen_height / 2.5)))","break statement is executed:Respond with which statement called zejun is executed if the break statement is executed
break statement is not executed:None"
DDParser,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DDParser/tools/representation/demo/ERNIE/tokenization.py,https://github.com/baidu/DDParser/tree/master/tools/representation/demo/ERNIE/tokenization.py,WordpieceTokenizer,tokenize$279,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
            input = ""unaffable""
            output = [""un"", ""##aff"", ""##able""]

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through `BasicTokenizer.

        Returns:
            A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)",1,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end
if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","break statement is executed:zejun1
break statement is not executed:None"
DDParser,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DDParser/tools/representation/demo/ERNIE/tokenization.py,https://github.com/baidu/DDParser/tree/master/tools/representation/demo/ERNIE/tokenization.py,WordpieceTokenizer,tokenize$279,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
            input = ""unaffable""
            output = [""un"", ""##aff"", ""##able""]

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through `BasicTokenizer.

        Returns:
            A list of wordpiece tokens.
        """"""

        text = convert_to_unicode(text)

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break",Cannot refactor,-1,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
if cur_substr is None:
    is_bad = True
    break","break statement is executed:None
break statement is not executed:zejun1"
mega.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mega.pytorch/mega_core/data/datasets/evaluation/cityscapes/eval_instances.py,https://github.com/Scalsol/mega.pytorch/tree/master/mega_core/data/datasets/evaluation/cityscapes/eval_instances.py,,evaluateMaskMatches$619,"def evaluateMaskMatches(matches, args):
    # In the end, we need two vectors for each class and for each overlap
    # The first vector (y_true) is binary and is 1, where the ground truth says true,
    # and is 0 otherwise.
    # The second vector (y_score) is float [0...1] and represents the confidence of
    # the prediction.
    #
    # We represent the following cases as:
    #                                       | y_true |   y_score
    #   gt instance with matched prediction |    1   | confidence
    #   gt instance w/o  matched prediction |    1   |     0.0
    #          false positive prediction    |    0   | confidence
    #
    # The current implementation makes only sense for an overlap threshold >= 0.5,
    # since only then, a single prediction can either be ignored or matched, but
    # never both. Further, it can never match to two gt instances.
    # For matching, we vary the overlap and do the following steps:
    #   1.) remove all predictions that satisfy the overlap criterion with an ignore region (either void or *group)
    #   2.) remove matches that do not satisfy the overlap
    #   3.) mark non-matched predictions as false positive

    # AP
    overlaps = args.overlaps
    # region size
    minRegionSizes = args.minRegionSizes

    # only keep the first, if distances are not available
    # if not args.distanceAvailable:
    #     minRegionSizes = [ minRegionSizes[0] ]
    #     distThs        = [ distThs       [0] ]
    #     distConfs      = [ distConfs     [0] ]

    # Here we hold the results
    # First dimension is class, second overlap
    ap = np.zeros((len(minRegionSizes), len(args.instLabels), len(overlaps)), np.float)

    for dI, minRegionSize in enumerate(minRegionSizes):
        for (oI, overlapTh) in enumerate(overlaps):
            for (lI, labelName) in enumerate(args.instLabels):
                y_true = np.empty(0)
                y_score = np.empty(0)
                # count hard false negatives
                hardFns = 0
                # found at least one gt and predicted instance?
                haveGt = False
                havePred = False

                for img in matches:
                    predInstances = img[""prediction""][labelName]
                    gtInstances = img[""groundTruth""][labelName]
                    # filter groups in ground truth
                    gtInstances = [
                        gt for gt in gtInstances if gt[""pixelCount""] >= minRegionSize
                    ]

                    if gtInstances:
                        haveGt = True
                    if predInstances:
                        havePred = True

                    curTrue = np.ones(len(gtInstances))
                    curScore = np.ones(len(gtInstances)) * (-float(""inf""))
                    curMatch = np.zeros(len(gtInstances), dtype=np.bool)

                    # collect matches
                    for (gtI, gt) in enumerate(gtInstances):
                        foundMatch = False
                        for pred in gt[""matchedPred""]:
                            overlap = float(pred[""maskIntersection""]) / (
                                gt[""pixelCount""]
                                + pred[""pixelCount""]
                                - pred[""maskIntersection""]
                            )
                            if overlap > overlapTh:
                                # the score
                                confidence = pred[""confidence""]

                                # if we already hat a prediction for this groundtruth
                                # the prediction with the lower score is automatically a false positive
                                if curMatch[gtI]:
                                    maxScore = max(curScore[gtI], confidence)
                                    minScore = min(curScore[gtI], confidence)
                                    curScore[gtI] = maxScore
                                    # append false positive
                                    curTrue = np.append(curTrue, 0)
                                    curScore = np.append(curScore, minScore)
                                    curMatch = np.append(curMatch, True)
                                # otherwise set score
                                else:
                                    foundMatch = True
                                    curMatch[gtI] = True
                                    curScore[gtI] = confidence

                        if not foundMatch:
                            hardFns += 1

                    # remove non-matched ground truth instances
                    curTrue = curTrue[curMatch == True]
                    curScore = curScore[curMatch == True]

                    # collect non-matched predictions as false positive
                    for pred in predInstances:
                        foundGt = False
                        for gt in pred[""matchedGt""]:
                            overlap = float(gt[""maskIntersection""]) / (
                                gt[""pixelCount""]
                                + pred[""pixelCount""]
                                - gt[""maskIntersection""]
                            )
                            if overlap > overlapTh:
                                foundGt = True
                                break
                        if not foundGt:
                            # collect number of void and *group pixels
                            nbIgnorePixels = 0
                            for gt in pred[""matchedGt""]:
                                # small ground truth instances
                                if gt[""pixelCount""] < minRegionSize:
                                    nbIgnorePixels += gt[""maskIntersection""]

                            if pred[""pixelCount""] <= 0:
                                proportionIgnore = 0
                            else:
                                proportionIgnore = (
                                    float(nbIgnorePixels) / pred[""pixelCount""]
                                )
                            # if not ignored
                            # append false positive
                            if proportionIgnore <= overlapTh:
                                curTrue = np.append(curTrue, 0)
                                confidence = pred[""confidence""]
                                curScore = np.append(curScore, confidence)

                    # append to overall results
                    y_true = np.append(y_true, curTrue)
                    y_score = np.append(y_score, curScore)

                # compute the average precision
                if haveGt and havePred:
                    # compute precision recall curve first

                    # sorting and cumsum
                    scoreArgSort = np.argsort(y_score)
                    yScoreSorted = y_score[scoreArgSort]
                    yTrueSorted = y_true[scoreArgSort]
                    yTrueSortedCumsum = np.cumsum(yTrueSorted)

                    # unique thresholds
                    (thresholds, uniqueIndices) = np.unique(
                        yScoreSorted, return_index=True
                    )

                    # since we need to add an artificial point to the precision-recall curve
                    # increase its length by 1
                    nbPrecRecall = len(uniqueIndices) + 1

                    # prepare precision recall
                    nbExamples = len(yScoreSorted)
                    nbTrueExamples = yTrueSortedCumsum[-1]
                    precision = np.zeros(nbPrecRecall)
                    recall = np.zeros(nbPrecRecall)

                    # deal with the first point
                    # only thing we need to do, is to append a zero to the cumsum at the end.
                    # an index of -1 uses that zero then
                    yTrueSortedCumsum = np.append(yTrueSortedCumsum, 0)

                    # deal with remaining
                    for idxRes, idxScores in enumerate(uniqueIndices):
                        cumSum = yTrueSortedCumsum[idxScores - 1]
                        tp = nbTrueExamples - cumSum
                        fp = nbExamples - idxScores - tp
                        fn = cumSum + hardFns
                        p = float(tp) / (tp + fp)
                        r = float(tp) / (tp + fn)
                        precision[idxRes] = p
                        recall[idxRes] = r

                    # first point in curve is artificial
                    precision[-1] = 1.0
                    recall[-1] = 0.0

                    # compute average of precision-recall curve
                    # integration is performed via zero order, or equivalently step-wise integration
                    # first compute the widths of each step:
                    # use a convolution with appropriate kernel, manually deal with the boundaries first
                    recallForConv = np.copy(recall)
                    recallForConv = np.append(recallForConv[0], recallForConv)
                    recallForConv = np.append(recallForConv, 0.0)

                    stepWidths = np.convolve(recallForConv, [-0.5, 0, 0.5], ""valid"")

                    # integrate is now simply a dot product
                    apCurrent = np.dot(precision, stepWidths)

                elif haveGt:
                    apCurrent = 0.0
                else:
                    apCurrent = float(""nan"")
                ap[dI, lI, oI] = apCurrent

    return ap","for gt in pred['matchedGt']:
    overlap = float(gt['maskIntersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['maskIntersection'])
    if overlap > overlapTh:
        foundGt = True
        break
if not foundGt:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['pixelCount'] < minRegionSize:
            nbIgnorePixels += gt['maskIntersection']
    if pred['pixelCount'] <= 0:
        proportionIgnore = 0
    else:
        proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","for gt in pred['matchedGt']:
    overlap = float(gt['maskIntersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['maskIntersection'])
    if overlap > overlapTh:
        break
else:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['pixelCount'] < minRegionSize:
            nbIgnorePixels += gt['maskIntersection']
    if pred['pixelCount'] <= 0:
        proportionIgnore = 0
    else:
        proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","for gt in pred['matchedGt']:
    overlap = float(gt['maskIntersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['maskIntersection'])
    if overlap > overlapTh:
        break
else:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['pixelCount'] < minRegionSize:
            nbIgnorePixels += gt['maskIntersection']
    if pred['pixelCount'] <= 0:
        proportionIgnore = 0
    else:
        proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)",1,"for gt in pred['matchedGt']:
    overlap = float(gt['maskIntersection']) / (gt['pixelCount'] + pred['pixelCount'] - gt['maskIntersection'])
    if overlap > overlapTh:
        foundGt = True
        break
if not foundGt:
    nbIgnorePixels = 0
    for gt in pred['matchedGt']:
        if gt['pixelCount'] < minRegionSize:
            nbIgnorePixels += gt['maskIntersection']
    if pred['pixelCount'] <= 0:
        proportionIgnore = 0
    else:
        proportionIgnore = float(nbIgnorePixels) / pred['pixelCount']
    if proportionIgnore <= overlapTh:
        curTrue = np.append(curTrue, 0)
        confidence = pred['confidence']
        curScore = np.append(curScore, confidence)","break statement is executed:None
break statement is not executed:zejun1"
ReAgent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/data/oss_data_fetcher.py,https://github.com/facebookresearch/ReAgent/tree/master/reagent/data/oss_data_fetcher.py,,upload_as_parquet$410,"def upload_as_parquet(df) -> Dataset:
    """"""Generate a random parquet. Fails if cannot generate a non-existent name.""""""

    # get a random tmp name and check if it exists
    sqlCtx = get_spark_session()
    success = False
    for _ in range(MAX_UPLOAD_PARQUET_TRIES):
        suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
        rand_name = f""tmp_parquet_{suffix}""
        if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
            success = True
            break
    if not success:
        raise Exception(f""Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries."")

    # perform the write
    # pyre-fixme[61]: `rand_name` may not be initialized here.
    df.write.mode(""errorifexists"").format(""parquet"").saveAsTable(rand_name)
    # pyre-fixme[61]: `rand_name` may not be initialized here.
    parquet_url = get_table_url(rand_name)
    logger.info(f""Saved parquet to {parquet_url}"")
    return Dataset(parquet_url=parquet_url)","for _ in range(MAX_UPLOAD_PARQUET_TRIES):
    suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
    rand_name = f'tmp_parquet_{suffix}'
    if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
        success = True
        break
if not success:
    raise Exception(f'Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries.')","for _ in range(MAX_UPLOAD_PARQUET_TRIES):
    suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
    rand_name = f'tmp_parquet_{suffix}'
    if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
        break
else:
    raise Exception(f'Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries.')","for _ in range(MAX_UPLOAD_PARQUET_TRIES):
    suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
    rand_name = f'tmp_parquet_{suffix}'
    if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
        break
else:
    raise Exception(f'Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries.')",1,"for _ in range(MAX_UPLOAD_PARQUET_TRIES):
    suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
    rand_name = f'tmp_parquet_{suffix}'
    if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
        success = True
        break
if not success:
    raise Exception(f'Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries.')","break statement is executed:None
break statement is not executed:zejun1"
open-event-server,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open-event-server/app/api/helpers/permission_manager.py,https://github.com/OpnTec/open-event-server/tree/master/app/api/helpers/permission_manager.py,,permission_manager$379,"def permission_manager(view, view_args, view_kwargs, *args, **kwargs):
    """"""The function use to check permissions

    :param callable view: the view
    :param list view_args: view args
    :param dict view_kwargs: view kwargs
    :param list args: decorator args
    :param dict kwargs: decorator kwargs
    """"""
    methods = 'GET,POST,DELETE,PATCH'

    if 'id' in kwargs:
        view_kwargs['id'] = kwargs['id']

    if kwargs.get('methods'):
        methods = kwargs['methods']

    if request.method not in methods:
        return view(*view_args, **view_kwargs)

    # leave_if checks if we have to bypass this request on the basis of lambda function
    if 'leave_if' in kwargs:
        check = kwargs['leave_if']
        if check(view_kwargs):
            return view(*view_args, **view_kwargs)

    # A check to ensure it is good to go ahead and check permissions
    if 'check' in kwargs:
        check = kwargs['check']
        if not check(view_kwargs):
            raise ForbiddenError({'source': ''}, 'Access forbidden')

    # For Orders API
    if 'order_identifier' in view_kwargs:
        try:
            order = Order.query.filter_by(
                identifier=view_kwargs['order_identifier']
            ).one()
        except NoResultFound:
            raise NotFoundError({'parameter': 'order_identifier'}, 'Order not found')
        view_kwargs['id'] = order.id

    # If event_identifier in route instead of event_id
    if 'event_identifier' in view_kwargs:
        try:
            event = Event.query.filter_by(
                identifier=view_kwargs['event_identifier']
            ).one()
        except NoResultFound:
            raise NotFoundError({'parameter': 'event_identifier'}, 'Event not found')
        view_kwargs['event_id'] = event.id

    if view_kwargs.get('event_invoice_identifier') is not None:
        try:
            event_invoice = EventInvoice.query.filter_by(
                identifier=view_kwargs['event_invoice_identifier']
            ).one()
        except NoResultFound:
            NotFoundError(
                {'parameter': 'event_invoice_identifier'}, 'Event Invoice not found'
            )
        view_kwargs['id'] = event_invoice.id

    # Only for events API
    if 'identifier' in view_kwargs:
        try:
            event = Event.query.filter_by(identifier=view_kwargs['identifier']).one()
        except NoResultFound:
            raise NotFoundError({'parameter': 'identifier'}, 'Event not found')
        view_kwargs['id'] = event.id

    if 'fetch' in kwargs:
        fetched = None
        if is_multiple(kwargs['fetch']):
            kwargs['fetch'] = [f.strip() for f in kwargs['fetch'].split("","")]
            for f in kwargs['fetch']:
                if f in view_kwargs:
                    fetched = view_kwargs.get(f)
                    break
        elif kwargs['fetch'] in view_kwargs:
            fetched = view_kwargs[kwargs['fetch']]
        if not fetched:
            model = kwargs['model']
            fetch = kwargs['fetch']
            fetch_key_url = 'id'
            fetch_key_model = 'id'
            if kwargs.get('fetch_key_url'):
                fetch_key_url = kwargs['fetch_key_url']

            if kwargs.get('fetch_key_model'):
                fetch_key_model = kwargs['fetch_key_model']

            if not is_multiple(model):
                model = [model]

            if isinstance(fetch_key_url, str) and is_multiple(fetch_key_url):
                fetch_key_url = fetch_key_url.split(  # pytype: disable=attribute-error
                    "",""
                )

            found = False
            for index, mod in enumerate(model):
                if is_multiple(fetch_key_url):
                    f_url = fetch_key_url[index].strip()
                else:
                    f_url = fetch_key_url
                if not view_kwargs.get(f_url):
                    continue
                try:
                    data = mod.query.filter(  # pytype: disable=attribute-error
                        getattr(mod, fetch_key_model) == view_kwargs[f_url]
                    ).one()
                except NoResultFound:
                    pass
                else:
                    found = True
                    break

            if not found:
                raise NotFoundError({'source': ''}, 'Object not found.')

            fetched = None
            if is_multiple(fetch):
                for f in fetch:
                    if hasattr(data, f):
                        fetched = getattr(data, f)
                        break
            else:
                fetched = getattr(data, fetch) if hasattr(data, fetch) else None

        if fetched:
            fetch_as = kwargs.get('fetch_as')
            fetch = kwargs.get('fetch')
            if fetch_as == fetch:
                logger.warning(
                    ""If 'fetch_as' is same as 'fetch', then it is redundant: %s"", fetch
                )
            if fetch_as:
                kwargs[fetch_as] = fetched
            elif fetch:
                kwargs[fetch] = fetched
        else:
            raise NotFoundError({'source': ''}, 'Object not found.')
    if args[0] in permissions:
        return permissions[args[0]](view, view_args, view_kwargs, *args, **kwargs)
    raise ForbiddenError({'source': ''}, 'Access forbidden')","for (index, mod) in enumerate(model):
    if is_multiple(fetch_key_url):
        f_url = fetch_key_url[index].strip()
    else:
        f_url = fetch_key_url
    if not view_kwargs.get(f_url):
        continue
    try:
        data = mod.query.filter(getattr(mod, fetch_key_model) == view_kwargs[f_url]).one()
    except NoResultFound:
        pass
    else:
        found = True
        break
if not found:
    raise NotFoundError({'source': ''}, 'Object not found.')","for (index, mod) in enumerate(model):
    if is_multiple(fetch_key_url):
        f_url = fetch_key_url[index].strip()
    else:
        f_url = fetch_key_url
    if not view_kwargs.get(f_url):
        continue
    try:
        data = mod.query.filter(getattr(mod, fetch_key_model) == view_kwargs[f_url]).one()
    except NoResultFound:
        pass
    else:
        break
else:
    raise NotFoundError({'source': ''}, 'Object not found.')","for (index, mod) in enumerate(model):
    if is_multiple(fetch_key_url):
        f_url = fetch_key_url[index].strip()
    else:
        f_url = fetch_key_url
    if not view_kwargs.get(f_url):
        continue
    try:
        data = mod.query.filter(getattr(mod, fetch_key_model) == view_kwargs[f_url]).one()
    except NoResultFound:
        pass
    else:
        break
else:
    raise NotFoundError({'source': ''}, 'Object not found.')",1,"for (index, mod) in enumerate(model):
    if is_multiple(fetch_key_url):
        f_url = fetch_key_url[index].strip()
    else:
        f_url = fetch_key_url
    if not view_kwargs.get(f_url):
        continue
    try:
        data = mod.query.filter(getattr(mod, fetch_key_model) == view_kwargs[f_url]).one()
    except NoResultFound:
        pass
    else:
        found = True
        break
if not found:
    raise NotFoundError({'source': ''}, 'Object not found.')","break statement is executed:None
break statement is not executed:zejun1"
Text-Pastry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Text-Pastry/text_pastry_selection.py,https://github.com/duydao/Text-Pastry/tree/master//text_pastry_selection.py,SelectionHelper,scroll_into_view$284,"def scroll_into_view(cls, view, regions):
        if regions and len(regions) > 0:
            # scroll to the first selection if no selections in viewport
            found_region = False
            visible_region = view.visible_region()
            for region in regions:
                if region.intersects(visible_region):
                    # we have found a selection in the visible region, do nothing
                    found_region = True
                    break
            if not found_region:
                view.show(regions[0], True)","for region in regions:
    if region.intersects(visible_region):
        found_region = True
        break
if not found_region:
    view.show(regions[0], True)","for region in regions:
    if region.intersects(visible_region):
        break
else:
    view.show(regions[0], True)","for region in regions:
    if region.intersects(visible_region):
        break
else:
    view.show(regions[0], True)",1,"for region in regions:
    if region.intersects(visible_region):
        found_region = True
        break
if not found_region:
    view.show(regions[0], True)","break statement is executed:None
break statement is not executed:zejun1"
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy_test/api/test_jobs.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy_test/api/test_jobs.py,JobsApiTestCase,test_index_state_filter$61,"def test_index_state_filter(self, history_id):
        # Initial number of ok jobs
        original_count = len(self.__uploads_with_state(""ok""))
        # Run through dataset upload to ensure num uplaods at least greater
        # by 1.
        self.__history_with_ok_dataset(history_id)

        # Verify number of ok jobs is actually greater.
        count_increased = False
        for _ in range(10):
            new_count = len(self.__uploads_with_state(""ok""))
            if original_count < new_count:
                count_increased = True
                break
            time.sleep(.1)

        if not count_increased:
            template = ""Jobs in ok state did not increase (was %d, now %d)""
            message = template % (original_count, new_count)
            raise AssertionError(message)","for _ in range(10):
    new_count = len(self.__uploads_with_state('ok'))
    if original_count < new_count:
        count_increased = True
        break
    time.sleep(0.1)
if not count_increased:
    template = 'Jobs in ok state did not increase (was %d, now %d)'
    message = template % (original_count, new_count)
    raise AssertionError(message)","for _ in range(10):
    new_count = len(self.__uploads_with_state('ok'))
    if original_count < new_count:
        break
    time.sleep(0.1)
else:
    template = 'Jobs in ok state did not increase (was %d, now %d)'
    message = template % (original_count, new_count)
    raise AssertionError(message)","for _ in range(10):
    new_count = len(self.__uploads_with_state('ok'))
    if original_count < new_count:
        break
    time.sleep(0.1)
else:
    template = 'Jobs in ok state did not increase (was %d, now %d)'
    message = template % (original_count, new_count)
    raise AssertionError(message)",1,"for _ in range(10):
    new_count = len(self.__uploads_with_state('ok'))
    if original_count < new_count:
        count_increased = True
        break
    time.sleep(0.1)
if not count_increased:
    template = 'Jobs in ok state did not increase (was %d, now %d)'
    message = template % (original_count, new_count)
    raise AssertionError(message)","break statement is executed:None
break statement is not executed:zejun1"
circus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/circus/circus/tests/test_arbiter.py,https://github.com/circus-tent/circus/tree/master/circus/tests/test_arbiter.py,TestTrainer,_test_udp_discovery$548,"def _test_udp_discovery(self):
        """"""test_udp_discovery: Test that when the circusd answer UDP call.

        """"""
        yield self._stop_runners()

        dummy_process = 'circus.tests.support.run_process'
        self._run_circus(dummy_process)

        ANY = '0.0.0.0'

        multicast_addr, multicast_port = urlparse(DEFAULT_ENDPOINT_MULTICAST)\
            .netloc.split(':')

        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM,
                             socket.IPPROTO_UDP)
        sock.bind((ANY, 0))
        sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 255)
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

        sock.sendto(json.dumps(''),
                    (multicast_addr, int(multicast_port)))

        timer = time()
        resp = False
        endpoints = []
        while time() - timer < 10:
            data, address = sock.recvfrom(1024)
            data = json.loads(data)
            endpoint = data.get('endpoint', """")

            if endpoint == DEFAULT_ENDPOINT_DEALER:
                resp = True
                break

            endpoints.append(endpoint)

        if not resp:
            print(endpoints)

        self.assertTrue(resp)","while time() - timer < 10:
    (data, address) = sock.recvfrom(1024)
    data = json.loads(data)
    endpoint = data.get('endpoint', '')
    if endpoint == DEFAULT_ENDPOINT_DEALER:
        resp = True
        break
    endpoints.append(endpoint)
if not resp:
    print(endpoints)","while time() - timer < 10:
    (data, address) = sock.recvfrom(1024)
    data = json.loads(data)
    endpoint = data.get('endpoint', '')
    if endpoint == DEFAULT_ENDPOINT_DEALER:
        resp = True
        break
    endpoints.append(endpoint)
else:
    print(endpoints)","while time() - timer < 10:
    (data, address) = sock.recvfrom(1024)
    data = json.loads(data)
    endpoint = data.get('endpoint', '')
    if endpoint == DEFAULT_ENDPOINT_DEALER:
        resp = True
        break
    endpoints.append(endpoint)
else:
    print(endpoints)",1,"while time() - timer < 10:
    (data, address) = sock.recvfrom(1024)
    data = json.loads(data)
    endpoint = data.get('endpoint', '')
    if endpoint == DEFAULT_ENDPOINT_DEALER:
        resp = True
        break
    endpoints.append(endpoint)
if not resp:
    print(endpoints)","break statement is executed:None
break statement is not executed:zejun1"
Jarvis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Jarvis/jarviscli/plugins/mips_conv.py,https://github.com/sukeesh/Jarvis/tree/master/jarviscli/plugins/mips_conv.py,MipsConverter,__getRegBin$234,"def __getRegBin(self, regR, regName, regCode, jarvis):
        regCntr = 0
        flag = False
        regBin = """"

        while (regCntr < len(regName)):
            if (regName[regCntr] == regR):
                flag = True
                break
            regCntr = regCntr + 1

        if (flag is False):
            jarvis.say(""Instruction is syntactically incorrect"")
        else:
            regBin = regCode[regCntr]

        return regBin","while regCntr < len(regName):
    if regName[regCntr] == regR:
        flag = True
        break
    regCntr = regCntr + 1
if flag is False:
    jarvis.say('Instruction is syntactically incorrect')
else:
    regBin = regCode[regCntr]","while regCntr < len(regName):
    if regName[regCntr] == regR:
        jarvis.say('Instruction is syntactically incorrect')
        break
    regCntr = regCntr + 1
else:
    regBin = regCode[regCntr]","while regCntr < len(regName):
    if regName[regCntr] == regR:
        regBin = regCode[regCntr]
        break
    regCntr = regCntr + 1
else:
    jarvis.say('Instruction is syntactically incorrect')",0,"while regCntr < len(regName):
    if regName[regCntr] == regR:
        flag = True
        break
    regCntr = regCntr + 1
if flag is False:
    jarvis.say('Instruction is syntactically incorrect')
else:
    regBin = regCode[regCntr]","break statement is executed:zejun1
break statement is not executed:None"
NOFOUND,,,,,,,,,,,
udocker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/udocker/tests/unit/test_fileutil.py,https://github.com/indigo-dc/udocker/tree/master/tests/unit/test_fileutil.py,,find_str$19,"def find_str(self, find_exp, where):
    """"""Find string in test output messages.""""""
    found = False
    for item in where:
        if find_exp in str(item):
            self.assertTrue(True)
            found = True
            break
    if not found:
        self.assertTrue(False)","for item in where:
    if find_exp in str(item):
        self.assertTrue(True)
        found = True
        break
if not found:
    self.assertTrue(False)","for item in where:
    if find_exp in str(item):
        self.assertTrue(True)
        break
else:
    self.assertTrue(False)",0,,,
sickbeard_mp4_automator,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sickbeard_mp4_automator/resources/metadata.py,https://github.com/mdhiggins/sickbeard_mp4_automator/tree/master/resources/metadata.py,Metadata,getArtwork$370,"def getArtwork(self, path, inputfile, thumbnail=False):
        # Check for artwork in the same directory as the source
        poster = None
        base, _ = os.path.splitext(inputfile)
        base2, _ = os.path.splitext(path)
        for b in [base, base2]:
            for e in valid_poster_extensions:
                path = b + os.extsep + e
                if (os.path.exists(path)):
                    poster = path
                    self.log.info(""Local artwork detected, using %s."" % path)
                    break
            if poster:
                break

        if not poster:
            d, f = os.path.split(path)
            for e in valid_poster_extensions:
                path = os.path.join(d, ""smaposter"" + os.extsep + e)
                if (os.path.exists(path)):
                    poster = path
                    self.log.info(""Local artwork detected, using %s."" % path)
                    break

        # If no local files are found, attempt to download them
        if not poster:
            poster_path = None
            if self.mediatype == MediaType.Movie:
                poster_path = self.moviedata.get('poster_path')
            elif self.mediatype == MediaType.TV:
                if thumbnail:
                    poster_path = self.episodedata.get('still_path')
                else:
                    poster_path = self.seasondata.get('poster_path')

                if not poster_path:
                    poster_path = self.showdata.get('poster_path')

            if not poster_path:
                self.log.debug(""No artwork found for media file."")
                return None

            savepath = os.path.join(tempfile.gettempdir(), ""poster-%s.jpg"" % (self.tmdbid))

            # Ensure the save path is clear
            if os.path.exists(savepath):
                try:
                    os.remove(savepath)
                except KeyboardInterrupt:
                    raise
                except:
                    i = 2
                    while os.path.exists(savepath):
                        savepath = os.path.join(tempfile.gettempdir(), ""poster-%s.%d.jpg"" % (self.tmdbid, i))
                        i += 1

            try:
                poster = self.urlretrieve(""https://image.tmdb.org/t/p/original"" + poster_path, savepath)[0]
            except Exception:
                self.log.exception(""Exception while retrieving poster"" % poster_path)
        return poster","for b in [base, base2]:
    for e in valid_poster_extensions:
        path = b + os.extsep + e
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break
    if poster:
        break
if not poster:
    (d, f) = os.path.split(path)
    for e in valid_poster_extensions:
        path = os.path.join(d, 'smaposter' + os.extsep + e)
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break","for b in [base, base2]:
    for e in valid_poster_extensions:
        path = b + os.extsep + e
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break
    if poster:
        break
else:
    (d, f) = os.path.split(path)
    for e in valid_poster_extensions:
        path = os.path.join(d, 'smaposter' + os.extsep + e)
        if os.path.exists(path):
            poster = path
            self.log.info('Local artwork detected, using %s.' % path)
            break",0,,,
sunpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sunpy/sunpy/net/helio/parser.py,https://github.com/sunpy/sunpy/tree/master/sunpy/net/helio/parser.py,,webservice_parser$23,"def webservice_parser(service='HEC'):
    """"""
    Quickly parses important contents from HELIO registry.

    Uses the link with 'service' appended and scrapes the web-service links contained on that webpage.

    Parameters
    ----------
    service : str
        Indicates which particular HELIO service is used. Defaults to HEC.

    Returns
    -------
    links: list or NoneType
        List of urls to registries containing WSDL endpoints.

    Examples
    --------
    >>> from sunpy.net.helio import parser
    >>> parser.webservice_parser()  # doctest: +REMOTE_DATA
    ['http://helio.mssl.ucl.ac.uk/helio-hec/HelioService',
    'http://msslkk.mssl.ucl.ac.uk/helio-hec/HelioService',
    'http://voparis-helio.obspm.fr/helio-hec/HelioService',
    'http://hec.helio-vo.eu/helio-hec/HelioService',
    'http://helio.mssl.ucl.ac.uk/helio-hec/HelioLongQueryService',
    'http://msslkk.mssl.ucl.ac.uk/helio-hec/HelioLongQueryService',
    'http://voparis-helio.obspm.fr/helio-hec/HelioLongQueryService',
    'http://hec.helio-vo.eu/helio-hec/HelioLongQueryService']
    """"""
    xml = None
    for REG_LINK in REG_LINKS:
        link = REG_LINK + service.lower()
        xml = link_test(link)
        if xml:
            break
    if xml is None:
        return None
    root = EL.fromstring(xml)
    links = []

    for interface in root.iter('interface'):
        service_type = interface.attrib
        key = list(service_type.keys())
        if len(key) > 0:
            value = service_type[key[0]]
            if value == 'vr:WebService':
                for url in interface.iter('accessURL'):
                    if url.text not in links:
                        links.append(url.text)
    return links","for REG_LINK in REG_LINKS:
    link = REG_LINK + service.lower()
    xml = link_test(link)
    if xml:
        break
if xml is None:
    return None","for REG_LINK in REG_LINKS:
    link = REG_LINK + service.lower()
    xml = link_test(link)
    if xml:
        break
else:
    return None",0,,,
prjxray,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/prjxray/fasm_disassembler.py,https://github.com/SymbiFlow/prjxray/tree/master/prjxray/fasm_disassembler.py,FasmDisassembler,find_features_in_bitstream$97,"def find_features_in_bitstream(self, bitdata, verbose=False):
        solved_bitdata = {}
        frames = set(bitdata.keys())
        tiles_checked = set()

        emitted_features = set()

        while len(frames) > 0:
            frame = frames.pop()

            # Skip frames that were emptied in a previous iteration.
            if not bitdata[frame]:
                continue

            # Iterate over all tiles that use this frame.
            for bits_info in self.segment_map.segment_info_for_frame(frame):
                # Don't examine a tile twice
                if (bits_info.tile, bits_info.block_type) in tiles_checked:
                    continue

                # Check if this frame has any data for the relevant tile.
                any_column = False
                for word_idx in range(bits_info.bits.words):
                    if word_idx + bits_info.bits.offset in bitdata[frame][0]:
                        any_column = True
                        break

                if not any_column:
                    continue

                tiles_checked.add((bits_info.tile, bits_info.block_type))

                for fasm_line in self.find_features_in_tile(
                        bits_info.tile, bits_info.block_type, bits_info.bits,
                        solved_bitdata, bitdata, verbose=verbose):
                    if fasm_line not in emitted_features:
                        emitted_features.add(fasm_line)
                        yield fasm_line

            remaining_bits = bitdata[frame][1]
            if frame in solved_bitdata:
                remaining_bits -= solved_bitdata[frame]

            if len(remaining_bits) > 0 and verbose:
                # Some bits were not decoded, add warning and annotations to
                # FASM.
                yield fasm.FasmLine(
                    set_feature=None,
                    annotations=None,
                    comment="" In frame 0x{:08x} {} bits were not converted."".
                    format(
                        frame,
                        len(remaining_bits),
                    ))

                for bit in sorted(remaining_bits):
                    frame_offset = frame % bitstream.FRAME_ALIGNMENT
                    aligned_frame = frame - frame_offset
                    wordidx = bit // bitstream.WORD_SIZE_BITS
                    bitidx = bit % bitstream.WORD_SIZE_BITS

                    annotations = []
                    annotations.append(
                        fasm.Annotation(
                            'unknown_bit', '{:08x}_{}_{}'.format(
                                frame, wordidx, bitidx)))
                    annotations.append(
                        fasm.Annotation(
                            'unknown_segment',
                            '0x{:08x}'.format(aligned_frame)))
                    annotations.append(
                        fasm.Annotation(
                            'unknown_segbit', '{:02d}_{:02d}'.format(
                                frame_offset, bit)))
                    yield fasm.FasmLine(
                        set_feature=None,
                        annotations=tuple(annotations),
                        comment=None,
                    )","for word_idx in range(bits_info.bits.words):
    if word_idx + bits_info.bits.offset in bitdata[frame][0]:
        any_column = True
        break
if not any_column:
    continue","for word_idx in range(bits_info.bits.words):
    if word_idx + bits_info.bits.offset in bitdata[frame][0]:
        break
else:
    continue",0,,,
toot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/toot/toot/tui/utils.py,https://github.com/ihabunek/toot/tree/master/toot/tui/utils.py,,show_media$55,"def show_media(paths):
    """"""
    Attempt to open an image viewer to show given media files.

    FIXME: This is not very thought out, but works for me.
    Once settings are implemented, add an option for the user to configure their
    prefered media viewer.
    """"""
    viewer = None
    potential_viewers = [
        ""feh"",
        ""eog"",
        ""display""
    ]
    for v in potential_viewers:
        viewer = shutil.which(v)
        if viewer:
            break

    if not viewer:
        raise Exception(""Cannot find an image viewer"")

    subprocess.run([viewer] + paths)","for v in potential_viewers:
    viewer = shutil.which(v)
    if viewer:
        break
if not viewer:
    raise Exception('Cannot find an image viewer')","for v in potential_viewers:
    viewer = shutil.which(v)
    if viewer:
        break
else:
    raise Exception('Cannot find an image viewer')",0,,,
keystone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/api/auth.py,https://github.com/openstack/keystone/tree/master/keystone/api/auth.py,AuthFederationWebSSOResource,_perform_auth$339,"def _perform_auth(cls, protocol_id):
        idps = PROVIDERS.federation_api.list_idps()
        remote_id = None
        for idp in idps:
            try:
                remote_id_name = federation_utils.get_remote_id_parameter(
                    idp, protocol_id)
            except exception.FederatedProtocolNotFound:
                # no protocol for this IdP, so this can't be the IdP we're
                # looking for
                continue
            remote_id = flask.request.environ.get(remote_id_name)
            if remote_id:
                break
        if not remote_id:
            msg = 'Missing entity ID from environment'
            tr_msg = _('Missing entity ID from environment')
            LOG.error(msg)
            raise exception.Unauthorized(tr_msg)

        host = _get_sso_origin_host()
        ref = PROVIDERS.federation_api.get_idp_from_remote_id(remote_id)
        identity_provider = ref['idp_id']
        token = authentication.federated_authenticate_for_token(
            identity_provider=identity_provider, protocol_id=protocol_id)
        return cls._render_template_response(host, token.id)","for idp in idps:
    try:
        remote_id_name = federation_utils.get_remote_id_parameter(idp, protocol_id)
    except exception.FederatedProtocolNotFound:
        continue
    remote_id = flask.request.environ.get(remote_id_name)
    if remote_id:
        break
if not remote_id:
    msg = 'Missing entity ID from environment'
    tr_msg = _('Missing entity ID from environment')
    LOG.error(msg)
    raise exception.Unauthorized(tr_msg)","for idp in idps:
    try:
        remote_id_name = federation_utils.get_remote_id_parameter(idp, protocol_id)
    except exception.FederatedProtocolNotFound:
        continue
    remote_id = flask.request.environ.get(remote_id_name)
    if remote_id:
        break
else:
    msg = 'Missing entity ID from environment'
    tr_msg = _('Missing entity ID from environment')
    LOG.error(msg)
    raise exception.Unauthorized(tr_msg)",0,,,
airflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/cli/commands/task_command.py,https://github.com/apache/airflow/tree/master/airflow/cli/commands/task_command.py,,task_test$425,"def task_test(args, dag=None):
    """"""Tests task for a given dag_id""""""
    # We want to log output from operators etc to show up here. Normally
    # airflow.task would redirect to a file, but here we want it to propagate
    # up to the normal airflow handler.

    settings.MASK_SECRETS_IN_LOGS = True

    handlers = logging.getLogger('airflow.task').handlers
    already_has_stream_handler = False
    for handler in handlers:
        already_has_stream_handler = isinstance(handler, logging.StreamHandler)
        if already_has_stream_handler:
            break
    if not already_has_stream_handler:
        logging.getLogger('airflow.task').propagate = True

    env_vars = {'AIRFLOW_TEST_MODE': 'True'}
    if args.env_vars:
        env_vars.update(args.env_vars)
        os.environ.update(env_vars)

    dag = dag or get_dag(args.subdir, args.dag_id)

    task = dag.get_task(task_id=args.task_id)
    # Add CLI provided task_params to task.params
    if args.task_params:
        passed_in_params = json.loads(args.task_params)
        task.params.update(passed_in_params)

    if task.params:
        task.params.validate()

    ti = _get_ti(task, args.execution_date_or_run_id, create_if_necessary=True)

    try:
        if args.dry_run:
            ti.dry_run()
        else:
            ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True)
    except Exception:
        if args.post_mortem:
            debugger = _guess_debugger()
            debugger.post_mortem()
        else:
            raise
    finally:
        if not already_has_stream_handler:
            # Make sure to reset back to normal. When run for CLI this doesn't
            # matter, but it does for test suite
            logging.getLogger('airflow.task').propagate = False","for handler in handlers:
    already_has_stream_handler = isinstance(handler, logging.StreamHandler)
    if already_has_stream_handler:
        break
if not already_has_stream_handler:
    logging.getLogger('airflow.task').propagate = True","for handler in handlers:
    already_has_stream_handler = isinstance(handler, logging.StreamHandler)
    if already_has_stream_handler:
        break
else:
    logging.getLogger('airflow.task').propagate = True",0,,,
pyclustering,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyclustering/pyclustering/nnet/sync.py,https://github.com/annoviko/pyclustering/tree/master/pyclustering/nnet/sync.py,sync_dynamic,allocate_sync_ensembles$174,"def allocate_sync_ensembles(self, tolerance=0.01, indexes=None, iteration=None):
        """"""!
        @brief Allocate clusters in line with ensembles of synchronous oscillators where each synchronous ensemble corresponds to only one cluster.
               
        @param[in] tolerance (double): Maximum error for allocation of synchronous ensemble oscillators.
        @param[in] indexes (list): List of real object indexes and it should be equal to amount of oscillators (in case of 'None' - indexes are in range [0; amount_oscillators]).
        @param[in] iteration (uint): Iteration of simulation that should be used for allocation.
        
        @return (list) Groups (lists) of indexes of synchronous oscillators.
                For example [ [index_osc1, index_osc3], [index_osc2], [index_osc4, index_osc5] ].
        
        """"""

        if self._ccore_sync_dynamic_pointer is not None:
            ensembles = wrapper.sync_dynamic_allocate_sync_ensembles(self._ccore_sync_dynamic_pointer, tolerance, iteration)

            if indexes is not None:
                for ensemble in ensembles:
                    for index in range(len(ensemble)):
                        ensemble[index] = indexes[ensemble[index]]

            return ensembles

        if (self._dynamic is None) or (len(self._dynamic) == 0):
            return []

        number_oscillators = len(self._dynamic[0])
        last_state = None

        if iteration is None:
            last_state = self._dynamic[len(self._dynamic) - 1]
        else:
            last_state = self._dynamic[iteration]

        clusters = []
        if number_oscillators > 0:
            clusters.append([0])

        for i in range(1, number_oscillators, 1):
            cluster_allocated = False
            for cluster in clusters:
                for neuron_index in cluster:
                    last_state_shifted = abs(last_state[i] - 2 * pi)

                    if ( ( (last_state[i] < (last_state[neuron_index] + tolerance)) and (last_state[i] > (last_state[neuron_index] - tolerance)) ) or
                         ( (last_state_shifted < (last_state[neuron_index] + tolerance)) and (last_state_shifted > (last_state[neuron_index] - tolerance)) ) ):
                        cluster_allocated = True

                        real_index = i
                        if indexes is not None:
                            real_index = indexes[i]

                        cluster.append(real_index)
                        break

                if cluster_allocated is True:
                    break

            if cluster_allocated is False:
                clusters.append([i])

        return clusters","for cluster in clusters:
    for neuron_index in cluster:
        last_state_shifted = abs(last_state[i] - 2 * pi)
        if last_state[i] < last_state[neuron_index] + tolerance and last_state[i] > last_state[neuron_index] - tolerance or (last_state_shifted < last_state[neuron_index] + tolerance and last_state_shifted > last_state[neuron_index] - tolerance):
            cluster_allocated = True
            real_index = i
            if indexes is not None:
                real_index = indexes[i]
            cluster.append(real_index)
            break
    if cluster_allocated is True:
        break
if cluster_allocated is False:
    clusters.append([i])","for cluster in clusters:
    for neuron_index in cluster:
        last_state_shifted = abs(last_state[i] - 2 * pi)
        if last_state[i] < last_state[neuron_index] + tolerance and last_state[i] > last_state[neuron_index] - tolerance or (last_state_shifted < last_state[neuron_index] + tolerance and last_state_shifted > last_state[neuron_index] - tolerance):
            cluster_allocated = True
            real_index = i
            if indexes is not None:
                real_index = indexes[i]
            cluster.append(real_index)
            break
    if cluster_allocated is True:
        break
else:
    clusters.append([i])",0,,,
