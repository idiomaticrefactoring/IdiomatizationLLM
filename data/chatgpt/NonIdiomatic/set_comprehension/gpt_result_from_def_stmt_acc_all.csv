repo_name,file_path,file_html,class_name,me_name,me_code,old_code,chatGPT_code,element_str,slice_str,truth_code
R-Drop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/fairseq_src/scripts/compare_namespaces.py,https://github.com/dropreg/R-Drop/tree/master/fairseq_src/scripts/compare_namespaces.py,,keys$12,"def keys(ns):
    ks = set()
    for k in dir(ns):
        if not k.startswith('_'):
            ks.add(k)
    return ks","for k in dir(ns):
    if not k.startswith('_'):
        ks.add(k)",ks = {k for k in dir(ns) if not k.startswith('_')},["ks = {k for k in dir(ns) if not k.startswith('_')}"],1,
Multilingual_Text_to_Speech,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Multilingual_Text_to_Speech/dataset/dataset.py,https://github.com/Tomiinek/Multilingual_Text_to_Speech/tree/master/dataset/dataset.py,TextToSpeechDataset,get_num_languages$185,"def get_num_languages(self):
    """"""Get number of unique languages in the dataset.""""""
    languages = set()
    for idx in range(len(self.items)):
        languages.add(self.items[idx]['language'])
    return len(languages)","for idx in range(len(self.items)):
    languages.add(self.items[idx]['language'])",languages = {self.items[idx]['language'] for idx in range(len(self.items))},["languages = {self.items[idx]['language'] for idx in range(len(self.items))}"],1,
pandas-datareader,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandas-datareader/versioneer.py,https://github.com/pydata/pandas-datareader/tree/master//versioneer.py,,do_setup$1754,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print('Adding sample versioneer config to setup.cfg', file=sys.stderr)
            with open(os.path.join(root, 'setup.cfg'), 'a') as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1
    print(' creating %s' % cfg.versionfile_source)
    with open(cfg.versionfile_source, 'w') as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {'DOLLAR': '$', 'STYLE': cfg.style, 'TAG_PREFIX': cfg.tag_prefix, 'PARENTDIR_PREFIX': cfg.parentdir_prefix, 'VERSIONFILE_SOURCE': cfg.versionfile_source})
    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), '__init__.py')
    if os.path.exists(ipy):
        try:
            with open(ipy, 'r') as f:
                old = f.read()
        except EnvironmentError:
            old = ''
        if INIT_PY_SNIPPET not in old:
            print(' appending to %s' % ipy)
            with open(ipy, 'a') as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print(' %s unmodified' % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None
    manifest_in = os.path.join(root, 'MANIFEST.in')
    simple_includes = set()
    try:
        with open(manifest_in, 'r') as f:
            for line in f:
                if line.startswith('include '):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    if 'versioneer.py' not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, 'a') as f:
            f.write('include versioneer.py\n')
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" % cfg.versionfile_source)
        with open(manifest_in, 'a') as f:
            f.write('include %s\n' % cfg.versionfile_source)
    else:
        print(' versionfile_source already in MANIFEST.in')
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f for include in line.split()[1:] if line.startswith('include ')},["simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}"],0,
prjxray,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/utils/create_timing_worksheet_db.py,https://github.com/SymbiFlow/prjxray/tree/master/utils/create_timing_worksheet_db.py,,build_wire_filter$482,"def build_wire_filter(wire_filter):
    wires_to_include = set()
    with OpenSafeFile(wire_filter) as f:
        for l in f:
            wire = l.strip()
            if not wire:
                continue
            wires_to_include.add(wire)

    def filter_net(net):
        wires_in_net = set()
        for node in net['nodes']:
            for wire in node['wires']:
                wires_in_net.add(wire['name'])
        return len(wires_in_net & wires_to_include) > 0
    return filter_net","for l in f:
    wire = l.strip()
    if not wire:
        continue
    wires_to_include.add(wire)",wires_to_include = {l.strip() for l in f if l.strip()},Cannot refactor,2,
prjxray,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/utils/create_timing_worksheet_db.py,https://github.com/SymbiFlow/prjxray/tree/master/utils/create_timing_worksheet_db.py,,build_wire_filter$482,"def build_wire_filter(wire_filter):
    wires_to_include = set()
    with OpenSafeFile(wire_filter) as f:
        for l in f:
            wire = l.strip()
            if not wire:
                continue
            wires_to_include.add(wire)

    def filter_net(net):
        wires_in_net = set()
        for node in net['nodes']:
            for wire in node['wires']:
                wires_in_net.add(wire['name'])
        return len(wires_in_net & wires_to_include) > 0
    return filter_net","for node in net['nodes']:
    for wire in node['wires']:
        wires_in_net.add(wire['name'])",wires_in_net = {wire['name'] for node in net['nodes'] for wire in node['wires']},["wires_in_net = {wire['name'] for node in net['nodes'] for wire in node['wires']}"],1,
Amulet-Map-Editor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Amulet-Map-Editor/versioneer.py,https://github.com/Amulet-Team/Amulet-Map-Editor/tree/master//versioneer.py,,do_setup$1753,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print('Adding sample versioneer config to setup.cfg', file=sys.stderr)
            with open(os.path.join(root, 'setup.cfg'), 'a') as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1
    print(' creating %s' % cfg.versionfile_source)
    with open(cfg.versionfile_source, 'w') as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {'DOLLAR': '$', 'STYLE': cfg.style, 'TAG_PREFIX': cfg.tag_prefix, 'PARENTDIR_PREFIX': cfg.parentdir_prefix, 'VERSIONFILE_SOURCE': cfg.versionfile_source})
    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), '__init__.py')
    if os.path.exists(ipy):
        try:
            with open(ipy, 'r') as f:
                old = f.read()
        except EnvironmentError:
            old = ''
        if INIT_PY_SNIPPET not in old:
            print(' appending to %s' % ipy)
            with open(ipy, 'a') as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print(' %s unmodified' % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None
    manifest_in = os.path.join(root, 'MANIFEST.in')
    simple_includes = set()
    try:
        with open(manifest_in, 'r') as f:
            for line in f:
                if line.startswith('include '):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    if 'versioneer.py' not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, 'a') as f:
            f.write('include versioneer.py\n')
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" % cfg.versionfile_source)
        with open(manifest_in, 'a') as f:
            f.write('include %s\n' % cfg.versionfile_source)
    else:
        print(' versionfile_source already in MANIFEST.in')
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f for include in line.split()[1:] if line.startswith('include ')},["simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}"],0,
ParlAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/fvqa/agents.py,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/fvqa/agents.py,SplitTeacher,_setup_data$161,"def _setup_data(self, questions_path, trainset_path, datatype, task_num):
    print('loading: ' + questions_path)
    with PathManager.open(questions_path) as questions_file:
        questions = json.load(questions_file)
    train_test_images = set()
    fn = os.path.join(trainset_path, '{}_list_{}.txt'.format(datatype, task_num))
    with PathManager.open(fn) as imageset:
        for line in imageset:
            train_test_images.add(line.strip())
    self.ques = [questions[k] for k in sorted(questions.keys()) if questions[k]['img_file'] in train_test_images]","for line in imageset:
    train_test_images.add(line.strip())",train_test_images = {line.strip() for line in imageset},['train_test_images = {line.strip() for line in imageset}'],1,
SatanSword,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SatanSword/Evil_Eye/wappalyzer/Wappalyzer.py,https://github.com/Lucifer1993/SatanSword/tree/master/Evil_Eye/wappalyzer/Wappalyzer.py,Wappalyzer,analyze$255,"def analyze(self, webpage):
    """"""
        Return a list of applications that can be detected on the web page.
        """"""
    detected_apps = set()
    for (app_name, app) in self.apps.items():
        if self._has_app(app, webpage):
            detected_apps.add(app_name)
    detected_apps |= self._get_implied_apps(detected_apps)
    return detected_apps","for (app_name, app) in self.apps.items():
    if self._has_app(app, webpage):
        detected_apps.add(app_name)","detected_apps = {app_name for (app_name, app) in self.apps.items() if self._has_app(app, webpage)}","['detected_apps = {app_name for (app_name, app) in self.apps.items() if self._has_app(app, webpage)}']",1,
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/couch/tests/test_couchv2.py,https://github.com/DataDog/integrations-core/tree/master/couch/tests/test_couchv2.py,,test_only_max_dbs_are_scanned$260,"def test_only_max_dbs_are_scanned(aggregator, gauges, number_db):
    config = deepcopy(common.NODE1)
    config['max_dbs_per_check'] = number_db
    check = CouchDb(common.CHECK_NAME, {}, [config])
    check.check(config)
    metrics = []
    for metric_list in aggregator._metrics.values():
        metrics.extend(metric_list)
    db_tags = set()
    for m in metrics:
        for tag in m.tags:
            if tag.startswith('db:'):
                db_tags.add(tag)
    assert len(db_tags) == number_db","for m in metrics:
    for tag in m.tags:
        if tag.startswith('db:'):
            db_tags.add(tag)",db_tags = {tag for m in metrics for tag in m.tags if tag.startswith('db:')},["db_tags = {tag for m in metrics for tag in m.tags if tag.startswith('db:')}"],1,
mlrun,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mlrun/tests/api/api/feature_store/test_feature_sets.py,https://github.com/mlrun/mlrun/tree/master/tests/api/api/feature_store/test_feature_sets.py,,test_feature_set_tagging_with_re_store$508,"def test_feature_set_tagging_with_re_store(db: Session, client: TestClient) -> None:
    project_name = f'prj-{uuid4().hex}'
    tests.api.api.utils.create_project(client, project_name)
    name = 'feature_set1'
    feature_set = _generate_feature_set(name)
    response = _store_and_assert_feature_set(client, project_name, name, 'tag1', feature_set)
    uid = response['metadata']['uid']
    response = _store_and_assert_feature_set(client, project_name, name, 'tag2', feature_set)
    assert response['metadata']['uid'] == uid
    response = _list_and_assert_objects(client, 'feature_sets', project_name, f'name={name}', 2)['feature_sets']
    expected_tags = {'tag1', 'tag2'}
    returned_tags = set()
    for feature_set_response in response:
        returned_tags.add(feature_set_response['metadata']['tag'])
    assert expected_tags == returned_tags
    feature_set['metadata']['extra_metadata'] = 200
    _store_and_assert_feature_set(client, project_name, name, 'tag2', feature_set)
    _list_and_assert_objects(client, 'feature_sets', project_name, f'name={name}', 2)
    response = _list_and_assert_objects(client, 'feature_sets', project_name, f'name={name}&tag=tag2', 1)['feature_sets']
    assert response[0]['metadata']['extra_metadata'] == 200","for feature_set_response in response:
    returned_tags.add(feature_set_response['metadata']['tag'])",returned_tags = {feature_set_response['metadata']['tag'] for feature_set_response in response},["returned_tags = {feature_set_response['metadata']['tag'] for feature_set_response in response}"],1,
aioprocessing,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aioprocessing/aioprocessing/executor.py,https://github.com/dano/aioprocessing/tree/master/aioprocessing/executor.py,CoroBuilder,__new__$105,"def __new__(cls, clsname, bases, dct, **kwargs):
    coro_list = dct.get('coroutines', [])
    existing_coros = set()

    def find_existing_coros(d):
        for attr in d:
            if attr.startswith('coro_') or attr.startswith('thread_'):
                existing_coros.add(attr)
    find_existing_coros(dct)
    for b in bases:
        b_dct = b.__dict__
        coro_list.extend(b_dct.get('coroutines', []))
        find_existing_coros(b_dct)
    if _ExecutorMixin not in bases:
        bases += (_ExecutorMixin,)
    for func in coro_list:
        coro_name = 'coro_{}'.format(func)
        if coro_name not in existing_coros:
            dct[coro_name] = cls.coro_maker(func)
    return super().__new__(cls, clsname, bases, dct)","for attr in d:
    if attr.startswith('coro_') or attr.startswith('thread_'):
        existing_coros.add(attr)",existing_coros = {attr for attr in d if attr.startswith('coro_') or attr.startswith('thread_')},["existing_coros = {attr for attr in d if attr.startswith('coro_') or attr.startswith('thread_')}"],1,
tapas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tapas/tapas/utils/sqa_utils.py,https://github.com/google-research/tapas/tree/master/tapas/utils/sqa_utils.py,,_add_tables$53,"def _add_tables(input_dir, interaction_dict):
    """"""Adds table protos to all interactions.""""""
    table_files = set()
    for interactions in interaction_dict.values():
        for interaction in interactions:
            table_files.add(interaction.table.table_id)
    table_dict = {}
    for (index, table_file) in enumerate(sorted(table_files)):
        logging.log_every_n(logging.INFO, 'Read %4d / %4d table files', 100, index, len(table_files))
        table_path = os.path.join(input_dir, table_file)
        with tf.io.gfile.GFile(table_path, 'r') as table_handle:
            table = interaction_pb2.Table()
            rows = list(csv.reader(table_handle))
            (headers, rows) = (rows[0], rows[1:])
            for header in headers:
                table.columns.add().text = header
            for row in rows:
                new_row = table.rows.add()
                for cell in row:
                    new_row.cells.add().text = cell
            table.table_id = table_file
            table_dict[table_file] = table
    for interactions in interaction_dict.values():
        for interaction in interactions:
            interaction.table.CopyFrom(table_dict[interaction.table.table_id])","for interactions in interaction_dict.values():
    for interaction in interactions:
        table_files.add(interaction.table.table_id)",table_files = {interaction.table.table_id for interactions in interaction_dict.values() for interaction in interactions},['table_files = {interaction.table.table_id for interactions in interaction_dict.values() for interaction in interactions}'],1,
tapas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tapas/tapas/utils/sqa_utils.py,https://github.com/google-research/tapas/tree/master/tapas/utils/sqa_utils.py,,_add_tables$53,"def _add_tables(input_dir, interaction_dict):
    """"""Adds table protos to all interactions.""""""
    table_files = set()
    for interactions in interaction_dict.values():
        for interaction in interactions:
            table_files.add(interaction.table.table_id)
    table_dict = {}
    for (index, table_file) in enumerate(sorted(table_files)):
        logging.log_every_n(logging.INFO, 'Read %4d / %4d table files', 100, index, len(table_files))
        table_path = os.path.join(input_dir, table_file)
        with tf.io.gfile.GFile(table_path, 'r') as table_handle:
            table = interaction_pb2.Table()
            rows = list(csv.reader(table_handle))
            (headers, rows) = (rows[0], rows[1:])
            for header in headers:
                table.columns.add().text = header
            for row in rows:
                new_row = table.rows.add()
                for cell in row:
                    new_row.cells.add().text = cell
            table.table_id = table_file
            table_dict[table_file] = table
    for interactions in interaction_dict.values():
        for interaction in interactions:
            interaction.table.CopyFrom(table_dict[interaction.table.table_id])","for header in headers:
    table.columns.add().text = header",table.columns = {header for header in headers},Cannot refactor,2,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/ext/tornado/test/web_test.py,https://github.com/saltstack/salt/tree/master/salt/ext/tornado/test/web_test.py,StaticFileTest,get_and_head$1034,"def get_and_head(self, *args, **kwargs):
    """"""Performs a GET and HEAD request and returns the GET response.

        Fails if any ``Content-*`` headers returned by the two requests
        differ.
        """"""
    head_response = self.fetch(*args, method='HEAD', **kwargs)
    get_response = self.fetch(*args, method='GET', **kwargs)
    content_headers = set()
    for h in itertools.chain(head_response.headers, get_response.headers):
        if h.startswith('Content-'):
            content_headers.add(h)
    for h in content_headers:
        self.assertEqual(head_response.headers.get(h), get_response.headers.get(h), '%s differs between GET (%s) and HEAD (%s)' % (h, head_response.headers.get(h), get_response.headers.get(h)))
    return get_response","for h in itertools.chain(head_response.headers, get_response.headers):
    if h.startswith('Content-'):
        content_headers.add(h)","content_headers = {h for h in itertools.chain(head_response.headers, get_response.headers) if h.startswith('Content-')}","[""content_headers = {h for h in itertools.chain(head_response.headers, get_response.headers) if h.startswith('Content-')}""]",1,
moto,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/tests/test_ec2/test_regions.py,https://github.com/spulec/moto/tree/master/tests/test_ec2/test_regions.py,,test_use_boto_regions$19,"def test_use_boto_regions():
    boto_regions = set()
    for region in Session().get_available_regions('ec2'):
        boto_regions.add(region)
    for region in Session().get_available_regions('ec2', partition_name='aws-us-gov'):
        boto_regions.add(region)
    for region in Session().get_available_regions('ec2', partition_name='aws-cn'):
        boto_regions.add(region)
    moto_regions = set(ec2_backends)
    moto_regions.should.equal(boto_regions)","for region in Session().get_available_regions('ec2'):
    boto_regions.add(region)",boto_regions = {region for region in Session().get_available_regions('ec2')},["boto_regions = {region for region in Session().get_available_regions('ec2')}"],1,
moto,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/tests/test_ec2/test_regions.py,https://github.com/spulec/moto/tree/master/tests/test_ec2/test_regions.py,,test_use_boto_regions$19,"def test_use_boto_regions():
    boto_regions = set()
    for region in Session().get_available_regions('ec2'):
        boto_regions.add(region)
    for region in Session().get_available_regions('ec2', partition_name='aws-us-gov'):
        boto_regions.add(region)
    for region in Session().get_available_regions('ec2', partition_name='aws-cn'):
        boto_regions.add(region)
    moto_regions = set(ec2_backends)
    moto_regions.should.equal(boto_regions)","for region in Session().get_available_regions('ec2', partition_name='aws-us-gov'):
    boto_regions.add(region)","boto_regions = {region for region in Session().get_available_regions('ec2', partition_name='aws-us-gov')}",Cannot refactor,2,
moto,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/tests/test_ec2/test_regions.py,https://github.com/spulec/moto/tree/master/tests/test_ec2/test_regions.py,,test_use_boto_regions$19,"def test_use_boto_regions():
    boto_regions = set()
    for region in Session().get_available_regions('ec2'):
        boto_regions.add(region)
    for region in Session().get_available_regions('ec2', partition_name='aws-us-gov'):
        boto_regions.add(region)
    for region in Session().get_available_regions('ec2', partition_name='aws-cn'):
        boto_regions.add(region)
    moto_regions = set(ec2_backends)
    moto_regions.should.equal(boto_regions)","for region in Session().get_available_regions('ec2', partition_name='aws-cn'):
    boto_regions.add(region)","boto_regions = {region for region in Session().get_available_regions('ec2', partition_name='aws-cn')}",Cannot refactor,2,
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/policy.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/policy.py,ConfigPollRuleMode,run$784,"def run(self, event, lambda_context):
    cfg_event = json.loads(event['invokingEvent'])
    resource_type = self.policy.resource_manager.resource_type.cfn_type
    resource_id = self.policy.resource_manager.resource_type.id
    client = self._get_client()
    token = event.get('resultToken')
    matched_resources = set()
    for r in PullMode.run(self):
        matched_resources.add(r[resource_id])
    unmatched_resources = set()
    for r in self.policy.resource_manager.get_resource_manager(self.policy.resource_type).resources():
        if r[resource_id] not in matched_resources:
            unmatched_resources.add(r[resource_id])
    evaluations = [dict(ComplianceResourceType=resource_type, ComplianceResourceId=r, ComplianceType='NON_COMPLIANT', OrderingTimestamp=cfg_event['notificationCreationTime'], Annotation='The resource is not compliant with policy:%s.' % self.policy.name) for r in matched_resources]
    if evaluations and token:
        self.put_evaluations(client, token, evaluations)
    evaluations = [dict(ComplianceResourceType=resource_type, ComplianceResourceId=r, ComplianceType='COMPLIANT', OrderingTimestamp=cfg_event['notificationCreationTime'], Annotation='The resource is compliant with policy:%s.' % self.policy.name) for r in unmatched_resources]
    if evaluations and token:
        self.put_evaluations(client, token, evaluations)
    return list(matched_resources)","for r in PullMode.run(self):
    matched_resources.add(r[resource_id])",matched_resources = {r[resource_id] for r in PullMode.run(self)},Cannot refactor,2,
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/policy.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/policy.py,ConfigPollRuleMode,run$784,"def run(self, event, lambda_context):
    cfg_event = json.loads(event['invokingEvent'])
    resource_type = self.policy.resource_manager.resource_type.cfn_type
    resource_id = self.policy.resource_manager.resource_type.id
    client = self._get_client()
    token = event.get('resultToken')
    matched_resources = set()
    for r in PullMode.run(self):
        matched_resources.add(r[resource_id])
    unmatched_resources = set()
    for r in self.policy.resource_manager.get_resource_manager(self.policy.resource_type).resources():
        if r[resource_id] not in matched_resources:
            unmatched_resources.add(r[resource_id])
    evaluations = [dict(ComplianceResourceType=resource_type, ComplianceResourceId=r, ComplianceType='NON_COMPLIANT', OrderingTimestamp=cfg_event['notificationCreationTime'], Annotation='The resource is not compliant with policy:%s.' % self.policy.name) for r in matched_resources]
    if evaluations and token:
        self.put_evaluations(client, token, evaluations)
    evaluations = [dict(ComplianceResourceType=resource_type, ComplianceResourceId=r, ComplianceType='COMPLIANT', OrderingTimestamp=cfg_event['notificationCreationTime'], Annotation='The resource is compliant with policy:%s.' % self.policy.name) for r in unmatched_resources]
    if evaluations and token:
        self.put_evaluations(client, token, evaluations)
    return list(matched_resources)","for r in self.policy.resource_manager.get_resource_manager(self.policy.resource_type).resources():
    if r[resource_id] not in matched_resources:
        unmatched_resources.add(r[resource_id])",unmatched_resources = {r[resource_id] for r in self.policy.resource_manager.get_resource_manager(self.policy.resource_type).resources() if r[resource_id] not in matched_resources},['unmatched_resources = {r[resource_id] for r in self.policy.resource_manager.get_resource_manager(self.policy.resource_type).resources() if r[resource_id] not in matched_resources}'],1,
nltk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/sem/evaluate.py,https://github.com/nltk/nltk/tree/master/nltk/sem/evaluate.py,,set2rel$79,"def set2rel(s):
    """"""
    Convert a set containing individuals (strings or numbers) into a set of
    unary tuples. Any tuples of strings already in the set are passed through
    unchanged.

    For example:
      - set(['a', 'b']) => set([('a',), ('b',)])
      - set([3, 27]) => set([('3',), ('27',)])

    :type s: set
    :rtype: set of tuple of str
    """"""
    new = set()
    for elem in s:
        if isinstance(elem, str):
            new.add((elem,))
        elif isinstance(elem, int):
            new.add(str(elem))
        else:
            new.add(elem)
    return new","for elem in s:
    if isinstance(elem, str):
        new.add((elem,))
    elif isinstance(elem, int):
        new.add(str(elem))
    else:
        new.add(elem)","new = {(elem,) if isinstance(elem, str) else str(elem) if isinstance(elem, int) else elem for elem in s}","['new = {(elem,) if isinstance(elem, str) else str(elem) if isinstance(elem, int) else elem for elem in s}']",1,
fitlog,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fitlog/fitlog/fastserver/server/chart_utils.py,https://github.com/fastnlp/fitlog/tree/master/fitlog/fastserver/server/chart_utils.py,,_refine_path$184,"def _refine_path(paths):
    """"""
    给定list的path，将公共的部分删掉一些. 这里只处理完全一样深度的metric. 主要为了删除相同的metric_name
        [['metric', 'BMESF1MEtric', 'f1'], ['metric', 'BMESF1Metric'], ...]
    :param paths:
    :return:
    """"""
    if len(set(map(len, paths))) != 1:
        path2shortpath = {'-'.join(path): '-'.join(path) for path in paths}
    elif len(paths) == 0:
        path2shortpath = {'-'.join(paths[0]): paths[0][-1]}
    else:
        delete_depths = []
        for depth in range(len(paths[0])):
            names = set()
            for path in paths:
                names.add(path[depth])
            if len(names) == 1:
                delete_depths.append(depth)
        for i in range(len(paths)):
            for d in reversed(delete_depths):
                paths[i].pop(d)
        path2shortpath = {'-'.join(path): '-'.join(path) for path in paths}
    return path2shortpath","for depth in range(len(paths[0])):
    names = set()
    for path in paths:
        names.add(path[depth])
    if len(names) == 1:
        delete_depths.append(depth)",delete_depths = {depth for depth in range(len(paths[0])) if len({path[depth] for path in paths}) == 1},Cannot refactor,2,
graph4nlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/graph4nlp/examples/pytorch/semantic_parsing/graph2tree/jobs/src/evaluation.py,https://github.com/graph4ai/graph4nlp/tree/master/examples/pytorch/semantic_parsing/graph2tree/jobs/src/evaluation.py,,get_split_comma$26,"def get_split_comma(input_str):
    input_str = input_str.replace(',', ' , ')
    input_list = [item.strip() for item in input_str.split()]
    ref_char = '$'
    for index in range(len(input_list)):
        if input_list[index] == ',':
            if input_list[:index].count('(') == input_list[:index].count(')'):
                if input_list[index + 1:].count('(') == input_list[index + 1:].count(')'):
                    if input_list[index] == ref_char:
                        raise RuntimeError
                    else:
                        input_list[index] = ref_char
    new_str = ' '.join(input_list).split('$')
    result_set = set()
    for str_ in new_str:
        result_set.add(str_.strip())
    return result_set","for str_ in new_str:
    result_set.add(str_.strip())",result_set = {str_.strip() for str_ in new_str},['result_set = {str_.strip() for str_ in new_str}'],1,
textflint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/textflint/textflint/generation/transformation/ABSA/absa_transformation.py,https://github.com/textflint/textflint/tree/master/textflint/generation/transformation/ABSA/absa_transformation.py,ABSATransformation,reverse_opinion$307,"def reverse_opinion(self, trans_words, trans_opinion_words, opinion_from, opinion_to, has_neg):
    """"""
        Reverse the polarity of original opinion and return the new
        transformed opinion words.

        :param list trans_words: tokenized words of transformed sentence
        :param list trans_opinion_words: transformed opinion words
        :param int opinion_from: start index of opinion
        :param int opinion_to: end index of opinion
        :param bool has_neg: whether exist negation in transformed sentence
        """"""
    opinion_list = trans_words[opinion_from:opinion_to]
    opinion_words = trans_words[opinion_from:opinion_to]
    opi = opinion_list[0]
    trans_opinion_word = None
    from_to = []
    if has_neg and [opinion_from, opinion_to] not in from_to:
        trans_opinion_word = [opinion_from, opinion_to, self.untokenize(opinion_words)]
    elif [opinion_from, opinion_to] not in from_to:
        opi_pos = self.get_postag(trans_words, opinion_from, opinion_to)
        antonyms = self.get_antonyms(opi_pos)[0]
        candidate = set()
        for antonym in antonyms:
            for ant_word in antonym.lemma_names(lang='eng'):
                if ant_word != opi and '_' not in ant_word:
                    candidate.add(ant_word)
        refined_candidate = self.refine_candidate(trans_words, opinion_from, opinion_to, candidate)
        if len(refined_candidate) == 0:
            trans_opinion_word = [opinion_from, opinion_to, self.untokenize(['not', opi])]
        else:
            select = random.randint(0, len(refined_candidate) - 1)
            trans_opinion_word = [opinion_from, opinion_to, self.untokenize([refined_candidate[select]])]
    if trans_opinion_word is not None:
        trans_opinion_words.append(trans_opinion_word)
        from_to.append([opinion_from, opinion_to])
        trans_words[opinion_from:opinion_to] = [trans_opinion_word[2]]
    return (trans_words, trans_opinion_words)","for antonym in antonyms:
    for ant_word in antonym.lemma_names(lang='eng'):
        if ant_word != opi and '_' not in ant_word:
            candidate.add(ant_word)",candidate = {ant_word for antonym in antonyms for ant_word in antonym.lemma_names(lang='eng') if ant_word != opi and '_' not in ant_word},["candidate = {ant_word for antonym in antonyms for ant_word in antonym.lemma_names(lang='eng') if ant_word != opi and '_' not in ant_word}"],1,
shuup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup_tests/admin/test_modules.py,https://github.com/shuup/shuup/tree/master/shuup_tests/admin/test_modules.py,,test_dashboard_blocks$75,"def test_dashboard_blocks(rf):
    request = rf.get('/')
    with replace_modules([ATestModule]):
        block_ids = set()
        for block in chain(*(m.get_dashboard_blocks(request) for m in get_modules())):
            block_ids.add(block.id)
        assert block_ids >= set(['test-0', 'test-1', 'test-2', 'test-3', 'test-4'])","for block in chain(*(m.get_dashboard_blocks(request) for m in get_modules())):
    block_ids.add(block.id)",block_ids = {block.id for block in chain(*(m.get_dashboard_blocks(request) for m in get_modules()))},['block_ids = {block.id for block in chain(*(m.get_dashboard_blocks(request) for m in get_modules()))}'],1,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_webapp/docassemble/webapp/backend.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/backend.py,,delete_user_data$743,"def delete_user_data(user_id, r, r_user):
    db.session.execute(delete(UserDict).where(UserDict.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserDictKeys).where(UserDictKeys.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UploadsUserAuth).where(UploadsUserAuth.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.owner_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(GlobalObjectStorage).where(GlobalObjectStorage.user_id == user_id))
    db.session.commit()
    for package_auth in db.session.execute(select(PackageAuth).filter_by(user_id=user_id)).scalars():
        package_auth.user_id = 1
    db.session.commit()
    files_to_delete = list()
    for short_code_item in db.session.execute(select(Shortener).filter_by(user_id=user_id)).scalars():
        for email in db.session.execute(select(Email).filter_by(short=short_code_item.short)).scalars():
            for attachment in db.session.execute(select(EmailAttachment).filter_by(email_id=email.id)).scalars():
                files_to_delete.append(attachment.upload)
    for file_number in files_to_delete:
        the_file = SavedFile(file_number)
        the_file.delete()
    db.session.execute(delete(Shortener).where(Shortener.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserRoles).where(UserRoles.user_id == user_id))
    db.session.commit()
    for user_auth in db.session.execute(select(UserAuthModel).filter_by(user_id=user_id).with_for_update()).scalars():
        user_auth.password = ''
        user_auth.reset_password_token = ''
    db.session.commit()
    for section in ('playground', 'playgroundmodules', 'playgroundpackages', 'playgroundsources', 'playgroundstatic', 'playgroundtemplate'):
        the_section = SavedFile(user_id, section=section)
        the_section.delete()
    old_email = None
    for user_object in db.session.execute(select(UserModel).filter_by(id=user_id)).scalars():
        old_email = user_object.email
        user_object.active = False
        user_object.first_name = ''
        user_object.last_name = ''
        user_object.nickname = ''
        user_object.email = None
        user_object.country = ''
        user_object.subdivisionfirst = ''
        user_object.subdivisionsecond = ''
        user_object.subdivisionthird = ''
        user_object.organization = ''
        user_object.timezone = None
        user_object.language = None
        user_object.pypi_username = None
        user_object.pypi_password = None
        user_object.otp_secret = None
        user_object.confirmed_at = None
        user_object.last_login = None
        user_object.social_id = 'disabled$' + str(user_id)
    db.session.commit()
    keys_to_delete = set()
    for key in r.keys('*userid:' + str(user_id)):
        keys_to_delete.add(key)
    for key in r.keys('*userid:' + str(user_id) + ':*'):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r.delete(key)
    keys_to_delete = set()
    for key in r_user.keys('*:user:' + str(old_email)):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r_user.delete(key)","for key in r.keys('*userid:' + str(user_id)):
    keys_to_delete.add(key)",keys_to_delete = {key for key in r.keys('*userid:' + str(user_id))},["keys_to_delete = {key for key in r.keys('*userid:' + str(user_id))}"],1,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_webapp/docassemble/webapp/backend.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/backend.py,,delete_user_data$743,"def delete_user_data(user_id, r, r_user):
    db.session.execute(delete(UserDict).where(UserDict.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserDictKeys).where(UserDictKeys.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UploadsUserAuth).where(UploadsUserAuth.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.owner_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(GlobalObjectStorage).where(GlobalObjectStorage.user_id == user_id))
    db.session.commit()
    for package_auth in db.session.execute(select(PackageAuth).filter_by(user_id=user_id)).scalars():
        package_auth.user_id = 1
    db.session.commit()
    files_to_delete = list()
    for short_code_item in db.session.execute(select(Shortener).filter_by(user_id=user_id)).scalars():
        for email in db.session.execute(select(Email).filter_by(short=short_code_item.short)).scalars():
            for attachment in db.session.execute(select(EmailAttachment).filter_by(email_id=email.id)).scalars():
                files_to_delete.append(attachment.upload)
    for file_number in files_to_delete:
        the_file = SavedFile(file_number)
        the_file.delete()
    db.session.execute(delete(Shortener).where(Shortener.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserRoles).where(UserRoles.user_id == user_id))
    db.session.commit()
    for user_auth in db.session.execute(select(UserAuthModel).filter_by(user_id=user_id).with_for_update()).scalars():
        user_auth.password = ''
        user_auth.reset_password_token = ''
    db.session.commit()
    for section in ('playground', 'playgroundmodules', 'playgroundpackages', 'playgroundsources', 'playgroundstatic', 'playgroundtemplate'):
        the_section = SavedFile(user_id, section=section)
        the_section.delete()
    old_email = None
    for user_object in db.session.execute(select(UserModel).filter_by(id=user_id)).scalars():
        old_email = user_object.email
        user_object.active = False
        user_object.first_name = ''
        user_object.last_name = ''
        user_object.nickname = ''
        user_object.email = None
        user_object.country = ''
        user_object.subdivisionfirst = ''
        user_object.subdivisionsecond = ''
        user_object.subdivisionthird = ''
        user_object.organization = ''
        user_object.timezone = None
        user_object.language = None
        user_object.pypi_username = None
        user_object.pypi_password = None
        user_object.otp_secret = None
        user_object.confirmed_at = None
        user_object.last_login = None
        user_object.social_id = 'disabled$' + str(user_id)
    db.session.commit()
    keys_to_delete = set()
    for key in r.keys('*userid:' + str(user_id)):
        keys_to_delete.add(key)
    for key in r.keys('*userid:' + str(user_id) + ':*'):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r.delete(key)
    keys_to_delete = set()
    for key in r_user.keys('*:user:' + str(old_email)):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r_user.delete(key)","for key in r.keys('*userid:' + str(user_id) + ':*'):
    keys_to_delete.add(key)",keys_to_delete = {key for key in r.keys('*userid:' + str(user_id) + ':*')},["keys_to_delete = {key for key in r.keys('*userid:' + str(user_id) + ':*')}"],1,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_webapp/docassemble/webapp/backend.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/backend.py,,delete_user_data$743,"def delete_user_data(user_id, r, r_user):
    db.session.execute(delete(UserDict).where(UserDict.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserDictKeys).where(UserDictKeys.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UploadsUserAuth).where(UploadsUserAuth.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.owner_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(GlobalObjectStorage).where(GlobalObjectStorage.user_id == user_id))
    db.session.commit()
    for package_auth in db.session.execute(select(PackageAuth).filter_by(user_id=user_id)).scalars():
        package_auth.user_id = 1
    db.session.commit()
    files_to_delete = list()
    for short_code_item in db.session.execute(select(Shortener).filter_by(user_id=user_id)).scalars():
        for email in db.session.execute(select(Email).filter_by(short=short_code_item.short)).scalars():
            for attachment in db.session.execute(select(EmailAttachment).filter_by(email_id=email.id)).scalars():
                files_to_delete.append(attachment.upload)
    for file_number in files_to_delete:
        the_file = SavedFile(file_number)
        the_file.delete()
    db.session.execute(delete(Shortener).where(Shortener.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserRoles).where(UserRoles.user_id == user_id))
    db.session.commit()
    for user_auth in db.session.execute(select(UserAuthModel).filter_by(user_id=user_id).with_for_update()).scalars():
        user_auth.password = ''
        user_auth.reset_password_token = ''
    db.session.commit()
    for section in ('playground', 'playgroundmodules', 'playgroundpackages', 'playgroundsources', 'playgroundstatic', 'playgroundtemplate'):
        the_section = SavedFile(user_id, section=section)
        the_section.delete()
    old_email = None
    for user_object in db.session.execute(select(UserModel).filter_by(id=user_id)).scalars():
        old_email = user_object.email
        user_object.active = False
        user_object.first_name = ''
        user_object.last_name = ''
        user_object.nickname = ''
        user_object.email = None
        user_object.country = ''
        user_object.subdivisionfirst = ''
        user_object.subdivisionsecond = ''
        user_object.subdivisionthird = ''
        user_object.organization = ''
        user_object.timezone = None
        user_object.language = None
        user_object.pypi_username = None
        user_object.pypi_password = None
        user_object.otp_secret = None
        user_object.confirmed_at = None
        user_object.last_login = None
        user_object.social_id = 'disabled$' + str(user_id)
    db.session.commit()
    keys_to_delete = set()
    for key in r.keys('*userid:' + str(user_id)):
        keys_to_delete.add(key)
    for key in r.keys('*userid:' + str(user_id) + ':*'):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r.delete(key)
    keys_to_delete = set()
    for key in r_user.keys('*:user:' + str(old_email)):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r_user.delete(key)","for key in r_user.keys('*:user:' + str(old_email)):
    keys_to_delete.add(key)",keys_to_delete = {key for key in r_user.keys('*:user:' + str(old_email))},["keys_to_delete = {key for key in r_user.keys('*:user:' + str(old_email))}"],1,
neutron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_db_sync.py,https://github.com/openstack/neutron/tree/master/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_db_sync.py,OvnNbSynchronizer,sync_port_groups$165,"def sync_port_groups(self, ctx):
    """"""Sync Port Groups between neutron and NB.

        @param ctx: neutron_lib.context
        @type  ctx: object of type neutron_lib.context.Context
        """"""
    neutron_sgs = {}
    neutron_pgs = set()
    with db_api.CONTEXT_READER.using(ctx):
        for sg in self.core_plugin.get_security_groups(ctx):
            pg_name = utils.ovn_port_group_name(sg['id'])
            neutron_pgs.add(pg_name)
            neutron_sgs[pg_name] = sg['id']
        neutron_pgs.add(ovn_const.OVN_DROP_PORT_GROUP_NAME)
    ovn_pgs = set()
    port_groups = self.ovn_api.db_list_rows('Port_Group').execute() or []
    for pg in port_groups:
        ovn_pgs.add(pg.name)
    add_pgs = neutron_pgs.difference(ovn_pgs)
    remove_pgs = ovn_pgs.difference(neutron_pgs)
    LOG.debug('Port Groups added %d, removed %d', len(add_pgs), len(remove_pgs))
    if self.mode == SYNC_MODE_REPAIR:
        LOG.debug('Port-Group-SYNC: transaction started @ %s', str(datetime.now()))
        if add_pgs:
            db_ports = self.core_plugin.get_ports(ctx)
            ovn_ports = set((p.name for p in self.ovn_api.lsp_list().execute()))
        with self.ovn_api.transaction(check_error=True) as txn:
            pg = ovn_const.OVN_DROP_PORT_GROUP_NAME
            if pg in add_pgs:
                txn.add(self.ovn_api.pg_add(name=pg, acls=[]))
                add_pgs.remove(pg)
                for n_port in db_ports:
                    if (utils.is_security_groups_enabled(n_port) or utils.is_port_security_enabled(n_port)) and n_port['id'] in ovn_ports:
                        txn.add(self.ovn_api.pg_add_ports(pg, n_port['id']))
            for pg in add_pgs:
                ext_ids = {ovn_const.OVN_SG_EXT_ID_KEY: neutron_sgs[pg]}
                txn.add(self.ovn_api.pg_add(name=pg, acls=[], external_ids=ext_ids))
                for n_port in db_ports:
                    if neutron_sgs[pg] in n_port['security_groups'] and n_port['id'] in ovn_ports:
                        txn.add(self.ovn_api.pg_add_ports(pg, n_port['id']))
            for pg in remove_pgs:
                txn.add(self.ovn_api.pg_del(pg))
        LOG.debug('Port-Group-SYNC: transaction finished @ %s', str(datetime.now()))","for pg in port_groups:
    ovn_pgs.add(pg.name)",ovn_pgs = {pg.name for pg in port_groups},['ovn_pgs = {pg.name for pg in port_groups}'],1,
neutron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_db_sync.py,https://github.com/openstack/neutron/tree/master/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_db_sync.py,OvnNbSynchronizer,sync_port_groups$165,"def sync_port_groups(self, ctx):
    """"""Sync Port Groups between neutron and NB.

        @param ctx: neutron_lib.context
        @type  ctx: object of type neutron_lib.context.Context
        """"""
    neutron_sgs = {}
    neutron_pgs = set()
    with db_api.CONTEXT_READER.using(ctx):
        for sg in self.core_plugin.get_security_groups(ctx):
            pg_name = utils.ovn_port_group_name(sg['id'])
            neutron_pgs.add(pg_name)
            neutron_sgs[pg_name] = sg['id']
        neutron_pgs.add(ovn_const.OVN_DROP_PORT_GROUP_NAME)
    ovn_pgs = set()
    port_groups = self.ovn_api.db_list_rows('Port_Group').execute() or []
    for pg in port_groups:
        ovn_pgs.add(pg.name)
    add_pgs = neutron_pgs.difference(ovn_pgs)
    remove_pgs = ovn_pgs.difference(neutron_pgs)
    LOG.debug('Port Groups added %d, removed %d', len(add_pgs), len(remove_pgs))
    if self.mode == SYNC_MODE_REPAIR:
        LOG.debug('Port-Group-SYNC: transaction started @ %s', str(datetime.now()))
        if add_pgs:
            db_ports = self.core_plugin.get_ports(ctx)
            ovn_ports = set((p.name for p in self.ovn_api.lsp_list().execute()))
        with self.ovn_api.transaction(check_error=True) as txn:
            pg = ovn_const.OVN_DROP_PORT_GROUP_NAME
            if pg in add_pgs:
                txn.add(self.ovn_api.pg_add(name=pg, acls=[]))
                add_pgs.remove(pg)
                for n_port in db_ports:
                    if (utils.is_security_groups_enabled(n_port) or utils.is_port_security_enabled(n_port)) and n_port['id'] in ovn_ports:
                        txn.add(self.ovn_api.pg_add_ports(pg, n_port['id']))
            for pg in add_pgs:
                ext_ids = {ovn_const.OVN_SG_EXT_ID_KEY: neutron_sgs[pg]}
                txn.add(self.ovn_api.pg_add(name=pg, acls=[], external_ids=ext_ids))
                for n_port in db_ports:
                    if neutron_sgs[pg] in n_port['security_groups'] and n_port['id'] in ovn_ports:
                        txn.add(self.ovn_api.pg_add_ports(pg, n_port['id']))
            for pg in remove_pgs:
                txn.add(self.ovn_api.pg_del(pg))
        LOG.debug('Port-Group-SYNC: transaction finished @ %s', str(datetime.now()))","for pg in add_pgs:
    ext_ids = {ovn_const.OVN_SG_EXT_ID_KEY: neutron_sgs[pg]}
    txn.add(self.ovn_api.pg_add(name=pg, acls=[], external_ids=ext_ids))
    for n_port in db_ports:
        if neutron_sgs[pg] in n_port['security_groups'] and n_port['id'] in ovn_ports:
            txn.add(self.ovn_api.pg_add_ports(pg, n_port['id']))","{txn.add(self.ovn_api.pg_add(name=pg, acls=[], external_ids={ovn_const.OVN_SG_EXT_ID_KEY: neutron_sgs[pg]})) for pg in add_pgs for n_port in db_ports if neutron_sgs[pg] in n_port['security_groups'] and n_port['id'] in ovn_ports}",Cannot refactor,2,
dowhy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dowhy/dowhy/causal_identifiers/backdoor.py,https://github.com/microsoft/dowhy/tree/master/dowhy/causal_identifiers/backdoor.py,HittingSetAlgorithm,_indices_covered$277,"def _indices_covered(self, el, set_index=None):
    """"""
        Obtain indices covered in a particular iteration of the algorithm.
        """"""
    covered = set()
    if set_index == None:
        set_index = set([i for i in range(len(self._list_of_sets))])
    for idx in set_index:
        if el in self._list_of_sets[idx]:
            covered.add(idx)
    return covered","for idx in set_index:
    if el in self._list_of_sets[idx]:
        covered.add(idx)",entries = {idx for idx in set_index for el in self._list_of_sets[idx] if el in self._list_of_sets[idx]},['covered = {idx for idx in set_index if el in self._list_of_sets[idx]}'],0,
zato,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zato/code/zato-common/src/zato/common/odb/api.py,https://github.com/zatosource/zato/tree/master/code/zato-common/src/zato/common/odb/api.py,ODBManager,get_missing_services$601,"def get_missing_services(self, server, locally_deployed):
    """""" Returns services deployed on the server given on input that are not among locally_deployed.
        """"""
    missing = set()
    with closing(self.session()) as session:
        server_services = session.query(Service.id, Service.name, DeployedService.source_path, DeployedService.source).join(DeployedService, Service.id == DeployedService.service_id).join(Server, DeployedService.server_id == Server.id).filter(Service.is_internal != true()).all()
        for item in server_services:
            if item.name not in locally_deployed:
                missing.add(item)
    return missing","for item in server_services:
    if item.name not in locally_deployed:
        missing.add(item)",missing = {item for item in server_services if item.name not in locally_deployed},['missing = {item for item in server_services if item.name not in locally_deployed}'],1,
yt-dlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/YoutubeDL.py,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/YoutubeDL.py,YoutubeDL,__init__$514,"def __init__(self, params=None, auto_init=True):
    """"""Create a FileDownloader object with the given options.
        @param auto_init    Whether to load the default extractors and print header (if verbose).
                            Set to 'no_verbose_header' to not print the header
        """"""
    if params is None:
        params = {}
    self._ies = {}
    self._ies_instances = {}
    self._pps = {'pre_process': [], 'before_dl': [], 'after_move': [], 'post_process': []}
    self._printed_messages = set()
    self._first_webpage_request = True
    self._post_hooks = []
    self._progress_hooks = []
    self._postprocessor_hooks = []
    self._download_retcode = 0
    self._num_downloads = 0
    self._screen_file = [sys.stdout, sys.stderr][params.get('logtostderr', False)]
    self._err_file = sys.stderr
    self.params = params
    self.cache = Cache(self)
    windows_enable_vt_mode()
    self._allow_colors = {'screen': not self.params.get('no_color') and supports_terminal_sequences(self._screen_file), 'err': not self.params.get('no_color') and supports_terminal_sequences(self._err_file)}
    if sys.version_info < (3, 6):
        self.report_warning('Python version %d.%d is not supported! Please update to Python 3.6 or above' % sys.version_info[:2])
    if self.params.get('allow_unplayable_formats'):
        self.report_warning(f""You have asked for {self._format_err('UNPLAYABLE', self.Styles.EMPHASIS)} formats to be listed/downloaded. This is a developer option intended for debugging. \n         If you experience any issues while using this option, {self._format_err('DO NOT', self.Styles.ERROR)} open a bug report"")

    def check_deprecated(param, option, suggestion):
        if self.params.get(param) is not None:
            self.report_warning('%s is deprecated. Use %s instead' % (option, suggestion))
            return True
        return False
    if check_deprecated('cn_verification_proxy', '--cn-verification-proxy', '--geo-verification-proxy'):
        if self.params.get('geo_verification_proxy') is None:
            self.params['geo_verification_proxy'] = self.params['cn_verification_proxy']
    check_deprecated('autonumber', '--auto-number', '-o ""%(autonumber)s-%(title)s.%(ext)s""')
    check_deprecated('usetitle', '--title', '-o ""%(title)s-%(id)s.%(ext)s""')
    check_deprecated('useid', '--id', '-o ""%(id)s.%(ext)s""')
    for msg in self.params.get('_warnings', []):
        self.report_warning(msg)
    for msg in self.params.get('_deprecation_warnings', []):
        self.deprecation_warning(msg)
    if 'list-formats' in self.params.get('compat_opts', []):
        self.params['listformats_table'] = False
    if 'overwrites' not in self.params and self.params.get('nooverwrites') is not None:
        self.params['overwrites'] = not self.params['nooverwrites']
    elif self.params.get('overwrites') is None:
        self.params.pop('overwrites', None)
    else:
        self.params['nooverwrites'] = not self.params['overwrites']
    if params.get('bidi_workaround', False):
        try:
            import pty
            (master, slave) = pty.openpty()
            width = compat_get_terminal_size().columns
            if width is None:
                width_args = []
            else:
                width_args = ['-w', str(width)]
            sp_kwargs = dict(stdin=subprocess.PIPE, stdout=slave, stderr=self._err_file)
            try:
                self._output_process = Popen(['bidiv'] + width_args, **sp_kwargs)
            except OSError:
                self._output_process = Popen(['fribidi', '-c', 'UTF-8'] + width_args, **sp_kwargs)
            self._output_channel = os.fdopen(master, 'rb')
        except OSError as ose:
            if ose.errno == errno.ENOENT:
                self.report_warning('Could not find fribidi executable, ignoring --bidi-workaround. Make sure that  fribidi  is an executable file in one of the directories in your $PATH.')
            else:
                raise
    if sys.platform != 'win32' and sys.getfilesystemencoding() in ['ascii', 'ANSI_X3.4-1968'] and (not params.get('restrictfilenames', False)):
        self.report_warning('Assuming --restrict-filenames since file system encoding cannot encode all characters. Set the LC_ALL environment variable to fix this.')
        self.params['restrictfilenames'] = True
    self.outtmpl_dict = self.parse_outtmpl()
    self.format_selector = None if self.params.get('format') is None else self.params['format'] if callable(self.params['format']) else self.build_format_selector(self.params['format'])
    self._setup_opener()
    if auto_init:
        if auto_init != 'no_verbose_header':
            self.print_debug_header()
        self.add_default_info_extractors()
    for pp_def_raw in self.params.get('postprocessors', []):
        pp_def = dict(pp_def_raw)
        when = pp_def.pop('when', 'post_process')
        pp_class = get_postprocessor(pp_def.pop('key'))
        pp = pp_class(self, **compat_kwargs(pp_def))
        self.add_post_processor(pp, when=when)
    hooks = {'post_hooks': self.add_post_hook, 'progress_hooks': self.add_progress_hook, 'postprocessor_hooks': self.add_postprocessor_hook}
    for (opt, fn) in hooks.items():
        for ph in self.params.get(opt, []):
            fn(ph)
    register_socks_protocols()

    def preload_download_archive(fn):
        """"""Preload the archive, if any is specified""""""
        if fn is None:
            return False
        self.write_debug(f'Loading archive file {fn!r}')
        try:
            with locked_file(fn, 'r', encoding='utf-8') as archive_file:
                for line in archive_file:
                    self.archive.add(line.strip())
        except IOError as ioe:
            if ioe.errno != errno.ENOENT:
                raise
            return False
        return True
    self.archive = set()
    preload_download_archive(self.params.get('download_archive'))","for line in archive_file:
    self.archive.add(line.strip())",entries = {line.strip() for line in archive_file},['self.archive = {line.strip() for line in archive_file}'],0,
watchdog,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/watchdog/src/watchdog/utils/dirsnapshot.py,https://github.com/gorakhargosh/watchdog/tree/master/src/watchdog/utils/dirsnapshot.py,DirectorySnapshotDiff,__init__$82,"def __init__(self, ref, snapshot, ignore_device=False):
    created = snapshot.paths - ref.paths
    deleted = ref.paths - snapshot.paths
    if ignore_device:

        def get_inode(directory, full_path):
            return directory.inode(full_path)[0]
    else:

        def get_inode(directory, full_path):
            return directory.inode(full_path)
    for path in ref.paths & snapshot.paths:
        if get_inode(ref, path) != get_inode(snapshot, path):
            created.add(path)
            deleted.add(path)
    moved = set()
    for path in set(deleted):
        inode = ref.inode(path)
        new_path = snapshot.path(inode)
        if new_path:
            deleted.remove(path)
            moved.add((path, new_path))
    for path in set(created):
        inode = snapshot.inode(path)
        old_path = ref.path(inode)
        if old_path:
            created.remove(path)
            moved.add((old_path, path))
    modified = set()
    for path in ref.paths & snapshot.paths:
        if get_inode(ref, path) == get_inode(snapshot, path):
            if ref.mtime(path) != snapshot.mtime(path) or ref.size(path) != snapshot.size(path):
                modified.add(path)
    for (old_path, new_path) in moved:
        if ref.mtime(old_path) != snapshot.mtime(new_path) or ref.size(old_path) != snapshot.size(new_path):
            modified.add(old_path)
    self._dirs_created = [path for path in created if snapshot.isdir(path)]
    self._dirs_deleted = [path for path in deleted if ref.isdir(path)]
    self._dirs_modified = [path for path in modified if ref.isdir(path)]
    self._dirs_moved = [(frm, to) for (frm, to) in moved if ref.isdir(frm)]
    self._files_created = list(created - set(self._dirs_created))
    self._files_deleted = list(deleted - set(self._dirs_deleted))
    self._files_modified = list(modified - set(self._dirs_modified))
    self._files_moved = list(moved - set(self._dirs_moved))","for path in ref.paths & snapshot.paths:
    if get_inode(ref, path) != get_inode(snapshot, path):
        created.add(path)
        deleted.add(path)","deleted = {path for path in ref.paths & snapshot.paths if get_inode(ref, path) != get_inode(snapshot, path)}",Cannot refactor,2,
watchdog,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/watchdog/src/watchdog/utils/dirsnapshot.py,https://github.com/gorakhargosh/watchdog/tree/master/src/watchdog/utils/dirsnapshot.py,DirectorySnapshotDiff,__init__$82,"def __init__(self, ref, snapshot, ignore_device=False):
    created = snapshot.paths - ref.paths
    deleted = ref.paths - snapshot.paths
    if ignore_device:

        def get_inode(directory, full_path):
            return directory.inode(full_path)[0]
    else:

        def get_inode(directory, full_path):
            return directory.inode(full_path)
    for path in ref.paths & snapshot.paths:
        if get_inode(ref, path) != get_inode(snapshot, path):
            created.add(path)
            deleted.add(path)
    moved = set()
    for path in set(deleted):
        inode = ref.inode(path)
        new_path = snapshot.path(inode)
        if new_path:
            deleted.remove(path)
            moved.add((path, new_path))
    for path in set(created):
        inode = snapshot.inode(path)
        old_path = ref.path(inode)
        if old_path:
            created.remove(path)
            moved.add((old_path, path))
    modified = set()
    for path in ref.paths & snapshot.paths:
        if get_inode(ref, path) == get_inode(snapshot, path):
            if ref.mtime(path) != snapshot.mtime(path) or ref.size(path) != snapshot.size(path):
                modified.add(path)
    for (old_path, new_path) in moved:
        if ref.mtime(old_path) != snapshot.mtime(new_path) or ref.size(old_path) != snapshot.size(new_path):
            modified.add(old_path)
    self._dirs_created = [path for path in created if snapshot.isdir(path)]
    self._dirs_deleted = [path for path in deleted if ref.isdir(path)]
    self._dirs_modified = [path for path in modified if ref.isdir(path)]
    self._dirs_moved = [(frm, to) for (frm, to) in moved if ref.isdir(frm)]
    self._files_created = list(created - set(self._dirs_created))
    self._files_deleted = list(deleted - set(self._dirs_deleted))
    self._files_modified = list(modified - set(self._dirs_modified))
    self._files_moved = list(moved - set(self._dirs_moved))","for path in set(deleted):
    inode = ref.inode(path)
    new_path = snapshot.path(inode)
    if new_path:
        deleted.remove(path)
        moved.add((path, new_path))",deleted = set(path for path in deleted if path not in moved),Cannot refactor,2,
watchdog,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/watchdog/src/watchdog/utils/dirsnapshot.py,https://github.com/gorakhargosh/watchdog/tree/master/src/watchdog/utils/dirsnapshot.py,DirectorySnapshotDiff,__init__$82,"def __init__(self, ref, snapshot, ignore_device=False):
    created = snapshot.paths - ref.paths
    deleted = ref.paths - snapshot.paths
    if ignore_device:

        def get_inode(directory, full_path):
            return directory.inode(full_path)[0]
    else:

        def get_inode(directory, full_path):
            return directory.inode(full_path)
    for path in ref.paths & snapshot.paths:
        if get_inode(ref, path) != get_inode(snapshot, path):
            created.add(path)
            deleted.add(path)
    moved = set()
    for path in set(deleted):
        inode = ref.inode(path)
        new_path = snapshot.path(inode)
        if new_path:
            deleted.remove(path)
            moved.add((path, new_path))
    for path in set(created):
        inode = snapshot.inode(path)
        old_path = ref.path(inode)
        if old_path:
            created.remove(path)
            moved.add((old_path, path))
    modified = set()
    for path in ref.paths & snapshot.paths:
        if get_inode(ref, path) == get_inode(snapshot, path):
            if ref.mtime(path) != snapshot.mtime(path) or ref.size(path) != snapshot.size(path):
                modified.add(path)
    for (old_path, new_path) in moved:
        if ref.mtime(old_path) != snapshot.mtime(new_path) or ref.size(old_path) != snapshot.size(new_path):
            modified.add(old_path)
    self._dirs_created = [path for path in created if snapshot.isdir(path)]
    self._dirs_deleted = [path for path in deleted if ref.isdir(path)]
    self._dirs_modified = [path for path in modified if ref.isdir(path)]
    self._dirs_moved = [(frm, to) for (frm, to) in moved if ref.isdir(frm)]
    self._files_created = list(created - set(self._dirs_created))
    self._files_deleted = list(deleted - set(self._dirs_deleted))
    self._files_modified = list(modified - set(self._dirs_modified))
    self._files_moved = list(moved - set(self._dirs_moved))","for path in set(created):
    inode = snapshot.inode(path)
    old_path = ref.path(inode)
    if old_path:
        created.remove(path)
        moved.add((old_path, path))","created = set(created) - {path for (_, path) in moved}",Cannot refactor,2,
watchdog,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/watchdog/src/watchdog/utils/dirsnapshot.py,https://github.com/gorakhargosh/watchdog/tree/master/src/watchdog/utils/dirsnapshot.py,DirectorySnapshotDiff,__init__$82,"def __init__(self, ref, snapshot, ignore_device=False):
    created = snapshot.paths - ref.paths
    deleted = ref.paths - snapshot.paths
    if ignore_device:

        def get_inode(directory, full_path):
            return directory.inode(full_path)[0]
    else:

        def get_inode(directory, full_path):
            return directory.inode(full_path)
    for path in ref.paths & snapshot.paths:
        if get_inode(ref, path) != get_inode(snapshot, path):
            created.add(path)
            deleted.add(path)
    moved = set()
    for path in set(deleted):
        inode = ref.inode(path)
        new_path = snapshot.path(inode)
        if new_path:
            deleted.remove(path)
            moved.add((path, new_path))
    for path in set(created):
        inode = snapshot.inode(path)
        old_path = ref.path(inode)
        if old_path:
            created.remove(path)
            moved.add((old_path, path))
    modified = set()
    for path in ref.paths & snapshot.paths:
        if get_inode(ref, path) == get_inode(snapshot, path):
            if ref.mtime(path) != snapshot.mtime(path) or ref.size(path) != snapshot.size(path):
                modified.add(path)
    for (old_path, new_path) in moved:
        if ref.mtime(old_path) != snapshot.mtime(new_path) or ref.size(old_path) != snapshot.size(new_path):
            modified.add(old_path)
    self._dirs_created = [path for path in created if snapshot.isdir(path)]
    self._dirs_deleted = [path for path in deleted if ref.isdir(path)]
    self._dirs_modified = [path for path in modified if ref.isdir(path)]
    self._dirs_moved = [(frm, to) for (frm, to) in moved if ref.isdir(frm)]
    self._files_created = list(created - set(self._dirs_created))
    self._files_deleted = list(deleted - set(self._dirs_deleted))
    self._files_modified = list(modified - set(self._dirs_modified))
    self._files_moved = list(moved - set(self._dirs_moved))","for path in ref.paths & snapshot.paths:
    if get_inode(ref, path) == get_inode(snapshot, path):
        if ref.mtime(path) != snapshot.mtime(path) or ref.size(path) != snapshot.size(path):
            modified.add(path)","modified = {path for path in ref.paths & snapshot.paths if get_inode(ref, path) == get_inode(snapshot, path) and (ref.mtime(path) != snapshot.mtime(path) or ref.size(path) != snapshot.size(path))}","['modified = {path for path in ref.paths & snapshot.paths if get_inode(ref, path) == get_inode(snapshot, path) if ref.mtime(path) != snapshot.mtime(path) or ref.size(path) != snapshot.size(path)}']",0,
watchdog,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/watchdog/src/watchdog/utils/dirsnapshot.py,https://github.com/gorakhargosh/watchdog/tree/master/src/watchdog/utils/dirsnapshot.py,DirectorySnapshotDiff,__init__$82,"def __init__(self, ref, snapshot, ignore_device=False):
    created = snapshot.paths - ref.paths
    deleted = ref.paths - snapshot.paths
    if ignore_device:

        def get_inode(directory, full_path):
            return directory.inode(full_path)[0]
    else:

        def get_inode(directory, full_path):
            return directory.inode(full_path)
    for path in ref.paths & snapshot.paths:
        if get_inode(ref, path) != get_inode(snapshot, path):
            created.add(path)
            deleted.add(path)
    moved = set()
    for path in set(deleted):
        inode = ref.inode(path)
        new_path = snapshot.path(inode)
        if new_path:
            deleted.remove(path)
            moved.add((path, new_path))
    for path in set(created):
        inode = snapshot.inode(path)
        old_path = ref.path(inode)
        if old_path:
            created.remove(path)
            moved.add((old_path, path))
    modified = set()
    for path in ref.paths & snapshot.paths:
        if get_inode(ref, path) == get_inode(snapshot, path):
            if ref.mtime(path) != snapshot.mtime(path) or ref.size(path) != snapshot.size(path):
                modified.add(path)
    for (old_path, new_path) in moved:
        if ref.mtime(old_path) != snapshot.mtime(new_path) or ref.size(old_path) != snapshot.size(new_path):
            modified.add(old_path)
    self._dirs_created = [path for path in created if snapshot.isdir(path)]
    self._dirs_deleted = [path for path in deleted if ref.isdir(path)]
    self._dirs_modified = [path for path in modified if ref.isdir(path)]
    self._dirs_moved = [(frm, to) for (frm, to) in moved if ref.isdir(frm)]
    self._files_created = list(created - set(self._dirs_created))
    self._files_deleted = list(deleted - set(self._dirs_deleted))
    self._files_modified = list(modified - set(self._dirs_modified))
    self._files_moved = list(moved - set(self._dirs_moved))","for (old_path, new_path) in moved:
    if ref.mtime(old_path) != snapshot.mtime(new_path) or ref.size(old_path) != snapshot.size(new_path):
        modified.add(old_path)","modified = {old_path for (old_path, new_path) in moved if ref.mtime(old_path) != snapshot.mtime(new_path) or ref.size(old_path) != snapshot.size(new_path)}",Cannot refactor,2,
weblate,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/weblate/weblate/checks/icu.py,https://github.com/WeblateOrg/weblate/tree/master/weblate/checks/icu.py,ICUMessageFormatCheck,check_bad_submessage$393,"def check_bad_submessage(self, result, name, data, src_data, flags):
    """"""Detect any bad sub-message selectors.""""""
    if '-submessage_selectors' in flags:
        return
    bad = set()
    if src_data and 'select' in data['types'] and ('select' in src_data['types']) and ('choices' in data) and ('choices' in src_data):
        choices = data['choices']
        src_choices = src_data['choices']
        for selector in choices:
            if selector not in src_choices:
                bad.add(selector)
    if bad:
        result['bad_submessage'].append([name, bad])","for selector in choices:
    if selector not in src_choices:
        bad.add(selector)",bad = {selector for selector in choices if selector not in src_choices},['bad = {selector for selector in choices if selector not in src_choices}'],1,
ShuiZe_0x727,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ShuiZe_0x727/Plugins/infoGather/subdomain/Sublist3r/sublist3r.py,https://github.com/0x727/ShuiZe_0x727/tree/master/Plugins/infoGather/subdomain/Sublist3r/sublist3r.py,,main$866,"def main(domain, threads, savefile, ports, silent, verbose, enable_bruteforce, engines):
    bruteforce_list = set()
    search_list = set()
    if is_windows:
        subdomains_queue = list()
    else:
        return []
    if enable_bruteforce or enable_bruteforce is None:
        enable_bruteforce = True
    domain_check = re.compile('^(http|https)?[a-zA-Z0-9]+([\\-\\.]{1}[a-zA-Z0-9]+)*\\.[a-zA-Z]{2,}$')
    if not domain_check.match(domain):
        if not silent:
            print(R + 'Error: Please enter a valid domain' + W)
        return []
    if not domain.startswith('http://') or not domain.startswith('https://'):
        domain = 'http://' + domain
    parsed_domain = urlparse.urlparse(domain)
    if not silent:
        pass
    if verbose and (not silent):
        pass
    supported_engines = {'baidu': BaiduEnum, 'yahoo': YahooEnum, 'bing': BingEnum, 'ask': AskEnum, 'netcraft': NetcraftEnum, 'dnsdumpster': DNSdumpster, 'virustotal': Virustotal, 'threatcrowd': ThreatCrowd, 'ssl': CrtSearch, 'passivedns': PassiveDNS}
    chosenEnums = []
    if engines is None:
        chosenEnums = [BaiduEnum, YahooEnum, GoogleEnum, BingEnum, AskEnum, NetcraftEnum, DNSdumpster, Virustotal, ThreatCrowd, CrtSearch, PassiveDNS]
    else:
        engines = engines.split(',')
        for engine in engines:
            if engine.lower() in supported_engines:
                chosenEnums.append(supported_engines[engine.lower()])
    enums = [enum(domain, [], q=subdomains_queue, silent=silent, verbose=verbose) for enum in chosenEnums]
    for enum in enums:
        enum.start()
    for enum in enums:
        enum.join()
    subdomains = set(subdomains_queue)
    for subdomain in subdomains:
        search_list.add(subdomain)
    if enable_bruteforce:
        if not silent:
            print(G + '[-] Starting bruteforce module now using subbrute..' + W)
        record_type = False
        path_to_file = os.path.dirname(os.path.realpath(__file__))
        subs = os.path.join(path_to_file, 'subbrute', 'names.txt')
        resolvers = os.path.join(path_to_file, 'subbrute', 'resolvers.txt')
        process_count = threads
        output = False
        json_output = False
        bruteforce_list = subbrute.print_target(parsed_domain.netloc, record_type, subs, resolvers, process_count, output, json_output, search_list, verbose)
    subdomains = search_list.union(bruteforce_list)
    if subdomains:
        subdomains = sorted(subdomains, key=subdomain_sorting_key)
        if savefile:
            write_file(savefile, subdomains)
        if not silent:
            print(Y + '[+] Total Unique Subdomains Found: %s' % len(subdomains) + W)
        if ports:
            if not silent:
                print(G + '[-] Start port scan now for the following ports: %s%s' % (Y, ports) + W)
            ports = ports.split(',')
            pscan = portscan(subdomains, ports)
            pscan.run()
        elif not silent:
            for subdomain in subdomains:
                pass
    return subdomains","for subdomain in subdomains:
    search_list.add(subdomain)",search_list = {subdomain for subdomain in subdomains},['search_list = {subdomain for subdomain in subdomains}'],1,
pandas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandas/pandas/tests/plotting/test_datetimelike.py,https://github.com/pandas-dev/pandas/tree/master/pandas/tests/plotting/test_datetimelike.py,TestTSPlot,test_secondary_legend$1122,"def test_secondary_legend(self):
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df = tm.makeTimeDataFrame()
    df.plot(secondary_y=['A', 'B'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert leg.get_texts()[0].get_text() == 'A (right)'
    assert leg.get_texts()[1].get_text() == 'B (right)'
    assert leg.get_texts()[2].get_text() == 'C'
    assert leg.get_texts()[3].get_text() == 'D'
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close(fig)
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df.plot(secondary_y=['A', 'C'], mark_right=False, ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert leg.get_texts()[0].get_text() == 'A'
    assert leg.get_texts()[1].get_text() == 'B'
    assert leg.get_texts()[2].get_text() == 'C'
    assert leg.get_texts()[3].get_text() == 'D'
    self.plt.close(fig)
    (fig, ax) = self.plt.subplots()
    df.plot(kind='bar', secondary_y=['A'], ax=ax)
    leg = ax.get_legend()
    assert leg.get_texts()[0].get_text() == 'A (right)'
    assert leg.get_texts()[1].get_text() == 'B'
    self.plt.close(fig)
    (fig, ax) = self.plt.subplots()
    df.plot(kind='bar', secondary_y=['A'], mark_right=False, ax=ax)
    leg = ax.get_legend()
    assert leg.get_texts()[0].get_text() == 'A'
    assert leg.get_texts()[1].get_text() == 'B'
    self.plt.close(fig)
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df = tm.makeTimeDataFrame()
    ax = df.plot(secondary_y=['C', 'D'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close(fig)
    df = tm.makeDataFrame()
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    ax = df.plot(secondary_y=['A', 'B'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close()
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    ax = df.plot(secondary_y=['C', 'D'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4","for line in leg.get_lines():
    colors.add(line.get_color())",colors = {line.get_color() for line in leg.get_lines()},['colors = {line.get_color() for line in leg.get_lines()}'],1,
pandas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandas/pandas/tests/plotting/test_datetimelike.py,https://github.com/pandas-dev/pandas/tree/master/pandas/tests/plotting/test_datetimelike.py,TestTSPlot,test_secondary_legend$1122,"def test_secondary_legend(self):
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df = tm.makeTimeDataFrame()
    df.plot(secondary_y=['A', 'B'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert leg.get_texts()[0].get_text() == 'A (right)'
    assert leg.get_texts()[1].get_text() == 'B (right)'
    assert leg.get_texts()[2].get_text() == 'C'
    assert leg.get_texts()[3].get_text() == 'D'
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close(fig)
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df.plot(secondary_y=['A', 'C'], mark_right=False, ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert leg.get_texts()[0].get_text() == 'A'
    assert leg.get_texts()[1].get_text() == 'B'
    assert leg.get_texts()[2].get_text() == 'C'
    assert leg.get_texts()[3].get_text() == 'D'
    self.plt.close(fig)
    (fig, ax) = self.plt.subplots()
    df.plot(kind='bar', secondary_y=['A'], ax=ax)
    leg = ax.get_legend()
    assert leg.get_texts()[0].get_text() == 'A (right)'
    assert leg.get_texts()[1].get_text() == 'B'
    self.plt.close(fig)
    (fig, ax) = self.plt.subplots()
    df.plot(kind='bar', secondary_y=['A'], mark_right=False, ax=ax)
    leg = ax.get_legend()
    assert leg.get_texts()[0].get_text() == 'A'
    assert leg.get_texts()[1].get_text() == 'B'
    self.plt.close(fig)
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df = tm.makeTimeDataFrame()
    ax = df.plot(secondary_y=['C', 'D'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close(fig)
    df = tm.makeDataFrame()
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    ax = df.plot(secondary_y=['A', 'B'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close()
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    ax = df.plot(secondary_y=['C', 'D'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4","for line in leg.get_lines():
    colors.add(line.get_color())",colors = {line.get_color() for line in leg.get_lines()},['colors = {line.get_color() for line in leg.get_lines()}'],1,
pandas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandas/pandas/tests/plotting/test_datetimelike.py,https://github.com/pandas-dev/pandas/tree/master/pandas/tests/plotting/test_datetimelike.py,TestTSPlot,test_secondary_legend$1122,"def test_secondary_legend(self):
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df = tm.makeTimeDataFrame()
    df.plot(secondary_y=['A', 'B'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert leg.get_texts()[0].get_text() == 'A (right)'
    assert leg.get_texts()[1].get_text() == 'B (right)'
    assert leg.get_texts()[2].get_text() == 'C'
    assert leg.get_texts()[3].get_text() == 'D'
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close(fig)
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df.plot(secondary_y=['A', 'C'], mark_right=False, ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert leg.get_texts()[0].get_text() == 'A'
    assert leg.get_texts()[1].get_text() == 'B'
    assert leg.get_texts()[2].get_text() == 'C'
    assert leg.get_texts()[3].get_text() == 'D'
    self.plt.close(fig)
    (fig, ax) = self.plt.subplots()
    df.plot(kind='bar', secondary_y=['A'], ax=ax)
    leg = ax.get_legend()
    assert leg.get_texts()[0].get_text() == 'A (right)'
    assert leg.get_texts()[1].get_text() == 'B'
    self.plt.close(fig)
    (fig, ax) = self.plt.subplots()
    df.plot(kind='bar', secondary_y=['A'], mark_right=False, ax=ax)
    leg = ax.get_legend()
    assert leg.get_texts()[0].get_text() == 'A'
    assert leg.get_texts()[1].get_text() == 'B'
    self.plt.close(fig)
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df = tm.makeTimeDataFrame()
    ax = df.plot(secondary_y=['C', 'D'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close(fig)
    df = tm.makeDataFrame()
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    ax = df.plot(secondary_y=['A', 'B'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close()
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    ax = df.plot(secondary_y=['C', 'D'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4","for line in leg.get_lines():
    colors.add(line.get_color())",colors = {line.get_color() for line in leg.get_lines()},['colors = {line.get_color() for line in leg.get_lines()}'],1,
pandas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandas/pandas/tests/plotting/test_datetimelike.py,https://github.com/pandas-dev/pandas/tree/master/pandas/tests/plotting/test_datetimelike.py,TestTSPlot,test_secondary_legend$1122,"def test_secondary_legend(self):
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df = tm.makeTimeDataFrame()
    df.plot(secondary_y=['A', 'B'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert leg.get_texts()[0].get_text() == 'A (right)'
    assert leg.get_texts()[1].get_text() == 'B (right)'
    assert leg.get_texts()[2].get_text() == 'C'
    assert leg.get_texts()[3].get_text() == 'D'
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close(fig)
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df.plot(secondary_y=['A', 'C'], mark_right=False, ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert leg.get_texts()[0].get_text() == 'A'
    assert leg.get_texts()[1].get_text() == 'B'
    assert leg.get_texts()[2].get_text() == 'C'
    assert leg.get_texts()[3].get_text() == 'D'
    self.plt.close(fig)
    (fig, ax) = self.plt.subplots()
    df.plot(kind='bar', secondary_y=['A'], ax=ax)
    leg = ax.get_legend()
    assert leg.get_texts()[0].get_text() == 'A (right)'
    assert leg.get_texts()[1].get_text() == 'B'
    self.plt.close(fig)
    (fig, ax) = self.plt.subplots()
    df.plot(kind='bar', secondary_y=['A'], mark_right=False, ax=ax)
    leg = ax.get_legend()
    assert leg.get_texts()[0].get_text() == 'A'
    assert leg.get_texts()[1].get_text() == 'B'
    self.plt.close(fig)
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    df = tm.makeTimeDataFrame()
    ax = df.plot(secondary_y=['C', 'D'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close(fig)
    df = tm.makeDataFrame()
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    ax = df.plot(secondary_y=['A', 'B'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4
    self.plt.close()
    fig = self.plt.figure()
    ax = fig.add_subplot(211)
    ax = df.plot(secondary_y=['C', 'D'], ax=ax)
    leg = ax.get_legend()
    assert len(leg.get_lines()) == 4
    assert ax.right_ax.get_legend() is None
    colors = set()
    for line in leg.get_lines():
        colors.add(line.get_color())
    assert len(colors) == 4","for line in leg.get_lines():
    colors.add(line.get_color())",colors = {line.get_color() for line in leg.get_lines()},['colors = {line.get_color() for line in leg.get_lines()}'],1,
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tools/c7n_gcp/tests/test_resourcemanager.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_gcp/tests/test_resourcemanager.py,ProjectTest,test_project_iam_policy_value_filter$311,"def test_project_iam_policy_value_filter(self):
    factory = self.replay_flight_data('project-iam-policy')
    p = self.load_policy({'name': 'resource', 'resource': 'gcp.project', 'filters': [{'type': 'iam-policy', 'doc': {'key': 'bindings[*].members[]', 'op': 'contains', 'value': 'user:abc@gmail.com'}}]}, session_factory=factory)
    resources = p.run()
    self.assertEqual(len(resources), 3)
    for resource in resources:
        self.assertTrue('c7n:iamPolicy' in resource)
        bindings = resource['c7n:iamPolicy']['bindings']
        members = set()
        for binding in bindings:
            for member in binding['members']:
                members.add(member)
        self.assertTrue('user:abc@gmail.com' in members)","for resource in resources:
    self.assertTrue('c7n:iamPolicy' in resource)
    bindings = resource['c7n:iamPolicy']['bindings']
    members = set()
    for binding in bindings:
        for member in binding['members']:
            members.add(member)
    self.assertTrue('user:abc@gmail.com' in members)",entries = {member for resource in resources if 'c7n:iamPolicy' in resource for binding in resource['c7n:iamPolicy']['bindings'] for member in binding['members'] if 'user:abc@gmail.com' in member},Cannot refactor,2,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/win_service.py,https://github.com/saltstack/salt/tree/master/salt/modules/win_service.py,,get_disabled$142,"def get_disabled():
    """"""
    Return a list of disabled services. Disabled is defined as a service that is
    marked 'Disabled' or 'Manual'.

    Returns:
        list: A list of disabled services.

    CLI Example:

    .. code-block:: bash

        salt '*' service.get_disabled
    """"""
    raw_services = _get_services()
    services = set()
    for service in raw_services:
        if info(service['ServiceName'])['StartType'] in ['Manual', 'Disabled']:
            services.add(service['ServiceName'])
    return sorted(services)","for service in raw_services:
    if info(service['ServiceName'])['StartType'] in ['Manual', 'Disabled']:
        services.add(service['ServiceName'])","services = {service['ServiceName'] for service in raw_services if info(service['ServiceName'])['StartType'] in ['Manual', 'Disabled']}","[""services = {service['ServiceName'] for service in raw_services if info(service['ServiceName'])['StartType'] in ['Manual', 'Disabled']}""]",1,
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/lib/resolver.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/resolver.py,,get_admin_resolvers$285,"def get_admin_resolvers():
    """""" """"""
    admin_realm_name = current_app.config['ADMIN_REALM_NAME'].lower()
    admin_realm_definition = getRealms(admin_realm_name).get(admin_realm_name, {})
    if not admin_realm_definition:
        return []
    admin_resolvers = set()
    for resolver_spec in admin_realm_definition['useridresolver']:
        admin_resolvers.add(resolver_spec.rpartition('.')[2])
    return list(admin_resolvers)","for resolver_spec in admin_realm_definition['useridresolver']:
    admin_resolvers.add(resolver_spec.rpartition('.')[2])",admin_resolvers = {resolver_spec.rpartition('.')[2] for resolver_spec in admin_realm_definition['useridresolver']},["admin_resolvers = {resolver_spec.rpartition('.')[2] for resolver_spec in admin_realm_definition['useridresolver']}"],1,
taurus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taurus/bzt/modules/blazemeter/blazemeter_reporter.py,https://github.com/Blazemeter/taurus/tree/master/bzt/modules/blazemeter/blazemeter_reporter.py,BlazeMeterUploader,__get_jtls_and_more$177,"def __get_jtls_and_more(self):
    """"""
        Compress all files in artifacts dir to single zipfile
        :rtype: (io.BytesIO,dict)
        """"""
    mfile = BytesIO()
    listing = {}
    logs = set()
    for handler in self.engine.log.parent.handlers:
        if isinstance(handler, logging.FileHandler):
            logs.add(handler.baseFilename)
    max_file_size = self.settings.get('artifact-upload-size-limit', 10) * 1024 * 1024
    with zipfile.ZipFile(mfile, mode='w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as zfh:
        for (root, _, files) in os.walk(self.engine.artifacts_dir):
            for filename in files:
                full_path = os.path.join(root, filename)
                if full_path in logs:
                    logs.remove(full_path)
                fsize = os.path.getsize(full_path)
                if fsize <= max_file_size:
                    zfh.write(full_path, os.path.join(os.path.relpath(root, self.engine.artifacts_dir), filename))
                    listing[full_path] = fsize
                else:
                    msg = ""File %s exceeds maximum size quota of %s and won't be included into upload""
                    self.log.warning(msg, filename, max_file_size)
        for filename in logs:
            zfh.write(filename, os.path.basename(filename))
            listing[filename] = os.path.getsize(filename)
    return (mfile, listing)","for handler in self.engine.log.parent.handlers:
    if isinstance(handler, logging.FileHandler):
        logs.add(handler.baseFilename)","logs = {handler.baseFilename for handler in self.engine.log.parent.handlers if isinstance(handler, logging.FileHandler)}","['logs = {handler.baseFilename for handler in self.engine.log.parent.handlers if isinstance(handler, logging.FileHandler)}']",1,
toil,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/toil/src/toil/test/batchSystems/batchSystemTest.py,https://github.com/DataBiosphere/toil/tree/master/src/toil/test/batchSystems/batchSystemTest.py,MaxCoresSingleMachineBatchSystemTest,test$867,"def test(self):
    F = Fraction
    minCores = F(1, 10)
    self.assertEqual(float(minCores), SingleMachineBatchSystem.minCores)
    for maxCores in {F(minCores), minCores * 10, F(1), F(numCores, 2), F(numCores)}:
        for coresPerJob in {F(minCores), F(minCores * 10), F(1), F(maxCores, 2), F(maxCores)}:
            for load in (F(1, 10), F(1), F(10)):
                jobs = int(maxCores / coresPerJob * load)
                if jobs >= 1 and minCores <= coresPerJob < maxCores:
                    self.assertEqual(maxCores, float(maxCores))
                    bs = SingleMachineBatchSystem(config=hidden.AbstractBatchSystemTest.createConfig(), maxCores=float(maxCores), maxMemory=jobs * 10, maxDisk=jobs * 10)
                    try:
                        jobIds = set()
                        for i in range(0, int(jobs)):
                            jobIds.add(bs.issueBatchJob(JobDescription(command=self.scriptCommand(), requirements=dict(cores=float(coresPerJob), memory=1, disk=1, accelerators=[], preemptable=preemptable), jobName=str(i), unitName='')))
                        self.assertEqual(len(jobIds), jobs)
                        while jobIds:
                            job = bs.getUpdatedBatchJob(maxWait=10)
                            self.assertIsNotNone(job)
                            (jobId, status, wallTime) = (job.jobID, job.exitStatus, job.wallTime)
                            self.assertEqual(status, 0)
                            jobIds.remove(jobId)
                    finally:
                        bs.shutdown()
                    (concurrentTasks, maxConcurrentTasks) = getCounters(self.counterPath)
                    self.assertEqual(concurrentTasks, 0)
                    logger.info('maxCores: {maxCores}, coresPerJob: {coresPerJob}, load: {load}'.format(**locals()))
                    expectedMaxConcurrentTasks = min(maxCores // coresPerJob, jobs)
                    self.assertEqual(maxConcurrentTasks, expectedMaxConcurrentTasks)
                    resetCounters(self.counterPath)","for i in range(0, int(jobs)):
    jobIds.add(bs.issueBatchJob(JobDescription(command=self.scriptCommand(), requirements=dict(cores=float(coresPerJob), memory=1, disk=1, accelerators=[], preemptable=preemptable), jobName=str(i), unitName='')))","jobIds = {bs.issueBatchJob(JobDescription(command=self.scriptCommand(), requirements=dict(cores=float(coresPerJob), memory=1, disk=1, accelerators=[], preemptable=preemptable), jobName=str(i), unitName='')) for i in range(0, int(jobs))}","[""jobIds = {bs.issueBatchJob(JobDescription(command=self.scriptCommand(), requirements=dict(cores=float(coresPerJob), memory=1, disk=1, accelerators=[], preemptable=preemptable), jobName=str(i), unitName='')) for i in range(0, int(jobs))}""]",1,
saleor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/saleor/saleor/graphql/attribute/utils.py,https://github.com/saleor/saleor/tree/master/saleor/graphql/attribute/utils.py,AttributeAssignmentMixin,_resolve_attribute_nodes$99,"def _resolve_attribute_nodes(cls, qs: 'QuerySet', error_class, *, global_ids: List[str], pks: Iterable[int]):
    """"""Retrieve attributes nodes from given global IDs.""""""
    qs = qs.filter(pk__in=pks)
    nodes: List[attribute_models.Attribute] = list(qs)
    if not nodes:
        raise ValidationError(f'Could not resolve to a node: ids={global_ids}.', code=error_class.NOT_FOUND.value)
    nodes_pk_list = set()
    for node in nodes:
        nodes_pk_list.add(node.pk)
    for (pk, global_id) in zip(pks, global_ids):
        if pk not in nodes_pk_list:
            raise ValidationError(f'Could not resolve {global_id!r} to Attribute', code=error_class.NOT_FOUND.value)
    return nodes","for node in nodes:
    nodes_pk_list.add(node.pk)",nodes_pk_list = {node.pk for node in nodes},['nodes_pk_list = {node.pk for node in nodes}'],1,
OctoPrint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OctoPrint/versioneer.py,https://github.com/OctoPrint/OctoPrint/tree/master//versioneer.py,,do_setup$2137,"def do_setup():
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print('Adding sample versioneer config to setup.cfg', file=sys.stderr)
            with io.open(os.path.join(root, 'setup.cfg'), 'at', encoding='utf-8') as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1
    print(' creating %s' % cfg.versionfile_source)
    with io.open(cfg.versionfile_source, 'wt', encoding='utf-8') as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {'DOLLAR': '$', 'STYLE': cfg.style, 'TAG_PREFIX': cfg.tag_prefix, 'PARENTDIR_PREFIX': cfg.parentdir_prefix, 'VERSIONFILE_SOURCE': cfg.versionfile_source, 'LOOKUP_FILE': cfg.lookupfile})
    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), '__init__.py')
    if os.path.exists(ipy):
        try:
            with io.open(ipy, 'rt', encoding='utf-8') as f:
                old = f.read()
        except EnvironmentError:
            old = ''
        if 'from ._version import get_versions' not in old:
            print(' appending to %s' % ipy)
            with io.open(ipy, 'at', encoding='utf-8') as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print(' %s unmodified' % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None
    manifest_in = os.path.join(root, 'MANIFEST.in')
    simple_includes = set()
    try:
        with io.open(manifest_in, 'r') as f:
            for line in f:
                if line.startswith('include '):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    if 'versioneer.py' not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with io.open(manifest_in, 'at', encoding='utf-8') as f:
            f.write('include versioneer.py\n')
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" % cfg.versionfile_source)
        with io.open(manifest_in, 'at', encoding='utf-8') as f:
            f.write('include %s\n' % cfg.versionfile_source)
    else:
        print(' versionfile_source already in MANIFEST.in')
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f for include in line.split()[1:] if line.startswith('include ')},["simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}"],0,
petastorm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/petastorm/petastorm/tests/test_predicates.py,https://github.com/uber/petastorm/tree/master/petastorm/tests/test_predicates.py,,test_and_argegarion$86,"def test_and_argegarion(all_values):
    for values1 in [{'guid_0', 'guid_1'}, {'guid_3', 'guid_6', 'guid_20'}, {'guid_2'}]:
        for values2 in [{'guid_2', 'guid_1'}, {'guid_5', 'guid_9'}, {'guid_2'}]:
            test_predicate = in_reduce([in_set(values1, 'volume_guid'), in_set(values2, 'volume_guid')], all)
            included_values = set()
            for val in all_values:
                if test_predicate.do_include({'volume_guid': val}):
                    included_values.add(val)
            assert included_values == values1.intersection(values2)","for values1 in [{'guid_0', 'guid_1'}, {'guid_3', 'guid_6', 'guid_20'}, {'guid_2'}]:
    for values2 in [{'guid_2', 'guid_1'}, {'guid_5', 'guid_9'}, {'guid_2'}]:
        test_predicate = in_reduce([in_set(values1, 'volume_guid'), in_set(values2, 'volume_guid')], all)
        included_values = set()
        for val in all_values:
            if test_predicate.do_include({'volume_guid': val}):
                included_values.add(val)
        assert included_values == values1.intersection(values2)",```,Cannot refactor,2,
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/tools/automation/cli_linter/linter.py,https://github.com/Azure/azure-cli/tree/master/tools/automation/cli_linter/linter.py,Linter,__init__$16,"def __init__(self, command_loader=None, help_file_entries=None, loaded_help=None):
    self._all_yaml_help = help_file_entries
    self._loaded_help = loaded_help
    self._command_loader = command_loader
    self._parameters = {}
    self._help_file_entries = set(help_file_entries.keys())
    self._command_parser = command_loader.cli_ctx.invocation.parser
    for (command_name, command) in self._command_loader.command_table.items():
        self._parameters[command_name] = set()
        for (name, param) in command.arguments.items():
            self._parameters[command_name].add(name)","for (command_name, command) in self._command_loader.command_table.items():
    self._parameters[command_name] = set()
    for (name, param) in command.arguments.items():
        self._parameters[command_name].add(name)","parameters = {command_name: {name for (name, param) in command.arguments.items()} for (command_name, command) in self._command_loader.command_table.items()}",Cannot refactor,2,
petastorm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/petastorm/petastorm/tests/test_predicates.py,https://github.com/uber/petastorm/tree/master/petastorm/tests/test_predicates.py,,test_pseudorandom_split_on_integer_field$123,"def test_pseudorandom_split_on_integer_field():
    split_list = [0.3, 0.4, 0.1, 0.0, 0.2]
    int_values = list(range(1000))
    values_num = len(int_values)
    for (idx, _) in enumerate(split_list):
        test_predicate = in_pseudorandom_split(split_list, idx, 'int_partitioning_field')
        included_values = set()
        for val in int_values:
            if test_predicate.do_include({'int_partitioning_field': val}):
                included_values.add(val)
        expected_num = values_num * split_list[idx]
        assert pytest.approx(len(included_values), expected_num * 0.1) == expected_num","for (idx, _) in enumerate(split_list):
    test_predicate = in_pseudorandom_split(split_list, idx, 'int_partitioning_field')
    included_values = set()
    for val in int_values:
        if test_predicate.do_include({'int_partitioning_field': val}):
            included_values.add(val)
    expected_num = values_num * split_list[idx]
    assert pytest.approx(len(included_values), expected_num * 0.1) == expected_num","{assert pytest.approx(len({val for val in int_values if test_predicate.do_include({'int_partitioning_field': val})}), values_num * split_list[idx] * 0.1) == values_num * split_list[idx] for (idx, _) in enumerate(split_list)}",Cannot refactor,2,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/qiskit/transpiler/passes/optimization/collect_multiqubit_blocks.py,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/transpiler/passes/optimization/collect_multiqubit_blocks.py,CollectMultiQBlocks,run$88,"def run(self, dag):
    """"""Run the CollectMultiQBlocks pass on `dag`.

        The blocks contain ""op"" nodes in topological sort order
        such that all gates in a block act on the same set of
        qubits and are adjacent in the circuit.

        The blocks are built by examining predecessors and successors of
        ""cx"" gates in the circuit. u1, u2, u3, cx, id gates will be included.

        After the execution, ``property_set['block_list']`` is set to
        a list of tuples of ``DAGNode`` objects
        """"""
    self.parent = {}
    self.bit_groups = {}
    self.gate_groups = {}
    block_list = []

    def collect_key(x):
        """"""special key function for topological ordering.
            Heuristic for this is to push all gates involving measurement
            or barriers, etc. as far back as possible (because they force
            blocks to end). After that, we process gates in order of lowest
            number of qubits acted on to largest number of qubits acted on
            because these have less chance of increasing the size of blocks
            The key also processes all the non operation notes first so that
            input nodes do not mess with the top sort of op nodes
            """"""
        if isinstance(x, DAGInNode):
            return 'a'
        if not isinstance(x, DAGOpNode):
            return 'd'
        if isinstance(x.op, Gate):
            if x.op.is_parameterized() or getattr(x.op, 'condition', None) is not None:
                return 'c'
            return 'b' + chr(ord('a') + len(x.qargs))
        return 'd'
    op_nodes = dag.topological_op_nodes(key=collect_key)
    qubit_indices = {bit: index for (index, bit) in enumerate(dag.qubits)}
    for nd in op_nodes:
        can_process = True
        makes_too_big = False
        if getattr(nd.op, 'condition', None) is not None or nd.op.is_parameterized() or (not isinstance(nd.op, Gate)):
            can_process = False
        cur_qubits = {qubit_indices[bit] for bit in nd.qargs}
        if can_process:
            c_tops = set()
            for bit in cur_qubits:
                c_tops.add(self.find_set(bit))
            tot_size = 0
            for group in c_tops:
                tot_size += len(self.bit_groups[group])
            if tot_size > self.max_block_size:
                makes_too_big = True
        if not can_process:
            for bit in cur_qubits:
                bit = self.find_set(bit)
                if len(self.gate_groups[bit]) == 0:
                    continue
                block_list.append(self.gate_groups[bit][:])
                cur_set = set(self.bit_groups[bit])
                for v in cur_set:
                    self.parent[v] = v
                    self.bit_groups[v] = [v]
                    self.gate_groups[v] = []
        if makes_too_big:
            savings = {}
            tot_size = 0
            for bit in cur_qubits:
                top = self.find_set(bit)
                if top in savings:
                    savings[top] = savings[top] - 1
                else:
                    savings[top] = len(self.bit_groups[top]) - 1
                    tot_size += len(self.bit_groups[top])
            slist = []
            for (item, value) in savings.items():
                slist.append((value, item))
            slist.sort(reverse=True)
            savings_need = tot_size - self.max_block_size
            for item in slist:
                if savings_need > 0:
                    savings_need = savings_need - item[0]
                    if len(self.gate_groups[item[1]]) >= 1:
                        block_list.append(self.gate_groups[item[1]][:])
                    cur_set = set(self.bit_groups[item[1]])
                    for v in cur_set:
                        self.parent[v] = v
                        self.bit_groups[v] = [v]
                        self.gate_groups[v] = []
        if can_process:
            if len(cur_qubits) > self.max_block_size:
                continue
            prev = -1
            for bit in cur_qubits:
                if prev != -1:
                    self.union_set(prev, bit)
                prev = bit
            self.gate_groups[self.find_set(prev)].append(nd)
    for index in self.parent:
        if self.parent[index] == index and len(self.gate_groups[index]) != 0:
            block_list.append(self.gate_groups[index][:])
    self.property_set['block_list'] = block_list
    return dag","for nd in op_nodes:
    can_process = True
    makes_too_big = False
    if getattr(nd.op, 'condition', None) is not None or nd.op.is_parameterized() or (not isinstance(nd.op, Gate)):
        can_process = False
    cur_qubits = {qubit_indices[bit] for bit in nd.qargs}
    if can_process:
        c_tops = set()
        for bit in cur_qubits:
            c_tops.add(self.find_set(bit))
        tot_size = 0
        for group in c_tops:
            tot_size += len(self.bit_groups[group])
        if tot_size > self.max_block_size:
            makes_too_big = True
    if not can_process:
        for bit in cur_qubits:
            bit = self.find_set(bit)
            if len(self.gate_groups[bit]) == 0:
                continue
            block_list.append(self.gate_groups[bit][:])
            cur_set = set(self.bit_groups[bit])
            for v in cur_set:
                self.parent[v] = v
                self.bit_groups[v] = [v]
                self.gate_groups[v] = []
    if makes_too_big:
        savings = {}
        tot_size = 0
        for bit in cur_qubits:
            top = self.find_set(bit)
            if top in savings:
                savings[top] = savings[top] - 1
            else:
                savings[top] = len(self.bit_groups[top]) - 1
                tot_size += len(self.bit_groups[top])
        slist = []
        for (item, value) in savings.items():
            slist.append((value, item))
        slist.sort(reverse=True)
        savings_need = tot_size - self.max_block_size
        for item in slist:
            if savings_need > 0:
                savings_need = savings_need - item[0]
                if len(self.gate_groups[item[1]]) >= 1:
                    block_list.append(self.gate_groups[item[1]][:])
                cur_set = set(self.bit_groups[item[1]])
                for v in cur_set:
                    self.parent[v] = v
                    self.bit_groups[v] = [v]
                    self.gate_groups[v] = []
    if can_process:
        if len(cur_qubits) > self.max_block_size:
            continue
        prev = -1
        for bit in cur_qubits:
            if prev != -1:
                self.union_set(prev, bit)
            prev = bit
        self.gate_groups[self.find_set(prev)].append(nd)","block_list = [self.gate_groups[bit][:] for bit in cur_qubits if len(self.gate_groups[self.find_set(bit)]) > 0 and (not getattr(nd.op, 'condition', None)) and (not nd.op.is_parameterized()) and (isinstance(nd.op, Gate)) and len(cur_qubits) <= self.max_block_size and (not makes_too_big) and len(self.bit_groups[self.find_set(bit)]) <= self.max_block_size]",Cannot refactor,2,
supervisor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/supervisor/supervisor/supervisorctl.py,https://github.com/home-assistant/supervisor/tree/master/supervisor/supervisorctl.py,DefaultControllerPlugin,do_update$1171,"def do_update(self, arg):

    def log(name, message):
        self.ctl.output('%s: %s' % (name, message))
    supervisor = self.ctl.get_supervisor()
    try:
        result = supervisor.reloadConfig()
    except xmlrpclib.Fault as e:
        self.ctl.exitstatus = LSBInitExitStatuses.GENERIC
        if e.faultCode == xmlrpc.Faults.SHUTDOWN_STATE:
            self.ctl.output('ERROR: already shutting down')
            return
        else:
            raise
    (added, changed, removed) = result[0]
    valid_gnames = set(arg.split())
    if 'all' in valid_gnames:
        valid_gnames = set()
    if valid_gnames:
        groups = set()
        for info in supervisor.getAllProcessInfo():
            groups.add(info['group'])
        groups.update(added)
        for gname in valid_gnames:
            if gname not in groups:
                self.ctl.output('ERROR: no such group: %s' % gname)
                self.ctl.exitstatus = LSBInitExitStatuses.GENERIC
    for gname in removed:
        if valid_gnames and gname not in valid_gnames:
            continue
        results = supervisor.stopProcessGroup(gname)
        log(gname, 'stopped')
        fails = [res for res in results if res['status'] == xmlrpc.Faults.FAILED]
        if fails:
            self.ctl.output('%s: %s' % (gname, 'has problems; not removing'))
            self.ctl.exitstatus = LSBInitExitStatuses.GENERIC
            continue
        supervisor.removeProcessGroup(gname)
        log(gname, 'removed process group')
    for gname in changed:
        if valid_gnames and gname not in valid_gnames:
            continue
        supervisor.stopProcessGroup(gname)
        log(gname, 'stopped')
        supervisor.removeProcessGroup(gname)
        supervisor.addProcessGroup(gname)
        log(gname, 'updated process group')
    for gname in added:
        if valid_gnames and gname not in valid_gnames:
            continue
        supervisor.addProcessGroup(gname)
        log(gname, 'added process group')","for info in supervisor.getAllProcessInfo():
    groups.add(info['group'])",groups = {info['group'] for info in supervisor.getAllProcessInfo()},["groups = {info['group'] for info in supervisor.getAllProcessInfo()}"],1,
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/units/core.py,https://github.com/astropy/astropy/tree/master/astropy/units/core.py,UnitBase,compose$1317,"def compose(self, equivalencies=[], units=None, max_depth=2, include_prefix_units=None):
    """"""
        Return the simplest possible composite unit(s) that represent
        the given unit.  Since there may be multiple equally simple
        compositions of the unit, a list of units is always returned.

        Parameters
        ----------
        equivalencies : list of tuple
            A list of equivalence pairs to also list.  See
            :ref:`astropy:unit_equivalencies`.
            This list is in addition to possible global defaults set by, e.g.,
            `set_enabled_equivalencies`.
            Use `None` to turn off all equivalencies.

        units : set of `~astropy.units.Unit`, optional
            If not provided, any known units may be used to compose
            into.  Otherwise, ``units`` is a dict, module or sequence
            containing the units to compose into.

        max_depth : int, optional
            The maximum recursion depth to use when composing into
            composite units.

        include_prefix_units : bool, optional
            When `True`, include prefixed units in the result.
            Default is `True` if a sequence is passed in to ``units``,
            `False` otherwise.

        Returns
        -------
        units : list of `CompositeUnit`
            A list of candidate compositions.  These will all be
            equally simple, but it may not be possible to
            automatically determine which of the candidates are
            better.
        """"""
    if include_prefix_units is None:
        include_prefix_units = isinstance(units, (list, tuple))
    equivalencies = self._normalize_equivalencies(equivalencies)

    def has_bases_in_common(a, b):
        if len(a.bases) == 0 and len(b.bases) == 0:
            return True
        for ab in a.bases:
            for bb in b.bases:
                if ab == bb:
                    return True
        return False

    def has_bases_in_common_with_equiv(unit, other):
        if has_bases_in_common(unit, other):
            return True
        for (funit, tunit, a, b) in equivalencies:
            if tunit is not None:
                if unit._is_equivalent(funit):
                    if has_bases_in_common(tunit.decompose(), other):
                        return True
                elif unit._is_equivalent(tunit):
                    if has_bases_in_common(funit.decompose(), other):
                        return True
            elif unit._is_equivalent(funit):
                if has_bases_in_common(dimensionless_unscaled, other):
                    return True
        return False

    def filter_units(units):
        filtered_namespace = set()
        for tunit in units:
            if isinstance(tunit, UnitBase) and (include_prefix_units or not isinstance(tunit, PrefixUnit)) and has_bases_in_common_with_equiv(decomposed, tunit.decompose()):
                filtered_namespace.add(tunit)
        return filtered_namespace
    decomposed = self.decompose()
    if units is None:
        units = filter_units(self._get_units_with_same_physical_type(equivalencies))
        if len(units) == 0:
            units = get_current_unit_registry().non_prefix_units
    elif isinstance(units, dict):
        units = set(filter_units(units.values()))
    elif inspect.ismodule(units):
        units = filter_units(vars(units).values())
    else:
        units = filter_units(_flatten_units_collection(units))

    def sort_results(results):
        if not len(results):
            return []
        results = list(results)
        results.sort(key=lambda x: np.abs(x.scale))
        results.sort(key=lambda x: np.sum(np.abs(x.powers)))
        results.sort(key=lambda x: np.sum(x.powers) < 0.0)
        results.sort(key=lambda x: not is_effectively_unity(x.scale))
        last_result = results[0]
        filtered = [last_result]
        for result in results[1:]:
            if str(result) != str(last_result):
                filtered.append(result)
            last_result = result
        return filtered
    return sort_results(self._compose(equivalencies=equivalencies, namespace=units, max_depth=max_depth, depth=0, cached_results={}))","for tunit in units:
    if isinstance(tunit, UnitBase) and (include_prefix_units or not isinstance(tunit, PrefixUnit)) and has_bases_in_common_with_equiv(decomposed, tunit.decompose()):
        filtered_namespace.add(tunit)","filtered_namespace = {tunit for tunit in units if isinstance(tunit, UnitBase) and (include_prefix_units or not isinstance(tunit, PrefixUnit)) and has_bases_in_common_with_equiv(decomposed, tunit.decompose())}","['filtered_namespace = {tunit for tunit in units if isinstance(tunit, UnitBase) and (include_prefix_units or not isinstance(tunit, PrefixUnit)) and has_bases_in_common_with_equiv(decomposed, tunit.decompose())}']",1,
pyelftools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyelftools/test/test_dynamic.py,https://github.com/eliben/pyelftools/tree/master/test/test_dynamic.py,TestDynamic,extract_sunw$102,"def extract_sunw(filename):
    with open(filename, 'rb') as f:
        elf = ELFFile(f)
        dyn = elf.get_section_by_name('.dynamic')
        seen = set()
        for tag in dyn.iter_tags():
            if type(tag.entry.d_tag) is str and tag.entry.d_tag.startswith('DT_SUNW'):
                seen.add(tag.entry.d_tag)
    return seen","for tag in dyn.iter_tags():
    if type(tag.entry.d_tag) is str and tag.entry.d_tag.startswith('DT_SUNW'):
        seen.add(tag.entry.d_tag)",entries = {tag.entry.d_tag for tag in dyn.iter_tags() if type(tag.entry.d_tag) is str and tag.entry.d_tag.startswith('DT_SUNW')},["seen = {tag.entry.d_tag for tag in dyn.iter_tags() if type(tag.entry.d_tag) is str and tag.entry.d_tag.startswith('DT_SUNW')}"],0,
python-for-android,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-for-android/testapps/on_device_unit_tests/test_app/tools.py,https://github.com/kivy/python-for-android/tree/master/testapps/on_device_unit_tests/test_app/tools.py,,get_failed_unittests_from$63,"def get_failed_unittests_from(unittests_output, set_of_tests):
    """"""Parse unittests output trying to find the failed tests""""""
    failed_tests = set()
    for test in set_of_tests:
        if test in unittests_output:
            failed_tests.add(test)
    return failed_tests","for test in set_of_tests:
    if test in unittests_output:
        failed_tests.add(test)",failed_tests = {test for test in set_of_tests if test in unittests_output},['failed_tests = {test for test in set_of_tests if test in unittests_output}'],1,
petastorm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/petastorm/petastorm/tests/test_predicates.py,https://github.com/uber/petastorm/tree/master/petastorm/tests/test_predicates.py,,test_pseudorandom_split_on_string_field$110,"def test_pseudorandom_split_on_string_field(all_values):
    split_list = [0.3, 0.4, 0.1, 0.0, 0.2]
    values_num = len(all_values)
    for idx in range(len(split_list)):
        test_predicate = in_pseudorandom_split(split_list, idx, 'string_partition_field')
        included_values = set()
        for val in all_values:
            if test_predicate.do_include({'string_partition_field': val}):
                included_values.add(val)
        expected_num = values_num * split_list[idx]
        assert pytest.approx(len(included_values), expected_num * 0.1) == expected_num","for idx in range(len(split_list)):
    test_predicate = in_pseudorandom_split(split_list, idx, 'string_partition_field')
    included_values = set()
    for val in all_values:
        if test_predicate.do_include({'string_partition_field': val}):
            included_values.add(val)
    expected_num = values_num * split_list[idx]
    assert pytest.approx(len(included_values), expected_num * 0.1) == expected_num","assert pytest.approx(len(included_values), expected_num * 0.1) == expected_num",Cannot refactor,2,
Silver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Silver/core/resolver.py,https://github.com/s0md3v/Silver/tree/master/core/resolver.py,,handler$14,"def handler(hostnames):
    ips = set()
    threadpool = concurrent.futures.ThreadPoolExecutor(max_workers=10)
    futures = (threadpool.submit(resolve, hostname) for hostname in hostnames)
    for (i, result) in enumerate(concurrent.futures.as_completed(futures)):
        if result.result():
            ips.add(result.result())
    return list(ips)","for (i, result) in enumerate(concurrent.futures.as_completed(futures)):
    if result.result():
        ips.add(result.result())","ips = {result.result() for (i, result) in enumerate(concurrent.futures.as_completed(futures)) if result.result()}","['ips = {result.result() for (i, result) in enumerate(concurrent.futures.as_completed(futures)) if result.result()}']",1,
learn-to-cluster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/learn-to-cluster/proposals/stat_cluster.py,https://github.com/yl-1993/learn-to-cluster/tree/master/proposals/stat_cluster.py,,inst2cls$34,"def inst2cls(inst_sets, idx2lb):
    cls_sets = []
    for inst_set in inst_sets:
        cls_set = set()
        for idx in inst_set:
            cls_set.add(idx2lb[idx])
        cls_sets.append(cls_set)
    return cls_sets","for inst_set in inst_sets:
    cls_set = set()
    for idx in inst_set:
        cls_set.add(idx2lb[idx])
    cls_sets.append(cls_set)",cls_sets = [{idx2lb[idx] for idx in inst_set} for inst_set in inst_sets],Cannot refactor,2,
electrum,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electrum/electrum/coinchooser.py,https://github.com/spesmilo/electrum/tree/master/electrum/coinchooser.py,CoinChooserRandom,bucket_candidates_any$352,"def bucket_candidates_any(self, buckets: List[Bucket], sufficient_funds) -> List[List[Bucket]]:
    """"""Returns a list of bucket sets.""""""
    if not buckets:
        if sufficient_funds([], bucket_value_sum=0):
            return [[]]
        else:
            raise NotEnoughFunds()
    candidates = set()
    for (n, bucket) in enumerate(buckets):
        if sufficient_funds([bucket], bucket_value_sum=bucket.value):
            candidates.add((n,))
    attempts = min(100, (len(buckets) - 1) * 10 + 1)
    permutation = list(range(len(buckets)))
    for i in range(attempts):
        self.p.shuffle(permutation)
        bkts = []
        bucket_value_sum = 0
        for (count, index) in enumerate(permutation):
            bucket = buckets[index]
            bkts.append(bucket)
            bucket_value_sum += bucket.value
            if sufficient_funds(bkts, bucket_value_sum=bucket_value_sum):
                candidates.add(tuple(sorted(permutation[:count + 1])))
                break
        else:
            raise NotEnoughFunds()
    candidates = [[buckets[n] for n in c] for c in candidates]
    return [strip_unneeded(c, sufficient_funds) for c in candidates]","for (n, bucket) in enumerate(buckets):
    if sufficient_funds([bucket], bucket_value_sum=bucket.value):
        candidates.add((n,))","candidates = {(n,) for (n, bucket) in enumerate(buckets) if sufficient_funds([bucket], bucket_value_sum=bucket.value)}","['candidates = {(n,) for (n, bucket) in enumerate(buckets) if sufficient_funds([bucket], bucket_value_sum=bucket.value)}']",1,
electrum,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electrum/electrum/coinchooser.py,https://github.com/spesmilo/electrum/tree/master/electrum/coinchooser.py,CoinChooserRandom,bucket_candidates_any$352,"def bucket_candidates_any(self, buckets: List[Bucket], sufficient_funds) -> List[List[Bucket]]:
    """"""Returns a list of bucket sets.""""""
    if not buckets:
        if sufficient_funds([], bucket_value_sum=0):
            return [[]]
        else:
            raise NotEnoughFunds()
    candidates = set()
    for (n, bucket) in enumerate(buckets):
        if sufficient_funds([bucket], bucket_value_sum=bucket.value):
            candidates.add((n,))
    attempts = min(100, (len(buckets) - 1) * 10 + 1)
    permutation = list(range(len(buckets)))
    for i in range(attempts):
        self.p.shuffle(permutation)
        bkts = []
        bucket_value_sum = 0
        for (count, index) in enumerate(permutation):
            bucket = buckets[index]
            bkts.append(bucket)
            bucket_value_sum += bucket.value
            if sufficient_funds(bkts, bucket_value_sum=bucket_value_sum):
                candidates.add(tuple(sorted(permutation[:count + 1])))
                break
        else:
            raise NotEnoughFunds()
    candidates = [[buckets[n] for n in c] for c in candidates]
    return [strip_unneeded(c, sufficient_funds) for c in candidates]","for i in range(attempts):
    self.p.shuffle(permutation)
    bkts = []
    bucket_value_sum = 0
    for (count, index) in enumerate(permutation):
        bucket = buckets[index]
        bkts.append(bucket)
        bucket_value_sum += bucket.value
        if sufficient_funds(bkts, bucket_value_sum=bucket_value_sum):
            candidates.add(tuple(sorted(permutation[:count + 1])))
            break
    else:
        raise NotEnoughFunds()","candidates = {tuple(sorted(permutation[:count + 1])) for i in range(attempts) for (count, index) in enumerate(self.p.shuffle(permutation)) if sufficient_funds([buckets[index] for index in permutation[:count+1]], bucket_value_sum=sum(bucket.value for bucket in buckets[index] for index in permutation[:count+1]))}",Cannot refactor,2,
intake,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/intake/versioneer.py,https://github.com/intake/intake/tree/master//versioneer.py,,do_setup$1696,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print('Adding sample versioneer config to setup.cfg', file=sys.stderr)
            with open(os.path.join(root, 'setup.cfg'), 'a') as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1
    print(' creating %s' % cfg.versionfile_source)
    with open(cfg.versionfile_source, 'w') as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {'DOLLAR': '$', 'STYLE': cfg.style, 'TAG_PREFIX': cfg.tag_prefix, 'PARENTDIR_PREFIX': cfg.parentdir_prefix, 'VERSIONFILE_SOURCE': cfg.versionfile_source})
    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), '__init__.py')
    if os.path.exists(ipy):
        try:
            with open(ipy, 'r') as f:
                old = f.read()
        except EnvironmentError:
            old = ''
        if INIT_PY_SNIPPET not in old:
            print(' appending to %s' % ipy)
            with open(ipy, 'a') as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print(' %s unmodified' % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None
    manifest_in = os.path.join(root, 'MANIFEST.in')
    simple_includes = set()
    try:
        with open(manifest_in, 'r') as f:
            for line in f:
                if line.startswith('include '):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    if 'versioneer.py' not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, 'a') as f:
            f.write('include versioneer.py\n')
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" % cfg.versionfile_source)
        with open(manifest_in, 'a') as f:
            f.write('include %s\n' % cfg.versionfile_source)
    else:
        print(' versionfile_source already in MANIFEST.in')
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f for include in line.split()[1:] if line.startswith('include ')},["simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}"],0,
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/solvers/polysys.py,https://github.com/sympy/sympy/tree/master/sympy/solvers/polysys.py,,solve_triangulated$302,"def solve_triangulated(polys, *gens, **args):
    """"""
    Solve a polynomial system using Gianni-Kalkbrenner algorithm.

    The algorithm proceeds by computing one Groebner basis in the ground
    domain and then by iteratively computing polynomial factorizations in
    appropriately constructed algebraic extensions of the ground domain.

    Parameters
    ==========

    polys: a list/tuple/set
        Listing all the equations that are needed to be solved
    gens: generators
        generators of the equations in polys for which we want the
        solutions
    args: Keyword arguments
        Special options for solving the equations

    Returns
    =======

    List[Tuple]
        A List of tuples. Solutions for symbols that satisfy the
        equations listed in polys

    Examples
    ========

    >>> from sympy import solve_triangulated
    >>> from sympy.abc import x, y, z

    >>> F = [x**2 + y + z - 1, x + y**2 + z - 1, x + y + z**2 - 1]

    >>> solve_triangulated(F, x, y, z)
    [(0, 0, 1), (0, 1, 0), (1, 0, 0)]

    References
    ==========

    1. Patrizia Gianni, Teo Mora, Algebraic Solution of System of
    Polynomial Equations using Groebner Bases, AAECC-5 on Applied Algebra,
    Algebraic Algorithms and Error-Correcting Codes, LNCS 356 247--257, 1989

    """"""
    G = groebner(polys, gens, polys=True)
    G = list(reversed(G))
    domain = args.get('domain')
    if domain is not None:
        for (i, g) in enumerate(G):
            G[i] = g.set_domain(domain)
    (f, G) = (G[0].ltrim(-1), G[1:])
    dom = f.get_domain()
    zeros = f.ground_roots()
    solutions = set()
    for zero in zeros:
        solutions.add(((zero,), dom))
    var_seq = reversed(gens[:-1])
    vars_seq = postfixes(gens[1:])
    for (var, vars) in zip(var_seq, vars_seq):
        _solutions = set()
        for (values, dom) in solutions:
            (H, mapping) = ([], list(zip(vars, values)))
            for g in G:
                _vars = (var,) + vars
                if g.has_only_gens(*_vars) and g.degree(var) != 0:
                    h = g.ltrim(var).eval(dict(mapping))
                    if g.degree(var) == h.degree():
                        H.append(h)
            p = min(H, key=lambda h: h.degree())
            zeros = p.ground_roots()
            for zero in zeros:
                if not zero.is_Rational:
                    dom_zero = dom.algebraic_field(zero)
                else:
                    dom_zero = dom
                _solutions.add(((zero,) + values, dom_zero))
        solutions = _solutions
    solutions = list(solutions)
    for (i, (solution, _)) in enumerate(solutions):
        solutions[i] = solution
    return sorted(solutions, key=default_sort_key)","for zero in zeros:
    solutions.add(((zero,), dom))","solutions = {((zero,), dom) for zero in zeros}","['solutions = {((zero,), dom) for zero in zeros}']",1,
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/solvers/polysys.py,https://github.com/sympy/sympy/tree/master/sympy/solvers/polysys.py,,solve_triangulated$302,"def solve_triangulated(polys, *gens, **args):
    """"""
    Solve a polynomial system using Gianni-Kalkbrenner algorithm.

    The algorithm proceeds by computing one Groebner basis in the ground
    domain and then by iteratively computing polynomial factorizations in
    appropriately constructed algebraic extensions of the ground domain.

    Parameters
    ==========

    polys: a list/tuple/set
        Listing all the equations that are needed to be solved
    gens: generators
        generators of the equations in polys for which we want the
        solutions
    args: Keyword arguments
        Special options for solving the equations

    Returns
    =======

    List[Tuple]
        A List of tuples. Solutions for symbols that satisfy the
        equations listed in polys

    Examples
    ========

    >>> from sympy import solve_triangulated
    >>> from sympy.abc import x, y, z

    >>> F = [x**2 + y + z - 1, x + y**2 + z - 1, x + y + z**2 - 1]

    >>> solve_triangulated(F, x, y, z)
    [(0, 0, 1), (0, 1, 0), (1, 0, 0)]

    References
    ==========

    1. Patrizia Gianni, Teo Mora, Algebraic Solution of System of
    Polynomial Equations using Groebner Bases, AAECC-5 on Applied Algebra,
    Algebraic Algorithms and Error-Correcting Codes, LNCS 356 247--257, 1989

    """"""
    G = groebner(polys, gens, polys=True)
    G = list(reversed(G))
    domain = args.get('domain')
    if domain is not None:
        for (i, g) in enumerate(G):
            G[i] = g.set_domain(domain)
    (f, G) = (G[0].ltrim(-1), G[1:])
    dom = f.get_domain()
    zeros = f.ground_roots()
    solutions = set()
    for zero in zeros:
        solutions.add(((zero,), dom))
    var_seq = reversed(gens[:-1])
    vars_seq = postfixes(gens[1:])
    for (var, vars) in zip(var_seq, vars_seq):
        _solutions = set()
        for (values, dom) in solutions:
            (H, mapping) = ([], list(zip(vars, values)))
            for g in G:
                _vars = (var,) + vars
                if g.has_only_gens(*_vars) and g.degree(var) != 0:
                    h = g.ltrim(var).eval(dict(mapping))
                    if g.degree(var) == h.degree():
                        H.append(h)
            p = min(H, key=lambda h: h.degree())
            zeros = p.ground_roots()
            for zero in zeros:
                if not zero.is_Rational:
                    dom_zero = dom.algebraic_field(zero)
                else:
                    dom_zero = dom
                _solutions.add(((zero,) + values, dom_zero))
        solutions = _solutions
    solutions = list(solutions)
    for (i, (solution, _)) in enumerate(solutions):
        solutions[i] = solution
    return sorted(solutions, key=default_sort_key)","for (values, dom) in solutions:
    (H, mapping) = ([], list(zip(vars, values)))
    for g in G:
        _vars = (var,) + vars
        if g.has_only_gens(*_vars) and g.degree(var) != 0:
            h = g.ltrim(var).eval(dict(mapping))
            if g.degree(var) == h.degree():
                H.append(h)
    p = min(H, key=lambda h: h.degree())
    zeros = p.ground_roots()
    for zero in zeros:
        if not zero.is_Rational:
            dom_zero = dom.algebraic_field(zero)
        else:
            dom_zero = dom
        _solutions.add(((zero,) + values, dom_zero))","_solutions = {((zero,) + values, dom.algebraic_field(zero)) if not zero.is_Rational else ((zero,) + values, dom) for (values, dom) in solutions for (H, mapping) in [([], list(zip(vars, values)))], p in [min(H, key=lambda h: h.degree())], zeros in [p.ground_roots()] for zero in zeros}",Cannot refactor,2,
simpleai,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/simpleai/samples/machine_learning/opinion.py,https://github.com/simpleai-team/simpleai/tree/master/samples/machine_learning/opinion.py,OpinionProblem,__init__$74,"def __init__(self, corpus):
    super(OpinionProblem, self).__init__()
    words = set()
    for opinion in corpus:
        for word in opinion.text.split():
            if word not in STOPWORDS:
                words.add(word)
    for word in words:
        self.attributes.append(WordIsPresent(word))","for opinion in corpus:
    for word in opinion.text.split():
        if word not in STOPWORDS:
            words.add(word)",words = {word for opinion in corpus for word in opinion.text.split() if word not in STOPWORDS},['words = {word for opinion in corpus for word in opinion.text.split() if word not in STOPWORDS}'],1,
BertSum,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BertSum/src/models/trainer.py,https://github.com/nlpyang/BertSum/tree/master/src/models/trainer.py,Trainer,test$207,"def test(self, test_iter, step, cal_lead=False, cal_oracle=False):
    """""" Validate model.
            valid_iter: validate data iterator
        Returns:
            :obj:`nmt.Statistics`: validation loss statistics
        """"""

    def _get_ngrams(n, text):
        ngram_set = set()
        text_length = len(text)
        max_index_ngram_start = text_length - n
        for i in range(max_index_ngram_start + 1):
            ngram_set.add(tuple(text[i:i + n]))
        return ngram_set

    def _block_tri(c, p):
        tri_c = _get_ngrams(3, c.split())
        for s in p:
            tri_s = _get_ngrams(3, s.split())
            if len(tri_c.intersection(tri_s)) > 0:
                return True
        return False
    if not cal_lead and (not cal_oracle):
        self.model.eval()
    stats = Statistics()
    can_path = '%s_step%d.candidate' % (self.args.result_path, step)
    gold_path = '%s_step%d.gold' % (self.args.result_path, step)
    with open(can_path, 'w') as save_pred:
        with open(gold_path, 'w') as save_gold:
            with torch.no_grad():
                for batch in test_iter:
                    src = batch.src
                    labels = batch.labels
                    segs = batch.segs
                    clss = batch.clss
                    mask = batch.mask
                    mask_cls = batch.mask_cls
                    gold = []
                    pred = []
                    if cal_lead:
                        selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size
                    elif cal_oracle:
                        selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in range(batch.batch_size)]
                    else:
                        (sent_scores, mask) = self.model(src, segs, clss, mask, mask_cls)
                        loss = self.loss(sent_scores, labels.float())
                        loss = (loss * mask.float()).sum()
                        batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))
                        stats.update(batch_stats)
                        sent_scores = sent_scores + mask.float()
                        sent_scores = sent_scores.cpu().data.numpy()
                        selected_ids = np.argsort(-sent_scores, 1)
                    for (i, idx) in enumerate(selected_ids):
                        _pred = []
                        if len(batch.src_str[i]) == 0:
                            continue
                        for j in selected_ids[i][:len(batch.src_str[i])]:
                            if j >= len(batch.src_str[i]):
                                continue
                            candidate = batch.src_str[i][j].strip()
                            if self.args.block_trigram:
                                if not _block_tri(candidate, _pred):
                                    _pred.append(candidate)
                            else:
                                _pred.append(candidate)
                            if not cal_oracle and (not self.args.recall_eval) and (len(_pred) == 3):
                                break
                        _pred = '<q>'.join(_pred)
                        if self.args.recall_eval:
                            _pred = ' '.join(_pred.split()[:len(batch.tgt_str[i].split())])
                        pred.append(_pred)
                        gold.append(batch.tgt_str[i])
                    for i in range(len(gold)):
                        save_gold.write(gold[i].strip() + '\n')
                    for i in range(len(pred)):
                        save_pred.write(pred[i].strip() + '\n')
    if step != -1 and self.args.report_rouge:
        rouges = test_rouge(self.args.temp_dir, can_path, gold_path)
        logger.info('Rouges at step %d \n%s' % (step, rouge_results_to_str(rouges)))
    self._report_step(0, step, valid_stats=stats)
    return stats","for i in range(max_index_ngram_start + 1):
    ngram_set.add(tuple(text[i:i + n]))",ngram_set = {tuple(text[i:i + n]) for i in range(max_index_ngram_start + 1)},['ngram_set = {tuple(text[i:i + n]) for i in range(max_index_ngram_start + 1)}'],1,
legendary,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/legendary/legendary/cli.py,https://github.com/derrod/legendary/tree/master/legendary/cli.py,LegendaryCLI,info$1538,"def info(self, args):
    name_or_path = args.app_name_or_manifest
    app_name = manifest_uri = None
    if os.path.exists(name_or_path) or name_or_path.startswith('http'):
        manifest_uri = name_or_path
    else:
        app_name = self._resolve_aliases(name_or_path)
    if not args.offline and (not manifest_uri):
        try:
            if not self.core.login():
                logger.error('Log in failed!')
                exit(1)
        except ValueError:
            pass
    info_items = dict(game=list(), manifest=list(), install=list())
    InfoItem = namedtuple('InfoItem', ['name', 'json_name', 'value', 'json_value'])
    if self.core.is_installed(app_name):
        installed_platform = self.core.get_installed_game(app_name).platform
        if installed_platform != args.platform:
            logger.warning(f'Game is installed for platform ""{installed_platform}"", but requested metadata is for ""{args.platform}"", this may lead to unexpected results.')
    game = self.core.get_game(app_name, update_meta=not args.offline, platform=args.platform)
    if game and (not self.core.asset_available(game, platform=args.platform)):
        logger.warning(f'Asset information for ""{game.app_name}"" is missing, this may be due to the game not being available on the selected platform or currently logged-in account.')
        args.offline = True
    manifest_data = None
    entitlements = None
    if args.offline or manifest_uri:
        if app_name and self.core.is_installed(app_name):
            (manifest_data, _) = self.core.get_installed_manifest(app_name)
        elif manifest_uri and manifest_uri.startswith('http'):
            r = self.core.egs.unauth_session.get(manifest_uri)
            r.raise_for_status()
            manifest_data = r.content
        elif manifest_uri and os.path.exists(manifest_uri):
            with open(manifest_uri, 'rb') as f:
                manifest_data = f.read()
        else:
            logger.info('Game not installed and offline mode enabled, cannot load manifest.')
    elif game:
        entitlements = self.core.egs.get_user_entitlements()
        egl_meta = self.core.egs.get_game_info(game.namespace, game.catalog_item_id)
        game.metadata = egl_meta
        if args.platform in game.asset_infos:
            (manifest_data, _) = self.core.get_cdn_manifest(game, args.platform)
    if game:
        game_infos = info_items['game']
        game_infos.append(InfoItem('App name', 'app_name', game.app_name, game.app_name))
        game_infos.append(InfoItem('Title', 'title', game.app_title, game.app_title))
        game_infos.append(InfoItem('Latest version', 'version', game.app_version(args.platform), game.app_version(args.platform)))
        all_versions = {k: v.build_version for (k, v) in game.asset_infos.items()}
        game_infos.append(InfoItem('All versions', 'platform_versions', all_versions, all_versions))
        game_infos.append(InfoItem('Cloud saves supported', 'cloud_saves_supported', game.supports_cloud_saves or game.supports_mac_cloud_saves, game.supports_cloud_saves or game.supports_mac_cloud_saves))
        cs_dir = None
        if game.supports_cloud_saves:
            cs_dir = game.metadata['customAttributes']['CloudSaveFolder']['value']
        game_infos.append(InfoItem('Cloud save folder (Windows)', 'cloud_save_folder', cs_dir, cs_dir))
        cs_dir = None
        if game.supports_mac_cloud_saves:
            cs_dir = game.metadata['customAttributes']['CloudSaveFolder_MAC']['value']
        game_infos.append(InfoItem('Cloud save folder (Mac)', 'cloud_save_folder_mac', cs_dir, cs_dir))
        game_infos.append(InfoItem('Is DLC', 'is_dlc', game.is_dlc, game.is_dlc))
        external_activation = game.third_party_store or game.partner_link_type
        game_infos.append(InfoItem('Activates on external platform', 'external_activation', external_activation or 'No', external_activation))
        launch_options = []
        i = 1
        while f'extraLaunchOption_{i:03d}_Name' in game.metadata['customAttributes']:
            launch_options.append((game.metadata['customAttributes'][f'extraLaunchOption_{i:03d}_Name']['value'], game.metadata['customAttributes'][f'extraLaunchOption_{i:03d}_Args']['value']))
            i += 1
        if launch_options:
            human_list = []
            json_list = []
            for (opt_name, opt_cmd) in sorted(launch_options):
                human_list.append(f'Name: ""{opt_name}"", Parameters: {opt_cmd}')
                json_list.append(dict(name=opt_name, parameters=opt_cmd))
            game_infos.append(InfoItem('Extra launch options', 'launch_options', human_list, json_list))
        else:
            game_infos.append(InfoItem('Extra launch options', 'launch_options', None, []))
        if entitlements and (not game.is_dlc):
            owned_entitlements = {i['entitlementName'] for i in entitlements}
            owned_app_names = {g.app_name for g in self.core.get_assets(args.platform)}
            owned_dlc = []
            for dlc in game.metadata.get('dlcItemList', []):
                installable = dlc.get('releaseInfo', None)
                if dlc['entitlementName'] in owned_entitlements:
                    owned_dlc.append((installable, None, dlc['title'], dlc['id']))
                elif installable:
                    app_name = dlc['releaseInfo'][0]['appId']
                    if app_name in owned_app_names:
                        owned_dlc.append((installable, app_name, dlc['title'], dlc['id']))
            if owned_dlc:
                human_list = []
                json_list = []
                for (installable, app_name, title, dlc_id) in owned_dlc:
                    json_list.append(dict(app_name=app_name, title=title, installable=installable, id=dlc_id))
                    if installable:
                        human_list.append(f'App name: {app_name}, Title: ""{title}""')
                    else:
                        human_list.append(f'Title: ""{title}"" (no installation required)')
                game_infos.append(InfoItem('Owned DLC', 'owned_dlc', human_list, json_list))
            else:
                game_infos.append(InfoItem('Owned DLC', 'owned_dlc', None, []))
        else:
            game_infos.append(InfoItem('Owned DLC', 'owned_dlc', None, []))
        igame = self.core.get_installed_game(app_name)
        if igame:
            installation_info = info_items['install']
            installation_info.append(InfoItem('Platform', 'platform', igame.platform, igame.platform))
            installation_info.append(InfoItem('Version', 'version', igame.version, igame.version))
            disk_size_human = f'{igame.install_size / 1024 / 1024 / 1024:.02f} GiB'
            installation_info.append(InfoItem('Install size', 'disk_size', disk_size_human, igame.install_size))
            installation_info.append(InfoItem('Install path', 'install_path', igame.install_path, igame.install_path))
            installation_info.append(InfoItem('Save data path', 'save_path', igame.save_path, igame.save_path))
            installation_info.append(InfoItem('EGL sync GUID', 'synced_egl_guid', igame.egl_guid, igame.egl_guid))
            if igame.install_tags:
                tags = ', '.join(igame.install_tags)
            else:
                tags = '(None, all game data selected for install)'
            installation_info.append(InfoItem('Install tags', 'install_tags', tags, igame.install_tags))
            installation_info.append(InfoItem('Requires ownership verification token (DRM)', 'requires_ovt', igame.requires_ot, igame.requires_ot))
            installed_dlc_human = []
            installed_dlc_json = []
            for dlc in game.metadata.get('dlcItemList', []):
                if not dlc.get('releaseInfo', None):
                    continue
                app_name = dlc['releaseInfo'][0]['appId']
                if (igame := self.core.get_installed_game(app_name)):
                    installed_dlc_json.append(dict(app_name=igame.app_name, title=igame.title, install_size=igame.install_size))
                    installed_dlc_human.append('App name: {}, Title: ""{}"", Size: {:.02f} GiB'.format(igame.app_name, igame.title, igame.install_size / 1024 / 1024 / 1024))
            installation_info.append(InfoItem('Installed DLC', 'installed_dlc', installed_dlc_human or None, installed_dlc_json))
    if manifest_data:
        manifest_info = info_items['manifest']
        manifest = self.core.load_manifest(manifest_data)
        manifest_size = len(manifest_data)
        manifest_size_human = f'{manifest_size / 1024:.01f} KiB'
        manifest_info.append(InfoItem('Manifest size', 'size', manifest_size_human, manifest_size))
        manifest_type = 'JSON' if hasattr(manifest, 'json_data') else 'Binary'
        manifest_info.append(InfoItem('Manifest type', 'type', manifest_type, manifest_type.lower()))
        manifest_info.append(InfoItem('Manifest version', 'version', manifest.version, manifest.version))
        manifest_info.append(InfoItem('Manifest feature level', 'feature_level', manifest.meta.feature_level, manifest.meta.feature_level))
        manifest_info.append(InfoItem('Manifest app name', 'app_name', manifest.meta.app_name, manifest.meta.app_name))
        manifest_info.append(InfoItem('Launch EXE', 'launch_exe', manifest.meta.launch_exe or 'N/A', manifest.meta.launch_exe))
        manifest_info.append(InfoItem('Launch Command', 'launch_command', manifest.meta.launch_command or '(None)', manifest.meta.launch_command))
        manifest_info.append(InfoItem('Build version', 'build_version', manifest.meta.build_version, manifest.meta.build_version))
        manifest_info.append(InfoItem('Build ID', 'build_id', manifest.meta.build_id, manifest.meta.build_id))
        if manifest.meta.prereq_ids:
            human_list = [f""Prerequisite IDs: {', '.join(manifest.meta.prereq_ids)}"", f'Prerequisite name: {manifest.meta.prereq_name}', f'Prerequisite path: {manifest.meta.prereq_path}', f""Prerequisite args: {manifest.meta.prereq_args or '(None)'}""]
            manifest_info.append(InfoItem('Prerequisites', 'prerequisites', human_list, dict(ids=manifest.meta.prereq_ids, name=manifest.meta.prereq_name, path=manifest.meta.prereq_path, args=manifest.meta.prereq_args)))
        else:
            manifest_info.append(InfoItem('Prerequisites', 'prerequisites', None, None))
        install_tags = {''}
        for fm in manifest.file_manifest_list.elements:
            for tag in fm.install_tags:
                install_tags.add(tag)
        install_tags = sorted(install_tags)
        install_tags_human = ', '.join((i if i else '(empty)' for i in install_tags))
        manifest_info.append(InfoItem('Install tags', 'install_tags', install_tags_human, install_tags))
        manifest_info.append(InfoItem('Files', 'num_files', manifest.file_manifest_list.count, manifest.file_manifest_list.count))
        manifest_info.append(InfoItem('Chunks', 'num_chunks', manifest.chunk_data_list.count, manifest.chunk_data_list.count))
        total_size = sum((fm.file_size for fm in manifest.file_manifest_list.elements))
        file_size = '{:.02f} GiB'.format(total_size / 1024 / 1024 / 1024)
        manifest_info.append(InfoItem('Disk size (uncompressed)', 'disk_size', file_size, total_size))
        total_size = sum((c.file_size for c in manifest.chunk_data_list.elements))
        chunk_size = '{:.02f} GiB'.format(total_size / 1024 / 1024 / 1024)
        manifest_info.append(InfoItem('Download size (compressed)', 'download_size', chunk_size, total_size))
        tag_disk_size = []
        tag_disk_size_human = []
        tag_download_size = []
        tag_download_size_human = []
        if len(install_tags) > 1:
            longest_tag = max(max((len(t) for t in install_tags)), len('(empty)'))
            for tag in install_tags:
                human_tag = tag or '(empty)'
                tag_files = [fm for fm in manifest.file_manifest_list.elements if tag in fm.install_tags or (not tag and (not fm.install_tags))]
                tag_file_size = sum((fm.file_size for fm in tag_files))
                tag_disk_size.append(dict(tag=tag, size=tag_file_size, count=len(tag_files)))
                tag_file_size_human = '{:.02f} GiB'.format(tag_file_size / 1024 / 1024 / 1024)
                tag_disk_size_human.append(f'{human_tag.ljust(longest_tag)} - {tag_file_size_human} (Files: {len(tag_files)})')
                tag_chunk_guids = set()
                for fm in tag_files:
                    for cp in fm.chunk_parts:
                        tag_chunk_guids.add(cp.guid_num)
                tag_chunk_size = sum((c.file_size for c in manifest.chunk_data_list.elements if c.guid_num in tag_chunk_guids))
                tag_download_size.append(dict(tag=tag, size=tag_chunk_size, count=len(tag_chunk_guids)))
                tag_chunk_size_human = '{:.02f} GiB'.format(tag_chunk_size / 1024 / 1024 / 1024)
                tag_download_size_human.append(f'{human_tag.ljust(longest_tag)} - {tag_chunk_size_human} (Chunks: {len(tag_chunk_guids)})')
        manifest_info.append(InfoItem('Disk size by install tag', 'tag_disk_size', tag_disk_size_human or 'N/A', tag_disk_size))
        manifest_info.append(InfoItem('Download size by install tag', 'tag_download_size', tag_download_size_human or 'N/A', tag_download_size))
    if not args.json:

        def print_info_item(item: InfoItem):
            if item.value is None:
                print(f'- {item.name}: (None)')
            elif isinstance(item.value, list):
                print(f'- {item.name}:')
                for list_item in item.value:
                    print(' + ', list_item)
            elif isinstance(item.value, dict):
                print(f'- {item.name}:')
                for (k, v) in item.value.items():
                    print(' + ', k, ':', v)
            else:
                print(f'- {item.name}: {item.value}')
        if info_items['game']:
            print('\nGame Information:')
            for info_item in info_items['game']:
                print_info_item(info_item)
        if info_items['install']:
            print('\nInstallation information:')
            for info_item in info_items['install']:
                print_info_item(info_item)
        if info_items['manifest']:
            print('\nManifest information:')
            for info_item in info_items['manifest']:
                print_info_item(info_item)
        if not any(info_items.values()):
            print('No game information available.')
    else:
        json_out = dict(game=dict(), install=dict(), manifest=dict())
        for info_item in info_items['game']:
            json_out['game'][info_item.json_name] = info_item.json_value
        for info_item in info_items['install']:
            json_out['install'][info_item.json_name] = info_item.json_value
        for info_item in info_items['manifest']:
            json_out['manifest'][info_item.json_name] = info_item.json_value
        for (key, value) in json_out.items():
            if not value:
                json_out[key] = None
        return self._print_json(json_out, args.pretty_json)","for fm in manifest.file_manifest_list.elements:
    for tag in fm.install_tags:
        install_tags.add(tag)",install_tags = {tag for fm in manifest.file_manifest_list.elements for tag in fm.install_tags},Cannot refactor,2,
legendary,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/legendary/legendary/cli.py,https://github.com/derrod/legendary/tree/master/legendary/cli.py,LegendaryCLI,info$1538,"def info(self, args):
    name_or_path = args.app_name_or_manifest
    app_name = manifest_uri = None
    if os.path.exists(name_or_path) or name_or_path.startswith('http'):
        manifest_uri = name_or_path
    else:
        app_name = self._resolve_aliases(name_or_path)
    if not args.offline and (not manifest_uri):
        try:
            if not self.core.login():
                logger.error('Log in failed!')
                exit(1)
        except ValueError:
            pass
    info_items = dict(game=list(), manifest=list(), install=list())
    InfoItem = namedtuple('InfoItem', ['name', 'json_name', 'value', 'json_value'])
    if self.core.is_installed(app_name):
        installed_platform = self.core.get_installed_game(app_name).platform
        if installed_platform != args.platform:
            logger.warning(f'Game is installed for platform ""{installed_platform}"", but requested metadata is for ""{args.platform}"", this may lead to unexpected results.')
    game = self.core.get_game(app_name, update_meta=not args.offline, platform=args.platform)
    if game and (not self.core.asset_available(game, platform=args.platform)):
        logger.warning(f'Asset information for ""{game.app_name}"" is missing, this may be due to the game not being available on the selected platform or currently logged-in account.')
        args.offline = True
    manifest_data = None
    entitlements = None
    if args.offline or manifest_uri:
        if app_name and self.core.is_installed(app_name):
            (manifest_data, _) = self.core.get_installed_manifest(app_name)
        elif manifest_uri and manifest_uri.startswith('http'):
            r = self.core.egs.unauth_session.get(manifest_uri)
            r.raise_for_status()
            manifest_data = r.content
        elif manifest_uri and os.path.exists(manifest_uri):
            with open(manifest_uri, 'rb') as f:
                manifest_data = f.read()
        else:
            logger.info('Game not installed and offline mode enabled, cannot load manifest.')
    elif game:
        entitlements = self.core.egs.get_user_entitlements()
        egl_meta = self.core.egs.get_game_info(game.namespace, game.catalog_item_id)
        game.metadata = egl_meta
        if args.platform in game.asset_infos:
            (manifest_data, _) = self.core.get_cdn_manifest(game, args.platform)
    if game:
        game_infos = info_items['game']
        game_infos.append(InfoItem('App name', 'app_name', game.app_name, game.app_name))
        game_infos.append(InfoItem('Title', 'title', game.app_title, game.app_title))
        game_infos.append(InfoItem('Latest version', 'version', game.app_version(args.platform), game.app_version(args.platform)))
        all_versions = {k: v.build_version for (k, v) in game.asset_infos.items()}
        game_infos.append(InfoItem('All versions', 'platform_versions', all_versions, all_versions))
        game_infos.append(InfoItem('Cloud saves supported', 'cloud_saves_supported', game.supports_cloud_saves or game.supports_mac_cloud_saves, game.supports_cloud_saves or game.supports_mac_cloud_saves))
        cs_dir = None
        if game.supports_cloud_saves:
            cs_dir = game.metadata['customAttributes']['CloudSaveFolder']['value']
        game_infos.append(InfoItem('Cloud save folder (Windows)', 'cloud_save_folder', cs_dir, cs_dir))
        cs_dir = None
        if game.supports_mac_cloud_saves:
            cs_dir = game.metadata['customAttributes']['CloudSaveFolder_MAC']['value']
        game_infos.append(InfoItem('Cloud save folder (Mac)', 'cloud_save_folder_mac', cs_dir, cs_dir))
        game_infos.append(InfoItem('Is DLC', 'is_dlc', game.is_dlc, game.is_dlc))
        external_activation = game.third_party_store or game.partner_link_type
        game_infos.append(InfoItem('Activates on external platform', 'external_activation', external_activation or 'No', external_activation))
        launch_options = []
        i = 1
        while f'extraLaunchOption_{i:03d}_Name' in game.metadata['customAttributes']:
            launch_options.append((game.metadata['customAttributes'][f'extraLaunchOption_{i:03d}_Name']['value'], game.metadata['customAttributes'][f'extraLaunchOption_{i:03d}_Args']['value']))
            i += 1
        if launch_options:
            human_list = []
            json_list = []
            for (opt_name, opt_cmd) in sorted(launch_options):
                human_list.append(f'Name: ""{opt_name}"", Parameters: {opt_cmd}')
                json_list.append(dict(name=opt_name, parameters=opt_cmd))
            game_infos.append(InfoItem('Extra launch options', 'launch_options', human_list, json_list))
        else:
            game_infos.append(InfoItem('Extra launch options', 'launch_options', None, []))
        if entitlements and (not game.is_dlc):
            owned_entitlements = {i['entitlementName'] for i in entitlements}
            owned_app_names = {g.app_name for g in self.core.get_assets(args.platform)}
            owned_dlc = []
            for dlc in game.metadata.get('dlcItemList', []):
                installable = dlc.get('releaseInfo', None)
                if dlc['entitlementName'] in owned_entitlements:
                    owned_dlc.append((installable, None, dlc['title'], dlc['id']))
                elif installable:
                    app_name = dlc['releaseInfo'][0]['appId']
                    if app_name in owned_app_names:
                        owned_dlc.append((installable, app_name, dlc['title'], dlc['id']))
            if owned_dlc:
                human_list = []
                json_list = []
                for (installable, app_name, title, dlc_id) in owned_dlc:
                    json_list.append(dict(app_name=app_name, title=title, installable=installable, id=dlc_id))
                    if installable:
                        human_list.append(f'App name: {app_name}, Title: ""{title}""')
                    else:
                        human_list.append(f'Title: ""{title}"" (no installation required)')
                game_infos.append(InfoItem('Owned DLC', 'owned_dlc', human_list, json_list))
            else:
                game_infos.append(InfoItem('Owned DLC', 'owned_dlc', None, []))
        else:
            game_infos.append(InfoItem('Owned DLC', 'owned_dlc', None, []))
        igame = self.core.get_installed_game(app_name)
        if igame:
            installation_info = info_items['install']
            installation_info.append(InfoItem('Platform', 'platform', igame.platform, igame.platform))
            installation_info.append(InfoItem('Version', 'version', igame.version, igame.version))
            disk_size_human = f'{igame.install_size / 1024 / 1024 / 1024:.02f} GiB'
            installation_info.append(InfoItem('Install size', 'disk_size', disk_size_human, igame.install_size))
            installation_info.append(InfoItem('Install path', 'install_path', igame.install_path, igame.install_path))
            installation_info.append(InfoItem('Save data path', 'save_path', igame.save_path, igame.save_path))
            installation_info.append(InfoItem('EGL sync GUID', 'synced_egl_guid', igame.egl_guid, igame.egl_guid))
            if igame.install_tags:
                tags = ', '.join(igame.install_tags)
            else:
                tags = '(None, all game data selected for install)'
            installation_info.append(InfoItem('Install tags', 'install_tags', tags, igame.install_tags))
            installation_info.append(InfoItem('Requires ownership verification token (DRM)', 'requires_ovt', igame.requires_ot, igame.requires_ot))
            installed_dlc_human = []
            installed_dlc_json = []
            for dlc in game.metadata.get('dlcItemList', []):
                if not dlc.get('releaseInfo', None):
                    continue
                app_name = dlc['releaseInfo'][0]['appId']
                if (igame := self.core.get_installed_game(app_name)):
                    installed_dlc_json.append(dict(app_name=igame.app_name, title=igame.title, install_size=igame.install_size))
                    installed_dlc_human.append('App name: {}, Title: ""{}"", Size: {:.02f} GiB'.format(igame.app_name, igame.title, igame.install_size / 1024 / 1024 / 1024))
            installation_info.append(InfoItem('Installed DLC', 'installed_dlc', installed_dlc_human or None, installed_dlc_json))
    if manifest_data:
        manifest_info = info_items['manifest']
        manifest = self.core.load_manifest(manifest_data)
        manifest_size = len(manifest_data)
        manifest_size_human = f'{manifest_size / 1024:.01f} KiB'
        manifest_info.append(InfoItem('Manifest size', 'size', manifest_size_human, manifest_size))
        manifest_type = 'JSON' if hasattr(manifest, 'json_data') else 'Binary'
        manifest_info.append(InfoItem('Manifest type', 'type', manifest_type, manifest_type.lower()))
        manifest_info.append(InfoItem('Manifest version', 'version', manifest.version, manifest.version))
        manifest_info.append(InfoItem('Manifest feature level', 'feature_level', manifest.meta.feature_level, manifest.meta.feature_level))
        manifest_info.append(InfoItem('Manifest app name', 'app_name', manifest.meta.app_name, manifest.meta.app_name))
        manifest_info.append(InfoItem('Launch EXE', 'launch_exe', manifest.meta.launch_exe or 'N/A', manifest.meta.launch_exe))
        manifest_info.append(InfoItem('Launch Command', 'launch_command', manifest.meta.launch_command or '(None)', manifest.meta.launch_command))
        manifest_info.append(InfoItem('Build version', 'build_version', manifest.meta.build_version, manifest.meta.build_version))
        manifest_info.append(InfoItem('Build ID', 'build_id', manifest.meta.build_id, manifest.meta.build_id))
        if manifest.meta.prereq_ids:
            human_list = [f""Prerequisite IDs: {', '.join(manifest.meta.prereq_ids)}"", f'Prerequisite name: {manifest.meta.prereq_name}', f'Prerequisite path: {manifest.meta.prereq_path}', f""Prerequisite args: {manifest.meta.prereq_args or '(None)'}""]
            manifest_info.append(InfoItem('Prerequisites', 'prerequisites', human_list, dict(ids=manifest.meta.prereq_ids, name=manifest.meta.prereq_name, path=manifest.meta.prereq_path, args=manifest.meta.prereq_args)))
        else:
            manifest_info.append(InfoItem('Prerequisites', 'prerequisites', None, None))
        install_tags = {''}
        for fm in manifest.file_manifest_list.elements:
            for tag in fm.install_tags:
                install_tags.add(tag)
        install_tags = sorted(install_tags)
        install_tags_human = ', '.join((i if i else '(empty)' for i in install_tags))
        manifest_info.append(InfoItem('Install tags', 'install_tags', install_tags_human, install_tags))
        manifest_info.append(InfoItem('Files', 'num_files', manifest.file_manifest_list.count, manifest.file_manifest_list.count))
        manifest_info.append(InfoItem('Chunks', 'num_chunks', manifest.chunk_data_list.count, manifest.chunk_data_list.count))
        total_size = sum((fm.file_size for fm in manifest.file_manifest_list.elements))
        file_size = '{:.02f} GiB'.format(total_size / 1024 / 1024 / 1024)
        manifest_info.append(InfoItem('Disk size (uncompressed)', 'disk_size', file_size, total_size))
        total_size = sum((c.file_size for c in manifest.chunk_data_list.elements))
        chunk_size = '{:.02f} GiB'.format(total_size / 1024 / 1024 / 1024)
        manifest_info.append(InfoItem('Download size (compressed)', 'download_size', chunk_size, total_size))
        tag_disk_size = []
        tag_disk_size_human = []
        tag_download_size = []
        tag_download_size_human = []
        if len(install_tags) > 1:
            longest_tag = max(max((len(t) for t in install_tags)), len('(empty)'))
            for tag in install_tags:
                human_tag = tag or '(empty)'
                tag_files = [fm for fm in manifest.file_manifest_list.elements if tag in fm.install_tags or (not tag and (not fm.install_tags))]
                tag_file_size = sum((fm.file_size for fm in tag_files))
                tag_disk_size.append(dict(tag=tag, size=tag_file_size, count=len(tag_files)))
                tag_file_size_human = '{:.02f} GiB'.format(tag_file_size / 1024 / 1024 / 1024)
                tag_disk_size_human.append(f'{human_tag.ljust(longest_tag)} - {tag_file_size_human} (Files: {len(tag_files)})')
                tag_chunk_guids = set()
                for fm in tag_files:
                    for cp in fm.chunk_parts:
                        tag_chunk_guids.add(cp.guid_num)
                tag_chunk_size = sum((c.file_size for c in manifest.chunk_data_list.elements if c.guid_num in tag_chunk_guids))
                tag_download_size.append(dict(tag=tag, size=tag_chunk_size, count=len(tag_chunk_guids)))
                tag_chunk_size_human = '{:.02f} GiB'.format(tag_chunk_size / 1024 / 1024 / 1024)
                tag_download_size_human.append(f'{human_tag.ljust(longest_tag)} - {tag_chunk_size_human} (Chunks: {len(tag_chunk_guids)})')
        manifest_info.append(InfoItem('Disk size by install tag', 'tag_disk_size', tag_disk_size_human or 'N/A', tag_disk_size))
        manifest_info.append(InfoItem('Download size by install tag', 'tag_download_size', tag_download_size_human or 'N/A', tag_download_size))
    if not args.json:

        def print_info_item(item: InfoItem):
            if item.value is None:
                print(f'- {item.name}: (None)')
            elif isinstance(item.value, list):
                print(f'- {item.name}:')
                for list_item in item.value:
                    print(' + ', list_item)
            elif isinstance(item.value, dict):
                print(f'- {item.name}:')
                for (k, v) in item.value.items():
                    print(' + ', k, ':', v)
            else:
                print(f'- {item.name}: {item.value}')
        if info_items['game']:
            print('\nGame Information:')
            for info_item in info_items['game']:
                print_info_item(info_item)
        if info_items['install']:
            print('\nInstallation information:')
            for info_item in info_items['install']:
                print_info_item(info_item)
        if info_items['manifest']:
            print('\nManifest information:')
            for info_item in info_items['manifest']:
                print_info_item(info_item)
        if not any(info_items.values()):
            print('No game information available.')
    else:
        json_out = dict(game=dict(), install=dict(), manifest=dict())
        for info_item in info_items['game']:
            json_out['game'][info_item.json_name] = info_item.json_value
        for info_item in info_items['install']:
            json_out['install'][info_item.json_name] = info_item.json_value
        for info_item in info_items['manifest']:
            json_out['manifest'][info_item.json_name] = info_item.json_value
        for (key, value) in json_out.items():
            if not value:
                json_out[key] = None
        return self._print_json(json_out, args.pretty_json)","for tag in install_tags:
    human_tag = tag or '(empty)'
    tag_files = [fm for fm in manifest.file_manifest_list.elements if tag in fm.install_tags or (not tag and (not fm.install_tags))]
    tag_file_size = sum((fm.file_size for fm in tag_files))
    tag_disk_size.append(dict(tag=tag, size=tag_file_size, count=len(tag_files)))
    tag_file_size_human = '{:.02f} GiB'.format(tag_file_size / 1024 / 1024 / 1024)
    tag_disk_size_human.append(f'{human_tag.ljust(longest_tag)} - {tag_file_size_human} (Files: {len(tag_files)})')
    tag_chunk_guids = set()
    for fm in tag_files:
        for cp in fm.chunk_parts:
            tag_chunk_guids.add(cp.guid_num)
    tag_chunk_size = sum((c.file_size for c in manifest.chunk_data_list.elements if c.guid_num in tag_chunk_guids))
    tag_download_size.append(dict(tag=tag, size=tag_chunk_size, count=len(tag_chunk_guids)))
    tag_chunk_size_human = '{:.02f} GiB'.format(tag_chunk_size / 1024 / 1024 / 1024)
    tag_download_size_human.append(f'{human_tag.ljust(longest_tag)} - {tag_chunk_size_human} (Chunks: {len(tag_chunk_guids)})')","tag_download_size = [{tag: tag_chunk_size, 'count': len(tag_chunk_guids)} for tag in install_tags for tag_files in [[fm for fm in manifest.file_manifest_list.elements if tag in fm.install_tags or (not tag and (not fm.install_tags))]] for tag_chunk_guids in [set(cp.guid_num for fm in tag_files for cp in fm.chunk_parts)] for tag_chunk_size in [sum((c.file_size for c in manifest.chunk_data_list.elements if c.guid_num in tag_chunk_guids))]]",Cannot refactor,2,
petastorm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/petastorm/petastorm/tests/test_end_to_end.py,https://github.com/uber/petastorm/tree/master/petastorm/tests/test_end_to_end.py,,test_pyarrow_filters_make_reader$864,"def test_pyarrow_filters_make_reader(synthetic_dataset):
    with make_reader(synthetic_dataset.url, workers_count=5, num_epochs=1, filters=[('partition_key', '=', 'p_5')]) as reader:
        uv = set()
        for data in reader:
            uv.add(data.partition_key)
        assert uv == {'p_5'}","for data in reader:
    uv.add(data.partition_key)",uv = {data.partition_key for data in reader},['uv = {data.partition_key for data in reader}'],1,
hydrus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydrus/hydrus/client/networking/ClientNetworkingDomain.py,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/networking/ClientNetworkingDomain.py,NetworkDomainManager,STATICLinkURLClassesAndParsers$2351,"def STATICLinkURLClassesAndParsers(url_classes, parsers, existing_url_class_keys_to_parser_keys):
    url_classes = list(url_classes)
    NetworkDomainManager.STATICSortURLClassesDescendingComplexity(url_classes)
    parsers = list(parsers)
    parsers.sort(key=lambda p: p.GetName())
    new_url_class_keys_to_parser_keys = {}
    api_pairs = ConvertURLClassesIntoAPIPairs(url_classes)
    api_pair_unparsable_url_classes = set()
    for (a, b) in api_pairs:
        api_pair_unparsable_url_classes.add(a)
    for parser in parsers:
        example_urls = parser.GetExampleURLs()
        for example_url in example_urls:
            for url_class in url_classes:
                if url_class in api_pair_unparsable_url_classes:
                    continue
                if url_class.Matches(example_url):
                    url_class_key = url_class.GetClassKey()
                    parsable = url_class.IsParsable()
                    linkable = url_class_key not in existing_url_class_keys_to_parser_keys and url_class_key not in new_url_class_keys_to_parser_keys
                    if parsable and linkable:
                        new_url_class_keys_to_parser_keys[url_class_key] = parser.GetParserKey()
                    break
    '\n        #\n        \n        for url_class in url_classes:\n            \n            if not url_class.IsParsable() or url_class in api_pair_unparsable_url_classes:\n                \n                continue\n                \n            \n            url_class_key = url_class.GetClassKey()\n            \n            if url_class_key in existing_url_class_keys_to_parser_keys:\n                \n                continue\n                \n            \n            for parser in parsers:\n                \n                example_urls = parser.GetExampleURLs()\n                \n                if True in ( url_class.Matches( example_url ) for example_url in example_urls ):\n                    \n                    new_url_class_keys_to_parser_keys[ url_class_key ] = parser.GetParserKey()\n                    \n                    break\n                    \n                \n            \n        '
    return new_url_class_keys_to_parser_keys","for (a, b) in api_pairs:
    api_pair_unparsable_url_classes.add(a)","api_pair_unparsable_url_classes = {a for (a, b) in api_pairs}","['api_pair_unparsable_url_classes = {a for (a, b) in api_pairs}']",1,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/win_service.py,https://github.com/saltstack/salt/tree/master/salt/modules/win_service.py,,get_enabled$119,"def get_enabled():
    """"""
    Return a list of enabled services. Enabled is defined as a service that is
    marked to Auto Start.

    Returns:
        list: A list of enabled services

    CLI Example:

    .. code-block:: bash

        salt '*' service.get_enabled
    """"""
    raw_services = _get_services()
    services = set()
    for service in raw_services:
        if info(service['ServiceName'])['StartType'] in ['Auto']:
            services.add(service['ServiceName'])
    return sorted(services)","for service in raw_services:
    if info(service['ServiceName'])['StartType'] in ['Auto']:
        services.add(service['ServiceName'])",services = {service['ServiceName'] for service in raw_services if info(service['ServiceName'])['StartType'] in ['Auto']},["services = {service['ServiceName'] for service in raw_services if info(service['ServiceName'])['StartType'] in ['Auto']}"],1,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/sysmod.py,https://github.com/saltstack/salt/tree/master/salt/modules/sysmod.py,,list_runners$620,"def list_runners(*args):
    """"""
    List the runners loaded on the minion

    .. versionadded:: 2014.7.0

    CLI Example:

    .. code-block:: bash

        salt '*' sys.list_runners

    Runner names can be specified as globs.

    .. versionadded:: 2015.5.0

    .. code-block:: bash

        salt '*' sys.list_runners 'm*'

    """"""
    run_ = salt.runner.Runner(__opts__)
    runners = set()
    if not args:
        for func in run_.functions:
            runners.add(func.split('.')[0])
        return sorted(runners)
    for module in args:
        if '*' in module:
            for func in fnmatch.filter(run_.functions, module):
                runners.add(func.split('.')[0])
        else:
            for func in run_.functions:
                mod_test = func.split('.')[0]
                if mod_test == module:
                    runners.add(mod_test)
    return sorted(runners)","for module in args:
    if '*' in module:
        for func in fnmatch.filter(run_.functions, module):
            runners.add(func.split('.')[0])
    else:
        for func in run_.functions:
            mod_test = func.split('.')[0]
            if mod_test == module:
                runners.add(mod_test)","runners = {func.split('.')[0] for module in args for func in fnmatch.filter(run_.functions, module)}.union({func.split('.')[0] for module in args for func in run_.functions if func.split('.')[0] == module})",Cannot refactor,2,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/sysmod.py,https://github.com/saltstack/salt/tree/master/salt/modules/sysmod.py,,list_runners$620,"def list_runners(*args):
    """"""
    List the runners loaded on the minion

    .. versionadded:: 2014.7.0

    CLI Example:

    .. code-block:: bash

        salt '*' sys.list_runners

    Runner names can be specified as globs.

    .. versionadded:: 2015.5.0

    .. code-block:: bash

        salt '*' sys.list_runners 'm*'

    """"""
    run_ = salt.runner.Runner(__opts__)
    runners = set()
    if not args:
        for func in run_.functions:
            runners.add(func.split('.')[0])
        return sorted(runners)
    for module in args:
        if '*' in module:
            for func in fnmatch.filter(run_.functions, module):
                runners.add(func.split('.')[0])
        else:
            for func in run_.functions:
                mod_test = func.split('.')[0]
                if mod_test == module:
                    runners.add(mod_test)
    return sorted(runners)","for func in run_.functions:
    runners.add(func.split('.')[0])",runners = {func.split('.')[0] for func in run_.functions},["runners = {func.split('.')[0] for func in run_.functions}"],1,
django-extensions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-extensions/tests/management/commands/shell_plus_tests/test_utils.py,https://github.com/django-extensions/django-extensions/tree/master/tests/management/commands/shell_plus_tests/test_utils.py,AutomaticShellPlusImportsTestCase,get_all_names_for_class$18,"def get_all_names_for_class(self, model_to_find_occurrences):
    """"""
        Returns all names under current class is imported.
        :param model_to_find_occurrences: class to find names
        :return: set of names under class is imported.
        """"""
    result = set()
    for (name, model_class) in self.imported_objects.items():
        if model_class == model_to_find_occurrences:
            result.add(name)
    return result","for (name, model_class) in self.imported_objects.items():
    if model_class == model_to_find_occurrences:
        result.add(name)","result = {name for (name, model_class) in self.imported_objects.items() if model_class == model_to_find_occurrences}","['result = {name for (name, model_class) in self.imported_objects.items() if model_class == model_to_find_occurrences}']",1,
LangSrcCurise,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LangSrcCurise/core/Subdomain_Api.py,https://github.com/LangziFun/LangSrcCurise/tree/master/core/Subdomain_Api.py,,Sec_Api$191,"def Sec_Api(domain):
    result = set()
    try:
        url = 'https://api.securitytrails.com/v1/domain/{}/subdomains'.format(domain)
        querystring = {'apikey': seckey}
        response = requests.request('GET', url, params=querystring)
        rest = response.json()
        subdomains = rest['subdomains']
        for s in subdomains:
            result.add(s + '.' + domain)
    except:
        pass
    print('[+ SecurityTrails API] SecTra接口 : {} 捕获子域名总数 : {}'.format(domain, len(result)))
    return list(result)","for s in subdomains:
    result.add(s + '.' + domain)",result = {s + '.' + domain for s in subdomains},["result = {s + '.' + domain for s in subdomains}"],1,
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/tests/services/api_tests/model_test.py,https://github.com/forseti-security/forseti-security/tree/master/tests/services/api_tests/model_test.py,,expand_message$130,"def expand_message(messages, type):
    """"""Get the access_details in the form of
       set([member resource permission ])
    """"""
    details = set()
    if type == 'access_by_resource':
        for access in messages:
            for member in access.members:
                details.add(' '.join([member, access.resource, access.role]))
    elif type == 'access_by_member':
        for access in messages:
            for resource in access.resources:
                details.add(' '.join([access.member, resource, access.role]))
    elif type == 'access_by_both':
        for access in messages:
            details.add(' '.join([access.member, access.resource, access.permission]))
    elif type == 'role_permission':
        for permissionsbyrole in messages:
            for permission in permissionsbyrole.permissions:
                details.add(' '.join([permissionsbyrole.role, permission]))
    else:
        raise Exception('type unrecognized')
    return details","for access in messages:
    for member in access.members:
        details.add(' '.join([member, access.resource, access.role]))","details = {' '.join([member, access.resource, access.role]) for access in messages for member in access.members}","[""details = {' '.join([member, access.resource, access.role]) for access in messages for member in access.members}""]",1,
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/tests/services/api_tests/model_test.py,https://github.com/forseti-security/forseti-security/tree/master/tests/services/api_tests/model_test.py,,expand_message$130,"def expand_message(messages, type):
    """"""Get the access_details in the form of
       set([member resource permission ])
    """"""
    details = set()
    if type == 'access_by_resource':
        for access in messages:
            for member in access.members:
                details.add(' '.join([member, access.resource, access.role]))
    elif type == 'access_by_member':
        for access in messages:
            for resource in access.resources:
                details.add(' '.join([access.member, resource, access.role]))
    elif type == 'access_by_both':
        for access in messages:
            details.add(' '.join([access.member, access.resource, access.permission]))
    elif type == 'role_permission':
        for permissionsbyrole in messages:
            for permission in permissionsbyrole.permissions:
                details.add(' '.join([permissionsbyrole.role, permission]))
    else:
        raise Exception('type unrecognized')
    return details","for access in messages:
    for resource in access.resources:
        details.add(' '.join([access.member, resource, access.role]))","details = {' '.join([access.member, resource, access.role]) for access in messages for resource in access.resources}",Cannot refactor,2,
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/tests/services/api_tests/model_test.py,https://github.com/forseti-security/forseti-security/tree/master/tests/services/api_tests/model_test.py,,expand_message$130,"def expand_message(messages, type):
    """"""Get the access_details in the form of
       set([member resource permission ])
    """"""
    details = set()
    if type == 'access_by_resource':
        for access in messages:
            for member in access.members:
                details.add(' '.join([member, access.resource, access.role]))
    elif type == 'access_by_member':
        for access in messages:
            for resource in access.resources:
                details.add(' '.join([access.member, resource, access.role]))
    elif type == 'access_by_both':
        for access in messages:
            details.add(' '.join([access.member, access.resource, access.permission]))
    elif type == 'role_permission':
        for permissionsbyrole in messages:
            for permission in permissionsbyrole.permissions:
                details.add(' '.join([permissionsbyrole.role, permission]))
    else:
        raise Exception('type unrecognized')
    return details","for access in messages:
    details.add(' '.join([access.member, access.resource, access.permission]))","details = {' '.join([access.member, access.resource, access.permission]) for access in messages}",Cannot refactor,2,
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/tests/services/api_tests/model_test.py,https://github.com/forseti-security/forseti-security/tree/master/tests/services/api_tests/model_test.py,,expand_message$130,"def expand_message(messages, type):
    """"""Get the access_details in the form of
       set([member resource permission ])
    """"""
    details = set()
    if type == 'access_by_resource':
        for access in messages:
            for member in access.members:
                details.add(' '.join([member, access.resource, access.role]))
    elif type == 'access_by_member':
        for access in messages:
            for resource in access.resources:
                details.add(' '.join([access.member, resource, access.role]))
    elif type == 'access_by_both':
        for access in messages:
            details.add(' '.join([access.member, access.resource, access.permission]))
    elif type == 'role_permission':
        for permissionsbyrole in messages:
            for permission in permissionsbyrole.permissions:
                details.add(' '.join([permissionsbyrole.role, permission]))
    else:
        raise Exception('type unrecognized')
    return details","for permissionsbyrole in messages:
    for permission in permissionsbyrole.permissions:
        details.add(' '.join([permissionsbyrole.role, permission]))","details = {' '.join([permissionsbyrole.role, permission]) for permissionsbyrole in messages for permission in permissionsbyrole.permissions}",Cannot refactor,2,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/freebsdkmod.py,https://github.com/saltstack/salt/tree/master/salt/modules/freebsdkmod.py,,_rm_mods$47,"def _rm_mods(pre_mods, post_mods):
    """"""
    Return a list of the new modules, pass an kldstat dict before running
    modprobe and one after modprobe has run
    """"""
    pre = set()
    post = set()
    for mod in pre_mods:
        pre.add(mod['module'])
    for mod in post_mods:
        post.add(mod['module'])
    return pre - post","for mod in pre_mods:
    pre.add(mod['module'])",pre = {mod['module'] for mod in pre_mods},["pre = {mod['module'] for mod in pre_mods}"],1,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/freebsdkmod.py,https://github.com/saltstack/salt/tree/master/salt/modules/freebsdkmod.py,,_rm_mods$47,"def _rm_mods(pre_mods, post_mods):
    """"""
    Return a list of the new modules, pass an kldstat dict before running
    modprobe and one after modprobe has run
    """"""
    pre = set()
    post = set()
    for mod in pre_mods:
        pre.add(mod['module'])
    for mod in post_mods:
        post.add(mod['module'])
    return pre - post","for mod in post_mods:
    post.add(mod['module'])",post = {mod['module'] for mod in post_mods},["post = {mod['module'] for mod in post_mods}"],1,
neutron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/db/migration/alembic_migrations/versions/newton/contract/a8b517cff8ab_add_routerport_bindings_for_ha.py,https://github.com/openstack/neutron/tree/master/neutron/db/migration/alembic_migrations/versions/newton/contract/a8b517cff8ab_add_routerport_bindings_for_ha.py,,upgrade$35,"def upgrade():
    ha_bindings = sa.Table(HA_AGENT_BINDINGS, sa.MetaData(), sa.Column('port_id', sa.String(36)), sa.Column('router_id', sa.String(36)), sa.Column('l3_agent_id', sa.String(36)), sa.Column('state', sa.Enum(constants.HA_ROUTER_STATE_ACTIVE, constants.HA_ROUTER_STATE_STANDBY, name='l3_ha_states')))
    router_ports = sa.Table(ROUTER_PORTS, sa.MetaData(), sa.Column('router_id', sa.String(36)), sa.Column('port_id', sa.String(36)), sa.Column('port_type', sa.String(255)))
    session = sa.orm.Session(bind=op.get_bind())
    with session.begin(subtransactions=True):
        router_port_tuples = set()
        for ha_bind in session.query(ha_bindings):
            router_port_tuples.add((ha_bind.router_id, ha_bind.port_id))
        for router_port in session.query(router_ports).filter(router_ports.c.port_type == constants.DEVICE_OWNER_ROUTER_HA_INTF):
            router_port_tuples.discard((router_port.router_id, router_port.port_id))
        new_records = [dict(router_id=router_id, port_id=port_id, port_type=constants.DEVICE_OWNER_ROUTER_HA_INTF) for (router_id, port_id) in router_port_tuples]
    op.bulk_insert(router_ports, new_records)
    session.commit()","for ha_bind in session.query(ha_bindings):
    router_port_tuples.add((ha_bind.router_id, ha_bind.port_id))","router_port_tuples = {(ha_bind.router_id, ha_bind.port_id) for ha_bind in session.query(ha_bindings)}","['router_port_tuples = {(ha_bind.router_id, ha_bind.port_id) for ha_bind in session.query(ha_bindings)}']",1,
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/swift/obj/server.py,https://github.com/openstack/swift/tree/master/swift/obj/server.py,ObjectController,__init__$132,"def __init__(self, conf, logger=None):
    """"""
        Creates a new WSGI application for the Swift Object Server. An
        example configuration is given at
        <source-dir>/etc/object-server.conf-sample or
        /etc/swift/object-server.conf-sample.
        """"""
    super(ObjectController, self).__init__(conf)
    self.logger = logger or get_logger(conf, log_route='object-server')
    self.node_timeout = float(conf.get('node_timeout', 3))
    self.container_update_timeout = float(conf.get('container_update_timeout', 1))
    self.conn_timeout = float(conf.get('conn_timeout', 0.5))
    self.client_timeout = float(conf.get('client_timeout', 60))
    self.disk_chunk_size = int(conf.get('disk_chunk_size', 65536))
    self.network_chunk_size = int(conf.get('network_chunk_size', 65536))
    self.log_requests = config_true_value(conf.get('log_requests', 'true'))
    self.max_upload_time = int(conf.get('max_upload_time', 86400))
    self.slow = int(conf.get('slow', 0))
    self.keep_cache_private = config_true_value(conf.get('keep_cache_private', 'false'))
    default_allowed_headers = '\n            content-disposition,\n            content-encoding,\n            x-delete-at,\n            x-object-manifest,\n            x-static-large-object,\n            cache-control,\n            content-language,\n            expires,\n            x-robots-tag\n        '
    extra_allowed_headers = [header.strip().lower() for header in conf.get('allowed_headers', default_allowed_headers).split(',') if header.strip()]
    self.allowed_headers = set()
    for header in extra_allowed_headers:
        if header not in RESERVED_DATAFILE_META:
            self.allowed_headers.add(header)
    if conf.get('auto_create_account_prefix'):
        self.logger.warning('Option auto_create_account_prefix is deprecated. Configure auto_create_account_prefix under the swift-constraints section of swift.conf. This option will be ignored in a future release.')
        self.auto_create_account_prefix = conf['auto_create_account_prefix']
    else:
        self.auto_create_account_prefix = AUTO_CREATE_ACCOUNT_PREFIX
    self.expiring_objects_account = self.auto_create_account_prefix + (conf.get('expiring_objects_account_name') or 'expiring_objects')
    self.expiring_objects_container_divisor = int(conf.get('expiring_objects_container_divisor') or 86400)
    if six.PY2:
        socket._fileobject.default_bufsize = self.network_chunk_size
    self.setup(conf)","for header in extra_allowed_headers:
    if header not in RESERVED_DATAFILE_META:
        self.allowed_headers.add(header)",allowed_headers = {header for header in extra_allowed_headers if header not in RESERVED_DATAFILE_META},['self.allowed_headers = {header for header in extra_allowed_headers if header not in RESERVED_DATAFILE_META}'],0,
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/knowledge_plugins/key_definitions/live_definitions.py,https://github.com/angr/angr/tree/master/angr/knowledge_plugins/key_definitions/live_definitions.py,LiveDefinitions,_mo_cmp$146,"def _mo_cmp(mo_self: Union['SimMemoryObject', Set['SimMemoryObject']], mo_other: Union['SimMemoryObject', Set['SimMemoryObject']], addr: int, size: int):
    if type(mo_self) is set and type(mo_other) is set and (len(mo_self) == 1) and (len(mo_other) == 1):
        a = next(iter(mo_self))
        b = next(iter(mo_other))
        return a.object is b.object and a.endness == b.endness
    values_self = set()
    values_other = set()
    if type(mo_self) is set:
        for mo in mo_self:
            values_self.add(mo.object)
    else:
        values_self.add(mo_self)
    if type(mo_other) is set:
        for mo in mo_other:
            values_other.add(mo.object)
    else:
        values_other.add(mo_other)
    return values_self == values_other","for mo in mo_self:
    values_self.add(mo.object)",values_self = {mo.object for mo in mo_self},['values_self = {mo.object for mo in mo_self}'],1,
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/knowledge_plugins/key_definitions/live_definitions.py,https://github.com/angr/angr/tree/master/angr/knowledge_plugins/key_definitions/live_definitions.py,LiveDefinitions,_mo_cmp$146,"def _mo_cmp(mo_self: Union['SimMemoryObject', Set['SimMemoryObject']], mo_other: Union['SimMemoryObject', Set['SimMemoryObject']], addr: int, size: int):
    if type(mo_self) is set and type(mo_other) is set and (len(mo_self) == 1) and (len(mo_other) == 1):
        a = next(iter(mo_self))
        b = next(iter(mo_other))
        return a.object is b.object and a.endness == b.endness
    values_self = set()
    values_other = set()
    if type(mo_self) is set:
        for mo in mo_self:
            values_self.add(mo.object)
    else:
        values_self.add(mo_self)
    if type(mo_other) is set:
        for mo in mo_other:
            values_other.add(mo.object)
    else:
        values_other.add(mo_other)
    return values_self == values_other","for mo in mo_other:
    values_other.add(mo.object)",values_other = {mo.object for mo in mo_other},['values_other = {mo.object for mo in mo_other}'],1,
mlxtend,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mlxtend/mlxtend/preprocessing/transactionencoder.py,https://github.com/rasbt/mlxtend/tree/master/mlxtend/preprocessing/transactionencoder.py,TransactionEncoder,fit$33,"def fit(self, X):
    """"""Learn unique column names from transaction DataFrame

        Parameters
        ------------
        X : list of lists
          A python list of lists, where the outer list stores the
          n transactions and the inner list stores the items in each
          transaction.

          For example,
          [['Apple', 'Beer', 'Rice', 'Chicken'],
           ['Apple', 'Beer', 'Rice'],
           ['Apple', 'Beer'],
           ['Apple', 'Bananas'],
           ['Milk', 'Beer', 'Rice', 'Chicken'],
           ['Milk', 'Beer', 'Rice'],
           ['Milk', 'Beer'],
           ['Apple', 'Bananas']]

        """"""
    unique_items = set()
    for transaction in X:
        for item in transaction:
            unique_items.add(item)
    self.columns_ = sorted(unique_items)
    columns_mapping = {}
    for (col_idx, item) in enumerate(self.columns_):
        columns_mapping[item] = col_idx
    self.columns_mapping_ = columns_mapping
    return self","for transaction in X:
    for item in transaction:
        unique_items.add(item)",unique_items = {item for transaction in X for item in transaction},['unique_items = {item for transaction in X for item in transaction}'],1,
dnsviz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dnsviz/dnsviz/analysis/status.py,https://github.com/dnsviz/dnsviz/tree/master/dnsviz/analysis/status.py,DSStatus,__init__$390,"def __init__(self, ds, ds_meta, dnskey, supported_digest_algs):
    self.ds = ds
    self.ds_meta = ds_meta
    self.dnskey = dnskey
    self.warnings = []
    self.errors = []
    if self.dnskey is None:
        self.digest_valid = None
    else:
        self.digest_valid = crypto.validate_ds_digest(ds.digest_type, ds.digest, dnskey.message_for_ds())
    self.validation_status = DS_STATUS_VALID
    if self.digest_valid is None or self.ds.digest_type not in supported_digest_algs:
        if self.dnskey is None:
            if self.validation_status == DS_STATUS_VALID:
                self.validation_status = DS_STATUS_INDETERMINATE_NO_DNSKEY
        elif self.ds.digest_type in DS_DIGEST_ALGS_VALIDATION_PROHIBITED:
            if self.validation_status == DS_STATUS_VALID:
                self.validation_status = DS_STATUS_ALGORITHM_IGNORED
        else:
            if self.validation_status == DS_STATUS_VALID:
                self.validation_status = DS_STATUS_INDETERMINATE_UNKNOWN_ALGORITHM
            self.warnings.append(Errors.DigestAlgorithmNotSupported(algorithm=self.ds.digest_type))
    if self.ds.digest_type in DS_DIGEST_ALGS_VALIDATION_PROHIBITED:
        self.warnings.append(Errors.DigestAlgorithmValidationProhibited(algorithm=self.ds.digest_type))
    if self.ds.digest_type in DS_DIGEST_ALGS_PROHIBITED:
        self.warnings.append(Errors.DigestAlgorithmProhibited(algorithm=self.ds.digest_type))
    elif self.ds.digest_type in DS_DIGEST_ALGS_NOT_RECOMMENDED:
        self.warnings.append(Errors.DigestAlgorithmNotRecommended(algorithm=self.ds.digest_type))
    if self.dnskey is not None and self.dnskey.rdata.flags & fmt.DNSKEY_FLAGS['revoke']:
        if self.dnskey.key_tag != self.ds.key_tag:
            if self.validation_status == DS_STATUS_VALID:
                self.validation_status = DS_STATUS_INDETERMINATE_MATCH_PRE_REVOKE
        else:
            self.errors.append(Errors.DNSKEYRevokedDS())
            if self.validation_status == DS_STATUS_VALID:
                self.validation_status = DS_STATUS_INVALID
    if self.digest_valid == False and self.ds.digest_type in supported_digest_algs:
        if self.dnskey.key_tag == self.ds.key_tag:
            if self.validation_status == DS_STATUS_VALID:
                self.validation_status = DS_STATUS_INVALID_DIGEST
            self.errors.append(Errors.DigestInvalid())
    if self.ds.digest_type == 1:
        stronger_algs_all_ds = set()
        for ds_rdata in self.ds_meta.rrset:
            if ds_rdata.digest_type in DS_DIGEST_ALGS_STRONGER_THAN_SHA1:
                stronger_algs_all_ds.add(ds_rdata.digest_type)
        stronger_algs_all_ds.intersection_update(supported_digest_algs)
        if stronger_algs_all_ds:
            for digest_alg in stronger_algs_all_ds:
                if digest_alg in DS_DIGEST_ALGS_IGNORING_SHA1:
                    if self.validation_status == DS_STATUS_VALID:
                        self.validation_status = DS_STATUS_ALGORITHM_IGNORED
                    self.warnings.append(Errors.DSDigestAlgorithmIgnored(algorithm=1, new_algorithm=digest_alg))
                else:
                    self.warnings.append(Errors.DSDigestAlgorithmMaybeIgnored(algorithm=1, new_algorithm=digest_alg))","for ds_rdata in self.ds_meta.rrset:
    if ds_rdata.digest_type in DS_DIGEST_ALGS_STRONGER_THAN_SHA1:
        stronger_algs_all_ds.add(ds_rdata.digest_type)",stronger_algs_all_ds = {ds_rdata.digest_type for ds_rdata in self.ds_meta.rrset if ds_rdata.digest_type in DS_DIGEST_ALGS_STRONGER_THAN_SHA1},['stronger_algs_all_ds = {ds_rdata.digest_type for ds_rdata in self.ds_meta.rrset if ds_rdata.digest_type in DS_DIGEST_ALGS_STRONGER_THAN_SHA1}'],1,
great_expectations,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/great_expectations/versioneer.py,https://github.com/great-expectations/great_expectations/tree/master//versioneer.py,,do_setup$1748,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print('Adding sample versioneer config to setup.cfg', file=sys.stderr)
            with open(os.path.join(root, 'setup.cfg'), 'a') as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1
    print(' creating %s' % cfg.versionfile_source)
    with open(cfg.versionfile_source, 'w') as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {'DOLLAR': '$', 'STYLE': cfg.style, 'TAG_PREFIX': cfg.tag_prefix, 'PARENTDIR_PREFIX': cfg.parentdir_prefix, 'VERSIONFILE_SOURCE': cfg.versionfile_source})
    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), '__init__.py')
    if os.path.exists(ipy):
        try:
            with open(ipy) as f:
                old = f.read()
        except OSError:
            old = ''
        if INIT_PY_SNIPPET not in old:
            print(' appending to %s' % ipy)
            with open(ipy, 'a') as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print(' %s unmodified' % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None
    manifest_in = os.path.join(root, 'MANIFEST.in')
    simple_includes = set()
    try:
        with open(manifest_in) as f:
            for line in f:
                if line.startswith('include '):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except OSError:
        pass
    if 'versioneer.py' not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, 'a') as f:
            f.write('include versioneer.py\n')
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" % cfg.versionfile_source)
        with open(manifest_in, 'a') as f:
            f.write('include %s\n' % cfg.versionfile_source)
    else:
        print(' versionfile_source already in MANIFEST.in')
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f for include in line.split()[1:] if line.startswith('include ')},["simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}"],0,
client_python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/client_python/prometheus_client/registry.py,https://github.com/prometheus/client_python/tree/master/prometheus_client/registry.py,RestrictedRegistry,collect$136,"def collect(self):
    collectors = set()
    target_info_metric = None
    with self._registry._lock:
        if 'target_info' in self._name_set and self._registry._target_info:
            target_info_metric = self._registry._target_info_metric()
        for name in self._name_set:
            if name != 'target_info' and name in self._registry._names_to_collectors:
                collectors.add(self._registry._names_to_collectors[name])
    if target_info_metric:
        yield target_info_metric
    for collector in collectors:
        for metric in collector.collect():
            m = metric._restricted_metric(self._name_set)
            if m:
                yield m","for name in self._name_set:
    if name != 'target_info' and name in self._registry._names_to_collectors:
        collectors.add(self._registry._names_to_collectors[name])",collectors = {self._registry._names_to_collectors[name] for name in self._name_set if name != 'target_info' and name in self._registry._names_to_collectors},["collectors = {self._registry._names_to_collectors[name] for name in self._name_set if name != 'target_info' and name in self._registry._names_to_collectors}"],1,
viztracer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/viztracer/tests/test_cmdline.py,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_cmdline.py,TestCommandLineBasic,test_log_async$427,"def test_log_async(self):

    def check_func(data):
        tids = set()
        for entry in data['traceEvents']:
            tids.add(entry['tid'])
        self.assertGreaterEqual(len(tids), 4)
    self.template(['viztracer', '--log_async', '-o', 'result.json', 'cmdline_test.py'], script=file_log_async, expected_output_file='result.json', check_func=check_func)","for entry in data['traceEvents']:
    tids.add(entry['tid'])",tids = {entry['tid'] for entry in data['traceEvents']},["tids = {entry['tid'] for entry in data['traceEvents']}"],1,
Amulet-Map-Editor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Amulet-Map-Editor/amulet_map_editor/api/lang.py,https://github.com/Amulet-Team/Amulet-Map-Editor/tree/master/amulet_map_editor/api/lang.py,,get_languages$91,"def get_languages() -> List[str]:
    """"""Get a list of all supported language codes.""""""
    langs = set()
    for d in _lang_dirs:
        for l in glob.glob(os.path.join(d, '*.lang')):
            langs.add(os.path.basename(l)[:-5])
    return sorted(langs)","for d in _lang_dirs:
    for l in glob.glob(os.path.join(d, '*.lang')):
        langs.add(os.path.basename(l)[:-5])","langs = {os.path.basename(l)[:-5] for d in _lang_dirs for l in glob.glob(os.path.join(d, '*.lang'))}","[""langs = {os.path.basename(l)[:-5] for d in _lang_dirs for l in glob.glob(os.path.join(d, '*.lang'))}""]",1,
SublimeJEDI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SublimeJEDI/dependencies/jedi/third_party/django-stubs/mypy_django_plugin/django/context.py,https://github.com/srusskih/SublimeJEDI/tree/master/dependencies/jedi/third_party/django-stubs/mypy_django_plugin/django/context.py,DjangoContext,all_registered_model_classes$210,"def all_registered_model_classes(self) -> Set[Type[models.Model]]:
    model_classes = self.apps_registry.get_models()
    all_model_bases = set()
    for model_cls in model_classes:
        for base_cls in model_cls.mro():
            if issubclass(base_cls, models.Model):
                all_model_bases.add(base_cls)
    return all_model_bases","for model_cls in model_classes:
    for base_cls in model_cls.mro():
        if issubclass(base_cls, models.Model):
            all_model_bases.add(base_cls)","all_model_bases = {base_cls for model_cls in model_classes for base_cls in model_cls.mro() if issubclass(base_cls, models.Model)}","['all_model_bases = {base_cls for model_cls in model_classes for base_cls in model_cls.mro() if issubclass(base_cls, models.Model)}']",1,
viztracer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/viztracer/tests/test_multiprocess.py,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_multiprocess.py,TestMultiprocessing,test_os_fork$276,"def test_os_fork(self):

    def check_func(data):
        pids = set()
        for entry in data['traceEvents']:
            pids.add(entry['pid'])
        self.assertGreater(len(pids), 1)
    if sys.platform in ['linux', 'linux2']:
        self.template(['viztracer', '-o', 'result.json', 'cmdline_test.py'], expected_output_file='result.json', script=file_fork, check_func=check_func)","for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']},["pids = {entry['pid'] for entry in data['traceEvents']}"],1,
huey,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/huey/huey/api.py,https://github.com/coleifer/huey/tree/master/huey/api.py,Huey,flush_locks$616,"def flush_locks(self, *names):
    flushed = set()
    locks = self._locks
    if names:
        lock_template = '%s.lock.%%s' % self.name
        named_locks = (lock_template % name.strip() for name in names)
        locks = itertools.chain(locks, named_locks)
    for lock_key in locks:
        if self.delete(lock_key):
            flushed.add(lock_key.split('.lock.', 1)[-1])
    return flushed","for lock_key in locks:
    if self.delete(lock_key):
        flushed.add(lock_key.split('.lock.', 1)[-1])","flushed = {lock_key.split('.lock.', 1)[-1] for lock_key in locks if self.delete(lock_key)}","[""flushed = {lock_key.split('.lock.', 1)[-1] for lock_key in locks if self.delete(lock_key)}""]",1,
networkx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/networkx/networkx/algorithms/approximation/tests/test_treewidth.py,https://github.com/networkx/networkx/tree/master/networkx/algorithms/approximation/tests/test_treewidth.py,TestTreewidthMinFillIn,test_heuristic_abort$215,"def test_heuristic_abort(self):
    """"""Test if min_fill_in returns None for fully connected graph""""""
    graph = {}
    for u in self.complete:
        graph[u] = set()
        for v in self.complete[u]:
            if u != v:
                graph[u].add(v)
    next_node = min_fill_in_heuristic(graph)
    if next_node is None:
        pass
    else:
        assert False","for u in self.complete:
    graph[u] = set()
    for v in self.complete[u]:
        if u != v:
            graph[u].add(v)",graph = {u: {v for v in self.complete[u] if u != v} for u in self.complete},Cannot refactor,2,
GitPython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GitPython/test/test_refs.py,https://github.com/gitpython-developers/GitPython/tree/master/test/test_refs.py,TestRefs,test_refs$175,"def test_refs(self):
    types_found = set()
    for ref in self.rorepo.refs:
        types_found.add(type(ref))
    assert len(types_found) >= 3","for ref in self.rorepo.refs:
    types_found.add(type(ref))",types_found = {type(ref) for ref in self.rorepo.refs},['types_found = {type(ref) for ref in self.rorepo.refs}'],1,
tahoe-lafs,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tahoe-lafs/src/allmydata/immutable/happiness_upload.py,https://github.com/tahoe-lafs/tahoe-lafs/tree/master/src/allmydata/immutable/happiness_upload.py,,share_placement$332,"def share_placement(peers, readonly_peers, shares, peers_to_shares):
    """"""
    Generates the allocations the upload should based on the given
    information. We construct a dictionary of 'share_num' ->
    'server_id' and return it to the caller. Existing allocations
    appear as placements because attempting to place an existing
    allocation will renew the share.

    For more information on the algorithm this class implements, refer to
    docs/specifications/servers-of-happiness.rst
    """"""
    if not peers:
        return dict()
    readonly_shares = set()
    readonly_map = {}
    for peer in sorted(peers_to_shares.keys()):
        if peer in readonly_peers:
            readonly_map.setdefault(peer, peers_to_shares[peer])
            for share in peers_to_shares[peer]:
                readonly_shares.add(share)
    readonly_mappings = _calculate_mappings(readonly_peers, readonly_shares, readonly_map)
    (used_peers, used_shares) = _extract_ids(readonly_mappings)
    new_peers = set(peers) - used_peers
    new_shares = shares - used_shares
    servermap = peers_to_shares.copy()
    for peer in sorted(peers_to_shares.keys()):
        if peer in used_peers:
            servermap.pop(peer, None)
        else:
            servermap[peer] = set(servermap[peer]) - used_shares
            if servermap[peer] == set():
                servermap.pop(peer, None)
                try:
                    new_peers.remove(peer)
                except KeyError:
                    pass
    existing_mappings = _calculate_mappings(new_peers, new_shares, servermap)
    (existing_peers, existing_shares) = _extract_ids(existing_mappings)
    new_peers = new_peers - existing_peers - used_peers
    new_shares = new_shares - existing_shares - used_shares
    new_mappings = _calculate_mappings(new_peers, new_shares)
    mappings = dict(list(readonly_mappings.items()) + list(existing_mappings.items()) + list(new_mappings.items()))
    homeless_shares = set()
    for share in mappings:
        if mappings[share] is None:
            homeless_shares.add(share)
    if len(homeless_shares) != 0:
        _distribute_homeless_shares(mappings, homeless_shares, {k: v for (k, v) in list(peers_to_shares.items()) if k not in readonly_peers})

    def round_robin(peers):
        while True:
            for peer in peers:
                yield peer
    peer_iter = round_robin(peers - readonly_peers)
    return {k: v.pop() if v else next(peer_iter) for (k, v) in list(mappings.items())}","for peer in sorted(peers_to_shares.keys()):
    if peer in readonly_peers:
        readonly_map.setdefault(peer, peers_to_shares[peer])
        for share in peers_to_shares[peer]:
            readonly_shares.add(share)",readonly_shares = {share for peer in sorted(peers_to_shares.keys()) if peer in readonly_peers for share in peers_to_shares[peer]},Cannot refactor,2,
tahoe-lafs,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tahoe-lafs/src/allmydata/immutable/happiness_upload.py,https://github.com/tahoe-lafs/tahoe-lafs/tree/master/src/allmydata/immutable/happiness_upload.py,,share_placement$332,"def share_placement(peers, readonly_peers, shares, peers_to_shares):
    """"""
    Generates the allocations the upload should based on the given
    information. We construct a dictionary of 'share_num' ->
    'server_id' and return it to the caller. Existing allocations
    appear as placements because attempting to place an existing
    allocation will renew the share.

    For more information on the algorithm this class implements, refer to
    docs/specifications/servers-of-happiness.rst
    """"""
    if not peers:
        return dict()
    readonly_shares = set()
    readonly_map = {}
    for peer in sorted(peers_to_shares.keys()):
        if peer in readonly_peers:
            readonly_map.setdefault(peer, peers_to_shares[peer])
            for share in peers_to_shares[peer]:
                readonly_shares.add(share)
    readonly_mappings = _calculate_mappings(readonly_peers, readonly_shares, readonly_map)
    (used_peers, used_shares) = _extract_ids(readonly_mappings)
    new_peers = set(peers) - used_peers
    new_shares = shares - used_shares
    servermap = peers_to_shares.copy()
    for peer in sorted(peers_to_shares.keys()):
        if peer in used_peers:
            servermap.pop(peer, None)
        else:
            servermap[peer] = set(servermap[peer]) - used_shares
            if servermap[peer] == set():
                servermap.pop(peer, None)
                try:
                    new_peers.remove(peer)
                except KeyError:
                    pass
    existing_mappings = _calculate_mappings(new_peers, new_shares, servermap)
    (existing_peers, existing_shares) = _extract_ids(existing_mappings)
    new_peers = new_peers - existing_peers - used_peers
    new_shares = new_shares - existing_shares - used_shares
    new_mappings = _calculate_mappings(new_peers, new_shares)
    mappings = dict(list(readonly_mappings.items()) + list(existing_mappings.items()) + list(new_mappings.items()))
    homeless_shares = set()
    for share in mappings:
        if mappings[share] is None:
            homeless_shares.add(share)
    if len(homeless_shares) != 0:
        _distribute_homeless_shares(mappings, homeless_shares, {k: v for (k, v) in list(peers_to_shares.items()) if k not in readonly_peers})

    def round_robin(peers):
        while True:
            for peer in peers:
                yield peer
    peer_iter = round_robin(peers - readonly_peers)
    return {k: v.pop() if v else next(peer_iter) for (k, v) in list(mappings.items())}","for share in mappings:
    if mappings[share] is None:
        homeless_shares.add(share)",homeless_shares = {share for share in mappings if mappings[share] is None},['homeless_shares = {share for share in mappings if mappings[share] is None}'],1,
xonsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xonsh/xonsh/ply/ply/lex.py,https://github.com/xonsh/xonsh/tree/master/xonsh/ply/ply/lex.py,,lex$864,"def lex(module=None, object=None, debug=False, optimize=False, lextab='lextab', reflags=int(re.VERBOSE), nowarn=False, outputdir=None, debuglog=None, errorlog=None):
    if lextab is None:
        lextab = 'lextab'
    global lexer
    ldict = None
    stateinfo = {'INITIAL': 'inclusive'}
    lexobj = Lexer()
    lexobj.lexoptimize = optimize
    global token, input
    if errorlog is None:
        errorlog = PlyLogger(sys.stderr)
    if debug:
        if debuglog is None:
            debuglog = PlyLogger(sys.stderr)
    if object:
        module = object
    if module:
        _items = [(k, getattr(module, k)) for k in dir(module)]
        ldict = dict(_items)
        if '__file__' not in ldict:
            ldict['__file__'] = sys.modules[ldict['__module__']].__file__
    else:
        ldict = get_caller_module_dict(2)
    pkg = ldict.get('__package__')
    if pkg and isinstance(lextab, str):
        if '.' not in lextab:
            lextab = pkg + '.' + lextab
    linfo = LexerReflect(ldict, log=errorlog, reflags=reflags)
    linfo.get_all()
    if not optimize:
        if linfo.validate_all():
            raise SyntaxError(""Can't build lexer"")
    if optimize and lextab:
        try:
            lexobj.readtab(lextab, ldict)
            token = lexobj.token
            input = lexobj.input
            lexer = lexobj
            return lexobj
        except ImportError:
            pass
    if debug:
        debuglog.info('lex: tokens   = %r', linfo.tokens)
        debuglog.info('lex: literals = %r', linfo.literals)
        debuglog.info('lex: states   = %r', linfo.stateinfo)
    lexobj.lextokens = set()
    for n in linfo.tokens:
        lexobj.lextokens.add(n)
    if isinstance(linfo.literals, (list, tuple)):
        lexobj.lexliterals = type(linfo.literals[0])().join(linfo.literals)
    else:
        lexobj.lexliterals = linfo.literals
    lexobj.lextokens_all = lexobj.lextokens | set(lexobj.lexliterals)
    stateinfo = linfo.stateinfo
    regexs = {}
    for state in stateinfo:
        regex_list = []
        for (fname, f) in linfo.funcsym[state]:
            regex_list.append('(?P<%s>%s)' % (fname, _get_regex(f)))
            if debug:
                debuglog.info(""lex: Adding rule %s -> '%s' (state '%s')"", fname, _get_regex(f), state)
        for (name, r) in linfo.strsym[state]:
            regex_list.append('(?P<%s>%s)' % (name, r))
            if debug:
                debuglog.info(""lex: Adding rule %s -> '%s' (state '%s')"", name, r, state)
        regexs[state] = regex_list
    if debug:
        debuglog.info('lex: ==== MASTER REGEXS FOLLOW ====')
    for state in regexs:
        (lexre, re_text, re_names) = _form_master_re(regexs[state], reflags, ldict, linfo.toknames)
        lexobj.lexstatere[state] = lexre
        lexobj.lexstateretext[state] = re_text
        lexobj.lexstaterenames[state] = re_names
        if debug:
            for (i, text) in enumerate(re_text):
                debuglog.info(""lex: state '%s' : regex[%d] = '%s'"", state, i, text)
    for (state, stype) in stateinfo.items():
        if state != 'INITIAL' and stype == 'inclusive':
            lexobj.lexstatere[state].extend(lexobj.lexstatere['INITIAL'])
            lexobj.lexstateretext[state].extend(lexobj.lexstateretext['INITIAL'])
            lexobj.lexstaterenames[state].extend(lexobj.lexstaterenames['INITIAL'])
    lexobj.lexstateinfo = stateinfo
    lexobj.lexre = lexobj.lexstatere['INITIAL']
    lexobj.lexretext = lexobj.lexstateretext['INITIAL']
    lexobj.lexreflags = reflags
    lexobj.lexstateignore = linfo.ignore
    lexobj.lexignore = lexobj.lexstateignore.get('INITIAL', '')
    lexobj.lexstateerrorf = linfo.errorf
    lexobj.lexerrorf = linfo.errorf.get('INITIAL', None)
    if not lexobj.lexerrorf:
        errorlog.warning('No t_error rule is defined')
    lexobj.lexstateeoff = linfo.eoff
    lexobj.lexeoff = linfo.eoff.get('INITIAL', None)
    for (s, stype) in stateinfo.items():
        if stype == 'exclusive':
            if s not in linfo.errorf:
                errorlog.warning(""No error rule is defined for exclusive state '%s'"", s)
            if s not in linfo.ignore and lexobj.lexignore:
                errorlog.warning(""No ignore rule is defined for exclusive state '%s'"", s)
        elif stype == 'inclusive':
            if s not in linfo.errorf:
                linfo.errorf[s] = linfo.errorf.get('INITIAL', None)
            if s not in linfo.ignore:
                linfo.ignore[s] = linfo.ignore.get('INITIAL', '')
    token = lexobj.token
    input = lexobj.input
    lexer = lexobj
    if lextab and optimize:
        if outputdir is None:
            if isinstance(lextab, types.ModuleType):
                srcfile = lextab.__file__
            elif '.' not in lextab:
                srcfile = ldict['__file__']
            else:
                parts = lextab.split('.')
                pkgname = '.'.join(parts[:-1])
                exec('import %s' % pkgname)
                srcfile = getattr(sys.modules[pkgname], '__file__', '')
            outputdir = os.path.dirname(srcfile)
        try:
            lexobj.writetab(lextab, outputdir)
            if lextab in sys.modules:
                del sys.modules[lextab]
        except IOError as e:
            errorlog.warning(""Couldn't write lextab module %r. %s"" % (lextab, e))
    return lexobj","for n in linfo.tokens:
    lexobj.lextokens.add(n)",lexobj.lextokens.update({n for n in linfo.tokens}),['lexobj.lextokens = {n for n in linfo.tokens}'],0,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_webapp/docassemble/webapp/server.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/server.py,,copy_playground_modules$1917,"def copy_playground_modules():
    root_dir = os.path.join(FULL_PACKAGE_DIRECTORY, 'docassemble')
    for d in os.listdir(root_dir):
        if re.search('^playground[0-9]', d) and os.path.isdir(os.path.join(root_dir, d)):
            try:
                shutil.rmtree(os.path.join(root_dir, d))
            except:
                sys.stderr.write('copy_playground_modules: error deleting ' + os.path.join(root_dir, d) + '\n')
    devs = set()
    for user in db.session.execute(select(UserModel.id).join(UserRoles, UserModel.id == UserRoles.user_id).join(Role, UserRoles.role_id == Role.id).where(and_(UserModel.active == True, or_(Role.name == 'admin', Role.name == 'developer')))):
        devs.add(user.id)
    for user_id in devs:
        mod_dir = SavedFile(user_id, fix=True, section='playgroundmodules')
        local_dirs = [(os.path.join(FULL_PACKAGE_DIRECTORY, 'docassemble', 'playground' + str(user_id)), mod_dir.directory)]
        for dirname in mod_dir.list_of_dirs():
            local_dirs.append((os.path.join(FULL_PACKAGE_DIRECTORY, 'docassemble', 'playground' + str(user_id) + dirname), os.path.join(mod_dir.directory, dirname)))
        for (local_dir, mod_directory) in local_dirs:
            if os.path.isdir(local_dir):
                try:
                    shutil.rmtree(local_dir)
                except:
                    sys.stderr.write('copy_playground_modules: error deleting ' + local_dir + ' before replacing it\n')
            os.makedirs(local_dir, exist_ok=True)
            for f in [f for f in os.listdir(mod_directory) if re.search('^[A-Za-z].*\\.py$', f)]:
                shutil.copyfile(os.path.join(mod_directory, f), os.path.join(local_dir, f))
            with open(os.path.join(local_dir, '__init__.py'), 'w', encoding='utf-8') as the_file:
                the_file.write(init_py_file)","for user in db.session.execute(select(UserModel.id).join(UserRoles, UserModel.id == UserRoles.user_id).join(Role, UserRoles.role_id == Role.id).where(and_(UserModel.active == True, or_(Role.name == 'admin', Role.name == 'developer')))):
    devs.add(user.id)","devs = {user.id for user in db.session.execute(select(UserModel.id).join(UserRoles, UserModel.id == UserRoles.user_id).join(Role, UserRoles.role_id == Role.id).where(and_(UserModel.active == True, or_(Role.name == 'admin', Role.name == 'developer'))))}","[""devs = {user.id for user in db.session.execute(select(UserModel.id).join(UserRoles, UserModel.id == UserRoles.user_id).join(Role, UserRoles.role_id == Role.id).where(and_(UserModel.active == True, or_(Role.name == 'admin', Role.name == 'developer'))))}""]",1,
PPLM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PPLM/paper_code/pytorch_pretrained_bert/file_utils.py,https://github.com/uber-research/PPLM/tree/master/paper_code/pytorch_pretrained_bert/file_utils.py,,read_set_from_file$255,"def read_set_from_file(filename):
    """"""
    Extract a de-duped collection (set) of text from a file.
    Expected file format is one item per line.
    """"""
    collection = set()
    with open(filename, 'r', encoding='utf-8') as file_:
        for line in file_:
            collection.add(line.rstrip())
    return collection","for line in file_:
    collection.add(line.rstrip())",collection = {line.rstrip() for line in file_},['collection = {line.rstrip() for line in file_}'],1,
dulwich,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dulwich/dulwich/object_store.py,https://github.com/dulwich/dulwich/tree/master/dulwich/object_store.py,PackBasedObjectStore,pack_loose_objects$483,"def pack_loose_objects(self):
    """"""Pack loose objects.

        Returns: Number of objects packed
        """"""
    objects = set()
    for sha in self._iter_loose_objects():
        objects.add((self._get_loose_object(sha), None))
    self.add_objects(list(objects))
    for (obj, path) in objects:
        self._remove_loose_object(obj.id)
    return len(objects)","for sha in self._iter_loose_objects():
    objects.add((self._get_loose_object(sha), None))","objects = {(self._get_loose_object(sha), None) for sha in self._iter_loose_objects()}","['objects = {(self._get_loose_object(sha), None) for sha in self._iter_loose_objects()}']",1,
watchdog,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/watchdog/tests/test_regex_matching_event_handler.py,https://github.com/gorakhargosh/watchdog/tree/master/tests/test_regex_matching_event_handler.py,,test_dispatch$42,"def test_dispatch():
    regexes = ['.*\\.py', '.*\\.txt']
    ignore_regexes = ['.*\\.pyc']

    def assert_regexes(handler, event):
        if hasattr(event, 'dest_path'):
            paths = [event.src_path, event.dest_path]
        else:
            paths = [event.src_path]
        filtered_paths = set()
        for p in paths:
            if any((r.match(p) for r in handler.regexes)):
                filtered_paths.add(p)
        assert filtered_paths
    dir_del_event_match = DirDeletedEvent('/path/blah.py')
    dir_del_event_not_match = DirDeletedEvent('/path/foobar')
    dir_del_event_ignored = DirDeletedEvent('/path/foobar.pyc')
    file_del_event_match = FileDeletedEvent('/path/blah.txt')
    file_del_event_not_match = FileDeletedEvent('/path/foobar')
    file_del_event_ignored = FileDeletedEvent('/path/blah.pyc')
    dir_cre_event_match = DirCreatedEvent('/path/blah.py')
    dir_cre_event_not_match = DirCreatedEvent('/path/foobar')
    dir_cre_event_ignored = DirCreatedEvent('/path/foobar.pyc')
    file_cre_event_match = FileCreatedEvent('/path/blah.txt')
    file_cre_event_not_match = FileCreatedEvent('/path/foobar')
    file_cre_event_ignored = FileCreatedEvent('/path/blah.pyc')
    dir_mod_event_match = DirModifiedEvent('/path/blah.py')
    dir_mod_event_not_match = DirModifiedEvent('/path/foobar')
    dir_mod_event_ignored = DirModifiedEvent('/path/foobar.pyc')
    file_mod_event_match = FileModifiedEvent('/path/blah.txt')
    file_mod_event_not_match = FileModifiedEvent('/path/foobar')
    file_mod_event_ignored = FileModifiedEvent('/path/blah.pyc')
    dir_mov_event_match = DirMovedEvent('/path/blah.py', '/path/blah')
    dir_mov_event_not_match = DirMovedEvent('/path/foobar', '/path/blah')
    dir_mov_event_ignored = DirMovedEvent('/path/foobar.pyc', '/path/blah')
    file_mov_event_match = FileMovedEvent('/path/blah.txt', '/path/blah')
    file_mov_event_not_match = FileMovedEvent('/path/foobar', '/path/blah')
    file_mov_event_ignored = FileMovedEvent('/path/blah.pyc', '/path/blah')
    all_dir_events = [dir_mod_event_match, dir_mod_event_not_match, dir_mod_event_ignored, dir_del_event_match, dir_del_event_not_match, dir_del_event_ignored, dir_cre_event_match, dir_cre_event_not_match, dir_cre_event_ignored, dir_mov_event_match, dir_mov_event_not_match, dir_mov_event_ignored]
    all_file_events = [file_mod_event_match, file_mod_event_not_match, file_mod_event_ignored, file_del_event_match, file_del_event_not_match, file_del_event_ignored, file_cre_event_match, file_cre_event_not_match, file_cre_event_ignored, file_mov_event_match, file_mov_event_not_match, file_mov_event_ignored]
    all_events = all_file_events + all_dir_events

    def assert_check_directory(handler, event):
        assert not (handler.ignore_directories and event.is_directory)

    class TestableEventHandler(RegexMatchingEventHandler):

        def on_any_event(self, event):
            assert_check_directory(self, event)

        def on_modified(self, event):
            assert_check_directory(self, event)
            assert event.event_type == EVENT_TYPE_MODIFIED
            assert_regexes(self, event)

        def on_deleted(self, event):
            assert_check_directory(self, event)
            assert event.event_type == EVENT_TYPE_DELETED
            assert_regexes(self, event)

        def on_moved(self, event):
            assert_check_directory(self, event)
            assert event.event_type == EVENT_TYPE_MOVED
            assert_regexes(self, event)

        def on_created(self, event):
            assert_check_directory(self, event)
            assert event.event_type == EVENT_TYPE_CREATED
            assert_regexes(self, event)
    no_dirs_handler = TestableEventHandler(regexes=regexes, ignore_regexes=ignore_regexes, ignore_directories=True)
    handler = TestableEventHandler(regexes=regexes, ignore_regexes=ignore_regexes, ignore_directories=False)
    for event in all_events:
        no_dirs_handler.dispatch(event)
    for event in all_events:
        handler.dispatch(event)","for p in paths:
    if any((r.match(p) for r in handler.regexes)):
        filtered_paths.add(p)",filtered_paths = {p for p in paths if any((r.match(p) for r in handler.regexes))},['filtered_paths = {p for p in paths if any((r.match(p) for r in handler.regexes))}'],1,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,TestQuantizationFreezePass,freeze_graph$273,"def freeze_graph(self, use_cuda, seed, activation_quant_type, bias_correction=False, weight_quant_type='abs_max', for_ci=True, quant_skip_pattern='skip_quant'):

    def build_program(main, startup, is_test):
        main.random_seed = seed
        startup.random_seed = seed
        with fluid.unique_name.guard():
            with fluid.program_guard(main, startup):
                img = fluid.layers.data(name='image', shape=[1, 28, 28], dtype='float32')
                label = fluid.layers.data(name='label', shape=[1], dtype='int64')
                loss = conv_net(img, label, quant_skip_pattern)
                if not is_test:
                    opt = fluid.optimizer.Adam(learning_rate=0.001)
                    opt.minimize(loss)
        return ([img, label], loss)
    random.seed(0)
    np.random.seed(0)
    main = fluid.Program()
    startup = fluid.Program()
    test_program = fluid.Program()
    (feeds, loss) = build_program(main, startup, False)
    build_program(test_program, startup, True)
    test_program = test_program.clone(for_test=True)
    main_graph = IrGraph(core.Graph(main.desc), for_test=False)
    test_graph = IrGraph(core.Graph(test_program.desc), for_test=True)
    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
    exe = fluid.Executor(place)
    scope = fluid.Scope()
    with fluid.scope_guard(scope):
        exe.run(startup)
    transform_pass = QuantizationTransformPass(scope=scope, place=place, activation_quantize_type=activation_quant_type, weight_quantize_type=weight_quant_type, skip_pattern=quant_skip_pattern)
    transform_pass.apply(main_graph)
    transform_pass = QuantizationTransformPass(scope=scope, place=place, activation_quantize_type=activation_quant_type, weight_quantize_type=weight_quant_type, skip_pattern=quant_skip_pattern)
    transform_pass.apply(test_graph)
    dev_name = '_gpu_' if use_cuda else '_cpu_'
    if not for_ci:
        marked_nodes = set()
        for op in main_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        main_graph.draw('.', 'main' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    build_strategy = fluid.BuildStrategy()
    build_strategy.memory_optimize = False
    build_strategy.enable_inplace = False
    build_strategy.fuse_all_reduce_ops = False
    binary = fluid.CompiledProgram(main_graph.graph).with_data_parallel(loss_name=loss.name, build_strategy=build_strategy)
    quantized_test_program = test_graph.to_program()
    iters = 5
    batch_size = 8
    train_reader = paddle.batch(paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500), batch_size=batch_size)
    test_reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=batch_size)
    feeder = fluid.DataFeeder(feed_list=feeds, place=place)
    with fluid.scope_guard(scope):
        for _ in range(iters):
            data = next(train_reader())
            loss_v = exe.run(binary, feed=feeder.feed(data), fetch_list=[loss])
            if not for_ci:
                print('{}: {}'.format('loss' + dev_name + activation_quant_type + '_' + weight_quant_type, loss_v))
    test_data = next(test_reader())
    with fluid.program_guard(quantized_test_program):
        w_var = fluid.framework._get_var('conv2d_1.w_0.quantized', quantized_test_program)
    with fluid.scope_guard(scope):
        (test_loss1, w_quant) = exe.run(program=quantized_test_program, feed=feeder.feed(test_data), fetch_list=[loss, w_var])
    freeze_pass = QuantizationFreezePass(scope=scope, place=place, bias_correction=bias_correction, weight_quantize_type=weight_quant_type)
    freeze_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    server_program = test_graph.to_program()
    with fluid.scope_guard(scope):
        (test_loss2,) = exe.run(program=server_program, feed=feeder.feed(test_data), fetch_list=[loss])
    self.assertAlmostEqual(test_loss1, test_loss2, delta=0.005)
    if not for_ci:
        print('{}: {}'.format('test_loss1' + dev_name + activation_quant_type + '_' + weight_quant_type, test_loss1))
        print('{}: {}'.format('test_loss2' + dev_name + activation_quant_type + '_' + weight_quant_type, test_loss2))
    w_freeze = np.array(scope.find_var('conv2d_1.w_0').get_tensor())
    if not for_ci:
        print('{}: {}'.format('w_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_freeze)))
        print('{}: {}'.format('w_quant' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_quant)))
    convert_int8_pass = ConvertToInt8Pass(scope=scope, place=place)
    convert_int8_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    server_program_int8 = test_graph.to_program()
    with fluid.scope_guard(scope):
        fluid.io.save_inference_model('server_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, ['image', 'label'], [loss], exe, server_program_int8)
        [infer, feed, fetch] = fluid.io.load_inference_model('server_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, exe)
    w_8bit = np.array(scope.find_var('conv2d_1.w_0.int8').get_tensor())
    self.assertEqual(w_8bit.dtype, np.int8)
    self.assertEqual(np.sum(w_8bit), np.sum(w_freeze))
    if not for_ci:
        print('{}: {}'.format('w_8bit' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_8bit)))
        print('{}: {}'.format('w_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_freeze)))
    mobile_pass = TransformForMobilePass()
    mobile_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_mobile' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    mobile_program = test_graph.to_program()
    with fluid.scope_guard(scope):
        fluid.io.save_inference_model('mobile_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, ['image', 'label'], [loss], exe, mobile_program)","for op in main_graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        marked_nodes.add(op)",marked_nodes = {op for op in main_graph.all_op_nodes() if op.name().find('quantize') > -1},["marked_nodes = {op for op in main_graph.all_op_nodes() if op.name().find('quantize') > -1}"],1,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,TestQuantizationFreezePass,freeze_graph$273,"def freeze_graph(self, use_cuda, seed, activation_quant_type, bias_correction=False, weight_quant_type='abs_max', for_ci=True, quant_skip_pattern='skip_quant'):

    def build_program(main, startup, is_test):
        main.random_seed = seed
        startup.random_seed = seed
        with fluid.unique_name.guard():
            with fluid.program_guard(main, startup):
                img = fluid.layers.data(name='image', shape=[1, 28, 28], dtype='float32')
                label = fluid.layers.data(name='label', shape=[1], dtype='int64')
                loss = conv_net(img, label, quant_skip_pattern)
                if not is_test:
                    opt = fluid.optimizer.Adam(learning_rate=0.001)
                    opt.minimize(loss)
        return ([img, label], loss)
    random.seed(0)
    np.random.seed(0)
    main = fluid.Program()
    startup = fluid.Program()
    test_program = fluid.Program()
    (feeds, loss) = build_program(main, startup, False)
    build_program(test_program, startup, True)
    test_program = test_program.clone(for_test=True)
    main_graph = IrGraph(core.Graph(main.desc), for_test=False)
    test_graph = IrGraph(core.Graph(test_program.desc), for_test=True)
    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
    exe = fluid.Executor(place)
    scope = fluid.Scope()
    with fluid.scope_guard(scope):
        exe.run(startup)
    transform_pass = QuantizationTransformPass(scope=scope, place=place, activation_quantize_type=activation_quant_type, weight_quantize_type=weight_quant_type, skip_pattern=quant_skip_pattern)
    transform_pass.apply(main_graph)
    transform_pass = QuantizationTransformPass(scope=scope, place=place, activation_quantize_type=activation_quant_type, weight_quantize_type=weight_quant_type, skip_pattern=quant_skip_pattern)
    transform_pass.apply(test_graph)
    dev_name = '_gpu_' if use_cuda else '_cpu_'
    if not for_ci:
        marked_nodes = set()
        for op in main_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        main_graph.draw('.', 'main' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    build_strategy = fluid.BuildStrategy()
    build_strategy.memory_optimize = False
    build_strategy.enable_inplace = False
    build_strategy.fuse_all_reduce_ops = False
    binary = fluid.CompiledProgram(main_graph.graph).with_data_parallel(loss_name=loss.name, build_strategy=build_strategy)
    quantized_test_program = test_graph.to_program()
    iters = 5
    batch_size = 8
    train_reader = paddle.batch(paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500), batch_size=batch_size)
    test_reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=batch_size)
    feeder = fluid.DataFeeder(feed_list=feeds, place=place)
    with fluid.scope_guard(scope):
        for _ in range(iters):
            data = next(train_reader())
            loss_v = exe.run(binary, feed=feeder.feed(data), fetch_list=[loss])
            if not for_ci:
                print('{}: {}'.format('loss' + dev_name + activation_quant_type + '_' + weight_quant_type, loss_v))
    test_data = next(test_reader())
    with fluid.program_guard(quantized_test_program):
        w_var = fluid.framework._get_var('conv2d_1.w_0.quantized', quantized_test_program)
    with fluid.scope_guard(scope):
        (test_loss1, w_quant) = exe.run(program=quantized_test_program, feed=feeder.feed(test_data), fetch_list=[loss, w_var])
    freeze_pass = QuantizationFreezePass(scope=scope, place=place, bias_correction=bias_correction, weight_quantize_type=weight_quant_type)
    freeze_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    server_program = test_graph.to_program()
    with fluid.scope_guard(scope):
        (test_loss2,) = exe.run(program=server_program, feed=feeder.feed(test_data), fetch_list=[loss])
    self.assertAlmostEqual(test_loss1, test_loss2, delta=0.005)
    if not for_ci:
        print('{}: {}'.format('test_loss1' + dev_name + activation_quant_type + '_' + weight_quant_type, test_loss1))
        print('{}: {}'.format('test_loss2' + dev_name + activation_quant_type + '_' + weight_quant_type, test_loss2))
    w_freeze = np.array(scope.find_var('conv2d_1.w_0').get_tensor())
    if not for_ci:
        print('{}: {}'.format('w_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_freeze)))
        print('{}: {}'.format('w_quant' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_quant)))
    convert_int8_pass = ConvertToInt8Pass(scope=scope, place=place)
    convert_int8_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    server_program_int8 = test_graph.to_program()
    with fluid.scope_guard(scope):
        fluid.io.save_inference_model('server_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, ['image', 'label'], [loss], exe, server_program_int8)
        [infer, feed, fetch] = fluid.io.load_inference_model('server_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, exe)
    w_8bit = np.array(scope.find_var('conv2d_1.w_0.int8').get_tensor())
    self.assertEqual(w_8bit.dtype, np.int8)
    self.assertEqual(np.sum(w_8bit), np.sum(w_freeze))
    if not for_ci:
        print('{}: {}'.format('w_8bit' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_8bit)))
        print('{}: {}'.format('w_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_freeze)))
    mobile_pass = TransformForMobilePass()
    mobile_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_mobile' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    mobile_program = test_graph.to_program()
    with fluid.scope_guard(scope):
        fluid.io.save_inference_model('mobile_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, ['image', 'label'], [loss], exe, mobile_program)","for op in test_graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        marked_nodes.add(op)",marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1},["marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1}"],1,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,TestQuantizationFreezePass,freeze_graph$273,"def freeze_graph(self, use_cuda, seed, activation_quant_type, bias_correction=False, weight_quant_type='abs_max', for_ci=True, quant_skip_pattern='skip_quant'):

    def build_program(main, startup, is_test):
        main.random_seed = seed
        startup.random_seed = seed
        with fluid.unique_name.guard():
            with fluid.program_guard(main, startup):
                img = fluid.layers.data(name='image', shape=[1, 28, 28], dtype='float32')
                label = fluid.layers.data(name='label', shape=[1], dtype='int64')
                loss = conv_net(img, label, quant_skip_pattern)
                if not is_test:
                    opt = fluid.optimizer.Adam(learning_rate=0.001)
                    opt.minimize(loss)
        return ([img, label], loss)
    random.seed(0)
    np.random.seed(0)
    main = fluid.Program()
    startup = fluid.Program()
    test_program = fluid.Program()
    (feeds, loss) = build_program(main, startup, False)
    build_program(test_program, startup, True)
    test_program = test_program.clone(for_test=True)
    main_graph = IrGraph(core.Graph(main.desc), for_test=False)
    test_graph = IrGraph(core.Graph(test_program.desc), for_test=True)
    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
    exe = fluid.Executor(place)
    scope = fluid.Scope()
    with fluid.scope_guard(scope):
        exe.run(startup)
    transform_pass = QuantizationTransformPass(scope=scope, place=place, activation_quantize_type=activation_quant_type, weight_quantize_type=weight_quant_type, skip_pattern=quant_skip_pattern)
    transform_pass.apply(main_graph)
    transform_pass = QuantizationTransformPass(scope=scope, place=place, activation_quantize_type=activation_quant_type, weight_quantize_type=weight_quant_type, skip_pattern=quant_skip_pattern)
    transform_pass.apply(test_graph)
    dev_name = '_gpu_' if use_cuda else '_cpu_'
    if not for_ci:
        marked_nodes = set()
        for op in main_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        main_graph.draw('.', 'main' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    build_strategy = fluid.BuildStrategy()
    build_strategy.memory_optimize = False
    build_strategy.enable_inplace = False
    build_strategy.fuse_all_reduce_ops = False
    binary = fluid.CompiledProgram(main_graph.graph).with_data_parallel(loss_name=loss.name, build_strategy=build_strategy)
    quantized_test_program = test_graph.to_program()
    iters = 5
    batch_size = 8
    train_reader = paddle.batch(paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500), batch_size=batch_size)
    test_reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=batch_size)
    feeder = fluid.DataFeeder(feed_list=feeds, place=place)
    with fluid.scope_guard(scope):
        for _ in range(iters):
            data = next(train_reader())
            loss_v = exe.run(binary, feed=feeder.feed(data), fetch_list=[loss])
            if not for_ci:
                print('{}: {}'.format('loss' + dev_name + activation_quant_type + '_' + weight_quant_type, loss_v))
    test_data = next(test_reader())
    with fluid.program_guard(quantized_test_program):
        w_var = fluid.framework._get_var('conv2d_1.w_0.quantized', quantized_test_program)
    with fluid.scope_guard(scope):
        (test_loss1, w_quant) = exe.run(program=quantized_test_program, feed=feeder.feed(test_data), fetch_list=[loss, w_var])
    freeze_pass = QuantizationFreezePass(scope=scope, place=place, bias_correction=bias_correction, weight_quantize_type=weight_quant_type)
    freeze_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    server_program = test_graph.to_program()
    with fluid.scope_guard(scope):
        (test_loss2,) = exe.run(program=server_program, feed=feeder.feed(test_data), fetch_list=[loss])
    self.assertAlmostEqual(test_loss1, test_loss2, delta=0.005)
    if not for_ci:
        print('{}: {}'.format('test_loss1' + dev_name + activation_quant_type + '_' + weight_quant_type, test_loss1))
        print('{}: {}'.format('test_loss2' + dev_name + activation_quant_type + '_' + weight_quant_type, test_loss2))
    w_freeze = np.array(scope.find_var('conv2d_1.w_0').get_tensor())
    if not for_ci:
        print('{}: {}'.format('w_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_freeze)))
        print('{}: {}'.format('w_quant' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_quant)))
    convert_int8_pass = ConvertToInt8Pass(scope=scope, place=place)
    convert_int8_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    server_program_int8 = test_graph.to_program()
    with fluid.scope_guard(scope):
        fluid.io.save_inference_model('server_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, ['image', 'label'], [loss], exe, server_program_int8)
        [infer, feed, fetch] = fluid.io.load_inference_model('server_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, exe)
    w_8bit = np.array(scope.find_var('conv2d_1.w_0.int8').get_tensor())
    self.assertEqual(w_8bit.dtype, np.int8)
    self.assertEqual(np.sum(w_8bit), np.sum(w_freeze))
    if not for_ci:
        print('{}: {}'.format('w_8bit' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_8bit)))
        print('{}: {}'.format('w_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_freeze)))
    mobile_pass = TransformForMobilePass()
    mobile_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_mobile' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    mobile_program = test_graph.to_program()
    with fluid.scope_guard(scope):
        fluid.io.save_inference_model('mobile_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, ['image', 'label'], [loss], exe, mobile_program)","for op in test_graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        marked_nodes.add(op)",marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1},["marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1}"],1,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,TestQuantizationFreezePass,freeze_graph$273,"def freeze_graph(self, use_cuda, seed, activation_quant_type, bias_correction=False, weight_quant_type='abs_max', for_ci=True, quant_skip_pattern='skip_quant'):

    def build_program(main, startup, is_test):
        main.random_seed = seed
        startup.random_seed = seed
        with fluid.unique_name.guard():
            with fluid.program_guard(main, startup):
                img = fluid.layers.data(name='image', shape=[1, 28, 28], dtype='float32')
                label = fluid.layers.data(name='label', shape=[1], dtype='int64')
                loss = conv_net(img, label, quant_skip_pattern)
                if not is_test:
                    opt = fluid.optimizer.Adam(learning_rate=0.001)
                    opt.minimize(loss)
        return ([img, label], loss)
    random.seed(0)
    np.random.seed(0)
    main = fluid.Program()
    startup = fluid.Program()
    test_program = fluid.Program()
    (feeds, loss) = build_program(main, startup, False)
    build_program(test_program, startup, True)
    test_program = test_program.clone(for_test=True)
    main_graph = IrGraph(core.Graph(main.desc), for_test=False)
    test_graph = IrGraph(core.Graph(test_program.desc), for_test=True)
    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
    exe = fluid.Executor(place)
    scope = fluid.Scope()
    with fluid.scope_guard(scope):
        exe.run(startup)
    transform_pass = QuantizationTransformPass(scope=scope, place=place, activation_quantize_type=activation_quant_type, weight_quantize_type=weight_quant_type, skip_pattern=quant_skip_pattern)
    transform_pass.apply(main_graph)
    transform_pass = QuantizationTransformPass(scope=scope, place=place, activation_quantize_type=activation_quant_type, weight_quantize_type=weight_quant_type, skip_pattern=quant_skip_pattern)
    transform_pass.apply(test_graph)
    dev_name = '_gpu_' if use_cuda else '_cpu_'
    if not for_ci:
        marked_nodes = set()
        for op in main_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        main_graph.draw('.', 'main' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    build_strategy = fluid.BuildStrategy()
    build_strategy.memory_optimize = False
    build_strategy.enable_inplace = False
    build_strategy.fuse_all_reduce_ops = False
    binary = fluid.CompiledProgram(main_graph.graph).with_data_parallel(loss_name=loss.name, build_strategy=build_strategy)
    quantized_test_program = test_graph.to_program()
    iters = 5
    batch_size = 8
    train_reader = paddle.batch(paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500), batch_size=batch_size)
    test_reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=batch_size)
    feeder = fluid.DataFeeder(feed_list=feeds, place=place)
    with fluid.scope_guard(scope):
        for _ in range(iters):
            data = next(train_reader())
            loss_v = exe.run(binary, feed=feeder.feed(data), fetch_list=[loss])
            if not for_ci:
                print('{}: {}'.format('loss' + dev_name + activation_quant_type + '_' + weight_quant_type, loss_v))
    test_data = next(test_reader())
    with fluid.program_guard(quantized_test_program):
        w_var = fluid.framework._get_var('conv2d_1.w_0.quantized', quantized_test_program)
    with fluid.scope_guard(scope):
        (test_loss1, w_quant) = exe.run(program=quantized_test_program, feed=feeder.feed(test_data), fetch_list=[loss, w_var])
    freeze_pass = QuantizationFreezePass(scope=scope, place=place, bias_correction=bias_correction, weight_quantize_type=weight_quant_type)
    freeze_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    server_program = test_graph.to_program()
    with fluid.scope_guard(scope):
        (test_loss2,) = exe.run(program=server_program, feed=feeder.feed(test_data), fetch_list=[loss])
    self.assertAlmostEqual(test_loss1, test_loss2, delta=0.005)
    if not for_ci:
        print('{}: {}'.format('test_loss1' + dev_name + activation_quant_type + '_' + weight_quant_type, test_loss1))
        print('{}: {}'.format('test_loss2' + dev_name + activation_quant_type + '_' + weight_quant_type, test_loss2))
    w_freeze = np.array(scope.find_var('conv2d_1.w_0').get_tensor())
    if not for_ci:
        print('{}: {}'.format('w_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_freeze)))
        print('{}: {}'.format('w_quant' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_quant)))
    convert_int8_pass = ConvertToInt8Pass(scope=scope, place=place)
    convert_int8_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    server_program_int8 = test_graph.to_program()
    with fluid.scope_guard(scope):
        fluid.io.save_inference_model('server_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, ['image', 'label'], [loss], exe, server_program_int8)
        [infer, feed, fetch] = fluid.io.load_inference_model('server_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, exe)
    w_8bit = np.array(scope.find_var('conv2d_1.w_0.int8').get_tensor())
    self.assertEqual(w_8bit.dtype, np.int8)
    self.assertEqual(np.sum(w_8bit), np.sum(w_freeze))
    if not for_ci:
        print('{}: {}'.format('w_8bit' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_8bit)))
        print('{}: {}'.format('w_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_freeze)))
    mobile_pass = TransformForMobilePass()
    mobile_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_mobile' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    mobile_program = test_graph.to_program()
    with fluid.scope_guard(scope):
        fluid.io.save_inference_model('mobile_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, ['image', 'label'], [loss], exe, mobile_program)","for op in test_graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        marked_nodes.add(op)",marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1},["marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1}"],1,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,TestQuantizationFreezePass,freeze_graph$273,"def freeze_graph(self, use_cuda, seed, activation_quant_type, bias_correction=False, weight_quant_type='abs_max', for_ci=True, quant_skip_pattern='skip_quant'):

    def build_program(main, startup, is_test):
        main.random_seed = seed
        startup.random_seed = seed
        with fluid.unique_name.guard():
            with fluid.program_guard(main, startup):
                img = fluid.layers.data(name='image', shape=[1, 28, 28], dtype='float32')
                label = fluid.layers.data(name='label', shape=[1], dtype='int64')
                loss = conv_net(img, label, quant_skip_pattern)
                if not is_test:
                    opt = fluid.optimizer.Adam(learning_rate=0.001)
                    opt.minimize(loss)
        return ([img, label], loss)
    random.seed(0)
    np.random.seed(0)
    main = fluid.Program()
    startup = fluid.Program()
    test_program = fluid.Program()
    (feeds, loss) = build_program(main, startup, False)
    build_program(test_program, startup, True)
    test_program = test_program.clone(for_test=True)
    main_graph = IrGraph(core.Graph(main.desc), for_test=False)
    test_graph = IrGraph(core.Graph(test_program.desc), for_test=True)
    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
    exe = fluid.Executor(place)
    scope = fluid.Scope()
    with fluid.scope_guard(scope):
        exe.run(startup)
    transform_pass = QuantizationTransformPass(scope=scope, place=place, activation_quantize_type=activation_quant_type, weight_quantize_type=weight_quant_type, skip_pattern=quant_skip_pattern)
    transform_pass.apply(main_graph)
    transform_pass = QuantizationTransformPass(scope=scope, place=place, activation_quantize_type=activation_quant_type, weight_quantize_type=weight_quant_type, skip_pattern=quant_skip_pattern)
    transform_pass.apply(test_graph)
    dev_name = '_gpu_' if use_cuda else '_cpu_'
    if not for_ci:
        marked_nodes = set()
        for op in main_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        main_graph.draw('.', 'main' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    build_strategy = fluid.BuildStrategy()
    build_strategy.memory_optimize = False
    build_strategy.enable_inplace = False
    build_strategy.fuse_all_reduce_ops = False
    binary = fluid.CompiledProgram(main_graph.graph).with_data_parallel(loss_name=loss.name, build_strategy=build_strategy)
    quantized_test_program = test_graph.to_program()
    iters = 5
    batch_size = 8
    train_reader = paddle.batch(paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500), batch_size=batch_size)
    test_reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=batch_size)
    feeder = fluid.DataFeeder(feed_list=feeds, place=place)
    with fluid.scope_guard(scope):
        for _ in range(iters):
            data = next(train_reader())
            loss_v = exe.run(binary, feed=feeder.feed(data), fetch_list=[loss])
            if not for_ci:
                print('{}: {}'.format('loss' + dev_name + activation_quant_type + '_' + weight_quant_type, loss_v))
    test_data = next(test_reader())
    with fluid.program_guard(quantized_test_program):
        w_var = fluid.framework._get_var('conv2d_1.w_0.quantized', quantized_test_program)
    with fluid.scope_guard(scope):
        (test_loss1, w_quant) = exe.run(program=quantized_test_program, feed=feeder.feed(test_data), fetch_list=[loss, w_var])
    freeze_pass = QuantizationFreezePass(scope=scope, place=place, bias_correction=bias_correction, weight_quantize_type=weight_quant_type)
    freeze_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    server_program = test_graph.to_program()
    with fluid.scope_guard(scope):
        (test_loss2,) = exe.run(program=server_program, feed=feeder.feed(test_data), fetch_list=[loss])
    self.assertAlmostEqual(test_loss1, test_loss2, delta=0.005)
    if not for_ci:
        print('{}: {}'.format('test_loss1' + dev_name + activation_quant_type + '_' + weight_quant_type, test_loss1))
        print('{}: {}'.format('test_loss2' + dev_name + activation_quant_type + '_' + weight_quant_type, test_loss2))
    w_freeze = np.array(scope.find_var('conv2d_1.w_0').get_tensor())
    if not for_ci:
        print('{}: {}'.format('w_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_freeze)))
        print('{}: {}'.format('w_quant' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_quant)))
    convert_int8_pass = ConvertToInt8Pass(scope=scope, place=place)
    convert_int8_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    server_program_int8 = test_graph.to_program()
    with fluid.scope_guard(scope):
        fluid.io.save_inference_model('server_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, ['image', 'label'], [loss], exe, server_program_int8)
        [infer, feed, fetch] = fluid.io.load_inference_model('server_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, exe)
    w_8bit = np.array(scope.find_var('conv2d_1.w_0.int8').get_tensor())
    self.assertEqual(w_8bit.dtype, np.int8)
    self.assertEqual(np.sum(w_8bit), np.sum(w_freeze))
    if not for_ci:
        print('{}: {}'.format('w_8bit' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_8bit)))
        print('{}: {}'.format('w_freeze' + dev_name + activation_quant_type + '_' + weight_quant_type, np.sum(w_freeze)))
    mobile_pass = TransformForMobilePass()
    mobile_pass.apply(test_graph)
    if not for_ci:
        marked_nodes = set()
        for op in test_graph.all_op_nodes():
            if op.name().find('quantize') > -1:
                marked_nodes.add(op)
        test_graph.draw('.', 'test_mobile' + dev_name + activation_quant_type + '_' + weight_quant_type, marked_nodes)
    mobile_program = test_graph.to_program()
    with fluid.scope_guard(scope):
        fluid.io.save_inference_model('mobile_int8' + dev_name + activation_quant_type + '_' + weight_quant_type, ['image', 'label'], [loss], exe, mobile_program)","for op in test_graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        marked_nodes.add(op)",marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1},["marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1}"],1,
llvmlite,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/llvmlite/versioneer.py,https://github.com/numba/llvmlite/tree/master//versioneer.py,cmd_update_files,run$970,"def run(self):
    print(' creating %s' % versionfile_source)
    with open(versionfile_source, 'w') as f:
        assert VCS is not None, 'please set versioneer.VCS'
        LONG = LONG_VERSION_PY[VCS]
        f.write(LONG % {'DOLLAR': '$', 'TAG_PREFIX': tag_prefix, 'PARENTDIR_PREFIX': parentdir_prefix, 'VERSIONFILE_SOURCE': versionfile_source})
    ipy = os.path.join(os.path.dirname(versionfile_source), '__init__.py')
    if os.path.exists(ipy):
        try:
            with open(ipy, 'r') as f:
                old = f.read()
        except EnvironmentError:
            old = ''
        if INIT_PY_SNIPPET not in old:
            print(' appending to %s' % ipy)
            with open(ipy, 'a') as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print(' %s unmodified' % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None
    manifest_in = os.path.join(get_root(), 'MANIFEST.in')
    simple_includes = set()
    try:
        with open(manifest_in, 'r') as f:
            for line in f:
                if line.startswith('include '):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    if 'versioneer.py' not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, 'a') as f:
            f.write('include versioneer.py\n')
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" % versionfile_source)
        with open(manifest_in, 'a') as f:
            f.write('include %s\n' % versionfile_source)
    else:
        print(' versionfile_source already in MANIFEST.in')
    do_vcs_install(manifest_in, versionfile_source, ipy)","for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f for include in line.split()[1:] if line.startswith('include ')},["simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}"],0,
MatchZoo-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MatchZoo-py/matchzoo/preprocessors/units/frequency_filter.py,https://github.com/NTMC-Community/MatchZoo-py/tree/master/matchzoo/preprocessors/units/frequency_filter.py,FrequencyFilter,fit$52,"def fit(self, list_of_tokens: typing.List[typing.List[str]]):
    """"""Fit `list_of_tokens` by calculating `mode` states.""""""
    valid_terms = set()
    if self._mode == 'tf':
        stats = self._tf(list_of_tokens)
    elif self._mode == 'df':
        stats = self._df(list_of_tokens)
    elif self._mode == 'idf':
        stats = self._idf(list_of_tokens)
    else:
        raise ValueError(f'{self._mode} is not a valid filtering mode.Mode must be one of `tf`, `df`, and `idf`.')
    for (k, v) in stats.items():
        if self._low <= v < self._high:
            valid_terms.add(k)
    self._context[self._mode] = valid_terms","for (k, v) in stats.items():
    if self._low <= v < self._high:
        valid_terms.add(k)","valid_terms = {k for (k, v) in stats.items() if self._low <= v < self._high}","['valid_terms = {k for (k, v) in stats.items() if self._low <= v < self._high}']",1,
viztracer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/viztracer/tests/test_multiprocess.py,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_multiprocess.py,TestSubprocess,check_func$256,"def check_func(data):
    pids = set()
    for entry in data['traceEvents']:
        pids.add(entry['pid'])
    self.assertEqual(len(pids), 3)","for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']},["pids = {entry['pid'] for entry in data['traceEvents']}"],1,
opt_einsum,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opt_einsum/versioneer.py,https://github.com/dgasmith/opt_einsum/tree/master//versioneer.py,,do_setup$1697,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print('Adding sample versioneer config to setup.cfg', file=sys.stderr)
            with open(os.path.join(root, 'setup.cfg'), 'a') as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1
    print(' creating %s' % cfg.versionfile_source)
    with open(cfg.versionfile_source, 'w') as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {'DOLLAR': '$', 'STYLE': cfg.style, 'TAG_PREFIX': cfg.tag_prefix, 'PARENTDIR_PREFIX': cfg.parentdir_prefix, 'VERSIONFILE_SOURCE': cfg.versionfile_source})
    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), '__init__.py')
    if os.path.exists(ipy):
        try:
            with open(ipy, 'r') as f:
                old = f.read()
        except EnvironmentError:
            old = ''
        if INIT_PY_SNIPPET not in old:
            print(' appending to %s' % ipy)
            with open(ipy, 'a') as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print(' %s unmodified' % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None
    manifest_in = os.path.join(root, 'MANIFEST.in')
    simple_includes = set()
    try:
        with open(manifest_in, 'r') as f:
            for line in f:
                if line.startswith('include '):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    if 'versioneer.py' not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, 'a') as f:
            f.write('include versioneer.py\n')
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" % cfg.versionfile_source)
        with open(manifest_in, 'a') as f:
            f.write('include %s\n' % cfg.versionfile_source)
    else:
        print(' versionfile_source already in MANIFEST.in')
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f for include in line.split()[1:] if line.startswith('include ')},["simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}"],0,
tiny_python_projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tiny_python_projects/20_password/solution.py,https://github.com/kyclark/tiny_python_projects/tree/master/20_password/solution.py,,main$67,"def main():
    args = get_args()
    random.seed(args.seed)
    words = set()

    def word_len(word):
        return args.min_word_len <= len(word) <= args.max_word_len
    for fh in args.file:
        for line in fh:
            for word in filter(word_len, map(clean, line.lower().split())):
                words.add(word.title())
    words = sorted(words)
    passwords = [''.join(random.sample(words, args.num_words)) for _ in range(args.num)]
    if args.l33t:
        passwords = map(l33t, passwords)
    print('\n'.join(passwords))","for fh in args.file:
    for line in fh:
        for word in filter(word_len, map(clean, line.lower().split())):
            words.add(word.title())","words = {word.title() for fh in args.file for line in fh for word in filter(word_len, map(clean, line.lower().split()))}","['words = {word.title() for fh in args.file for line in fh for word in filter(word_len, map(clean, line.lower().split()))}']",1,
viztracer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/viztracer/tests/test_multiprocess.py,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_multiprocess.py,TestSubprocess,test_basic$222,"def test_basic(self):

    def check_func(data):
        pids = set()
        for entry in data['traceEvents']:
            pids.add(entry['pid'])
        self.assertEqual(len(pids), 4)
    self.template(['viztracer', '-o', 'result.json', 'cmdline_test.py'], expected_output_file='result.json', script=file_parent, check_func=check_func)","for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']},["pids = {entry['pid'] for entry in data['traceEvents']}"],1,
self-critical.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/self-critical.pytorch/scripts/dump_to_lmdb.py,https://github.com/ruotianluo/self-critical.pytorch/tree/master/scripts/dump_to_lmdb.py,,if_main_my$207,"if __name__ == '__main__':
    global args
    args = parse_args()
    args.output_file += args.folder.split('/')[-1]
    if args.folder.find('/') > 0:
        args.output_file = args.folder[:args.folder.rfind('/') + 1] + args.output_file
    print(args.output_file)
    img_list = json.load(open(args.input_json, 'r'))['images']
    fn_list = [str(_['cocoid']) for _ in img_list]
    found_ids = set()
    try:
        with open(args.output_file, 'r') as tsvfile:
            reader = csv.DictReader(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
            for item in reader:
                if item['status'] == 'True':
                    found_ids.add(item['image_id'])
    except:
        pass
    fn_list = [_ for _ in fn_list if _ not in found_ids]
    folder2lmdb(args.folder, fn_list)
    found_ids = set()
    with open(args.output_file, 'r') as tsvfile:
        reader = csv.DictReader(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
        for item in reader:
            if item['status'] == 'True':
                found_ids.add(item['image_id'])
    folder_dataset = FolderLMDB(args.folder + '.lmdb', list(found_ids))
    data_loader = DataLoader(folder_dataset, num_workers=16, collate_fn=lambda x: x)
    for data in tqdm.tqdm(data_loader):
        assert data[0] is not None","for item in reader:
    if item['status'] == 'True':
        found_ids.add(item['image_id'])",found_ids = {item['image_id'] for item in reader if item['status'] == 'True'},["found_ids = {item['image_id'] for item in reader if item['status'] == 'True'}"],1,
self-critical.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/self-critical.pytorch/scripts/dump_to_lmdb.py,https://github.com/ruotianluo/self-critical.pytorch/tree/master/scripts/dump_to_lmdb.py,,if_main_my$207,"if __name__ == '__main__':
    global args
    args = parse_args()
    args.output_file += args.folder.split('/')[-1]
    if args.folder.find('/') > 0:
        args.output_file = args.folder[:args.folder.rfind('/') + 1] + args.output_file
    print(args.output_file)
    img_list = json.load(open(args.input_json, 'r'))['images']
    fn_list = [str(_['cocoid']) for _ in img_list]
    found_ids = set()
    try:
        with open(args.output_file, 'r') as tsvfile:
            reader = csv.DictReader(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
            for item in reader:
                if item['status'] == 'True':
                    found_ids.add(item['image_id'])
    except:
        pass
    fn_list = [_ for _ in fn_list if _ not in found_ids]
    folder2lmdb(args.folder, fn_list)
    found_ids = set()
    with open(args.output_file, 'r') as tsvfile:
        reader = csv.DictReader(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
        for item in reader:
            if item['status'] == 'True':
                found_ids.add(item['image_id'])
    folder_dataset = FolderLMDB(args.folder + '.lmdb', list(found_ids))
    data_loader = DataLoader(folder_dataset, num_workers=16, collate_fn=lambda x: x)
    for data in tqdm.tqdm(data_loader):
        assert data[0] is not None","for item in reader:
    if item['status'] == 'True':
        found_ids.add(item['image_id'])",found_ids = {item['image_id'] for item in reader if item['status'] == 'True'},["found_ids = {item['image_id'] for item in reader if item['status'] == 'True'}"],1,
coveragepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coveragepy/coverage/parser.py,https://github.com/nedbat/coveragepy/tree/master/coverage/parser.py,PythonParser,lines_matching$87,"def lines_matching(self, *regexes):
    """"""Find the lines matching one of a list of regexes.

        Returns a set of line numbers, the lines that contain a match for one
        of the regexes in `regexes`.  The entire line needn't match, just a
        part of it.

        """"""
    combined = join_regex(regexes)
    regex_c = re.compile(combined)
    matches = set()
    for (i, ltext) in enumerate(self.lines, start=1):
        if regex_c.search(ltext):
            matches.add(i)
    return matches","for (i, ltext) in enumerate(self.lines, start=1):
    if regex_c.search(ltext):
        matches.add(i)","matches = {i for (i, ltext) in enumerate(self.lines, start=1) if regex_c.search(ltext)}","['matches = {i for (i, ltext) in enumerate(self.lines, start=1) if regex_c.search(ltext)}']",1,
shuup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/campaigns/utils/campaigns.py,https://github.com/shuup/shuup/tree/master/shuup/campaigns/utils/campaigns.py,,get_lines_suppliers$11,"def get_lines_suppliers(basket):
    """"""
    Returns a list of all suppliers from the basket
    """"""
    suppliers = set()
    for line in basket.get_lines():
        if line.supplier:
            suppliers.add(line.supplier)
    return suppliers","for line in basket.get_lines():
    if line.supplier:
        suppliers.add(line.supplier)",suppliers = {line.supplier for line in basket.get_lines() if line.supplier},['suppliers = {line.supplier for line in basket.get_lines() if line.supplier}'],1,
aiortc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aiortc/src/aiortc/rtcdtlstransport.py,https://github.com/aiortc/aiortc/tree/master/src/aiortc/rtcdtlstransport.py,RTCDtlsTransport,_register_rtp_receiver$638,"def _register_rtp_receiver(self, receiver, parameters: RTCRtpReceiveParameters) -> None:
    ssrcs = set()
    for encoding in parameters.encodings:
        ssrcs.add(encoding.ssrc)
    self._rtp_header_extensions_map.configure(parameters)
    self._rtp_router.register_receiver(receiver, ssrcs=list(ssrcs), payload_types=[codec.payloadType for codec in parameters.codecs], mid=parameters.muxId)","for encoding in parameters.encodings:
    ssrcs.add(encoding.ssrc)",ssrcs = {encoding.ssrc for encoding in parameters.encodings},['ssrcs = {encoding.ssrc for encoding in parameters.encodings}'],1,
pyro,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyro/tests/nn/test_autoregressive.py,https://github.com/pyro-ppl/pyro/tree/master/tests/nn/test_autoregressive.py,AutoRegressiveNNTests,_test_masks$61,"def _test_masks(self, input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier):
    (masks, mask_skip) = create_mask(input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier)
    permutation = list(permutation.numpy())
    for idx in range(input_dim):
        correct = torch.cat((torch.arange(observed_dim, dtype=torch.long), torch.tensor(sorted(permutation[0:permutation.index(idx)]), dtype=torch.long) + observed_dim))
        for jdx in range(output_dim_multiplier):
            prev_connections = set()
            for kdx in range(masks[-1].size(1)):
                if masks[-1][idx + jdx * input_dim, kdx]:
                    prev_connections.add(kdx)
            for m in reversed(masks[:-1]):
                this_connections = set()
                for kdx in prev_connections:
                    for ldx in range(m.size(1)):
                        if m[kdx, ldx]:
                            this_connections.add(ldx)
                prev_connections = this_connections
            assert (torch.tensor(list(sorted(prev_connections)), dtype=torch.long) == correct).all()
            skip_connections = set()
            for kdx in range(mask_skip.size(1)):
                if mask_skip[idx + jdx * input_dim, kdx]:
                    skip_connections.add(kdx)
            assert (torch.tensor(list(sorted(skip_connections)), dtype=torch.long) == correct).all()","for kdx in range(masks[-1].size(1)):
    if masks[-1][idx + jdx * input_dim, kdx]:
        prev_connections.add(kdx)","prev_connections = {kdx for kdx in range(masks[-1].size(1)) if masks[-1][idx + jdx * input_dim, kdx]}","['prev_connections = {kdx for kdx in range(masks[-1].size(1)) if masks[-1][idx + jdx * input_dim, kdx]}']",1,
pyro,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyro/tests/nn/test_autoregressive.py,https://github.com/pyro-ppl/pyro/tree/master/tests/nn/test_autoregressive.py,AutoRegressiveNNTests,_test_masks$61,"def _test_masks(self, input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier):
    (masks, mask_skip) = create_mask(input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier)
    permutation = list(permutation.numpy())
    for idx in range(input_dim):
        correct = torch.cat((torch.arange(observed_dim, dtype=torch.long), torch.tensor(sorted(permutation[0:permutation.index(idx)]), dtype=torch.long) + observed_dim))
        for jdx in range(output_dim_multiplier):
            prev_connections = set()
            for kdx in range(masks[-1].size(1)):
                if masks[-1][idx + jdx * input_dim, kdx]:
                    prev_connections.add(kdx)
            for m in reversed(masks[:-1]):
                this_connections = set()
                for kdx in prev_connections:
                    for ldx in range(m.size(1)):
                        if m[kdx, ldx]:
                            this_connections.add(ldx)
                prev_connections = this_connections
            assert (torch.tensor(list(sorted(prev_connections)), dtype=torch.long) == correct).all()
            skip_connections = set()
            for kdx in range(mask_skip.size(1)):
                if mask_skip[idx + jdx * input_dim, kdx]:
                    skip_connections.add(kdx)
            assert (torch.tensor(list(sorted(skip_connections)), dtype=torch.long) == correct).all()","for kdx in prev_connections:
    for ldx in range(m.size(1)):
        if m[kdx, ldx]:
            this_connections.add(ldx)","this_connections = {ldx for kdx in prev_connections for ldx in range(m.size(1)) if m[kdx, ldx]}","['this_connections = {ldx for kdx in prev_connections for ldx in range(m.size(1)) if m[kdx, ldx]}']",1,
oppia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/suggestion_registry.py,https://github.com/oppia/oppia/tree/master/core/domain/suggestion_registry.py,CommunityContributionStats,get_translation_language_codes_that_need_reviewers$1344,"def get_translation_language_codes_that_need_reviewers(self) -> Set[str]:
    """"""Returns the language codes where more reviewers are needed to review
        translations in those language codes. Translation suggestions in a
        given language need more reviewers if the number of translation
        suggestions in that language divided by the number of translation
        reviewers in that language is greater than
        config_domain.MAX_NUMBER_OF_SUGGESTIONS_PER_REVIEWER.

        Returns:
            set. A set of of the language codes where more translation reviewers
            are needed.
        """"""
    language_codes_that_need_reviewers = set()
    for language_code in self.translation_suggestion_counts_by_lang_code:
        if self.are_translation_reviewers_needed_for_lang_code(language_code):
            language_codes_that_need_reviewers.add(language_code)
    return language_codes_that_need_reviewers","for language_code in self.translation_suggestion_counts_by_lang_code:
    if self.are_translation_reviewers_needed_for_lang_code(language_code):
        language_codes_that_need_reviewers.add(language_code)",language_codes_that_need_reviewers = {language_code for language_code in self.translation_suggestion_counts_by_lang_code if self.are_translation_reviewers_needed_for_lang_code(language_code)},['language_codes_that_need_reviewers = {language_code for language_code in self.translation_suggestion_counts_by_lang_code if self.are_translation_reviewers_needed_for_lang_code(language_code)}'],1,
DeepCTR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepCTR/deepctr/layers/interaction.py,https://github.com/shenweichen/DeepCTR/tree/master/deepctr/layers/interaction.py,OutterProductLayer,build$805,"def build(self, input_shape):
    if not isinstance(input_shape, list) or len(input_shape) < 2:
        raise ValueError('A `OutterProductLayer` layer should be called on a list of at least 2 inputs')
    reduced_inputs_shapes = [shape.as_list() for shape in input_shape]
    shape_set = set()
    for i in range(len(input_shape)):
        shape_set.add(tuple(reduced_inputs_shapes[i]))
    if len(shape_set) > 1:
        raise ValueError('A `OutterProductLayer` layer requires inputs with same shapes Got different shapes: %s' % shape_set)
    if len(input_shape[0]) != 3 or input_shape[0][1] != 1:
        raise ValueError('A `OutterProductLayer` layer requires inputs of a list with same shape tensor like (None,1,embedding_size)Got different shapes: %s' % input_shape[0])
    num_inputs = len(input_shape)
    num_pairs = int(num_inputs * (num_inputs - 1) / 2)
    input_shape = input_shape[0]
    embed_size = int(input_shape[-1])
    if self.kernel_type == 'mat':
        self.kernel = self.add_weight(shape=(embed_size, num_pairs, embed_size), initializer=glorot_uniform(seed=self.seed), name='kernel')
    elif self.kernel_type == 'vec':
        self.kernel = self.add_weight(shape=(num_pairs, embed_size), initializer=glorot_uniform(self.seed), name='kernel')
    elif self.kernel_type == 'num':
        self.kernel = self.add_weight(shape=(num_pairs, 1), initializer=glorot_uniform(self.seed), name='kernel')
    super(OutterProductLayer, self).build(input_shape)","for i in range(len(input_shape)):
    shape_set.add(tuple(reduced_inputs_shapes[i]))",shape_set = {tuple(reduced_inputs_shapes[i]) for i in range(len(input_shape))},['shape_set = {tuple(reduced_inputs_shapes[i]) for i in range(len(input_shape))}'],1,
pywikibot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pywikibot/scripts/archivebot.py,https://github.com/wikimedia/pywikibot/tree/master/scripts/archivebot.py,PageArchiver,run$761,"def run(self) -> None:
    """"""Process a single DiscussionPage object.""""""
    if not self.page.botMayEdit():
        return
    whys = self.analyze_page()
    mintoarchive = int(self.get_attr('minthreadstoarchive', 2))
    if self.archived_threads < mintoarchive:
        pywikibot.output('Only {} (< {}) threads are old enough. Skipping'.format(self.archived_threads, mintoarchive))
        return
    if whys:
        rx = re.compile('\\{\\{%s\\s*?\\n.*?\\n\\}\\}' % template_title_regex(self.tpl).pattern, re.DOTALL)
        if not rx.search(self.page.header):
            raise MalformedConfigError(""Couldn't find the template in the header"")
        pywikibot.output('Archiving {} thread(s).'.format(self.archived_threads))
        for (title, archive) in sorted(self.archives.items()):
            count = archive.archived_threads
            if count == 0:
                continue
            self.comment_params['count'] = count
            comment = i18n.twtranslate(self.site.code, 'archivebot-archive-summary', self.comment_params)
            archive.update(comment)
        self.page.header = rx.sub(self.attr2text(), self.page.header)
        self.comment_params['count'] = self.archived_threads
        comma = self.site.mediawiki_message('comma-separator')
        self.comment_params['archives'] = comma.join((a.title(as_link=True) for a in self.archives.values() if a.archived_threads > 0))
        translated_whys = set()
        for (why, arg) in whys:
            if why == 'duration':
                translated_whys.add(i18n.twtranslate(self.site.code, 'archivebot-older-than', {'duration': arg, 'count': self.archived_threads}))
        self.comment_params['why'] = comma.join(translated_whys)
        comment = i18n.twtranslate(self.site.code, 'archivebot-page-summary', self.comment_params)
        self.page.update(comment)","for (why, arg) in whys:
    if why == 'duration':
        translated_whys.add(i18n.twtranslate(self.site.code, 'archivebot-older-than', {'duration': arg, 'count': self.archived_threads}))","translated_whys = {i18n.twtranslate(self.site.code, 'archivebot-older-than', {'duration': arg, 'count': self.archived_threads}) for (why, arg) in whys if why == 'duration'}","[""translated_whys = {i18n.twtranslate(self.site.code, 'archivebot-older-than', {'duration': arg, 'count': self.archived_threads}) for (why, arg) in whys if why == 'duration'}""]",1,
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/decompiler/ail_simplifier.py,https://github.com/angr/angr/tree/master/angr/analyses/decompiler/ail_simplifier.py,AILSimplifier,_unify_local_variables$148,"def _unify_local_variables(self) -> bool:
    """"""
        Find variables that are definitely equivalent and then eliminate unnecessary copies.
        """"""
    simplified = False
    prop = self._compute_propagation()
    if not prop.equivalence:
        return simplified
    addr_and_idx_to_block: Dict[Tuple[int, int], Block] = {}
    for block in self.func_graph.nodes():
        addr_and_idx_to_block[block.addr, block.idx] = block
    equivalences: Dict[Any, Set[Equivalence]] = defaultdict(set)
    for eq in prop.equivalence:
        equivalences[eq.atom1].add(eq)
    for (_, eqs) in equivalences.items():
        if len(eqs) > 1:
            continue
        eq = next(iter(eqs))
        the_def = None
        if isinstance(eq.atom0, SimStackVariable):
            if isinstance(eq.atom1, Register):
                reg = eq.atom1
            elif isinstance(eq.atom1, Convert) and isinstance(eq.atom1.operand, Register):
                reg = eq.atom1.operand
            else:
                continue
        elif isinstance(eq.atom0, Register):
            if isinstance(eq.atom1, Register):
                reg = eq.atom1
            else:
                continue
        else:
            continue
        rd = self._compute_reaching_definitions()
        defs = rd.all_uses.get_uses_by_location(eq.codeloc)
        if len(defs) != 1:
            continue
        for def_ in defs:
            def_: Definition
            if isinstance(def_.atom, atoms.Register) and def_.atom.reg_offset == reg.reg_offset:
                the_def = def_
                break
        if the_def is None:
            continue
        if isinstance(the_def.codeloc, ExternalCodeLocation):
            defs = [def_ for def_ in rd.all_definitions if def_.codeloc == eq.codeloc]
            all_uses_with_def = None
            (to_replace, replace_with) = (None, None)
            remove_initial_assignment = None
            if defs and len(defs) == 1:
                stackvar_def = defs[0]
                if isinstance(stackvar_def.atom, atoms.MemoryLocation) and isinstance(stackvar_def.atom.addr, SpOffset):
                    if any((def_ != stackvar_def and def_.atom == stackvar_def.atom for def_ in rd.all_definitions if isinstance(def_.atom, atoms.MemoryLocation))):
                        continue
                    if any((def_ != the_def and def_.atom == the_def.atom for def_ in rd.all_definitions if isinstance(def_.atom, atoms.Register))):
                        continue
                    all_stackvar_uses: Set[CodeLocation] = set(rd.all_uses.get_uses(stackvar_def))
                    all_uses_with_def = set()
                    for use in all_stackvar_uses:
                        all_uses_with_def.add((stackvar_def, use))
                    to_replace = Load(None, StackBaseOffset(None, self.project.arch.bits, eq.atom0.offset), eq.atom0.size, endness=self.project.arch.memory_endness)
                    replace_with = eq.atom1
                    remove_initial_assignment = True
            if all_uses_with_def is None:
                continue
        else:
            all_uses: Set[CodeLocation] = set(rd.all_uses.get_uses(the_def))
            all_uses_with_def = set(((the_def, use) for use in all_uses))
            remove_initial_assignment = False
            if isinstance(eq.atom0, SimStackVariable):
                to_replace = eq.atom1
                replace_with = Load(None, StackBaseOffset(None, self.project.arch.bits, eq.atom0.offset), eq.atom0.size, endness=self.project.arch.memory_endness)
            elif isinstance(eq.atom0, Register):
                to_replace = eq.atom1
                replace_with = eq.atom0
            else:
                raise RuntimeError('Unsupported atom0 type %s.' % type(eq.atom0))
        all_uses_replaced = True
        for (def_, u) in all_uses_with_def:
            if u == eq.codeloc:
                continue
            old_block = addr_and_idx_to_block.get((u.block_addr, u.block_idx), None)
            if old_block is None:
                continue
            the_block = self.blocks.get(old_block, old_block)
            stmt: Statement = the_block.statements[u.stmt_idx]
            (r, new_block) = self._replace_expr_and_update_block(the_block, u.stmt_idx, stmt, def_, u, to_replace, replace_with)
            if r:
                self.blocks[old_block] = new_block
            else:
                all_uses_replaced = False
            simplified |= r
        if all_uses_replaced and remove_initial_assignment:
            self._assignments_to_remove.add(eq.codeloc)
    return simplified","for eq in prop.equivalence:
    equivalences[eq.atom1].add(eq)",equivalences = {eq.atom1: {eq for eq in prop.equivalence if eq.atom1 == eq.atom1} for eq in prop.equivalence},Cannot refactor,2,
Automatic-Udemy-Course-Enroller-GET-PAID-UDEMY-COURSES-for-FREE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Automatic-Udemy-Course-Enroller-GET-PAID-UDEMY-COURSES-for-FREE/udemy_enroller/scrapers/tutorialbar.py,https://github.com/aapatre/Automatic-Udemy-Course-Enroller-GET-PAID-UDEMY-COURSES-for-FREE/tree/master/udemy_enroller/scrapers/tutorialbar.py,TutorialBarScraper,_filter_ad_domains$62,"def _filter_ad_domains(self, udemy_links) -> List:
    """"""
        Filter out any known ad domains from the links scraped

        :param list udemy_links: List of urls to filter ad domains from
        :return: A list of filtered urls
        """"""
    ad_links = set()
    for link in udemy_links:
        for ad_domain in self.AD_DOMAINS:
            if link.startswith(ad_domain):
                ad_links.add(link)
    if ad_links:
        logger.info(f'Removing ad links from courses: {ad_links}')
    return list(set(udemy_links) - ad_links)","for link in udemy_links:
    for ad_domain in self.AD_DOMAINS:
        if link.startswith(ad_domain):
            ad_links.add(link)",ad_links = {link for link in udemy_links for ad_domain in self.AD_DOMAINS if link.startswith(ad_domain)},['ad_links = {link for link in udemy_links for ad_domain in self.AD_DOMAINS if link.startswith(ad_domain)}'],1,
neutron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/agent/l3/extensions/port_forwarding.py,https://github.com/openstack/neutron/tree/master/neutron/agent/l3/extensions/port_forwarding.py,PortForwardingAgentExtension,_sync_and_remove_fip$395,"def _sync_and_remove_fip(self, context, fip_id_cidrs, device, ri):
    if not fip_id_cidrs:
        return
    ha_port = ri.router.get(constants.HA_INTERFACE_KEY)
    fip_ids = [item[0] for item in fip_id_cidrs]
    pfs = self.resource_rpc.bulk_pull(context, resources.PORTFORWARDING, filter_kwargs={'floatingip_id': fip_ids})
    exist_fips = set()
    fip_status = {}
    for pf in pfs:
        exist_fips.add(pf.floatingip_id)
    for fip_id_cidr in fip_id_cidrs:
        if fip_id_cidr[0] not in exist_fips:
            if ha_port:
                ri._remove_vip(fip_id_cidr[1])
            else:
                device.delete_addr_and_conntrack_state(fip_id_cidr[1])
            fip_status[fip_id_cidr[0]] = 'DOWN'
    if ha_port:
        ri.enable_keepalived()
    self._sending_port_forwarding_fip_status(ri, fip_status)
    for fip_id in fip_status.keys():
        self.mapping.clear_by_fip(fip_id, ri.router_id)","for pf in pfs:
    exist_fips.add(pf.floatingip_id)",exist_fips = {pf.floatingip_id for pf in pfs},['exist_fips = {pf.floatingip_id for pf in pfs}'],1,
cloudtracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloudtracker/cloudtracker/datasources/athena.py,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,__init__$164,"def __init__(self, config, account, start, end, args):
    logging.getLogger('botocore').setLevel(logging.WARN)
    logging.info('Source of CloudTrail logs: s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    if (datetime.datetime.now() - datetime.datetime.strptime(start, '%Y-%m-%d')).days > 365:
        raise Exception('Start date is over a year old. CloudTracker does not create or use partitions over a year old.')
    month_restrictions = set()
    start = start.split('-')
    end = end.split('-')
    if start[0] == end[0]:
        for month in range(int(start[1]), int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
    else:
        for month in range(int(start[1]), 12 + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
        for year in range(int(start[0]), int(end[0])):
            for month in (1, 12 + 1):
                month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month))
        for month in range(1, int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month))
    self.search_filter = '((' + ' or '.join(month_restrictions) + ') and errorcode IS NULL)'
    self.table_name = 'cloudtrail_logs_{}'.format(account['id'])
    sts = boto3.client('sts')
    identity = sts.get_caller_identity()
    logging.info('Using AWS identity: {}'.format(identity['Arn']))
    current_account_id = identity['Account']
    region = boto3.session.Session().region_name
    if 'output_s3_bucket' in config:
        self.output_bucket = config['output_s3_bucket']
    else:
        self.output_bucket = 's3://aws-athena-query-results-{}-{}'.format(current_account_id, region)
    logging.info('Using output bucket: {}'.format(self.output_bucket))
    if 'workgroup' in config:
        self.workgroup = config['workgroup']
    logging.info('Using workgroup: {}'.format(self.workgroup))
    if not config.get('org_id'):
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], account_id=account['id'])
    else:
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], org_id=config['org_id'], account_id=account['id'])
    logging.info('Account cloudtrail log path: {}'.format(cloudtrail_log_path))
    self.athena = boto3.client('athena')
    self.s3 = boto3.client('s3')
    if args.skip_setup:
        logging.info('Skipping initial table creation')
        return
    resp = self.s3.list_objects_v2(Bucket=config['s3_bucket'], Prefix=config['path'], MaxKeys=1)
    if 'Contents' not in resp or len(resp['Contents']) == 0:
        exit('ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    self.query_athena('CREATE DATABASE IF NOT EXISTS {db} {comment}'.format(db=self.database, comment=""COMMENT 'Created by CloudTracker'""), context=None)
    query = ""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (\n            `eventversion` string COMMENT 'from deserializer', \n            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', \n            `eventtime` string COMMENT 'from deserializer', \n            `eventsource` string COMMENT 'from deserializer', \n            `eventname` string COMMENT 'from deserializer', \n            `awsregion` string COMMENT 'from deserializer', \n            `sourceipaddress` string COMMENT 'from deserializer', \n            `useragent` string COMMENT 'from deserializer', \n            `errorcode` string COMMENT 'from deserializer', \n            `errormessage` string COMMENT 'from deserializer', \n            `requestparameters` string COMMENT 'from deserializer', \n            `responseelements` string COMMENT 'from deserializer', \n            `additionaleventdata` string COMMENT 'from deserializer', \n            `requestid` string COMMENT 'from deserializer', \n            `eventid` string COMMENT 'from deserializer', \n            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', \n            `eventtype` string COMMENT 'from deserializer', \n            `apiversion` string COMMENT 'from deserializer', \n            `readonly` string COMMENT 'from deserializer', \n            `recipientaccountid` string COMMENT 'from deserializer', \n            `serviceeventdetails` string COMMENT 'from deserializer', \n            `sharedeventid` string COMMENT 'from deserializer', \n            `vpcendpointid` string COMMENT 'from deserializer')\n            PARTITIONED BY (region string, year string, month string)\n            ROW FORMAT SERDE \n            'com.amazon.emr.hive.serde.CloudTrailSerde' \n            STORED AS INPUTFORMAT \n            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' \n            OUTPUTFORMAT \n            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n            LOCATION '{cloudtrail_log_path}'"".format(table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path)
    self.query_athena(query)
    logging.info('Checking if all partitions for the past {} months exist'.format(NUM_MONTHS_FOR_PARTITIONS))
    query = 'SHOW PARTITIONS {table_name}'.format(table_name=self.table_name)
    partition_list = self.query_athena(query, skip_header=False)
    partition_set = set()
    for partition in partition_list:
        partition_set.add(partition[0])
    regions = boto3.session.Session().get_available_regions('ec2')
    queries_to_make = set()
    for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
        date_of_interest = datetime.datetime.now() - relativedelta(months=num_months_ago)
        year = date_of_interest.year
        month = '{:0>2}'.format(date_of_interest.month)
        query = ''
        for region in regions:
            if 'region={region}/year={year}/month={month}'.format(region=region, year=year, month=month) in partition_set:
                continue
            query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(region=region, year=year, month=month, cloudtrail_log_path=cloudtrail_log_path)
        if query != '':
            queries_to_make.add('ALTER TABLE {table_name} ADD '.format(table_name=self.table_name) + query)
    query_count = len(queries_to_make)
    for query in queries_to_make:
        logging.info('Partition groups remaining to create: {}'.format(query_count))
        self.query_athena(query)
        query_count -= 1","for partition in partition_list:
    partition_set.add(partition[0])",partition_set = {partition[0] for partition in partition_list},['partition_set = {partition[0] for partition in partition_list}'],1,
cloudtracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloudtracker/cloudtracker/datasources/athena.py,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,__init__$164,"def __init__(self, config, account, start, end, args):
    logging.getLogger('botocore').setLevel(logging.WARN)
    logging.info('Source of CloudTrail logs: s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    if (datetime.datetime.now() - datetime.datetime.strptime(start, '%Y-%m-%d')).days > 365:
        raise Exception('Start date is over a year old. CloudTracker does not create or use partitions over a year old.')
    month_restrictions = set()
    start = start.split('-')
    end = end.split('-')
    if start[0] == end[0]:
        for month in range(int(start[1]), int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
    else:
        for month in range(int(start[1]), 12 + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
        for year in range(int(start[0]), int(end[0])):
            for month in (1, 12 + 1):
                month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month))
        for month in range(1, int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month))
    self.search_filter = '((' + ' or '.join(month_restrictions) + ') and errorcode IS NULL)'
    self.table_name = 'cloudtrail_logs_{}'.format(account['id'])
    sts = boto3.client('sts')
    identity = sts.get_caller_identity()
    logging.info('Using AWS identity: {}'.format(identity['Arn']))
    current_account_id = identity['Account']
    region = boto3.session.Session().region_name
    if 'output_s3_bucket' in config:
        self.output_bucket = config['output_s3_bucket']
    else:
        self.output_bucket = 's3://aws-athena-query-results-{}-{}'.format(current_account_id, region)
    logging.info('Using output bucket: {}'.format(self.output_bucket))
    if 'workgroup' in config:
        self.workgroup = config['workgroup']
    logging.info('Using workgroup: {}'.format(self.workgroup))
    if not config.get('org_id'):
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], account_id=account['id'])
    else:
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], org_id=config['org_id'], account_id=account['id'])
    logging.info('Account cloudtrail log path: {}'.format(cloudtrail_log_path))
    self.athena = boto3.client('athena')
    self.s3 = boto3.client('s3')
    if args.skip_setup:
        logging.info('Skipping initial table creation')
        return
    resp = self.s3.list_objects_v2(Bucket=config['s3_bucket'], Prefix=config['path'], MaxKeys=1)
    if 'Contents' not in resp or len(resp['Contents']) == 0:
        exit('ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    self.query_athena('CREATE DATABASE IF NOT EXISTS {db} {comment}'.format(db=self.database, comment=""COMMENT 'Created by CloudTracker'""), context=None)
    query = ""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (\n            `eventversion` string COMMENT 'from deserializer', \n            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', \n            `eventtime` string COMMENT 'from deserializer', \n            `eventsource` string COMMENT 'from deserializer', \n            `eventname` string COMMENT 'from deserializer', \n            `awsregion` string COMMENT 'from deserializer', \n            `sourceipaddress` string COMMENT 'from deserializer', \n            `useragent` string COMMENT 'from deserializer', \n            `errorcode` string COMMENT 'from deserializer', \n            `errormessage` string COMMENT 'from deserializer', \n            `requestparameters` string COMMENT 'from deserializer', \n            `responseelements` string COMMENT 'from deserializer', \n            `additionaleventdata` string COMMENT 'from deserializer', \n            `requestid` string COMMENT 'from deserializer', \n            `eventid` string COMMENT 'from deserializer', \n            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', \n            `eventtype` string COMMENT 'from deserializer', \n            `apiversion` string COMMENT 'from deserializer', \n            `readonly` string COMMENT 'from deserializer', \n            `recipientaccountid` string COMMENT 'from deserializer', \n            `serviceeventdetails` string COMMENT 'from deserializer', \n            `sharedeventid` string COMMENT 'from deserializer', \n            `vpcendpointid` string COMMENT 'from deserializer')\n            PARTITIONED BY (region string, year string, month string)\n            ROW FORMAT SERDE \n            'com.amazon.emr.hive.serde.CloudTrailSerde' \n            STORED AS INPUTFORMAT \n            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' \n            OUTPUTFORMAT \n            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n            LOCATION '{cloudtrail_log_path}'"".format(table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path)
    self.query_athena(query)
    logging.info('Checking if all partitions for the past {} months exist'.format(NUM_MONTHS_FOR_PARTITIONS))
    query = 'SHOW PARTITIONS {table_name}'.format(table_name=self.table_name)
    partition_list = self.query_athena(query, skip_header=False)
    partition_set = set()
    for partition in partition_list:
        partition_set.add(partition[0])
    regions = boto3.session.Session().get_available_regions('ec2')
    queries_to_make = set()
    for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
        date_of_interest = datetime.datetime.now() - relativedelta(months=num_months_ago)
        year = date_of_interest.year
        month = '{:0>2}'.format(date_of_interest.month)
        query = ''
        for region in regions:
            if 'region={region}/year={year}/month={month}'.format(region=region, year=year, month=month) in partition_set:
                continue
            query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(region=region, year=year, month=month, cloudtrail_log_path=cloudtrail_log_path)
        if query != '':
            queries_to_make.add('ALTER TABLE {table_name} ADD '.format(table_name=self.table_name) + query)
    query_count = len(queries_to_make)
    for query in queries_to_make:
        logging.info('Partition groups remaining to create: {}'.format(query_count))
        self.query_athena(query)
        query_count -= 1","for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
    date_of_interest = datetime.datetime.now() - relativedelta(months=num_months_ago)
    year = date_of_interest.year
    month = '{:0>2}'.format(date_of_interest.month)
    query = ''
    for region in regions:
        if 'region={region}/year={year}/month={month}'.format(region=region, year=year, month=month) in partition_set:
            continue
        query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(region=region, year=year, month=month, cloudtrail_log_path=cloudtrail_log_path)
    if query != '':
        queries_to_make.add('ALTER TABLE {table_name} ADD '.format(table_name=self.table_name) + query)","queries_to_make = {'ALTER TABLE {table_name} ADD '.format(table_name=self.table_name) + ''.join([""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(region=region, year=date_of_interest.year, month='{0:0>2}'.format(date_of_interest.month), cloudtrail_log_path=cloudtrail_log_path) for region in regions if 'region={region}/year={year}/month={month}'.format(region=region, year=date_of_interest.year, month='{0:0>2}'.format(date_of_interest.month)) not in partition_set]) for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS) for date_of_interest in [datetime.datetime.now() - relativedelta(months=num_months_ago)]}",Cannot refactor,2,
cloudtracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloudtracker/cloudtracker/datasources/athena.py,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,__init__$164,"def __init__(self, config, account, start, end, args):
    logging.getLogger('botocore').setLevel(logging.WARN)
    logging.info('Source of CloudTrail logs: s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    if (datetime.datetime.now() - datetime.datetime.strptime(start, '%Y-%m-%d')).days > 365:
        raise Exception('Start date is over a year old. CloudTracker does not create or use partitions over a year old.')
    month_restrictions = set()
    start = start.split('-')
    end = end.split('-')
    if start[0] == end[0]:
        for month in range(int(start[1]), int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
    else:
        for month in range(int(start[1]), 12 + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
        for year in range(int(start[0]), int(end[0])):
            for month in (1, 12 + 1):
                month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month))
        for month in range(1, int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month))
    self.search_filter = '((' + ' or '.join(month_restrictions) + ') and errorcode IS NULL)'
    self.table_name = 'cloudtrail_logs_{}'.format(account['id'])
    sts = boto3.client('sts')
    identity = sts.get_caller_identity()
    logging.info('Using AWS identity: {}'.format(identity['Arn']))
    current_account_id = identity['Account']
    region = boto3.session.Session().region_name
    if 'output_s3_bucket' in config:
        self.output_bucket = config['output_s3_bucket']
    else:
        self.output_bucket = 's3://aws-athena-query-results-{}-{}'.format(current_account_id, region)
    logging.info('Using output bucket: {}'.format(self.output_bucket))
    if 'workgroup' in config:
        self.workgroup = config['workgroup']
    logging.info('Using workgroup: {}'.format(self.workgroup))
    if not config.get('org_id'):
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], account_id=account['id'])
    else:
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], org_id=config['org_id'], account_id=account['id'])
    logging.info('Account cloudtrail log path: {}'.format(cloudtrail_log_path))
    self.athena = boto3.client('athena')
    self.s3 = boto3.client('s3')
    if args.skip_setup:
        logging.info('Skipping initial table creation')
        return
    resp = self.s3.list_objects_v2(Bucket=config['s3_bucket'], Prefix=config['path'], MaxKeys=1)
    if 'Contents' not in resp or len(resp['Contents']) == 0:
        exit('ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    self.query_athena('CREATE DATABASE IF NOT EXISTS {db} {comment}'.format(db=self.database, comment=""COMMENT 'Created by CloudTracker'""), context=None)
    query = ""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (\n            `eventversion` string COMMENT 'from deserializer', \n            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', \n            `eventtime` string COMMENT 'from deserializer', \n            `eventsource` string COMMENT 'from deserializer', \n            `eventname` string COMMENT 'from deserializer', \n            `awsregion` string COMMENT 'from deserializer', \n            `sourceipaddress` string COMMENT 'from deserializer', \n            `useragent` string COMMENT 'from deserializer', \n            `errorcode` string COMMENT 'from deserializer', \n            `errormessage` string COMMENT 'from deserializer', \n            `requestparameters` string COMMENT 'from deserializer', \n            `responseelements` string COMMENT 'from deserializer', \n            `additionaleventdata` string COMMENT 'from deserializer', \n            `requestid` string COMMENT 'from deserializer', \n            `eventid` string COMMENT 'from deserializer', \n            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', \n            `eventtype` string COMMENT 'from deserializer', \n            `apiversion` string COMMENT 'from deserializer', \n            `readonly` string COMMENT 'from deserializer', \n            `recipientaccountid` string COMMENT 'from deserializer', \n            `serviceeventdetails` string COMMENT 'from deserializer', \n            `sharedeventid` string COMMENT 'from deserializer', \n            `vpcendpointid` string COMMENT 'from deserializer')\n            PARTITIONED BY (region string, year string, month string)\n            ROW FORMAT SERDE \n            'com.amazon.emr.hive.serde.CloudTrailSerde' \n            STORED AS INPUTFORMAT \n            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' \n            OUTPUTFORMAT \n            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n            LOCATION '{cloudtrail_log_path}'"".format(table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path)
    self.query_athena(query)
    logging.info('Checking if all partitions for the past {} months exist'.format(NUM_MONTHS_FOR_PARTITIONS))
    query = 'SHOW PARTITIONS {table_name}'.format(table_name=self.table_name)
    partition_list = self.query_athena(query, skip_header=False)
    partition_set = set()
    for partition in partition_list:
        partition_set.add(partition[0])
    regions = boto3.session.Session().get_available_regions('ec2')
    queries_to_make = set()
    for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
        date_of_interest = datetime.datetime.now() - relativedelta(months=num_months_ago)
        year = date_of_interest.year
        month = '{:0>2}'.format(date_of_interest.month)
        query = ''
        for region in regions:
            if 'region={region}/year={year}/month={month}'.format(region=region, year=year, month=month) in partition_set:
                continue
            query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(region=region, year=year, month=month, cloudtrail_log_path=cloudtrail_log_path)
        if query != '':
            queries_to_make.add('ALTER TABLE {table_name} ADD '.format(table_name=self.table_name) + query)
    query_count = len(queries_to_make)
    for query in queries_to_make:
        logging.info('Partition groups remaining to create: {}'.format(query_count))
        self.query_athena(query)
        query_count -= 1","for month in range(int(start[1]), int(end[1]) + 1):
    month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))","month_restrictions = {""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month) for month in range(int(start[1]), int(end[1]) + 1)}","['month_restrictions = {""(year = \'{:0>2}\' and month = \'{:0>2}\')"".format(start[0], month) for month in range(int(start[1]), int(end[1]) + 1)}']",1,
cloudtracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloudtracker/cloudtracker/datasources/athena.py,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,__init__$164,"def __init__(self, config, account, start, end, args):
    logging.getLogger('botocore').setLevel(logging.WARN)
    logging.info('Source of CloudTrail logs: s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    if (datetime.datetime.now() - datetime.datetime.strptime(start, '%Y-%m-%d')).days > 365:
        raise Exception('Start date is over a year old. CloudTracker does not create or use partitions over a year old.')
    month_restrictions = set()
    start = start.split('-')
    end = end.split('-')
    if start[0] == end[0]:
        for month in range(int(start[1]), int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
    else:
        for month in range(int(start[1]), 12 + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
        for year in range(int(start[0]), int(end[0])):
            for month in (1, 12 + 1):
                month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month))
        for month in range(1, int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month))
    self.search_filter = '((' + ' or '.join(month_restrictions) + ') and errorcode IS NULL)'
    self.table_name = 'cloudtrail_logs_{}'.format(account['id'])
    sts = boto3.client('sts')
    identity = sts.get_caller_identity()
    logging.info('Using AWS identity: {}'.format(identity['Arn']))
    current_account_id = identity['Account']
    region = boto3.session.Session().region_name
    if 'output_s3_bucket' in config:
        self.output_bucket = config['output_s3_bucket']
    else:
        self.output_bucket = 's3://aws-athena-query-results-{}-{}'.format(current_account_id, region)
    logging.info('Using output bucket: {}'.format(self.output_bucket))
    if 'workgroup' in config:
        self.workgroup = config['workgroup']
    logging.info('Using workgroup: {}'.format(self.workgroup))
    if not config.get('org_id'):
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], account_id=account['id'])
    else:
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], org_id=config['org_id'], account_id=account['id'])
    logging.info('Account cloudtrail log path: {}'.format(cloudtrail_log_path))
    self.athena = boto3.client('athena')
    self.s3 = boto3.client('s3')
    if args.skip_setup:
        logging.info('Skipping initial table creation')
        return
    resp = self.s3.list_objects_v2(Bucket=config['s3_bucket'], Prefix=config['path'], MaxKeys=1)
    if 'Contents' not in resp or len(resp['Contents']) == 0:
        exit('ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    self.query_athena('CREATE DATABASE IF NOT EXISTS {db} {comment}'.format(db=self.database, comment=""COMMENT 'Created by CloudTracker'""), context=None)
    query = ""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (\n            `eventversion` string COMMENT 'from deserializer', \n            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', \n            `eventtime` string COMMENT 'from deserializer', \n            `eventsource` string COMMENT 'from deserializer', \n            `eventname` string COMMENT 'from deserializer', \n            `awsregion` string COMMENT 'from deserializer', \n            `sourceipaddress` string COMMENT 'from deserializer', \n            `useragent` string COMMENT 'from deserializer', \n            `errorcode` string COMMENT 'from deserializer', \n            `errormessage` string COMMENT 'from deserializer', \n            `requestparameters` string COMMENT 'from deserializer', \n            `responseelements` string COMMENT 'from deserializer', \n            `additionaleventdata` string COMMENT 'from deserializer', \n            `requestid` string COMMENT 'from deserializer', \n            `eventid` string COMMENT 'from deserializer', \n            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', \n            `eventtype` string COMMENT 'from deserializer', \n            `apiversion` string COMMENT 'from deserializer', \n            `readonly` string COMMENT 'from deserializer', \n            `recipientaccountid` string COMMENT 'from deserializer', \n            `serviceeventdetails` string COMMENT 'from deserializer', \n            `sharedeventid` string COMMENT 'from deserializer', \n            `vpcendpointid` string COMMENT 'from deserializer')\n            PARTITIONED BY (region string, year string, month string)\n            ROW FORMAT SERDE \n            'com.amazon.emr.hive.serde.CloudTrailSerde' \n            STORED AS INPUTFORMAT \n            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' \n            OUTPUTFORMAT \n            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n            LOCATION '{cloudtrail_log_path}'"".format(table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path)
    self.query_athena(query)
    logging.info('Checking if all partitions for the past {} months exist'.format(NUM_MONTHS_FOR_PARTITIONS))
    query = 'SHOW PARTITIONS {table_name}'.format(table_name=self.table_name)
    partition_list = self.query_athena(query, skip_header=False)
    partition_set = set()
    for partition in partition_list:
        partition_set.add(partition[0])
    regions = boto3.session.Session().get_available_regions('ec2')
    queries_to_make = set()
    for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
        date_of_interest = datetime.datetime.now() - relativedelta(months=num_months_ago)
        year = date_of_interest.year
        month = '{:0>2}'.format(date_of_interest.month)
        query = ''
        for region in regions:
            if 'region={region}/year={year}/month={month}'.format(region=region, year=year, month=month) in partition_set:
                continue
            query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(region=region, year=year, month=month, cloudtrail_log_path=cloudtrail_log_path)
        if query != '':
            queries_to_make.add('ALTER TABLE {table_name} ADD '.format(table_name=self.table_name) + query)
    query_count = len(queries_to_make)
    for query in queries_to_make:
        logging.info('Partition groups remaining to create: {}'.format(query_count))
        self.query_athena(query)
        query_count -= 1","for month in range(int(start[1]), 12 + 1):
    month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))","month_restrictions = {""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month) for month in range(int(start[1]), 12 + 1)}",Cannot refactor,2,
cloudtracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloudtracker/cloudtracker/datasources/athena.py,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,__init__$164,"def __init__(self, config, account, start, end, args):
    logging.getLogger('botocore').setLevel(logging.WARN)
    logging.info('Source of CloudTrail logs: s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    if (datetime.datetime.now() - datetime.datetime.strptime(start, '%Y-%m-%d')).days > 365:
        raise Exception('Start date is over a year old. CloudTracker does not create or use partitions over a year old.')
    month_restrictions = set()
    start = start.split('-')
    end = end.split('-')
    if start[0] == end[0]:
        for month in range(int(start[1]), int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
    else:
        for month in range(int(start[1]), 12 + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
        for year in range(int(start[0]), int(end[0])):
            for month in (1, 12 + 1):
                month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month))
        for month in range(1, int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month))
    self.search_filter = '((' + ' or '.join(month_restrictions) + ') and errorcode IS NULL)'
    self.table_name = 'cloudtrail_logs_{}'.format(account['id'])
    sts = boto3.client('sts')
    identity = sts.get_caller_identity()
    logging.info('Using AWS identity: {}'.format(identity['Arn']))
    current_account_id = identity['Account']
    region = boto3.session.Session().region_name
    if 'output_s3_bucket' in config:
        self.output_bucket = config['output_s3_bucket']
    else:
        self.output_bucket = 's3://aws-athena-query-results-{}-{}'.format(current_account_id, region)
    logging.info('Using output bucket: {}'.format(self.output_bucket))
    if 'workgroup' in config:
        self.workgroup = config['workgroup']
    logging.info('Using workgroup: {}'.format(self.workgroup))
    if not config.get('org_id'):
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], account_id=account['id'])
    else:
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], org_id=config['org_id'], account_id=account['id'])
    logging.info('Account cloudtrail log path: {}'.format(cloudtrail_log_path))
    self.athena = boto3.client('athena')
    self.s3 = boto3.client('s3')
    if args.skip_setup:
        logging.info('Skipping initial table creation')
        return
    resp = self.s3.list_objects_v2(Bucket=config['s3_bucket'], Prefix=config['path'], MaxKeys=1)
    if 'Contents' not in resp or len(resp['Contents']) == 0:
        exit('ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    self.query_athena('CREATE DATABASE IF NOT EXISTS {db} {comment}'.format(db=self.database, comment=""COMMENT 'Created by CloudTracker'""), context=None)
    query = ""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (\n            `eventversion` string COMMENT 'from deserializer', \n            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', \n            `eventtime` string COMMENT 'from deserializer', \n            `eventsource` string COMMENT 'from deserializer', \n            `eventname` string COMMENT 'from deserializer', \n            `awsregion` string COMMENT 'from deserializer', \n            `sourceipaddress` string COMMENT 'from deserializer', \n            `useragent` string COMMENT 'from deserializer', \n            `errorcode` string COMMENT 'from deserializer', \n            `errormessage` string COMMENT 'from deserializer', \n            `requestparameters` string COMMENT 'from deserializer', \n            `responseelements` string COMMENT 'from deserializer', \n            `additionaleventdata` string COMMENT 'from deserializer', \n            `requestid` string COMMENT 'from deserializer', \n            `eventid` string COMMENT 'from deserializer', \n            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', \n            `eventtype` string COMMENT 'from deserializer', \n            `apiversion` string COMMENT 'from deserializer', \n            `readonly` string COMMENT 'from deserializer', \n            `recipientaccountid` string COMMENT 'from deserializer', \n            `serviceeventdetails` string COMMENT 'from deserializer', \n            `sharedeventid` string COMMENT 'from deserializer', \n            `vpcendpointid` string COMMENT 'from deserializer')\n            PARTITIONED BY (region string, year string, month string)\n            ROW FORMAT SERDE \n            'com.amazon.emr.hive.serde.CloudTrailSerde' \n            STORED AS INPUTFORMAT \n            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' \n            OUTPUTFORMAT \n            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n            LOCATION '{cloudtrail_log_path}'"".format(table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path)
    self.query_athena(query)
    logging.info('Checking if all partitions for the past {} months exist'.format(NUM_MONTHS_FOR_PARTITIONS))
    query = 'SHOW PARTITIONS {table_name}'.format(table_name=self.table_name)
    partition_list = self.query_athena(query, skip_header=False)
    partition_set = set()
    for partition in partition_list:
        partition_set.add(partition[0])
    regions = boto3.session.Session().get_available_regions('ec2')
    queries_to_make = set()
    for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
        date_of_interest = datetime.datetime.now() - relativedelta(months=num_months_ago)
        year = date_of_interest.year
        month = '{:0>2}'.format(date_of_interest.month)
        query = ''
        for region in regions:
            if 'region={region}/year={year}/month={month}'.format(region=region, year=year, month=month) in partition_set:
                continue
            query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(region=region, year=year, month=month, cloudtrail_log_path=cloudtrail_log_path)
        if query != '':
            queries_to_make.add('ALTER TABLE {table_name} ADD '.format(table_name=self.table_name) + query)
    query_count = len(queries_to_make)
    for query in queries_to_make:
        logging.info('Partition groups remaining to create: {}'.format(query_count))
        self.query_athena(query)
        query_count -= 1","for year in range(int(start[0]), int(end[0])):
    for month in (1, 12 + 1):
        month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month))","month_restrictions = {""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month) for year in range(int(start[0]), int(end[0])) for month in range(1, 12 + 1)}",Cannot refactor,2,
cloudtracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloudtracker/cloudtracker/datasources/athena.py,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,__init__$164,"def __init__(self, config, account, start, end, args):
    logging.getLogger('botocore').setLevel(logging.WARN)
    logging.info('Source of CloudTrail logs: s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    if (datetime.datetime.now() - datetime.datetime.strptime(start, '%Y-%m-%d')).days > 365:
        raise Exception('Start date is over a year old. CloudTracker does not create or use partitions over a year old.')
    month_restrictions = set()
    start = start.split('-')
    end = end.split('-')
    if start[0] == end[0]:
        for month in range(int(start[1]), int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
    else:
        for month in range(int(start[1]), 12 + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
        for year in range(int(start[0]), int(end[0])):
            for month in (1, 12 + 1):
                month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month))
        for month in range(1, int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month))
    self.search_filter = '((' + ' or '.join(month_restrictions) + ') and errorcode IS NULL)'
    self.table_name = 'cloudtrail_logs_{}'.format(account['id'])
    sts = boto3.client('sts')
    identity = sts.get_caller_identity()
    logging.info('Using AWS identity: {}'.format(identity['Arn']))
    current_account_id = identity['Account']
    region = boto3.session.Session().region_name
    if 'output_s3_bucket' in config:
        self.output_bucket = config['output_s3_bucket']
    else:
        self.output_bucket = 's3://aws-athena-query-results-{}-{}'.format(current_account_id, region)
    logging.info('Using output bucket: {}'.format(self.output_bucket))
    if 'workgroup' in config:
        self.workgroup = config['workgroup']
    logging.info('Using workgroup: {}'.format(self.workgroup))
    if not config.get('org_id'):
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], account_id=account['id'])
    else:
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], org_id=config['org_id'], account_id=account['id'])
    logging.info('Account cloudtrail log path: {}'.format(cloudtrail_log_path))
    self.athena = boto3.client('athena')
    self.s3 = boto3.client('s3')
    if args.skip_setup:
        logging.info('Skipping initial table creation')
        return
    resp = self.s3.list_objects_v2(Bucket=config['s3_bucket'], Prefix=config['path'], MaxKeys=1)
    if 'Contents' not in resp or len(resp['Contents']) == 0:
        exit('ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    self.query_athena('CREATE DATABASE IF NOT EXISTS {db} {comment}'.format(db=self.database, comment=""COMMENT 'Created by CloudTracker'""), context=None)
    query = ""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (\n            `eventversion` string COMMENT 'from deserializer', \n            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', \n            `eventtime` string COMMENT 'from deserializer', \n            `eventsource` string COMMENT 'from deserializer', \n            `eventname` string COMMENT 'from deserializer', \n            `awsregion` string COMMENT 'from deserializer', \n            `sourceipaddress` string COMMENT 'from deserializer', \n            `useragent` string COMMENT 'from deserializer', \n            `errorcode` string COMMENT 'from deserializer', \n            `errormessage` string COMMENT 'from deserializer', \n            `requestparameters` string COMMENT 'from deserializer', \n            `responseelements` string COMMENT 'from deserializer', \n            `additionaleventdata` string COMMENT 'from deserializer', \n            `requestid` string COMMENT 'from deserializer', \n            `eventid` string COMMENT 'from deserializer', \n            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', \n            `eventtype` string COMMENT 'from deserializer', \n            `apiversion` string COMMENT 'from deserializer', \n            `readonly` string COMMENT 'from deserializer', \n            `recipientaccountid` string COMMENT 'from deserializer', \n            `serviceeventdetails` string COMMENT 'from deserializer', \n            `sharedeventid` string COMMENT 'from deserializer', \n            `vpcendpointid` string COMMENT 'from deserializer')\n            PARTITIONED BY (region string, year string, month string)\n            ROW FORMAT SERDE \n            'com.amazon.emr.hive.serde.CloudTrailSerde' \n            STORED AS INPUTFORMAT \n            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' \n            OUTPUTFORMAT \n            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n            LOCATION '{cloudtrail_log_path}'"".format(table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path)
    self.query_athena(query)
    logging.info('Checking if all partitions for the past {} months exist'.format(NUM_MONTHS_FOR_PARTITIONS))
    query = 'SHOW PARTITIONS {table_name}'.format(table_name=self.table_name)
    partition_list = self.query_athena(query, skip_header=False)
    partition_set = set()
    for partition in partition_list:
        partition_set.add(partition[0])
    regions = boto3.session.Session().get_available_regions('ec2')
    queries_to_make = set()
    for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
        date_of_interest = datetime.datetime.now() - relativedelta(months=num_months_ago)
        year = date_of_interest.year
        month = '{:0>2}'.format(date_of_interest.month)
        query = ''
        for region in regions:
            if 'region={region}/year={year}/month={month}'.format(region=region, year=year, month=month) in partition_set:
                continue
            query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(region=region, year=year, month=month, cloudtrail_log_path=cloudtrail_log_path)
        if query != '':
            queries_to_make.add('ALTER TABLE {table_name} ADD '.format(table_name=self.table_name) + query)
    query_count = len(queries_to_make)
    for query in queries_to_make:
        logging.info('Partition groups remaining to create: {}'.format(query_count))
        self.query_athena(query)
        query_count -= 1","for month in range(1, int(end[1]) + 1):
    month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month))","month_restrictions = {""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month) for month in range(1, int(end[1]) + 1)}",Cannot refactor,2,
text_renderer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/text_renderer/libs/font_utils.py,https://github.com/Sanster/text_renderer/tree/master/libs/font_utils.py,,check_font_chars$62,"def check_font_chars(ttf, charset):
    """"""
    Get font supported chars and unsupported chars
    :param ttf: TTFont ojbect
    :param charset: chars
    :return: unsupported_chars, supported_chars
    """"""
    chars_int = set()
    for table in ttf['cmap'].tables:
        for (k, v) in table.cmap.items():
            chars_int.add(k)
    unsupported_chars = []
    supported_chars = []
    for c in charset:
        if ord(c) not in chars_int:
            unsupported_chars.append(c)
        else:
            supported_chars.append(c)
    ttf.close()
    return (unsupported_chars, supported_chars)","for table in ttf['cmap'].tables:
    for (k, v) in table.cmap.items():
        chars_int.add(k)","chars_int = {k for table in ttf['cmap'].tables for (k, v) in table.cmap.items()}","[""chars_int = {k for table in ttf['cmap'].tables for (k, v) in table.cmap.items()}""]",1,
MozDef,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MozDef/alerts/proxy_drop_non_standard_port.py,https://github.com/mozilla/MozDef/tree/master/alerts/proxy_drop_non_standard_port.py,AlertProxyDropNonStandardPort,onAggregation$38,"def onAggregation(self, aggreg):
    category = 'squid'
    tags = ['squid', 'proxy']
    severity = 'WARNING'
    destinations = set()
    for event in aggreg['allevents']:
        destinations.add(event['_source']['details']['destination'])
    summary = 'Suspicious Proxy DROP event(s) detected from {0} to the following non-std port destination(s): {1}'.format(aggreg['value'], ','.join(sorted(destinations)))
    return self.createAlertDict(summary, category, tags, aggreg['events'], severity)","for event in aggreg['allevents']:
    destinations.add(event['_source']['details']['destination'])",destinations = {event['_source']['details']['destination'] for event in aggreg['allevents']},["destinations = {event['_source']['details']['destination'] for event in aggreg['allevents']}"],1,
redis-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/redis-py/redis/utils.py,https://github.com/redis/redis-py/tree/master/redis/utils.py,,merge_result$68,"def merge_result(command, res):
    """"""
    Merge all items in `res` into a list.

    This command is used when sending a command to multiple nodes
    and the result from each node should be merged into a single list.

    res : 'dict'
    """"""
    result = set()
    for v in res.values():
        for value in v:
            result.add(value)
    return list(result)","for v in res.values():
    for value in v:
        result.add(value)",result = {value for v in res.values() for value in v},['result = {value for v in res.values() for value in v}'],1,
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/lib/support.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/support.py,,get_public_keys$917,"def get_public_keys():
    """"""
    get a list of all public keys, which could be used to verify
    a linOTP license

    :return: list with unique public keys
    """"""
    pubKeys = {}
    pubKeys['linotp'] = PUB_KEY_LINOTP
    key_files = set()
    for key_dir in PUB_KEY_DIRS:
        if os.path.isdir(key_dir):
            for key_file in os.listdir(key_dir):
                for extension in PUB_KEY_EXTS:
                    if key_file.endswith(extension):
                        key_files.add(os.path.join(key_dir, key_file))
    for key_file in key_files:
        try:
            key_text = readPublicKey(key_file)
            if key_text and key_text not in list(pubKeys.values()):
                idx = os.path.split(key_file)[-1]
                if idx[-4:] == '.pem':
                    (idx, _sep, _rest) = idx.rpartition('.pem')
                if idx[-4:] == '_pub':
                    (idx, _sep, _rest) = idx.rpartition('_pub')
                pubKeys[idx] = key_text
            else:
                log.error('Licence: Public key file is not valid (%r)', key_file)
        except Exception as exx:
            log.error('Licence: error during reading public key file (%s): %r', key_file, exx)
    return pubKeys","for key_dir in PUB_KEY_DIRS:
    if os.path.isdir(key_dir):
        for key_file in os.listdir(key_dir):
            for extension in PUB_KEY_EXTS:
                if key_file.endswith(extension):
                    key_files.add(os.path.join(key_dir, key_file))","key_files = {os.path.join(key_dir, key_file) for key_dir in PUB_KEY_DIRS if os.path.isdir(key_dir) for key_file in os.listdir(key_dir) for extension in PUB_KEY_EXTS if key_file.endswith(extension)}","['key_files = {os.path.join(key_dir, key_file) for key_dir in PUB_KEY_DIRS if os.path.isdir(key_dir) for key_file in os.listdir(key_dir) for extension in PUB_KEY_EXTS if key_file.endswith(extension)}']",1,
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/utils/graph.py,https://github.com/angr/angr/tree/master/angr/utils/graph.py,,compute_dominance_frontier$128,"def compute_dominance_frontier(graph, domtree):
    """"""
    Compute a dominance frontier based on the given post-dominator tree.

    This implementation is based on figure 2 of paper An Efficient Method of Computing Static Single Assignment
    Form by Ron Cytron, etc.

    :param graph:   The graph where we want to compute the dominance frontier.
    :param domtree: The dominator tree
    :returns:       A dict of dominance frontier
    """"""
    df = {}
    for x in networkx.dfs_postorder_nodes(domtree):
        if x not in graph:
            continue
        df[x] = set()
        for y in graph.successors(x):
            if x not in domtree.predecessors(y):
                df[x].add(y)
        if x is None:
            continue
        for z in domtree.successors(x):
            if z is x:
                continue
            if z not in df:
                continue
            for y in df[z]:
                if x not in list(domtree.predecessors(y)):
                    df[x].add(y)
    return df","for x in networkx.dfs_postorder_nodes(domtree):
    if x not in graph:
        continue
    df[x] = set()
    for y in graph.successors(x):
        if x not in domtree.predecessors(y):
            df[x].add(y)
    if x is None:
        continue
    for z in domtree.successors(x):
        if z is x:
            continue
        if z not in df:
            continue
        for y in df[z]:
            if x not in list(domtree.predecessors(y)):
                df[x].add(y)",df[x].update({y for y in df[z] if x in domtree.predecessors(y)}),Cannot refactor,2,
codechecker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/codechecker/web/tests/functional/diff_local_remote/test_diff_local_remote.py,https://github.com/Ericsson/codechecker/tree/master/web/tests/functional/diff_local_remote/test_diff_local_remote.py,LocalRemote,test_local_compare_res_html_output_unresolved$216,"def test_local_compare_res_html_output_unresolved(self):
    """"""Check that html files will be generated by using diff command.""""""
    html_reports = os.path.join(self._local_reports, 'html_reports')
    get_diff_results([self._run_names[0]], [self._local_reports], '--unresolved', 'html', ['--url', self._url, '-e', html_reports, '--verbose', 'debug'])
    checked_files = set()
    for res in self.get_local_remote_diff(None, 'json'):
        checked_files.add(os.path.basename(res['file']['path']))
    html_index = os.path.join(html_reports, 'index.html')
    self.assertTrue(os.path.exists(html_index))
    html_statistics = os.path.join(html_reports, 'statistics.html')
    self.assertTrue(os.path.exists(html_statistics))
    for html_file_names in os.listdir(html_reports):
        suffix = html_file_names.rfind('_')
        file_name = html_file_names[:suffix] if suffix != -1 else html_file_names
        if file_name in ['index.html', 'statistics.html']:
            continue
        self.assertIn(file_name, checked_files)","for res in self.get_local_remote_diff(None, 'json'):
    checked_files.add(os.path.basename(res['file']['path']))","checked_files = {os.path.basename(res['file']['path']) for res in self.get_local_remote_diff(None, 'json')}","[""checked_files = {os.path.basename(res['file']['path']) for res in self.get_local_remote_diff(None, 'json')}""]",1,
toga,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/toga/examples/textinput/textinput/app.py,https://github.com/beeware/toga/tree/master/examples/textinput/textinput/app.py,TextInputApp,get_password_content_label$129,"def get_password_content_label(self, content):
    if content.strip() == '':
        return EMPTY_PASSWORD
    contains = set()
    for letter in content:
        if letter in ascii_uppercase:
            contains.add('uppercase letters')
        elif letter in ascii_lowercase:
            contains.add('lowercase letters')
        elif letter in digits:
            contains.add('digits')
        else:
            contains.add('special characters')
    return 'Password contains: {}'.format(', '.join(contains))","for letter in content:
    if letter in ascii_uppercase:
        contains.add('uppercase letters')
    elif letter in ascii_lowercase:
        contains.add('lowercase letters')
    elif letter in digits:
        contains.add('digits')
    else:
        contains.add('special characters')",contains = {'uppercase letters' for letter in content if letter in ascii_uppercase} | {'lowercase letters' for letter in content if letter in ascii_lowercase} | {'digits' for letter in content if letter in digits} | {'special characters' for letter in content if letter not in ascii_uppercase and letter not in ascii_lowercase and letter not in digits},["contains = {'uppercase letters' if letter in ascii_uppercase else 'lowercase letters' if letter in ascii_lowercase else 'digits' if letter in digits else 'special characters' for letter in content}"],0,
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/sanity/code-smell/update-bundled.py,https://github.com/ansible/ansible/tree/master/test/sanity/code-smell/update-bundled.py,,get_bundled_libs$41,"def get_bundled_libs(paths):
    """"""
    Return the set of known bundled libraries

    :arg paths: The paths which the test has been instructed to check
    :returns: The list of all files which we know to contain bundled libraries.  If a bundled
        library consists of multiple files, this should be the file which has metadata included.
    """"""
    bundled_libs = set()
    for filename in fnmatch.filter(paths, 'lib/ansible/compat/*/__init__.py'):
        bundled_libs.add(filename)
    bundled_libs.add('lib/ansible/module_utils/compat/selectors.py')
    bundled_libs.add('lib/ansible/module_utils/distro/__init__.py')
    bundled_libs.add('lib/ansible/module_utils/six/__init__.py')
    bundled_libs.add('lib/ansible/module_utils/urls.py')
    return bundled_libs","for filename in fnmatch.filter(paths, 'lib/ansible/compat/*/__init__.py'):
    bundled_libs.add(filename)","bundled_libs = {filename for filename in fnmatch.filter(paths, 'lib/ansible/compat/*/__init__.py')}","[""bundled_libs = {filename for filename in fnmatch.filter(paths, 'lib/ansible/compat/*/__init__.py')}""]",1,
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/google/cloud/forseti/scanner/scanners/iap_scanner.py,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/scanner/scanners/iap_scanner.py,_RunData,make_iap_resource$320,"def make_iap_resource(self, backend_service, project_full_name):
    """"""Get an IapResource for a service.

        Args:
            backend_service (BackendService): service to create a resource for
            project_full_name (str): The full path to the parent project
                including all ancestors.

        Returns:
            IapResource: the corresponding resource
        """"""
    alternate_services = set()
    direct_access_sources = set()
    for backend in backend_service.backends:
        instance_group = self.find_instance_group_by_url(backend.get('group'))
        if not instance_group:
            continue
        network_port = self.instance_group_network_port(backend_service, instance_group)
        if not network_port:
            continue
        direct_access_sources.update(self.firewall_allowed_sources(network_port, None))
        tags = self.tags_for_instance_group(instance_group)
        for tag in tags:
            direct_access_sources.update(self.firewall_allowed_sources(network_port, tag))
    direct_access_sources.discard('130.211.0.0/22')
    direct_access_sources.discard('35.191.0.0/16')
    for backend_service2 in self.backend_services:
        if self.is_alternate_service(backend_service, backend_service2):
            alternate_services.add(backend_service2.key)
    return IapResource(project_full_name=project_full_name, backend_service=backend_service, alternate_services=alternate_services, direct_access_sources=sorted(direct_access_sources), iap_enabled=backend_service.iap.get('enabled', False) if backend_service.iap else False)","for backend_service2 in self.backend_services:
    if self.is_alternate_service(backend_service, backend_service2):
        alternate_services.add(backend_service2.key)","alternate_services = {backend_service2.key for backend_service2 in self.backend_services if self.is_alternate_service(backend_service, backend_service2)}","['alternate_services = {backend_service2.key for backend_service2 in self.backend_services if self.is_alternate_service(backend_service, backend_service2)}']",1,
core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/homeassistant/auth/mfa_modules/notify.py,https://github.com/home-assistant/core/tree/master/homeassistant/auth/mfa_modules/notify.py,NotifyAuthModule,aync_get_available_notify_services$150,"def aync_get_available_notify_services(self) -> list[str]:
    """"""Return list of notify services.""""""
    unordered_services = set()
    for service in self.hass.services.async_services().get('notify', {}):
        if service not in self._exclude:
            unordered_services.add(service)
    if self._include:
        unordered_services &= set(self._include)
    return sorted(unordered_services)","for service in self.hass.services.async_services().get('notify', {}):
    if service not in self._exclude:
        unordered_services.add(service)","unordered_services = {service for service in self.hass.services.async_services().get('notify', {}) if service not in self._exclude}","[""unordered_services = {service for service in self.hass.services.async_services().get('notify', {}) if service not in self._exclude}""]",1,
pychess,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/testing/sittuyin.py,https://github.com/pychess/pychess/tree/master/testing/sittuyin.py,SittuyinTestCase,test_geCaptures$157,"def test_geCaptures(self):
    """"""Testing validate move in Sittuyin variant""""""
    board = SittuyinBoard(setup=FEN4)
    print(board)
    moves = set()
    for move in genCaptures(board.board):
        moves.add(toAN(board.board, move))
    self.assertEqual(moves, set(('d5c6',)))","for move in genCaptures(board.board):
    moves.add(toAN(board.board, move))","moves = {toAN(board.board, move) for move in genCaptures(board.board)}","['moves = {toAN(board.board, move) for move in genCaptures(board.board)}']",1,
lxmert,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lxmert/data/mscoco_imgfeat/extract_coco_image.py,https://github.com/airsplay/lxmert/tree/master/data/mscoco_imgfeat/extract_coco_image.py,,generate_tsv$54,"def generate_tsv(prototxt, weights, image_ids, outfile):
    wanted_ids = set([image_id[1] for image_id in image_ids])
    found_ids = set()
    if os.path.exists(outfile):
        with open(outfile, 'r') as tsvfile:
            reader = csv.DictReader(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
            for item in reader:
                found_ids.add(item['img_id'])
    missing = wanted_ids - found_ids
    if len(missing) == 0:
        print('already completed {:d}'.format(len(image_ids)))
    else:
        print('missing {:d}/{:d}'.format(len(missing), len(image_ids)))
    if len(missing) > 0:
        caffe.set_mode_gpu()
        caffe.set_device(0)
        net = caffe.Net(prototxt, caffe.TEST, weights=weights)
        with open(outfile, 'ab') as tsvfile:
            writer = csv.DictWriter(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
            for (im_file, image_id) in tqdm(image_ids):
                if image_id in missing:
                    try:
                        writer.writerow(get_detections_from_im(net, im_file, image_id))
                    except Exception as e:
                        print(e)","for item in reader:
    found_ids.add(item['img_id'])",found_ids = {item['img_id'] for item in reader},["found_ids = {item['img_id'] for item in reader}"],1,
numpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/distutils/ccompiler_opt.py,https://github.com/numpy/numpy/tree/master/numpy/distutils/ccompiler_opt.py,_Feature,feature_names$1261,"def feature_names(self, names=None, force_flags=None, macros=[]):
    """"""
        Returns a set of CPU feature names that supported by platform and the **C** compiler.

        Parameters
        ----------
        names : sequence or None, optional
            Specify certain CPU features to test it against the **C** compiler.
            if None(default), it will test all current supported features.
            **Note**: feature names must be in upper-case.

        force_flags : list or None, optional
            If None(default), default compiler flags for every CPU feature will
            be used during the test.

        macros : list of tuples, optional
            A list of C macro definitions.
        """"""
    assert names is None or (not isinstance(names, str) and hasattr(names, '__iter__'))
    assert force_flags is None or isinstance(force_flags, list)
    if names is None:
        names = self.feature_supported.keys()
    supported_names = set()
    for f in names:
        if self.feature_is_supported(f, force_flags=force_flags, macros=macros):
            supported_names.add(f)
    return supported_names","for f in names:
    if self.feature_is_supported(f, force_flags=force_flags, macros=macros):
        supported_names.add(f)","supported_names = {f for f in names if self.feature_is_supported(f, force_flags=force_flags, macros=macros)}","['supported_names = {f for f in names if self.feature_is_supported(f, force_flags=force_flags, macros=macros)}']",1,
django-autofixture,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-autofixture/autofixture_tests/tests/test_base.py,https://github.com/gregmuellegger/django-autofixture/tree/master/autofixture_tests/tests/test_base.py,TestRelations,test_generate_fk_for_o2o$192,"def test_generate_fk_for_o2o(self):
    filler = AutoFixture(O2OModel, generate_fk=True)
    all_o2o = set()
    for obj in filler.create(10):
        all_o2o.add(obj.o2o)
    self.assertEqual(set(SimpleModel.objects.all()), all_o2o)","for obj in filler.create(10):
    all_o2o.add(obj.o2o)",all_o2o = {obj.o2o for obj in filler.create(10)},['all_o2o = {obj.o2o for obj in filler.create(10)}'],1,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/win_service.py,https://github.com/saltstack/salt/tree/master/salt/modules/win_service.py,,get_all$225,"def get_all():
    """"""
    Return all installed services

    Returns:
        list: Returns a list of all services on the system.

    CLI Example:

    .. code-block:: bash

        salt '*' service.get_all
    """"""
    services = _get_services()
    ret = set()
    for service in services:
        ret.add(service['ServiceName'])
    return sorted(ret)","for service in services:
    ret.add(service['ServiceName'])",ret = {service['ServiceName'] for service in services},["ret = {service['ServiceName'] for service in services}"],1,
TexTools-Blender,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TexTools-Blender/utilities_uv.py,https://github.com/SavMartin/TexTools-Blender/tree/master//utilities_uv.py,,get_selected_uvs$290,"def get_selected_uvs(bm, uv_layers):
    """"""Returns selected mesh vertices of selected UV's""""""
    uvs = set()
    for face in bm.faces:
        if face.select:
            for loop in face.loops:
                if loop[uv_layers].select:
                    uvs.add(loop[uv_layers])
    return uvs","for face in bm.faces:
    if face.select:
        for loop in face.loops:
            if loop[uv_layers].select:
                uvs.add(loop[uv_layers])",uvs = {loop[uv_layers] for face in bm.faces if face.select for loop in face.loops if loop[uv_layers].select},['uvs = {loop[uv_layers] for face in bm.faces if face.select for loop in face.loops if loop[uv_layers].select}'],1,
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/cfg/cfg_fast.py,https://github.com/angr/angr/tree/master/angr/analyses/cfg/cfg_fast.py,CFGFast,_get_jumpout_targets$3137,"def _get_jumpout_targets(self, func):
    jumpout_targets = set()
    callgraph_outedges = self.functions.callgraph.out_edges(func.addr, data=True)
    for (_, dst, data) in callgraph_outedges:
        if data.get('type', None) == 'transition':
            jumpout_targets.add(dst)
    return jumpout_targets","for (_, dst, data) in callgraph_outedges:
    if data.get('type', None) == 'transition':
        jumpout_targets.add(dst)","jumpout_targets = {dst for (_, dst, data) in callgraph_outedges if data.get('type', None) == 'transition'}","[""jumpout_targets = {dst for (_, dst, data) in callgraph_outedges if data.get('type', None) == 'transition'}""]",1,
attn2d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/attn2d/fairseq/file_utils.py,https://github.com/elbayadm/attn2d/tree/master/fairseq/file_utils.py,,read_set_from_file$316,"def read_set_from_file(filename):
    """"""
    Extract a de-duped collection (set) of text from a file.
    Expected file format is one item per line.
    """"""
    collection = set()
    with open(filename, 'r', encoding='utf-8') as file_:
        for line in file_:
            collection.add(line.rstrip())
    return collection","for line in file_:
    collection.add(line.rstrip())",collection = {line.rstrip() for line in file_},['collection = {line.rstrip() for line in file_}'],1,
pretix,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/base/models/vouchers.py,https://github.com/pretix/pretix/tree/master/src/pretix/base/models/vouchers.py,,generate_codes$77,"def generate_codes(organizer, num=1, prefix=None):
    codes = set()
    batch_size = 500
    if 'postgres' in settings.DATABASES['default']['ENGINE']:
        batch_size = 5000
    ""\n    We're trying to check if any of the requested codes already exists in the database. Generally, this is a\n\n        SELECT code FROM voucher WHERE code IN (…)\n\n    query. However, it turns out that this query get's rather slow if an organizer has lots of vouchers, even\n    with a organizer with just over 50_000 vouchers, we've seen that creating 20_000 new voucher codes took\n    just over 30 seconds. There's another way of doing this query on PostgreSQL, which is joining with a\n    temporary table\n\n        SELECT code FROM voucher INNER JOIN (VALUES …) vals(v) ON (code = v)\n\n    This is significantly faster, inserting 20_000 vouchers now takes 2-3s instead of 31s on the same dataset.\n    It's still slow, and removing the JOIN to the event table doesn't significantly speed it up. We might need\n    an entirely different approach at some point.\n    ""
    while len(codes) < num:
        new_codes = set()
        for i in range(min(num - len(codes), batch_size)):
            new_codes.add(_generate_random_code(prefix=prefix))
        if 'postgres' in settings.DATABASES['default']['ENGINE']:
            with connection.cursor() as cursor:
                args = list(new_codes) + [organizer.pk]
                tmptable = 'VALUES ' + ', '.join(['(%s)'] * len(new_codes))
                cursor.execute(f'SELECT code FROM ""{Voucher._meta.db_table}"" INNER JOIN ({tmptable}) vals(v) ON (""{Voucher._meta.db_table}"".""code"" = ""v"")INNER JOIN ""{Event._meta.db_table}"" ON (""{Voucher._meta.db_table}"".""event_id"" = ""{Event._meta.db_table}"".""id"") WHERE ""{Event._meta.db_table}"".""organizer_id"" = %s', args)
                for row in cursor.fetchall():
                    new_codes.remove(row[0])
        else:
            new_codes -= set([v['code'] for v in Voucher.objects.filter(code__in=new_codes).values('code')])
        codes |= new_codes
    return list(codes)","for i in range(min(num - len(codes), batch_size)):
    new_codes.add(_generate_random_code(prefix=prefix))","new_codes = {_generate_random_code(prefix=prefix) for i in range(min(num - len(codes), batch_size))}","['new_codes = {_generate_random_code(prefix=prefix) for i in range(min(num - len(codes), batch_size))}']",1,
pytorch-metric-learning,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch-metric-learning/tests/utils/test_loss_and_miner_utils.py,https://github.com/KevinMusgrave/pytorch-metric-learning/tree/master/tests/utils/test_loss_and_miner_utils.py,TestLossAndMinerUtils,test_convert_to_triplets$90,"def test_convert_to_triplets(self):
    a1 = torch.LongTensor([0, 1, 2, 3])
    p = torch.LongTensor([4, 4, 4, 4])
    a2 = torch.LongTensor([4, 5, 6, 7])
    n = torch.LongTensor([5, 5, 6, 6])
    triplets = lmu.convert_to_triplets((a1, p, a2, n), labels=torch.arange(7))
    self.assertTrue(all((len(x) == 0 for x in triplets)))
    a2 = torch.LongTensor([0, 4, 5, 6])
    triplets = lmu.convert_to_triplets((a1, p, a2, n), labels=torch.arange(7))
    self.assertTrue(triplets == (torch.tensor([0]), torch.tensor([4]), torch.tensor([5])))
    a1 = torch.LongTensor([0, 1, 0, 2])
    p = torch.LongTensor([5, 6, 7, 8])
    a2 = torch.LongTensor([0, 1, 2, 0])
    n = torch.LongTensor([9, 10, 11, 12])
    triplets = lmu.convert_to_triplets((a1, p, a2, n), labels=torch.arange(13))
    triplets = torch.stack(triplets, dim=1)
    found_set = set()
    for t in triplets:
        found_set.add(tuple(t.cpu().numpy()))
    correct_triplets = {(0, 5, 9), (0, 5, 12), (0, 7, 9), (0, 7, 12), (1, 6, 10), (2, 8, 11)}
    self.assertTrue(found_set == correct_triplets)","for t in triplets:
    found_set.add(tuple(t.cpu().numpy()))",found_set = {tuple(t.cpu().numpy()) for t in triplets},['found_set = {tuple(t.cpu().numpy()) for t in triplets}'],1,
sqlalchemy-utils,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sqlalchemy-utils/sqlalchemy_utils/functions/foreign_keys.py,https://github.com/kvesteri/sqlalchemy-utils/tree/master/sqlalchemy_utils/functions/foreign_keys.py,,get_referencing_foreign_keys$53,"def get_referencing_foreign_keys(mixed):
    """"""
    Returns referencing foreign keys for given Table object or declarative
    class.

    :param mixed:
        SA Table object or SA declarative class

    ::

        get_referencing_foreign_keys(User)  # set([ForeignKey('user.id')])

        get_referencing_foreign_keys(User.__table__)


    This function also understands inheritance. This means it returns
    all foreign keys that reference any table in the class inheritance tree.

    Let's say you have three classes which use joined table inheritance,
    namely TextItem, Article and BlogPost with Article and BlogPost inheriting
    TextItem.

    ::

        # This will check all foreign keys that reference either article table
        # or textitem table.
        get_referencing_foreign_keys(Article)

    .. seealso:: :func:`get_tables`
    """"""
    if isinstance(mixed, sa.Table):
        tables = [mixed]
    else:
        tables = get_tables(mixed)
    referencing_foreign_keys = set()
    for table in mixed.metadata.tables.values():
        if table not in tables:
            for constraint in table.constraints:
                if isinstance(constraint, sa.sql.schema.ForeignKeyConstraint):
                    for fk in constraint.elements:
                        if any((fk.references(t) for t in tables)):
                            referencing_foreign_keys.add(fk)
    return referencing_foreign_keys","for table in mixed.metadata.tables.values():
    if table not in tables:
        for constraint in table.constraints:
            if isinstance(constraint, sa.sql.schema.ForeignKeyConstraint):
                for fk in constraint.elements:
                    if any((fk.references(t) for t in tables)):
                        referencing_foreign_keys.add(fk)","referencing_foreign_keys = {fk for table in mixed.metadata.tables.values() if table not in tables for constraint in table.constraints if isinstance(constraint, sa.sql.schema.ForeignKeyConstraint) for fk in constraint.elements if any((fk.references(t) for t in tables))}","['referencing_foreign_keys = {fk for table in mixed.metadata.tables.values() if table not in tables for constraint in table.constraints if isinstance(constraint, sa.sql.schema.ForeignKeyConstraint) for fk in constraint.elements if any((fk.references(t) for t in tables))}']",1,
aswan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aswan/www/rule/forms.py,https://github.com/momosecurity/aswan/tree/master/www/rule/forms.py,RulesForm,_check_names$78,"def _check_names(self, names, choices, sep=None):
    valid_names = set()
    for (english, chinese) in choices:
        if english:
            valid_names.add(english)
    if sep:
        all_names = []
        for name in names:
            for e in name.split(sep):
                all_names.append(e)
        names = all_names
    return all([name in valid_names for name in names])","for (english, chinese) in choices:
    if english:
        valid_names.add(english)","valid_names = {english for (english, chinese) in choices if english}","['valid_names = {english for (english, chinese) in choices if english}']",1,
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/extern/ply/lex.py,https://github.com/astropy/astropy/tree/master/astropy/extern/ply/lex.py,,lex$862,"def lex(module=None, object=None, debug=False, optimize=False, lextab='lextab', reflags=int(re.VERBOSE), nowarn=False, outputdir=None, debuglog=None, errorlog=None):
    if lextab is None:
        lextab = 'lextab'
    global lexer
    ldict = None
    stateinfo = {'INITIAL': 'inclusive'}
    lexobj = Lexer()
    lexobj.lexoptimize = optimize
    global token, input
    if errorlog is None:
        errorlog = PlyLogger(sys.stderr)
    if debug:
        if debuglog is None:
            debuglog = PlyLogger(sys.stderr)
    if object:
        module = object
    if module:
        _items = [(k, getattr(module, k)) for k in dir(module)]
        ldict = dict(_items)
        if '__file__' not in ldict:
            ldict['__file__'] = sys.modules[ldict['__module__']].__file__
    else:
        ldict = get_caller_module_dict(2)
    pkg = ldict.get('__package__')
    if pkg and isinstance(lextab, str):
        if '.' not in lextab:
            lextab = pkg + '.' + lextab
    linfo = LexerReflect(ldict, log=errorlog, reflags=reflags)
    linfo.get_all()
    if not optimize:
        if linfo.validate_all():
            raise SyntaxError(""Can't build lexer"")
    if optimize and lextab:
        try:
            lexobj.readtab(lextab, ldict)
            token = lexobj.token
            input = lexobj.input
            lexer = lexobj
            return lexobj
        except ImportError:
            pass
    if debug:
        debuglog.info('lex: tokens   = %r', linfo.tokens)
        debuglog.info('lex: literals = %r', linfo.literals)
        debuglog.info('lex: states   = %r', linfo.stateinfo)
    lexobj.lextokens = set()
    for n in linfo.tokens:
        lexobj.lextokens.add(n)
    if isinstance(linfo.literals, (list, tuple)):
        lexobj.lexliterals = type(linfo.literals[0])().join(linfo.literals)
    else:
        lexobj.lexliterals = linfo.literals
    lexobj.lextokens_all = lexobj.lextokens | set(lexobj.lexliterals)
    stateinfo = linfo.stateinfo
    regexs = {}
    for state in stateinfo:
        regex_list = []
        for (fname, f) in linfo.funcsym[state]:
            regex_list.append('(?P<%s>%s)' % (fname, _get_regex(f)))
            if debug:
                debuglog.info(""lex: Adding rule %s -> '%s' (state '%s')"", fname, _get_regex(f), state)
        for (name, r) in linfo.strsym[state]:
            regex_list.append('(?P<%s>%s)' % (name, r))
            if debug:
                debuglog.info(""lex: Adding rule %s -> '%s' (state '%s')"", name, r, state)
        regexs[state] = regex_list
    if debug:
        debuglog.info('lex: ==== MASTER REGEXS FOLLOW ====')
    for state in regexs:
        (lexre, re_text, re_names) = _form_master_re(regexs[state], reflags, ldict, linfo.toknames)
        lexobj.lexstatere[state] = lexre
        lexobj.lexstateretext[state] = re_text
        lexobj.lexstaterenames[state] = re_names
        if debug:
            for (i, text) in enumerate(re_text):
                debuglog.info(""lex: state '%s' : regex[%d] = '%s'"", state, i, text)
    for (state, stype) in stateinfo.items():
        if state != 'INITIAL' and stype == 'inclusive':
            lexobj.lexstatere[state].extend(lexobj.lexstatere['INITIAL'])
            lexobj.lexstateretext[state].extend(lexobj.lexstateretext['INITIAL'])
            lexobj.lexstaterenames[state].extend(lexobj.lexstaterenames['INITIAL'])
    lexobj.lexstateinfo = stateinfo
    lexobj.lexre = lexobj.lexstatere['INITIAL']
    lexobj.lexretext = lexobj.lexstateretext['INITIAL']
    lexobj.lexreflags = reflags
    lexobj.lexstateignore = linfo.ignore
    lexobj.lexignore = lexobj.lexstateignore.get('INITIAL', '')
    lexobj.lexstateerrorf = linfo.errorf
    lexobj.lexerrorf = linfo.errorf.get('INITIAL', None)
    if not lexobj.lexerrorf:
        errorlog.warning('No t_error rule is defined')
    lexobj.lexstateeoff = linfo.eoff
    lexobj.lexeoff = linfo.eoff.get('INITIAL', None)
    for (s, stype) in stateinfo.items():
        if stype == 'exclusive':
            if s not in linfo.errorf:
                errorlog.warning(""No error rule is defined for exclusive state '%s'"", s)
            if s not in linfo.ignore and lexobj.lexignore:
                errorlog.warning(""No ignore rule is defined for exclusive state '%s'"", s)
        elif stype == 'inclusive':
            if s not in linfo.errorf:
                linfo.errorf[s] = linfo.errorf.get('INITIAL', None)
            if s not in linfo.ignore:
                linfo.ignore[s] = linfo.ignore.get('INITIAL', '')
    token = lexobj.token
    input = lexobj.input
    lexer = lexobj
    if lextab and optimize:
        if outputdir is None:
            if isinstance(lextab, types.ModuleType):
                srcfile = lextab.__file__
            elif '.' not in lextab:
                srcfile = ldict['__file__']
            else:
                parts = lextab.split('.')
                pkgname = '.'.join(parts[:-1])
                exec('import %s' % pkgname)
                srcfile = getattr(sys.modules[pkgname], '__file__', '')
            outputdir = os.path.dirname(srcfile)
        try:
            lexobj.writetab(lextab, outputdir)
            if lextab in sys.modules:
                del sys.modules[lextab]
        except IOError as e:
            errorlog.warning(""Couldn't write lextab module %r. %s"" % (lextab, e))
    return lexobj","for n in linfo.tokens:
    lexobj.lextokens.add(n)",lexobj.lextokens.update({n for n in linfo.tokens}),['lexobj.lextokens = {n for n in linfo.tokens}'],0,
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/resources/vpc.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/vpc.py,SGUsage,get_eni_sgs$790,"def get_eni_sgs(self):
    sg_ids = set()
    for nic in self.manager.get_resource_manager('eni').resources():
        for g in nic['Groups']:
            sg_ids.add(g['GroupId'])
    return sg_ids","for nic in self.manager.get_resource_manager('eni').resources():
    for g in nic['Groups']:
        sg_ids.add(g['GroupId'])",sg_ids = {g['GroupId'] for nic in self.manager.get_resource_manager('eni').resources() for g in nic['Groups']},["sg_ids = {g['GroupId'] for nic in self.manager.get_resource_manager('eni').resources() for g in nic['Groups']}"],1,
graphql-compiler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/graphql-compiler/graphql_compiler/schema_generation/graphql_schema.py,https://github.com/kensho-technologies/graphql-compiler/tree/master/graphql_compiler/schema_generation/graphql_schema.py,,_get_referenced_type_equivalences$38,"def _get_referenced_type_equivalences(graphql_types, type_equivalence_hints):
    """"""Filter union types with no edges from the type equivalence hints dict.""""""
    referenced_types = set()
    for graphql_type in graphql_types.values():
        if isinstance(graphql_type, (GraphQLObjectType, GraphQLInterfaceType)):
            for (_, field) in graphql_type.fields.items():
                if isinstance(field.type, GraphQLList):
                    referenced_types.add(field.type.of_type.name)
    return {original: union for (original, union) in type_equivalence_hints.items() if union.name in referenced_types}","for graphql_type in graphql_types.values():
    if isinstance(graphql_type, (GraphQLObjectType, GraphQLInterfaceType)):
        for (_, field) in graphql_type.fields.items():
            if isinstance(field.type, GraphQLList):
                referenced_types.add(field.type.of_type.name)","referenced_types = {field.type.of_type.name for graphql_type in graphql_types.values() if isinstance(graphql_type, (GraphQLObjectType, GraphQLInterfaceType)) for (_, field) in graphql_type.fields.items() if isinstance(field.type, GraphQLList)}","['referenced_types = {field.type.of_type.name for graphql_type in graphql_types.values() if isinstance(graphql_type, (GraphQLObjectType, GraphQLInterfaceType)) for (_, field) in graphql_type.fields.items() if isinstance(field.type, GraphQLList)}']",1,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/pulse/test_transforms.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/pulse/test_transforms.py,,get_pulse_ids$376,"def get_pulse_ids(schedules: List[Schedule]) -> Set[int]:
    """"""Returns ids of pulses used in Schedules.""""""
    ids = set()
    for schedule in schedules:
        for (_, inst) in schedule.instructions:
            ids.add(inst.pulse.id)
    return ids","for schedule in schedules:
    for (_, inst) in schedule.instructions:
        ids.add(inst.pulse.id)","ids = {inst.pulse.id for schedule in schedules for (_, inst) in schedule.instructions}","['ids = {inst.pulse.id for schedule in schedules for (_, inst) in schedule.instructions}']",1,
Kiwi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Kiwi/tcms/signals.py,https://github.com/kiwitcms/Kiwi/tree/master/tcms/signals.py,,notify_admins$47,"def notify_admins(sender, **kwargs):
    """"""
    Very simple signal handler which sends emails to site
    admins when a new user has been registered!

    .. warning::

        This handler isn't connected to the ``USER_REGISTERED_SIGNAL`` by default!
    """"""
    from django.conf import settings
    from django.contrib.auth import get_user_model
    from django.urls import reverse
    from tcms.core.utils import request_host_link
    from tcms.core.utils.mailto import mailto
    if kwargs.get('raw', False):
        return
    admin_emails = set()
    for super_user in get_user_model().objects.filter(is_superuser=True):
        admin_emails.add(super_user.email)
    for (_name, email) in settings.ADMINS:
        admin_emails.add(email)
    request = kwargs.get('request')
    user = kwargs.get('user')
    user_url = request_host_link(request) + reverse('admin:auth_user_change', args=[user.pk])
    mailto(template_name='email/user_registered/notify_admins.txt', recipients=list(admin_emails), subject=str(_('New user awaiting approval')), context={'username': user.username, 'user_url': user_url})","for super_user in get_user_model().objects.filter(is_superuser=True):
    admin_emails.add(super_user.email)",admin_emails = {super_user.email for super_user in get_user_model().objects.filter(is_superuser=True)},['admin_emails = {super_user.email for super_user in get_user_model().objects.filter(is_superuser=True)}'],1,
Kiwi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Kiwi/tcms/signals.py,https://github.com/kiwitcms/Kiwi/tree/master/tcms/signals.py,,notify_admins$47,"def notify_admins(sender, **kwargs):
    """"""
    Very simple signal handler which sends emails to site
    admins when a new user has been registered!

    .. warning::

        This handler isn't connected to the ``USER_REGISTERED_SIGNAL`` by default!
    """"""
    from django.conf import settings
    from django.contrib.auth import get_user_model
    from django.urls import reverse
    from tcms.core.utils import request_host_link
    from tcms.core.utils.mailto import mailto
    if kwargs.get('raw', False):
        return
    admin_emails = set()
    for super_user in get_user_model().objects.filter(is_superuser=True):
        admin_emails.add(super_user.email)
    for (_name, email) in settings.ADMINS:
        admin_emails.add(email)
    request = kwargs.get('request')
    user = kwargs.get('user')
    user_url = request_host_link(request) + reverse('admin:auth_user_change', args=[user.pk])
    mailto(template_name='email/user_registered/notify_admins.txt', recipients=list(admin_emails), subject=str(_('New user awaiting approval')), context={'username': user.username, 'user_url': user_url})","for (_name, email) in settings.ADMINS:
    admin_emails.add(email)","admin_emails = {email for (_name, email) in settings.ADMINS}",Cannot refactor,2,
buildbot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/buildbot/master/buildbot/reporters/utils.py,https://github.com/buildbot/buildbot/tree/master/master/buildbot/reporters/utils.py,,getResponsibleUsersForSourceStamp$174,"def getResponsibleUsersForSourceStamp(master, sourcestampid):
    changesd = master.data.get(('sourcestamps', sourcestampid, 'changes'))
    sourcestampd = master.data.get(('sourcestamps', sourcestampid))
    (changes, sourcestamp) = (yield defer.gatherResults([changesd, sourcestampd]))
    blamelist = set()
    for c in changes:
        blamelist.add(c['author'])
    if 'patch' in sourcestamp and sourcestamp['patch'] is not None:
        blamelist.add(sourcestamp['patch']['author'])
    blamelist = list(blamelist)
    blamelist.sort()
    return blamelist","for c in changes:
    blamelist.add(c['author'])",blamelist = {c['author'] for c in changes},["blamelist = {c['author'] for c in changes}"],1,
mopidy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mopidy/mopidy/http/handlers.py,https://github.com/mopidy/mopidy/tree/master/mopidy/http/handlers.py,ClientListHandler,get$275,"def get(self):
    set_mopidy_headers(self)
    names = set()
    for app in self.apps:
        names.add(app['name'])
    for static in self.statics:
        names.add(static['name'])
    names.discard('mopidy')
    self.render('data/clients.html', apps=sorted(list(names)))","for app in self.apps:
    names.add(app['name'])",names = {app['name'] for app in self.apps},["names = {app['name'] for app in self.apps}"],1,
mopidy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mopidy/mopidy/http/handlers.py,https://github.com/mopidy/mopidy/tree/master/mopidy/http/handlers.py,ClientListHandler,get$275,"def get(self):
    set_mopidy_headers(self)
    names = set()
    for app in self.apps:
        names.add(app['name'])
    for static in self.statics:
        names.add(static['name'])
    names.discard('mopidy')
    self.render('data/clients.html', apps=sorted(list(names)))","for static in self.statics:
    names.add(static['name'])",names = {static['name'] for static in self.statics},Cannot refactor,2,
luigi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/luigi/test/task_forwarded_attributes_test.py,https://github.com/spotify/luigi/tree/master/test/task_forwarded_attributes_test.py,NonYieldingTask,gather_forwarded_attributes$33,"def gather_forwarded_attributes(self):
    """"""
        Returns a set of names of attributes that are forwarded by the TaskProcess and that are not
        *None*. The tests in this file check if and which attributes are present at different times,
        e.g. while running, or before and after a dynamic dependency was yielded.
        """"""
    attrs = set()
    for attr in FORWARDED_ATTRIBUTES:
        if getattr(self, attr, None) is not None:
            attrs.add(attr)
    return attrs","for attr in FORWARDED_ATTRIBUTES:
    if getattr(self, attr, None) is not None:
        attrs.add(attr)","attrs = {attr for attr in FORWARDED_ATTRIBUTES if getattr(self, attr, None) is not None}","['attrs = {attr for attr in FORWARDED_ATTRIBUTES if getattr(self, attr, None) is not None}']",1,
ARL,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ARL/app/services/searchEngines.py,https://github.com/TophantTechnology/ARL/tree/master/app/services/searchEngines.py,BingSearch,match_urls$99,"def match_urls(self, html):
    dom = pq(html)
    result_items = dom(self.pq_query).items()
    urls_result = [item.attr('href') for item in result_items]
    urls = set()
    for u in urls_result:
        urls.add(u)
    return list(urls)","for u in urls_result:
    urls.add(u)",urls = {u for u in urls_result},['urls = {u for u in urls_result}'],1,
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/world/resourcehandler.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/world/resourcehandler.py,ResourceHandler,_load_provided_resources$195,"def _load_provided_resources(self):
    """"""Returns a iterable obj containing all resources this building provides.
		This is outsourced from initialization to a method for the possibility of
		overwriting it.
		Do not alter the returned list; if you need to do so, then copy it.""""""
    produced_resources = set()
    for prod in self.get_component(Producer).get_productions():
        for res in prod.get_produced_resources():
            produced_resources.add(res)
    for res in self.additional_provided_resources:
        produced_resources.add(res)
    return produced_resources","for prod in self.get_component(Producer).get_productions():
    for res in prod.get_produced_resources():
        produced_resources.add(res)",produced_resources = {res for prod in self.get_component(Producer).get_productions() for res in prod.get_produced_resources()},['produced_resources = {res for prod in self.get_component(Producer).get_productions() for res in prod.get_produced_resources()}'],1,
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/world/resourcehandler.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/world/resourcehandler.py,ResourceHandler,_load_provided_resources$195,"def _load_provided_resources(self):
    """"""Returns a iterable obj containing all resources this building provides.
		This is outsourced from initialization to a method for the possibility of
		overwriting it.
		Do not alter the returned list; if you need to do so, then copy it.""""""
    produced_resources = set()
    for prod in self.get_component(Producer).get_productions():
        for res in prod.get_produced_resources():
            produced_resources.add(res)
    for res in self.additional_provided_resources:
        produced_resources.add(res)
    return produced_resources","for res in self.additional_provided_resources:
    produced_resources.add(res)",produced_resources = {res for res in self.additional_provided_resources},Cannot refactor,2,
mars,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/ci/importcheck.py,https://github.com/mars-project/mars/tree/master/ci/importcheck.py,,_check_absolute_import$42,"def _check_absolute_import(node: ast.AST) -> List[Tuple[int, int]]:
    res = set()
    if isinstance(node, ast.Import):
        for import_name in node.names:
            if import_name.name.startswith('mars.'):
                res.add((node.lineno, node.end_lineno))
    elif isinstance(node, ast.ImportFrom):
        if node.level == 0 and node.module.startswith('mars.'):
            res.add((node.lineno, node.end_lineno))
    elif getattr(node, 'body', []):
        for body_item in node.body:
            res.update(_check_absolute_import(body_item))
    return sorted(res)","for import_name in node.names:
    if import_name.name.startswith('mars.'):
        res.add((node.lineno, node.end_lineno))","res = {(node.lineno, node.end_lineno) for import_name in node.names if import_name.name.startswith('mars.')}","[""res = {(node.lineno, node.end_lineno) for import_name in node.names if import_name.name.startswith('mars.')}""]",1,
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/resources/ebs.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/ebs.py,SnapshotUnusedFilter,_pull_ami_snapshots$301,"def _pull_ami_snapshots(self):
    amis = self.manager.get_resource_manager('ami').resources()
    ami_snaps = set()
    for i in amis:
        for dev in i.get('BlockDeviceMappings'):
            if 'Ebs' in dev and 'SnapshotId' in dev['Ebs']:
                ami_snaps.add(dev['Ebs']['SnapshotId'])
    return ami_snaps","for i in amis:
    for dev in i.get('BlockDeviceMappings'):
        if 'Ebs' in dev and 'SnapshotId' in dev['Ebs']:
            ami_snaps.add(dev['Ebs']['SnapshotId'])",ami_snaps = {dev['Ebs']['SnapshotId'] for i in amis for dev in i.get('BlockDeviceMappings') if 'Ebs' in dev and 'SnapshotId' in dev['Ebs']},["ami_snaps = {dev['Ebs']['SnapshotId'] for i in amis for dev in i.get('BlockDeviceMappings') if 'Ebs' in dev and 'SnapshotId' in dev['Ebs']}"],1,
cmakeconverter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cmakeconverter/cmake_converter/data_converter.py,https://github.com/pavelliavonau/cmakeconverter/tree/master/cmake_converter/data_converter.py,DataConverter,__verify_configurations_to_parse$96,"def __verify_configurations_to_parse(context):
    absent_settings = set()
    for setting in context.configurations_to_parse:
        if setting not in context.settings:
            absent_settings.add(setting)
    if len(absent_settings) > 0:
        context.configurations_to_parse -= absent_settings
        message(context, 'There are absent settings at {}: {}\nskipping conversion. Add lost settings or fix mapping of settings at solution'.format(context.vcxproj_path, absent_settings), 'error')
        return False
    return True","for setting in context.configurations_to_parse:
    if setting not in context.settings:
        absent_settings.add(setting)",absent_settings = {setting for setting in context.configurations_to_parse if setting not in context.settings},['absent_settings = {setting for setting in context.configurations_to_parse if setting not in context.settings}'],1,
pychess,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/testing/seirawan.py,https://github.com/pychess/pychess/tree/master/testing/seirawan.py,SchessTestCase,test_disambig_gating_san$205,"def test_disambig_gating_san(self):
    FEN = 'rnbqkb1r/ppp1pppp/3p1n2/8/2PP4/2N5/PP2PPPP/RHBQKBNR[Eeh] b KQACDEFGHkqabcdefh - 1 3'
    board = LBoard(SCHESS)
    board.applyFen(FEN)
    moves = set()
    for move in genAllMoves(board):
        moves.add(toAN(board, move))
    print('--------')
    print(board)
    self.assertIn('f6d7', moves)
    self.assertNotIn('f6d7/H', moves)
    self.assertNotIn('f6d7/E', moves)
    self.assertIn('b8d7', moves)
    self.assertIn('b8d7h', moves)
    self.assertIn('b8d7e', moves)
    self.assertEqual(repr(Move.Move(parseSAN(board, 'Nfd7'))), 'f6d7')
    self.assertEqual(repr(Move.Move(parseSAN(board, 'Nbd7'))), 'b8d7')
    self.assertEqual(repr(Move.Move(parseSAN(board, 'Nbd7/H'))), 'b8d7h')
    self.assertEqual(repr(Move.Move(parseSAN(board, 'Nbd7/E'))), 'b8d7e')","for move in genAllMoves(board):
    moves.add(toAN(board, move))","moves = {toAN(board, move) for move in genAllMoves(board)}","['moves = {toAN(board, move) for move in genAllMoves(board)}']",1,
TextAttack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TextAttack/textattack/shared/attacked_text.py,https://github.com/QData/TextAttack/tree/master/textattack/shared/attacked_text.py,AttackedText,all_words_diff$241,"def all_words_diff(self, other_attacked_text):
    """"""Returns the set of indices for which this and other_attacked_text
        have different words.""""""
    indices = set()
    w1 = self.words
    w2 = other_attacked_text.words
    for i in range(min(len(w1), len(w2))):
        if w1[i] != w2[i]:
            indices.add(i)
    return indices","for i in range(min(len(w1), len(w2))):
    if w1[i] != w2[i]:
        indices.add(i)","indices = {i for i in range(min(len(w1), len(w2))) if w1[i] != w2[i]}","['indices = {i for i in range(min(len(w1), len(w2))) if w1[i] != w2[i]}']",1,
fairseq,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairseq/scripts/compare_namespaces.py,https://github.com/pytorch/fairseq/tree/master/scripts/compare_namespaces.py,,keys$12,"def keys(ns):
    ks = set()
    for k in dir(ns):
        if not k.startswith('_'):
            ks.add(k)
    return ks","for k in dir(ns):
    if not k.startswith('_'):
        ks.add(k)",ks = {k for k in dir(ns) if not k.startswith('_')},["ks = {k for k in dir(ns) if not k.startswith('_')}"],1,
shuup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/utils/permissions.py,https://github.com/shuup/shuup/tree/master/shuup/admin/utils/permissions.py,,get_default_model_permissions$18,"def get_default_model_permissions(model):
    """"""
    Return a set of all default permissions for a given model.

    :param model: Model class
    :type model: django.db.Model
    :return: Set of default model permissions as strings
    :rtype: set[str]
    """"""
    warnings.warn('Warning! `get_default_model_permissions` is deprecated in Shuup 2.0. Use human readable permission strings instead.', DeprecationWarning)
    permissions = set()
    for default in model._meta.default_permissions:
        permissions.add('%s.%s_%s' % (model._meta.app_label, default, model._meta.model_name))
    return permissions","for default in model._meta.default_permissions:
    permissions.add('%s.%s_%s' % (model._meta.app_label, default, model._meta.model_name))","permissions = {'%s.%s_%s' % (model._meta.app_label, default, model._meta.model_name) for default in model._meta.default_permissions}","[""permissions = {'%s.%s_%s' % (model._meta.app_label, default, model._meta.model_name) for default in model._meta.default_permissions}""]",1,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/runit.py,https://github.com/saltstack/salt/tree/master/salt/modules/runit.py,,get_svc_broken_path$274,"def get_svc_broken_path(name='*'):
    """"""
    Return list of broken path(s) in SERVICE_DIR that match ``name``

    A path is broken if it is a broken symlink or can not be a runit service

    name
        a glob for service name. default is '*'

    CLI Example:

    .. code-block:: bash

        salt '*' runit.get_svc_broken_path <service name>
    """"""
    if not SERVICE_DIR:
        raise CommandExecutionError('Could not find service directory.')
    ret = set()
    for el in glob.glob(os.path.join(SERVICE_DIR, name)):
        if not _is_svc(el):
            ret.add(el)
    return sorted(ret)","for el in glob.glob(os.path.join(SERVICE_DIR, name)):
    if not _is_svc(el):
        ret.add(el)","ret = {el for el in glob.glob(os.path.join(SERVICE_DIR, name)) if not _is_svc(el)}","['ret = {el for el in glob.glob(os.path.join(SERVICE_DIR, name)) if not _is_svc(el)}']",1,
oio-sds,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oio-sds/oio/cli/admin/service_check.py,https://github.com/open-io/oio-sds/tree/master/oio/cli/admin/service_check.py,DirectoryCheck,_take_action$206,"def _take_action(self, parsed_args):
    import subprocess
    from oio.directory.meta0 import Meta0Client
    from oio.common.json import json
    self.logger.debug('Checking the directory bootstrap.')
    m0 = Meta0Client({'namespace': self.app.options.ns})
    prefixes = m0.list()
    if len(prefixes) != CID_PREFIX_COUNT:
        raise ValueError('Found %d entries in meta0, expected %d' % (len(prefixes), CID_PREFIX_COUNT))
    self.logger.info('The proxy serves a full meta0 dump.')
    for (_, host, port, _) in self.filter_services(self.catalog, 'meta0'):
        url = '%s:%d' % (host, port)
        res = subprocess.check_output(['oio-meta0-client', url, 'get', '0000'])
        self.logger.info(res)
    self.logger.info('All meta0 services are complete.')
    dump0 = None
    first = None
    for (_, host, port, _) in self.filter_services(self.catalog, 'meta0'):
        url = '%s:%d' % (host, port)
        dump = subprocess.check_output(['oio-meta0-client', url, 'list'])
        if dump0 is None:
            dump0 = dump
            first = url
        elif dump0 != dump:
            raise ValueError('The dump returned by meta0 %s differs from the dump returned by %s' % (url, first))
    self.logger.info('All meta0 services serve the same base.')
    reverse_dump = set()
    for (_, v) in iteritems(json.loads(dump0)):
        for url in v:
            reverse_dump.add(url)
    m1 = {':'.join((descr[1], str(descr[2]))) for descr in self.filter_services(self.catalog, 'meta1')}
    if m1 != reverse_dump:
        raise ValueError('Meta1 used but not visible: %s, meta1 visible but not used: %s' % (reverse_dump - m1, m1 - reverse_dump))
    self.logger.info('All meta1 services have been assigned.')
    yield ('OK', None)","for (_, v) in iteritems(json.loads(dump0)):
    for url in v:
        reverse_dump.add(url)","reverse_dump = {url for (_, v) in iteritems(json.loads(dump0)) for url in v}","['reverse_dump = {url for (_, v) in iteritems(json.loads(dump0)) for url in v}']",1,
meta-dataset,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meta-dataset/meta_dataset/data/imagenet_specification_test.py,https://github.com/google-research/meta-dataset/tree/master/meta_dataset/data/imagenet_specification_test.py,GraphCopyTest,validate_copy$254,"def validate_copy(self, graph, graph_copy):
    """"""Make sure graph_copy is a correct copy of graph.""""""
    for n in graph:
        wn_id = n.wn_id
        found_wn_in_copy = False
        for n_copy in graph_copy:
            if n_copy.wn_id == wn_id:
                found_wn_in_copy = True
                break
        self.assertTrue(found_wn_in_copy)
    graph_parent_child_links = set()
    for s in graph:
        for c in s.children:
            graph_parent_child_links.add((s.wn_id, c.wn_id))
    for s in graph_copy:
        for (p, c) in graph_parent_child_links:
            for n in graph_copy:
                if n.wn_id == c:
                    c_node = n
                if n.wn_id == p:
                    p_node = n
            self.assertIn(c_node, p_node.children)
            self.assertIn(p_node, c_node.parents)","for s in graph:
    for c in s.children:
        graph_parent_child_links.add((s.wn_id, c.wn_id))","graph_parent_child_links = {(s.wn_id, c.wn_id) for s in graph for c in s.children}","['graph_parent_child_links = {(s.wn_id, c.wn_id) for s in graph for c in s.children}']",1,
Sublist3r,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Sublist3r/sublist3r.py,https://github.com/aboul3la/Sublist3r/tree/master//sublist3r.py,,main$884,"def main(domain, threads, savefile, ports, silent, verbose, enable_bruteforce, engines):
    bruteforce_list = set()
    search_list = set()
    if is_windows:
        subdomains_queue = list()
    else:
        subdomains_queue = multiprocessing.Manager().list()
    if enable_bruteforce or enable_bruteforce is None:
        enable_bruteforce = True
    domain_check = re.compile('^(http|https)?[a-zA-Z0-9]+([\\-\\.]{1}[a-zA-Z0-9]+)*\\.[a-zA-Z]{2,}$')
    if not domain_check.match(domain):
        if not silent:
            print(R + 'Error: Please enter a valid domain' + W)
        return []
    if not domain.startswith('http://') or not domain.startswith('https://'):
        domain = 'http://' + domain
    parsed_domain = urlparse.urlparse(domain)
    if not silent:
        print(B + '[-] Enumerating subdomains now for %s' % parsed_domain.netloc + W)
    if verbose and (not silent):
        print(Y + '[-] verbosity is enabled, will show the subdomains results in realtime' + W)
    supported_engines = {'baidu': BaiduEnum, 'yahoo': YahooEnum, 'google': GoogleEnum, 'bing': BingEnum, 'ask': AskEnum, 'netcraft': NetcraftEnum, 'dnsdumpster': DNSdumpster, 'virustotal': Virustotal, 'threatcrowd': ThreatCrowd, 'ssl': CrtSearch, 'passivedns': PassiveDNS}
    chosenEnums = []
    if engines is None:
        chosenEnums = [BaiduEnum, YahooEnum, GoogleEnum, BingEnum, AskEnum, NetcraftEnum, DNSdumpster, Virustotal, ThreatCrowd, CrtSearch, PassiveDNS]
    else:
        engines = engines.split(',')
        for engine in engines:
            if engine.lower() in supported_engines:
                chosenEnums.append(supported_engines[engine.lower()])
    enums = [enum(domain, [], q=subdomains_queue, silent=silent, verbose=verbose) for enum in chosenEnums]
    for enum in enums:
        enum.start()
    for enum in enums:
        enum.join()
    subdomains = set(subdomains_queue)
    for subdomain in subdomains:
        search_list.add(subdomain)
    if enable_bruteforce:
        if not silent:
            print(G + '[-] Starting bruteforce module now using subbrute..' + W)
        record_type = False
        path_to_file = os.path.dirname(os.path.realpath(__file__))
        subs = os.path.join(path_to_file, 'subbrute', 'names.txt')
        resolvers = os.path.join(path_to_file, 'subbrute', 'resolvers.txt')
        process_count = threads
        output = False
        json_output = False
        bruteforce_list = subbrute.print_target(parsed_domain.netloc, record_type, subs, resolvers, process_count, output, json_output, search_list, verbose)
    subdomains = search_list.union(bruteforce_list)
    if subdomains:
        subdomains = sorted(subdomains, key=subdomain_sorting_key)
        if savefile:
            write_file(savefile, subdomains)
        if not silent:
            print(Y + '[-] Total Unique Subdomains Found: %s' % len(subdomains) + W)
        if ports:
            if not silent:
                print(G + '[-] Start port scan now for the following ports: %s%s' % (Y, ports) + W)
            ports = ports.split(',')
            pscan = portscan(subdomains, ports)
            pscan.run()
        elif not silent:
            for subdomain in subdomains:
                print(G + subdomain + W)
    return subdomains","for subdomain in subdomains:
    search_list.add(subdomain)",search_list = {subdomain for subdomain in subdomains},['search_list = {subdomain for subdomain in subdomains}'],1,
hivemind,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hivemind/hivemind/averaging/allreduce.py,https://github.com/learning-at-home/hivemind/tree/master/hivemind/averaging/allreduce.py,AllReduceRunner,finalize$281,"def finalize(self, *, cancel: bool=False, exception: Optional[BaseException]=None):
    """"""finish or terminate AllReduceRunner, propagate any errors / cancellations to peers.""""""
    assert not cancel or not exception, 'finalize accepts either exception or cancel, but not both'
    pending_tasks = set()
    if cancel or exception:
        if cancel or isinstance(exception, asyncio.CancelledError):
            code = averaging_pb2.CANCELLED
        else:
            code = averaging_pb2.INTERNAL_ERROR
        logger.debug(f'{self} - notifying peers about {averaging_pb2.MessageCode.Name(code)}')
        for (peer_id, mode) in zip(self.ordered_peer_ids, self.modes):
            if peer_id != self.peer_id and mode != AveragingMode.CLIENT:
                pending_tasks.add(asyncio.create_task(self._send_error_to_peer(peer_id, code)))
    if not self._future.done():
        if cancel:
            logger.debug(f'{self} - cancelled')
            self._future.cancel()
        elif exception:
            logger.debug(f'{self} - caught {exception}')
            self._future.set_exception(exception)
        else:
            logger.debug(f'{self} - finished')
            self._future.set_result(None)
        self.tensor_part_container.finalize()
        self.tensor_part_reducer.finalize()
        return pending_tasks
    else:
        logger.debug(f'{self} - could not finish: allreduce is already finished: {self._future}')
        return pending_tasks","for (peer_id, mode) in zip(self.ordered_peer_ids, self.modes):
    if peer_id != self.peer_id and mode != AveragingMode.CLIENT:
        pending_tasks.add(asyncio.create_task(self._send_error_to_peer(peer_id, code)))","pending_tasks = {asyncio.create_task(self._send_error_to_peer(peer_id, code)) for (peer_id, mode) in zip(self.ordered_peer_ids, self.modes) if peer_id != self.peer_id and mode != AveragingMode.CLIENT}","['pending_tasks = {asyncio.create_task(self._send_error_to_peer(peer_id, code)) for (peer_id, mode) in zip(self.ordered_peer_ids, self.modes) if peer_id != self.peer_id and mode != AveragingMode.CLIENT}']",1,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_webapp/docassemble/webapp/backend.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/backend.py,,delete_temp_user_data$714,"def delete_temp_user_data(temp_user_id, r):
    db.session.execute(delete(UserDictKeys).where(UserDictKeys.temp_user_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(UploadsUserAuth).where(UploadsUserAuth.temp_user_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.temp_owner_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.temp_user_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(GlobalObjectStorage).where(GlobalObjectStorage.temp_user_id == temp_user_id))
    db.session.commit()
    files_to_delete = list()
    for short_code_item in db.session.execute(select(Shortener).filter_by(temp_user_id=temp_user_id)).scalars():
        for email in db.session.execute(select(Email).filter_by(short=short_code_item.short)).scalars():
            for attachment in db.session.execute(select(EmailAttachment).filter_by(email_id=email.id)).scalars():
                files_to_delete.append(attachment.upload)
    for file_number in files_to_delete:
        the_file = SavedFile(file_number)
        the_file.delete()
    db.session.execute(delete(Shortener).where(Shortener.temp_user_id == temp_user_id))
    db.session.commit()
    keys_to_delete = set()
    for key in r.keys('*userid:t' + str(temp_user_id)):
        keys_to_delete.add(key)
    for key in r.keys('*userid:t' + str(temp_user_id) + ':*'):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r.delete(key)","for key in r.keys('*userid:t' + str(temp_user_id)):
    keys_to_delete.add(key)",keys_to_delete = {key for key in r.keys('*userid:t' + str(temp_user_id))},["keys_to_delete = {key for key in r.keys('*userid:t' + str(temp_user_id))}"],1,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_webapp/docassemble/webapp/backend.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/backend.py,,delete_temp_user_data$714,"def delete_temp_user_data(temp_user_id, r):
    db.session.execute(delete(UserDictKeys).where(UserDictKeys.temp_user_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(UploadsUserAuth).where(UploadsUserAuth.temp_user_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.temp_owner_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.temp_user_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(GlobalObjectStorage).where(GlobalObjectStorage.temp_user_id == temp_user_id))
    db.session.commit()
    files_to_delete = list()
    for short_code_item in db.session.execute(select(Shortener).filter_by(temp_user_id=temp_user_id)).scalars():
        for email in db.session.execute(select(Email).filter_by(short=short_code_item.short)).scalars():
            for attachment in db.session.execute(select(EmailAttachment).filter_by(email_id=email.id)).scalars():
                files_to_delete.append(attachment.upload)
    for file_number in files_to_delete:
        the_file = SavedFile(file_number)
        the_file.delete()
    db.session.execute(delete(Shortener).where(Shortener.temp_user_id == temp_user_id))
    db.session.commit()
    keys_to_delete = set()
    for key in r.keys('*userid:t' + str(temp_user_id)):
        keys_to_delete.add(key)
    for key in r.keys('*userid:t' + str(temp_user_id) + ':*'):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r.delete(key)","for key in r.keys('*userid:t' + str(temp_user_id) + ':*'):
    keys_to_delete.add(key)",keys_to_delete = {key for key in r.keys('*userid:t' + str(temp_user_id) + ':*')},Cannot refactor,2,
node-gyp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/node-gyp/gyp/pylib/gyp/generator/cmake.py,https://github.com/nodejs/node-gyp/tree/master/gyp/pylib/gyp/generator/cmake.py,,GenerateOutputForConfig$1143,"def GenerateOutputForConfig(target_list, target_dicts, data, params, config_to_use):
    options = params['options']
    generator_flags = params['generator_flags']
    flavor = gyp.common.GetFlavor(params)
    generator_dir = os.path.relpath(options.generator_output or '.')
    output_dir = generator_flags.get('output_dir', 'out')
    build_dir = os.path.normpath(os.path.join(generator_dir, output_dir, config_to_use))
    toplevel_build = os.path.join(options.toplevel_dir, build_dir)
    output_file = os.path.join(toplevel_build, 'CMakeLists.txt')
    gyp.common.EnsureDirExists(output_file)
    output = open(output_file, 'w')
    output.write('cmake_minimum_required(VERSION 2.8.8 FATAL_ERROR)\n')
    output.write('cmake_policy(VERSION 2.8.8)\n')
    (gyp_file, project_target, _) = gyp.common.ParseQualifiedTarget(target_list[-1])
    output.write('project(')
    output.write(project_target)
    output.write(')\n')
    SetVariable(output, 'configuration', config_to_use)
    ar = None
    cc = None
    cxx = None
    make_global_settings = data[gyp_file].get('make_global_settings', [])
    build_to_top = gyp.common.InvertRelativePath(build_dir, options.toplevel_dir)
    for (key, value) in make_global_settings:
        if key == 'AR':
            ar = os.path.join(build_to_top, value)
        if key == 'CC':
            cc = os.path.join(build_to_top, value)
        if key == 'CXX':
            cxx = os.path.join(build_to_top, value)
    ar = gyp.common.GetEnvironFallback(['AR_target', 'AR'], ar)
    cc = gyp.common.GetEnvironFallback(['CC_target', 'CC'], cc)
    cxx = gyp.common.GetEnvironFallback(['CXX_target', 'CXX'], cxx)
    if ar:
        SetVariable(output, 'CMAKE_AR', ar)
    if cc:
        SetVariable(output, 'CMAKE_C_COMPILER', cc)
    if cxx:
        SetVariable(output, 'CMAKE_CXX_COMPILER', cxx)
    output.write('enable_language(ASM)\n')
    if cc:
        SetVariable(output, 'CMAKE_ASM_COMPILER', cc)
    SetVariable(output, 'builddir', '${CMAKE_CURRENT_BINARY_DIR}')
    SetVariable(output, 'obj', '${builddir}/obj')
    output.write('\n')
    output.write('set(CMAKE_C_OUTPUT_EXTENSION_REPLACE 1)\n')
    output.write('set(CMAKE_CXX_OUTPUT_EXTENSION_REPLACE 1)\n')
    output.write('\n')
    if flavor != 'mac':
        output.write('set(CMAKE_NINJA_FORCE_RESPONSE_FILE 1)\n')
    output.write('\n')
    namer = CMakeNamer(target_list)
    all_qualified_targets = set()
    for build_file in params['build_files']:
        for qualified_target in gyp.common.AllTargets(target_list, target_dicts, os.path.normpath(build_file)):
            all_qualified_targets.add(qualified_target)
    for qualified_target in target_list:
        if flavor == 'mac':
            (gyp_file, _, _) = gyp.common.ParseQualifiedTarget(qualified_target)
            spec = target_dicts[qualified_target]
            gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[gyp_file], spec)
        WriteTarget(namer, qualified_target, target_dicts, build_dir, config_to_use, options, generator_flags, all_qualified_targets, flavor, output)
    output.close()","for build_file in params['build_files']:
    for qualified_target in gyp.common.AllTargets(target_list, target_dicts, os.path.normpath(build_file)):
        all_qualified_targets.add(qualified_target)","all_qualified_targets = {qualified_target for build_file in params['build_files'] for qualified_target in gyp.common.AllTargets(target_list, target_dicts, os.path.normpath(build_file))}","[""all_qualified_targets = {qualified_target for build_file in params['build_files'] for qualified_target in gyp.common.AllTargets(target_list, target_dicts, os.path.normpath(build_file))}""]",1,
plop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plop/plop/viewer.py,https://github.com/bdarnell/plop/tree/master/plop/viewer.py,,profile_to_json$46,"def profile_to_json(filename):
    root = os.path.abspath(options.datadir) + os.path.sep
    abspath = os.path.abspath(os.path.join(root, filename))
    assert (abspath + os.path.sep).startswith(root)
    graph = CallGraph.load(abspath)
    total = sum((stack.weights['calls'] for stack in graph.stacks))
    top_stacks = graph.stacks
    filtered_nodes = set()
    for stack in top_stacks:
        filtered_nodes.update(stack.nodes)
    nodes = [dict(attrs=node.attrs, weights=node.weights, id=node.id) for node in filtered_nodes]
    nodes = sorted(nodes, key=lambda n: -n['weights']['calls'])
    index = dict([(node['id'], i) for (i, node) in enumerate(nodes)])
    degrees = Counter()
    dropped = set()
    for edge in six.itervalues(graph.edges):
        degrees[edge.child.id] += 1
        degrees[edge.parent.id] += 1
    for (node, degree) in six.iteritems(degrees):
        if degree > 6:
            dropped.add(node)
    edges = [dict(source=index[edge.parent.id], target=index[edge.child.id], weights=edge.weights) for edge in six.itervalues(graph.edges) if edge.parent.id in index and edge.child.id in index and (edge.parent.id not in dropped) and (edge.child.id not in dropped)]
    stacks = [dict(nodes=[index[n.id] for n in stack.nodes], weights=stack.weights) for stack in top_stacks]
    return dict(nodes=nodes, edges=edges, stacks=stacks)","for (node, degree) in six.iteritems(degrees):
    if degree > 6:
        dropped.add(node)","dropped = {node for (node, degree) in six.iteritems(degrees) if degree > 6}","['dropped = {node for (node, degree) in six.iteritems(degrees) if degree > 6}']",1,
aswan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aswan/www/rule/views.py,https://github.com/momosecurity/aswan/tree/master/www/rule/views.py,RulesDataView,_get_freq_strategy_args$326,"def _get_freq_strategy_args(self, uuids, client):
    ret = set()
    strategy_bodys = self._get_one_kind_fields_from_uuids(uuids, 'freq_strategy', 'strategy_body', client)
    for names in strategy_bodys:
        for name in names.split(','):
            ret.add(name)
    return ret","for names in strategy_bodys:
    for name in names.split(','):
        ret.add(name)","ret = {name for names in strategy_bodys for name in names.split(',')}","[""ret = {name for names in strategy_bodys for name in names.split(',')}""]",1,
ScoutSuite,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ScoutSuite/ScoutSuite/providers/aws/provider.py,https://github.com/nccgroup/ScoutSuite/tree/master/ScoutSuite/providers/aws/provider.py,AWSProvider,_check_ec2_zone_distribution$157,"def _check_ec2_zone_distribution(self):
    regions = self.services['ec2']['regions'].values()
    self.services['ec2']['number_of_regions_with_instances'] = sum((r['instances_count'] > 0 for r in regions))
    for regions in self.services['ec2']['regions'].values():
        instances_availability_zones = set()
        for vpcs in regions['vpcs'].values():
            for instance in vpcs['instances'].values():
                instances_availability_zones.add(instance.get('availability_zone'))
        regions['instances_availability_zones'] = len(instances_availability_zones)","for regions in self.services['ec2']['regions'].values():
    instances_availability_zones = set()
    for vpcs in regions['vpcs'].values():
        for instance in vpcs['instances'].values():
            instances_availability_zones.add(instance.get('availability_zone'))
    regions['instances_availability_zones'] = len(instances_availability_zones)",{regions['instances_availability_zones'] = len({instance.get('availability_zone') for instance in vpcs['instances'].values()} for vpcs in regions['vpcs'].values()) for regions in self.services['ec2']['regions'].values()},Cannot refactor,2,
viztracer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/viztracer/tests/test_multiprocess.py,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_multiprocess.py,TestMultiprocessing,check_func$320,"def check_func(data):
    pids = set()
    for entry in data['traceEvents']:
        pids.add(entry['pid'])
    self.assertEqual(len(pids), 3)","for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']},["pids = {entry['pid'] for entry in data['traceEvents']}"],1,
neat-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neat-python/neat/graphs.py,https://github.com/CodeReclaimers/neat-python/tree/master/neat/graphs.py,,feed_forward_layers$59,"def feed_forward_layers(inputs, outputs, connections):
    """"""
    Collect the layers whose members can be evaluated in parallel in a feed-forward network.
    :param inputs: list of the network input nodes
    :param outputs: list of the output node identifiers
    :param connections: list of (input, output) connections in the network.

    Returns a list of layers, with each layer consisting of a set of node identifiers.
    Note that the returned layers do not contain nodes whose output is ultimately
    never used to compute the final network output.
    """"""
    required = required_for_output(inputs, outputs, connections)
    layers = []
    s = set(inputs)
    while 1:
        c = set((b for (a, b) in connections if a in s and b not in s))
        t = set()
        for n in c:
            if n in required and all((a in s for (a, b) in connections if b == n)):
                t.add(n)
        if not t:
            break
        layers.append(t)
        s = s.union(t)
    return layers","for n in c:
    if n in required and all((a in s for (a, b) in connections if b == n)):
        t.add(n)","t = {n for n in c if n in required and all((a in s for (a, b) in connections if b == n))}","['t = {n for n in c if n in required and all((a in s for (a, b) in connections if b == n))}']",1,
lite-transformer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lite-transformer/fairseq/file_utils.py,https://github.com/mit-han-lab/lite-transformer/tree/master/fairseq/file_utils.py,,read_set_from_file$303,"def read_set_from_file(filename):
    """"""
    Extract a de-duped collection (set) of text from a file.
    Expected file format is one item per line.
    """"""
    collection = set()
    with open(filename, 'r', encoding='utf-8') as file_:
        for line in file_:
            collection.add(line.rstrip())
    return collection","for line in file_:
    collection.add(line.rstrip())",collection = {line.rstrip() for line in file_},['collection = {line.rstrip() for line in file_}'],1,
pkuseg-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pkuseg-python/pkuseg/scorer.py,https://github.com/lancopku/pkuseg-python/tree/master/pkuseg/scorer.py,,getFscore$4,"def getFscore(goldTagList, resTagList, idx_to_chunk_tag):
    scoreList = []
    assert len(resTagList) == len(goldTagList)
    getNewTagList(idx_to_chunk_tag, goldTagList)
    getNewTagList(idx_to_chunk_tag, resTagList)
    goldChunkList = getChunks(goldTagList)
    resChunkList = getChunks(resTagList)
    gold_chunk = 0
    res_chunk = 0
    correct_chunk = 0
    for i in range(len(goldChunkList)):
        res = resChunkList[i]
        gold = goldChunkList[i]
        resChunkAry = res.split(Config.comma)
        tmp = []
        for t in resChunkAry:
            if len(t) > 0:
                tmp.append(t)
        resChunkAry = tmp
        goldChunkAry = gold.split(Config.comma)
        tmp = []
        for t in goldChunkAry:
            if len(t) > 0:
                tmp.append(t)
        goldChunkAry = tmp
        gold_chunk += len(goldChunkAry)
        res_chunk += len(resChunkAry)
        goldChunkSet = set()
        for im in goldChunkAry:
            goldChunkSet.add(im)
        for im in resChunkAry:
            if im in goldChunkSet:
                correct_chunk += 1
    pre = correct_chunk / res_chunk * 100
    rec = correct_chunk / gold_chunk * 100
    f1 = 0 if correct_chunk == 0 else 2 * pre * rec / (pre + rec)
    scoreList.append(f1)
    scoreList.append(pre)
    scoreList.append(rec)
    infoList = []
    infoList.append(gold_chunk)
    infoList.append(res_chunk)
    infoList.append(correct_chunk)
    return (scoreList, infoList)","for im in goldChunkAry:
    goldChunkSet.add(im)",goldChunkSet = {im for im in goldChunkAry},['goldChunkSet = {im for im in goldChunkAry}'],1,
redis-memory-analyzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/redis-memory-analyzer/rma/splitter.py,https://github.com/gamenet/redis-memory-analyzer/tree/master/rma/splitter.py,SimpleSplitter,unfold_to_list$83,"def unfold_to_list(tree, separator):
    res = set()
    for sub_tree in tree.values():
        for compound_key in dict_build(sub_tree):
            res.add(separator.join(compound_key))
    return res","for sub_tree in tree.values():
    for compound_key in dict_build(sub_tree):
        res.add(separator.join(compound_key))",res = {separator.join(compound_key) for sub_tree in tree.values() for compound_key in dict_build(sub_tree)},['res = {separator.join(compound_key) for sub_tree in tree.values() for compound_key in dict_build(sub_tree)}'],1,
nvim-completion-manager,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nvim-completion-manager/pythonx/cm_sources/cm_tmux.py,https://github.com/roxma/nvim-completion-manager/tree/master/pythonx/cm_sources/cm_tmux.py,Source,refresh_keyword$40,"def refresh_keyword(self):
    pat = re.compile(self._split_pattern)
    self._words = set()
    proc = subprocess.Popen(args=['tmux', 'list-window', '-F', '#{window_index}'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    (outs, errs) = proc.communicate(timeout=15)
    window_indices = outs.decode('utf-8')
    logger.info('list-window: %s', window_indices)
    panes = []
    for win_index in window_indices.strip().split('\n'):
        proc = subprocess.Popen(args=['tmux', 'list-panes', '-t', win_index, '-F', '#{pane_index}'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        (outs, errs) = proc.communicate(timeout=15)
        pane_ids = outs.decode('utf-8')
        for pane_id in pane_ids.strip().split('\n'):
            proc = subprocess.Popen(args=['tmux', 'capture-pane', '-p', '-t', '{}.{}'.format(win_index, pane_id)], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            (outs, errs) = proc.communicate(timeout=15)
            try:
                outs = outs.decode('utf-8')
                panes.append(outs)
            except Exception as ex:
                logger.exception('exception, failed to decode output, %s', ex)
                pass
    for pane in panes:
        for word in re.split(pat, pane):
            self._words.add(word)
    logger.info('keyword refresh complete, count: %s', len(self._words))","for pane in panes:
    for word in re.split(pat, pane):
        self._words.add(word)",,"['self._words = {word for pane in panes for word in re.split(pat, pane)}']",0,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/sysmod.py,https://github.com/saltstack/salt/tree/master/salt/modules/sysmod.py,,list_modules$369,"def list_modules(*args):
    """"""
    List the modules loaded on the minion

    .. versionadded:: 2015.5.0

    CLI Example:

    .. code-block:: bash

        salt '*' sys.list_modules

    Module names can be specified as globs.

    .. code-block:: bash

        salt '*' sys.list_modules 's*'

    """"""
    modules = set()
    if not args:
        for func in __salt__:
            modules.add(func.split('.')[0])
        return sorted(modules)
    for module in args:
        if '*' in module:
            for func in fnmatch.filter(__salt__, module):
                modules.add(func.split('.')[0])
        else:
            for func in __salt__:
                mod_test = func.split('.')[0]
                if mod_test == module:
                    modules.add(mod_test)
    return sorted(modules)","for module in args:
    if '*' in module:
        for func in fnmatch.filter(__salt__, module):
            modules.add(func.split('.')[0])
    else:
        for func in __salt__:
            mod_test = func.split('.')[0]
            if mod_test == module:
                modules.add(mod_test)","modules = {func.split('.')[0] for module in args for func in fnmatch.filter(__salt__, module) if '*' in module} | {func.split('.')[0] for module in args for func in __salt__ if func.split('.')[0] == module and '*' not in module}",Cannot refactor,2,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/sysmod.py,https://github.com/saltstack/salt/tree/master/salt/modules/sysmod.py,,list_modules$369,"def list_modules(*args):
    """"""
    List the modules loaded on the minion

    .. versionadded:: 2015.5.0

    CLI Example:

    .. code-block:: bash

        salt '*' sys.list_modules

    Module names can be specified as globs.

    .. code-block:: bash

        salt '*' sys.list_modules 's*'

    """"""
    modules = set()
    if not args:
        for func in __salt__:
            modules.add(func.split('.')[0])
        return sorted(modules)
    for module in args:
        if '*' in module:
            for func in fnmatch.filter(__salt__, module):
                modules.add(func.split('.')[0])
        else:
            for func in __salt__:
                mod_test = func.split('.')[0]
                if mod_test == module:
                    modules.add(mod_test)
    return sorted(modules)","for func in __salt__:
    modules.add(func.split('.')[0])",modules = {func.split('.')[0] for func in __salt__},["modules = {func.split('.')[0] for func in __salt__}"],1,
in-toto,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/in-toto/in_toto/verifylib.py,https://github.com/in-toto/in-toto/tree/master/in_toto/verifylib.py,,verify_modify_rule$800,"def verify_modify_rule(rule_pattern, artifacts_queue, materials, products):
    """"""
  <Purpose>
    Filters artifacts from artifacts queue using rule pattern and consumes them
    if they are in both the materials dict and in the products doct, but have
    different hashes, i.e. were modified.

  <Arguments>
    rule_pattern:
            A ""MODIFY"" rule pattern (see in_toto.rulelib).

    artifacts_queue:
            Not yet consumed artifacts (paths only).

    materials:
            All materials of an item (including hashes).

    products:
            All products of an item (including hashes).

  <Exceptions>
    None.

  <Side Effects>
    None.

  <Returns>
    The set of consumed artifacts (paths only).

  """"""
    filtered_artifacts = fnmatch.filter(artifacts_queue, rule_pattern)
    filtered_artifacts = set(filtered_artifacts) & set(materials.keys()) & set(products.keys())
    consumed = set()
    for path in filtered_artifacts:
        if materials[path] != products[path]:
            consumed.add(path)
    return consumed","for path in filtered_artifacts:
    if materials[path] != products[path]:
        consumed.add(path)",consumed = {path for path in filtered_artifacts if materials[path] != products[path]},['consumed = {path for path in filtered_artifacts if materials[path] != products[path]}'],1,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/webapps/galaxy/controllers/history.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/galaxy/controllers/history.py,HistoryController,permissions$644,"def permissions(self, trans, payload=None, **kwd):
    """"""
        Sets the permissions on a history.
        """"""
    history_id = kwd.get('id')
    if not history_id:
        return self.message_exception(trans, f'Invalid history id ({str(history_id)}) received')
    history = self.history_manager.get_owned(self.decode_id(history_id), trans.user, current_history=trans.history)
    if trans.request.method == 'GET':
        inputs = []
        all_roles = trans.user.all_roles()
        current_actions = history.default_permissions
        for (action_key, action) in trans.app.model.Dataset.permitted_actions.items():
            in_roles = set()
            for a in current_actions:
                if a.action == action.action:
                    in_roles.add(a.role)
            inputs.append({'type': 'select', 'multiple': True, 'optional': True, 'individual': True, 'name': action_key, 'label': action.action, 'help': action.description, 'options': [(role.name, trans.security.encode_id(role.id)) for role in set(all_roles)], 'value': [trans.security.encode_id(role.id) for role in in_roles]})
        return {'title': ""Change default dataset permissions for history '%s'"" % history.name, 'inputs': inputs}
    else:
        permissions = {}
        for (action_key, action) in trans.app.model.Dataset.permitted_actions.items():
            in_roles = payload.get(action_key) or []
            in_roles = [trans.sa_session.query(trans.app.model.Role).get(trans.security.decode_id(x)) for x in in_roles]
            permissions[trans.app.security_agent.get_action(action.action)] = in_roles
        trans.app.security_agent.history_set_default_permissions(history, permissions)
        return {'message': ""Default history '%s' dataset permissions have been changed."" % history.name}","for (action_key, action) in trans.app.model.Dataset.permitted_actions.items():
    in_roles = set()
    for a in current_actions:
        if a.action == action.action:
            in_roles.add(a.role)
    inputs.append({'type': 'select', 'multiple': True, 'optional': True, 'individual': True, 'name': action_key, 'label': action.action, 'help': action.description, 'options': [(role.name, trans.security.encode_id(role.id)) for role in set(all_roles)], 'value': [trans.security.encode_id(role.id) for role in in_roles]})","inputs = [{'type': 'select', 'multiple': True, 'optional': True, 'individual': True, 'name': action_key, 'label': action.action, 'help': action.description, 'options': [(role.name, trans.security.encode_id(role.id)) for role in set(all_roles)], 'value': [trans.security.encode_id(role.id) for role in {a.role for a in current_actions if a.action == action.action}]} for (action_key, action) in trans.app.model.Dataset.permitted_actions.items()]",Cannot refactor,2,
urh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/urh/src/urh/ui/views/TableView.py,https://github.com/jopohl/urh/tree/master/src/urh/ui/views/TableView.py,TableView,selected_rows$89,"def selected_rows(self):
    rows = set()
    for index in self.selectionModel().selectedIndexes():
        rows.add(index.row())
    return sorted(rows)","for index in self.selectionModel().selectedIndexes():
    rows.add(index.row())",rows = {index.row() for index in self.selectionModel().selectedIndexes()},['rows = {index.row() for index in self.selectionModel().selectedIndexes()}'],1,
neutron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/agent/l3/dvr_local_router.py,https://github.com/openstack/neutron/tree/master/neutron/agent/l3/dvr_local_router.py,DvrLocalRouter,_delete_arp_cache_for_internal_port$306,"def _delete_arp_cache_for_internal_port(self, subnet_id):
    """"""Function to delete the cached arp entries.""""""
    arp_delete = set()
    for arp_entry in self._pending_arp_set:
        if subnet_id == arp_entry.subnet_id:
            arp_delete.add(arp_entry)
    self._pending_arp_set -= arp_delete","for arp_entry in self._pending_arp_set:
    if subnet_id == arp_entry.subnet_id:
        arp_delete.add(arp_entry)",arp_delete = {arp_entry for arp_entry in self._pending_arp_set if subnet_id == arp_entry.subnet_id},['arp_delete = {arp_entry for arp_entry in self._pending_arp_set if subnet_id == arp_entry.subnet_id}'],1,
viztracer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/viztracer/tests/test_multiprocess.py,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_multiprocess.py,TestMultiprocessing,test_ignore_multiprosessing$340,"def test_ignore_multiprosessing(self):

    def check_func(data):
        pids = set()
        for entry in data['traceEvents']:
            pids.add(entry['pid'])
        self.assertEqual(len(pids), 1)
    self.template(['viztracer', '-o', 'result.json', '--ignore_multiprocess', 'cmdline_test.py'], expected_output_file='result.json', script=file_multiprocessing, check_func=check_func, concurrency='multiprocessing')","for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']},["pids = {entry['pid'] for entry in data['traceEvents']}"],1,
mailinabox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mailinabox/management/mailconfig.py,https://github.com/mail-in-a-box/mailinabox/tree/master/management/mailconfig.py,,get_admins$179,"def get_admins(env):
    users = set()
    for domain in get_mail_users_ex(env):
        for user in domain['users']:
            if 'admin' in user['privileges']:
                users.add(user['email'])
    return users","for domain in get_mail_users_ex(env):
    for user in domain['users']:
        if 'admin' in user['privileges']:
            users.add(user['email'])",users = {user['email'] for domain in get_mail_users_ex(env) for user in domain['users'] if 'admin' in user['privileges']},["users = {user['email'] for domain in get_mail_users_ex(env) for user in domain['users'] if 'admin' in user['privileges']}"],1,
shuup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/modules/orders/json_order_creator.py,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/orders/json_order_creator.py,JsonOrderCreator,get_removed_product_ids$438,"def get_removed_product_ids(self, state, order_to_update):
    """"""
        Collect product ids for products which were removed from the order.

        :param state: State dictionary.
        :type state: dict
        :param order_to_update: Order object to edit.
        :type order_to_update: shuup.core.models.Order
        :return: set
        """"""
    current_lines = state.get('lines', [])
    current_product_ids = set()
    for line in current_lines:
        if line['type'] == 'product' and line['product'] is not None:
            current_product_ids.add(line['product']['id'])
    old_prod_ids = set()
    for line in order_to_update.lines.exclude(product_id=None):
        old_prod_ids.add(line.product.id)
    return old_prod_ids - current_product_ids","for line in current_lines:
    if line['type'] == 'product' and line['product'] is not None:
        current_product_ids.add(line['product']['id'])",current_product_ids = {line['product']['id'] for line in current_lines if line['type'] == 'product' and line['product'] is not None},["current_product_ids = {line['product']['id'] for line in current_lines if line['type'] == 'product' and line['product'] is not None}"],1,
shuup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/modules/orders/json_order_creator.py,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/orders/json_order_creator.py,JsonOrderCreator,get_removed_product_ids$438,"def get_removed_product_ids(self, state, order_to_update):
    """"""
        Collect product ids for products which were removed from the order.

        :param state: State dictionary.
        :type state: dict
        :param order_to_update: Order object to edit.
        :type order_to_update: shuup.core.models.Order
        :return: set
        """"""
    current_lines = state.get('lines', [])
    current_product_ids = set()
    for line in current_lines:
        if line['type'] == 'product' and line['product'] is not None:
            current_product_ids.add(line['product']['id'])
    old_prod_ids = set()
    for line in order_to_update.lines.exclude(product_id=None):
        old_prod_ids.add(line.product.id)
    return old_prod_ids - current_product_ids","for line in order_to_update.lines.exclude(product_id=None):
    old_prod_ids.add(line.product.id)",old_prod_ids = {line.product.id for line in order_to_update.lines.exclude(product_id=None)},['old_prod_ids = {line.product.id for line in order_to_update.lines.exclude(product_id=None)}'],1,
FARM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FARM/farm/file_utils.py,https://github.com/deepset-ai/FARM/tree/master/farm/file_utils.py,,read_set_from_file$301,"def read_set_from_file(filename):
    """"""
    Extract a de-duped collection (set) of text from a file.
    Expected file format is one item per line.
    """"""
    collection = set()
    with open(filename, 'r', encoding='utf-8') as file_:
        for line in file_:
            collection.add(line.rstrip())
    return collection","for line in file_:
    collection.add(line.rstrip())",collection = {line.rstrip() for line in file_},['collection = {line.rstrip() for line in file_}'],1,
spiderfoot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spiderfoot/sflib.py,https://github.com/smicallef/spiderfoot/tree/master//sflib.py,SpiderFoot,parseEmails$1240,"def parseEmails(self, data: str) -> list:
    """"""Extract all email addresses within the supplied content.

        Args:
            data (str): text to search for email addresses

        Returns:
            list: list of email addresses
        """"""
    if not isinstance(data, str):
        return list()
    emails = set()
    matches = re.findall('([\\%a-zA-Z\\.0-9_\\-\\+]+@[a-zA-Z\\.0-9\\-]+\\.[a-zA-Z\\.0-9\\-]+)', data)
    for match in matches:
        if self.validEmail(match):
            emails.add(match)
    return list(emails)","for match in matches:
    if self.validEmail(match):
        emails.add(match)",emails = {match for match in matches if self.validEmail(match)},['emails = {match for match in matches if self.validEmail(match)}'],1,
goatools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/goatools/versioneer.py,https://github.com/tanghaibao/goatools/tree/master//versioneer.py,,do_setup$1697,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print('Adding sample versioneer config to setup.cfg', file=sys.stderr)
            with open(os.path.join(root, 'setup.cfg'), 'a') as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1
    print(' creating %s' % cfg.versionfile_source)
    with open(cfg.versionfile_source, 'w') as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {'DOLLAR': '$', 'STYLE': cfg.style, 'TAG_PREFIX': cfg.tag_prefix, 'PARENTDIR_PREFIX': cfg.parentdir_prefix, 'VERSIONFILE_SOURCE': cfg.versionfile_source})
    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), '__init__.py')
    if os.path.exists(ipy):
        try:
            with open(ipy, 'r') as f:
                old = f.read()
        except EnvironmentError:
            old = ''
        if INIT_PY_SNIPPET not in old:
            print(' appending to %s' % ipy)
            with open(ipy, 'a') as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print(' %s unmodified' % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None
    manifest_in = os.path.join(root, 'MANIFEST.in')
    simple_includes = set()
    try:
        with open(manifest_in, 'r') as f:
            for line in f:
                if line.startswith('include '):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    if 'versioneer.py' not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, 'a') as f:
            f.write('include versioneer.py\n')
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" % cfg.versionfile_source)
        with open(manifest_in, 'a') as f:
            f.write('include %s\n' % cfg.versionfile_source)
    else:
        print(' versionfile_source already in MANIFEST.in')
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f for include in line.split()[1:] if line.startswith('include ')},["simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}"],0,
FreeCAD_assembly3,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FreeCAD_assembly3/freecad/asm3/assembly.py,https://github.com/realthunder/FreeCAD_assembly3/tree/master/freecad/asm3/assembly.py,AsmElementGroup,onChildLabelChange$3305,"def onChildLabelChange(self, obj, label):
    names = set()
    label = label.replace('.', '_')
    for o in flattenGroup(self.Object):
        if o != obj:
            names.add(o.Label)
    if label not in names:
        return label
    for (i, c) in enumerate(reversed(label)):
        if not c.isdigit():
            if i:
                label = label[:-i]
            break
    i = 0
    while True:
        i = i + 1
        newLabel = '{}{:03d}'.format(label, i)
        if newLabel != obj.Label and newLabel not in names:
            return newLabel
    return label","for o in flattenGroup(self.Object):
    if o != obj:
        names.add(o.Label)",names = {o.Label for o in flattenGroup(self.Object) if o != obj},['names = {o.Label for o in flattenGroup(self.Object) if o != obj}'],1,
detection-rules,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/detection-rules/detection_rules/rule_formatter.py,https://github.com/elastic/detection-rules/tree/master/detection_rules/rule_formatter.py,,get_preserved_fmt_fields$27,"def get_preserved_fmt_fields():
    from .rule import BaseRuleData
    preserved_keys = set()
    for field in dataclasses.fields(BaseRuleData):
        if field.type in (definitions.Markdown, typing.Optional[definitions.Markdown]):
            preserved_keys.add(field.metadata.get('data_key', field.name))
    return preserved_keys","for field in dataclasses.fields(BaseRuleData):
    if field.type in (definitions.Markdown, typing.Optional[definitions.Markdown]):
        preserved_keys.add(field.metadata.get('data_key', field.name))","preserved_keys = {field.metadata.get('data_key', field.name) for field in dataclasses.fields(BaseRuleData) if field.type in (definitions.Markdown, typing.Optional[definitions.Markdown])}","[""preserved_keys = {field.metadata.get('data_key', field.name) for field in dataclasses.fields(BaseRuleData) if field.type in (definitions.Markdown, typing.Optional[definitions.Markdown])}""]",1,
networkx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/networkx/networkx/algorithms/approximation/tests/test_treewidth.py,https://github.com/networkx/networkx/tree/master/networkx/algorithms/approximation/tests/test_treewidth.py,TestTreewidthMinDegree,test_heuristic_abort$108,"def test_heuristic_abort(self):
    """"""Test heuristic abort condition for fully connected graph""""""
    graph = {}
    for u in self.complete:
        graph[u] = set()
        for v in self.complete[u]:
            if u != v:
                graph[u].add(v)
    deg_heuristic = MinDegreeHeuristic(graph)
    node = deg_heuristic.best_node(graph)
    if node is None:
        pass
    else:
        assert False","for u in self.complete:
    graph[u] = set()
    for v in self.complete[u]:
        if u != v:
            graph[u].add(v)",graph = {u: {v for v in self.complete[u] if u != v} for u in self.complete},Cannot refactor,2,
modin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/modin/versioneer.py,https://github.com/modin-project/modin/tree/master//versioneer.py,,do_setup$1697,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print('Adding sample versioneer config to setup.cfg', file=sys.stderr)
            with open(os.path.join(root, 'setup.cfg'), 'a') as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1
    print(' creating %s' % cfg.versionfile_source)
    with open(cfg.versionfile_source, 'w') as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {'DOLLAR': '$', 'STYLE': cfg.style, 'TAG_PREFIX': cfg.tag_prefix, 'PARENTDIR_PREFIX': cfg.parentdir_prefix, 'VERSIONFILE_SOURCE': cfg.versionfile_source})
    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), '__init__.py')
    if os.path.exists(ipy):
        try:
            with open(ipy, 'r') as f:
                old = f.read()
        except EnvironmentError:
            old = ''
        if INIT_PY_SNIPPET not in old:
            print(' appending to %s' % ipy)
            with open(ipy, 'a') as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print(' %s unmodified' % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None
    manifest_in = os.path.join(root, 'MANIFEST.in')
    simple_includes = set()
    try:
        with open(manifest_in, 'r') as f:
            for line in f:
                if line.startswith('include '):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    if 'versioneer.py' not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, 'a') as f:
            f.write('include versioneer.py\n')
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" % cfg.versionfile_source)
        with open(manifest_in, 'a') as f:
            f.write('include %s\n' % cfg.versionfile_source)
    else:
        print(' versionfile_source already in MANIFEST.in')
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f for include in line.split()[1:] if line.startswith('include ')},["simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}"],0,
aiokafka,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aiokafka/aiokafka/consumer/subscription_state.py,https://github.com/aio-libs/aiokafka/tree/master/aiokafka/consumer/subscription_state.py,SubscriptionState,paused_partitions$283,"def paused_partitions(self) -> Set[TopicPartition]:
    res = set()
    for tp in self.assigned_partitions():
        if self._assigned_state(tp).paused:
            res.add(tp)
    return res","for tp in self.assigned_partitions():
    if self._assigned_state(tp).paused:
        res.add(tp)",res = {tp for tp in self.assigned_partitions() if self._assigned_state(tp).paused},['res = {tp for tp in self.assigned_partitions() if self._assigned_state(tp).paused}'],1,
featuretools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/featuretools/featuretools/synthesis/deep_feature_synthesis.py,https://github.com/alteryx/featuretools/tree/master/featuretools/synthesis/deep_feature_synthesis.py,DeepFeatureSynthesis,__init__$131,"def __init__(self, target_dataframe_name, entityset, agg_primitives=None, trans_primitives=None, where_primitives=None, groupby_trans_primitives=None, max_depth=2, max_features=-1, allowed_paths=None, ignore_dataframes=None, ignore_columns=None, primitive_options=None, seed_features=None, drop_contains=None, drop_exact=None, where_stacking_limit=1):
    if target_dataframe_name not in entityset.dataframe_dict:
        es_name = entityset.id or 'entity set'
        msg = 'Provided target dataframe %s does not exist in %s' % (target_dataframe_name, es_name)
        raise KeyError(msg)
    feature_cache.clear_all()
    feature_cache.enabled = True
    if max_depth == -1:
        max_depth = None
    if len(entityset.dataframe_dict) == 1 and (max_depth is None or max_depth > 1):
        warnings.warn('Only one dataframe in entityset, changing max_depth to 1 since deeper features cannot be created')
        max_depth = 1
    self.max_depth = max_depth
    self.max_features = max_features
    self.allowed_paths = allowed_paths
    if self.allowed_paths:
        self.allowed_paths = set()
        for path in allowed_paths:
            self.allowed_paths.add(tuple(path))
    if ignore_dataframes is None:
        self.ignore_dataframes = set()
    else:
        if not isinstance(ignore_dataframes, list):
            raise TypeError('ignore_dataframes must be a list')
        assert target_dataframe_name not in ignore_dataframes, ""Can't ignore target_dataframe!""
        self.ignore_dataframes = set(ignore_dataframes)
    self.ignore_columns = defaultdict(set)
    if ignore_columns is not None:
        if not all((isinstance(i, str) for i in ignore_columns.keys())) or not all((isinstance(i, list) for i in ignore_columns.values())):
            raise TypeError('ignore_columns should be dict[str -> list]')
        elif not all((all((isinstance(v, str) for v in value)) for value in ignore_columns.values())):
            raise TypeError('list values should be of type str')
        for (df_name, cols) in ignore_columns.items():
            self.ignore_columns[df_name] = set(cols)
    self.target_dataframe_name = target_dataframe_name
    self.es = entityset
    for library in Library:
        if library.value == self.es.dataframe_type:
            df_library = library
            break
    aggregation_primitive_dict = primitives.get_aggregation_primitives()
    transform_primitive_dict = primitives.get_transform_primitives()
    if agg_primitives is None:
        agg_primitives = [p for p in primitives.get_default_aggregation_primitives() if df_library in p.compatibility]
    self.agg_primitives = sorted([check_primitive(p, 'aggregation', aggregation_primitive_dict, transform_primitive_dict) for p in agg_primitives])
    if trans_primitives is None:
        trans_primitives = [p for p in primitives.get_default_transform_primitives() if df_library in p.compatibility]
    self.trans_primitives = sorted([check_primitive(p, 'transform', aggregation_primitive_dict, transform_primitive_dict) for p in trans_primitives])
    if where_primitives is None:
        where_primitives = [primitives.Count]
    self.where_primitives = sorted([check_primitive(p, 'where', aggregation_primitive_dict, transform_primitive_dict) for p in where_primitives])
    if groupby_trans_primitives is None:
        groupby_trans_primitives = []
    self.groupby_trans_primitives = sorted([check_primitive(p, 'groupby transform', aggregation_primitive_dict, transform_primitive_dict) for p in groupby_trans_primitives])
    if primitive_options is None:
        primitive_options = {}
    all_primitives = self.trans_primitives + self.agg_primitives + self.where_primitives + self.groupby_trans_primitives
    bad_primitives = [prim.name for prim in all_primitives if df_library not in prim.compatibility]
    if bad_primitives:
        msg = 'Selected primitives are incompatible with {} EntitySets: {}'
        raise ValueError(msg.format(df_library.value, ', '.join(bad_primitives)))
    (self.primitive_options, self.ignore_dataframes, self.ignore_columns) = generate_all_primitive_options(all_primitives, primitive_options, self.ignore_dataframes, self.ignore_columns, self.es)
    self.seed_features = sorted(seed_features or [], key=lambda f: f.unique_name())
    self.drop_exact = drop_exact or []
    self.drop_contains = drop_contains or []
    self.where_stacking_limit = where_stacking_limit","for path in allowed_paths:
    self.allowed_paths.add(tuple(path))",entries = {tuple(path) for path in allowed_paths},['self.allowed_paths = {tuple(path) for path in allowed_paths}'],0,
aws-parallelcluster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-parallelcluster/util/rollback_s3_objects.py,https://github.com/aws/aws-parallelcluster/tree/master/util/rollback_s3_objects.py,,main$88,"def main():
    args = _parse_args()
    logging.info('Parsed cli args: %s', vars(args))
    regions = set()
    with open(args.rollback_file_path, encoding='utf-8') as rollback_file:
        rollback_data = json.load(rollback_file)
        for bucket in rollback_data.keys():
            regions.add(rollback_data[bucket]['region'])
    sts_credentials = retrieve_sts_credentials(args.credentials, PARTITION_TO_MAIN_REGION[args.partition], regions)
    execute_rollback(args.rollback_file_path, sts_credentials, args.deploy)","for bucket in rollback_data.keys():
    regions.add(rollback_data[bucket]['region'])",regions = {rollback_data[bucket]['region'] for bucket in rollback_data.keys()},["regions = {rollback_data[bucket]['region'] for bucket in rollback_data.keys()}"],1,
viztracer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/viztracer/tests/test_regression.py,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_regression.py,TestFunctionArg,test_functionarg$79,"def test_functionarg(self):

    def f(n):
        tracer.add_func_args('input', n)
        if n < 2:
            return 1
        return f(n - 1) + f(n - 2)
    tracer = VizTracer(verbose=0)
    tracer.start()
    f(5)
    tracer.stop()
    tracer.parse()
    inputs = set()
    for d in tracer.data['traceEvents']:
        if d['ph'] == 'X':
            inputs.add(d['args']['input'])
    self.assertEqual(inputs, set([0, 1, 2, 3, 4, 5]))","for d in tracer.data['traceEvents']:
    if d['ph'] == 'X':
        inputs.add(d['args']['input'])",inputs = {d['args']['input'] for d in tracer.data['traceEvents'] if d['ph'] == 'X'},["inputs = {d['args']['input'] for d in tracer.data['traceEvents'] if d['ph'] == 'X'}"],1,
python-driver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-driver/tests/integration/long/test_loadbalancingpolicies.py,https://github.com/datastax/python-driver/tree/master/tests/integration/long/test_loadbalancingpolicies.py,LoadBalancingPolicyTests,test_dc_aware_roundrobin_one_remote_host$329,"def test_dc_aware_roundrobin_one_remote_host(self):
    use_multidc([2, 2])
    keyspace = 'test_dc_aware_roundrobin_one_remote_host'
    (cluster, session) = self._cluster_session_with_lbp(DCAwareRoundRobinPolicy('dc2', used_hosts_per_remote_dc=1))
    self.addCleanup(cluster.shutdown)
    self._wait_for_nodes_up(range(1, 5))
    create_schema(cluster, session, keyspace, replication_strategy=[2, 2])
    self._insert(session, keyspace)
    self._query(session, keyspace)
    self.coordinator_stats.assert_query_count_equals(self, 1, 0)
    self.coordinator_stats.assert_query_count_equals(self, 2, 0)
    self.coordinator_stats.assert_query_count_equals(self, 3, 6)
    self.coordinator_stats.assert_query_count_equals(self, 4, 6)
    self.coordinator_stats.reset_counts()
    bootstrap(5, 'dc1')
    self._wait_for_nodes_up([5])
    self._query(session, keyspace)
    self.coordinator_stats.assert_query_count_equals(self, 1, 0)
    self.coordinator_stats.assert_query_count_equals(self, 2, 0)
    self.coordinator_stats.assert_query_count_equals(self, 3, 6)
    self.coordinator_stats.assert_query_count_equals(self, 4, 6)
    self.coordinator_stats.assert_query_count_equals(self, 5, 0)
    self.coordinator_stats.reset_counts()
    decommission(3)
    decommission(4)
    self._wait_for_nodes_down([3, 4])
    self._query(session, keyspace)
    self.coordinator_stats.assert_query_count_equals(self, 3, 0)
    self.coordinator_stats.assert_query_count_equals(self, 4, 0)
    responses = set()
    for node in [1, 2, 5]:
        responses.add(self.coordinator_stats.get_query_count(node))
    self.assertEqual(set([0, 0, 12]), responses)
    self.coordinator_stats.reset_counts()
    decommission(5)
    self._wait_for_nodes_down([5])
    self._query(session, keyspace)
    self.coordinator_stats.assert_query_count_equals(self, 3, 0)
    self.coordinator_stats.assert_query_count_equals(self, 4, 0)
    self.coordinator_stats.assert_query_count_equals(self, 5, 0)
    responses = set()
    for node in [1, 2]:
        responses.add(self.coordinator_stats.get_query_count(node))
    self.assertEqual(set([0, 12]), responses)
    self.coordinator_stats.reset_counts()
    decommission(1)
    self._wait_for_nodes_down([1])
    self._query(session, keyspace)
    self.coordinator_stats.assert_query_count_equals(self, 1, 0)
    self.coordinator_stats.assert_query_count_equals(self, 2, 12)
    self.coordinator_stats.assert_query_count_equals(self, 3, 0)
    self.coordinator_stats.assert_query_count_equals(self, 4, 0)
    self.coordinator_stats.assert_query_count_equals(self, 5, 0)
    self.coordinator_stats.reset_counts()
    force_stop(2)
    try:
        self._query(session, keyspace)
        self.fail()
    except NoHostAvailable:
        pass","for node in [1, 2, 5]:
    responses.add(self.coordinator_stats.get_query_count(node))","responses = {self.coordinator_stats.get_query_count(node) for node in [1, 2, 5]}","['responses = {self.coordinator_stats.get_query_count(node) for node in [1, 2, 5]}']",1,
python-driver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-driver/tests/integration/long/test_loadbalancingpolicies.py,https://github.com/datastax/python-driver/tree/master/tests/integration/long/test_loadbalancingpolicies.py,LoadBalancingPolicyTests,test_dc_aware_roundrobin_one_remote_host$329,"def test_dc_aware_roundrobin_one_remote_host(self):
    use_multidc([2, 2])
    keyspace = 'test_dc_aware_roundrobin_one_remote_host'
    (cluster, session) = self._cluster_session_with_lbp(DCAwareRoundRobinPolicy('dc2', used_hosts_per_remote_dc=1))
    self.addCleanup(cluster.shutdown)
    self._wait_for_nodes_up(range(1, 5))
    create_schema(cluster, session, keyspace, replication_strategy=[2, 2])
    self._insert(session, keyspace)
    self._query(session, keyspace)
    self.coordinator_stats.assert_query_count_equals(self, 1, 0)
    self.coordinator_stats.assert_query_count_equals(self, 2, 0)
    self.coordinator_stats.assert_query_count_equals(self, 3, 6)
    self.coordinator_stats.assert_query_count_equals(self, 4, 6)
    self.coordinator_stats.reset_counts()
    bootstrap(5, 'dc1')
    self._wait_for_nodes_up([5])
    self._query(session, keyspace)
    self.coordinator_stats.assert_query_count_equals(self, 1, 0)
    self.coordinator_stats.assert_query_count_equals(self, 2, 0)
    self.coordinator_stats.assert_query_count_equals(self, 3, 6)
    self.coordinator_stats.assert_query_count_equals(self, 4, 6)
    self.coordinator_stats.assert_query_count_equals(self, 5, 0)
    self.coordinator_stats.reset_counts()
    decommission(3)
    decommission(4)
    self._wait_for_nodes_down([3, 4])
    self._query(session, keyspace)
    self.coordinator_stats.assert_query_count_equals(self, 3, 0)
    self.coordinator_stats.assert_query_count_equals(self, 4, 0)
    responses = set()
    for node in [1, 2, 5]:
        responses.add(self.coordinator_stats.get_query_count(node))
    self.assertEqual(set([0, 0, 12]), responses)
    self.coordinator_stats.reset_counts()
    decommission(5)
    self._wait_for_nodes_down([5])
    self._query(session, keyspace)
    self.coordinator_stats.assert_query_count_equals(self, 3, 0)
    self.coordinator_stats.assert_query_count_equals(self, 4, 0)
    self.coordinator_stats.assert_query_count_equals(self, 5, 0)
    responses = set()
    for node in [1, 2]:
        responses.add(self.coordinator_stats.get_query_count(node))
    self.assertEqual(set([0, 12]), responses)
    self.coordinator_stats.reset_counts()
    decommission(1)
    self._wait_for_nodes_down([1])
    self._query(session, keyspace)
    self.coordinator_stats.assert_query_count_equals(self, 1, 0)
    self.coordinator_stats.assert_query_count_equals(self, 2, 12)
    self.coordinator_stats.assert_query_count_equals(self, 3, 0)
    self.coordinator_stats.assert_query_count_equals(self, 4, 0)
    self.coordinator_stats.assert_query_count_equals(self, 5, 0)
    self.coordinator_stats.reset_counts()
    force_stop(2)
    try:
        self._query(session, keyspace)
        self.fail()
    except NoHostAvailable:
        pass","for node in [1, 2]:
    responses.add(self.coordinator_stats.get_query_count(node))","responses = {self.coordinator_stats.get_query_count(node) for node in [1, 2]}","['responses = {self.coordinator_stats.get_query_count(node) for node in [1, 2]}']",1,
Mobile-Security-Framework-MobSF,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mobile-Security-Framework-MobSF/mobsf/MobSF/utils.py,https://github.com/MobSF/Mobile-Security-Framework-MobSF/tree/master/mobsf/MobSF/utils.py,,find_process_by$315,"def find_process_by(name):
    """"""Return a set of process path matching name.""""""
    proc = set()
    for p in psutil.process_iter(attrs=['name']):
        if name == p.info['name']:
            proc.add(p.exe())
    return proc","for p in psutil.process_iter(attrs=['name']):
    if name == p.info['name']:
        proc.add(p.exe())",proc = {p.exe() for p in psutil.process_iter(attrs=['name']) if name == p.info['name']},["proc = {p.exe() for p in psutil.process_iter(attrs=['name']) if name == p.info['name']}"],1,
pywikibot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pywikibot/scripts/maintenance/wikimedia_sites.py,https://github.com/wikimedia/pywikibot/tree/master/scripts/maintenance/wikimedia_sites.py,,if_main_my$94,"if __name__ == '__main__':
    fam = set()
    for arg in pywikibot.handle_args():
        if arg in families_list:
            fam.add(arg)
    update_family(fam)","for arg in pywikibot.handle_args():
    if arg in families_list:
        fam.add(arg)",fam = {arg for arg in pywikibot.handle_args() if arg in families_list},['fam = {arg for arg in pywikibot.handle_args() if arg in families_list}'],1,
KILT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/KILT/kilt/readers/fid/preprocess.py,https://github.com/facebookresearch/KILT/tree/master/kilt/readers/fid/preprocess.py,,convert_kilt$13,"def convert_kilt(inputpath, outputpath):
    data = []
    inputdata = open(inputpath, 'r')
    for example in tqdm(inputdata):
        d = {}
        ex = json.loads(example)
        d['question'] = ex['input']
        answers = set()
        for a in ex['output']:
            if 'answer' in a:
                answers.add(a['answer'])
        d['answers'] = list(answers)
        d['id'] = ex['id']
        passages = []
        for c in ex['output'][0]['provenance']:
            p = {'text': c['text'], 'title': ''}
            if 'wikipedia_title' in c:
                p['title'] = c['wikipedia_title']
            if 'wikipedia_id' in c:
                p['wikipedia_id'] = c['wikipedia_id']
            passages.append(p)
        d['ctxs'] = passages
        data.append(d)
    with open(outputpath, 'w') as fout:
        json.dump(data, fout)","for example in tqdm(inputdata):
    d = {}
    ex = json.loads(example)
    d['question'] = ex['input']
    answers = set()
    for a in ex['output']:
        if 'answer' in a:
            answers.add(a['answer'])
    d['answers'] = list(answers)
    d['id'] = ex['id']
    passages = []
    for c in ex['output'][0]['provenance']:
        p = {'text': c['text'], 'title': ''}
        if 'wikipedia_title' in c:
            p['title'] = c['wikipedia_title']
        if 'wikipedia_id' in c:
            p['wikipedia_id'] = c['wikipedia_id']
        passages.append(p)
    d['ctxs'] = passages
    data.append(d)","data = [{'question': json.loads(example)['input'], 'answers': list({a['answer'] for a in json.loads(example)['output'] if 'answer' in a}), 'id': json.loads(example)['id'], 'ctxs': [{'text': c['text'], 'title': c.get('wikipedia_title', ''), 'wikipedia_id': c.get('wikipedia_id', '')} for c in json.loads(example)['output'][0]['provenance']]} for example in tqdm(inputdata)]",Cannot refactor,2,
Malt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Malt/BlenderMalt/MaltNodes.py,https://github.com/bnpr/Malt/tree/master/BlenderMalt/MaltNodes.py,,preload_menus$1015,"def preload_menus(structs, functions):
    files = set()
    for (name, struct) in structs.items():
        files.add(struct['file'])
    for file in files:
        get_structs_menu(file)
    files = set()
    for (name, function) in functions.items():
        files.add(function['file'])
    for file in files:
        get_functions_menu(file)","for (name, struct) in structs.items():
    files.add(struct['file'])","files = {struct['file'] for (name, struct) in structs.items()}","[""files = {struct['file'] for (name, struct) in structs.items()}""]",1,
Malt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Malt/BlenderMalt/MaltNodes.py,https://github.com/bnpr/Malt/tree/master/BlenderMalt/MaltNodes.py,,preload_menus$1015,"def preload_menus(structs, functions):
    files = set()
    for (name, struct) in structs.items():
        files.add(struct['file'])
    for file in files:
        get_structs_menu(file)
    files = set()
    for (name, function) in functions.items():
        files.add(function['file'])
    for file in files:
        get_functions_menu(file)","for (name, function) in functions.items():
    files.add(function['file'])","files = {function['file'] for (name, function) in functions.items()}","[""files = {function['file'] for (name, function) in functions.items()}""]",1,
UNITER,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UNITER/train_vcr.py,https://github.com/ChenRocks/UNITER/tree/master//train_vcr.py,,main$116,"def main(opts):
    hvd.init()
    n_gpu = hvd.size()
    device = torch.device('cuda', hvd.local_rank())
    torch.cuda.set_device(hvd.local_rank())
    rank = hvd.rank()
    opts.rank = rank
    LOGGER.info('device: {} n_gpu: {}, rank: {}, 16-bits training: {}'.format(device, n_gpu, hvd.rank(), opts.fp16))
    if opts.gradient_accumulation_steps < 1:
        raise ValueError('Invalid gradient_accumulation_steps parameter: {}, should be >= 1'.format(opts.gradient_accumulation_steps))
    set_random_seed(opts.seed)
    all_img_dbs = ImageLmdbGroup(opts.conf_th, opts.max_bb, opts.min_bb, opts.num_bb, opts.compressed_db)
    LOGGER.info(f'Loading Train Dataset {opts.train_txt_dbs}, {opts.train_img_dbs}')
    train_datasets = []
    for (txt_path, img_path) in zip(opts.train_txt_dbs, opts.train_img_dbs):
        (img_db, img_db_gt) = load_img_feat(img_path, all_img_dbs, opts)
        qa_txt_db = VcrTxtTokLmdb(txt_path, opts.max_txt_len, task='qa')
        qar_txt_db = VcrTxtTokLmdb(txt_path, opts.max_txt_len, task='qar')
        train_datasets.append(VcrDataset(qa_txt_db, img_db_gt=img_db_gt, img_db=img_db))
        train_datasets.append(VcrDataset(qar_txt_db, img_db_gt=img_db_gt, img_db=img_db))
    train_dataset = ConcatDatasetWithLens(train_datasets)
    train_dataloader = build_dataloader(train_dataset, vcr_collate, True, opts)
    LOGGER.info(f'Loading Val Dataset {opts.val_txt_db}, {opts.val_img_db}')
    (val_img_db, val_img_db_gt) = load_img_feat(opts.val_img_db, all_img_dbs, opts)
    val_txt_db = VcrTxtTokLmdb(opts.val_txt_db, -1)
    val_dataset = VcrEvalDataset('val', val_txt_db, img_db=val_img_db, img_db_gt=val_img_db_gt)
    val_final_dataset = VcrEvalDataset('test', val_txt_db, img_db=val_img_db, img_db_gt=val_img_db_gt)
    val_dataloader = build_dataloader(val_dataset, vcr_eval_collate, False, opts)
    val_final_dataloader = build_dataloader(val_final_dataset, vcr_eval_collate, False, opts)
    if opts.checkpoint and opts.checkpoint_from == 'pretrain':
        checkpoint = torch.load(opts.checkpoint)
    else:
        checkpoint = {}
    all_dbs = opts.train_txt_dbs + [opts.val_txt_db]
    toker = json.load(open(f'{all_dbs[0]}/meta.json'))['bert']
    assert all((toker == json.load(open(f'{db}/meta.json'))['bert'] for db in all_dbs))
    model = UniterForVisualCommonsenseReasoning.from_pretrained(opts.model_config, checkpoint, img_dim=IMG_DIM)
    model.init_type_embedding()
    model.init_word_embedding(NUM_SPECIAL_TOKENS)
    if opts.checkpoint_from == 'vcr_pretrain':
        checkpoint = torch.load(opts.checkpoint)
        state_dict = checkpoint.get('model_state', checkpoint)
        matched_state_dict = {}
        unexpected_keys = set()
        missing_keys = set()
        for (name, param) in model.named_parameters():
            missing_keys.add(name)
        for (key, data) in state_dict.items():
            if key in missing_keys:
                matched_state_dict[key] = data
                missing_keys.remove(key)
            else:
                unexpected_keys.add(key)
        print('Unexpected_keys:', list(unexpected_keys))
        print('Missing_keys:', list(missing_keys))
        model.load_state_dict(matched_state_dict, strict=False)
    del checkpoint
    model.to(device)
    broadcast_tensors([p.data for p in model.parameters()], 0)
    set_dropout(model, opts.dropout)
    optimizer = build_optimizer(model, opts)
    (model, optimizer) = amp.initialize(model, optimizer, enabled=opts.fp16, opt_level='O2')
    global_step = 0
    if rank == 0:
        save_training_meta(opts)
        TB_LOGGER.create(join(opts.output_dir, 'log'))
        pbar = tqdm(total=opts.num_train_steps)
        model_saver = ModelSaver(join(opts.output_dir, 'ckpt'))
        os.makedirs(join(opts.output_dir, 'results'))
        add_log_to_file(join(opts.output_dir, 'log', 'log.txt'))
    else:
        LOGGER.disabled = True
        pbar = NoOp()
        model_saver = NoOp()
    LOGGER.info(f'***** Running training with {n_gpu} GPUs *****')
    LOGGER.info('  Num examples = %d', len(train_dataset) * hvd.size())
    LOGGER.info('  Batch size = %d', opts.train_batch_size)
    LOGGER.info('  Accumulate steps = %d', opts.gradient_accumulation_steps)
    LOGGER.info('  Num steps = %d', opts.num_train_steps)
    running_loss = RunningMeter('loss')
    model.train()
    n_examples = 0
    n_epoch = 0
    start = time()
    optimizer.zero_grad()
    optimizer.step()
    while True:
        for (step, batch) in enumerate(train_dataloader):
            n_examples += batch['input_ids'].size(0)
            loss = model(batch, compute_loss=True)
            loss = loss.mean()
            delay_unscale = (step + 1) % opts.gradient_accumulation_steps != 0
            with amp.scale_loss(loss, optimizer, delay_unscale=delay_unscale) as scaled_loss:
                scaled_loss.backward()
                if not delay_unscale:
                    grads = [p.grad.data for p in model.parameters() if p.requires_grad and p.grad is not None]
                    all_reduce_and_rescale_tensors(grads, float(1))
            running_loss(loss.item())
            if (step + 1) % opts.gradient_accumulation_steps == 0:
                global_step += 1
                lr_this_step = get_lr_sched(global_step, opts)
                for (i, param_group) in enumerate(optimizer.param_groups):
                    if i == 0 or i == 1:
                        param_group['lr'] = lr_this_step * opts.lr_mul
                    elif i == 2 or i == 3:
                        param_group['lr'] = lr_this_step
                    else:
                        raise ValueError()
                TB_LOGGER.add_scalar('lr', lr_this_step, global_step)
                TB_LOGGER.add_scalar('loss', running_loss.val, global_step)
                TB_LOGGER.step()
                if opts.grad_norm != -1:
                    grad_norm = clip_grad_norm_(amp.master_params(optimizer), opts.grad_norm)
                    TB_LOGGER.add_scalar('grad_norm', grad_norm, global_step)
                optimizer.step()
                optimizer.zero_grad()
                pbar.update(1)
                if global_step % 100 == 0:
                    LOGGER.info(f'============Step {global_step}=============')
                    tot_ex = sum(all_gather_list(n_examples))
                    ex_per_sec = int(tot_ex / (time() - start))
                    LOGGER.info(f'{tot_ex} examples trained at {ex_per_sec} ex/s')
                    TB_LOGGER.add_scalar('perf/ex_per_s', ex_per_sec, global_step)
                    LOGGER.info('===========================================')
                if global_step % opts.valid_steps == 0:
                    (val_log, results) = validate(model, val_dataloader)
                    TB_LOGGER.log_scaler_dict(val_log)
                    model_saver.save(model, global_step)
            if global_step >= opts.num_train_steps:
                break
        if global_step >= opts.num_train_steps:
            break
        n_epoch += 1
        LOGGER.info(f'finished {n_epoch} epochs')
    if global_step % opts.valid_steps != 0:
        (val_log, results) = validate(model, val_dataloader)
        TB_LOGGER.log_scaler_dict(val_log)
    (val_log, results) = validate(model, val_final_dataloader)
    with open(f'{opts.output_dir}/results/results_{global_step}_final_qa_qar_rank{rank}.json', 'w') as f:
        json.dump(results, f)
    TB_LOGGER.log_scaler_dict(val_log)
    model_saver.save(model, global_step)","for (name, param) in model.named_parameters():
    missing_keys.add(name)","missing_keys = {name for (name, param) in model.named_parameters()}","['missing_keys = {name for (name, param) in model.named_parameters()}']",1,
UNITER,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UNITER/train_vcr.py,https://github.com/ChenRocks/UNITER/tree/master//train_vcr.py,,main$116,"def main(opts):
    hvd.init()
    n_gpu = hvd.size()
    device = torch.device('cuda', hvd.local_rank())
    torch.cuda.set_device(hvd.local_rank())
    rank = hvd.rank()
    opts.rank = rank
    LOGGER.info('device: {} n_gpu: {}, rank: {}, 16-bits training: {}'.format(device, n_gpu, hvd.rank(), opts.fp16))
    if opts.gradient_accumulation_steps < 1:
        raise ValueError('Invalid gradient_accumulation_steps parameter: {}, should be >= 1'.format(opts.gradient_accumulation_steps))
    set_random_seed(opts.seed)
    all_img_dbs = ImageLmdbGroup(opts.conf_th, opts.max_bb, opts.min_bb, opts.num_bb, opts.compressed_db)
    LOGGER.info(f'Loading Train Dataset {opts.train_txt_dbs}, {opts.train_img_dbs}')
    train_datasets = []
    for (txt_path, img_path) in zip(opts.train_txt_dbs, opts.train_img_dbs):
        (img_db, img_db_gt) = load_img_feat(img_path, all_img_dbs, opts)
        qa_txt_db = VcrTxtTokLmdb(txt_path, opts.max_txt_len, task='qa')
        qar_txt_db = VcrTxtTokLmdb(txt_path, opts.max_txt_len, task='qar')
        train_datasets.append(VcrDataset(qa_txt_db, img_db_gt=img_db_gt, img_db=img_db))
        train_datasets.append(VcrDataset(qar_txt_db, img_db_gt=img_db_gt, img_db=img_db))
    train_dataset = ConcatDatasetWithLens(train_datasets)
    train_dataloader = build_dataloader(train_dataset, vcr_collate, True, opts)
    LOGGER.info(f'Loading Val Dataset {opts.val_txt_db}, {opts.val_img_db}')
    (val_img_db, val_img_db_gt) = load_img_feat(opts.val_img_db, all_img_dbs, opts)
    val_txt_db = VcrTxtTokLmdb(opts.val_txt_db, -1)
    val_dataset = VcrEvalDataset('val', val_txt_db, img_db=val_img_db, img_db_gt=val_img_db_gt)
    val_final_dataset = VcrEvalDataset('test', val_txt_db, img_db=val_img_db, img_db_gt=val_img_db_gt)
    val_dataloader = build_dataloader(val_dataset, vcr_eval_collate, False, opts)
    val_final_dataloader = build_dataloader(val_final_dataset, vcr_eval_collate, False, opts)
    if opts.checkpoint and opts.checkpoint_from == 'pretrain':
        checkpoint = torch.load(opts.checkpoint)
    else:
        checkpoint = {}
    all_dbs = opts.train_txt_dbs + [opts.val_txt_db]
    toker = json.load(open(f'{all_dbs[0]}/meta.json'))['bert']
    assert all((toker == json.load(open(f'{db}/meta.json'))['bert'] for db in all_dbs))
    model = UniterForVisualCommonsenseReasoning.from_pretrained(opts.model_config, checkpoint, img_dim=IMG_DIM)
    model.init_type_embedding()
    model.init_word_embedding(NUM_SPECIAL_TOKENS)
    if opts.checkpoint_from == 'vcr_pretrain':
        checkpoint = torch.load(opts.checkpoint)
        state_dict = checkpoint.get('model_state', checkpoint)
        matched_state_dict = {}
        unexpected_keys = set()
        missing_keys = set()
        for (name, param) in model.named_parameters():
            missing_keys.add(name)
        for (key, data) in state_dict.items():
            if key in missing_keys:
                matched_state_dict[key] = data
                missing_keys.remove(key)
            else:
                unexpected_keys.add(key)
        print('Unexpected_keys:', list(unexpected_keys))
        print('Missing_keys:', list(missing_keys))
        model.load_state_dict(matched_state_dict, strict=False)
    del checkpoint
    model.to(device)
    broadcast_tensors([p.data for p in model.parameters()], 0)
    set_dropout(model, opts.dropout)
    optimizer = build_optimizer(model, opts)
    (model, optimizer) = amp.initialize(model, optimizer, enabled=opts.fp16, opt_level='O2')
    global_step = 0
    if rank == 0:
        save_training_meta(opts)
        TB_LOGGER.create(join(opts.output_dir, 'log'))
        pbar = tqdm(total=opts.num_train_steps)
        model_saver = ModelSaver(join(opts.output_dir, 'ckpt'))
        os.makedirs(join(opts.output_dir, 'results'))
        add_log_to_file(join(opts.output_dir, 'log', 'log.txt'))
    else:
        LOGGER.disabled = True
        pbar = NoOp()
        model_saver = NoOp()
    LOGGER.info(f'***** Running training with {n_gpu} GPUs *****')
    LOGGER.info('  Num examples = %d', len(train_dataset) * hvd.size())
    LOGGER.info('  Batch size = %d', opts.train_batch_size)
    LOGGER.info('  Accumulate steps = %d', opts.gradient_accumulation_steps)
    LOGGER.info('  Num steps = %d', opts.num_train_steps)
    running_loss = RunningMeter('loss')
    model.train()
    n_examples = 0
    n_epoch = 0
    start = time()
    optimizer.zero_grad()
    optimizer.step()
    while True:
        for (step, batch) in enumerate(train_dataloader):
            n_examples += batch['input_ids'].size(0)
            loss = model(batch, compute_loss=True)
            loss = loss.mean()
            delay_unscale = (step + 1) % opts.gradient_accumulation_steps != 0
            with amp.scale_loss(loss, optimizer, delay_unscale=delay_unscale) as scaled_loss:
                scaled_loss.backward()
                if not delay_unscale:
                    grads = [p.grad.data for p in model.parameters() if p.requires_grad and p.grad is not None]
                    all_reduce_and_rescale_tensors(grads, float(1))
            running_loss(loss.item())
            if (step + 1) % opts.gradient_accumulation_steps == 0:
                global_step += 1
                lr_this_step = get_lr_sched(global_step, opts)
                for (i, param_group) in enumerate(optimizer.param_groups):
                    if i == 0 or i == 1:
                        param_group['lr'] = lr_this_step * opts.lr_mul
                    elif i == 2 or i == 3:
                        param_group['lr'] = lr_this_step
                    else:
                        raise ValueError()
                TB_LOGGER.add_scalar('lr', lr_this_step, global_step)
                TB_LOGGER.add_scalar('loss', running_loss.val, global_step)
                TB_LOGGER.step()
                if opts.grad_norm != -1:
                    grad_norm = clip_grad_norm_(amp.master_params(optimizer), opts.grad_norm)
                    TB_LOGGER.add_scalar('grad_norm', grad_norm, global_step)
                optimizer.step()
                optimizer.zero_grad()
                pbar.update(1)
                if global_step % 100 == 0:
                    LOGGER.info(f'============Step {global_step}=============')
                    tot_ex = sum(all_gather_list(n_examples))
                    ex_per_sec = int(tot_ex / (time() - start))
                    LOGGER.info(f'{tot_ex} examples trained at {ex_per_sec} ex/s')
                    TB_LOGGER.add_scalar('perf/ex_per_s', ex_per_sec, global_step)
                    LOGGER.info('===========================================')
                if global_step % opts.valid_steps == 0:
                    (val_log, results) = validate(model, val_dataloader)
                    TB_LOGGER.log_scaler_dict(val_log)
                    model_saver.save(model, global_step)
            if global_step >= opts.num_train_steps:
                break
        if global_step >= opts.num_train_steps:
            break
        n_epoch += 1
        LOGGER.info(f'finished {n_epoch} epochs')
    if global_step % opts.valid_steps != 0:
        (val_log, results) = validate(model, val_dataloader)
        TB_LOGGER.log_scaler_dict(val_log)
    (val_log, results) = validate(model, val_final_dataloader)
    with open(f'{opts.output_dir}/results/results_{global_step}_final_qa_qar_rank{rank}.json', 'w') as f:
        json.dump(results, f)
    TB_LOGGER.log_scaler_dict(val_log)
    model_saver.save(model, global_step)","for (key, data) in state_dict.items():
    if key in missing_keys:
        matched_state_dict[key] = data
        missing_keys.remove(key)
    else:
        unexpected_keys.add(key)","unexpected_keys = {key for (key, data) in state_dict.items() if key not in missing_keys}",Cannot refactor,2,
prjxray,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/fuzzers/060-bram-cascades/top.py,https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/060-bram-cascades/top.py,,random_sdp_bram$120,"def random_sdp_bram(luts, name, modules, lines):
    sdp_choices = set()
    for width in (1, 2, 4, 8, 16, 18, 32, 36):
        sdp_choices.add((width, (1, max_address_bits(width))))
    for nbram in range(2, MAX_BRAM + 1):
        sdp_choices.add((nbram * 32, (1, max_address_bits(nbram * 32))))
        sdp_choices.add((nbram * 36, (1, max_address_bits(nbram * 36))))
        sdp_choices.add((nbram * 16, (1, max_address_bits(nbram * 16))))
        sdp_choices.add((nbram * 32, (1, max_address_bits(nbram * 32))))
        for address_bits in range(1, 4):
            sdp_choices.add((nbram * 16, (address_bits, address_bits)))
    sdp_choices = sorted(sdp_choices)
    (width, address_bits_range) = random.choice(sdp_choices)
    address_bits = random.randint(*address_bits_range)
    return emit_sdp_bram(luts, name, modules, lines, width, address_bits)","for width in (1, 2, 4, 8, 16, 18, 32, 36):
    sdp_choices.add((width, (1, max_address_bits(width))))","sdp_choices = {(width, (1, max_address_bits(width))) for width in (1, 2, 4, 8, 16, 18, 32, 36)}","['sdp_choices = {(width, (1, max_address_bits(width))) for width in (1, 2, 4, 8, 16, 18, 32, 36)}']",1,
prjxray,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/fuzzers/060-bram-cascades/top.py,https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/060-bram-cascades/top.py,,random_sdp_bram$120,"def random_sdp_bram(luts, name, modules, lines):
    sdp_choices = set()
    for width in (1, 2, 4, 8, 16, 18, 32, 36):
        sdp_choices.add((width, (1, max_address_bits(width))))
    for nbram in range(2, MAX_BRAM + 1):
        sdp_choices.add((nbram * 32, (1, max_address_bits(nbram * 32))))
        sdp_choices.add((nbram * 36, (1, max_address_bits(nbram * 36))))
        sdp_choices.add((nbram * 16, (1, max_address_bits(nbram * 16))))
        sdp_choices.add((nbram * 32, (1, max_address_bits(nbram * 32))))
        for address_bits in range(1, 4):
            sdp_choices.add((nbram * 16, (address_bits, address_bits)))
    sdp_choices = sorted(sdp_choices)
    (width, address_bits_range) = random.choice(sdp_choices)
    address_bits = random.randint(*address_bits_range)
    return emit_sdp_bram(luts, name, modules, lines, width, address_bits)","for nbram in range(2, MAX_BRAM + 1):
    sdp_choices.add((nbram * 32, (1, max_address_bits(nbram * 32))))
    sdp_choices.add((nbram * 36, (1, max_address_bits(nbram * 36))))
    sdp_choices.add((nbram * 16, (1, max_address_bits(nbram * 16))))
    sdp_choices.add((nbram * 32, (1, max_address_bits(nbram * 32))))
    for address_bits in range(1, 4):
        sdp_choices.add((nbram * 16, (address_bits, address_bits)))","sdp_choices = {(nbram * size, (1, max_address_bits(nbram * size))) for nbram in range(2, MAX_BRAM + 1) for size in [32, 36, 16]} | {(nbram * 16, (address_bits, address_bits)) for nbram in range(2, MAX_BRAM + 1) for address_bits in range(1, 4)}",Cannot refactor,2,
habu,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/habu/habu/cli/cmd_gateway_find.py,https://github.com/fportantier/habu/tree/master/habu/cli/cmd_gateway_find.py,,cmd_gateway_find$21,"def cmd_gateway_find(network, iface, host, tcp, dport, timeout, verbose):
    """"""
    Try to reach an external IP using any host has a router.

    Useful to find routers in your network.

    First, uses arping to detect alive hosts and obtain MAC addresses.

    Later, create a network packet and put each MAC address as destination.

    Last, print the devices that forwarded correctly the packets.

    Example:

    \x08
    # habu.find.gateway 192.168.0.0/24
    192.168.0.1 a4:08:f5:19:17:a4 Sagemcom
    192.168.0.7 b0:98:2b:5d:22:70 Sagemcom
    192.168.0.8 b0:98:2b:5d:1f:e8 Sagemcom
    """"""
    if verbose:
        logging.basicConfig(level=logging.INFO, format='%(message)s')
    conf.verb = False
    if iface:
        iface = search_iface(iface)
        if iface:
            conf.iface = iface['name']
        else:
            logging.error('Interface {} not found. Use habu.interfaces to show valid network interfaces'.format(iface))
            return False
    (res, unans) = srp(Ether(dst='ff:ff:ff:ff:ff:ff') / ARP(pdst=network), timeout=2)
    neighbors = set()
    for (_, pkt) in res:
        neighbors.add((pkt['Ether'].src, pkt['Ether'].psrc))
    for (mac, ip) in neighbors:
        if tcp:
            (res, unans) = srp(Ether(dst=mac) / IP(dst=host) / TCP(dport=dport), timeout=timeout)
        else:
            (res, unans) = srp(Ether(dst=mac) / IP(dst=host) / ICMP(), timeout=timeout)
        for (_, pkt) in res:
            if pkt:
                if verbose:
                    print(pkt.show())
                else:
                    print(ip, mac, conf.manufdb._get_manuf(mac))","for (_, pkt) in res:
    neighbors.add((pkt['Ether'].src, pkt['Ether'].psrc))","neighbors = {(pkt['Ether'].src, pkt['Ether'].psrc) for (_, pkt) in res}","[""neighbors = {(pkt['Ether'].src, pkt['Ether'].psrc) for (_, pkt) in res}""]",1,
ralph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ralph/src/ralph/lib/permissions/models.py,https://github.com/allegro/ralph/tree/master/src/ralph/lib/permissions/models.py,PermByFieldMixin,allowed_fields$227,"def allowed_fields(cls, user, action='change'):
    """"""
        Returns a list with the names of the fields to which the user has
        permission.

        :Example:

            >> user = User.objects.get(username='root')
            >> model.allowed_fields(user, 'change')
            ['parent', 'remarks', 'service_env']

        :param user: User object
        :type user: django User object
        :param action: permission action (change/view)
        :type action: str

        :return: List of field names
        :rtype: list
        """"""
    result = set()
    blacklist = cls._permissions.blacklist
    for field in cls._meta.fields + cls._meta.many_to_many:
        if field.name not in blacklist and cls.has_access_to_field(field.name, user, action):
            result.add(field.name)
    if action == 'view':
        result |= cls.allowed_fields(user, 'change')
    return result","for field in cls._meta.fields + cls._meta.many_to_many:
    if field.name not in blacklist and cls.has_access_to_field(field.name, user, action):
        result.add(field.name)","result = {field.name for field in cls._meta.fields + cls._meta.many_to_many if field.name not in blacklist and cls.has_access_to_field(field.name, user, action)}","['result = {field.name for field in cls._meta.fields + cls._meta.many_to_many if field.name not in blacklist and cls.has_access_to_field(field.name, user, action)}']",1,
bridgy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bridgy/original_post_discovery.py,https://github.com/snarfed/bridgy/tree/master//original_post_discovery.py,,_merge_hfeeds$389,"def _merge_hfeeds(feed1, feed2):
    """"""Merge items from two h-feeds into a composite feed. Skips items in
  feed2 that are already represented in feed1, based on the ""url"" property.

  Args:
    feed1: a list of dicts
    feed2: a list of dicts

  Returns:
    a list of dicts
  """"""
    seen = set()
    for item in feed1:
        for url in item.get('properties', {}).get('url', []):
            if isinstance(url, str):
                seen.add(url)
    return feed1 + [item for item in feed2 if all((url not in seen for url in item.get('properties', {}).get('url', []) if isinstance(url, str)))]","for item in feed1:
    for url in item.get('properties', {}).get('url', []):
        if isinstance(url, str):
            seen.add(url)","entries = {url for item in feed1 for url in item.get('properties', {}).get('url', []) if isinstance(url, str)}","[""seen = {url for item in feed1 for url in item.get('properties', {}).get('url', []) if isinstance(url, str)}""]",0,
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/swift/container/backend.py,https://github.com/openstack/swift/tree/master/swift/container/backend.py,ContainerBroker,_get_shard_range_rows$1688,"def _get_shard_range_rows(self, connection=None, includes=None, include_deleted=False, states=None, include_own=False, exclude_others=False):
    """"""
        Returns a list of shard range rows.

        To get all shard ranges use ``include_own=True``. To get only the
        broker's own shard range use ``include_own=True`` and
        ``exclude_others=True``.

        :param connection: db connection
        :param includes: restricts the returned list to the shard range that
            includes the given value
        :param include_deleted: include rows marked as deleted
        :param states: include only rows matching the given state(s); can be an
            int or a list of ints.
        :param include_own: boolean that governs whether the row whose name
            matches the broker's path is included in the returned list. If
            True, that row is included, otherwise it is not included. Default
            is False.
        :param exclude_others: boolean that governs whether the rows whose
            names do not match the broker's path are included in the returned
            list. If True, those rows are not included, otherwise they are
            included. Default is False.
        :return: a list of tuples.
        """"""
    if exclude_others and (not include_own):
        return []
    included_states = set()
    if isinstance(states, (list, tuple, set)):
        included_states.update(states)
    elif states is not None:
        included_states.add(states)
    default_values = {'reported': 0, 'tombstones': -1}

    def do_query(conn, defaults=None):
        condition = ''
        conditions = []
        params = []
        if not include_deleted:
            conditions.append('deleted=0')
        if included_states:
            conditions.append('state in (%s)' % ','.join('?' * len(included_states)))
            params.extend(included_states)
        if not include_own:
            conditions.append('name != ?')
            params.append(self.path)
        if exclude_others:
            conditions.append('name = ?')
            params.append(self.path)
        if includes is not None:
            conditions.extend(('lower < ?', ""(upper = '' OR upper >= ?)""))
            params.extend((includes, includes))
        if conditions:
            condition = ' WHERE ' + ' AND '.join(conditions)
        columns = SHARD_RANGE_KEYS[:-2]
        for column in SHARD_RANGE_KEYS[-2:]:
            if column in defaults:
                columns += ('%s as %s' % (default_values[column], column),)
            else:
                columns += (column,)
        sql = '\n            SELECT %s\n            FROM %s%s;\n            ' % (', '.join(columns), SHARD_RANGE_TABLE, condition)
        data = conn.execute(sql, params)
        data.row_factory = None
        return [row for row in data]
    with self.maybe_get(connection) as conn:
        defaults = set()
        attempts = len(default_values) + 1
        while attempts:
            attempts -= 1
            try:
                return do_query(conn, defaults)
            except sqlite3.OperationalError as err:
                if 'no such table: %s' % SHARD_RANGE_TABLE in str(err):
                    return []
                if not attempts:
                    raise
                new_defaults = set()
                for column in default_values.keys():
                    if 'no such column: %s' % column in str(err):
                        new_defaults.add(column)
                if not new_defaults:
                    raise
                if new_defaults.intersection(defaults):
                    raise
                defaults.update(new_defaults)","for column in default_values.keys():
    if 'no such column: %s' % column in str(err):
        new_defaults.add(column)",new_defaults = {column for column in default_values.keys() if 'no such column: %s' % column in str(err)},["new_defaults = {column for column in default_values.keys() if 'no such column: %s' % column in str(err)}"],1,
clevr-dataset-gen,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clevr-dataset-gen/question_generation/question_engine.py,https://github.com/facebookresearch/clevr-dataset-gen/tree/master/question_generation/question_engine.py,,vg_relate_handler$51,"def vg_relate_handler(scene_struct, inputs, side_inputs):
    assert len(inputs) == 1
    assert len(side_inputs) == 1
    output = set()
    for rel in scene_struct['relationships']:
        if rel['predicate'] == side_inputs[0] and rel['subject_idx'] == inputs[0]:
            output.add(rel['object_idx'])
    return sorted(list(output))","for rel in scene_struct['relationships']:
    if rel['predicate'] == side_inputs[0] and rel['subject_idx'] == inputs[0]:
        output.add(rel['object_idx'])",output = {rel['object_idx'] for rel in scene_struct['relationships'] if rel['predicate'] == side_inputs[0] and rel['subject_idx'] == inputs[0]},["output = {rel['object_idx'] for rel in scene_struct['relationships'] if rel['predicate'] == side_inputs[0] and rel['subject_idx'] == inputs[0]}"],1,
DeTTECT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeTTECT/eql_yaml.py,https://github.com/rabobank-cdc/DeTTECT/tree/master//eql_yaml.py,,_get_applicable_to_yaml_values$409,"def _get_applicable_to_yaml_values(filename, type):
    """"""
    Get all the applicable to values, in lower case, from the provided YAML file.
    :param filename: file path of the YAML file
    :param type: type of YAML object to get the applicable to values from
    :retturn: set with all applicable to values in lower case
    """"""
    app_to_values = set()
    if type == FILE_TYPE_DATA_SOURCE_ADMINISTRATION:
        (_, _, systems, _, _) = load_data_sources(filename)
        for system in systems:
            app_to_values.add(system['applicable_to'].lower())
    return app_to_values","for system in systems:
    app_to_values.add(system['applicable_to'].lower())",app_to_values = {system['applicable_to'].lower() for system in systems},["app_to_values = {system['applicable_to'].lower() for system in systems}"],1,
django-simple-history,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-simple-history/simple_history/management/commands/clean_old_history.py,https://github.com/jazzband/django-simple-history/tree/master/simple_history/management/commands/clean_old_history.py,Command,handle$37,"def handle(self, *args, **options):
    self.verbosity = options['verbosity']
    to_process = set()
    model_strings = options.get('models', []) or args
    if model_strings:
        for model_pair in self._handle_model_list(*model_strings):
            to_process.add(model_pair)
    elif options['auto']:
        to_process = self._auto_models()
    else:
        self.log(self.COMMAND_HINT)
    self._process(to_process, days_back=options['days'], dry_run=options['dry'])","for model_pair in self._handle_model_list(*model_strings):
    to_process.add(model_pair)",to_process = {model_pair for model_pair in self._handle_model_list(*model_strings)},['to_process = {model_pair for model_pair in self._handle_model_list(*model_strings)}'],1,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/qiskit/pulse/macros.py,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/pulse/macros.py,,measure$22,"def measure(qubits: List[int], backend=None, inst_map: Optional[InstructionScheduleMap]=None, meas_map: Optional[Union[List[List[int]], Dict[int, List[int]]]]=None, qubit_mem_slots: Optional[Dict[int, int]]=None, measure_name: str='measure') -> Schedule:
    """"""Return a schedule which measures the requested qubits according to the given
    instruction mapping and measure map, or by using the defaults provided by the backend.

    By default, the measurement results for each qubit are trivially mapped to the qubit
    index. This behavior is overridden by qubit_mem_slots. For instance, to measure
    qubit 0 into MemorySlot(1), qubit_mem_slots can be provided as {0: 1}.

    Args:
        qubits: List of qubits to be measured.
        backend (Union[Backend, BaseBackend]): A backend instance, which contains
            hardware-specific data required for scheduling.
        inst_map: Mapping of circuit operations to pulse schedules. If None, defaults to the
                  ``instruction_schedule_map`` of ``backend``.
        meas_map: List of sets of qubits that must be measured together. If None, defaults to
                  the ``meas_map`` of ``backend``.
        qubit_mem_slots: Mapping of measured qubit index to classical bit index.
        measure_name: Name of the measurement schedule.

    Returns:
        A measurement schedule corresponding to the inputs provided.

    Raises:
        PulseError: If both ``inst_map`` or ``meas_map``, and ``backend`` is None.
    """"""
    schedule = Schedule(name=f'Default measurement schedule for qubits {qubits}')
    try:
        inst_map = inst_map or backend.defaults().instruction_schedule_map
        meas_map = meas_map or backend.configuration().meas_map
    except AttributeError as ex:
        raise exceptions.PulseError('inst_map or meas_map, and backend cannot be None simultaneously') from ex
    if isinstance(meas_map, list):
        meas_map = utils.format_meas_map(meas_map)
    measure_groups = set()
    for qubit in qubits:
        measure_groups.add(tuple(meas_map[qubit]))
    for measure_group_qubits in measure_groups:
        if qubit_mem_slots is not None:
            unused_mem_slots = set(measure_group_qubits) - set(qubit_mem_slots.values())
        try:
            default_sched = inst_map.get(measure_name, measure_group_qubits)
        except exceptions.PulseError as ex:
            raise exceptions.PulseError(""We could not find a default measurement schedule called '{}'. Please provide another name using the 'measure_name' keyword argument. For assistance, the instructions which are defined are: {}"".format(measure_name, inst_map.instructions)) from ex
        for (time, inst) in default_sched.instructions:
            if inst.channel.index not in qubits:
                continue
            if qubit_mem_slots and isinstance(inst, instructions.Acquire):
                if inst.channel.index in qubit_mem_slots:
                    mem_slot = channels.MemorySlot(qubit_mem_slots[inst.channel.index])
                else:
                    mem_slot = channels.MemorySlot(unused_mem_slots.pop())
                inst = instructions.Acquire(inst.duration, inst.channel, mem_slot=mem_slot)
            schedule = schedule.insert(time, inst)
    return schedule","for qubit in qubits:
    measure_groups.add(tuple(meas_map[qubit]))",measure_groups = {tuple(meas_map[qubit]) for qubit in qubits},['measure_groups = {tuple(meas_map[qubit]) for qubit in qubits}'],1,
ccks_baidu_entity_link,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ccks_baidu_entity_link/code/evaluate.py,https://github.com/panchunguang/ccks_baidu_entity_link/tree/master/code/evaluate.py,,link_eval$121,"def link_eval(y_ture, y_pred):
    true_num = 1e-10
    pred_num = 1e-10
    equal_num = 1e-10
    y_ture_file = open(y_ture, 'r')
    y_pred_file = open(y_pred, 'r')
    for (line_true, line_pred) in zip(y_ture_file, y_pred_file):
        line_true = json.loads(line_true)
        line_pred = json.loads(line_pred)
        mention_data_true = line_true['mention_data']
        mention_data_pred = line_pred['mention_data']
        true_set = set()
        pred_set = set()
        for mention in mention_data_true:
            if mention['kb_id'] != 'NIL':
                true_set.add((mention['kb_id'], mention['mention'], mention['offset']))
        for mention in mention_data_pred:
            if mention['kb_id'] != 'NIL':
                pred_set.add((mention['kb_id'], mention['mention'], mention['offset']))
        true_num += len(true_set)
        pred_num += len(pred_set)
        equal_num += len(true_set & pred_set)
    precision = equal_num / pred_num
    recall = equal_num / true_num
    f1 = 2 * precision * recall / (precision + recall)
    print('equal_num:', equal_num)
    print('true_num:', true_num)
    print('pred_num:', pred_num)
    print('precision:', precision)
    print('recall:', recall)
    print('f1:', f1)
    return (precision, recall, f1)","for mention in mention_data_true:
    if mention['kb_id'] != 'NIL':
        true_set.add((mention['kb_id'], mention['mention'], mention['offset']))","true_set = {(mention['kb_id'], mention['mention'], mention['offset']) for mention in mention_data_true if mention['kb_id'] != 'NIL'}","[""true_set = {(mention['kb_id'], mention['mention'], mention['offset']) for mention in mention_data_true if mention['kb_id'] != 'NIL'}""]",1,
ccks_baidu_entity_link,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ccks_baidu_entity_link/code/evaluate.py,https://github.com/panchunguang/ccks_baidu_entity_link/tree/master/code/evaluate.py,,link_eval$121,"def link_eval(y_ture, y_pred):
    true_num = 1e-10
    pred_num = 1e-10
    equal_num = 1e-10
    y_ture_file = open(y_ture, 'r')
    y_pred_file = open(y_pred, 'r')
    for (line_true, line_pred) in zip(y_ture_file, y_pred_file):
        line_true = json.loads(line_true)
        line_pred = json.loads(line_pred)
        mention_data_true = line_true['mention_data']
        mention_data_pred = line_pred['mention_data']
        true_set = set()
        pred_set = set()
        for mention in mention_data_true:
            if mention['kb_id'] != 'NIL':
                true_set.add((mention['kb_id'], mention['mention'], mention['offset']))
        for mention in mention_data_pred:
            if mention['kb_id'] != 'NIL':
                pred_set.add((mention['kb_id'], mention['mention'], mention['offset']))
        true_num += len(true_set)
        pred_num += len(pred_set)
        equal_num += len(true_set & pred_set)
    precision = equal_num / pred_num
    recall = equal_num / true_num
    f1 = 2 * precision * recall / (precision + recall)
    print('equal_num:', equal_num)
    print('true_num:', true_num)
    print('pred_num:', pred_num)
    print('precision:', precision)
    print('recall:', recall)
    print('f1:', f1)
    return (precision, recall, f1)","for mention in mention_data_pred:
    if mention['kb_id'] != 'NIL':
        pred_set.add((mention['kb_id'], mention['mention'], mention['offset']))","pred_set = {(mention['kb_id'], mention['mention'], mention['offset']) for mention in mention_data_pred if mention['kb_id'] != 'NIL'}","[""pred_set = {(mention['kb_id'], mention['mention'], mention['offset']) for mention in mention_data_pred if mention['kb_id'] != 'NIL'}""]",1,
GeneticAlgorithmsWithPython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GeneticAlgorithmsWithPython/ch18/ticTacToeTests.py,https://github.com/handcraftsman/GeneticAlgorithmsWithPython/tree/master/ch18/ticTacToeTests.py,CenterFilter,get_matches$661,"def get_matches(board, squares):
    result = set()
    for square in squares:
        if square.IsCenter:
            result.add(square.Index)
    return result","for square in squares:
    if square.IsCenter:
        result.add(square.Index)",result = {square.Index for square in squares if square.IsCenter},['result = {square.Index for square in squares if square.IsCenter}'],1,
auto-editor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/auto-editor/auto_editor/vanparse.py,https://github.com/WyattBlue/auto-editor/tree/master/auto_editor/vanparse.py,ParseOptions,__init__$223,"def __init__(self, sys_args, log, root, *args):
    option_names = []
    for options in args:
        for option in options:
            option_names.append(option['names'][0])
            key = _to_key(option)
            if option['action'] == 'store_true':
                value = False
            elif option['action'] == 'store_false':
                value = True
            elif option['nargs'] != 1:
                value = []
            else:
                value = option['default']
            setattr(self, key, value)
    dirpath = os.path.dirname(os.path.realpath(__file__))
    self.set_config(os.path.join(dirpath, 'config.txt'), root)
    my_list = []
    used_options = []
    _set = []
    setting_inputs = True
    option_list = 'input'
    list_type = str
    i = 0
    group = None
    while i < len(sys_args):
        item = sys_args[i]
        label = 'option' if item.startswith('--') else 'short'
        option = get_option(item, the_args=args)

        def error_message(args, item, label):

            def all_names(args):
                name_set = set()
                for options in args:
                    for opt in options:
                        for names in opt['names']:
                            name_set.add(names)
                return name_set
            opt_list = all_names(args)
            close_matches = difflib.get_close_matches(item, opt_list)
            if close_matches:
                return 'Unknown {}: {}\n\n    Did you mean:\n        '.format(label, item) + ', '.join(close_matches)
            return 'Unknown {}: {}'.format(label, item)
        if option is None:
            if setting_inputs and (option_list != 'input' or (option_list == 'input' and (not item.startswith('-')))):
                if option_list != 'input':
                    _op = used_options[-1]
                    if _op['keywords'] != []:
                        item = self.parse_parameters(item, _op)
                my_list.append(item)
            else:
                log.error(error_message(args, item, label))
        else:
            if option_list is not None:
                setattr(self, option_list, list(map(list_type, my_list)))
            setting_inputs = False
            option_list = None
            my_list = []
            if option in used_options:
                log.error('Cannot repeat option {} twice.'.format(option['names'][0]))
            used_options.append(option)
            key = _to_key(option)
            _set.append(key)
            if option['action'] == 'grouping':
                group = key
            nextItem = None if i == len(sys_args) - 1 else sys_args[i + 1]
            if nextItem == '-h' or nextItem == '--help':
                print_option_help(args, option)
                sys.exit()
            if option['nargs'] != 1:
                setting_inputs = True
                option_list = key
                list_type = option['type']
            elif option['action'] == 'store_true':
                value = True
            elif option['action'] == 'store_false':
                value = False
            else:
                value = option['type'](nextItem)
                if option['choices'] is not None and value not in option['choices']:
                    option_name = option['names'][0]
                    my_choices = ', '.join(option['choices'])
                    log.error('{} is not a choice for {}\nchoices are:\n  {}'.format(value, option_name, my_choices))
                i += 1
            setattr(self, key, value)
        i += 1
    if setting_inputs:
        setattr(self, option_list, list(map(list_type, my_list)))
    setattr(self, '_set', _set)
    if self.help:
        print_program_help(root, args)
        sys.exit()","for options in args:
    for opt in options:
        for names in opt['names']:
            name_set.add(names)",name_set = {names for options in args for opt in options for names in opt['names']},["name_set = {names for options in args for opt in options for names in opt['names']}"],1,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/saltutil.py,https://github.com/saltstack/salt/tree/master/salt/modules/saltutil.py,,list_extmods$852,"def list_extmods():
    """"""
    .. versionadded:: 2017.7.0

    List Salt modules which have been synced externally

    CLI Examples:

    .. code-block:: bash

        salt '*' saltutil.list_extmods
    """"""
    ret = {}
    ext_dir = os.path.join(__opts__['cachedir'], 'extmods')
    mod_types = os.listdir(ext_dir)
    for mod_type in mod_types:
        ret[mod_type] = set()
        for (_, _, files) in salt.utils.path.os_walk(os.path.join(ext_dir, mod_type)):
            for fh_ in files:
                ret[mod_type].add(fh_.split('.')[0])
        ret[mod_type] = list(ret[mod_type])
    return ret","for mod_type in mod_types:
    ret[mod_type] = set()
    for (_, _, files) in salt.utils.path.os_walk(os.path.join(ext_dir, mod_type)):
        for fh_ in files:
            ret[mod_type].add(fh_.split('.')[0])
    ret[mod_type] = list(ret[mod_type])","entries = {mod_type: list({fh_.split('.')[0] for (_, _, files) in salt.utils.path.os_walk(os.path.join(ext_dir, mod_type)) for fh_ in files}) for mod_type in mod_types}",Cannot refactor,2,
nibabel,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nibabel/versioneer.py,https://github.com/nipy/nibabel/tree/master//versioneer.py,,do_setup$1758,"def do_setup():
    """"""Do main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print('Adding sample versioneer config to setup.cfg', file=sys.stderr)
            with open(os.path.join(root, 'setup.cfg'), 'a') as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1
    print(' creating %s' % cfg.versionfile_source)
    with open(cfg.versionfile_source, 'w') as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {'DOLLAR': '$', 'STYLE': cfg.style, 'TAG_PREFIX': cfg.tag_prefix, 'PARENTDIR_PREFIX': cfg.parentdir_prefix, 'VERSIONFILE_SOURCE': cfg.versionfile_source})
    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), '__init__.py')
    if os.path.exists(ipy):
        try:
            with open(ipy, 'r') as f:
                old = f.read()
        except EnvironmentError:
            old = ''
        if INIT_PY_SNIPPET not in old:
            print(' appending to %s' % ipy)
            with open(ipy, 'a') as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print(' %s unmodified' % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None
    manifest_in = os.path.join(root, 'MANIFEST.in')
    simple_includes = set()
    try:
        with open(manifest_in, 'r') as f:
            for line in f:
                if line.startswith('include '):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    if 'versioneer.py' not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, 'a') as f:
            f.write('include versioneer.py\n')
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" % cfg.versionfile_source)
        with open(manifest_in, 'a') as f:
            f.write('include %s\n' % cfg.versionfile_source)
    else:
        print(' versionfile_source already in MANIFEST.in')
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f for include in line.split()[1:] if line.startswith('include ')},["simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}"],0,
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/stats/tests/test_axis_nan_policy.py,https://github.com/scipy/scipy/tree/master/scipy/stats/tests/test_axis_nan_policy.py,,_check_arrays_broadcastable$448,"def _check_arrays_broadcastable(arrays, axis):
    n_dims = max([arr.ndim for arr in arrays])
    if axis is not None:
        axis = -n_dims + axis if axis >= 0 else axis
    for dim in range(1, n_dims + 1):
        if -dim == axis:
            continue
        dim_lengths = set()
        for arr in arrays:
            if dim <= arr.ndim and arr.shape[-dim] != 1:
                dim_lengths.add(arr.shape[-dim])
        if len(dim_lengths) > 1:
            return False
    return True","for arr in arrays:
    if dim <= arr.ndim and arr.shape[-dim] != 1:
        dim_lengths.add(arr.shape[-dim])",dim_lengths = {arr.shape[-dim] for arr in arrays if dim <= arr.ndim and arr.shape[-dim] != 1},['dim_lengths = {arr.shape[-dim] for arr in arrays if dim <= arr.ndim and arr.shape[-dim] != 1}'],1,
torchgeo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torchgeo/torchgeo/datasets/ucmerced.py,https://github.com/microsoft/torchgeo/tree/master/torchgeo/datasets/ucmerced.py,UCMerced,__init__$105,"def __init__(self, root: str='data', split: str='train', transforms: Optional[Callable[[Dict[str, Tensor]], Dict[str, Tensor]]]=None, download: bool=False, checksum: bool=False) -> None:
    """"""Initialize a new UC Merced dataset instance.

        Args:
            root: root directory where dataset can be found
            split: one of ""train"", ""val"", or ""test""
            transforms: a function/transform that takes input sample and its target as
                entry and returns a transformed version
            download: if True, download dataset and store it in the root directory
            checksum: if True, check the MD5 of the downloaded files (may be slow)

        Raises:
            RuntimeError: if ``download=False`` and data is not found, or checksums
                don't match
        """"""
    assert split in self.splits
    self.root = root
    self.transforms = transforms
    self.download = download
    self.checksum = checksum
    self._verify()
    valid_fns = set()
    with open(os.path.join(self.root, f'uc_merced-{split}.txt')) as f:
        for fn in f:
            valid_fns.add(fn.strip())
    is_in_split: Callable[[str], bool] = lambda x: os.path.basename(x) in valid_fns
    super().__init__(root=os.path.join(root, self.base_dir), transforms=transforms, is_valid_file=is_in_split)","for fn in f:
    valid_fns.add(fn.strip())",valid_fns = {fn.strip() for fn in f},['valid_fns = {fn.strip() for fn in f}'],1,
coding-interview-gym,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coding-interview-gym/leetcode.com/python/939_Minimum_Area_Rectangle.py,https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/939_Minimum_Area_Rectangle.py,Solution,minAreaRect$31,"def minAreaRect(self, points):
    """"""
        :type points: List[List[int]]
        :rtype: int
        """"""
    minArea = float('inf')
    pointsSet = set()
    for (x, y) in points:
        pointsSet.add((x, y))
    for (x1, y1) in points:
        for (x2, y2) in points:
            if (x1, y1) != (x2, y2) and (x1 > x2 and y1 > y2):
                if (x1, y2) in pointsSet and (x2, y1) in pointsSet:
                    area = abs(x1 - x2) * abs(y1 - y2)
                    minArea = min(minArea, area)
    return 0 if minArea == float('inf') else minArea","for (x, y) in points:
    pointsSet.add((x, y))","pointsSet = {(x, y) for (x, y) in points}","['pointsSet = {(x, y) for (x, y) in points}']",1,
python-string-similarity,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-string-similarity/strsimpy/qgram.py,https://github.com/luozhouyang/python-string-similarity/tree/master/strsimpy/qgram.py,QGram,distance_profile$43,"def distance_profile(profile0, profile1):
    union = set()
    for k in profile0.keys():
        union.add(k)
    for k in profile1.keys():
        union.add(k)
    agg = 0
    for k in union:
        (v0, v1) = (0, 0)
        if profile0.get(k) is not None:
            v0 = int(profile0.get(k))
        if profile1.get(k) is not None:
            v1 = int(profile1.get(k))
        agg += abs(v0 - v1)
    return agg","for k in profile0.keys():
    union.add(k)",union = {k for k in profile0.keys()},['union = {k for k in profile0.keys()}'],1,
RedditDownloader,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RedditDownloader/redditdownloader/__main__.py,https://github.com/shadowmoose/RedditDownloader/tree/master/redditdownloader/__main__.py,,run$44,"def run():
    logging.basicConfig(level=logging.WARN, format='%(levelname)-5.5s [%(name)s] %(message)s', datefmt='%H:%M:%S')
    su.print_color('green', '\r\n' + '====================================\r\n' + '   Reddit Media Downloader %s\r\n' % meta.current_version + '====================================\r\n' + '    (By ShadowMoose @ Github)\r\n')
    if args.version:
        sys.exit(0)
    if args.run_tests:
        error_count = tests.runner.run_tests(test_subdir=args.run_tests)
        sys.exit(error_count)
    if args.list_settings:
        print('All valid overridable settings:')
        for _s in settings.get_all():
            if _s.public:
                print('%s.%s' % (_s.category, _s.name))
                print('\tDescription: %s' % _s.description)
                if not _s.opts:
                    print('\tValid value: \n\t\tAny %s' % _s.type)
                else:
                    print('\tValid values:')
                    for o in _s.opts:
                        print('\t\t""%s"": %s' % o)
                print()
        sys.exit()
    settings_file = args.settings or fs.find_file('settings.json')
    _loaded = settings.load(settings_file)
    for ua in unknown_args:
        if '=' not in ua or '/comments/' in ua:
            if '/comments/' in ua:
                direct_sources.append(DirectURLSource(url=ua))
                continue
            elif 'r/' or 'u/' in ua:
                direct_sources.append(DirectInputSource(txt=ua, args={'limit': args.limit}))
                continue
            else:
                su.error('ERROR: Unkown argument: %s' % ua)
                sys.exit(1)
        k = ua.split('=')[0].strip('- ')
        v = ua.split('=', 2)[1].strip()
        try:
            settings.put(k, v, save_after=False)
        except KeyError:
            print('Unknown setting: %s' % k)
            sys.exit(50)
    if args.source:
        matched_sources = set()
        for s in args.source:
            for stt in settings.get_sources():
                if re.match(s, stt.get_alias()):
                    matched_sources.add(stt)
        direct_sources.extend(matched_sources)
    if args.import_csv:
        direct_sources.append(DirectFileSource(file=args.import_csv, slow_fallback=args.full_csv))
    first_time_auth = False
    if not _loaded and (not direct_sources) and (not args.docker):
        su.error('Could not find an existing settings file. A new one will be generated!')
        if not console.confirm('Would you like to start the WebUI to help set things up?', True):
            su.print_color('red', ""If you don't open the webUI now, you'll need to edit the settings file yourself."")
            if console.confirm(""Are you sure you'd like to edit settings without the UI (if 'yes', these prompts will not show again)?""):
                settings.put('interface.start_server', False, save_after=True)
                print('A settings file has been created for you, at ""%s"". Please customize it.' % settings_file)
                first_time_auth = True
            else:
                print('Please re-run RMD to configure again.')
                sys.exit(1)
        else:
            mode = console.prompt_list('How would you like to open the UI?', settings.get('interface.browser', full_obj=True).opts)
            settings.put('interface.browser', mode, save_after=False)
            settings.put('interface.start_server', True)
    if args.docker:
        print('Running in ""Docker"" mode. Assuming some default settings.')
        settings.put('interface.host', '0.0.0.0', save_after=False)
        settings.put('interface.browser', 'off', save_after=False)
        settings.put('interface.keep_open', True, save_after=False)
        settings.put('interface.start_server', True)
    if args.authorize or first_time_auth:
        from static import praw_wrapper
        from urllib.parse import urlparse, parse_qs
        url = praw_wrapper.get_reddit_token_url()
        su.print_color('green', '\nTo manually authorize your account, visit the below URL.')
        su.print_color('yellow', 'Once there, authorize RMD, then copy the URL it redirects you to.')
        su.print_color('yellow', 'NOTE: The redirect page will likely not load, and that is ok.')
        su.print_color('cyan', '\n%s\n' % url)
        token_url = console.col_input('Paste the URL you are redirected to here: ')
        if token_url.strip():
            qs = parse_qs(urlparse(token_url).query)
            if 'state' not in qs or 'code' not in qs:
                su.error('The url provided was not a valid reddit redirect. Please make sure you copied it right!')
            elif qs['state'][0].strip() != settings.get('auth.oauth_key').strip():
                su.error('Invalid reddit redirect state. Please restart and try again.')
            else:
                code = qs['code'][0]
                su.print_color('green', 'Got code. Authorizing account...')
                refresh = praw_wrapper.get_refresh_token(code)
                if refresh:
                    settings.put('auth.refresh_token', refresh)
                    usr = praw_wrapper.get_current_username()
                    su.print_color('cyan', 'Authorized to view account: %s' % usr)
                    su.print_color('green', 'Saved authorization token! Please restart RMD to begin downloading!')
                else:
                    su.error('Failed to gain an account access token from Reddit with that code. Please try again.')
        sys.exit(0)
    if not ffmpeg_download.install_local():
        print('RMD was unable to locate (or download) a working FFmpeg binary.')
        print('For downloading and post-processing, this is a required tool.')
        print('Please Install FFmpeg manually, or download it from here: https://rmd.page.link/ffmpeg')
        sys.exit(15)
    sql.init_from_settings()
    print('Using manifest file [%s].' % sql.get_file_location())
    if direct_sources:
        settings.disable_saving()
        settings.put('processing.retry_failed', False)
        for s in settings.get_sources():
            settings.remove_source(s, save_after=False)
        for d in direct_sources:
            settings.add_source(d, prevent_duplicate=False, save_after=False)
    if settings.get('interface.start_server') and (not direct_sources):
        print('Starting WebUI...')
        ui = WebUI()
    else:
        ui = TerminalUI()
    ui.display()","for s in args.source:
    for stt in settings.get_sources():
        if re.match(s, stt.get_alias()):
            matched_sources.add(stt)","matched_sources = {stt for s in args.source for stt in settings.get_sources() if re.match(s, stt.get_alias())}","['matched_sources = {stt for s in args.source for stt in settings.get_sources() if re.match(s, stt.get_alias())}']",1,
MozDef,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MozDef/alerts/auth0_bruteforce_user.py,https://github.com/mozilla/MozDef/tree/master/alerts/auth0_bruteforce_user.py,AlertAuth0BruteforceUser,onAggregation$29,"def onAggregation(self, aggreg):
    category = 'bruteforce'
    tags = ['auth0']
    severity = self.config.severity
    ip_list = set()
    for event in aggreg['allevents']:
        ip_list.add(event['_source']['details']['sourceipaddress'])
    summary = 'Auth0 Username/Password Bruteforce Attack in Progress against user ({0}) from the following source ip(s): {1}'.format(aggreg['value'], ', '.join(sorted(ip_list)[:10]))
    if len(ip_list) >= 10:
        summary += '...'
    return self.createAlertDict(summary, category, tags, aggreg['events'], severity)","for event in aggreg['allevents']:
    ip_list.add(event['_source']['details']['sourceipaddress'])",ip_list = {event['_source']['details']['sourceipaddress'] for event in aggreg['allevents']},["ip_list = {event['_source']['details']['sourceipaddress'] for event in aggreg['allevents']}"],1,
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/special_agents/agent_elasticsearch.py,https://github.com/tribe29/checkmk/tree/master/cmk/special_agents/agent_elasticsearch.py,,handle_stats$117,"def handle_stats(response):
    shards = response.get('_shards')
    if shards is not None:
        sys.stdout.write('<<<elasticsearch_shards>>>\n')
        sys.stdout.write('%s %s %s\n' % (shards.get('total'), shards.get('successful'), shards.get('failed')))
    docs = response.get('_all', {}).get('total')
    if docs is not None:
        sys.stdout.write('<<<elasticsearch_cluster>>>\n')
        count = docs.get('docs', {}).get('count')
        size = docs.get('store', {}).get('size_in_bytes')
        sys.stdout.write('%s %s\n' % (count, size))
    indices_data = response.get('indices')
    if indices_data is not None:
        indices = set()
        sys.stdout.write('<<<elasticsearch_indices>>>\n')
        for index in indices_data:
            indices.add(index.split('-')[0])
        for indice in list(indices):
            all_counts = []
            all_sizes = []
            for index in indices_data:
                if index.split('-')[0] == indice:
                    all_counts.append(indices_data.get(index, {}).get('primaries', {}).get('docs', {}).get('count'))
                    all_sizes.append(indices_data.get(index, {}).get('total', {}).get('store', {}).get('size_in_bytes'))
            sys.stdout.write('%s %s %s\n' % (indice, sum(all_counts) / len(all_counts), sum(all_sizes) / len(all_sizes)))","for index in indices_data:
    indices.add(index.split('-')[0])",indices = {index.split('-')[0] for index in indices_data},["indices = {index.split('-')[0] for index in indices_data}"],1,
TextAttack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TextAttack/textattack/transformations/word_insertions/word_insertion_random_synonym.py,https://github.com/QData/TextAttack/tree/master/textattack/transformations/word_insertions/word_insertion_random_synonym.py,WordInsertionRandomSynonym,_get_synonyms$19,"def _get_synonyms(self, word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            if lemma.name() != word and check_if_one_word(lemma.name()):
                synonyms.add(lemma.name())
    return list(synonyms)","for syn in wordnet.synsets(word):
    for lemma in syn.lemmas():
        if lemma.name() != word and check_if_one_word(lemma.name()):
            synonyms.add(lemma.name())",synonyms = {lemma.name() for syn in wordnet.synsets(word) for lemma in syn.lemmas() if lemma.name() != word and check_if_one_word(lemma.name())},['synonyms = {lemma.name() for syn in wordnet.synsets(word) for lemma in syn.lemmas() if lemma.name() != word and check_if_one_word(lemma.name())}'],1,
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tools/c7n_org/c7n_org/cli.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_org/c7n_org/cli.py,,filter_accounts$250,"def filter_accounts(accounts_config, tags, accounts, not_accounts=None):
    filtered_accounts = []
    accounts = comma_expand(accounts)
    not_accounts = comma_expand(not_accounts)
    for a in accounts_config.get('accounts', ()):
        if not_accounts and a['name'] in not_accounts:
            continue
        account_id = a.get('account_id') or a.get('project_id') or a.get('subscription_id') or ''
        if accounts and a['name'] not in accounts and (account_id not in accounts):
            continue
        if tags:
            found = set()
            for t in tags:
                if t in a.get('tags', ()):
                    found.add(t)
            if not found == set(tags):
                continue
        filtered_accounts.append(a)
    accounts_config['accounts'] = filtered_accounts","for a in accounts_config.get('accounts', ()):
    if not_accounts and a['name'] in not_accounts:
        continue
    account_id = a.get('account_id') or a.get('project_id') or a.get('subscription_id') or ''
    if accounts and a['name'] not in accounts and (account_id not in accounts):
        continue
    if tags:
        found = set()
        for t in tags:
            if t in a.get('tags', ()):
                found.add(t)
        if not found == set(tags):
            continue
    filtered_accounts.append(a)","filtered_accounts = [a for a in accounts_config.get('accounts', ()) if (not not_accounts and True or a['name'] not in not_accounts) and (not accounts or a['name'] in accounts or (a.get('account_id') or a.get('project_id') or a.get('subscription_id') or '') in accounts) and (not tags or set(tags).issubset(set(a.get('tags', ())))) ]",Cannot refactor,2,
PythonProgrammingPuzzles,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonProgrammingPuzzles/generators/human_eval.py,https://github.com/microsoft/PythonProgrammingPuzzles/tree/master/generators/human_eval.py,PrimeSel,sat$5005,"def sat(neighbors: List[int], nums=[14, 7, 11, 13, 7, 4, 19, 2, 55, 13, 31, 14, 2, 9, -7, 0, 88, 13, 13]):
    """"""Find a list of all numbers that are adjacent to a prime number in the list, sorted without duplicates

        [2, 17, 16, 0, 6, 4, 5] => [2, 4, 16, 17]""""""

    def prime(m):
        return all((m % i for i in range(2, m - 1)))
    goods = set()
    for (i, n) in enumerate(nums):
        if i > 0 and prime(nums[i - 1]) or (i < len(nums) - 1 and prime(nums[i + 1])):
            goods.add(n)
    return set(neighbors) == goods and all((n == min(neighbors[i:]) for (i, n) in enumerate(neighbors)))","for (i, n) in enumerate(nums):
    if i > 0 and prime(nums[i - 1]) or (i < len(nums) - 1 and prime(nums[i + 1])):
        goods.add(n)","goods = {n for (i, n) in enumerate(nums) if (i > 0 and prime(nums[i - 1])) or (i < len(nums) - 1 and prime(nums[i + 1]))}","['goods = {n for (i, n) in enumerate(nums) if i > 0 and prime(nums[i - 1]) or (i < len(nums) - 1 and prime(nums[i + 1]))}']",0,
DeepCTR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepCTR/deepctr/layers/interaction.py,https://github.com/shenweichen/DeepCTR/tree/master/deepctr/layers/interaction.py,AFMLayer,build$58,"def build(self, input_shape):
    if not isinstance(input_shape, list) or len(input_shape) < 2:
        raise ValueError('A `AttentionalFM` layer should be called on a list of at least 2 inputs')
    shape_set = set()
    reduced_input_shape = [shape.as_list() for shape in input_shape]
    for i in range(len(input_shape)):
        shape_set.add(tuple(reduced_input_shape[i]))
    if len(shape_set) > 1:
        raise ValueError('A `AttentionalFM` layer requires inputs with same shapes Got different shapes: %s' % shape_set)
    if len(input_shape[0]) != 3 or input_shape[0][1] != 1:
        raise ValueError('A `AttentionalFM` layer requires inputs of a list with same shape tensor like                             (None, 1, embedding_size)Got different shapes: %s' % input_shape[0])
    embedding_size = int(input_shape[0][-1])
    self.attention_W = self.add_weight(shape=(embedding_size, self.attention_factor), initializer=glorot_normal(seed=self.seed), regularizer=l2(self.l2_reg_w), name='attention_W')
    self.attention_b = self.add_weight(shape=(self.attention_factor,), initializer=Zeros(), name='attention_b')
    self.projection_h = self.add_weight(shape=(self.attention_factor, 1), initializer=glorot_normal(seed=self.seed), name='projection_h')
    self.projection_p = self.add_weight(shape=(embedding_size, 1), initializer=glorot_normal(seed=self.seed), name='projection_p')
    self.dropout = tf.keras.layers.Dropout(self.dropout_rate, seed=self.seed)
    self.tensordot = tf.keras.layers.Lambda(lambda x: tf.tensordot(x[0], x[1], axes=(-1, 0)))
    super(AFMLayer, self).build(input_shape)","for i in range(len(input_shape)):
    shape_set.add(tuple(reduced_input_shape[i]))",shape_set = {tuple(reduced_input_shape[i]) for i in range(len(input_shape))},['shape_set = {tuple(reduced_input_shape[i]) for i in range(len(input_shape))}'],1,
GeneticAlgorithmsWithPython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GeneticAlgorithmsWithPython/ch18/ticTacToeTests.py,https://github.com/handcraftsman/GeneticAlgorithmsWithPython/tree/master/ch18/ticTacToeTests.py,SideFilter,get_matches$687,"def get_matches(board, squares):
    result = set()
    for square in squares:
        if square.IsSide:
            result.add(square.Index)
    return result","for square in squares:
    if square.IsSide:
        result.add(square.Index)",result = {square.Index for square in squares if square.IsSide},['result = {square.Index for square in squares if square.IsSide}'],1,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/distributed/fleet/meta_optimizers/sharding_optimizer.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/distributed/fleet/meta_optimizers/sharding_optimizer.py,ShardingOptimizer,_initialization_broadcast$1817,"def _initialization_broadcast(self):
    """"""
        this funtion is to ensure the initialization between dp group to be
        identical when hybrid-dp is used, and the initialization of
        not distributed param between mp group to be identical.
        """"""
    if self.dp_degree <= 1 and self.mp_degree <= 1:
        return
    startup_block = self._startup_program.global_block()
    params = startup_block.all_parameters()
    params_name = []
    not_dist_param_name = set()
    for param in params:
        params_name.append(param.name)
        if not hasattr(param, 'is_distributed') or not param.is_distributed:
            not_dist_param_name.add(param.name)
    broadcast_params = set()
    for op in startup_block.ops:
        if op.type == 'c_broadcast':
            broadcast_params.add(op.desc.output_arg_names()[0])
    for param in params_name:
        if param in broadcast_params:
            continue
        rings = []
        if self.mp_degree > 1 and param in not_dist_param_name:
            rings.append(self.mp_ring_id)
        if self.dp_degree > 1:
            rings.append(self.dp_ring_id)
        for ring in rings:
            startup_block.append_op(type='c_broadcast', inputs={'X': param}, outputs={'Out': param}, attrs={'ring_id': ring, 'root': 0, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Forward})
    startup_block._sync_with_cpp()","for param in params:
    params_name.append(param.name)
    if not hasattr(param, 'is_distributed') or not param.is_distributed:
        not_dist_param_name.add(param.name)","not_dist_param_name = {param.name for param in params if not hasattr(param, 'is_distributed') or not param.is_distributed}",Cannot refactor,2,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/distributed/fleet/meta_optimizers/sharding_optimizer.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/distributed/fleet/meta_optimizers/sharding_optimizer.py,ShardingOptimizer,_initialization_broadcast$1817,"def _initialization_broadcast(self):
    """"""
        this funtion is to ensure the initialization between dp group to be
        identical when hybrid-dp is used, and the initialization of
        not distributed param between mp group to be identical.
        """"""
    if self.dp_degree <= 1 and self.mp_degree <= 1:
        return
    startup_block = self._startup_program.global_block()
    params = startup_block.all_parameters()
    params_name = []
    not_dist_param_name = set()
    for param in params:
        params_name.append(param.name)
        if not hasattr(param, 'is_distributed') or not param.is_distributed:
            not_dist_param_name.add(param.name)
    broadcast_params = set()
    for op in startup_block.ops:
        if op.type == 'c_broadcast':
            broadcast_params.add(op.desc.output_arg_names()[0])
    for param in params_name:
        if param in broadcast_params:
            continue
        rings = []
        if self.mp_degree > 1 and param in not_dist_param_name:
            rings.append(self.mp_ring_id)
        if self.dp_degree > 1:
            rings.append(self.dp_ring_id)
        for ring in rings:
            startup_block.append_op(type='c_broadcast', inputs={'X': param}, outputs={'Out': param}, attrs={'ring_id': ring, 'root': 0, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Forward})
    startup_block._sync_with_cpp()","for op in startup_block.ops:
    if op.type == 'c_broadcast':
        broadcast_params.add(op.desc.output_arg_names()[0])",broadcast_params = {op.desc.output_arg_names()[0] for op in startup_block.ops if op.type == 'c_broadcast'},["broadcast_params = {op.desc.output_arg_names()[0] for op in startup_block.ops if op.type == 'c_broadcast'}"],1,
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/world/traderoute.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/world/traderoute.py,TradeRoute,can_enable$256,"def can_enable(self):
    warehouses = set()
    for waypoint in self.waypoints:
        warehouses.add(waypoint['warehouse'])
    return len(warehouses) > 1","for waypoint in self.waypoints:
    warehouses.add(waypoint['warehouse'])",warehouses = {waypoint['warehouse'] for waypoint in self.waypoints},["warehouses = {waypoint['warehouse'] for waypoint in self.waypoints}"],1,
river,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/river/river/tree/hoeffding_tree_classifier.py,https://github.com/online-ml/river/tree/master/river/tree/hoeffding_tree_classifier.py,HoeffdingTreeClassifier,_attempt_to_split$219,"def _attempt_to_split(self, leaf: HTLeaf, parent: DTBranch, parent_branch: int, **kwargs):
    """"""Attempt to split a leaf.

        If the samples seen so far are not from the same class then:

        1. Find split candidates and select the top 2.
        2. Compute the Hoeffding bound.
        3. If the difference between the top 2 split candidates is larger than the Hoeffding bound:
           3.1 Replace the leaf node by a split node (branch node).
           3.2 Add a new leaf node on each branch of the new split node.
           3.3 Update tree's metrics

        Optional: Disable poor attributes. Depends on the tree's configuration.

        Parameters
        ----------
        leaf
            The leaf to evaluate.
        parent
            The leaf's parent.
        parent_branch
            Parent leaf's branch index.
        kwargs
            Other parameters passed to the new branch.
        """"""
    if not leaf.observed_class_distribution_is_pure():
        split_criterion = self._new_split_criterion()
        best_split_suggestions = leaf.best_split_suggestions(split_criterion, self)
        best_split_suggestions.sort()
        should_split = False
        if len(best_split_suggestions) < 2:
            should_split = len(best_split_suggestions) > 0
        else:
            hoeffding_bound = self._hoeffding_bound(split_criterion.range_of_merit(leaf.stats), self.split_confidence, leaf.total_weight)
            best_suggestion = best_split_suggestions[-1]
            second_best_suggestion = best_split_suggestions[-2]
            if best_suggestion.merit - second_best_suggestion.merit > hoeffding_bound or hoeffding_bound < self.tie_threshold:
                should_split = True
            if self.remove_poor_attrs:
                poor_atts = set()
                for suggestion in best_split_suggestions:
                    if suggestion.feature and best_suggestion.merit - suggestion.merit > hoeffding_bound:
                        poor_atts.add(suggestion.feature)
                for poor_att in poor_atts:
                    leaf.disable_attribute(poor_att)
        if should_split:
            split_decision = best_split_suggestions[-1]
            if split_decision.feature is None:
                leaf.deactivate()
                self._n_inactive_leaves += 1
                self._n_active_leaves -= 1
            else:
                branch = self._branch_selector(split_decision.numerical_feature, split_decision.multiway_split)
                leaves = tuple((self._new_leaf(initial_stats, parent=leaf) for initial_stats in split_decision.children_stats))
                new_split = split_decision.assemble(branch, leaf.stats, leaf.depth, *leaves, **kwargs)
                self._n_active_leaves -= 1
                self._n_active_leaves += len(leaves)
                if parent is None:
                    self._root = new_split
                else:
                    parent.children[parent_branch] = new_split
            self._enforce_size_limit()","for suggestion in best_split_suggestions:
    if suggestion.feature and best_suggestion.merit - suggestion.merit > hoeffding_bound:
        poor_atts.add(suggestion.feature)",poor_atts = {suggestion.feature for suggestion in best_split_suggestions if suggestion.feature and best_suggestion.merit - suggestion.merit > hoeffding_bound},['poor_atts = {suggestion.feature for suggestion in best_split_suggestions if suggestion.feature and best_suggestion.merit - suggestion.merit > hoeffding_bound}'],1,
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/contrib/test_ethosu/test_lower_to_te.py,https://github.com/apache/tvm/tree/master/tests/python/contrib/test_ethosu/test_lower_to_te.py,,test_ethosu_conv2d$27,"def test_ethosu_conv2d():
    ifm = relay.var('ifm', shape=(1, 10, 20, 30), dtype='uint8')
    weight = relay.var('weight', shape=(40, 3, 3, 30), dtype='uint8')
    scale_bias = relay.var('scale_bias', shape=(40, 10), dtype='uint8')
    lut = relay.var('lut', shape=(), dtype='uint8')
    conv = ethosu_ops.ethosu_conv2d(ifm, weight, scale_bias, lut, ifm_scale=0.5, ifm_zero_point=10, weight_zero_point=12, ofm_scale=0.25, ofm_zero_point=14, ofm_channels=40, padding=(1, 1, 1, 1), kernel_shape=(3, 3), strides=(1, 1), dilation=(1, 1))
    expr = relay.Function(relay.analysis.free_vars(conv), conv)
    mod = tvm.IRModule.from_expr(expr)
    mod = relay.transform.InferType()(mod)
    lowered = lower_to_te(mod['main'])
    assert len(lowered.outputs) == 1
    assert len(lowered.inputs) == 4
    conv2d_compute = OperatorCompute.from_output(lowered.outputs[0])
    assert conv2d_compute.op.name == 'ethosu_conv2d'
    input_shapes = set()
    for inp in lowered.inputs:
        input_shapes.add(tuple([x.value for x in inp.shape]))
    assert input_shapes == {(40, 10), (1, 10, 20, 30), (40, 3, 3, 30), ()}","for inp in lowered.inputs:
    input_shapes.add(tuple([x.value for x in inp.shape]))",input_shapes = {tuple([x.value for x in inp.shape]) for inp in lowered.inputs},['input_shapes = {tuple([x.value for x in inp.shape]) for inp in lowered.inputs}'],1,
lemur,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lemur/lemur/sources/service.py,https://github.com/Netflix/lemur/tree/master/lemur/sources/service.py,,add_aws_destination_to_sources$422,"def add_aws_destination_to_sources(dst):
    """"""
    Given a destination, check if it can be added as sources, and include it if not already a source
    We identify qualified destinations based on the sync_as_source attributed of the plugin.
    The destination sync_as_source_name reveals the name of the suitable source-plugin.
    We rely on account numbers to avoid duplicates.
    :return: true for success and false for not adding the destination as source
    """"""
    destination_plugin = plugins.get(dst.plugin_name)
    if destination_plugin.sync_as_source is None or not destination_plugin.sync_as_source:
        return False
    account_number = get_plugin_option('accountNumber', dst.options)
    if account_number is None:
        return False
    path = get_plugin_option('path', dst.options)
    if path is None:
        return False
    src_account_paths = set()
    sources = get_all()
    for src in sources:
        src_account_paths.add((get_plugin_option('accountNumber', src.options), get_plugin_option('path', src.options)))
    if (account_number, path) not in src_account_paths:
        src_options = copy.deepcopy(plugins.get(destination_plugin.sync_as_source_name).options)
        set_plugin_option('accountNumber', account_number, src_options)
        set_plugin_option('path', path, src_options)
        if get_plugin_option('endpointType', src_options) is not None and path == '/cloudfront/':
            set_plugin_option('endpointType', 'cloudfront', src_options)
        create(label=dst.label, plugin_name=destination_plugin.sync_as_source_name, options=src_options, description=dst.description)
        return True
    return False","for src in sources:
    src_account_paths.add((get_plugin_option('accountNumber', src.options), get_plugin_option('path', src.options)))","src_account_paths = {(get_plugin_option('accountNumber', src.options), get_plugin_option('path', src.options)) for src in sources}","[""src_account_paths = {(get_plugin_option('accountNumber', src.options), get_plugin_option('path', src.options)) for src in sources}""]",1,
pgmpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgmpy/pgmpy/inference/mplp.py,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/inference/mplp.py,Mplp,__init__$50,"def __init__(self, model):
    if not isinstance(model, MarkovNetwork):
        raise TypeError('Only MarkovNetwork is supported')
    super(Mplp, self).__init__(model)
    self._initialize_structures()
    self.intersection_set_variables = set()
    for edge_pair in it.combinations(model.edges(), 2):
        self.intersection_set_variables.add(frozenset(edge_pair[0]) & frozenset(edge_pair[1]))
    self.objective = {}
    self.cluster_set = {}
    for factor in model.get_factors():
        scope = frozenset(factor.scope())
        self.objective[scope] = factor
        if len(scope) > 1:
            self.cluster_set[scope] = self.Cluster(self.intersection_set_variables, factor)
    self.dual_lp = sum([np.amax(self.objective[obj].values) for obj in self.objective])
    self.best_int_objective = 0
    self.best_assignment = {}
    self.best_decoded_result = {}
    self.dual_threshold = 0.0002
    self.integrality_gap_threshold = 0.0002","for edge_pair in it.combinations(model.edges(), 2):
    self.intersection_set_variables.add(frozenset(edge_pair[0]) & frozenset(edge_pair[1]))","intersection_set_variables = {frozenset(edge_pair[0]) & frozenset(edge_pair[1]) for edge_pair in it.combinations(model.edges(), 2)}","['self.intersection_set_variables = {frozenset(edge_pair[0]) & frozenset(edge_pair[1]) for edge_pair in it.combinations(model.edges(), 2)}']",0,
TensorNetwork,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorNetwork/tensornetwork/tests/network_test.py,https://github.com/google/TensorNetwork/tree/master/tensornetwork/tests/network_test.py,,test_get_parallel_edge$431,"def test_get_parallel_edge(backend):
    a = tn.Node(np.ones((2,) * 5), backend=backend)
    b = tn.Node(np.ones((2,) * 5), backend=backend)
    edges = set()
    for i in {0, 1, 3}:
        edges.add(tn.connect(a[i], b[i]))
    for e in edges:
        assert set(tn.get_parallel_edges(e)) == edges","for i in {0, 1, 3}:
    edges.add(tn.connect(a[i], b[i]))","edges = {tn.connect(a[i], b[i]) for i in {0, 1, 3}}","['edges = {tn.connect(a[i], b[i]) for i in {0, 1, 3}}']",1,
PaddleSpeech,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSpeech/paddlespeech/t2s/exps/transformer_tts/preprocess.py,https://github.com/PaddlePaddle/PaddleSpeech/tree/master/paddlespeech/t2s/exps/transformer_tts/preprocess.py,,get_input_token$58,"def get_input_token(sentence, output_path):
    """"""get phone set from training data and save it
    
    Args:
        sentence (Dict): sentence: {'utt': ([char], str)}
        output_path (str or path): path to save phone_id_map
    """"""
    phn_token = set()
    for utt in sentence:
        for phn in sentence[utt][0]:
            if phn != '<eos>':
                phn_token.add(phn)
    phn_token = list(phn_token)
    phn_token.sort()
    phn_token = ['<pad>', '<unk>'] + phn_token
    phn_token += ['<eos>']
    with open(output_path, 'w') as f:
        for (i, phn) in enumerate(phn_token):
            f.write(phn + ' ' + str(i) + '\n')","for utt in sentence:
    for phn in sentence[utt][0]:
        if phn != '<eos>':
            phn_token.add(phn)",phn_token = {phn for utt in sentence for phn in sentence[utt][0] if phn != '<eos>'},["phn_token = {phn for utt in sentence for phn in sentence[utt][0] if phn != '<eos>'}"],1,
django-guardian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-guardian/guardian/shortcuts.py,https://github.com/django-guardian/django-guardian/tree/master/guardian/shortcuts.py,,get_objects_for_group$655,"def get_objects_for_group(group, perms, klass=None, any_perm=False, accept_global_perms=True):
    """"""
    Returns queryset of objects for which a given ``group`` has *all*
    permissions present at ``perms``.

    :param group: ``Group`` instance for which objects would be returned.
    :param perms: single permission string, or sequence of permission strings
      which should be checked.
      If ``klass`` parameter is not given, those should be full permission
      names rather than only codenames (i.e. ``auth.change_user``). If more than
      one permission is present within sequence, their content type **must** be
      the same or ``MixedContentTypeError`` exception would be raised.
    :param klass: may be a Model, Manager or QuerySet object. If not given
      this parameter would be computed based on given ``params``.
    :param any_perm: if True, any of permission in sequence is accepted
    :param accept_global_perms: if ``True`` takes global permissions into account.
      If any_perm is set to false then the intersection of matching objects based on global and object based permissions
      is returned. Default is ``True``.

    :raises MixedContentTypeError: when computed content type for ``perms``
      and/or ``klass`` clashes.
    :raises WrongAppError: if cannot compute app label for given ``perms``/
      ``klass``.

    Example:

    Let's assume we have a ``Task`` model belonging to the ``tasker`` app with
    the default add_task, change_task and delete_task permissions provided
    by Django::

        >>> from guardian.shortcuts import get_objects_for_group
        >>> from tasker import Task
        >>> group = Group.objects.create('some group')
        >>> task = Task.objects.create('some task')
        >>> get_objects_for_group(group, 'tasker.add_task')
        []
        >>> from guardian.shortcuts import assign_perm
        >>> assign_perm('tasker.add_task', group, task)
        >>> get_objects_for_group(group, 'tasker.add_task')
        [<Task some task>]

    The permission string can also be an iterable. Continuing with the previous example:
        >>> get_objects_for_group(group, ['tasker.add_task', 'tasker.delete_task'])
        []
        >>> assign_perm('tasker.delete_task', group, task)
        >>> get_objects_for_group(group, ['tasker.add_task', 'tasker.delete_task'])
        [<Task some task>]

    Global permissions assigned to the group are also taken into account. Continuing with previous example:
        >>> task_other = Task.objects.create('other task')
        >>> assign_perm('tasker.change_task', group)
        >>> get_objects_for_group(group, ['tasker.change_task'])
        [<Task some task>, <Task other task>]
        >>> get_objects_for_group(group, ['tasker.change_task'], accept_global_perms=False)
        [<Task some task>]

    """"""
    if isinstance(perms, str):
        perms = [perms]
    ctype = None
    app_label = None
    codenames = set()
    for perm in perms:
        if '.' in perm:
            (new_app_label, codename) = perm.split('.', 1)
            if app_label is not None and app_label != new_app_label:
                raise MixedContentTypeError('Given perms must have same app label (%s != %s)' % (app_label, new_app_label))
            else:
                app_label = new_app_label
        else:
            codename = perm
        codenames.add(codename)
        if app_label is not None:
            new_ctype = ContentType.objects.get(app_label=app_label, permission__codename=codename)
            if ctype is not None and ctype != new_ctype:
                raise MixedContentTypeError('ContentType was once computed to be %s and another one %s' % (ctype, new_ctype))
            else:
                ctype = new_ctype
    if ctype is None and klass is not None:
        queryset = _get_queryset(klass)
        ctype = get_content_type(queryset.model)
    elif ctype is not None and klass is None:
        queryset = _get_queryset(ctype.model_class())
    elif klass is None:
        raise WrongAppError('Cannot determine content type')
    else:
        queryset = _get_queryset(klass)
        if ctype.model_class() != queryset.model:
            raise MixedContentTypeError('Content type for given perms and klass differs')
    global_perms = set()
    if accept_global_perms:
        global_perm_set = group.permissions.values_list('codename', flat=True)
        for code in codenames:
            if code in global_perm_set:
                global_perms.add(code)
        for code in global_perms:
            codenames.remove(code)
        if len(global_perms) > 0 and (len(codenames) == 0 or any_perm):
            return queryset
    group_model = get_group_obj_perms_model(queryset.model)
    groups_obj_perms_queryset = filter_perms_queryset_by_objects(group_model.objects.filter(group=group).filter(permission__content_type=ctype), klass)
    if len(codenames):
        groups_obj_perms_queryset = groups_obj_perms_queryset.filter(permission__codename__in=codenames)
    if group_model.objects.is_generic():
        fields = ['object_pk', 'permission__codename']
    else:
        fields = ['content_object__pk', 'permission__codename']
    if not any_perm and len(codenames):
        groups_obj_perms = groups_obj_perms_queryset.values_list(*fields)
        data = list(groups_obj_perms)
        keyfunc = lambda t: t[0]
        data = sorted(data, key=keyfunc)
        pk_list = []
        for (pk, group) in groupby(data, keyfunc):
            obj_codenames = {e[1] for e in group}
            if any_perm or codenames.issubset(obj_codenames):
                pk_list.append(pk)
        objects = queryset.filter(pk__in=pk_list)
        return objects
    field_pk = fields[0]
    values = groups_obj_perms_queryset
    handle_pk_field = _handle_pk_field(queryset)
    if handle_pk_field is not None:
        values = values.annotate(obj_pk=handle_pk_field(expression=field_pk))
        field_pk = 'obj_pk'
    values = values.values_list(field_pk, flat=True)
    return queryset.filter(pk__in=values)","for code in codenames:
    if code in global_perm_set:
        global_perms.add(code)",global_perms = {code for code in codenames if code in global_perm_set},['global_perms = {code for code in codenames if code in global_perm_set}'],1,
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/paloaltofw.py,https://github.com/google/capirca/tree/master/capirca/lib/paloaltofw.py,Rule,TermToOptions$123,"def TermToOptions(from_zone, to_zone, term, service_map):
    """"""Convert term to Palo Alto security rule options.""""""
    options = {}
    options['from_zone'] = [from_zone]
    options['to_zone'] = [to_zone]
    options['description'] = []
    options['source'] = []
    options['destination'] = []
    options['application'] = []
    options['service'] = []
    options['logging'] = []
    ACTIONS = {'accept': 'allow', 'deny': 'deny', 'reject': 'reset-client', 'reject-with-tcp-rst': 'reset-client'}
    new_term = None

    def pan_ports(ports):
        x = []
        for tup in ports:
            if len(tup) > 1 and tup[0] != tup[1]:
                x.append(str(tup[0]) + '-' + str(tup[1]))
            else:
                x.append(str(tup[0]))
        return tuple(x)
    if term.comment:
        options['description'] = term.comment
    if term.logging:
        for item in term.logging:
            if item.value in ['disable']:
                options['logging'] = ['disable']
                break
            elif item.value in ['log-both']:
                options['logging'].append('log-start')
                options['logging'].append('log-end')
            elif item.value in ['True', 'true', 'syslog', 'local']:
                options['logging'].append('log-end')
    if term.source_address:
        saddr_check = set()
        for saddr in term.source_address:
            saddr_check.add(saddr.parent_token)
        saddr_check = sorted(saddr_check)
        for addr in saddr_check:
            options['source'].append(str(addr))
    if term.destination_address:
        daddr_check = set()
        for daddr in term.destination_address:
            daddr_check.add(daddr.parent_token)
        daddr_check = sorted(daddr_check)
        for addr in daddr_check:
            options['destination'].append(str(addr))
    if term.action:
        options['action'] = ACTIONS[term.action[0]]
    if term.option:
        options['option'] = term.option
    if term.pan_application:
        for pan_app in term.pan_application:
            options['application'].append(pan_app)
    if term.source_port or term.destination_port:
        src_ports = pan_ports(term.source_port)
        if term.destination_port:
            ports = pan_ports(term.destination_port)
        else:
            ports = pan_ports([('0', '65535')])
        for p in term.protocol:
            service_name = service_map.get_service_name(term.name, src_ports, ports, p)
            if service_name not in options['service']:
                options['service'].append(service_name)
    elif 'tcp' in term.protocol or 'udp' in term.protocol:
        services = {'tcp', 'udp'} & set(term.protocol)
        others = set(term.protocol) - services
        if others:
            logging.info('INFO: Term %s in policy %s>%s contains port-less %s with non-port protocol(s). Moving %s to a new term.', term.name, from_zone, to_zone, ', '.join(list(services)), ', '.join(list(others)))
            new_term = copy.deepcopy(term)
            new_term.protocol = list(others)
            term.protocol = list(services)
            options['application'] = []
        for p in term.protocol:
            ports = pan_ports([('0', '65535')])
            service_name = service_map.get_service_name('any', (), ports, p, '')
            if service_name not in options['service']:
                options['service'].append(service_name)
    if term.protocol:
        for proto_name in term.protocol:
            if proto_name in ['igmp', 'sctp', 'gre'] and proto_name not in options['application']:
                options['application'].append(proto_name)
    return (options, new_term)","for saddr in term.source_address:
    saddr_check.add(saddr.parent_token)",saddr_check = {saddr.parent_token for saddr in term.source_address},['saddr_check = {saddr.parent_token for saddr in term.source_address}'],1,
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/paloaltofw.py,https://github.com/google/capirca/tree/master/capirca/lib/paloaltofw.py,Rule,TermToOptions$123,"def TermToOptions(from_zone, to_zone, term, service_map):
    """"""Convert term to Palo Alto security rule options.""""""
    options = {}
    options['from_zone'] = [from_zone]
    options['to_zone'] = [to_zone]
    options['description'] = []
    options['source'] = []
    options['destination'] = []
    options['application'] = []
    options['service'] = []
    options['logging'] = []
    ACTIONS = {'accept': 'allow', 'deny': 'deny', 'reject': 'reset-client', 'reject-with-tcp-rst': 'reset-client'}
    new_term = None

    def pan_ports(ports):
        x = []
        for tup in ports:
            if len(tup) > 1 and tup[0] != tup[1]:
                x.append(str(tup[0]) + '-' + str(tup[1]))
            else:
                x.append(str(tup[0]))
        return tuple(x)
    if term.comment:
        options['description'] = term.comment
    if term.logging:
        for item in term.logging:
            if item.value in ['disable']:
                options['logging'] = ['disable']
                break
            elif item.value in ['log-both']:
                options['logging'].append('log-start')
                options['logging'].append('log-end')
            elif item.value in ['True', 'true', 'syslog', 'local']:
                options['logging'].append('log-end')
    if term.source_address:
        saddr_check = set()
        for saddr in term.source_address:
            saddr_check.add(saddr.parent_token)
        saddr_check = sorted(saddr_check)
        for addr in saddr_check:
            options['source'].append(str(addr))
    if term.destination_address:
        daddr_check = set()
        for daddr in term.destination_address:
            daddr_check.add(daddr.parent_token)
        daddr_check = sorted(daddr_check)
        for addr in daddr_check:
            options['destination'].append(str(addr))
    if term.action:
        options['action'] = ACTIONS[term.action[0]]
    if term.option:
        options['option'] = term.option
    if term.pan_application:
        for pan_app in term.pan_application:
            options['application'].append(pan_app)
    if term.source_port or term.destination_port:
        src_ports = pan_ports(term.source_port)
        if term.destination_port:
            ports = pan_ports(term.destination_port)
        else:
            ports = pan_ports([('0', '65535')])
        for p in term.protocol:
            service_name = service_map.get_service_name(term.name, src_ports, ports, p)
            if service_name not in options['service']:
                options['service'].append(service_name)
    elif 'tcp' in term.protocol or 'udp' in term.protocol:
        services = {'tcp', 'udp'} & set(term.protocol)
        others = set(term.protocol) - services
        if others:
            logging.info('INFO: Term %s in policy %s>%s contains port-less %s with non-port protocol(s). Moving %s to a new term.', term.name, from_zone, to_zone, ', '.join(list(services)), ', '.join(list(others)))
            new_term = copy.deepcopy(term)
            new_term.protocol = list(others)
            term.protocol = list(services)
            options['application'] = []
        for p in term.protocol:
            ports = pan_ports([('0', '65535')])
            service_name = service_map.get_service_name('any', (), ports, p, '')
            if service_name not in options['service']:
                options['service'].append(service_name)
    if term.protocol:
        for proto_name in term.protocol:
            if proto_name in ['igmp', 'sctp', 'gre'] and proto_name not in options['application']:
                options['application'].append(proto_name)
    return (options, new_term)","for daddr in term.destination_address:
    daddr_check.add(daddr.parent_token)",daddr_check = {daddr.parent_token for daddr in term.destination_address},['daddr_check = {daddr.parent_token for daddr in term.destination_address}'],1,
gsutil,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gsutil/gslib/wildcard_iterator.py,https://github.com/GoogleCloudPlatform/gsutil/tree/master/gslib/wildcard_iterator.py,CloudWildcardIterator,_GetToListFields$445,"def _GetToListFields(self, get_fields=None):
    """"""Prepends 'items/' to the input fields and converts it to a set.

    This way field sets requested for GetBucket can be used in ListBucket calls.
    Note that the input set must contain only bucket or object fields; listing
    fields such as prefixes or nextPageToken should be added after calling
    this function.

    Args:
      get_fields: Iterable fields usable in GetBucket/GetObject calls.

    Returns:
      Set of fields usable in ListBuckets/ListObjects calls.
    """"""
    if get_fields:
        list_fields = set()
        for field in get_fields:
            list_fields.add('items/' + field)
        return list_fields","for field in get_fields:
    list_fields.add('items/' + field)",list_fields = {'items/' + field for field in get_fields},["list_fields = {'items/' + field for field in get_fields}"],1,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/functions.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/functions.py,,parse_accept_language$227,"def parse_accept_language(language_header, restrict=True):
    ok_languages = set()
    for lang in word_collection.keys():
        ok_languages.add(lang)
    languages = list()
    for item in language_header.split(','):
        q = 1.0
        lang = item.strip()
        if ';' in lang:
            parts = item.split(';')
            lang = parts[0]
            q = parts[1]
            try:
                q = float(re.sub('^q=', '', q, flags=re.IGNORECASE))
            except:
                q = 1.0
        parts = re.split('-|_', lang)
        languages.append([parts[0].strip().lower(), q])
    output = list()
    for item in sorted(languages, key=lambda y: y[1], reverse=True):
        if restrict and item[0] not in ok_languages:
            continue
        if item[0] not in output:
            output.append(item[0])
    return output","for lang in word_collection.keys():
    ok_languages.add(lang)",ok_languages = {lang for lang in word_collection.keys()},['ok_languages = {lang for lang in word_collection.keys()}'],1,
core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/homeassistant/components/unifi/controller.py,https://github.com/home-assistant/core/tree/master/homeassistant/components/unifi/controller.py,UniFiController,update_wireless_clients$287,"def update_wireless_clients(self):
    """"""Update set of known to be wireless clients.""""""
    new_wireless_clients = set()
    for client_id in self.api.clients:
        if client_id not in self.wireless_clients and (not self.api.clients[client_id].is_wired):
            new_wireless_clients.add(client_id)
    if new_wireless_clients:
        self.wireless_clients |= new_wireless_clients
        unifi_wireless_clients = self.hass.data[UNIFI_WIRELESS_CLIENTS]
        unifi_wireless_clients.update_data(self.wireless_clients, self.config_entry)","for client_id in self.api.clients:
    if client_id not in self.wireless_clients and (not self.api.clients[client_id].is_wired):
        new_wireless_clients.add(client_id)",new_wireless_clients = {client_id for client_id in self.api.clients if client_id not in self.wireless_clients and (not self.api.clients[client_id].is_wired)},['new_wireless_clients = {client_id for client_id in self.api.clients if client_id not in self.wireless_clients and (not self.api.clients[client_id].is_wired)}'],1,
pyproj,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyproj/test/test_sync.py,https://github.com/pyproj4/pyproj/tree/master/test/test_sync.py,,test_get_transform_grid_list__contains$75,"def test_get_transform_grid_list__contains():
    grids = get_transform_grid_list(bbox=BBox(170, -90, -170, 90), spatial_test='contains', include_already_downloaded=True)
    assert len(grids) > 5
    source_ids = set()
    for grid in grids:
        source_ids.add(grid['properties']['source_id'])
    assert sorted(source_ids) == ['nz_linz']","for grid in grids:
    source_ids.add(grid['properties']['source_id'])",source_ids = {grid['properties']['source_id'] for grid in grids},["source_ids = {grid['properties']['source_id'] for grid in grids}"],1,
pyproj,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyproj/test/test_sync.py,https://github.com/pyproj4/pyproj/tree/master/test/test_sync.py,,test_get_transform_grid_list__bbox__out_of_bounds$42,"def test_get_transform_grid_list__bbox__out_of_bounds():
    grids = get_transform_grid_list(bbox=BBox(170, -90, 190, 90), include_already_downloaded=True)
    assert len(grids) > 10
    source_ids = set()
    for grid in grids:
        source_ids.add(grid['properties']['source_id'])
    assert sorted(source_ids) == ['au_ga', 'nc_dittt', 'no_kv', 'nz_linz', 'us_nga', 'us_noaa']","for grid in grids:
    source_ids.add(grid['properties']['source_id'])",source_ids = {grid['properties']['source_id'] for grid in grids},["source_ids = {grid['properties']['source_id'] for grid in grids}"],1,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/beacons/inotify.py,https://github.com/saltstack/salt/tree/master/salt/beacons/inotify.py,,beacon$174,"def beacon(config):
    """"""
    Watch the configured files

    Example Config

    .. code-block:: yaml

        beacons:
          inotify:
            - files:
                /path/to/file/or/dir:
                  mask:
                    - open
                    - create
                    - close_write
                  recurse: True
                  auto_add: True
                  exclude:
                    - /path/to/file/or/dir/exclude1
                    - /path/to/file/or/dir/exclude2
                    - /path/to/file/or/dir/regex[a-m]*$:
                        regex: True
            - coalesce: True

    The mask list can contain the following events (the default mask is create,
    delete, and modify):

    * access            - File accessed
    * attrib            - File metadata changed
    * close_nowrite     - Unwritable file closed
    * close_write       - Writable file closed
    * create            - File created in watched directory
    * delete            - File deleted from watched directory
    * delete_self       - Watched file or directory deleted
    * modify            - File modified
    * moved_from        - File moved out of watched directory
    * moved_to          - File moved into watched directory
    * move_self         - Watched file moved
    * open              - File opened

    The mask can also contain the following options:

    * dont_follow       - Don't dereference symbolic links
    * excl_unlink       - Omit events for children after they have been unlinked
    * oneshot           - Remove watch after one event
    * onlydir           - Operate only if name is directory

    recurse:
      Recursively watch files in the directory
    auto_add:
      Automatically start watching files that are created in the watched directory
    exclude:
      Exclude directories or files from triggering events in the watched directory.
      Can use regex if regex is set to True
    coalesce:
      If this coalescing option is enabled, events are filtered based on
      their unicity, only unique events are enqueued, doublons are discarded.
      An event is unique when the combination of its fields (wd, mask,
      cookie, name) is unique among events of a same batch. After a batch of
      events is processed any events are accepted again.
      This option is top-level (at the same level as the path) and therefore
      affects all paths that are being watched. This is due to this option
      being at the Notifier level in pyinotify.
    """"""
    whitelist = ['_beacon_name']
    config = salt.utils.beacons.remove_hidden_options(config, whitelist)
    config = salt.utils.beacons.list_to_dict(config)
    ret = []
    notifier = _get_notifier(config)
    wm = notifier._watch_manager
    if notifier.check_events(1):
        notifier.read_events()
        notifier.process_events()
        queue = __context__['inotify.queue']
        while queue:
            event = queue.popleft()
            _append = True
            path = event.path
            while path != '/':
                if path in config.get('files', {}):
                    break
                path = os.path.dirname(path)
            excludes = config['files'].get(path, {}).get('exclude', '')
            if excludes and isinstance(excludes, list):
                for exclude in excludes:
                    if isinstance(exclude, dict):
                        _exclude = next(iter(exclude))
                        if exclude[_exclude].get('regex', False):
                            try:
                                if re.search(_exclude, event.pathname):
                                    _append = False
                            except Exception:
                                log.warning('Failed to compile regex: %s', _exclude)
                        else:
                            exclude = _exclude
                    elif '*' in exclude:
                        if fnmatch.fnmatch(event.pathname, exclude):
                            _append = False
                    elif event.pathname.startswith(exclude):
                        _append = False
            if _append:
                sub = {'tag': event.path, 'path': event.pathname, 'change': event.maskname}
                ret.append(sub)
            else:
                log.info('Excluding %s from event for %s', event.pathname, path)
    current = set()
    for wd in wm.watches:
        current.add(wm.watches[wd].path)
    for path in config.get('files', ()):
        if isinstance(config['files'][path], dict):
            mask = config['files'][path].get('mask', DEFAULT_MASK)
            if isinstance(mask, list):
                r_mask = 0
                for sub in mask:
                    r_mask |= _get_mask(sub)
            elif isinstance(mask, bytes):
                r_mask = _get_mask(mask)
            else:
                r_mask = mask
            mask = r_mask
            rec = config['files'][path].get('recurse', False)
            auto_add = config['files'][path].get('auto_add', False)
        else:
            mask = DEFAULT_MASK
            rec = False
            auto_add = False
        if path in current:
            for wd in wm.watches:
                if path == wm.watches[wd].path:
                    update = False
                    if wm.watches[wd].mask != mask:
                        update = True
                    if wm.watches[wd].auto_add != auto_add:
                        update = True
                    if update:
                        wm.update_watch(wd, mask=mask, rec=rec, auto_add=auto_add)
        elif os.path.exists(path):
            excludes = config['files'][path].get('exclude', '')
            excl = None
            if isinstance(excludes, list):
                excl = []
                for exclude in excludes:
                    if isinstance(exclude, dict):
                        excl.append(list(exclude)[0])
                    else:
                        excl.append(exclude)
                excl = pyinotify.ExcludeFilter(excl)
            wm.add_watch(path, mask, rec=rec, auto_add=auto_add, exclude_filter=excl)
    return ret","for wd in wm.watches:
    current.add(wm.watches[wd].path)",current = {wm.watches[wd].path for wd in wm.watches},['current = {wm.watches[wd].path for wd in wm.watches}'],1,
WhatBreach,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WhatBreach/whatbreach/main.py,https://github.com/Ekultek/WhatBreach/tree/master/whatbreach/main.py,,main$29,"def main():
    try:
        opt = Parser().optparse()
        print(BANNER)
        res = Parser().check_opts(opt)
        if res is not None:
            to_search = res
        else:
            to_search = []
        do_not_search = []
        if len(to_search) == 0:
            if opt.singleEmail is None and opt.emailFile is None:
                warn('you have not provided an email to scan, redirecting to the help menu')
                subprocess.call(['python', 'whatbreach.py', '--help'])
                exit(1)
            api_tokens = grab_api_tokens()
            if opt.searchHunterIo and opt.singleEmail is not None:
                info('starting search on hunter.io using {}'.format(opt.singleEmail))
                file_results = HunterIoHook(opt.singleEmail, api_tokens['hunter.io'], verify_emails=opt.verifyEmailsThroughHunterIo).hooker()
                if file_results is not None:
                    with open(file_results) as data:
                        emails = json.loads(data.read())['discovered_emails']
                    for email in emails:
                        to_search.append(email)
                else:
                    to_search.append(opt.singleEmail)
            elif opt.searchHunterIo and opt.emailFile is not None:
                if not test_file(opt.emailFile):
                    error('unable to open filename, does it exist?')
                    exit(1)
                api_tokens = grab_api_tokens()
                with open(opt.emailFile) as data:
                    for email in data.readlines():
                        email = email.strip()
                        file_results = HunterIoHook(email, api_tokens['hunter.io'], verify_emails=opt.verifyEmailsThroughHunterIo).hooker()
                        with open(file_results) as results:
                            discovered_emails = json.loads(results.read())['discovered_emails']
                        for discovered in discovered_emails:
                            to_search.append(discovered)
            elif opt.singleEmail is not None:
                info('starting search on single email address: {}'.format(opt.singleEmail))
                to_search = [opt.singleEmail]
            elif opt.emailFile is not None:
                if not test_file(opt.emailFile):
                    error('unable to open filename, does it exist?')
                    exit(1)
                with open(opt.emailFile) as emails:
                    info('parsing email file: {}'.format(opt.emailFile))
                    to_search = emails.readlines()
                info('starting search on a total of {} email(s)'.format(len(to_search)))
        for email in to_search:
            email = email.strip()
            if opt.checkTenMinuteEmail:
                if check_ten_minute_email(email, TEN_MINUTE_EMAIL_EXTENSION_LIST):
                    warn('email: {} appears to be a ten minute email'.format(email))
                    answer = prompt('would you like to process the email[y/N]')
                    if answer.startswith('n'):
                        do_not_search.append(email)
            if opt.checkEmailAccounts:
                info('searching for possible profiles related to {}'.format(email))
                searcher = EmailRepHook(email)
                results = searcher.hooker()
                if results is not None and len(results) != 0:
                    info('found a total of {} possible profiles associated with {} on the following domains:'.format(len(results), email))
                    for domain in results:
                        print('\t-> {}'.format(domain.title()))
                else:
                    warn('no possible profiles discovered for email: {}'.format(email))
            if email not in do_not_search:
                if opt.throttleRequests != 0:
                    time.sleep(opt.throttleRequests)
                info('searching breached accounts on HIBP related to: {}'.format(email))
                account_dumps = BeenPwnedHook(email, api_tokens['haveibeenpwned.com'], opt, retry=opt.retryOnFail).account_hooker()
                info('searching for paste dumps on HIBP related to: {}'.format(email))
                if opt.searchPastebin:
                    paste_dumps = BeenPwnedHook(email, api_tokens['haveibeenpwned.com'], opt, retry=opt.retryOnFail).paste_hooker()
                else:
                    warn('suppressing discovered pastes')
                    paste_dumps = []
                if opt.searchWeLeakInfo:
                    info('searching weleakinfo.com for breaches related to: {}'.format(email))
                    searcher = WeLeakInfoHook(email, api_tokens['weleakinfo.com'])
                    tmp = set()
                    results = searcher.hooker()
                    if results is not None:
                        if account_dumps is not None:
                            original_length = len(account_dumps)
                        else:
                            original_length = 0
                        if account_dumps is not None:
                            for item in account_dumps:
                                tmp.add(item)
                        if results is not None:
                            for item in results:
                                tmp.add(item)
                        if len(tmp) != 0:
                            account_dumps = list(tmp)
                            new_length = len(account_dumps)
                            amount_discovered = new_length - original_length
                            if amount_discovered != 0:
                                info('discovered a total of {} more breaches from weleakinfo.com'.format(new_length - original_length))
                            else:
                                warn('did not discover any breaches')
                        else:
                            warn('did not discover any new databases from weleakinfo.com')
                    else:
                        warn('no databases discovered on weleakinfo')
                if opt.searchSnusBase:
                    info(""searching snusbase.com for breaches related to '{}'"".format(email))
                    snusbase_leaks = SnusbaseHooker(email, api_tokens['snusbase.com']['username'], api_tokens['snusbase.com']['password']).main()
                    if snusbase_leaks is not None and len(snusbase_leaks) != 0:
                        info('found a total of {} more leaks using snusbase'.format(len(snusbase_leaks)))
                        for item in snusbase_leaks:
                            account_dumps.append(item)
                            set(account_dumps)
                            account_dumps = list(account_dumps)
                    else:
                        warn('did not find anymore leaks using snusbase')
                if account_dumps is not None and paste_dumps is not None and (len(account_dumps) != 0):
                    info('found a total of {} database breach(es) and a total of {} paste(s) pertaining to: {}'.format(len(account_dumps), len(paste_dumps), email))
                    if opt.searchDehashed:
                        if len(account_dumps) > 20:
                            warn('large amount of database breaches, obtaining links from dehashed (this may take a minute)')
                        found_databases = DehashedHook(account_dumps).hooker()
                    else:
                        warn('suppressing discovered databases')
                        found_databases = {}
                    for (i, dump) in enumerate(paste_dumps, start=1):
                        found_databases['Paste#{}'.format(i)] = str(dump)
                    display_found_databases(found_databases, download_pastes=opt.downloadPastes)
                    if opt.downloadDatabase:
                        for item in found_databases.keys():
                            if 'Paste' not in item:
                                info('searching for downloadable databases using query: {}'.format(item.lower()))
                                downloaded = DatabasesTodayHook(str(item), downloads_directory=opt.saveDirectory).hooker()
                                if len(downloaded) != 0:
                                    info('downloaded a total of {} database(s) pertaining to query: {}'.format(len(downloaded), item))
                                    display_found_databases(downloaded, is_downloaded=True, download_pastes=opt.downloadPastes)
                                else:
                                    warn('no databases appeared to be present and downloadable related to query: {}'.format(str(item)))
                elif account_dumps is not None and paste_dumps is None and (len(account_dumps) != 0):
                    info('found a total of {} database breach(es) pertaining to: {}'.format(len(account_dumps), email))
                    if opt.searchDehashed:
                        if len(account_dumps) > 20:
                            warn('large amount of database breaches, obtaining links from dehashed (this may take a minute)')
                        found_databases = DehashedHook(account_dumps).hooker()
                    else:
                        warn('suppressing discovered databases')
                        found_databases = {}
                    if len(found_databases) != 0:
                        display_found_databases(found_databases, download_pastes=opt.downloadPastes)
                        if opt.downloadDatabase:
                            for item in found_databases.keys():
                                if 'Paste' not in item:
                                    info('searching for downloadable databases using query: {}'.format(item.lower()))
                                    downloaded = DatabasesTodayHook(str(item), downloads_directory=opt.saveDirectory).hooker()
                                    if len(downloaded) != 0:
                                        info('downloaded a total of {} database(s) pertaining to query: {}'.format(len(downloaded), item))
                                        display_found_databases(downloaded, is_downloaded=True, download_pastes=opt.downloadPastes)
                                    else:
                                        warn('no databases appeared to be present and downloadable related to query: {}'.format(str(item)))
                    else:
                        warn('no output to show, most likely due to output suppression or dehashed')
                elif account_dumps is None and paste_dumps is not None:
                    error('no database dumps found nor any pastes found for: {}'.format(email))
                else:
                    error('email {} was not found in any breach'.format(email))
        if opt.staySalty:
            warn('all this code was stolen with <3 by Eku')
    except KeyboardInterrupt:
        error('user quit the session')","for item in account_dumps:
    tmp.add(item)",tmp = {item for item in account_dumps},['tmp = {item for item in account_dumps}'],1,
WhatBreach,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WhatBreach/whatbreach/main.py,https://github.com/Ekultek/WhatBreach/tree/master/whatbreach/main.py,,main$29,"def main():
    try:
        opt = Parser().optparse()
        print(BANNER)
        res = Parser().check_opts(opt)
        if res is not None:
            to_search = res
        else:
            to_search = []
        do_not_search = []
        if len(to_search) == 0:
            if opt.singleEmail is None and opt.emailFile is None:
                warn('you have not provided an email to scan, redirecting to the help menu')
                subprocess.call(['python', 'whatbreach.py', '--help'])
                exit(1)
            api_tokens = grab_api_tokens()
            if opt.searchHunterIo and opt.singleEmail is not None:
                info('starting search on hunter.io using {}'.format(opt.singleEmail))
                file_results = HunterIoHook(opt.singleEmail, api_tokens['hunter.io'], verify_emails=opt.verifyEmailsThroughHunterIo).hooker()
                if file_results is not None:
                    with open(file_results) as data:
                        emails = json.loads(data.read())['discovered_emails']
                    for email in emails:
                        to_search.append(email)
                else:
                    to_search.append(opt.singleEmail)
            elif opt.searchHunterIo and opt.emailFile is not None:
                if not test_file(opt.emailFile):
                    error('unable to open filename, does it exist?')
                    exit(1)
                api_tokens = grab_api_tokens()
                with open(opt.emailFile) as data:
                    for email in data.readlines():
                        email = email.strip()
                        file_results = HunterIoHook(email, api_tokens['hunter.io'], verify_emails=opt.verifyEmailsThroughHunterIo).hooker()
                        with open(file_results) as results:
                            discovered_emails = json.loads(results.read())['discovered_emails']
                        for discovered in discovered_emails:
                            to_search.append(discovered)
            elif opt.singleEmail is not None:
                info('starting search on single email address: {}'.format(opt.singleEmail))
                to_search = [opt.singleEmail]
            elif opt.emailFile is not None:
                if not test_file(opt.emailFile):
                    error('unable to open filename, does it exist?')
                    exit(1)
                with open(opt.emailFile) as emails:
                    info('parsing email file: {}'.format(opt.emailFile))
                    to_search = emails.readlines()
                info('starting search on a total of {} email(s)'.format(len(to_search)))
        for email in to_search:
            email = email.strip()
            if opt.checkTenMinuteEmail:
                if check_ten_minute_email(email, TEN_MINUTE_EMAIL_EXTENSION_LIST):
                    warn('email: {} appears to be a ten minute email'.format(email))
                    answer = prompt('would you like to process the email[y/N]')
                    if answer.startswith('n'):
                        do_not_search.append(email)
            if opt.checkEmailAccounts:
                info('searching for possible profiles related to {}'.format(email))
                searcher = EmailRepHook(email)
                results = searcher.hooker()
                if results is not None and len(results) != 0:
                    info('found a total of {} possible profiles associated with {} on the following domains:'.format(len(results), email))
                    for domain in results:
                        print('\t-> {}'.format(domain.title()))
                else:
                    warn('no possible profiles discovered for email: {}'.format(email))
            if email not in do_not_search:
                if opt.throttleRequests != 0:
                    time.sleep(opt.throttleRequests)
                info('searching breached accounts on HIBP related to: {}'.format(email))
                account_dumps = BeenPwnedHook(email, api_tokens['haveibeenpwned.com'], opt, retry=opt.retryOnFail).account_hooker()
                info('searching for paste dumps on HIBP related to: {}'.format(email))
                if opt.searchPastebin:
                    paste_dumps = BeenPwnedHook(email, api_tokens['haveibeenpwned.com'], opt, retry=opt.retryOnFail).paste_hooker()
                else:
                    warn('suppressing discovered pastes')
                    paste_dumps = []
                if opt.searchWeLeakInfo:
                    info('searching weleakinfo.com for breaches related to: {}'.format(email))
                    searcher = WeLeakInfoHook(email, api_tokens['weleakinfo.com'])
                    tmp = set()
                    results = searcher.hooker()
                    if results is not None:
                        if account_dumps is not None:
                            original_length = len(account_dumps)
                        else:
                            original_length = 0
                        if account_dumps is not None:
                            for item in account_dumps:
                                tmp.add(item)
                        if results is not None:
                            for item in results:
                                tmp.add(item)
                        if len(tmp) != 0:
                            account_dumps = list(tmp)
                            new_length = len(account_dumps)
                            amount_discovered = new_length - original_length
                            if amount_discovered != 0:
                                info('discovered a total of {} more breaches from weleakinfo.com'.format(new_length - original_length))
                            else:
                                warn('did not discover any breaches')
                        else:
                            warn('did not discover any new databases from weleakinfo.com')
                    else:
                        warn('no databases discovered on weleakinfo')
                if opt.searchSnusBase:
                    info(""searching snusbase.com for breaches related to '{}'"".format(email))
                    snusbase_leaks = SnusbaseHooker(email, api_tokens['snusbase.com']['username'], api_tokens['snusbase.com']['password']).main()
                    if snusbase_leaks is not None and len(snusbase_leaks) != 0:
                        info('found a total of {} more leaks using snusbase'.format(len(snusbase_leaks)))
                        for item in snusbase_leaks:
                            account_dumps.append(item)
                            set(account_dumps)
                            account_dumps = list(account_dumps)
                    else:
                        warn('did not find anymore leaks using snusbase')
                if account_dumps is not None and paste_dumps is not None and (len(account_dumps) != 0):
                    info('found a total of {} database breach(es) and a total of {} paste(s) pertaining to: {}'.format(len(account_dumps), len(paste_dumps), email))
                    if opt.searchDehashed:
                        if len(account_dumps) > 20:
                            warn('large amount of database breaches, obtaining links from dehashed (this may take a minute)')
                        found_databases = DehashedHook(account_dumps).hooker()
                    else:
                        warn('suppressing discovered databases')
                        found_databases = {}
                    for (i, dump) in enumerate(paste_dumps, start=1):
                        found_databases['Paste#{}'.format(i)] = str(dump)
                    display_found_databases(found_databases, download_pastes=opt.downloadPastes)
                    if opt.downloadDatabase:
                        for item in found_databases.keys():
                            if 'Paste' not in item:
                                info('searching for downloadable databases using query: {}'.format(item.lower()))
                                downloaded = DatabasesTodayHook(str(item), downloads_directory=opt.saveDirectory).hooker()
                                if len(downloaded) != 0:
                                    info('downloaded a total of {} database(s) pertaining to query: {}'.format(len(downloaded), item))
                                    display_found_databases(downloaded, is_downloaded=True, download_pastes=opt.downloadPastes)
                                else:
                                    warn('no databases appeared to be present and downloadable related to query: {}'.format(str(item)))
                elif account_dumps is not None and paste_dumps is None and (len(account_dumps) != 0):
                    info('found a total of {} database breach(es) pertaining to: {}'.format(len(account_dumps), email))
                    if opt.searchDehashed:
                        if len(account_dumps) > 20:
                            warn('large amount of database breaches, obtaining links from dehashed (this may take a minute)')
                        found_databases = DehashedHook(account_dumps).hooker()
                    else:
                        warn('suppressing discovered databases')
                        found_databases = {}
                    if len(found_databases) != 0:
                        display_found_databases(found_databases, download_pastes=opt.downloadPastes)
                        if opt.downloadDatabase:
                            for item in found_databases.keys():
                                if 'Paste' not in item:
                                    info('searching for downloadable databases using query: {}'.format(item.lower()))
                                    downloaded = DatabasesTodayHook(str(item), downloads_directory=opt.saveDirectory).hooker()
                                    if len(downloaded) != 0:
                                        info('downloaded a total of {} database(s) pertaining to query: {}'.format(len(downloaded), item))
                                        display_found_databases(downloaded, is_downloaded=True, download_pastes=opt.downloadPastes)
                                    else:
                                        warn('no databases appeared to be present and downloadable related to query: {}'.format(str(item)))
                    else:
                        warn('no output to show, most likely due to output suppression or dehashed')
                elif account_dumps is None and paste_dumps is not None:
                    error('no database dumps found nor any pastes found for: {}'.format(email))
                else:
                    error('email {} was not found in any breach'.format(email))
        if opt.staySalty:
            warn('all this code was stolen with <3 by Eku')
    except KeyboardInterrupt:
        error('user quit the session')","for item in results:
    tmp.add(item)",tmp = {item for item in results},Cannot refactor,2,
mindmeld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mindmeld/mindmeld/components/role_classifier.py,https://github.com/cisco/mindmeld/tree/master/mindmeld/components/role_classifier.py,RoleClassifier,fit$79,"def fit(self, queries=None, label_set=None, incremental_timestamp=None, load_cached=True, **kwargs):
    """"""Trains a statistical model for role classification using the provided training examples.

        Args:
            queries (list of ProcessedQuery): The labeled queries to use as training data
            label_set (list, optional): A label set to load. If not specified, the default
                training set will be loaded.
            incremental_timestamp (str, optional): The timestamp folder to cache models in
        """"""
    logger.info('Fitting role classifier: domain=%r, intent=%r, entity_type=%r', self.domain, self.intent, self.entity_type)
    model_config = self._get_model_config(**kwargs)
    label_set = label_set or model_config.train_label_set or DEFAULT_TRAIN_SET_REGEX
    queries = self._resolve_queries(queries, label_set)
    new_hash = self._get_model_hash(model_config, queries)
    cached_model_path = self._resource_loader.hash_to_model_path.get(new_hash)
    if incremental_timestamp and cached_model_path:
        logger.info('No need to fit. Previous model is cached.')
        if load_cached:
            self.load(cached_model_path)
            return True
        return False
    (examples, labels) = self._get_examples_and_labels(queries)
    if examples:
        self.roles = set()
        for label in labels:
            self.roles.add(label)
        model = create_model(model_config)
        model.initialize_resources(self._resource_loader, examples, labels)
        model.fit(examples, labels)
        self._model = model
        self.config = ClassifierConfig.from_model_config(self._model.config)
    self.hash = new_hash
    self.ready = True
    self.dirty = True
    return True","for label in labels:
    self.roles.add(label)",roles = {label for label in labels},['self.roles = {label for label in labels}'],0,
yandex_smart_home,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yandex_smart_home/custom_components/yandex_smart_home/capability_color.py,https://github.com/dmitry-k/yandex_smart_home/tree/master/custom_components/yandex_smart_home/capability_color.py,ColorSettingCapability,get_supported_scenes$150,"def get_supported_scenes(scenes_map: dict[str, list[str]], entity_effect_list: list[str]) -> list[str]:
    yandex_scenes = set()
    for effect in entity_effect_list:
        for (yandex_scene, ha_effects) in scenes_map.items():
            if effect in ha_effects:
                yandex_scenes.add(yandex_scene)
    return sorted(list(yandex_scenes))","for effect in entity_effect_list:
    for (yandex_scene, ha_effects) in scenes_map.items():
        if effect in ha_effects:
            yandex_scenes.add(yandex_scene)","yandex_scenes = {yandex_scene for effect in entity_effect_list for (yandex_scene, ha_effects) in scenes_map.items() if effect in ha_effects}","['yandex_scenes = {yandex_scene for effect in entity_effect_list for (yandex_scene, ha_effects) in scenes_map.items() if effect in ha_effects}']",1,
GeneticAlgorithmsWithPython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GeneticAlgorithmsWithPython/ch18/ticTacToeTests.py,https://github.com/handcraftsman/GeneticAlgorithmsWithPython/tree/master/ch18/ticTacToeTests.py,LocationFilter,get_matches$524,"def get_matches(self, board, squares):
    result = set()
    for square in squares:
        if self.func(square):
            result.add(square.Index)
    return result","for square in squares:
    if self.func(square):
        result.add(square.Index)",result = {square.Index for square in squares if self.func(square)},['result = {square.Index for square in squares if self.func(square)}'],1,
onnxmltools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/onnxmltools/onnxmltools/convert/coreml/_parse.py,https://github.com/onnx/onnxmltools/tree/master/onnxmltools/convert/coreml/_parse.py,,parse_coreml$427,"def parse_coreml(model, initial_types=None, target_opset=None, custom_conversion_functions=None, custom_shape_calculators=None):
    """"""
    This is the root function of the whole parsing procedure.
    :param model: CoreML model
    :param initial_types: A list providing some types for some root variables. Each element is a tuple of a variable
    name and a type defined in data_types.py.
    :param target_opset: number, for example, 7 for ONNX 1.2, and 8 for ONNX 1.3.
    :param custom_conversion_functions: a dictionary for specifying the user customized conversion function
    :param custom_shape_calculators: a dictionary for specifying the user customized shape calculator
    :return: a Topology object. It's a intermediate representation of the input CoreML model
    """"""
    reserved_variable_names = set()
    for var in list(model.description.input) + list(model.description.output):
        reserved_variable_names.add(var.name)
    default_batch_size = 'None'
    topology = Topology(CoremlModelContainer(model), default_batch_size, initial_types, reserved_variable_names, target_opset=target_opset, custom_conversion_functions=custom_conversion_functions, custom_shape_calculators=custom_shape_calculators)
    scope = topology.declare_scope('__root__')
    _parse_model(topology, scope, model)
    topology.compile()
    for variable in topology.find_root_and_sink_variables():
        color_space = getattr(variable.type, 'color_space', None)
        if color_space:
            if topology.metadata_props.setdefault('Image.BitmapPixelFormat', color_space) != color_space:
                warnings.warn('Conflicting pixel formats found. In ONNX, all input/output images must use the same pixel format.')
        if variable.raw_name not in reserved_variable_names:
            continue
        topology.rename_variable(variable.onnx_name, variable.raw_name)
    return topology","for var in list(model.description.input) + list(model.description.output):
    reserved_variable_names.add(var.name)",reserved_variable_names = {var.name for var in list(model.description.input) + list(model.description.output)},['reserved_variable_names = {var.name for var in list(model.description.input) + list(model.description.output)}'],1,
alot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alot/alot/commands/envelope.py,https://github.com/pazz/alot/tree/master/alot/commands/envelope.py,SendCommand,_get_keys_addresses$176,"def _get_keys_addresses(self):
    addresses = set()
    for key in self.envelope.encrypt_keys.values():
        for uid in key.uids:
            addresses.add(uid.email)
    return addresses","for key in self.envelope.encrypt_keys.values():
    for uid in key.uids:
        addresses.add(uid.email)",addresses = {uid.email for key in self.envelope.encrypt_keys.values() for uid in key.uids},['addresses = {uid.email for key in self.envelope.encrypt_keys.values() for uid in key.uids}'],1,
mongoaudit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mongoaudit/mongoaudit/testers/testers.py,https://github.com/stampery/mongoaudit/tree/master/mongoaudit/testers/testers.py,,try_dedicated_user$222,"def try_dedicated_user(test):
    """"""
    Verify that the role only applies to one database
    """"""
    roles = test.tester.get_roles()
    user_role_dbs = set()
    for role in roles['roles']:
        user_role_dbs.add(role['db'])
    return TestResult(success=bool(len(user_role_dbs)), message=decode_to_string(user_role_dbs))","for role in roles['roles']:
    user_role_dbs.add(role['db'])",user_role_dbs = {role['db'] for role in roles['roles']},["user_role_dbs = {role['db'] for role in roles['roles']}"],1,
neural-backed-decision-trees,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neural-backed-decision-trees/nbdt/hierarchy.py,https://github.com/alvinwan/neural-backed-decision-trees/tree/master/nbdt/hierarchy.py,,match_wnid_leaves$146,"def match_wnid_leaves(wnids, G, tree_name):
    wnid_set = set()
    for wnid in wnids:
        wnid_set.add(wnid.strip())
    leaves_seen = get_seen_wnids(wnid_set, get_leaves(G))
    return (leaves_seen, wnid_set)","for wnid in wnids:
    wnid_set.add(wnid.strip())",entries = {wnid.strip() for wnid in wnids},['wnid_set = {wnid.strip() for wnid in wnids}'],0,
horovod,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/horovod/horovod/runner/util/network.py,https://github.com/horovod/horovod/tree/master/horovod/runner/util/network.py,,get_local_host_addresses$28,"def get_local_host_addresses():
    local_addresses = set()
    for intf_info_list in psutil.net_if_addrs().values():
        for intf_info in intf_info_list:
            if intf_info.family == socket.AF_INET:
                local_addresses.add(intf_info.address)
    return local_addresses","for intf_info_list in psutil.net_if_addrs().values():
    for intf_info in intf_info_list:
        if intf_info.family == socket.AF_INET:
            local_addresses.add(intf_info.address)",local_addresses = {intf_info.address for intf_info_list in psutil.net_if_addrs().values() for intf_info in intf_info_list if intf_info.family == socket.AF_INET},['local_addresses = {intf_info.address for intf_info_list in psutil.net_if_addrs().values() for intf_info in intf_info_list if intf_info.family == socket.AF_INET}'],1,
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/resources/hsm.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/hsm.py,HSMClusterSubnet,get_related_ids$33,"def get_related_ids(self, clusters):
    subnet_ids = set()
    for cluster in clusters:
        for subnet in cluster.get('SubnetMapping').values():
            subnet_ids.add(subnet)
    return list(subnet_ids)","for cluster in clusters:
    for subnet in cluster.get('SubnetMapping').values():
        subnet_ids.add(subnet)",subnet_ids = {subnet for cluster in clusters for subnet in cluster.get('SubnetMapping').values()},["subnet_ids = {subnet for cluster in clusters for subnet in cluster.get('SubnetMapping').values()}"],1,
rope,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rope/rope/refactor/importutils/module_imports.py,https://github.com/python-rope/rope/tree/master/rope/refactor/importutils/module_imports.py,_GlobalUnboundNameFinder,__init__$428,"def __init__(self, pymodule, wanted_pyobject):
    super(_GlobalUnboundNameFinder, self).__init__(pymodule)
    self.unbound = set()
    self.names = set()
    for (name, pyname) in pymodule._get_structural_attributes().items():
        if not isinstance(pyname, (pynames.ImportedName, pynames.ImportedModule)):
            self.names.add(name)
    wanted_scope = wanted_pyobject.get_scope()
    self.start = wanted_scope.get_start()
    self.end = wanted_scope.get_end() + 1","for (name, pyname) in pymodule._get_structural_attributes().items():
    if not isinstance(pyname, (pynames.ImportedName, pynames.ImportedModule)):
        self.names.add(name)","entries = {name for (name, pyname) in pymodule._get_structural_attributes().items() if not isinstance(pyname, (pynames.ImportedName, pynames.ImportedModule))}","['self.names = {name for (name, pyname) in pymodule._get_structural_attributes().items() if not isinstance(pyname, (pynames.ImportedName, pynames.ImportedModule))}']",0,
skywater-pdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/skywater-pdk/scripts/python-skywater-pdk/skywater_pdk/liberty.py,https://github.com/google/skywater-pdk/tree/master/scripts/python-skywater-pdk/skywater_pdk/liberty.py,,remove_ccsnoise_from_dict$289,"def remove_ccsnoise_from_dict(data, dataname):
    if 'timing' in data:
        remove_ccsnoise_from_timing(data, dataname)
    ccsn_keys = set()
    for k in data:
        if 'ccsn_' in k:
            ccsn_keys.add(k)
    for k in ccsn_keys:
        if debug:
            print('{:s}: Removing {}'.format(dataname, k))
        del data[k]","for k in data:
    if 'ccsn_' in k:
        ccsn_keys.add(k)",ccsn_keys = {k for k in data if 'ccsn_' in k},["ccsn_keys = {k for k in data if 'ccsn_' in k}"],1,
botflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/botflow/botflow/botbase.py,https://github.com/kkyon/botflow/tree/master/botflow/botbase.py,BotManager,get_all_q$143,"def get_all_q(self):
    qs = set()
    for b in self._bots:
        for q in b.iq + b.oq:
            if not isinstance(q, SinkQueue):
                qs.add(q)
    return qs","for b in self._bots:
    for q in b.iq + b.oq:
        if not isinstance(q, SinkQueue):
            qs.add(q)","qs = {q for b in self._bots for q in b.iq + b.oq if not isinstance(q, SinkQueue)}","['qs = {q for b in self._bots for q in b.iq + b.oq if not isinstance(q, SinkQueue)}']",1,
news-please,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/news-please/newsplease/crawler/spiders/gdelt_crawler.py,https://github.com/fhamborg/news-please/tree/master/newsplease/crawler/spiders/gdelt_crawler.py,GdeltCrawler,rss_parse$52,"def rss_parse(self, response):
    """"""
        Extracts all article links and initiates crawling them.

        :param obj response: The scrapy response
        """"""
    match = re.match(re_export, response.text)
    if match:
        last_update_zip_url = match.group(1)
        r = requests.get(last_update_zip_url)
        z = zipfile.ZipFile(io.BytesIO(r.content))
        extracted = z.namelist()
        z.extractall('/tmp')
        csv_file_path = '/tmp/%s' % extracted[0]
        urls = set()
        with open(csv_file_path) as csv_file:
            csv_reader = csv.reader(csv_file, delimiter='\t')
            for row in csv_reader:
                urls.add(row[-1])
        os.remove(csv_file_path)
        for url in urls:
            yield scrapy.Request(url, lambda resp: self.article_parse(resp, 'gdelt'))","for row in csv_reader:
    urls.add(row[-1])",urls = {row[-1] for row in csv_reader},['urls = {row[-1] for row in csv_reader}'],1,
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/building/farm.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/building/farm.py,FarmOptionCache,_get_raw_options$44,"def _get_raw_options(self, farm_spots_set, field_spots_set, road_spots_set):
    field_row3 = {}
    field_col3 = {}
    for coords in farm_spots_set:
        (x, y) = coords
        row_score = 1
        if (x - 3, y) in field_spots_set:
            row_score += 1
        if (x + 3, y) in field_spots_set:
            row_score += 1
        field_row3[coords] = row_score
        col_score = 1
        if (x, y - 3) in field_spots_set:
            col_score += 1
        if (x, y + 3) in field_spots_set:
            col_score += 1
        field_col3[coords] = col_score
    road_row3 = set()
    road_col3 = set()
    for (x, y) in road_spots_set:
        if (x + 2, y) in road_spots_set and (x + 1, y) in road_spots_set:
            road_row3.add((x, y))
        if (x, y + 2) in road_spots_set and (x, y + 1) in road_spots_set:
            road_col3.add((x, y))
    road_row9 = set()
    for (x, y) in road_row3:
        if (x - 3, y) in road_row3 and (x + 3, y) in road_row3:
            road_row9.add((x, y))
    road_col9 = set()
    for (x, y) in road_col3:
        if (x, y - 3) in road_col3 and (x, y + 3) in road_col3:
            road_col9.add((x, y))
    raw_options = []
    for coords in sorted(farm_spots_set):
        (x, y) = coords
        row_score = field_row3[coords] - 1
        if (x, y - 1) in road_row9:
            score = row_score
            if (x, y - 4) in field_row3:
                score += field_row3[x, y - 4]
            if (x, y + 3) in field_row3:
                score += field_row3[x, y + 3]
            if score > 0:
                raw_options.append((score, coords, 0))
        if (x, y + 3) in road_row9:
            score = row_score
            if (x, y - 3) in field_row3:
                score += field_row3[x, y - 3]
            if (x, y + 4) in field_row3:
                score += field_row3[x, y + 4]
            if score > 0:
                raw_options.append((score, coords, 1))
        col_score = field_col3[coords] - 1
        if (x - 1, y) in road_col9:
            score = col_score
            if (x - 4, y) in field_col3:
                score += field_col3[x - 4, y]
            if (x + 3, y) in field_col3:
                score += field_col3[x + 3, y]
            if score > 0:
                raw_options.append((score, coords, 2))
        if (x + 3, y) in road_col9:
            score = col_score
            if (x - 3, y) in field_col3:
                score += field_col3[x - 3, y]
            if (x + 4, y) in field_col3:
                score += field_col3[x + 4, y]
            if score > 0:
                raw_options.append((score, coords, 3))
    return raw_options","for (x, y) in road_spots_set:
    if (x + 2, y) in road_spots_set and (x + 1, y) in road_spots_set:
        road_row3.add((x, y))
    if (x, y + 2) in road_spots_set and (x, y + 1) in road_spots_set:
        road_col3.add((x, y))","road_col3 = {(x, y) for (x, y) in road_spots_set if (x, y + 2) in road_spots_set and (x, y + 1) in road_spots_set}",Cannot refactor,2,
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/building/farm.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/building/farm.py,FarmOptionCache,_get_raw_options$44,"def _get_raw_options(self, farm_spots_set, field_spots_set, road_spots_set):
    field_row3 = {}
    field_col3 = {}
    for coords in farm_spots_set:
        (x, y) = coords
        row_score = 1
        if (x - 3, y) in field_spots_set:
            row_score += 1
        if (x + 3, y) in field_spots_set:
            row_score += 1
        field_row3[coords] = row_score
        col_score = 1
        if (x, y - 3) in field_spots_set:
            col_score += 1
        if (x, y + 3) in field_spots_set:
            col_score += 1
        field_col3[coords] = col_score
    road_row3 = set()
    road_col3 = set()
    for (x, y) in road_spots_set:
        if (x + 2, y) in road_spots_set and (x + 1, y) in road_spots_set:
            road_row3.add((x, y))
        if (x, y + 2) in road_spots_set and (x, y + 1) in road_spots_set:
            road_col3.add((x, y))
    road_row9 = set()
    for (x, y) in road_row3:
        if (x - 3, y) in road_row3 and (x + 3, y) in road_row3:
            road_row9.add((x, y))
    road_col9 = set()
    for (x, y) in road_col3:
        if (x, y - 3) in road_col3 and (x, y + 3) in road_col3:
            road_col9.add((x, y))
    raw_options = []
    for coords in sorted(farm_spots_set):
        (x, y) = coords
        row_score = field_row3[coords] - 1
        if (x, y - 1) in road_row9:
            score = row_score
            if (x, y - 4) in field_row3:
                score += field_row3[x, y - 4]
            if (x, y + 3) in field_row3:
                score += field_row3[x, y + 3]
            if score > 0:
                raw_options.append((score, coords, 0))
        if (x, y + 3) in road_row9:
            score = row_score
            if (x, y - 3) in field_row3:
                score += field_row3[x, y - 3]
            if (x, y + 4) in field_row3:
                score += field_row3[x, y + 4]
            if score > 0:
                raw_options.append((score, coords, 1))
        col_score = field_col3[coords] - 1
        if (x - 1, y) in road_col9:
            score = col_score
            if (x - 4, y) in field_col3:
                score += field_col3[x - 4, y]
            if (x + 3, y) in field_col3:
                score += field_col3[x + 3, y]
            if score > 0:
                raw_options.append((score, coords, 2))
        if (x + 3, y) in road_col9:
            score = col_score
            if (x - 3, y) in field_col3:
                score += field_col3[x - 3, y]
            if (x + 4, y) in field_col3:
                score += field_col3[x + 4, y]
            if score > 0:
                raw_options.append((score, coords, 3))
    return raw_options","for (x, y) in road_row3:
    if (x - 3, y) in road_row3 and (x + 3, y) in road_row3:
        road_row9.add((x, y))","road_row9 = {(x, y) for (x, y) in road_row3 if (x - 3, y) in road_row3 and (x + 3, y) in road_row3}","['road_row9 = {(x, y) for (x, y) in road_row3 if (x - 3, y) in road_row3 and (x + 3, y) in road_row3}']",1,
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/building/farm.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/building/farm.py,FarmOptionCache,_get_raw_options$44,"def _get_raw_options(self, farm_spots_set, field_spots_set, road_spots_set):
    field_row3 = {}
    field_col3 = {}
    for coords in farm_spots_set:
        (x, y) = coords
        row_score = 1
        if (x - 3, y) in field_spots_set:
            row_score += 1
        if (x + 3, y) in field_spots_set:
            row_score += 1
        field_row3[coords] = row_score
        col_score = 1
        if (x, y - 3) in field_spots_set:
            col_score += 1
        if (x, y + 3) in field_spots_set:
            col_score += 1
        field_col3[coords] = col_score
    road_row3 = set()
    road_col3 = set()
    for (x, y) in road_spots_set:
        if (x + 2, y) in road_spots_set and (x + 1, y) in road_spots_set:
            road_row3.add((x, y))
        if (x, y + 2) in road_spots_set and (x, y + 1) in road_spots_set:
            road_col3.add((x, y))
    road_row9 = set()
    for (x, y) in road_row3:
        if (x - 3, y) in road_row3 and (x + 3, y) in road_row3:
            road_row9.add((x, y))
    road_col9 = set()
    for (x, y) in road_col3:
        if (x, y - 3) in road_col3 and (x, y + 3) in road_col3:
            road_col9.add((x, y))
    raw_options = []
    for coords in sorted(farm_spots_set):
        (x, y) = coords
        row_score = field_row3[coords] - 1
        if (x, y - 1) in road_row9:
            score = row_score
            if (x, y - 4) in field_row3:
                score += field_row3[x, y - 4]
            if (x, y + 3) in field_row3:
                score += field_row3[x, y + 3]
            if score > 0:
                raw_options.append((score, coords, 0))
        if (x, y + 3) in road_row9:
            score = row_score
            if (x, y - 3) in field_row3:
                score += field_row3[x, y - 3]
            if (x, y + 4) in field_row3:
                score += field_row3[x, y + 4]
            if score > 0:
                raw_options.append((score, coords, 1))
        col_score = field_col3[coords] - 1
        if (x - 1, y) in road_col9:
            score = col_score
            if (x - 4, y) in field_col3:
                score += field_col3[x - 4, y]
            if (x + 3, y) in field_col3:
                score += field_col3[x + 3, y]
            if score > 0:
                raw_options.append((score, coords, 2))
        if (x + 3, y) in road_col9:
            score = col_score
            if (x - 3, y) in field_col3:
                score += field_col3[x - 3, y]
            if (x + 4, y) in field_col3:
                score += field_col3[x + 4, y]
            if score > 0:
                raw_options.append((score, coords, 3))
    return raw_options","for (x, y) in road_col3:
    if (x, y - 3) in road_col3 and (x, y + 3) in road_col3:
        road_col9.add((x, y))","road_col9 = {(x, y) for (x, y) in road_col3 if (x, y - 3) in road_col3 and (x, y + 3) in road_col3}","['road_col9 = {(x, y) for (x, y) in road_col3 if (x, y - 3) in road_col3 and (x, y + 3) in road_col3}']",1,
DALEX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DALEX/python/dalex/dalex/aspect/_predict_triplot/utils.py,https://github.com/ModelOriented/DALEX/tree/master/python/dalex/dalex/aspect/_predict_triplot/utils.py,,text_abbreviate$130,"def text_abbreviate(text, max_length, skip_chars='[!@#$=., _^*]', split_char='='):
    if max_length < 1:
        return text
    max_length = int(max_length)
    txt = text.rsplit(split_char, 1)
    var_name = txt[0]
    if len(var_name) <= max_length:
        return text
    var_name = re.sub(skip_chars, '', var_name)
    if len(var_name) <= max_length:
        return var_name + '=' + txt[1]
    abbreviate_index = set()
    for (i, char) in enumerate(var_name):
        if char == char.upper():
            abbreviate_index.add(i)
    if len(abbreviate_index) == 0:
        abbreviate_index.add(0)
    uppers_set = deepcopy(abbreviate_index)
    curr_len = len(abbreviate_index)
    if curr_len < max_length:
        i = 1
        while curr_len < max_length:
            for ind in uppers_set:
                if curr_len < max_length:
                    if ind + i not in abbreviate_index:
                        abbreviate_index.add(ind + i)
                        curr_len += 1
            i += 1
    abbreviate = ''
    for ind in sorted(abbreviate_index):
        abbreviate += var_name[ind]
    return abbreviate[:max_length] + ' =' + txt[1]","for (i, char) in enumerate(var_name):
    if char == char.upper():
        abbreviate_index.add(i)","abbreviate_index = {i for (i, char) in enumerate(var_name) if char == char.upper()}","['abbreviate_index = {i for (i, char) in enumerate(var_name) if char == char.upper()}']",1,
textflint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/textflint/textflint/generation/transformation/UT/swap_syn_wordnet.py,https://github.com/textflint/textflint/tree/master/textflint/generation/transformation/UT/swap_syn_wordnet.py,SwapSynWordNet,_get_candidates$38,"def _get_candidates(self, word, pos=None, n=5):
    """"""
        Returns a list containing all possible words with 1 character replaced
        by a homoglyph.

        """"""
    synonym = set()
    synsets = self.processor.get_synsets([(word, pos)])[0]
    for syn in synsets:
        for syn_word in syn.lemma_names(lang=self.language):
            if syn_word != word and '_' not in syn_word:
                synonym.add(syn_word)
    if not synonym:
        return []
    return list(synonym)[:n]","for syn in synsets:
    for syn_word in syn.lemma_names(lang=self.language):
        if syn_word != word and '_' not in syn_word:
            synonym.add(syn_word)",synonym = {syn_word for syn in synsets for syn_word in syn.lemma_names(lang=self.language) if syn_word != word and '_' not in syn_word},["synonym = {syn_word for syn in synsets for syn_word in syn.lemma_names(lang=self.language) if syn_word != word and '_' not in syn_word}"],1,
ludwig,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ludwig/ludwig/visualize.py,https://github.com/ludwig-ai/ludwig/tree/master/ludwig/visualize.py,,_validate_output_feature_name_from_test_stats$137,"def _validate_output_feature_name_from_test_stats(output_feature_name, test_stats_per_model):
    """"""Validate prediction output_feature_name from model test stats and return it as list.

    :param output_feature_name: output_feature_name containing ground truth
    :param test_stats_per_model: list of per model test stats
    :return output_feature_names: list of output_feature_name(s) containing ground truth
    """"""
    output_feature_names_set = set()
    for ls in test_stats_per_model:
        for key in ls:
            output_feature_names_set.add(key)
    try:
        if output_feature_name in output_feature_names_set:
            return [output_feature_name]
        else:
            return output_feature_names_set
    except TypeError:
        return output_feature_names_set","for ls in test_stats_per_model:
    for key in ls:
        output_feature_names_set.add(key)",output_feature_names_set = {key for ls in test_stats_per_model for key in ls},['output_feature_names_set = {key for ls in test_stats_per_model for key in ls}'],1,
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/identifier/functions/printf.py,https://github.com/angr/angr/tree/master/angr/analyses/identifier/functions/printf.py,printf,pre_test$44,"def pre_test(self, func, runner):
    length = 10
    test_str = self.rand_str(length, string.ascii_letters + string.digits)
    test_input = [test_str]
    test_output = [test_str]
    max_steps = len(test_str) * 3 + 20
    stdout = test_str
    test = TestData(test_input, test_output, None, max_steps, expected_stdout=stdout)
    if not runner.test(func, test):
        return False
    test_input = [claripy.BVS('input', 10 * 8)]
    test_output = [None]
    test = TestData(test_input, test_output, None, max_steps)
    s = runner.get_base_call_state(func, test)
    pg = runner.project.factory.simulation_manager(s)
    pg.run(n=18)
    interesting_chars = set()
    for p in pg.active:
        for g in p.history.jump_guards:
            if g.op == '__ne__' or g.op == '__eq__':
                for a in g.args:
                    if not a.symbolic:
                        interesting_chars.add(s.solver.eval(a))
    interesting_chars = set((chr(a) for a in interesting_chars if 0 < a < 128))
    alphanum = set(string.ascii_letters + string.digits)
    possible_format_specifiers = [c for c in interesting_chars if c not in alphanum and c in string.printable and (c not in string.whitespace)]
    possible_formats = [c for c in interesting_chars if c in alphanum]
    if len(possible_format_specifiers) > 10:
        return False
    second_str = 'findme'
    for char in possible_format_specifiers:
        if self.format_spec_char is not None:
            break
        for cc in possible_formats:
            test_str = char + cc + '\n\x00'
            test_input = [test_str, second_str]
            test_output = [test_str, second_str]
            stdout = second_str + '\n'
            max_steps = 20
            test = TestData(test_input, test_output, None, max_steps, expected_stdout=stdout)
            if runner.test(func, test):
                self.format_spec_char = char
                self.string_spec_char = cc
                break
    if self.format_spec_char is None:
        second_str = 'findme'
        for char in possible_format_specifiers:
            if self.format_spec_char is not None:
                break
            for cc in string.ascii_lowercase:
                if cc in possible_formats:
                    continue
                test_str = char + cc + '\n\x00'
                test_input = [test_str, second_str]
                test_output = [test_str, second_str]
                stdout = second_str + '\n'
                max_steps = 10
                test = TestData(test_input, test_output, None, max_steps, expected_stdout=stdout)
                if runner.test(func, test):
                    self.format_spec_char = char
                    self.string_spec_char = cc
                    break
    if self.format_spec_char is None:
        l.warning('format spec is none :(')
        return False
    return True","for p in pg.active:
    for g in p.history.jump_guards:
        if g.op == '__ne__' or g.op == '__eq__':
            for a in g.args:
                if not a.symbolic:
                    interesting_chars.add(s.solver.eval(a))","interesting_chars = {s.solver.eval(a) for p in pg.active for g in p.history.jump_guards if g.op in ('__ne__', '__eq__') for a in g.args if not a.symbolic}",["interesting_chars = {s.solver.eval(a) for p in pg.active for g in p.history.jump_guards if g.op == '__ne__' or g.op == '__eq__' for a in g.args if not a.symbolic}"],0,
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/lib/monitoring.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/monitoring.py,MonitorHandler,token_count$48,"def token_count(self, realm_list, status=None):
    """"""
        Give the number of tokens (with status) per realm
        if multiple tokens are given, give summary for all tokens
        tokens which are in multiple realms are only counted once!

        :param realm_list: list of realms which must be queried
        :param status: string which contains requested token status
        :return: dict with the keys: active, inactive,
            assigned, unassigned, total
        """"""
    if not isinstance(realm_list, (list, tuple)):
        realms = [realm_list]
    else:
        realms = realm_list[:]
    if len(realms) < 1:
        realms = ['/:no realm:/']
    result = {}
    cond = tuple()
    for realm in realms:
        realm = realm.strip().lower()
        if '/:no realm:/' in realm or realm == '':
            token_id_tuples = db.session.query(TokenRealm.token_id).all()
            token_ids = set()
            for token_tuple in token_id_tuples:
                token_ids.add(token_tuple[0])
            cond += (and_(not_(Token.LinOtpTokenId.in_(token_ids))),)
            if '/:no realm:/' in realm:
                realms.remove('/:no realm:/')
        else:
            cond += (and_(TokenRealm.realm_id == Realm.id, Realm.name == realm, TokenRealm.token_id == Token.LinOtpTokenId),)
    r_condition = or_(*cond)
    if 'total' in status:
        result['total'] = db.session.query(Token.LinOtpTokenId).filter(r_condition).distinct().count()
    if 'total users' in status:
        result['total users'] = db.session.query(Token.LinOtpUserid, Token.LinOtpIdResClass).filter(r_condition).filter(Token.LinOtpUserid != '').filter(Token.LinOtpIsactive).distinct().count()
    for stat in status:
        if stat in ['total users', 'total']:
            continue
        conditions = (and_(r_condition),)
        if '&' in stat:
            stati = stat.split('&')
            if 'assigned' in stati:
                conditions += (and_(Token.LinOtpUserid != ''),)
            else:
                conditions += (and_(Token.LinOtpUserid == ''),)
            if 'active' in stati:
                conditions += (and_(Token.LinOtpIsactive),)
            else:
                conditions += (and_(Token.LinOtpIsactive == False),)
        elif 'assigned' == stat:
            conditions += (and_(Token.LinOtpUserid != ''),)
        elif 'unassigned' == stat:
            conditions += (and_(Token.LinOtpUserid == ''),)
        elif 'active' == stat:
            conditions += (and_(Token.LinOtpIsactive),)
        elif 'inactive' == stat:
            conditions += (and_(Token.LinOtpIsactive == False),)
        condition = and_(*conditions)
        result[stat] = db.session.query(Token.LinOtpTokenId).filter(condition).distinct().count()
    return result","for token_tuple in token_id_tuples:
    token_ids.add(token_tuple[0])",token_ids = {token_tuple[0] for token_tuple in token_id_tuples},['token_ids = {token_tuple[0] for token_tuple in token_id_tuples}'],1,
SerpentAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SerpentAI/serpent/machine_learning/reinforcement_learning/agents/rainbow_dqn_agent.py,https://github.com/SerpentAI/SerpentAI/tree/master/serpent/machine_learning/reinforcement_learning/agents/rainbow_dqn_agent.py,RainbowDQNAgent,add_human_observations_to_replay_memory$271,"def add_human_observations_to_replay_memory(self):
    keyboard_key_value_label_mapping = self._generate_keyboard_key_value_mapping()
    input_label_action_space_mapping = dict()
    for label_action_space_value in list(enumerate(self.game_inputs[0]['inputs'])):
        input_label_action_space_mapping[label_action_space_value[1]] = label_action_space_value[0]
    with h5py.File(f'datasets/{self.name}_input_recording.h5', 'r') as f:
        timestamps = set()
        for key in f.keys():
            timestamps.add(float(key.split('-')[0]))
        for timestamp in sorted(list(timestamps)):
            png_frames = f[f'{timestamp}-frames'].value
            numpy_frames = [skimage.util.img_as_float(skimage.io.imread(io.BytesIO(b))) for b in png_frames]
            pytorch_frames = [torch.tensor(torch.from_numpy(frame), dtype=torch.float32) for frame in numpy_frames]
            frames = torch.stack(pytorch_frames, 0)
            action_key = tuple(sorted([b.decode('utf-8') for b in f[f'{timestamp}-keyboard-inputs-active']]))
            if action_key not in keyboard_key_value_label_mapping:
                continue
            label = keyboard_key_value_label_mapping[action_key]
            action = input_label_action_space_mapping[label]
            reward = f[f'{timestamp}-reward'].value
            terminal = f[f'{timestamp}-terminal'].value
            self.replay_memory.append(frames, action, reward, terminal)
            self.remaining_observe_steps -= 1
            if self.remaining_observe_steps == 0:
                self.set_mode(RainbowDQNAgentModes.TRAIN)
                break","for key in f.keys():
    timestamps.add(float(key.split('-')[0]))",timestamps = {float(key.split('-')[0]) for key in f.keys()},["timestamps = {float(key.split('-')[0]) for key in f.keys()}"],1,
tuna,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tuna/tuna/_runtime_profile.py,https://github.com/nschloe/tuna/tree/master/tuna/_runtime_profile.py,,read_runtime_profile$4,"def read_runtime_profile(prof_filename):
    stats = pstats.Stats(prof_filename)
    roots = set()
    for (key, value) in stats.stats.items():
        if not value[4] and value[3] > 1e-05:
            roots.add(key)
    default_roots = [('~', 0, '<built-in method builtins.exec>'), ('~', 0, '<built-in method exec>')]
    for default_root in default_roots:
        if default_root in stats.stats:
            roots.add(default_root)
    roots = list(roots)
    children = {key: [] for key in stats.stats.keys()}
    for (key, value) in stats.stats.items():
        (_, _, _, _, parents) = value
        for parent in parents:
            children[parent].append(key)

    def populate(key, parent, all_ancestors):
        if parent is None:
            parent_times = {}
            (_, _, selftime, cumtime, _) = stats.stats[key]
        else:
            (_, _, _, _, parent_times) = stats.stats[key]
            (_, _, selftime, cumtime) = parent_times[parent]
        name = '{}::{}::{}'.format(*key)
        if key in all_ancestors:
            return {}
        if len(parent_times) <= 1:
            c = [populate(child, key, all_ancestors + [key]) for child in children[key]]
            c.append({'text': [name + '::self', f'{selftime:.3} s'], 'color': 0, 'value': selftime})
            return {'text': [name], 'color': 0, 'children': c}
        if children[key]:
            c = [{'text': ['Possible calls of', ', '.join(('{}::{}::{}'.format(*child) for child in children[key]))], 'color': 3, 'value': cumtime}]
            return {'text': [name], 'color': 0, 'children': c}
        return {'text': [name, f'{selftime:.3f}'], 'color': 0, 'value': selftime}
    if len(roots) == 1:
        data = populate(roots[0], None, [])
    else:
        assert len(roots) > 1
        data = {'text': ['root'], 'color': 0, 'children': [populate(root, None, []) for root in roots]}
    return data","for default_root in default_roots:
    if default_root in stats.stats:
        roots.add(default_root)",roots = {default_root for default_root in default_roots if default_root in stats.stats},Cannot refactor,2,
metaworld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/metaworld/tests/integration/test_new_api.py,https://github.com/rlworkgroup/metaworld/tree/master/tests/integration/test_new_api.py,,test_all_ml45$111,"def test_all_ml45():
    ml45 = ML45()
    train_env_instances = {env_name: env_cls() for (env_name, env_cls) in ml45.train_classes.items()}
    train_env_rand_vecs = check_tasks_unique(ml45.train_tasks, ml45._train_classes.keys())
    for task in ml45.train_tasks:
        env = train_env_instances[task.env_name]
        env.set_task(task)
        obs = env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
    for env in train_env_instances.values():
        env.close()
    del train_env_instances
    test_env_instances = {env_name: env_cls() for (env_name, env_cls) in ml45.test_classes.items()}
    test_env_rand_vecs = check_tasks_unique(ml45.test_tasks, ml45._test_classes.keys())
    for task in ml45.test_tasks:
        env = test_env_instances[task.env_name]
        env.set_task(task)
        obs = env.reset()
        assert np.all(obs[-3:] == np.array([0, 0, 0]))
        assert env.observation_space.shape == (39,)
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
    for env in test_env_instances.values():
        env.close()
    train_test_rand_vecs = set()
    for rand_vecs in train_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    for rand_vecs in test_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    assert len(train_test_rand_vecs) == (len(ml45.test_classes.keys()) + len(ml45.train_classes.keys())) * metaworld._N_GOALS
    del test_env_instances","for rand_vecs in train_env_rand_vecs.values():
    for rand_vec in rand_vecs:
        train_test_rand_vecs.add(tuple(rand_vec))",train_test_rand_vecs = {tuple(rand_vec) for rand_vecs in train_env_rand_vecs.values() for rand_vec in rand_vecs},['train_test_rand_vecs = {tuple(rand_vec) for rand_vecs in train_env_rand_vecs.values() for rand_vec in rand_vecs}'],1,
metaworld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/metaworld/tests/integration/test_new_api.py,https://github.com/rlworkgroup/metaworld/tree/master/tests/integration/test_new_api.py,,test_all_ml45$111,"def test_all_ml45():
    ml45 = ML45()
    train_env_instances = {env_name: env_cls() for (env_name, env_cls) in ml45.train_classes.items()}
    train_env_rand_vecs = check_tasks_unique(ml45.train_tasks, ml45._train_classes.keys())
    for task in ml45.train_tasks:
        env = train_env_instances[task.env_name]
        env.set_task(task)
        obs = env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
    for env in train_env_instances.values():
        env.close()
    del train_env_instances
    test_env_instances = {env_name: env_cls() for (env_name, env_cls) in ml45.test_classes.items()}
    test_env_rand_vecs = check_tasks_unique(ml45.test_tasks, ml45._test_classes.keys())
    for task in ml45.test_tasks:
        env = test_env_instances[task.env_name]
        env.set_task(task)
        obs = env.reset()
        assert np.all(obs[-3:] == np.array([0, 0, 0]))
        assert env.observation_space.shape == (39,)
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
    for env in test_env_instances.values():
        env.close()
    train_test_rand_vecs = set()
    for rand_vecs in train_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    for rand_vecs in test_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    assert len(train_test_rand_vecs) == (len(ml45.test_classes.keys()) + len(ml45.train_classes.keys())) * metaworld._N_GOALS
    del test_env_instances","for rand_vecs in test_env_rand_vecs.values():
    for rand_vec in rand_vecs:
        train_test_rand_vecs.add(tuple(rand_vec))",train_test_rand_vecs = {tuple(rand_vec) for rand_vecs in test_env_rand_vecs.values() for rand_vec in rand_vecs},Cannot refactor,2,
sentence-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentence-transformers/sentence_transformers/evaluation/ParaphraseMiningEvaluator.py,https://github.com/UKPLab/sentence-transformers/tree/master/sentence_transformers/evaluation/ParaphraseMiningEvaluator.py,ParaphraseMiningEvaluator,__init__$21,"def __init__(self, sentences_map: Dict[str, str], duplicates_list: List[Tuple[str, str]]=None, duplicates_dict: Dict[str, Dict[str, bool]]=None, add_transitive_closure: bool=False, query_chunk_size: int=5000, corpus_chunk_size: int=100000, max_pairs: int=500000, top_k: int=100, show_progress_bar: bool=False, batch_size: int=16, name: str='', write_csv: bool=True):
    """"""

        :param sentences_map: A dictionary that maps sentence-ids to sentences, i.e. sentences_map[id] => sentence.
        :param duplicates_list: Duplicates_list is a list with id pairs [(id1, id2), (id1, id5)] that identifies the duplicates / paraphrases in the sentences_map
        :param duplicates_dict: A default dictionary mapping [id1][id2] to true if id1 and id2 are duplicates. Must be symmetric, i.e., if [id1][id2] => True, then [id2][id1] => True.
        :param add_transitive_closure: If true, it adds a transitive closure, i.e. if dup[a][b] and dup[b][c], then dup[a][c]
        :param query_chunk_size: To identify the paraphrases, the cosine-similarity between all sentence-pairs will be computed. As this might require a lot of memory, we perform a batched computation.  #query_batch_size sentences will be compared against up to #corpus_batch_size sentences. In the default setting, 5000 sentences will be grouped together and compared up-to against 100k other sentences.
        :param corpus_chunk_size: The corpus will be batched, to reduce the memory requirement
        :param max_pairs: We will only extract up to #max_pairs potential paraphrase candidates.
        :param top_k: For each query, we extract the top_k most similar pairs and add it to a sorted list. I.e., for one sentence we cannot find more than top_k paraphrases
        :param show_progress_bar: Output a progress bar
        :param batch_size: Batch size for computing sentence embeddings
        :param name: Name of the experiment
        :param write_csv: Write results to CSV file
        """"""
    self.sentences = []
    self.ids = []
    for (id, sentence) in sentences_map.items():
        self.sentences.append(sentence)
        self.ids.append(id)
    self.name = name
    self.show_progress_bar = show_progress_bar
    self.batch_size = batch_size
    self.query_chunk_size = query_chunk_size
    self.corpus_chunk_size = corpus_chunk_size
    self.max_pairs = max_pairs
    self.top_k = top_k
    self.duplicates = duplicates_dict if duplicates_dict is not None else defaultdict(lambda : defaultdict(bool))
    if duplicates_list is not None:
        for (id1, id2) in duplicates_list:
            if id1 in sentences_map and id2 in sentences_map:
                self.duplicates[id1][id2] = True
                self.duplicates[id2][id1] = True
    if add_transitive_closure:
        self.duplicates = self.add_transitive_closure(self.duplicates)
    positive_key_pairs = set()
    for key1 in self.duplicates:
        for key2 in self.duplicates[key1]:
            if key1 in sentences_map and key2 in sentences_map and (self.duplicates[key1][key2] or self.duplicates[key2][key1]):
                positive_key_pairs.add(tuple(sorted([key1, key2])))
    self.total_num_duplicates = len(positive_key_pairs)
    if name:
        name = '_' + name
    self.csv_file: str = 'paraphrase_mining_evaluation' + name + '_results.csv'
    self.csv_headers = ['epoch', 'steps', 'precision', 'recall', 'f1', 'threshold', 'average_precision']
    self.write_csv = write_csv","for key1 in self.duplicates:
    for key2 in self.duplicates[key1]:
        if key1 in sentences_map and key2 in sentences_map and (self.duplicates[key1][key2] or self.duplicates[key2][key1]):
            positive_key_pairs.add(tuple(sorted([key1, key2])))","positive_key_pairs = {tuple(sorted([key1, key2])) for key1 in self.duplicates for key2 in self.duplicates[key1] if key1 in sentences_map and key2 in sentences_map and (self.duplicates[key1][key2] or self.duplicates[key2][key1])}","['positive_key_pairs = {tuple(sorted([key1, key2])) for key1 in self.duplicates for key2 in self.duplicates[key1] if key1 in sentences_map and key2 in sentences_map and (self.duplicates[key1][key2] or self.duplicates[key2][key1])}']",1,
hamster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hamster/waflib/Tools/errcheck.py,https://github.com/projecthamster/hamster/tree/master/waflib/Tools/errcheck.py,,check_invalid_constraints$79,"def check_invalid_constraints(self):
    feat = set()
    for x in list(TaskGen.feats.values()):
        feat.union(set(x))
    for (x, y) in TaskGen.task_gen.prec.items():
        feat.add(x)
        feat.union(set(y))
    ext = set()
    for x in TaskGen.task_gen.mappings.values():
        ext.add(x.__name__)
    invalid = ext & feat
    if invalid:
        Logs.error('The methods %r have invalid annotations:  @extension <-> @feature/@before_method/@after_method', list(invalid))
    for cls in list(Task.classes.values()):
        if sys.hexversion > 50331648 and issubclass(cls, Task.Task) and isinstance(cls.hcode, str):
            raise Errors.WafError('Class %r has hcode value %r of type <str>, expecting <bytes> (use Utils.h_cmd() ?)' % (cls, cls.hcode))
        for x in ('before', 'after'):
            for y in Utils.to_list(getattr(cls, x, [])):
                if not Task.classes.get(y):
                    Logs.error('Erroneous order constraint %r=%r on task class %r', x, y, cls.__name__)
        if getattr(cls, 'rule', None):
            Logs.error('Erroneous attribute ""rule"" on task class %r (rename to ""run_str"")', cls.__name__)","for x in TaskGen.task_gen.mappings.values():
    ext.add(x.__name__)",ext = {x.__name__ for x in TaskGen.task_gen.mappings.values()},['ext = {x.__name__ for x in TaskGen.task_gen.mappings.values()}'],1,
pyproj,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyproj/test/test_sync.py,https://github.com/pyproj4/pyproj/tree/master/test/test_sync.py,,test_get_transform_grid_list__source_id$61,"def test_get_transform_grid_list__source_id():
    grids = get_transform_grid_list(bbox=BBox(170, -90, -170, 90), source_id='us_noaa', include_already_downloaded=True)
    assert len(grids) > 5
    source_ids = set()
    for grid in grids:
        source_ids.add(grid['properties']['source_id'])
    assert sorted(source_ids) == ['us_noaa']","for grid in grids:
    source_ids.add(grid['properties']['source_id'])",source_ids = {grid['properties']['source_id'] for grid in grids},["source_ids = {grid['properties']['source_id'] for grid in grids}"],1,
mypy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mypy/mypyc/analysis/dataflow.py,https://github.com/python/mypy/tree/master/mypyc/analysis/dataflow.py,,non_trivial_sources$397,"def non_trivial_sources(op: Op) -> Set[Value]:
    result = set()
    for source in op.sources():
        if not isinstance(source, Integer):
            result.add(source)
    return result","for source in op.sources():
    if not isinstance(source, Integer):
        result.add(source)","result = {source for source in op.sources() if not isinstance(source, Integer)}","['result = {source for source in op.sources() if not isinstance(source, Integer)}']",1,
mutagen,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mutagen/tests/test_tools_mid3iconv.py,https://github.com/quodlibet/mutagen/tree/master/tests/test_tools_mid3iconv.py,TMid3Iconv,test_test_data$45,"def test_test_data(self):
    results = set()
    for codec in CODECS:
        results.add(AMBIGUOUS.decode(codec))
    self.failUnlessEqual(len(results), len(CODECS))","for codec in CODECS:
    results.add(AMBIGUOUS.decode(codec))",results = {AMBIGUOUS.decode(codec) for codec in CODECS},['results = {AMBIGUOUS.decode(codec) for codec in CODECS}'],1,
petastorm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/petastorm/petastorm/tests/test_predicates.py,https://github.com/uber/petastorm/tree/master/petastorm/tests/test_predicates.py,,test_negation$76,"def test_negation(all_values):
    for values in [{'guid_2', 'guid_1'}, {'guid_5', 'guid_9'}, {'guid_2'}]:
        test_predicate = in_negate(in_set(values, 'volume_guid'))
        included_values = set()
        for val in all_values:
            if test_predicate.do_include({'volume_guid': val}):
                included_values.add(val)
        assert included_values == all_values.difference(values)","for values in [{'guid_2', 'guid_1'}, {'guid_5', 'guid_9'}, {'guid_2'}]:
    test_predicate = in_negate(in_set(values, 'volume_guid'))
    included_values = set()
    for val in all_values:
        if test_predicate.do_include({'volume_guid': val}):
            included_values.add(val)
    assert included_values == all_values.difference(values)","{included_values := {val for val in all_values if in_negate(in_set(values, 'volume_guid')).do_include({'volume_guid': val})}}; assert included_values == all_values.difference(values)",Cannot refactor,2,
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/mu.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/mu.py,LambdaManager,diff_tags$444,"def diff_tags(old_tags, new_tags):
    add = {}
    remove = set()
    for (k, v) in new_tags.items():
        if k not in old_tags or old_tags[k] != v:
            add[k] = v
    for k in old_tags:
        if k not in new_tags:
            remove.add(k)
    return (add, list(remove))","for k in old_tags:
    if k not in new_tags:
        remove.add(k)",remove = {k for k in old_tags if k not in new_tags},['remove = {k for k in old_tags if k not in new_tags}'],1,
nltk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/classify/decisiontree.py,https://github.com/nltk/nltk/tree/master/nltk/classify/decisiontree.py,DecisionTreeClassifier,train$137,"def train(labeled_featuresets, entropy_cutoff=0.05, depth_cutoff=100, support_cutoff=10, binary=False, feature_values=None, verbose=False):
    """"""
        :param binary: If true, then treat all feature/value pairs as
            individual binary features, rather than using a single n-way
            branch for each feature.
        """"""
    feature_names = set()
    for (featureset, label) in labeled_featuresets:
        for fname in featureset:
            feature_names.add(fname)
    if feature_values is None and binary:
        feature_values = defaultdict(set)
        for (featureset, label) in labeled_featuresets:
            for (fname, fval) in featureset.items():
                feature_values[fname].add(fval)
    if not binary:
        tree = DecisionTreeClassifier.best_stump(feature_names, labeled_featuresets, verbose)
    else:
        tree = DecisionTreeClassifier.best_binary_stump(feature_names, labeled_featuresets, feature_values, verbose)
    tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff - 1, support_cutoff, binary, feature_values, verbose)
    return tree","for (featureset, label) in labeled_featuresets:
    for (fname, fval) in featureset.items():
        feature_values[fname].add(fval)","feature_values = {fname: {fval for (featureset, label) in labeled_featuresets for (fname, fval) in featureset.items() if fname == fname} for fname in set(fname for (featureset, label) in labeled_featuresets for (fname, fval) in featureset.items())}",Cannot refactor,2,
metaworld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/metaworld/tests/integration/test_new_api.py,https://github.com/rlworkgroup/metaworld/tree/master/tests/integration/test_new_api.py,,test_all_ml10$62,"def test_all_ml10():
    ml10 = ML10()
    train_env_instances = {env_name: env_cls() for (env_name, env_cls) in ml10.train_classes.items()}
    train_env_rand_vecs = check_tasks_unique(ml10.train_tasks, ml10._train_classes.keys())
    for task in ml10.train_tasks:
        env = train_env_instances[task.env_name]
        env.set_task(task)
        env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
        step_env(env, max_path_length=STEPS, render=False)
    for env in train_env_instances.values():
        env.close()
    del train_env_instances
    test_env_instances = {env_name: env_cls() for (env_name, env_cls) in ml10.test_classes.items()}
    test_env_rand_vecs = check_tasks_unique(ml10.test_tasks, ml10._test_classes.keys())
    for task in ml10.test_tasks:
        env = test_env_instances[task.env_name]
        env.set_task(task)
        env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
        step_env(env, max_path_length=STEPS, render=False)
    for env in test_env_instances.values():
        env.close()
    train_test_rand_vecs = set()
    for rand_vecs in train_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    for rand_vecs in test_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    assert len(train_test_rand_vecs) == (len(ml10.test_classes.keys()) + len(ml10.train_classes.keys())) * metaworld._N_GOALS
    del test_env_instances","for rand_vecs in train_env_rand_vecs.values():
    for rand_vec in rand_vecs:
        train_test_rand_vecs.add(tuple(rand_vec))",train_test_rand_vecs = {tuple(rand_vec) for rand_vecs in train_env_rand_vecs.values() for rand_vec in rand_vecs},['train_test_rand_vecs = {tuple(rand_vec) for rand_vecs in train_env_rand_vecs.values() for rand_vec in rand_vecs}'],1,
metaworld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/metaworld/tests/integration/test_new_api.py,https://github.com/rlworkgroup/metaworld/tree/master/tests/integration/test_new_api.py,,test_all_ml10$62,"def test_all_ml10():
    ml10 = ML10()
    train_env_instances = {env_name: env_cls() for (env_name, env_cls) in ml10.train_classes.items()}
    train_env_rand_vecs = check_tasks_unique(ml10.train_tasks, ml10._train_classes.keys())
    for task in ml10.train_tasks:
        env = train_env_instances[task.env_name]
        env.set_task(task)
        env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
        step_env(env, max_path_length=STEPS, render=False)
    for env in train_env_instances.values():
        env.close()
    del train_env_instances
    test_env_instances = {env_name: env_cls() for (env_name, env_cls) in ml10.test_classes.items()}
    test_env_rand_vecs = check_tasks_unique(ml10.test_tasks, ml10._test_classes.keys())
    for task in ml10.test_tasks:
        env = test_env_instances[task.env_name]
        env.set_task(task)
        env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
        step_env(env, max_path_length=STEPS, render=False)
    for env in test_env_instances.values():
        env.close()
    train_test_rand_vecs = set()
    for rand_vecs in train_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    for rand_vecs in test_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    assert len(train_test_rand_vecs) == (len(ml10.test_classes.keys()) + len(ml10.train_classes.keys())) * metaworld._N_GOALS
    del test_env_instances","for rand_vecs in test_env_rand_vecs.values():
    for rand_vec in rand_vecs:
        train_test_rand_vecs.add(tuple(rand_vec))",train_test_rand_vecs = {tuple(rand_vec) for rand_vecs in test_env_rand_vecs.values() for rand_vec in rand_vecs},Cannot refactor,2,
fuck-coding-interviews,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fuck-coding-interviews/problems/pangrams.py,https://github.com/vinta/fuck-coding-interviews/tree/master/problems/pangrams.py,,pangrams$12,"def pangrams(s):
    alphabet = set()
    for char in s.lower():
        if char != ' ':
            alphabet.add(char)
    if len(alphabet) == 26:
        return 'pangram'
    else:
        return 'not pangram'","for char in s.lower():
    if char != ' ':
        alphabet.add(char)",alphabet = {char for char in s.lower() if char != ' '},["alphabet = {char for char in s.lower() if char != ' '}"],1,
Diamond,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Diamond/src/collectors/filestat/filestat.py,https://github.com/python-diamond/Diamond/tree/master/src/collectors/filestat/filestat.py,FilestatCollector,get_userlist$181,"def get_userlist(self):
    """"""
        This collects all the users with open files on the system, and filters
        based on the variables user_include and user_exclude
        """"""
    if isinstance(self.config['user_include'], basestring):
        self.config['user_include'] = self.config['user_include'].split()
    if isinstance(self.config['user_exclude'], basestring):
        self.config['user_exclude'] = self.config['user_exclude'].split()
    if isinstance(self.config['group_include'], basestring):
        self.config['group_include'] = self.config['group_include'].split()
    if isinstance(self.config['group_exclude'], basestring):
        self.config['group_exclude'] = self.config['group_exclude'].split()
    rawusers = set()
    for line in self.get_output('L'):
        if line.startswith('L'):
            rawusers.add(line[1:].rstrip('\n'))
    rawusers = list(rawusers)
    userlist = []
    if self.config['user_include'] is None or len(self.config['user_include']) == 0:
        userlist = rawusers
    else:
        userlist = []
    addedByGroup = []
    if self.config['group_include'] is not None and len(self.config['group_include']) > 0:
        for u in rawusers:
            self.log.info(u)
            user_groups = os.popen('id -Gn %s' % u).read().split()
            for gi in self.config['group_include']:
                if gi in user_groups and u not in userlist:
                    userlist.append(u)
                    addedByGroup.append(u)
                    break
    if self.config['group_exclude'] is not None and len(self.config['group_exclude']) > 0:
        tmplist = userlist[:]
        for u in tmplist:
            groups = os.popen('id -Gn %s' % u).read().split()
            for gi in self.config['group_exclude']:
                if gi in groups:
                    userlist.remove(u)
                    break
    self.config['uid_min'] = int(self.config['uid_min'])
    self.config['uid_max'] = int(self.config['uid_max'])
    tmplist = userlist[:]
    for u in tmplist:
        if self.config['user_include'] is None or u not in self.config['user_include']:
            if u not in addedByGroup:
                uid = int(os.popen('id -u %s' % u).read())
                if uid < self.config['uid_min'] and self.config['uid_min'] is not None and (u in userlist):
                    userlist.remove(u)
                if uid > self.config['uid_max'] and self.config['uid_max'] is not None and (u in userlist):
                    userlist.remove(u)
    if self.config['user_include'] is not None and len(self.config['user_include']) > 0:
        for u in self.config['user_include']:
            if u in rawusers and u not in userlist:
                userlist.append(u)
    if self.config['user_exclude'] is not None and len(self.config['user_exclude']) > 0:
        for u in self.config['user_exclude']:
            if u in userlist:
                userlist.remove(u)
    return userlist","for line in self.get_output('L'):
    if line.startswith('L'):
        rawusers.add(line[1:].rstrip('\n'))",rawusers = {line[1:].rstrip('\n') for line in self.get_output('L') if line.startswith('L')},["rawusers = {line[1:].rstrip('\\n') for line in self.get_output('L') if line.startswith('L')}"],1,
nltk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/test/unit/translate/test_ibm_model.py,https://github.com/nltk/nltk/tree/master/nltk/test/unit/translate/test_ibm_model.py,TestIBMModel,test_neighboring_finds_neighbor_alignments$137,"def test_neighboring_finds_neighbor_alignments(self):
    a_info = AlignmentInfo((0, 3, 2), (None, 'des', 'œufs', 'verts'), ('UNUSED', 'green', 'eggs'), [[], [], [2], [1]])
    ibm_model = IBMModel([])
    neighbors = ibm_model.neighboring(a_info)
    neighbor_alignments = set()
    for neighbor in neighbors:
        neighbor_alignments.add(neighbor.alignment)
    expected_alignments = {(0, 0, 2), (0, 1, 2), (0, 2, 2), (0, 3, 0), (0, 3, 1), (0, 3, 3), (0, 2, 3), (0, 3, 2)}
    self.assertEqual(neighbor_alignments, expected_alignments)","for neighbor in neighbors:
    neighbor_alignments.add(neighbor.alignment)",neighbor_alignments = {neighbor.alignment for neighbor in neighbors},['neighbor_alignments = {neighbor.alignment for neighbor in neighbors}'],1,
lbry-sdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lbry-sdk/lbry/testcase.py,https://github.com/lbryio/lbry-sdk/tree/master/lbry/testcase.py,CommandTestCase,get_all_addresses$459,"def get_all_addresses(tx):
    addresses = set()
    for txi in tx['inputs']:
        addresses.add(txi['address'])
    for txo in tx['outputs']:
        addresses.add(txo['address'])
    return list(addresses)","for txo in tx['outputs']:
    addresses.add(txo['address'])",addresses = {txo['address'] for txo in tx['outputs']},Cannot refactor,2,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/framework.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/framework.py,IrGraph,draw$5141,"def draw(self, save_path, name, marked_nodes=None, remove_ctr_var=True):
    """"""
        Draw the graph. If `dot` command is installed, the drawn graph
        will be saved as pdf file type, otherwise dot file type is used.

        Args:
            save_path(str): the save path of drawn graph.
            name(str): the name of drawn graph.
            marked_nodes(set(IrNode)): nodes that are needed to be marked.
            Default value is None.
            remove_ctr_var(bool): If it is set True, all control variable nodes
            in the graph will be removed. Default value is True.
        """"""

    def _convert_to_pdf(dot_file_path):
        pdf_save_path = os.path.splitext(dot_file_path)[0] + '.pdf'
        exited_code = subprocess.call('dot -Tpdf ' + dot_file_path + ' -o ' + pdf_save_path, shell=True)
        if exited_code != 0:
            print('The dot command is needed for creating pdf files.')
            print('The {} is saved as the dot filetype.'.format(dot_file_path))
    remove_ctr_vars = set()
    if remove_ctr_var:
        for node in self.all_var_nodes():
            if node.is_ctrl_var():
                remove_ctr_vars.add(node)
        self.safe_remove_nodes(remove_ctr_vars)
    print('Total ops num = {}.'.format(len(self.all_op_nodes())))
    if marked_nodes is not None:
        if not isinstance(marked_nodes, set):
            if isinstance(marked_nodes, Iterable):
                marked_nodes = set(marked_nodes)
            else:
                marked_nodes = {marked_nodes}
        marked_nodes = {n.node for n in marked_nodes}
        remove_ctr_vars = {n.node for n in remove_ctr_vars}
        marked_nodes = marked_nodes - remove_ctr_vars
        if self.graph.has('__graphviz__marked_node__'):
            self.graph.erase('__graphviz__marked_node__')
        self.graph.set('__graphviz__marked_node__', marked_nodes)
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    viz_dot_path = os.path.join(save_path, name) + '.dot'
    viz_pass = core.get_pass('graph_viz_pass')
    viz_pass.set('graph_viz_path', viz_dot_path)
    viz_pass.apply(self.graph)
    _convert_to_pdf(viz_dot_path)","for node in self.all_var_nodes():
    if node.is_ctrl_var():
        remove_ctr_vars.add(node)",remove_ctr_vars = {node for node in self.all_var_nodes() if node.is_ctrl_var()},['remove_ctr_vars = {node for node in self.all_var_nodes() if node.is_ctrl_var()}'],1,
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,_init_road_connectivity_cache$95,"def _init_road_connectivity_cache(self):
    self.road_connectivity_cache = PotentialRoadConnectivityCache(self)
    coords_set = set()
    for coords in self.plan:
        coords_set.add(coords)
    for coords in self.land_manager.roads:
        coords_set.add(coords)
    self.road_connectivity_cache.modify_area(list(sorted(coords_set)))","for coords in self.plan:
    coords_set.add(coords)",entries = {coords for coords in self.plan},['coords_set = {coords for coords in self.plan}'],0,
unknown-horizons,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,_init_road_connectivity_cache$95,"def _init_road_connectivity_cache(self):
    self.road_connectivity_cache = PotentialRoadConnectivityCache(self)
    coords_set = set()
    for coords in self.plan:
        coords_set.add(coords)
    for coords in self.land_manager.roads:
        coords_set.add(coords)
    self.road_connectivity_cache.modify_area(list(sorted(coords_set)))","for coords in self.land_manager.roads:
    coords_set.add(coords)",entries = {coords for coords in self.land_manager.roads},Cannot refactor,2,
Mobile-Security-Framework-MobSF,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mobile-Security-Framework-MobSF/mobsf/StaticAnalyzer/views/ios/macho_analysis.py,https://github.com/MobSF/Mobile-Security-Framework-MobSF/tree/master/mobsf/StaticAnalyzer/views/ios/macho_analysis.py,Checksec,has_canary$200,"def has_canary(self):
    stk_check = '___stack_chk_fail'
    stk_guard = '___stack_chk_guard'
    ipt_list = set()
    for ipt in self.macho.imported_functions:
        ipt_list.add(str(ipt))
    return stk_check in ipt_list and stk_guard in ipt_list","for ipt in self.macho.imported_functions:
    ipt_list.add(str(ipt))",ipt_list = {str(ipt) for ipt in self.macho.imported_functions},['ipt_list = {str(ipt) for ipt in self.macho.imported_functions}'],1,
viztracer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/viztracer/tests/test_regression.py,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_regression.py,TestIssue160,check_func$271,"def check_func(data):
    pids = set()
    for entry in data['traceEvents']:
        pids.add(entry['pid'])
    self.assertEqual(len(pids), 2)","for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']},["pids = {entry['pid'] for entry in data['traceEvents']}"],1,
clusterfuzz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/clusterfuzz/_internal/bot/fuzzers/dictionary_manager.py,https://github.com/google/clusterfuzz/tree/master/src/clusterfuzz/_internal/bot/fuzzers/dictionary_manager.py,,merge_dictionary_files$112,"def merge_dictionary_files(original_dictionary_path, recommended_dictionary_path, merged_dictionary_path):
    """"""Merge a list of dictionaries with given paths into a singe dictionary.""""""
    if original_dictionary_path and os.path.exists(original_dictionary_path):
        merged_dictionary_data = utils.read_data_from_file(original_dictionary_path, eval_data=False).decode('utf-8')
    else:
        merged_dictionary_data = ''
    recommended_dictionary_lines = utils.read_data_from_file(recommended_dictionary_path, eval_data=False).decode('utf-8').splitlines()
    dictionary_lines_to_add = set()
    for line in recommended_dictionary_lines:
        if line not in merged_dictionary_data:
            dictionary_lines_to_add.add(line)
    merged_dictionary_data += '\n%s\n' % RECOMMENDED_DICTIONARY_HEADER
    merged_dictionary_data += '\n'.join(dictionary_lines_to_add)
    utils.write_data_to_file(merged_dictionary_data, merged_dictionary_path)","for line in recommended_dictionary_lines:
    if line not in merged_dictionary_data:
        dictionary_lines_to_add.add(line)",dictionary_lines_to_add = {line for line in recommended_dictionary_lines if line not in merged_dictionary_data},['dictionary_lines_to_add = {line for line in recommended_dictionary_lines if line not in merged_dictionary_data}'],1,
pycorrector,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycorrector/pycorrector/corrector.py,https://github.com/shibing624/pycorrector/tree/master/pycorrector/corrector.py,Corrector,_confusion_word_set$162,"def _confusion_word_set(self, word):
    confusion_word_set = set()
    candidate_words = list(self.known(edit_distance_word(word, self.cn_char_set)))
    for candidate_word in candidate_words:
        if pypinyin.lazy_pinyin(candidate_word) == pypinyin.lazy_pinyin(word):
            confusion_word_set.add(candidate_word)
    return confusion_word_set","for candidate_word in candidate_words:
    if pypinyin.lazy_pinyin(candidate_word) == pypinyin.lazy_pinyin(word):
        confusion_word_set.add(candidate_word)",confusion_word_set = {candidate_word for candidate_word in candidate_words if pypinyin.lazy_pinyin(candidate_word) == pypinyin.lazy_pinyin(word)},['confusion_word_set = {candidate_word for candidate_word in candidate_words if pypinyin.lazy_pinyin(candidate_word) == pypinyin.lazy_pinyin(word)}'],1,
word_cloud,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/word_cloud/versioneer.py,https://github.com/amueller/word_cloud/tree/master//versioneer.py,,do_setup$1697,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print('Adding sample versioneer config to setup.cfg', file=sys.stderr)
            with open(os.path.join(root, 'setup.cfg'), 'a') as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1
    print(' creating %s' % cfg.versionfile_source)
    with open(cfg.versionfile_source, 'w') as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {'DOLLAR': '$', 'STYLE': cfg.style, 'TAG_PREFIX': cfg.tag_prefix, 'PARENTDIR_PREFIX': cfg.parentdir_prefix, 'VERSIONFILE_SOURCE': cfg.versionfile_source})
    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), '__init__.py')
    if os.path.exists(ipy):
        try:
            with open(ipy, 'r') as f:
                old = f.read()
        except EnvironmentError:
            old = ''
        if INIT_PY_SNIPPET not in old:
            print(' appending to %s' % ipy)
            with open(ipy, 'a') as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print(' %s unmodified' % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None
    manifest_in = os.path.join(root, 'MANIFEST.in')
    simple_includes = set()
    try:
        with open(manifest_in, 'r') as f:
            for line in f:
                if line.startswith('include '):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    if 'versioneer.py' not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, 'a') as f:
            f.write('include versioneer.py\n')
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" % cfg.versionfile_source)
        with open(manifest_in, 'a') as f:
            f.write('include %s\n' % cfg.versionfile_source)
    else:
        print(' versionfile_source already in MANIFEST.in')
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f for include in line.split()[1:] if line.startswith('include ')},["simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}"],0,
awx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/awx/awx/sso/fields.py,https://github.com/ansible/awx/tree/master/awx/sso/fields.py,SAMLOrgInfoField,to_internal_value$655,"def to_internal_value(self, data):
    data = super(SAMLOrgInfoField, self).to_internal_value(data)
    invalid_keys = set()
    for key in data.keys():
        if not re.match('^[a-z]{2}(?:-[a-z]{2})??$', key, re.I):
            invalid_keys.add(key)
    if invalid_keys:
        invalid_keys = sorted(list(invalid_keys))
        keys_display = json.dumps(invalid_keys).lstrip('[').rstrip(']')
        self.fail('invalid_lang_code', invalid_lang_codes=keys_display)
    return data","for key in data.keys():
    if not re.match('^[a-z]{2}(?:-[a-z]{2})??$', key, re.I):
        invalid_keys.add(key)","invalid_keys = {key for key in data.keys() if not re.match('^[a-z]{2}(?:-[a-z]{2})??$', key, re.I)}","[""invalid_keys = {key for key in data.keys() if not re.match('^[a-z]{2}(?:-[a-z]{2})??$', key, re.I)}""]",1,
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/google/cloud/forseti/notifier/notifiers/inventory_summary.py,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/notifier/notifiers/inventory_summary.py,InventorySummary,_get_gsuite_dwd_status$211,"def _get_gsuite_dwd_status(summary_data):
    """"""Get the status of whether G Suite DwD is enabled or not.

        Args:
            summary_data (list): Summary of inventory data as a list of dicts.
                Example: [{resource_type, count}, {}, {}, ...]

        Returns:
            str: disabled or enabled.
        """"""
    gsuite_types = set(['gsuite_user'])
    summary_data_keys = set()
    if summary_data is None:
        return 'disabled'
    for resource in summary_data:
        summary_data_keys.add(resource['resource_type'])
    if gsuite_types.issubset(summary_data_keys):
        return 'enabled'
    return 'disabled'","for resource in summary_data:
    summary_data_keys.add(resource['resource_type'])",summary_data_keys = {resource['resource_type'] for resource in summary_data},["summary_data_keys = {resource['resource_type'] for resource in summary_data}"],1,
pifuhd,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pifuhd/lib/model/HGPIFuNetwNML.py,https://github.com/facebookresearch/pifuhd/tree/master/lib/model/HGPIFuNetwNML.py,HGPIFuNetwNML,loadFromHGHPIFu$73,"def loadFromHGHPIFu(self, net):
    hgnet = net.image_filter
    pretrained_dict = hgnet.state_dict()
    model_dict = self.image_filter.state_dict()
    pretrained_dict = {k: v for (k, v) in hgnet.state_dict().items() if k in model_dict}
    for (k, v) in pretrained_dict.items():
        if v.size() == model_dict[k].size():
            model_dict[k] = v
    not_initialized = set()
    for (k, v) in model_dict.items():
        if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
            not_initialized.add(k.split('.')[0])
    print('not initialized', sorted(not_initialized))
    self.image_filter.load_state_dict(model_dict)
    pretrained_dict = net.mlp.state_dict()
    model_dict = self.mlp.state_dict()
    pretrained_dict = {k: v for (k, v) in net.mlp.state_dict().items() if k in model_dict}
    for (k, v) in pretrained_dict.items():
        if v.size() == model_dict[k].size():
            model_dict[k] = v
    not_initialized = set()
    for (k, v) in model_dict.items():
        if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
            not_initialized.add(k.split('.')[0])
    print('not initialized', sorted(not_initialized))
    self.mlp.load_state_dict(model_dict)","for (k, v) in model_dict.items():
    if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
        not_initialized.add(k.split('.')[0])","not_initialized = {k.split('.')[0] for (k, v) in model_dict.items() if k not in pretrained_dict or v.size() != pretrained_dict[k].size()}","[""not_initialized = {k.split('.')[0] for (k, v) in model_dict.items() if k not in pretrained_dict or v.size() != pretrained_dict[k].size()}""]",1,
pifuhd,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pifuhd/lib/model/HGPIFuNetwNML.py,https://github.com/facebookresearch/pifuhd/tree/master/lib/model/HGPIFuNetwNML.py,HGPIFuNetwNML,loadFromHGHPIFu$73,"def loadFromHGHPIFu(self, net):
    hgnet = net.image_filter
    pretrained_dict = hgnet.state_dict()
    model_dict = self.image_filter.state_dict()
    pretrained_dict = {k: v for (k, v) in hgnet.state_dict().items() if k in model_dict}
    for (k, v) in pretrained_dict.items():
        if v.size() == model_dict[k].size():
            model_dict[k] = v
    not_initialized = set()
    for (k, v) in model_dict.items():
        if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
            not_initialized.add(k.split('.')[0])
    print('not initialized', sorted(not_initialized))
    self.image_filter.load_state_dict(model_dict)
    pretrained_dict = net.mlp.state_dict()
    model_dict = self.mlp.state_dict()
    pretrained_dict = {k: v for (k, v) in net.mlp.state_dict().items() if k in model_dict}
    for (k, v) in pretrained_dict.items():
        if v.size() == model_dict[k].size():
            model_dict[k] = v
    not_initialized = set()
    for (k, v) in model_dict.items():
        if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
            not_initialized.add(k.split('.')[0])
    print('not initialized', sorted(not_initialized))
    self.mlp.load_state_dict(model_dict)","for (k, v) in model_dict.items():
    if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
        not_initialized.add(k.split('.')[0])","not_initialized = {k.split('.')[0] for (k, v) in model_dict.items() if k not in pretrained_dict or v.size() != pretrained_dict[k].size()}","[""not_initialized = {k.split('.')[0] for (k, v) in model_dict.items() if k not in pretrained_dict or v.size() != pretrained_dict[k].size()}""]",1,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/pulse/test_transforms.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/pulse/test_transforms.py,TestAddImplicitAcquires,test_dont_add_all$234,"def test_dont_add_all(self):
    """"""Test that acquires aren't added if no qubits in the sublist aren't being acquired.""""""
    sched = transforms.add_implicit_acquires(self.sched, [[4, 5], [0, 2], [1, 3]])
    acquired_qubits = set()
    for (_, inst) in sched.instructions:
        if isinstance(inst, Acquire):
            acquired_qubits.add(inst.acquire.index)
    self.assertEqual(acquired_qubits, {0, 1, 2, 3})","for (_, inst) in sched.instructions:
    if isinstance(inst, Acquire):
        acquired_qubits.add(inst.acquire.index)","acquired_qubits = {inst.acquire.index for (_, inst) in sched.instructions if isinstance(inst, Acquire)}","['acquired_qubits = {inst.acquire.index for (_, inst) in sched.instructions if isinstance(inst, Acquire)}']",1,
unilm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unilm/infoxlm/fairseq/scripts/compare_namespaces.py,https://github.com/microsoft/unilm/tree/master/infoxlm/fairseq/scripts/compare_namespaces.py,,main$7,"def main():
    ns1 = eval(input('Namespace 1: '))
    ns2 = eval(input('Namespace 2: '))

    def keys(ns):
        ks = set()
        for k in dir(ns):
            if not k.startswith('_'):
                ks.add(k)
        return ks
    k1 = keys(ns1)
    k2 = keys(ns2)

    def print_keys(ks, ns1, ns2=None):
        for k in ks:
            if ns2 is None:
                print('{}\t{}'.format(k, getattr(ns1, k, None)))
            else:
                print('{}\t{}\t{}'.format(k, getattr(ns1, k, None), getattr(ns2, k, None)))
    print('Keys unique to namespace 1:')
    print_keys(k1 - k2, ns1)
    print()
    print('Keys unique to namespace 2:')
    print_keys(k2 - k1, ns2)
    print()
    print('Overlapping keys with different values:')
    ks = [k for k in k1 & k2 if getattr(ns1, k, 'None') != getattr(ns2, k, 'None')]
    print_keys(ks, ns1, ns2)
    print()","for k in dir(ns):
    if not k.startswith('_'):
        ks.add(k)",ks = {k for k in dir(ns) if not k.startswith('_')},["ks = {k for k in dir(ns) if not k.startswith('_')}"],1,
opencensus-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opencensus-python/opencensus/metrics/export/gauge.py,https://github.com/census-instrumentation/opencensus-python/tree/master/opencensus/metrics/export/gauge.py,Registry,get_metrics$501,"def get_metrics(self):
    """"""Get a metric for each gauge in the registry at the current time.

        :rtype: set(:class:`opencensus.metrics.export.metric.Metric`)
        :return: A set of `Metric`s, one for each registered gauge.
        """"""
    now = datetime.utcnow()
    metrics = set()
    for gauge in self.gauges.values():
        metrics.add(gauge.get_metric(now))
    return metrics","for gauge in self.gauges.values():
    metrics.add(gauge.get_metric(now))",metrics = {gauge.get_metric(now) for gauge in self.gauges.values()},['metrics = {gauge.get_metric(now) for gauge in self.gauges.values()}'],1,
devpi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/devpi/.ci/cleanup_devpi_indices.py,https://github.com/devpi/devpi/tree/master/.ci/cleanup_devpi_indices.py,,get_release_dates$26,"def get_release_dates(baseurl, username, indexname, projectname):
    response = session.get(baseurl + username + '/' + indexname + '/' + projectname)
    assert response.status_code == 200
    result = response.json()['result']
    dates = set()
    for value in result.values():
        for link in value.get('+links', []):
            for log in link.get('log', []):
                dates.add(tuple(log['when']))
    return dates","for value in result.values():
    for link in value.get('+links', []):
        for log in link.get('log', []):
            dates.add(tuple(log['when']))","dates = {tuple(log['when']) for value in result.values() for link in value.get('+links', []) for log in link.get('log', [])}","[""dates = {tuple(log['when']) for value in result.values() for link in value.get('+links', []) for log in link.get('log', [])}""]",1,
ARL,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ARL/app/tasks/scheduler.py,https://github.com/TophantTechnology/ARL/tree/master/app/tasks/scheduler.py,DomainExecutor,run$100,"def run(self):
    self.update_task_field('start_time', utils.curr_date())
    self.domain_fetch()
    for domain_info in self.domain_info_list:
        self.domain_set.add(domain_info.domain)
    self.set_scope_domain()
    new_domain_set = self.domain_set - self.scope_domain_set
    self.new_domain_set = new_domain_set
    self.set_wildcard_ip_set()
    self.set_domain_info_list()
    self.start_ip_fetch()
    self.start_site_fetch()
    self.insert_cip_stat()
    self.insert_finger_stat()
    self.insert_task_stat()
    self.update_task_field('status', TaskStatus.DONE)
    self.update_task_field('end_time', utils.curr_date())
    ret_new_domain_set = set()
    for domain_info in self.domain_info_list:
        ret_new_domain_set.add(domain_info.domain)
    return ret_new_domain_set","for domain_info in self.domain_info_list:
    self.domain_set.add(domain_info.domain)",entries = {domain_info.domain for domain_info in self.domain_info_list},Cannot refactor,2,
ARL,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ARL/app/tasks/scheduler.py,https://github.com/TophantTechnology/ARL/tree/master/app/tasks/scheduler.py,DomainExecutor,run$100,"def run(self):
    self.update_task_field('start_time', utils.curr_date())
    self.domain_fetch()
    for domain_info in self.domain_info_list:
        self.domain_set.add(domain_info.domain)
    self.set_scope_domain()
    new_domain_set = self.domain_set - self.scope_domain_set
    self.new_domain_set = new_domain_set
    self.set_wildcard_ip_set()
    self.set_domain_info_list()
    self.start_ip_fetch()
    self.start_site_fetch()
    self.insert_cip_stat()
    self.insert_finger_stat()
    self.insert_task_stat()
    self.update_task_field('status', TaskStatus.DONE)
    self.update_task_field('end_time', utils.curr_date())
    ret_new_domain_set = set()
    for domain_info in self.domain_info_list:
        ret_new_domain_set.add(domain_info.domain)
    return ret_new_domain_set","for domain_info in self.domain_info_list:
    ret_new_domain_set.add(domain_info.domain)",ret_new_domain_set = {domain_info.domain for domain_info in self.domain_info_list},['ret_new_domain_set = {domain_info.domain for domain_info in self.domain_info_list}'],1,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,Question,sub_fields_used$6151,"def sub_fields_used(self):
    all_fields_used = set()
    for var_name in self.fields_used:
        all_fields_used.add(var_name)
    if len(self.fields) > 0 and hasattr(self.fields[0], 'choices'):
        for choice in self.fields[0].choices:
            if isinstance(choice['key'], Question):
                all_fields_used.update(choice['key'].sub_fields_used())
    return all_fields_used","for var_name in self.fields_used:
    all_fields_used.add(var_name)",all_fields_used = {var_name for var_name in self.fields_used},['all_fields_used = {var_name for var_name in self.fields_used}'],1,
aws-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-cli/awscli/text.py,https://github.com/aws/aws-cli/tree/master/awscli/text.py,,_all_scalar_keys$85,"def _all_scalar_keys(list_of_dicts):
    keys_seen = set()
    for item_dict in list_of_dicts:
        for (key, value) in item_dict.items():
            if not isinstance(value, (dict, list)):
                keys_seen.add(key)
    return list(sorted(keys_seen))","for item_dict in list_of_dicts:
    for (key, value) in item_dict.items():
        if not isinstance(value, (dict, list)):
            keys_seen.add(key)","keys_seen = {key for item_dict in list_of_dicts for (key, value) in item_dict.items() if not isinstance(value, (dict, list))}","['keys_seen = {key for item_dict in list_of_dicts for (key, value) in item_dict.items() if not isinstance(value, (dict, list))}']",1,
word2vec-tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/word2vec-tutorial/segment.py,https://github.com/zake7749/word2vec-tutorial/tree/master//segment.py,,main$6,"def main():
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    jieba.set_dictionary('jieba_dict/dict.txt.big')
    stopword_set = set()
    with open('jieba_dict/stopwords.txt', 'r', encoding='utf-8') as stopwords:
        for stopword in stopwords:
            stopword_set.add(stopword.strip('\n'))
    output = open('wiki_seg.txt', 'w', encoding='utf-8')
    with open('wiki_zh_tw.txt', 'r', encoding='utf-8') as content:
        for (texts_num, line) in enumerate(content):
            line = line.strip('\n')
            words = jieba.cut(line, cut_all=False)
            for word in words:
                if word not in stopword_set:
                    output.write(word + ' ')
            output.write('\n')
            if (texts_num + 1) % 10000 == 0:
                logging.info('已完成前 %d 行的斷詞' % (texts_num + 1))
    output.close()","for stopword in stopwords:
    stopword_set.add(stopword.strip('\n'))",stopword_set = {stopword.strip('\n') for stopword in stopwords},["stopword_set = {stopword.strip('\\n') for stopword in stopwords}"],1,
imgaug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgaug/test/augmenters/test_size.py,https://github.com/aleju/imgaug/tree/master/test/augmenters/test_size.py,TestResize,test_decrease_size_by_tuples_of_floats__one_per_side$1551,"def test_decrease_size_by_tuples_of_floats__one_per_side(self):
    image2d = self.image2d[0:4, 0:4]
    image3d = self.image3d[0:4, 0:4, :]
    aug = iaa.Resize({'height': (0.76, 1.0), 'width': (0.76, 1.0)})
    not_seen2d = set()
    not_seen3d = set()
    for hsize in sm.xrange(3, 4 + 1):
        for wsize in sm.xrange(3, 4 + 1):
            not_seen2d.add((hsize, wsize))
    for hsize in sm.xrange(3, 4 + 1):
        for wsize in sm.xrange(3, 4 + 1):
            not_seen3d.add((hsize, wsize, 3))
    possible2d = set(list(not_seen2d))
    possible3d = set(list(not_seen3d))
    for _ in sm.xrange(100):
        observed2d = aug.augment_image(image2d)
        observed3d = aug.augment_image(image3d)
        assert observed2d.shape in possible2d
        assert observed3d.shape in possible3d
        if observed2d.shape in not_seen2d:
            not_seen2d.remove(observed2d.shape)
        if observed3d.shape in not_seen3d:
            not_seen3d.remove(observed3d.shape)
        if not not_seen2d and (not not_seen3d):
            break
    assert not not_seen2d
    assert not not_seen3d","for hsize in sm.xrange(3, 4 + 1):
    for wsize in sm.xrange(3, 4 + 1):
        not_seen2d.add((hsize, wsize))","not_seen2d = {(hsize, wsize) for hsize in sm.xrange(3, 4 + 1) for wsize in sm.xrange(3, 4 + 1)}","['not_seen2d = {(hsize, wsize) for hsize in sm.xrange(3, 4 + 1) for wsize in sm.xrange(3, 4 + 1)}']",1,
imgaug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgaug/test/augmenters/test_size.py,https://github.com/aleju/imgaug/tree/master/test/augmenters/test_size.py,TestResize,test_decrease_size_by_tuples_of_floats__one_per_side$1551,"def test_decrease_size_by_tuples_of_floats__one_per_side(self):
    image2d = self.image2d[0:4, 0:4]
    image3d = self.image3d[0:4, 0:4, :]
    aug = iaa.Resize({'height': (0.76, 1.0), 'width': (0.76, 1.0)})
    not_seen2d = set()
    not_seen3d = set()
    for hsize in sm.xrange(3, 4 + 1):
        for wsize in sm.xrange(3, 4 + 1):
            not_seen2d.add((hsize, wsize))
    for hsize in sm.xrange(3, 4 + 1):
        for wsize in sm.xrange(3, 4 + 1):
            not_seen3d.add((hsize, wsize, 3))
    possible2d = set(list(not_seen2d))
    possible3d = set(list(not_seen3d))
    for _ in sm.xrange(100):
        observed2d = aug.augment_image(image2d)
        observed3d = aug.augment_image(image3d)
        assert observed2d.shape in possible2d
        assert observed3d.shape in possible3d
        if observed2d.shape in not_seen2d:
            not_seen2d.remove(observed2d.shape)
        if observed3d.shape in not_seen3d:
            not_seen3d.remove(observed3d.shape)
        if not not_seen2d and (not not_seen3d):
            break
    assert not not_seen2d
    assert not not_seen3d","for hsize in sm.xrange(3, 4 + 1):
    for wsize in sm.xrange(3, 4 + 1):
        not_seen3d.add((hsize, wsize, 3))","not_seen3d = {(hsize, wsize, 3) for hsize in sm.xrange(3, 4 + 1) for wsize in sm.xrange(3, 4 + 1)}","['not_seen3d = {(hsize, wsize, 3) for hsize in sm.xrange(3, 4 + 1) for wsize in sm.xrange(3, 4 + 1)}']",1,
opentelemetry-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opentelemetry-python/opentelemetry-api/tests/propagators/test_composite.py,https://github.com/open-telemetry/opentelemetry-python/tree/master/opentelemetry-api/tests/propagators/test_composite.py,TestCompositePropagator,test_fields$122,"def test_fields(self):
    propagator = CompositePropagator([self.mock_propagator_0, self.mock_propagator_1, self.mock_propagator_2])
    mock_setter = Mock()
    propagator.inject({}, setter=mock_setter)
    inject_fields = set()
    for mock_call in mock_setter.mock_calls:
        inject_fields.add(mock_call[1][1])
    self.assertEqual(inject_fields, propagator.fields)","for mock_call in mock_setter.mock_calls:
    inject_fields.add(mock_call[1][1])",entries = {mock_call[1][1] for mock_call in mock_setter.mock_calls},['inject_fields = {mock_call[1][1] for mock_call in mock_setter.mock_calls}'],0,
anago,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anago/tests/test_wrapper.py,https://github.com/Hironsan/anago/tree/master/tests/test_wrapper.py,TestWrapper,test_train_vocab_init$84,"def test_train_vocab_init(self):
    vocab = set()
    for words in np.r_[self.x_train, self.x_test, self.x_test]:
        for word in words:
            vocab.add(word)
    model = anago.Sequence(initial_vocab=vocab, embeddings=self.embeddings)
    model.fit(self.x_train, self.y_train, self.x_test, self.y_test)","for word in words:
    vocab.add(word)",vocab = {word for word in words},Cannot refactor,2,
Cura,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Cura/cura/Settings/IntentManager.py,https://github.com/Ultimaker/Cura/tree/master/cura/Settings/IntentManager.py,IntentManager,intentCategories$61,"def intentCategories(self, definition_id: str, nozzle_id: str, material_id: str) -> List[str]:
    """"""Collects and returns all intent categories available for the given

        parameters. Note that the 'default' category is always available.

        :param definition_id: ID of the printer.
        :param nozzle_name: Name of the nozzle.
        :param material_id: ID of the material.
        :return: A set of intent category names.
        """"""
    categories = set()
    for intent in self.intentMetadatas(definition_id, nozzle_id, material_id):
        categories.add(intent['intent_category'])
    categories.add('default')
    return list(categories)","for intent in self.intentMetadatas(definition_id, nozzle_id, material_id):
    categories.add(intent['intent_category'])","categories = {intent['intent_category'] for intent in self.intentMetadatas(definition_id, nozzle_id, material_id)}","[""categories = {intent['intent_category'] for intent in self.intentMetadatas(definition_id, nozzle_id, material_id)}""]",1,
brian2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brian2/brian2/synapses/synapses.py,https://github.com/brian-team/brian2/tree/master/brian2/synapses/synapses.py,Synapses,_expression_index_dependence$1913,"def _expression_index_dependence(self, expr, namespace, additional_indices=None):
    """"""
        Returns the set of synaptic indices that expr depends on
        """"""
    nr = NodeRenderer()
    expr = nr.render_expr(expr)
    deps = set()
    if additional_indices is None:
        additional_indices = {}
    identifiers = get_identifiers_recursively([expr], self.variables)
    variables = self.resolve_all({name for name in identifiers if not name in additional_indices}, namespace)
    if any((getattr(var, 'auto_vectorise', False) for var in variables.values())):
        identifiers.add('_vectorisation_idx')
    for varname in identifiers:
        if varname == 'i':
            deps.add('_presynaptic_idx')
        elif varname == 'j':
            deps.add('_iterator_idx')
        elif varname in additional_indices:
            deps.add(additional_indices[varname])
        else:
            deps.add(self.variables.indices[varname])
    if '0' in deps:
        deps.remove('0')
    return deps","for varname in identifiers:
    if varname == 'i':
        deps.add('_presynaptic_idx')
    elif varname == 'j':
        deps.add('_iterator_idx')
    elif varname in additional_indices:
        deps.add(additional_indices[varname])
    else:
        deps.add(self.variables.indices[varname])",deps = {('_presynaptic_idx' if varname == 'i' else '_iterator_idx' if varname == 'j' else additional_indices[varname] if varname in additional_indices else self.variables.indices[varname]) for varname in identifiers},["deps = {'_presynaptic_idx' if varname == 'i' else '_iterator_idx' if varname == 'j' else additional_indices[varname] if varname in additional_indices else self.variables.indices[varname] for varname in identifiers}"],0,
polyglot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/polyglot/polyglot/downloader.py,https://github.com/aboSamoor/polyglot/tree/master/polyglot/downloader.py,,_unzip_iter$1221,"def _unzip_iter(filename, root, verbose=True):
    if verbose:
        sys.stdout.write('Unzipping %s' % os.path.split(filename)[1])
        sys.stdout.flush()
    try:
        zf = zipfile.ZipFile(filename)
    except zipfile.error as e:
        yield ErrorMessage(filename, 'Error with downloaded zip file')
        return
    except Exception as e:
        yield ErrorMessage(filename, e)
        return
    namelist = zf.namelist()
    dirlist = set()
    for x in namelist:
        if x.endswith('/'):
            dirlist.add(x)
        else:
            dirlist.add(x.rsplit('/', 1)[0] + '/')
    filelist = [x for x in namelist if not x.endswith('/')]
    if not os.path.exists(root):
        os.mkdir(root)
    for dirname in sorted(dirlist):
        pieces = dirname[:-1].split('/')
        for i in range(len(pieces)):
            dirpath = os.path.join(root, *pieces[:i + 1])
            if not os.path.exists(dirpath):
                os.mkdir(dirpath)
    for (i, filename) in enumerate(filelist):
        filepath = os.path.join(root, *filename.split('/'))
        with open(filepath, 'wb') as outfile:
            try:
                contents = zf.read(filename)
            except Exception as e:
                yield ErrorMessage(filename, e)
                return
            outfile.write(contents)
        if verbose and i * 10 / len(filelist) > (i - 1) * 10 / len(filelist):
            sys.stdout.write('.')
            sys.stdout.flush()
    if verbose:
        print()","for x in namelist:
    if x.endswith('/'):
        dirlist.add(x)
    else:
        dirlist.add(x.rsplit('/', 1)[0] + '/')","dirlist = {x if x.endswith('/') else x.rsplit('/', 1)[0] + '/' for x in namelist}","[""dirlist = {x if x.endswith('/') else x.rsplit('/', 1)[0] + '/' for x in namelist}""]",1,
FastMOT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FastMOT/fastmot/plugins/get_compute.py,https://github.com/GeekAlexis/FastMOT/tree/master/fastmot/plugins/get_compute.py,,main$19,"def main():
    libnames = ('libcuda.so', 'libcuda.dylib', 'cuda.dll')
    for libname in libnames:
        try:
            cuda = ctypes.CDLL(libname)
        except OSError:
            continue
        else:
            break
    else:
        return
    gpu_archs = set()
    n_gpus = ctypes.c_int()
    cc_major = ctypes.c_int()
    cc_minor = ctypes.c_int()
    result = ctypes.c_int()
    device = ctypes.c_int()
    error_str = ctypes.c_char_p()
    result = cuda.cuInit(0)
    if result != CUDA_SUCCESS:
        cuda.cuGetErrorString(result, ctypes.byref(error_str))
        print('cuInit failed with error code %d: %s' % (result, error_str.value.decode()))
        return 1
    result = cuda.cuDeviceGetCount(ctypes.byref(n_gpus))
    if result != CUDA_SUCCESS:
        cuda.cuGetErrorString(result, ctypes.byref(error_str))
        print('cuDeviceGetCount failed with error code %d: %s' % (result, error_str.value.decode()))
        return 1
    for i in range(n_gpus.value):
        if cuda.cuDeviceComputeCapability(ctypes.byref(cc_major), ctypes.byref(cc_minor), device) == CUDA_SUCCESS:
            gpu_archs.add(str(cc_major.value) + str(cc_minor.value))
    print(' '.join(gpu_archs))
    return 0","for i in range(n_gpus.value):
    if cuda.cuDeviceComputeCapability(ctypes.byref(cc_major), ctypes.byref(cc_minor), device) == CUDA_SUCCESS:
        gpu_archs.add(str(cc_major.value) + str(cc_minor.value))","gpu_archs = {str(cuda.cuDeviceComputeCapability(ctypes.byref(cc_major), ctypes.byref(cc_minor), device)[0].value) + str(cuda.cuDeviceComputeCapability(ctypes.byref(cc_major), ctypes.byref(cc_minor), device)[1].value) for i in range(n_gpus.value)}","['gpu_archs = {str(cc_major.value) + str(cc_minor.value) for i in range(n_gpus.value) if cuda.cuDeviceComputeCapability(ctypes.byref(cc_major), ctypes.byref(cc_minor), device) == CUDA_SUCCESS}']",0,
Cura,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Cura/plugins/UM3NetworkPrinting/src/Cloud/CloudOutputDeviceManager.py,https://github.com/Ultimaker/Cura/tree/master/plugins/UM3NetworkPrinting/src/Cloud/CloudOutputDeviceManager.py,CloudOutputDeviceManager,_devicesRemovedFromAccount$305,"def _devicesRemovedFromAccount(self, removed_device_ids: Set[str]) -> None:
    """"""
        Removes the CloudOutputDevice from the received device ids and marks the specific printers as ""removed from
        account"". In addition, it generates a message to inform the user about the printers that are no longer linked to
        his/her account. The message is not generated if all the printers have been previously reported as not linked
        to the account.

        :param removed_device_ids: Set of device ids, whose CloudOutputDevice needs to be removed
        :return: None
        """"""
    if not CuraApplication.getInstance().getCuraAPI().account.isLoggedIn:
        return
    ignored_device_ids = set()
    for device_id in removed_device_ids:
        if not parseBool(self._um_cloud_printers[device_id].getMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, 'true')):
            ignored_device_ids.add(device_id)
    self.reported_device_ids = removed_device_ids - ignored_device_ids
    if not self.reported_device_ids:
        return
    output_device_manager = CuraApplication.getInstance().getOutputDeviceManager()
    for device_id in removed_device_ids:
        global_stack: Optional[GlobalStack] = self._um_cloud_printers.get(device_id, None)
        if not global_stack:
            continue
        if device_id in output_device_manager.getOutputDeviceIds():
            output_device_manager.removeOutputDevice(device_id)
        if device_id in self._remote_clusters:
            del self._remote_clusters[device_id]
        global_stack.setMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, False)
    device_names = ''.join(['<li>{} ({})</li>'.format(self._um_cloud_printers[device].name, self._um_cloud_printers[device].definition.name) for device in self.reported_device_ids])
    self._removed_printers_message = RemovedPrintersMessage(self.reported_device_ids, device_names)
    self._removed_printers_message.actionTriggered.connect(self._onRemovedPrintersMessageActionTriggered)
    self._removed_printers_message.show()","for device_id in removed_device_ids:
    if not parseBool(self._um_cloud_printers[device_id].getMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, 'true')):
        ignored_device_ids.add(device_id)","ignored_device_ids = {device_id for device_id in removed_device_ids if not parseBool(self._um_cloud_printers[device_id].getMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, 'true'))}","[""ignored_device_ids = {device_id for device_id in removed_device_ids if not parseBool(self._um_cloud_printers[device_id].getMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, 'true'))}""]",1,
tenet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tenet/plugins/tenet/trace/file.py,https://github.com/gaasedelen/tenet/tree/master/plugins/tenet/trace/file.py,TraceFile,get_reg_mask_ids_containing$395,"def get_reg_mask_ids_containing(self, reg_name):
    """"""
        Return a set of reg mask ids containing the given register name.
        """"""
    reg_id = self.arch.REGISTERS.index(reg_name.upper())
    reg_mask = 1 << reg_id
    found = set()
    for (i, current_mask) in enumerate(self.masks):
        if current_mask & reg_mask:
            found.add(i)
    return found","for (i, current_mask) in enumerate(self.masks):
    if current_mask & reg_mask:
        found.add(i)","found = {i for (i, current_mask) in enumerate(self.masks) if current_mask & reg_mask}","['found = {i for (i, current_mask) in enumerate(self.masks) if current_mask & reg_mask}']",1,
taurus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taurus/bzt/modules/reporting.py,https://github.com/Blazemeter/taurus/tree/master/bzt/modules/reporting.py,FinalStatus,__get_sample_element$188,"def __get_sample_element(self, sample, label_name):
    failed_samples_count = sample['fail']
    success_samples_count = sample['succ']
    total_samples_count = failed_samples_count + success_samples_count
    assert total_samples_count > 0, 'Total samples is zero for %s' % label_name
    success_samples_perc = success_samples_count * 100 / total_samples_count
    errors = set()
    for err_desc in sample['errors']:
        errors.add(err_desc['msg'])
    return (label_name, 'FAIL' if failed_samples_count > 0 else 'OK', '{0:.2f}%'.format(round(success_samples_perc, 2)), '{0:.3f}'.format(round(sample['avg_rt'], 3)), '\n'.join(errors))","for err_desc in sample['errors']:
    errors.add(err_desc['msg'])",errors = {err_desc['msg'] for err_desc in sample['errors']},["errors = {err_desc['msg'] for err_desc in sample['errors']}"],1,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/contrib/slim/quantization/post_training_quantization.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/quantization/post_training_quantization.py,,_remove_ctrl_vars$86,"def _remove_ctrl_vars(graph):
    remove_ctr_vars = set()
    for node in graph.all_var_nodes():
        if node.is_ctrl_var():
            remove_ctr_vars.add(node)
    graph.safe_remove_nodes(remove_ctr_vars)
    return graph","for node in graph.all_var_nodes():
    if node.is_ctrl_var():
        remove_ctr_vars.add(node)",remove_ctr_vars = {node for node in graph.all_var_nodes() if node.is_ctrl_var()},['remove_ctr_vars = {node for node in graph.all_var_nodes() if node.is_ctrl_var()}'],1,
hydrus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydrus/hydrus/client/gui/ClientGUIScrolledPanelsManagement.py,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/gui/ClientGUIScrolledPanelsManagement.py,_MediaPanel,_GetUnsetMediaViewFiletypes$2056,"def _GetUnsetMediaViewFiletypes(self):
    editable_mimes = set(HC.SEARCHABLE_MIMES)
    set_mimes = set()
    for (mime, media_show_action, media_start_paused, media_start_with_embed, preview_show_action, preview_start_paused, preview_start_with_embed, zoom_info) in self._media_viewer_options.GetData():
        set_mimes.add(mime)
    unset_mimes = editable_mimes.difference(set_mimes)
    return unset_mimes","for (mime, media_show_action, media_start_paused, media_start_with_embed, preview_show_action, preview_start_paused, preview_start_with_embed, zoom_info) in self._media_viewer_options.GetData():
    set_mimes.add(mime)","set_mimes = {mime for (mime, media_show_action, media_start_paused, media_start_with_embed, preview_show_action, preview_start_paused, preview_start_with_embed, zoom_info) in self._media_viewer_options.GetData()}","['set_mimes = {mime for (mime, media_show_action, media_start_paused, media_start_with_embed, preview_show_action, preview_start_paused, preview_start_with_embed, zoom_info) in self._media_viewer_options.GetData()}']",1,
