repo_name,file_path,file_html,class_name,me_name,me_code,old_code,new_code,bool_code,chatGPT_code,if_correct,instruction,sys_msg,exam_msg,user_msg,
find_or_refactor_wrong,,,,,,,,,,,,,,,
rasa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/tests/core/policies/test_ted_policy.py,https://github.com/RasaHQ/rasa/tree/master/tests/core/policies/test_ted_policy.py,TestTEDPolicy,test_gen_batch$317,"def test_gen_batch(
        self, trained_policy: TEDPolicy, default_domain: Domain, stories_path: Path
    ):
        training_trackers = tests.core.test_policies.train_trackers(
            default_domain, stories_path, augmentation_factor=0
        )
        precomputations = None
        training_data, label_ids, entity_tags = trained_policy._featurize_for_training(
            training_trackers, default_domain, precomputations
        )

        _, all_labels = trained_policy._create_label_data(
            default_domain, precomputations
        )
        model_data = trained_policy._create_model_data(
            training_data, label_ids, entity_tags, all_labels
        )
        batch_size = 2
        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=False, batch_strategy=""sequence""
        )
        iterator = iter(data_generator)
        # model data keys were sorted, so the order is alphabetical
        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # batch and dialogue dimensions are NOT combined for masks
        assert (
            batch_slots_mask.shape[0] == batch_size
            and batch_intent_mask.shape[0] == batch_size
            and batch_entities_mask.shape[0] == batch_size
            and batch_action_name_mask.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )

        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=True, batch_strategy=""balanced""
        )
        iterator = iter(data_generator)

        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )",batch_slots_mask.shape[0] == batch_size and batch_intent_mask.shape[0] == batch_size and (batch_entities_mask.shape[0] == batch_size) and (batch_action_name_mask.shape[0] == batch_size),batch_slots_mask.shape[0] == batch_size and batch_intent_mask.shape[0] == batch_size and (batch_entities_mask.shape[0] == batch_size) and (batch_action_name_mask.shape[0] == batch_size == batch_entities_mask.shape[0]),Cannot refactor,2,0,0,1,,,it actually cannot refactor
taurus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taurus/tests/unit/modules/jmeter/test_ScenarioBuilder.py,https://github.com/Blazemeter/taurus/tree/master/tests/unit/modules/jmeter/test_ScenarioBuilder.py,TestScenarioBuilder,get_plugin_json_extractor_config$34,"def get_plugin_json_extractor_config(xml_tree):
        cfg = {}
        block_name = ""com.atlantbh.jmeter.plugins.jsonutils.jsonpathextractor.JSONPathExtractor""
        blocks = xml_tree.findall("".//%s"" % block_name)
        for block in blocks:
            varname = block.find("".//stringProp[@name='VAR']"")
            jsonpath = block.find("".//stringProp[@name='JSONPATH']"")
            default = block.find("".//stringProp[@name='DEFAULT']"")
            subject = block.find("".//stringProp[@name='SUBJECT']"")
            from_variable = block.find("".//stringProp[@name='VARIABLE']"")
            varname = varname.text
            jsonpath = jsonpath.text
            if default is not None:
                default = default.text
            if (subject is not None) and subject.text == ""VAR"" and (from_variable is not None):
                from_variable = from_variable.text
            else:
                from_variable = None
            cfg[varname] = {""jsonpath"": jsonpath, ""default"": default, ""from_variable"": from_variable}
        return cfg",subject is not None and subject.text == 'VAR' and (from_variable is not None),subject.text == 'VAR' and subject is not None is not from_variable,subject is not None is not from_variable and subject.text == 'VAR',0,0,,,,,
zmirror,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zmirror/tests/base_class.py,https://github.com/aploium/zmirror/tree/master/tests/base_class.py,ZmirrorTestBase,reload_zmirror$56,"def reload_zmirror(self, configs_dict=None):
        self.del_temp_var()

        import config
        importlib.reload(config)

        test_config_names = (name for name in dir(self.C) if name[:2] != '__' and name[-2:] != '__')
        for config_name in test_config_names:
            config_value = getattr(self.C, config_name)
            setattr(config, config_name, config_value)

        if configs_dict is not None:
            for config_name, config_value in configs_dict.items():
                setattr(config, config_name, config_value)

        import zmirror.cache_system as cache_system
        import zmirror.zmirror as zmirror
        importlib.reload(cache_system)
        importlib.reload(zmirror)

        zmirror.app.config['TESTING'] = True

        # 处理有端口号的测试, 在 del_temp_var() 中回滚
        if hasattr(self.C, ""my_host_port""):
            port = getattr(self.C, ""my_host_port"", None)
            my_host_name = getattr(self.C, ""my_host_name"", ""127.0.0.1"")
            if port is not None:
                self.C.my_host_name_no_port = my_host_name
                self.C.my_host_name = self.C.my_host_name_no_port + "":"" + str(port)
            else:
                self.C.my_host_name_no_port = my_host_name
        elif hasattr(self.C, ""my_host_name""):
            self.C.my_host_name_no_port = self.C.my_host_name

        self.client = zmirror.app.test_client()  # type: FlaskClient
        self.app = zmirror.app  # type: Flask
        self.zmirror = zmirror",name[:2] != '__' and name[-2:] != '__',v3 != '__' != v1,name[:2] != '__' != name[-2:],0,0,,,,,
Parakeet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Parakeet/parakeet/frontend/tone_sandhi.py,https://github.com/PaddlePaddle/Parakeet/tree/master/parakeet/frontend/tone_sandhi.py,ToneSandhi,_three_sandhi$166,"def _three_sandhi(self, word: str, finals: List[str]) -> List[str]:
        if len(word) == 2 and self._all_tone_three(finals):
            finals[0] = finals[0][:-1] + ""2""
        elif len(word) == 3:
            word_list = self._split_word(word)
            if self._all_tone_three(finals):
                #  disyllabic + monosyllabic, e.g. 蒙古/包
                if len(word_list[0]) == 2:
                    finals[0] = finals[0][:-1] + ""2""
                    finals[1] = finals[1][:-1] + ""2""
                #  monosyllabic + disyllabic, e.g. 纸/老虎
                elif len(word_list[0]) == 1:
                    finals[1] = finals[1][:-1] + ""2""
            else:
                finals_list = [
                    finals[:len(word_list[0])], finals[len(word_list[0]):]
                ]
                if len(finals_list) == 2:
                    for i, sub in enumerate(finals_list):
                        # e.g. 所有/人
                        if self._all_tone_three(sub) and len(sub) == 2:
                            finals_list[i][0] = finals_list[i][0][:-1] + ""2""
                        # e.g. 好/喜欢
                        elif i == 1 and not self._all_tone_three(sub) and finals_list[i][0][-1] == ""3"" and \
                                finals_list[0][-1][-1] == ""3"":

                            finals_list[0][-1] = finals_list[0][-1][:-1] + ""2""
                        finals = sum(finals_list, [])
        # split idiom into two words who's length is 2
        elif len(word) == 4:
            finals_list = [finals[:2], finals[2:]]
            finals = []
            for sub in finals_list:
                if self._all_tone_three(sub):
                    sub[0] = sub[0][:-1] + ""2""
                finals += sub

        return finals",i == 1 and (not self._all_tone_three(sub)) and (finals_list[i][0][-1] == '3') and (finals_list[0][-1][-1] == '3'),i == 1 and (not self._all_tone_three(sub)) and (finals_list[i][0][-1] == '3') and (finals_list[0][-1][-1] == '3' == finals_list[i][0][-1]),i == 1 and finals_list[i][0][-1] == '3' == finals_list[0][-1][-1] and (not self._all_tone_three(sub)),0,0,,,,,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/ir/inference/test_trt_convert_prelu.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/ir/inference/test_trt_convert_prelu.py,TrtConvertPreluTest,sample_predictor_configs$150,"def sample_predictor_configs(
        self, program_config
    ) -> (paddle_infer.Config, List[int], float):
        def generate_dynamic_shape(attrs):
            if self.dim1 == 0:
                self.dynamic_shape.min_input_shape = {
                    ""input_data"": [1],
                }
                self.dynamic_shape.max_input_shape = {
                    ""input_data"": [4],
                }
                self.dynamic_shape.opt_input_shape = {
                    ""input_data"": [2],
                }
            else:
                if self.dim2 == 0 and self.dim3 == 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3],
                    }
                elif self.dim2 != 0 and self.dim3 != 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1, 1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 3, 16, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3, 16, 32],
                    }
                elif self.dim3 == 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 3, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3, 16],
                    }

        def clear_dynamic_shape():
            self.dynamic_shape.max_input_shape = {}
            self.dynamic_shape.min_input_shape = {}
            self.dynamic_shape.opt_input_shape = {}

        attrs = [
            program_config.ops[i].attrs for i in range(len(program_config.ops))
        ]

        def generate_trt_nodes_num(attrs, dynamic_shape):
            if (
                not dynamic_shape
                and self.dim1 == 0
                and self.dim2 == 0
                and self.dim3 == 0
            ):
                return 0, 3
            return 1, 2

        # for static_shape
        clear_dynamic_shape()
        self.trt_param.precision = paddle_infer.PrecisionType.Float32
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, False
        ), 1e-5
        self.trt_param.precision = paddle_infer.PrecisionType.Half
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, False
        ), (1e-3, 1e-3)

        # for dynamic_shape
        generate_dynamic_shape(attrs)
        self.trt_param.precision = paddle_infer.PrecisionType.Float32
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, True
        ), 1e-5
        self.trt_param.precision = paddle_infer.PrecisionType.Half
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, True
        ), (1e-3, 1e-3)",not dynamic_shape and self.dim1 == 0 and (self.dim2 == 0) and (self.dim3 == 0),not dynamic_shape and self.dim1 == 0 and (self.dim2 == 0) and (self.dim3 == 0 == self.dim2),Cannot refactor,2,0,,,,,it actually cannot refactor
aws-data-wrangler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-data-wrangler/awswrangler/quicksight/_describe.py,https://github.com/awslabs/aws-data-wrangler/tree/master/awswrangler/quicksight/_describe.py,,describe_dashboard$14,"def describe_dashboard(
    name: Optional[str] = None,
    dashboard_id: Optional[str] = None,
    account_id: Optional[str] = None,
    boto3_session: Optional[boto3.Session] = None,
) -> Dict[str, Any]:
    """"""Describe a QuickSight dashboard by name or ID.

    Note
    ----
    You must pass a not None ``name`` or ``dashboard_id`` argument.

    Parameters
    ----------
    name : str, optional
        Dashboard name.
    dashboard_id : str, optional
        Dashboard ID.
    account_id : str, optional
        If None, the account ID will be inferred from your boto3 session.
    boto3_session : boto3.Session(), optional
        Boto3 Session. The default boto3 session will be used if boto3_session receive None.

    Returns
    -------
    Dict[str, Any]
        Dashboad Description.

    Examples
    --------
    >>> import awswrangler as wr
    >>> description = wr.quicksight.describe_dashboard(name=""my-dashboard"")
    """"""
    if (name is None) and (dashboard_id is None):
        raise exceptions.InvalidArgument(""You must pass a not None name or dashboard_id argument."")
    session: boto3.Session = _utils.ensure_session(session=boto3_session)
    if account_id is None:
        account_id = sts.get_account_id(boto3_session=session)
    if (dashboard_id is None) and (name is not None):
        dashboard_id = get_dashboard_id(name=name, account_id=account_id, boto3_session=session)
    client: boto3.client = _utils.client(service_name=""quicksight"", session=session)
    return cast(
        Dict[str, Any], client.describe_dashboard(AwsAccountId=account_id, DashboardId=dashboard_id)[""Dashboard""]
    )",dashboard_id is None and name is not None,name != None == dashboard_id,dashboard_id is None is not name,0,0,,,,,
tensorflow-DeepFM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-DeepFM/DeepFM.py,https://github.com/ChenglongChen/tensorflow-DeepFM/tree/master//DeepFM.py,DeepFM,training_termination$327,"def training_termination(self, valid_result):
        if len(valid_result) > 5:
            if self.greater_is_better:
                if valid_result[-1] < valid_result[-2] and \
                    valid_result[-2] < valid_result[-3] and \
                    valid_result[-3] < valid_result[-4] and \
                    valid_result[-4] < valid_result[-5]:
                    return True
            else:
                if valid_result[-1] > valid_result[-2] and \
                    valid_result[-2] > valid_result[-3] and \
                    valid_result[-3] > valid_result[-4] and \
                    valid_result[-4] > valid_result[-5]:
                    return True
        return False",valid_result[-1] < valid_result[-2] and valid_result[-2] < valid_result[-3] and (valid_result[-3] < valid_result[-4]) and (valid_result[-4] < valid_result[-5]),valid_result[-1] < valid_result[-2] and valid_result[-2] < valid_result[-3] and (valid_result[-3] < valid_result[-4]) and (valid_result[-3] < valid_result[-4] < valid_result[-5]),Cannot refactor,2,0,0,1,,,it actually cannot refactor
tensorflow-DeepFM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-DeepFM/DeepFM.py,https://github.com/ChenglongChen/tensorflow-DeepFM/tree/master//DeepFM.py,DeepFM,training_termination$327,"def training_termination(self, valid_result):
        if len(valid_result) > 5:
            if self.greater_is_better:
                if valid_result[-1] < valid_result[-2] and \
                    valid_result[-2] < valid_result[-3] and \
                    valid_result[-3] < valid_result[-4] and \
                    valid_result[-4] < valid_result[-5]:
                    return True
            else:
                if valid_result[-1] > valid_result[-2] and \
                    valid_result[-2] > valid_result[-3] and \
                    valid_result[-3] > valid_result[-4] and \
                    valid_result[-4] > valid_result[-5]:
                    return True
        return False",valid_result[-1] > valid_result[-2] and valid_result[-2] > valid_result[-3] and (valid_result[-3] > valid_result[-4]) and (valid_result[-4] > valid_result[-5]),valid_result[-1] > valid_result[-2] and valid_result[-2] > valid_result[-3] and (valid_result[-3] > valid_result[-4]) and (valid_result[-3] > valid_result[-4] > valid_result[-5]),Cannot refactor,2,0,0,1,,,it actually cannot refactor
BeRoot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BeRoot/Windows/templates/MS16-075/webclient/secretsdump.py,https://github.com/AlessandroZ/BeRoot/tree/master/Windows/templates/MS16-075/webclient/secretsdump.py,,if_main_my$2299,"if __name__ == '__main__':
    # Init the example's logger theme
    # logger.init()
    # Explicitly changing the stdout encoding format
    if sys.stdout.encoding is None:
        # Output is redirected to a file
        sys.stdout = codecs.getwriter('utf8')(sys.stdout)

    print(version.BANNER)

    parser = argparse.ArgumentParser(add_help=True,
                                     description=""Performs various techniques to dump secrets from the remote machine without executing any agent there."")

    parser.add_argument('target', action='store',
                        help='[[domain/]username[:password]@]<targetName or address> or LOCAL (if you want to parse local files)')
    parser.add_argument('-debug', action='store_true', help='Turn DEBUG output ON')
    parser.add_argument('-system', action='store', help='SYSTEM hive to parse')
    parser.add_argument('-security', action='store', help='SECURITY hive to parse')
    parser.add_argument('-sam', action='store', help='SAM hive to parse')
    parser.add_argument('-ntds', action='store', help='NTDS.DIT file to parse')
    parser.add_argument('-resumefile', action='store',
                        help='resume file name to resume NTDS.DIT session dump (only available to DRSUAPI approach). This file will also be used to keep updating the session\'s state')
    parser.add_argument('-outputfile', action='store',
                        help='base output filename. Extensions will be added for sam, secrets, cached and ntds')
    parser.add_argument('-use-vss', action='store_true', default=False,
                        help='Use the VSS method insead of default DRSUAPI')
    group = parser.add_argument_group('display options')
    group.add_argument('-just-dc-user', action='store', metavar='USERNAME',
                       help='Extract only NTDS.DIT data for the user specified. Only available for DRSUAPI approach. Implies also -just-dc switch')
    group.add_argument('-just-dc', action='store_true', default=False,
                       help='Extract only NTDS.DIT data (NTLM hashes and Kerberos keys)')
    group.add_argument('-just-dc-ntlm', action='store_true', default=False,
                       help='Extract only NTDS.DIT data (NTLM hashes only)')
    group.add_argument('-pwd-last-set', action='store_true', default=False,
                       help='Shows pwdLastSet attribute for each NTDS.DIT account. Doesn\'t apply to -outputfile data')
    group.add_argument('-history', action='store_true', help='Dump password history')
    group = parser.add_argument_group('authentication')

    group.add_argument('-hashes', action=""store"", metavar=""LMHASH:NTHASH"", help='NTLM hashes, format is LMHASH:NTHASH')
    group.add_argument('-no-pass', action=""store_true"", help='don\'t ask for password (useful for -k)')
    group.add_argument('-k', action=""store_true"",
                       help='Use Kerberos authentication. Grabs credentials from ccache file (KRB5CCNAME) based on target parameters. If valid credentials cannot be found, it will use the ones specified in the command line')
    group.add_argument('-aesKey', action=""store"", metavar=""hex key"",
                       help='AES key to use for Kerberos Authentication (128 or 256 bits)')
    group.add_argument('-dc-ip', action='store', metavar=""ip address"",
                       help='IP Address of the domain controller. If ommited it use the domain part (FQDN) specified in the target parameter')

    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)

    options = parser.parse_args()

    # if options.debug is True:
    #     logging.getLogger().setLevel(logging.DEBUG)
    # else:
    #     logging.getLogger().setLevel(logging.INFO)

    import re

    domain, username, password, address = re.compile('(?:(?:([^/@:]*)/)?([^@:]*)(?::([^@]*))?@)?(.*)').match(
        options.target).groups('')

    # In case the password contains '@'
    if '@' in address:
        password = password + '@' + address.rpartition('@')[0]
        address = address.rpartition('@')[2]

    if options.just_dc_user is not None:
        if options.use_vss is True:
            logging.error('-just-dc-user switch is not supported in VSS mode')
            sys.exit(1)
        elif options.resumefile is not None:
            logging.error('resuming a previous NTDS.DIT dump session not compatible with -just-dc-user switch')
            sys.exit(1)
        elif address.upper() == 'LOCAL' and username == '':
            logging.error('-just-dc-user not compatible in LOCAL mode')
            sys.exit(1)
        else:
            # Having this switch on implies not asking for anything else.
            options.just_dc = True

    if options.use_vss is True and options.resumefile is not None:
        logging.error('resuming a previous NTDS.DIT dump session is not supported in VSS mode')
        sys.exit(1)

    if address.upper() == 'LOCAL' and username == '' and options.resumefile is not None:
        logging.error('resuming a previous NTDS.DIT dump session is not supported in LOCAL mode')
        sys.exit(1)

    if address.upper() == 'LOCAL' and username == '':
        if options.system is None:
            logging.error('SYSTEM hive is always required for local parsing, check help')
            sys.exit(1)
    else:

        if domain is None:
            domain = ''

        if password == '' and username != '' and options.hashes is None and options.no_pass is False and options.aesKey is None:
            from getpass import getpass

            password = getpass(""Password:"")

        if options.aesKey is not None:
            options.k = True

    dumper = DumpSecrets(address, username, password, domain, options)
    try:
        dumper.dump()
    except Exception as e:
        logging.error(e)",password == '' and username != '' and (options.hashes is None) and (options.no_pass is False) and (options.aesKey is None),password == '' and username != '' and (options.hashes is None) and (options.no_pass is False) and (options.aesKey is None is options.hashes),Cannot refactor,2,0,0,1,,,it actually cannot refactor
capa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capa/capa/ida/plugin/proxy.py,https://github.com/mandiant/capa/tree/master/capa/ida/plugin/proxy.py,CapaExplorerRangeProxyModel,lessThan$29,"def lessThan(self, left, right):
        """"""return True if left item is less than right item, else False

        @param left: QModelIndex of left
        @param right: QModelIndex of right
        """"""
        ldata = left.internalPointer().data(left.column())
        rdata = right.internalPointer().data(right.column())

        if (
            ldata
            and rdata
            and left.column() == CapaExplorerDataModel.COLUMN_INDEX_VIRTUAL_ADDRESS
            and left.column() == right.column()
        ):
            # convert virtual address before compare
            return int(ldata, 16) < int(rdata, 16)
        else:
            # compare as lowercase
            return ldata.lower() < rdata.lower()",ldata and rdata and (left.column() == CapaExplorerDataModel.COLUMN_INDEX_VIRTUAL_ADDRESS) and (left.column() == right.column()),ldata and rdata and (left.column() == CapaExplorerDataModel.COLUMN_INDEX_VIRTUAL_ADDRESS) and (CapaExplorerDataModel.COLUMN_INDEX_VIRTUAL_ADDRESS == left.column() == right.column()),CapaExplorerDataModel.COLUMN_INDEX_VIRTUAL_ADDRESS == left.column() == right.column() and ldata and rdata,0,0,,,,,
PyGaze,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyGaze/pygaze/_eyetracker/libeyelogic.py,https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libeyelogic.py,EyeLogicTracker,calibrate$391,"def calibrate(self):
        #self.screen.clear()
        #self.screen.draw_text(
        #    text=""Calibrate EyeTracker"",
        #    fontsize=20)
        #self.disp.fill(self.screen)
        #self.disp.show()

        if (not self._recording.is_set()):
            resultTracking = self.api.requestTracking(0)
            if (resultTracking != ELApi.ReturnStart.SUCCESS):
                raise Exception(""unable to start eye tracker"")

        resultCalibrate = self.api.calibrate(0)
        if (resultCalibrate != ELApi.ReturnCalibrate.SUCCESS):
            self.api.unrequestTracking()
            self.errorbeep.play()
            raise Exception(""Calibration failed = {}"".format(errorstringCalibrate(resultCalibrate)))
        self._calibrated.set()

        # NOISE CALIBRATION
        self.screen.clear()
        self.screen.draw_text(
            text=""Noise calibration. Please look at the dot, and press any key to start."",
            fontsize=20, \
            pos=(int(self.dispsize[0]/2),int(self.dispsize[1]*0.3)))
        x = int(float(self.dispsize[0]) / 2.0)
        y = int(float(self.dispsize[1]) / 2.0)
        self.screen.draw_fixation(fixtype=""dot"", pos=(x,y))
        self.disp.fill(self.screen)
        self.disp.show()
        self.kb.get_key(keylist=None, timeout=None, flush=True)

        # wait for a bit, to allow participant to fixate
        clock.pause(500)

        # get distance to screen
        screendist = 0
        i = 0
        while screendist == 0 and i < self.maxtries:
            i = i+1
            self.sampleLock.acquire()
            if (self.lastSample is not None):
                if self.eye_used != 1 and self.lastSample.eyePositionLeftZ != ELInvalidValue:
                    screendist = self.lastSample.eyePositionLeftZ / 10.0 # eyePositionZ is in mm; screendist is in cm
                elif self.eye_used != 0 and self.lastSample.eyePositionRightZ != ELInvalidValue:
                    screendist = self.lastSample.eyePositionRightZ / 10.0
            self.sampleLock.release()
            clock.pause(int(self.sampleTime))
        if i >= self.maxtries:
            self.api.unrequestTracking()
            self.errorbeep.play()
            raise Exception(""unable to receive gaze data for noise calibration"")

        # get samples
        sl = [self.sample()] # samplelist, prefilled with 1 sample to prevent sl[-1] from producing an error; first sample will be ignored for RMS calculation
        t0 = clock.get_time() # starting time
        while clock.get_time() - t0 < 1000:
            s = self.sample() # sample
            if s[0] != -1 and s[1] != -1 and s[0] != ELInvalidValue and s[1] != ELInvalidValue:
                sl.append(s)
            clock.pause(int(self.sampleTime))
        if (len(sl) < 2):
            if (not self._recording.is_set()):
                self.api.unrequestTracking()
            return False

        # calculate RMS noise
        Xvar = []
        Yvar = []
        Xmean = 0.
        Ymean = 0.
        for i in range(2,len(sl)):
            Xvar.append((sl[i][0]-sl[i-1][0])**2)
            Yvar.append((sl[i][1]-sl[i-1][1])**2)
            Xmean += sl[i][0]
            Ymean += sl[i][1]
        XRMS = (sum(Xvar) / len(Xvar))**0.5
        YRMS = (sum(Yvar) / len(Yvar))**0.5
        Xmean = Xmean / (len(sl)-2)
        Ymean = Ymean / (len(sl)-2)
        self.pxdsttresh = (XRMS, YRMS)

        # calculate pixels per cm
        pixpercm = (self.dispsize[0]/float(self.screensize[0]) + self.dispsize[1]/float(self.screensize[1])) / 2

        # get accuracy
        accuracyPxX = abs( Xmean - x )
        accuracyPxY = abs( Ymean - y )
        self.accuracy = ( pix2deg(screendist, accuracyPxX, pixpercm), \
                          pix2deg(screendist, accuracyPxY, pixpercm) )

        # calculate thresholds based on tracker settings
        self.pxfixtresh = deg2pix(screendist, self.fixtresh, pixpercm)
        self.pxaccuracy = (accuracyPxX, accuracyPxY )
        self.pxspdtresh = deg2pix(screendist, self.spdtresh/1000.0, pixpercm) # in pixels per millisecond
        self.pxacctresh = deg2pix(screendist, self.accthresh/1000.0, pixpercm) # in pixels per millisecond**2

        ## log
        self.log(""pygaze calibration"")
        self.log(""accuracy (degrees) = X={}, Y={}"".format( \
            self.accuracy[0], self.accuracy[1] ))
        self.log(""accuracy (in pixels) = X={}, Y={}"".format( \
            self.pxaccuracy[0], self.pxaccuracy[1]))
        self.log(""precision (RMS noise in pixels) = X={}, Y={}"".format( \
            self.pxdsttresh[0], self.pxdsttresh[1]))
        self.log(""distance between participant and display = {} cm"".format(screendist))
        self.log(""fixation threshold = {} pixels"".format(self.pxfixtresh))
        self.log(""speed threshold = {} pixels/ms"".format(self.pxspdtresh))
        self.log(""acceleration threshold = {} pixels/ms**2"".format(self.pxacctresh))
        
        if (not self._recording.is_set()):
            self.api.unrequestTracking()
        return True",s[0] != -1 and s[1] != -1 and (s[0] != ELInvalidValue) and (s[1] != ELInvalidValue),s[0] != -1 and s[1] != -1 and (s[0] != ELInvalidValue) and (s[1] != ELInvalidValue != s[0]),Cannot refactor,2,0,0,1,,,it actually cannot refactor
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/lxmert/modeling_tf_lxmert.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/lxmert/modeling_tf_lxmert.py,TFLxmertForPreTraining,call$1290,"def call(
        self,
        input_ids=None,
        visual_feats=None,
        visual_pos=None,
        attention_mask=None,
        visual_attention_mask=None,
        token_type_ids=None,
        inputs_embeds=None,
        masked_lm_labels=None,
        obj_labels=None,
        matched_label=None,
        ans=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        training=False,
    ):
        r""""""
        masked_lm_labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the
            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`
        obj_labels: (`Dict[Str: Tuple[tf.Tensor, tf.Tensor]]`, *optional*, defaults to `None`):
            each key is named after each one of the visual losses and each element of the tuple is of the shape
            `(batch_size, num_features)` and `(batch_size, num_features, visual_feature_dim)` for each the label id and
            the label score respectively
        matched_label (`tf.Tensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the whether or not the text input matches the image (classification) loss. Input
            should be a sequence pair (see `input_ids` docstring) Indices should be in `[0, 1]`:

            - 0 indicates that the sentence does not match the image,
            - 1 indicates that the sentence does match the image.
        ans (`Torch.Tensor` of shape `(batch_size)`, *optional*, defaults to `None`):
            a one hot representation hof the correct answer *optional*

        Returns:
        """"""

        lxmert_output = self.lxmert(
            input_ids,
            visual_feats,
            visual_pos,
            attention_mask,
            visual_attention_mask,
            token_type_ids,
            inputs_embeds,
            output_attentions,
            output_hidden_states,
            return_dict,
            training,
        )

        lang_output, visual_output, pooled_output = (
            lxmert_output[0],
            lxmert_output[1],
            lxmert_output[2],
        )
        lang_prediction_scores, cross_relationship_score = self.cls(lang_output, pooled_output)
        if self.task_qa:
            answer_score = self.answer_head(pooled_output)
        else:
            answer_score = pooled_output[0][0]

        total_loss = (
            None
            if (masked_lm_labels is None and matched_label is None and obj_labels is None and ans is None)
            else tf.constant(0.0)
        )
        losses = ()
        if masked_lm_labels is not None and self.task_mask_lm:
            masked_lm_loss = self.loss_fcts[""ce""](
                tf.reshape(masked_lm_labels, [-1]),
                tf.reshape(lang_prediction_scores, [-1, self.config.vocab_size]),
            )
            total_loss += masked_lm_loss
            losses += (masked_lm_loss,)
        if matched_label is not None and self.task_matched:
            matched_loss = self.loss_fcts[""ce""](
                tf.reshape(matched_label, [-1]),
                tf.reshape(cross_relationship_score, [-1, 2]),
            )
            total_loss += matched_loss
            losses += (matched_loss,)
        if obj_labels is not None and self.task_obj_predict:
            total_visn_loss = 0.0
            visn_prediction_scores_dict = self.obj_predict_head(visual_output)
            for key, key_info in self.visual_losses.items():
                label, mask_conf = obj_labels[key]
                output_dim = key_info[""num""]
                loss_fct_name = key_info[""loss""]
                label_shape = key_info[""shape""]
                weight = self.visual_loss_normalizer
                visn_loss_fct = self.loss_fcts[loss_fct_name]
                visn_prediction_scores = visn_prediction_scores_dict[key]
                visn_loss = visn_loss_fct(
                    tf.reshape(label, label_shape),
                    tf.reshape(visn_prediction_scores, [-1, output_dim]),
                )

                if visn_loss.ndim > 1:  # Regression Losses
                    visn_loss = tf.reduce_mean(visn_loss)
                visn_loss = tf.reduce_mean(visn_loss * tf.cast(tf.reshape(mask_conf, [-1]), visn_loss.dtype)) * weight
                total_visn_loss += visn_loss
                losses += (visn_loss,)
            total_loss += total_visn_loss
        if ans is not None and self.task_qa:
            answer_loss = self.loss_fcts[""ce""](
                tf.reshape(ans, [-1]), tf.reshape(answer_score, [-1, self.num_qa_labels])
            )
            # exclude ""*2"" here to match the effect of QA losses.
            # Previous: (loss *0) for 6 epochs, (loss *2) for 6 epochs.   (Used 10 instead of 6 in EMNLP paper)
            # Now     : (loss *1) for 12 epochs
            #
            # * 2       # Multiply by 2 because > half of the data will not have label
            total_loss += answer_loss
            losses += (answer_loss,)
        # return total_loss, tf.stack(losses)[tf.new_axis, ...], answer_score.detach()

        if not return_dict:
            output = (
                lang_prediction_scores,
                cross_relationship_score,
                answer_score,
            ) + lxmert_output[3:]
            return ((total_loss,) + output) if total_loss is not None else output

        return TFLxmertForPreTrainingOutput(
            loss=total_loss,
            prediction_logits=lang_prediction_scores,
            cross_relationship_score=cross_relationship_score,
            question_answering_score=answer_score,
            language_hidden_states=lxmert_output.language_hidden_states,
            vision_hidden_states=lxmert_output.vision_hidden_states,
            language_attentions=lxmert_output.language_attentions,
            vision_attentions=lxmert_output.vision_attentions,
            cross_encoder_attentions=lxmert_output.cross_encoder_attentions,
        )",masked_lm_labels is None and matched_label is None and (obj_labels is None) and (ans is None),masked_lm_labels is None and matched_label is None and (obj_labels is None) and (ans is None is obj_labels),Cannot refactor,2,0,0,1,,,it actually cannot refactor
khard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/khard/khard/carddav_object.py,https://github.com/scheibler/khard/tree/master/khard/carddav_object.py,YAMLEditable,_format_date_object$928,"def _format_date_object(date: Optional[Date], localize: bool) -> str:
        if not date:
            return """"
        if isinstance(date, str):
            return date
        if date.year == 1900 and date.month != 0 and date.day != 0 \
                and date.hour == 0 and date.minute == 0 and date.second == 0:
            return date.strftime(""--%m-%d"")
        tz = date.tzname()
        if (tz and tz[3:]) or (date.hour != 0 or date.minute != 0
                               or date.second != 0):
            if localize:
                return date.strftime(locale.nl_langinfo(locale.D_T_FMT))
            utc_offset = -time.timezone / 60 / 60
            return date.strftime(""%FT%T+{:02}:00"".format(int(utc_offset)))
        if localize:
            return date.strftime(locale.nl_langinfo(locale.D_FMT))
        return date.strftime(""%F"")",date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0),date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0 != date.day) and (date.minute == 0) and (date.second == 0),Cannot refactor,2,0,0,1,,,it actually cannot refactor
khard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/khard/khard/carddav_object.py,https://github.com/scheibler/khard/tree/master/khard/carddav_object.py,YAMLEditable,_format_date_object$928,"def _format_date_object(date: Optional[Date], localize: bool) -> str:
        if not date:
            return """"
        if isinstance(date, str):
            return date
        if date.year == 1900 and date.month != 0 and date.day != 0 \
                and date.hour == 0 and date.minute == 0 and date.second == 0:
            return date.strftime(""--%m-%d"")
        tz = date.tzname()
        if (tz and tz[3:]) or (date.hour != 0 or date.minute != 0
                               or date.second != 0):
            if localize:
                return date.strftime(locale.nl_langinfo(locale.D_T_FMT))
            utc_offset = -time.timezone / 60 / 60
            return date.strftime(""%FT%T+{:02}:00"".format(int(utc_offset)))
        if localize:
            return date.strftime(locale.nl_langinfo(locale.D_FMT))
        return date.strftime(""%F"")",date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0),date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0 != date.day),Cannot refactor,2,0,0,1,,,it actually cannot refactor
khard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/khard/khard/carddav_object.py,https://github.com/scheibler/khard/tree/master/khard/carddav_object.py,YAMLEditable,_format_date_object$928,"def _format_date_object(date: Optional[Date], localize: bool) -> str:
        if not date:
            return """"
        if isinstance(date, str):
            return date
        if date.year == 1900 and date.month != 0 and date.day != 0 \
                and date.hour == 0 and date.minute == 0 and date.second == 0:
            return date.strftime(""--%m-%d"")
        tz = date.tzname()
        if (tz and tz[3:]) or (date.hour != 0 or date.minute != 0
                               or date.second != 0):
            if localize:
                return date.strftime(locale.nl_langinfo(locale.D_T_FMT))
            utc_offset = -time.timezone / 60 / 60
            return date.strftime(""%FT%T+{:02}:00"".format(int(utc_offset)))
        if localize:
            return date.strftime(locale.nl_langinfo(locale.D_FMT))
        return date.strftime(""%F"")",date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0),date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0 == date.hour) and (date.second == 0),Cannot refactor,2,0,0,1,,,it actually cannot refactor
khard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/khard/khard/carddav_object.py,https://github.com/scheibler/khard/tree/master/khard/carddav_object.py,YAMLEditable,_format_date_object$928,"def _format_date_object(date: Optional[Date], localize: bool) -> str:
        if not date:
            return """"
        if isinstance(date, str):
            return date
        if date.year == 1900 and date.month != 0 and date.day != 0 \
                and date.hour == 0 and date.minute == 0 and date.second == 0:
            return date.strftime(""--%m-%d"")
        tz = date.tzname()
        if (tz and tz[3:]) or (date.hour != 0 or date.minute != 0
                               or date.second != 0):
            if localize:
                return date.strftime(locale.nl_langinfo(locale.D_T_FMT))
            utc_offset = -time.timezone / 60 / 60
            return date.strftime(""%FT%T+{:02}:00"".format(int(utc_offset)))
        if localize:
            return date.strftime(locale.nl_langinfo(locale.D_FMT))
        return date.strftime(""%F"")",date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0),date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0 == date.hour),Cannot refactor,2,0,0,1,,,it actually cannot refactor
khard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/khard/khard/carddav_object.py,https://github.com/scheibler/khard/tree/master/khard/carddav_object.py,YAMLEditable,_format_date_object$928,"def _format_date_object(date: Optional[Date], localize: bool) -> str:
        if not date:
            return """"
        if isinstance(date, str):
            return date
        if date.year == 1900 and date.month != 0 and date.day != 0 \
                and date.hour == 0 and date.minute == 0 and date.second == 0:
            return date.strftime(""--%m-%d"")
        tz = date.tzname()
        if (tz and tz[3:]) or (date.hour != 0 or date.minute != 0
                               or date.second != 0):
            if localize:
                return date.strftime(locale.nl_langinfo(locale.D_T_FMT))
            utc_offset = -time.timezone / 60 / 60
            return date.strftime(""%FT%T+{:02}:00"".format(int(utc_offset)))
        if localize:
            return date.strftime(locale.nl_langinfo(locale.D_FMT))
        return date.strftime(""%F"")",date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0),date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0 == date.minute),Cannot refactor,2,0,0,1,,,it actually cannot refactor
rasa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/rasa/shared/nlu/training_data/formats/readerwriter.py,https://github.com/RasaHQ/rasa/tree/master/rasa/shared/nlu/training_data/formats/readerwriter.py,TrainingDataWriter,generate_entity_attributes$151,"def generate_entity_attributes(
        text: Text, entity: Dict[Text, Any], short_allowed: bool = True
    ) -> Text:
        """"""Generates text for the entity attributes.

        Args:
            text: The text that is annotated with the entity
            entity: Entity data
            short_allowed: If `True`, allow shorthand annotation with parenthesis

        Returns:
            The annotation text that should follow the given text
        """"""
        entity_text = text
        entity_type = entity.get(ENTITY_ATTRIBUTE_TYPE)
        entity_value = entity.get(ENTITY_ATTRIBUTE_VALUE)
        entity_role = entity.get(ENTITY_ATTRIBUTE_ROLE)
        entity_group = entity.get(ENTITY_ATTRIBUTE_GROUP)

        if entity_value and entity_value == entity_text:
            entity_value = None

        use_short_syntax = (
            short_allowed
            and entity_value is None
            and entity_role is None
            and entity_group is None
        )

        if use_short_syntax:
            return f""({entity_type})""
        else:
            entity_dict = OrderedDict(
                [
                    (ENTITY_ATTRIBUTE_TYPE, entity_type),
                    (ENTITY_ATTRIBUTE_ROLE, entity_role),
                    (ENTITY_ATTRIBUTE_GROUP, entity_group),
                    (ENTITY_ATTRIBUTE_VALUE, entity_value),
                ]
            )
            entity_dict = OrderedDict(
                [(k, v) for k, v in entity_dict.items() if v is not None]
            )

            return f""{json.dumps(entity_dict)}""",short_allowed and entity_value is None and (entity_role is None) and (entity_group is None),short_allowed and entity_value is None and (entity_role is None) and (entity_role is None is entity_group),Cannot refactor,2,0,0,1,,,it actually cannot refactor
LeaderF,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LeaderF/autoload/leaderf/python/leaderf/manager.py,https://github.com/Yggdroot/LeaderF/tree/master/autoload/leaderf/python/leaderf/manager.py,Manager,_toUp$1067,"def _toUp(self):
        if self._getInstance().getWinPos() == 'popup':
            lfCmd(""call win_execute(%d, 'norm! k')"" % (self._getInstance().getPopupWinId()))
            self._getInstance().refreshPopupStatusline()
            return

        adjust = False
        if self._getInstance().isReverseOrder() and self._getInstance().getCurrentPos()[0] == 1:
            adjust = True
            self._setResultContent()
            if self._cli.pattern and self._cli.isFuzzy \
                    and len(self._highlight_pos) < (len(self._getInstance().buffer) - self._help_length) // self._getUnit() \
                    and len(self._highlight_pos) < int(lfEval(""g:Lf_NumberOfHighlight"")):
                self._highlight_method()

        lfCmd(""norm! k"")

        if adjust:
            lfCmd(""norm! zt"")

        self._getInstance().setLineNumber()
        lfCmd(""setlocal cursorline!"")   # these two help to redraw the statusline,
        lfCmd(""setlocal cursorline!"")",self._cli.pattern and self._cli.isFuzzy and (len(self._highlight_pos) < (len(self._getInstance().buffer) - self._help_length) // self._getUnit()) and (len(self._highlight_pos) < int(lfEval('g:Lf_NumberOfHighlight'))),self._cli.pattern and self._cli.isFuzzy and (len(self._highlight_pos) < (len(self._getInstance().buffer) - self._help_length) // self._getUnit()) and ((len(self._getInstance().buffer) - self._help_length) // self._getUnit() > len(self._highlight_pos) < v3),(len(self._getInstance().buffer) - self._help_length) // self._getUnit() > len(self._highlight_pos) < int(lfEval('g:Lf_NumberOfHighlight')) and self._cli.pattern and self._cli.isFuzzy,0,0,,,,,
new_find,,,,,,,,,,,,,,,
rasa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/tests/core/policies/test_ted_policy.py,https://github.com/RasaHQ/rasa/tree/master/tests/core/policies/test_ted_policy.py,TestTEDPolicy,test_gen_batch$317,"def test_gen_batch(
        self, trained_policy: TEDPolicy, default_domain: Domain, stories_path: Path
    ):
        training_trackers = tests.core.test_policies.train_trackers(
            default_domain, stories_path, augmentation_factor=0
        )
        precomputations = None
        training_data, label_ids, entity_tags = trained_policy._featurize_for_training(
            training_trackers, default_domain, precomputations
        )

        _, all_labels = trained_policy._create_label_data(
            default_domain, precomputations
        )
        model_data = trained_policy._create_model_data(
            training_data, label_ids, entity_tags, all_labels
        )
        batch_size = 2
        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=False, batch_strategy=""sequence""
        )
        iterator = iter(data_generator)
        # model data keys were sorted, so the order is alphabetical
        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # batch and dialogue dimensions are NOT combined for masks
        assert (
            batch_slots_mask.shape[0] == batch_size
            and batch_intent_mask.shape[0] == batch_size
            and batch_entities_mask.shape[0] == batch_size
            and batch_action_name_mask.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )

        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=True, batch_strategy=""balanced""
        )
        iterator = iter(data_generator)

        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )",batch_slots_mask.shape[0] == batch_size and batch_intent_mask.shape[0] == batch_size and (batch_entities_mask.shape[0] == batch_size) and (batch_action_name_mask.shape[0] == batch_size),batch_intent_mask.shape[0] == batch_size and batch_entities_mask.shape[0] == batch_size == batch_slots_mask.shape[0] and (batch_action_name_mask.shape[0] == batch_size),Cannot refactor,2,1,,,,,
rasa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/tests/core/policies/test_ted_policy.py,https://github.com/RasaHQ/rasa/tree/master/tests/core/policies/test_ted_policy.py,TestTEDPolicy,test_gen_batch$317,"def test_gen_batch(
        self, trained_policy: TEDPolicy, default_domain: Domain, stories_path: Path
    ):
        training_trackers = tests.core.test_policies.train_trackers(
            default_domain, stories_path, augmentation_factor=0
        )
        precomputations = None
        training_data, label_ids, entity_tags = trained_policy._featurize_for_training(
            training_trackers, default_domain, precomputations
        )

        _, all_labels = trained_policy._create_label_data(
            default_domain, precomputations
        )
        model_data = trained_policy._create_model_data(
            training_data, label_ids, entity_tags, all_labels
        )
        batch_size = 2
        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=False, batch_strategy=""sequence""
        )
        iterator = iter(data_generator)
        # model data keys were sorted, so the order is alphabetical
        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # batch and dialogue dimensions are NOT combined for masks
        assert (
            batch_slots_mask.shape[0] == batch_size
            and batch_intent_mask.shape[0] == batch_size
            and batch_entities_mask.shape[0] == batch_size
            and batch_action_name_mask.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )

        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=True, batch_strategy=""balanced""
        )
        iterator = iter(data_generator)

        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )",batch_slots_mask.shape[0] == batch_size and batch_intent_mask.shape[0] == batch_size and (batch_entities_mask.shape[0] == batch_size) and (batch_action_name_mask.shape[0] == batch_size),batch_intent_mask.shape[0] == batch_size and batch_entities_mask.shape[0] == batch_size and (batch_action_name_mask.shape[0] == batch_size == batch_slots_mask.shape[0]),Cannot refactor,2,1,,,,,
rasa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/tests/core/policies/test_ted_policy.py,https://github.com/RasaHQ/rasa/tree/master/tests/core/policies/test_ted_policy.py,TestTEDPolicy,test_gen_batch$317,"def test_gen_batch(
        self, trained_policy: TEDPolicy, default_domain: Domain, stories_path: Path
    ):
        training_trackers = tests.core.test_policies.train_trackers(
            default_domain, stories_path, augmentation_factor=0
        )
        precomputations = None
        training_data, label_ids, entity_tags = trained_policy._featurize_for_training(
            training_trackers, default_domain, precomputations
        )

        _, all_labels = trained_policy._create_label_data(
            default_domain, precomputations
        )
        model_data = trained_policy._create_model_data(
            training_data, label_ids, entity_tags, all_labels
        )
        batch_size = 2
        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=False, batch_strategy=""sequence""
        )
        iterator = iter(data_generator)
        # model data keys were sorted, so the order is alphabetical
        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # batch and dialogue dimensions are NOT combined for masks
        assert (
            batch_slots_mask.shape[0] == batch_size
            and batch_intent_mask.shape[0] == batch_size
            and batch_entities_mask.shape[0] == batch_size
            and batch_action_name_mask.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )

        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=True, batch_strategy=""balanced""
        )
        iterator = iter(data_generator)

        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )",batch_slots_mask.shape[0] == batch_size and batch_intent_mask.shape[0] == batch_size and (batch_entities_mask.shape[0] == batch_size) and (batch_action_name_mask.shape[0] == batch_size),batch_slots_mask.shape[0] == batch_size and batch_entities_mask.shape[0] == batch_size == batch_intent_mask.shape[0] and (batch_action_name_mask.shape[0] == batch_size),Cannot refactor,2,1,,,,,
rasa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/tests/core/policies/test_ted_policy.py,https://github.com/RasaHQ/rasa/tree/master/tests/core/policies/test_ted_policy.py,TestTEDPolicy,test_gen_batch$317,"def test_gen_batch(
        self, trained_policy: TEDPolicy, default_domain: Domain, stories_path: Path
    ):
        training_trackers = tests.core.test_policies.train_trackers(
            default_domain, stories_path, augmentation_factor=0
        )
        precomputations = None
        training_data, label_ids, entity_tags = trained_policy._featurize_for_training(
            training_trackers, default_domain, precomputations
        )

        _, all_labels = trained_policy._create_label_data(
            default_domain, precomputations
        )
        model_data = trained_policy._create_model_data(
            training_data, label_ids, entity_tags, all_labels
        )
        batch_size = 2
        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=False, batch_strategy=""sequence""
        )
        iterator = iter(data_generator)
        # model data keys were sorted, so the order is alphabetical
        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # batch and dialogue dimensions are NOT combined for masks
        assert (
            batch_slots_mask.shape[0] == batch_size
            and batch_intent_mask.shape[0] == batch_size
            and batch_entities_mask.shape[0] == batch_size
            and batch_action_name_mask.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )

        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=True, batch_strategy=""balanced""
        )
        iterator = iter(data_generator)

        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )",batch_slots_mask.shape[0] == batch_size and batch_intent_mask.shape[0] == batch_size and (batch_entities_mask.shape[0] == batch_size) and (batch_action_name_mask.shape[0] == batch_size),batch_slots_mask.shape[0] == batch_size and batch_entities_mask.shape[0] == batch_size and (batch_action_name_mask.shape[0] == batch_size == batch_intent_mask.shape[0]),Cannot refactor,2,1,,,,,
deepdrive,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deepdrive/sim/gym_env.py,https://github.com/deepdrive/deepdrive/tree/master/sim/gym_env.py,DeepDriveEnv,is_stuck$739,"def is_stuck(self, obz):
        start_is_stuck = time.time()

        # TODO: Get this from the game instead
        ret = False
        if 'TEST_END_OF_EPISODE' in os.environ and self.step_num >= 9:
            log.warn('TEST_END_OF_EPISODE is set triggering end of episode'
                     ' via is_stuck!')
            ret = True
        elif obz is None:
            log.debug('obz is None, not checking if stuck')
            ret = False
        elif obz['speed'] < 100:  # cm/s
            log.debug('speed less than 1m/s, checking if stuck')
            self.steps_crawling += 1
            if obz['throttle'] > 0 and obz['brake'] == 0 and obz['handbrake'] == 0:
                self.steps_crawling_with_throttle_on += 1
                log.debug('crawling detected num steps crawling is %d', self.steps_crawling_with_throttle_on)
            else:
                log.debug('not stuck, throttle %f, brake %f, handbrake %f',
                          obz['throttle'], obz['brake'], obz['handbrake'])

            time_crawling = time.time() - self.last_not_stuck_time

            # This was to detect legitimate stops, but we will have real
            # collision detection before the need to stop
            # portion_crawling = self.steps_crawling_with_throttle_on / max(1, self.steps_crawling)

            if self.steps_crawling_with_throttle_on > 40 and time_crawling > 5:
                log.warn('No progress made while throttle on - '
                         'assuming stuck and ending episode. '
                         'steps crawling: %r, '
                         'steps crawling with throttle on: %r, '
                         'time crawling: %r',
                         self.steps_crawling,
                         self.steps_crawling_with_throttle_on, time_crawling)
                self.set_forward_progress()
                self.episode_return.got_stuck = True
                ret = True
        else:
            log.debug('speed greater than 1m/s, not stuck')
            self.set_forward_progress()
        log.debug('is stuck took %fs', time.time() - start_is_stuck)

        if ret:
            log.info('episode finished, detected we were stuck')

        return ret",obz['throttle'] > 0 and obz['brake'] == 0 and (obz['handbrake'] == 0),obz['brake'] == 0 and obz['handbrake'] == 0 < obz['throttle'],Cannot refactor,2,1,,,,,
deepdrive,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deepdrive/sim/gym_env.py,https://github.com/deepdrive/deepdrive/tree/master/sim/gym_env.py,DeepDriveEnv,is_stuck$739,"def is_stuck(self, obz):
        start_is_stuck = time.time()

        # TODO: Get this from the game instead
        ret = False
        if 'TEST_END_OF_EPISODE' in os.environ and self.step_num >= 9:
            log.warn('TEST_END_OF_EPISODE is set triggering end of episode'
                     ' via is_stuck!')
            ret = True
        elif obz is None:
            log.debug('obz is None, not checking if stuck')
            ret = False
        elif obz['speed'] < 100:  # cm/s
            log.debug('speed less than 1m/s, checking if stuck')
            self.steps_crawling += 1
            if obz['throttle'] > 0 and obz['brake'] == 0 and obz['handbrake'] == 0:
                self.steps_crawling_with_throttle_on += 1
                log.debug('crawling detected num steps crawling is %d', self.steps_crawling_with_throttle_on)
            else:
                log.debug('not stuck, throttle %f, brake %f, handbrake %f',
                          obz['throttle'], obz['brake'], obz['handbrake'])

            time_crawling = time.time() - self.last_not_stuck_time

            # This was to detect legitimate stops, but we will have real
            # collision detection before the need to stop
            # portion_crawling = self.steps_crawling_with_throttle_on / max(1, self.steps_crawling)

            if self.steps_crawling_with_throttle_on > 40 and time_crawling > 5:
                log.warn('No progress made while throttle on - '
                         'assuming stuck and ending episode. '
                         'steps crawling: %r, '
                         'steps crawling with throttle on: %r, '
                         'time crawling: %r',
                         self.steps_crawling,
                         self.steps_crawling_with_throttle_on, time_crawling)
                self.set_forward_progress()
                self.episode_return.got_stuck = True
                ret = True
        else:
            log.debug('speed greater than 1m/s, not stuck')
            self.set_forward_progress()
        log.debug('is stuck took %fs', time.time() - start_is_stuck)

        if ret:
            log.info('episode finished, detected we were stuck')

        return ret",obz['throttle'] > 0 and obz['brake'] == 0 and (obz['handbrake'] == 0),obz['throttle'] > 0 and obz['handbrake'] == 0 == obz['brake'],Cannot refactor,2,1,,,,,
GhIDA,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GhIDA/ghida_plugin/idaxml.py,https://github.com/Cisco-Talos/GhIDA/tree/master/ghida_plugin/idaxml.py,XmlExporter,check_char$353,"def check_char(self, ch):
        """"""
        Replaces a special XML character with an entity string.

        Args:
            ch: String containing the character to check.

        Returns:
            String containing either the character or the entity
            substition string.
        """"""
        if ((ord(ch) < 0x20) and (ord(ch) != 0x09 and
                                  ord(ch) != 0x0A and ord(ch) != 0x0D)):
            return ''
        elif ch == '&':
            return '&amp;'
        elif ch == '<':
            return ""&lt;""
        elif ch == '>':
            return ""&gt;""
        elif ch == '\'':
            return ""&apos;""
        elif ch == '""':
            return ""&quot;""
        elif ch == '\x7F':
            return ''
        elif ord(ch) > 0x7F:
            return '&#x' + format(ord(ch), ""x"") + "";""
        return ch",ord(ch) != 9 and ord(ch) != 10 and (ord(ch) != 13),ord(ch) != 10 and 9 != ord(ch) != 13,Cannot refactor,2,1,,,,,
GhIDA,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GhIDA/ghida_plugin/idaxml.py,https://github.com/Cisco-Talos/GhIDA/tree/master/ghida_plugin/idaxml.py,XmlExporter,check_char$353,"def check_char(self, ch):
        """"""
        Replaces a special XML character with an entity string.

        Args:
            ch: String containing the character to check.

        Returns:
            String containing either the character or the entity
            substition string.
        """"""
        if ((ord(ch) < 0x20) and (ord(ch) != 0x09 and
                                  ord(ch) != 0x0A and ord(ch) != 0x0D)):
            return ''
        elif ch == '&':
            return '&amp;'
        elif ch == '<':
            return ""&lt;""
        elif ch == '>':
            return ""&gt;""
        elif ch == '\'':
            return ""&apos;""
        elif ch == '""':
            return ""&quot;""
        elif ch == '\x7F':
            return ''
        elif ord(ch) > 0x7F:
            return '&#x' + format(ord(ch), ""x"") + "";""
        return ch",ord(ch) != 9 and ord(ch) != 10 and (ord(ch) != 13),ord(ch) != 9 and 10 != ord(ch) != 13,Cannot refactor,2,1,,,,,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/ir/inference/test_trt_convert_prelu.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/ir/inference/test_trt_convert_prelu.py,TrtConvertPreluTest,sample_predictor_configs$150,"def sample_predictor_configs(
        self, program_config
    ) -> (paddle_infer.Config, List[int], float):
        def generate_dynamic_shape(attrs):
            if self.dim1 == 0:
                self.dynamic_shape.min_input_shape = {
                    ""input_data"": [1],
                }
                self.dynamic_shape.max_input_shape = {
                    ""input_data"": [4],
                }
                self.dynamic_shape.opt_input_shape = {
                    ""input_data"": [2],
                }
            else:
                if self.dim2 == 0 and self.dim3 == 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3],
                    }
                elif self.dim2 != 0 and self.dim3 != 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1, 1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 3, 16, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3, 16, 32],
                    }
                elif self.dim3 == 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 3, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3, 16],
                    }

        def clear_dynamic_shape():
            self.dynamic_shape.max_input_shape = {}
            self.dynamic_shape.min_input_shape = {}
            self.dynamic_shape.opt_input_shape = {}

        attrs = [
            program_config.ops[i].attrs for i in range(len(program_config.ops))
        ]

        def generate_trt_nodes_num(attrs, dynamic_shape):
            if (
                not dynamic_shape
                and self.dim1 == 0
                and self.dim2 == 0
                and self.dim3 == 0
            ):
                return 0, 3
            return 1, 2

        # for static_shape
        clear_dynamic_shape()
        self.trt_param.precision = paddle_infer.PrecisionType.Float32
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, False
        ), 1e-5
        self.trt_param.precision = paddle_infer.PrecisionType.Half
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, False
        ), (1e-3, 1e-3)

        # for dynamic_shape
        generate_dynamic_shape(attrs)
        self.trt_param.precision = paddle_infer.PrecisionType.Float32
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, True
        ), 1e-5
        self.trt_param.precision = paddle_infer.PrecisionType.Half
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, True
        ), (1e-3, 1e-3)",not dynamic_shape and self.dim1 == 0 and (self.dim2 == 0) and (self.dim3 == 0),not dynamic_shape and self.dim2 == 0 and (self.dim3 == 0 == self.dim1),Cannot refactor,2,1,,,,,
hypothesis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hypothesis/hypothesis-python/tests/cover/test_numerics.py,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/tests/cover/test_numerics.py,,test_fuzz_floats_bounds$39,"def test_fuzz_floats_bounds(data):
    bound = none() | floats(allow_nan=False)
    low, high = data.draw(tuples(bound, bound), label=""low, high"")
    if low is not None and high is not None and low > high:
        low, high = high, low
    exmin = low is not None and low != inf and data.draw(booleans(), label=""ex_min"")
    exmax = high is not None and high != -inf and data.draw(booleans(), label=""ex_max"")
    try:
        val = data.draw(
            floats(low, high, exclude_min=exmin, exclude_max=exmax), label=""value""
        )
        assume(val)  # positive/negative zero is an issue
    except (InvalidArgument, HypothesisDeprecationWarning):
        assert (
            (exmin and exmax and low == next_down(high))
            or (low == high and (exmin or exmax))
            or (
                low == high == 0
                and copysign(1.0, low) == 1
                and copysign(1.0, high) == -1
            )
        )
        reject()  # no floats in required range
    if low is not None:
        assert low <= val
    if high is not None:
        assert val <= high
    if exmin:
        assert low != val
    if exmax:
        assert high != val",low is not None and high is not None and (low > high),high is not None and None is not low > high,Cannot refactor,2,1,,,,,
hypothesis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hypothesis/hypothesis-python/tests/cover/test_numerics.py,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/tests/cover/test_numerics.py,,test_fuzz_floats_bounds$39,"def test_fuzz_floats_bounds(data):
    bound = none() | floats(allow_nan=False)
    low, high = data.draw(tuples(bound, bound), label=""low, high"")
    if low is not None and high is not None and low > high:
        low, high = high, low
    exmin = low is not None and low != inf and data.draw(booleans(), label=""ex_min"")
    exmax = high is not None and high != -inf and data.draw(booleans(), label=""ex_max"")
    try:
        val = data.draw(
            floats(low, high, exclude_min=exmin, exclude_max=exmax), label=""value""
        )
        assume(val)  # positive/negative zero is an issue
    except (InvalidArgument, HypothesisDeprecationWarning):
        assert (
            (exmin and exmax and low == next_down(high))
            or (low == high and (exmin or exmax))
            or (
                low == high == 0
                and copysign(1.0, low) == 1
                and copysign(1.0, high) == -1
            )
        )
        reject()  # no floats in required range
    if low is not None:
        assert low <= val
    if high is not None:
        assert val <= high
    if exmin:
        assert low != val
    if exmax:
        assert high != val",low is not None and high is not None and (low > high),low is not None and None is not high < low,Cannot refactor,2,1,,,,,
saleor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/saleor/saleor/graphql/shipping/mutations/channels.py,https://github.com/saleor/saleor/tree/master/saleor/graphql/shipping/mutations/channels.py,ShippingMethodChannelListingUpdate,clean_input$137,"def clean_input(cls, data, shipping_method, errors):
        cleaned_input = data.get(""add_channels"")
        cls.clean_add_channels(shipping_method, cleaned_input)
        channel_listing_to_update = cls.get_shipping_method_channel_listing_to_update(
            shipping_method.id, cleaned_input
        )
        for channel_input in cleaned_input:
            channel_id = channel_input.get(""channel_id"")
            price_amount = channel_input.pop(""price"", None)
            if price_amount is not None:
                try:
                    validate_price_precision(
                        price_amount, channel_input[""channel""].currency_code
                    )
                    validate_decimal_max_value(price_amount)
                    channel_input[""price_amount""] = price_amount
                except ValidationError as error:
                    error.code = ShippingErrorCode.INVALID.value
                    error.params = {
                        ""channels"": [channel_id],
                    }
                    errors[""price""].append(error)
            else:
                if channel_id not in channel_listing_to_update:
                    errors[""price""].append(
                        ValidationError(
                            ""This field is required."",
                            code=ShippingErrorCode.REQUIRED,
                            params={""channels"": [channel_id]},
                        )
                    )

            min_price = None
            max_price = None
            if ""minimum_order_price"" in channel_input:
                min_price = channel_input.pop(""minimum_order_price"")
                channel_input[""minimum_order_price_amount""] = min_price
            if min_price is not None:
                try:
                    validate_price_precision(
                        min_price, channel_input[""channel""].currency_code
                    )
                    validate_decimal_max_value(min_price)
                except ValidationError as error:
                    error.code = ShippingErrorCode.INVALID.value
                    error.params = {
                        ""channels"": [channel_id],
                    }
                    errors[""minimum_order_price""].append(error)

            if ""maximum_order_price"" in channel_input:
                max_price = channel_input.pop(""maximum_order_price"")
                channel_input[""maximum_order_price_amount""] = max_price
            if max_price is not None:
                try:
                    validate_price_precision(
                        max_price, channel_input[""channel""].currency_code
                    )
                    validate_decimal_max_value(max_price)
                except ValidationError as error:
                    error.code = ShippingErrorCode.INVALID.value
                    error.params = {
                        ""channels"": [channel_id],
                    }
                    errors[""maximum_order_price""].append(error)

            if (
                min_price is not None
                and max_price is not None
                and max_price <= min_price
            ):
                errors[""maximum_order_price""].append(
                    ValidationError(
                        (
                            ""Maximum order price should be larger than ""
                            ""the minimum order price.""
                        ),
                        code=ShippingErrorCode.MAX_LESS_THAN_MIN,
                        params={""channels"": [channel_id]},
                    )
                )

        return data",min_price is not None and max_price is not None and (max_price <= min_price),max_price is not None and None is not min_price >= max_price,Cannot refactor,2,1,,,,,
saleor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/saleor/saleor/graphql/shipping/mutations/channels.py,https://github.com/saleor/saleor/tree/master/saleor/graphql/shipping/mutations/channels.py,ShippingMethodChannelListingUpdate,clean_input$137,"def clean_input(cls, data, shipping_method, errors):
        cleaned_input = data.get(""add_channels"")
        cls.clean_add_channels(shipping_method, cleaned_input)
        channel_listing_to_update = cls.get_shipping_method_channel_listing_to_update(
            shipping_method.id, cleaned_input
        )
        for channel_input in cleaned_input:
            channel_id = channel_input.get(""channel_id"")
            price_amount = channel_input.pop(""price"", None)
            if price_amount is not None:
                try:
                    validate_price_precision(
                        price_amount, channel_input[""channel""].currency_code
                    )
                    validate_decimal_max_value(price_amount)
                    channel_input[""price_amount""] = price_amount
                except ValidationError as error:
                    error.code = ShippingErrorCode.INVALID.value
                    error.params = {
                        ""channels"": [channel_id],
                    }
                    errors[""price""].append(error)
            else:
                if channel_id not in channel_listing_to_update:
                    errors[""price""].append(
                        ValidationError(
                            ""This field is required."",
                            code=ShippingErrorCode.REQUIRED,
                            params={""channels"": [channel_id]},
                        )
                    )

            min_price = None
            max_price = None
            if ""minimum_order_price"" in channel_input:
                min_price = channel_input.pop(""minimum_order_price"")
                channel_input[""minimum_order_price_amount""] = min_price
            if min_price is not None:
                try:
                    validate_price_precision(
                        min_price, channel_input[""channel""].currency_code
                    )
                    validate_decimal_max_value(min_price)
                except ValidationError as error:
                    error.code = ShippingErrorCode.INVALID.value
                    error.params = {
                        ""channels"": [channel_id],
                    }
                    errors[""minimum_order_price""].append(error)

            if ""maximum_order_price"" in channel_input:
                max_price = channel_input.pop(""maximum_order_price"")
                channel_input[""maximum_order_price_amount""] = max_price
            if max_price is not None:
                try:
                    validate_price_precision(
                        max_price, channel_input[""channel""].currency_code
                    )
                    validate_decimal_max_value(max_price)
                except ValidationError as error:
                    error.code = ShippingErrorCode.INVALID.value
                    error.params = {
                        ""channels"": [channel_id],
                    }
                    errors[""maximum_order_price""].append(error)

            if (
                min_price is not None
                and max_price is not None
                and max_price <= min_price
            ):
                errors[""maximum_order_price""].append(
                    ValidationError(
                        (
                            ""Maximum order price should be larger than ""
                            ""the minimum order price.""
                        ),
                        code=ShippingErrorCode.MAX_LESS_THAN_MIN,
                        params={""channels"": [channel_id]},
                    )
                )

        return data",min_price is not None and max_price is not None and (max_price <= min_price),min_price is not None and None is not max_price <= min_price,Cannot refactor,2,1,,,,,
tensorflow-DeepFM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-DeepFM/DeepFM.py,https://github.com/ChenglongChen/tensorflow-DeepFM/tree/master//DeepFM.py,DeepFM,training_termination$327,"def training_termination(self, valid_result):
        if len(valid_result) > 5:
            if self.greater_is_better:
                if valid_result[-1] < valid_result[-2] and \
                    valid_result[-2] < valid_result[-3] and \
                    valid_result[-3] < valid_result[-4] and \
                    valid_result[-4] < valid_result[-5]:
                    return True
            else:
                if valid_result[-1] > valid_result[-2] and \
                    valid_result[-2] > valid_result[-3] and \
                    valid_result[-3] > valid_result[-4] and \
                    valid_result[-4] > valid_result[-5]:
                    return True
        return False",valid_result[-1] < valid_result[-2] and valid_result[-2] < valid_result[-3] and (valid_result[-3] < valid_result[-4]) and (valid_result[-4] < valid_result[-5]),valid_result[-1] < valid_result[-2] and valid_result[-2] < valid_result[-3] < valid_result[-4] and (valid_result[-4] < valid_result[-5]),Cannot refactor,2,1,,,,,
tensorflow-DeepFM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-DeepFM/DeepFM.py,https://github.com/ChenglongChen/tensorflow-DeepFM/tree/master//DeepFM.py,DeepFM,training_termination$327,"def training_termination(self, valid_result):
        if len(valid_result) > 5:
            if self.greater_is_better:
                if valid_result[-1] < valid_result[-2] and \
                    valid_result[-2] < valid_result[-3] and \
                    valid_result[-3] < valid_result[-4] and \
                    valid_result[-4] < valid_result[-5]:
                    return True
            else:
                if valid_result[-1] > valid_result[-2] and \
                    valid_result[-2] > valid_result[-3] and \
                    valid_result[-3] > valid_result[-4] and \
                    valid_result[-4] > valid_result[-5]:
                    return True
        return False",valid_result[-1] > valid_result[-2] and valid_result[-2] > valid_result[-3] and (valid_result[-3] > valid_result[-4]) and (valid_result[-4] > valid_result[-5]),valid_result[-1] > valid_result[-2] and valid_result[-2] > valid_result[-3] > valid_result[-4] and (valid_result[-4] > valid_result[-5]),Cannot refactor,2,1,,,,,
PyGaze,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyGaze/pygaze/_eyetracker/libeyelogic.py,https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libeyelogic.py,EyeLogicTracker,calibrate$391,"def calibrate(self):
        #self.screen.clear()
        #self.screen.draw_text(
        #    text=""Calibrate EyeTracker"",
        #    fontsize=20)
        #self.disp.fill(self.screen)
        #self.disp.show()

        if (not self._recording.is_set()):
            resultTracking = self.api.requestTracking(0)
            if (resultTracking != ELApi.ReturnStart.SUCCESS):
                raise Exception(""unable to start eye tracker"")

        resultCalibrate = self.api.calibrate(0)
        if (resultCalibrate != ELApi.ReturnCalibrate.SUCCESS):
            self.api.unrequestTracking()
            self.errorbeep.play()
            raise Exception(""Calibration failed = {}"".format(errorstringCalibrate(resultCalibrate)))
        self._calibrated.set()

        # NOISE CALIBRATION
        self.screen.clear()
        self.screen.draw_text(
            text=""Noise calibration. Please look at the dot, and press any key to start."",
            fontsize=20, \
            pos=(int(self.dispsize[0]/2),int(self.dispsize[1]*0.3)))
        x = int(float(self.dispsize[0]) / 2.0)
        y = int(float(self.dispsize[1]) / 2.0)
        self.screen.draw_fixation(fixtype=""dot"", pos=(x,y))
        self.disp.fill(self.screen)
        self.disp.show()
        self.kb.get_key(keylist=None, timeout=None, flush=True)

        # wait for a bit, to allow participant to fixate
        clock.pause(500)

        # get distance to screen
        screendist = 0
        i = 0
        while screendist == 0 and i < self.maxtries:
            i = i+1
            self.sampleLock.acquire()
            if (self.lastSample is not None):
                if self.eye_used != 1 and self.lastSample.eyePositionLeftZ != ELInvalidValue:
                    screendist = self.lastSample.eyePositionLeftZ / 10.0 # eyePositionZ is in mm; screendist is in cm
                elif self.eye_used != 0 and self.lastSample.eyePositionRightZ != ELInvalidValue:
                    screendist = self.lastSample.eyePositionRightZ / 10.0
            self.sampleLock.release()
            clock.pause(int(self.sampleTime))
        if i >= self.maxtries:
            self.api.unrequestTracking()
            self.errorbeep.play()
            raise Exception(""unable to receive gaze data for noise calibration"")

        # get samples
        sl = [self.sample()] # samplelist, prefilled with 1 sample to prevent sl[-1] from producing an error; first sample will be ignored for RMS calculation
        t0 = clock.get_time() # starting time
        while clock.get_time() - t0 < 1000:
            s = self.sample() # sample
            if s[0] != -1 and s[1] != -1 and s[0] != ELInvalidValue and s[1] != ELInvalidValue:
                sl.append(s)
            clock.pause(int(self.sampleTime))
        if (len(sl) < 2):
            if (not self._recording.is_set()):
                self.api.unrequestTracking()
            return False

        # calculate RMS noise
        Xvar = []
        Yvar = []
        Xmean = 0.
        Ymean = 0.
        for i in range(2,len(sl)):
            Xvar.append((sl[i][0]-sl[i-1][0])**2)
            Yvar.append((sl[i][1]-sl[i-1][1])**2)
            Xmean += sl[i][0]
            Ymean += sl[i][1]
        XRMS = (sum(Xvar) / len(Xvar))**0.5
        YRMS = (sum(Yvar) / len(Yvar))**0.5
        Xmean = Xmean / (len(sl)-2)
        Ymean = Ymean / (len(sl)-2)
        self.pxdsttresh = (XRMS, YRMS)

        # calculate pixels per cm
        pixpercm = (self.dispsize[0]/float(self.screensize[0]) + self.dispsize[1]/float(self.screensize[1])) / 2

        # get accuracy
        accuracyPxX = abs( Xmean - x )
        accuracyPxY = abs( Ymean - y )
        self.accuracy = ( pix2deg(screendist, accuracyPxX, pixpercm), \
                          pix2deg(screendist, accuracyPxY, pixpercm) )

        # calculate thresholds based on tracker settings
        self.pxfixtresh = deg2pix(screendist, self.fixtresh, pixpercm)
        self.pxaccuracy = (accuracyPxX, accuracyPxY )
        self.pxspdtresh = deg2pix(screendist, self.spdtresh/1000.0, pixpercm) # in pixels per millisecond
        self.pxacctresh = deg2pix(screendist, self.accthresh/1000.0, pixpercm) # in pixels per millisecond**2

        ## log
        self.log(""pygaze calibration"")
        self.log(""accuracy (degrees) = X={}, Y={}"".format( \
            self.accuracy[0], self.accuracy[1] ))
        self.log(""accuracy (in pixels) = X={}, Y={}"".format( \
            self.pxaccuracy[0], self.pxaccuracy[1]))
        self.log(""precision (RMS noise in pixels) = X={}, Y={}"".format( \
            self.pxdsttresh[0], self.pxdsttresh[1]))
        self.log(""distance between participant and display = {} cm"".format(screendist))
        self.log(""fixation threshold = {} pixels"".format(self.pxfixtresh))
        self.log(""speed threshold = {} pixels/ms"".format(self.pxspdtresh))
        self.log(""acceleration threshold = {} pixels/ms**2"".format(self.pxacctresh))
        
        if (not self._recording.is_set()):
            self.api.unrequestTracking()
        return True",s[0] != -1 and s[1] != -1 and (s[0] != ELInvalidValue) and (s[1] != ELInvalidValue),s[1] != -1 and -1 != s[0] != ELInvalidValue and (s[1] != ELInvalidValue),Cannot refactor,2,1,,,,,
PyGaze,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyGaze/pygaze/_eyetracker/libeyelogic.py,https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libeyelogic.py,EyeLogicTracker,calibrate$391,"def calibrate(self):
        #self.screen.clear()
        #self.screen.draw_text(
        #    text=""Calibrate EyeTracker"",
        #    fontsize=20)
        #self.disp.fill(self.screen)
        #self.disp.show()

        if (not self._recording.is_set()):
            resultTracking = self.api.requestTracking(0)
            if (resultTracking != ELApi.ReturnStart.SUCCESS):
                raise Exception(""unable to start eye tracker"")

        resultCalibrate = self.api.calibrate(0)
        if (resultCalibrate != ELApi.ReturnCalibrate.SUCCESS):
            self.api.unrequestTracking()
            self.errorbeep.play()
            raise Exception(""Calibration failed = {}"".format(errorstringCalibrate(resultCalibrate)))
        self._calibrated.set()

        # NOISE CALIBRATION
        self.screen.clear()
        self.screen.draw_text(
            text=""Noise calibration. Please look at the dot, and press any key to start."",
            fontsize=20, \
            pos=(int(self.dispsize[0]/2),int(self.dispsize[1]*0.3)))
        x = int(float(self.dispsize[0]) / 2.0)
        y = int(float(self.dispsize[1]) / 2.0)
        self.screen.draw_fixation(fixtype=""dot"", pos=(x,y))
        self.disp.fill(self.screen)
        self.disp.show()
        self.kb.get_key(keylist=None, timeout=None, flush=True)

        # wait for a bit, to allow participant to fixate
        clock.pause(500)

        # get distance to screen
        screendist = 0
        i = 0
        while screendist == 0 and i < self.maxtries:
            i = i+1
            self.sampleLock.acquire()
            if (self.lastSample is not None):
                if self.eye_used != 1 and self.lastSample.eyePositionLeftZ != ELInvalidValue:
                    screendist = self.lastSample.eyePositionLeftZ / 10.0 # eyePositionZ is in mm; screendist is in cm
                elif self.eye_used != 0 and self.lastSample.eyePositionRightZ != ELInvalidValue:
                    screendist = self.lastSample.eyePositionRightZ / 10.0
            self.sampleLock.release()
            clock.pause(int(self.sampleTime))
        if i >= self.maxtries:
            self.api.unrequestTracking()
            self.errorbeep.play()
            raise Exception(""unable to receive gaze data for noise calibration"")

        # get samples
        sl = [self.sample()] # samplelist, prefilled with 1 sample to prevent sl[-1] from producing an error; first sample will be ignored for RMS calculation
        t0 = clock.get_time() # starting time
        while clock.get_time() - t0 < 1000:
            s = self.sample() # sample
            if s[0] != -1 and s[1] != -1 and s[0] != ELInvalidValue and s[1] != ELInvalidValue:
                sl.append(s)
            clock.pause(int(self.sampleTime))
        if (len(sl) < 2):
            if (not self._recording.is_set()):
                self.api.unrequestTracking()
            return False

        # calculate RMS noise
        Xvar = []
        Yvar = []
        Xmean = 0.
        Ymean = 0.
        for i in range(2,len(sl)):
            Xvar.append((sl[i][0]-sl[i-1][0])**2)
            Yvar.append((sl[i][1]-sl[i-1][1])**2)
            Xmean += sl[i][0]
            Ymean += sl[i][1]
        XRMS = (sum(Xvar) / len(Xvar))**0.5
        YRMS = (sum(Yvar) / len(Yvar))**0.5
        Xmean = Xmean / (len(sl)-2)
        Ymean = Ymean / (len(sl)-2)
        self.pxdsttresh = (XRMS, YRMS)

        # calculate pixels per cm
        pixpercm = (self.dispsize[0]/float(self.screensize[0]) + self.dispsize[1]/float(self.screensize[1])) / 2

        # get accuracy
        accuracyPxX = abs( Xmean - x )
        accuracyPxY = abs( Ymean - y )
        self.accuracy = ( pix2deg(screendist, accuracyPxX, pixpercm), \
                          pix2deg(screendist, accuracyPxY, pixpercm) )

        # calculate thresholds based on tracker settings
        self.pxfixtresh = deg2pix(screendist, self.fixtresh, pixpercm)
        self.pxaccuracy = (accuracyPxX, accuracyPxY )
        self.pxspdtresh = deg2pix(screendist, self.spdtresh/1000.0, pixpercm) # in pixels per millisecond
        self.pxacctresh = deg2pix(screendist, self.accthresh/1000.0, pixpercm) # in pixels per millisecond**2

        ## log
        self.log(""pygaze calibration"")
        self.log(""accuracy (degrees) = X={}, Y={}"".format( \
            self.accuracy[0], self.accuracy[1] ))
        self.log(""accuracy (in pixels) = X={}, Y={}"".format( \
            self.pxaccuracy[0], self.pxaccuracy[1]))
        self.log(""precision (RMS noise in pixels) = X={}, Y={}"".format( \
            self.pxdsttresh[0], self.pxdsttresh[1]))
        self.log(""distance between participant and display = {} cm"".format(screendist))
        self.log(""fixation threshold = {} pixels"".format(self.pxfixtresh))
        self.log(""speed threshold = {} pixels/ms"".format(self.pxspdtresh))
        self.log(""acceleration threshold = {} pixels/ms**2"".format(self.pxacctresh))
        
        if (not self._recording.is_set()):
            self.api.unrequestTracking()
        return True",s[0] != -1 and s[1] != -1 and (s[0] != ELInvalidValue) and (s[1] != ELInvalidValue),s[0] != -1 and s[0] != ELInvalidValue and (-1 != s[1] != ELInvalidValue),Cannot refactor,2,1,,,,,
hypothesis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hypothesis/hypothesis-python/src/hypothesis/strategies/_internal/numbers.py,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/src/hypothesis/strategies/_internal/numbers.py,IntegersStrategy,filter$96,"def filter(self, condition):
        kwargs, pred = get_integer_predicate_bounds(condition)

        start, end = self.start, self.end
        if ""min_value"" in kwargs:
            start = max(kwargs[""min_value""], -math.inf if start is None else start)
        if ""max_value"" in kwargs:
            end = min(kwargs[""max_value""], math.inf if end is None else end)

        if start != self.start or end != self.end:
            if start is not None and end is not None and start > end:
                return nothing()
            self = type(self)(start, end)
        if pred is None:
            return self
        return super().filter(pred)",start is not None and end is not None and (start > end),end is not None and None is not start > end,Cannot refactor,2,1,,,,,
hypothesis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hypothesis/hypothesis-python/src/hypothesis/strategies/_internal/numbers.py,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/src/hypothesis/strategies/_internal/numbers.py,IntegersStrategy,filter$96,"def filter(self, condition):
        kwargs, pred = get_integer_predicate_bounds(condition)

        start, end = self.start, self.end
        if ""min_value"" in kwargs:
            start = max(kwargs[""min_value""], -math.inf if start is None else start)
        if ""max_value"" in kwargs:
            end = min(kwargs[""max_value""], math.inf if end is None else end)

        if start != self.start or end != self.end:
            if start is not None and end is not None and start > end:
                return nothing()
            self = type(self)(start, end)
        if pred is None:
            return self
        return super().filter(pred)",start is not None and end is not None and (start > end),start is not None and None is not end < start,Cannot refactor,2,1,,,,,
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/lxmert/modeling_tf_lxmert.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/lxmert/modeling_tf_lxmert.py,TFLxmertForPreTraining,call$1290,"def call(
        self,
        input_ids=None,
        visual_feats=None,
        visual_pos=None,
        attention_mask=None,
        visual_attention_mask=None,
        token_type_ids=None,
        inputs_embeds=None,
        masked_lm_labels=None,
        obj_labels=None,
        matched_label=None,
        ans=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        training=False,
    ):
        r""""""
        masked_lm_labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the
            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`
        obj_labels: (`Dict[Str: Tuple[tf.Tensor, tf.Tensor]]`, *optional*, defaults to `None`):
            each key is named after each one of the visual losses and each element of the tuple is of the shape
            `(batch_size, num_features)` and `(batch_size, num_features, visual_feature_dim)` for each the label id and
            the label score respectively
        matched_label (`tf.Tensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the whether or not the text input matches the image (classification) loss. Input
            should be a sequence pair (see `input_ids` docstring) Indices should be in `[0, 1]`:

            - 0 indicates that the sentence does not match the image,
            - 1 indicates that the sentence does match the image.
        ans (`Torch.Tensor` of shape `(batch_size)`, *optional*, defaults to `None`):
            a one hot representation hof the correct answer *optional*

        Returns:
        """"""

        lxmert_output = self.lxmert(
            input_ids,
            visual_feats,
            visual_pos,
            attention_mask,
            visual_attention_mask,
            token_type_ids,
            inputs_embeds,
            output_attentions,
            output_hidden_states,
            return_dict,
            training,
        )

        lang_output, visual_output, pooled_output = (
            lxmert_output[0],
            lxmert_output[1],
            lxmert_output[2],
        )
        lang_prediction_scores, cross_relationship_score = self.cls(lang_output, pooled_output)
        if self.task_qa:
            answer_score = self.answer_head(pooled_output)
        else:
            answer_score = pooled_output[0][0]

        total_loss = (
            None
            if (masked_lm_labels is None and matched_label is None and obj_labels is None and ans is None)
            else tf.constant(0.0)
        )
        losses = ()
        if masked_lm_labels is not None and self.task_mask_lm:
            masked_lm_loss = self.loss_fcts[""ce""](
                tf.reshape(masked_lm_labels, [-1]),
                tf.reshape(lang_prediction_scores, [-1, self.config.vocab_size]),
            )
            total_loss += masked_lm_loss
            losses += (masked_lm_loss,)
        if matched_label is not None and self.task_matched:
            matched_loss = self.loss_fcts[""ce""](
                tf.reshape(matched_label, [-1]),
                tf.reshape(cross_relationship_score, [-1, 2]),
            )
            total_loss += matched_loss
            losses += (matched_loss,)
        if obj_labels is not None and self.task_obj_predict:
            total_visn_loss = 0.0
            visn_prediction_scores_dict = self.obj_predict_head(visual_output)
            for key, key_info in self.visual_losses.items():
                label, mask_conf = obj_labels[key]
                output_dim = key_info[""num""]
                loss_fct_name = key_info[""loss""]
                label_shape = key_info[""shape""]
                weight = self.visual_loss_normalizer
                visn_loss_fct = self.loss_fcts[loss_fct_name]
                visn_prediction_scores = visn_prediction_scores_dict[key]
                visn_loss = visn_loss_fct(
                    tf.reshape(label, label_shape),
                    tf.reshape(visn_prediction_scores, [-1, output_dim]),
                )

                if visn_loss.ndim > 1:  # Regression Losses
                    visn_loss = tf.reduce_mean(visn_loss)
                visn_loss = tf.reduce_mean(visn_loss * tf.cast(tf.reshape(mask_conf, [-1]), visn_loss.dtype)) * weight
                total_visn_loss += visn_loss
                losses += (visn_loss,)
            total_loss += total_visn_loss
        if ans is not None and self.task_qa:
            answer_loss = self.loss_fcts[""ce""](
                tf.reshape(ans, [-1]), tf.reshape(answer_score, [-1, self.num_qa_labels])
            )
            # exclude ""*2"" here to match the effect of QA losses.
            # Previous: (loss *0) for 6 epochs, (loss *2) for 6 epochs.   (Used 10 instead of 6 in EMNLP paper)
            # Now     : (loss *1) for 12 epochs
            #
            # * 2       # Multiply by 2 because > half of the data will not have label
            total_loss += answer_loss
            losses += (answer_loss,)
        # return total_loss, tf.stack(losses)[tf.new_axis, ...], answer_score.detach()

        if not return_dict:
            output = (
                lang_prediction_scores,
                cross_relationship_score,
                answer_score,
            ) + lxmert_output[3:]
            return ((total_loss,) + output) if total_loss is not None else output

        return TFLxmertForPreTrainingOutput(
            loss=total_loss,
            prediction_logits=lang_prediction_scores,
            cross_relationship_score=cross_relationship_score,
            question_answering_score=answer_score,
            language_hidden_states=lxmert_output.language_hidden_states,
            vision_hidden_states=lxmert_output.vision_hidden_states,
            language_attentions=lxmert_output.language_attentions,
            vision_attentions=lxmert_output.vision_attentions,
            cross_encoder_attentions=lxmert_output.cross_encoder_attentions,
        )",masked_lm_labels is None and matched_label is None and (obj_labels is None) and (ans is None),matched_label is None and obj_labels is None is masked_lm_labels and (ans is None),Cannot refactor,2,1,,,,,
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/lxmert/modeling_tf_lxmert.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/lxmert/modeling_tf_lxmert.py,TFLxmertForPreTraining,call$1290,"def call(
        self,
        input_ids=None,
        visual_feats=None,
        visual_pos=None,
        attention_mask=None,
        visual_attention_mask=None,
        token_type_ids=None,
        inputs_embeds=None,
        masked_lm_labels=None,
        obj_labels=None,
        matched_label=None,
        ans=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        training=False,
    ):
        r""""""
        masked_lm_labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the
            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`
        obj_labels: (`Dict[Str: Tuple[tf.Tensor, tf.Tensor]]`, *optional*, defaults to `None`):
            each key is named after each one of the visual losses and each element of the tuple is of the shape
            `(batch_size, num_features)` and `(batch_size, num_features, visual_feature_dim)` for each the label id and
            the label score respectively
        matched_label (`tf.Tensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the whether or not the text input matches the image (classification) loss. Input
            should be a sequence pair (see `input_ids` docstring) Indices should be in `[0, 1]`:

            - 0 indicates that the sentence does not match the image,
            - 1 indicates that the sentence does match the image.
        ans (`Torch.Tensor` of shape `(batch_size)`, *optional*, defaults to `None`):
            a one hot representation hof the correct answer *optional*

        Returns:
        """"""

        lxmert_output = self.lxmert(
            input_ids,
            visual_feats,
            visual_pos,
            attention_mask,
            visual_attention_mask,
            token_type_ids,
            inputs_embeds,
            output_attentions,
            output_hidden_states,
            return_dict,
            training,
        )

        lang_output, visual_output, pooled_output = (
            lxmert_output[0],
            lxmert_output[1],
            lxmert_output[2],
        )
        lang_prediction_scores, cross_relationship_score = self.cls(lang_output, pooled_output)
        if self.task_qa:
            answer_score = self.answer_head(pooled_output)
        else:
            answer_score = pooled_output[0][0]

        total_loss = (
            None
            if (masked_lm_labels is None and matched_label is None and obj_labels is None and ans is None)
            else tf.constant(0.0)
        )
        losses = ()
        if masked_lm_labels is not None and self.task_mask_lm:
            masked_lm_loss = self.loss_fcts[""ce""](
                tf.reshape(masked_lm_labels, [-1]),
                tf.reshape(lang_prediction_scores, [-1, self.config.vocab_size]),
            )
            total_loss += masked_lm_loss
            losses += (masked_lm_loss,)
        if matched_label is not None and self.task_matched:
            matched_loss = self.loss_fcts[""ce""](
                tf.reshape(matched_label, [-1]),
                tf.reshape(cross_relationship_score, [-1, 2]),
            )
            total_loss += matched_loss
            losses += (matched_loss,)
        if obj_labels is not None and self.task_obj_predict:
            total_visn_loss = 0.0
            visn_prediction_scores_dict = self.obj_predict_head(visual_output)
            for key, key_info in self.visual_losses.items():
                label, mask_conf = obj_labels[key]
                output_dim = key_info[""num""]
                loss_fct_name = key_info[""loss""]
                label_shape = key_info[""shape""]
                weight = self.visual_loss_normalizer
                visn_loss_fct = self.loss_fcts[loss_fct_name]
                visn_prediction_scores = visn_prediction_scores_dict[key]
                visn_loss = visn_loss_fct(
                    tf.reshape(label, label_shape),
                    tf.reshape(visn_prediction_scores, [-1, output_dim]),
                )

                if visn_loss.ndim > 1:  # Regression Losses
                    visn_loss = tf.reduce_mean(visn_loss)
                visn_loss = tf.reduce_mean(visn_loss * tf.cast(tf.reshape(mask_conf, [-1]), visn_loss.dtype)) * weight
                total_visn_loss += visn_loss
                losses += (visn_loss,)
            total_loss += total_visn_loss
        if ans is not None and self.task_qa:
            answer_loss = self.loss_fcts[""ce""](
                tf.reshape(ans, [-1]), tf.reshape(answer_score, [-1, self.num_qa_labels])
            )
            # exclude ""*2"" here to match the effect of QA losses.
            # Previous: (loss *0) for 6 epochs, (loss *2) for 6 epochs.   (Used 10 instead of 6 in EMNLP paper)
            # Now     : (loss *1) for 12 epochs
            #
            # * 2       # Multiply by 2 because > half of the data will not have label
            total_loss += answer_loss
            losses += (answer_loss,)
        # return total_loss, tf.stack(losses)[tf.new_axis, ...], answer_score.detach()

        if not return_dict:
            output = (
                lang_prediction_scores,
                cross_relationship_score,
                answer_score,
            ) + lxmert_output[3:]
            return ((total_loss,) + output) if total_loss is not None else output

        return TFLxmertForPreTrainingOutput(
            loss=total_loss,
            prediction_logits=lang_prediction_scores,
            cross_relationship_score=cross_relationship_score,
            question_answering_score=answer_score,
            language_hidden_states=lxmert_output.language_hidden_states,
            vision_hidden_states=lxmert_output.vision_hidden_states,
            language_attentions=lxmert_output.language_attentions,
            vision_attentions=lxmert_output.vision_attentions,
            cross_encoder_attentions=lxmert_output.cross_encoder_attentions,
        )",masked_lm_labels is None and matched_label is None and (obj_labels is None) and (ans is None),matched_label is None and obj_labels is None and (ans is None is masked_lm_labels),Cannot refactor,2,1,,,,,
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/lxmert/modeling_tf_lxmert.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/lxmert/modeling_tf_lxmert.py,TFLxmertForPreTraining,call$1290,"def call(
        self,
        input_ids=None,
        visual_feats=None,
        visual_pos=None,
        attention_mask=None,
        visual_attention_mask=None,
        token_type_ids=None,
        inputs_embeds=None,
        masked_lm_labels=None,
        obj_labels=None,
        matched_label=None,
        ans=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        training=False,
    ):
        r""""""
        masked_lm_labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the
            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`
        obj_labels: (`Dict[Str: Tuple[tf.Tensor, tf.Tensor]]`, *optional*, defaults to `None`):
            each key is named after each one of the visual losses and each element of the tuple is of the shape
            `(batch_size, num_features)` and `(batch_size, num_features, visual_feature_dim)` for each the label id and
            the label score respectively
        matched_label (`tf.Tensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the whether or not the text input matches the image (classification) loss. Input
            should be a sequence pair (see `input_ids` docstring) Indices should be in `[0, 1]`:

            - 0 indicates that the sentence does not match the image,
            - 1 indicates that the sentence does match the image.
        ans (`Torch.Tensor` of shape `(batch_size)`, *optional*, defaults to `None`):
            a one hot representation hof the correct answer *optional*

        Returns:
        """"""

        lxmert_output = self.lxmert(
            input_ids,
            visual_feats,
            visual_pos,
            attention_mask,
            visual_attention_mask,
            token_type_ids,
            inputs_embeds,
            output_attentions,
            output_hidden_states,
            return_dict,
            training,
        )

        lang_output, visual_output, pooled_output = (
            lxmert_output[0],
            lxmert_output[1],
            lxmert_output[2],
        )
        lang_prediction_scores, cross_relationship_score = self.cls(lang_output, pooled_output)
        if self.task_qa:
            answer_score = self.answer_head(pooled_output)
        else:
            answer_score = pooled_output[0][0]

        total_loss = (
            None
            if (masked_lm_labels is None and matched_label is None and obj_labels is None and ans is None)
            else tf.constant(0.0)
        )
        losses = ()
        if masked_lm_labels is not None and self.task_mask_lm:
            masked_lm_loss = self.loss_fcts[""ce""](
                tf.reshape(masked_lm_labels, [-1]),
                tf.reshape(lang_prediction_scores, [-1, self.config.vocab_size]),
            )
            total_loss += masked_lm_loss
            losses += (masked_lm_loss,)
        if matched_label is not None and self.task_matched:
            matched_loss = self.loss_fcts[""ce""](
                tf.reshape(matched_label, [-1]),
                tf.reshape(cross_relationship_score, [-1, 2]),
            )
            total_loss += matched_loss
            losses += (matched_loss,)
        if obj_labels is not None and self.task_obj_predict:
            total_visn_loss = 0.0
            visn_prediction_scores_dict = self.obj_predict_head(visual_output)
            for key, key_info in self.visual_losses.items():
                label, mask_conf = obj_labels[key]
                output_dim = key_info[""num""]
                loss_fct_name = key_info[""loss""]
                label_shape = key_info[""shape""]
                weight = self.visual_loss_normalizer
                visn_loss_fct = self.loss_fcts[loss_fct_name]
                visn_prediction_scores = visn_prediction_scores_dict[key]
                visn_loss = visn_loss_fct(
                    tf.reshape(label, label_shape),
                    tf.reshape(visn_prediction_scores, [-1, output_dim]),
                )

                if visn_loss.ndim > 1:  # Regression Losses
                    visn_loss = tf.reduce_mean(visn_loss)
                visn_loss = tf.reduce_mean(visn_loss * tf.cast(tf.reshape(mask_conf, [-1]), visn_loss.dtype)) * weight
                total_visn_loss += visn_loss
                losses += (visn_loss,)
            total_loss += total_visn_loss
        if ans is not None and self.task_qa:
            answer_loss = self.loss_fcts[""ce""](
                tf.reshape(ans, [-1]), tf.reshape(answer_score, [-1, self.num_qa_labels])
            )
            # exclude ""*2"" here to match the effect of QA losses.
            # Previous: (loss *0) for 6 epochs, (loss *2) for 6 epochs.   (Used 10 instead of 6 in EMNLP paper)
            # Now     : (loss *1) for 12 epochs
            #
            # * 2       # Multiply by 2 because > half of the data will not have label
            total_loss += answer_loss
            losses += (answer_loss,)
        # return total_loss, tf.stack(losses)[tf.new_axis, ...], answer_score.detach()

        if not return_dict:
            output = (
                lang_prediction_scores,
                cross_relationship_score,
                answer_score,
            ) + lxmert_output[3:]
            return ((total_loss,) + output) if total_loss is not None else output

        return TFLxmertForPreTrainingOutput(
            loss=total_loss,
            prediction_logits=lang_prediction_scores,
            cross_relationship_score=cross_relationship_score,
            question_answering_score=answer_score,
            language_hidden_states=lxmert_output.language_hidden_states,
            vision_hidden_states=lxmert_output.vision_hidden_states,
            language_attentions=lxmert_output.language_attentions,
            vision_attentions=lxmert_output.vision_attentions,
            cross_encoder_attentions=lxmert_output.cross_encoder_attentions,
        )",masked_lm_labels is None and matched_label is None and (obj_labels is None) and (ans is None),masked_lm_labels is None and obj_labels is None is matched_label and (ans is None),Cannot refactor,2,1,,,,,
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/lxmert/modeling_tf_lxmert.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/lxmert/modeling_tf_lxmert.py,TFLxmertForPreTraining,call$1290,"def call(
        self,
        input_ids=None,
        visual_feats=None,
        visual_pos=None,
        attention_mask=None,
        visual_attention_mask=None,
        token_type_ids=None,
        inputs_embeds=None,
        masked_lm_labels=None,
        obj_labels=None,
        matched_label=None,
        ans=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        training=False,
    ):
        r""""""
        masked_lm_labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the
            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`
        obj_labels: (`Dict[Str: Tuple[tf.Tensor, tf.Tensor]]`, *optional*, defaults to `None`):
            each key is named after each one of the visual losses and each element of the tuple is of the shape
            `(batch_size, num_features)` and `(batch_size, num_features, visual_feature_dim)` for each the label id and
            the label score respectively
        matched_label (`tf.Tensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the whether or not the text input matches the image (classification) loss. Input
            should be a sequence pair (see `input_ids` docstring) Indices should be in `[0, 1]`:

            - 0 indicates that the sentence does not match the image,
            - 1 indicates that the sentence does match the image.
        ans (`Torch.Tensor` of shape `(batch_size)`, *optional*, defaults to `None`):
            a one hot representation hof the correct answer *optional*

        Returns:
        """"""

        lxmert_output = self.lxmert(
            input_ids,
            visual_feats,
            visual_pos,
            attention_mask,
            visual_attention_mask,
            token_type_ids,
            inputs_embeds,
            output_attentions,
            output_hidden_states,
            return_dict,
            training,
        )

        lang_output, visual_output, pooled_output = (
            lxmert_output[0],
            lxmert_output[1],
            lxmert_output[2],
        )
        lang_prediction_scores, cross_relationship_score = self.cls(lang_output, pooled_output)
        if self.task_qa:
            answer_score = self.answer_head(pooled_output)
        else:
            answer_score = pooled_output[0][0]

        total_loss = (
            None
            if (masked_lm_labels is None and matched_label is None and obj_labels is None and ans is None)
            else tf.constant(0.0)
        )
        losses = ()
        if masked_lm_labels is not None and self.task_mask_lm:
            masked_lm_loss = self.loss_fcts[""ce""](
                tf.reshape(masked_lm_labels, [-1]),
                tf.reshape(lang_prediction_scores, [-1, self.config.vocab_size]),
            )
            total_loss += masked_lm_loss
            losses += (masked_lm_loss,)
        if matched_label is not None and self.task_matched:
            matched_loss = self.loss_fcts[""ce""](
                tf.reshape(matched_label, [-1]),
                tf.reshape(cross_relationship_score, [-1, 2]),
            )
            total_loss += matched_loss
            losses += (matched_loss,)
        if obj_labels is not None and self.task_obj_predict:
            total_visn_loss = 0.0
            visn_prediction_scores_dict = self.obj_predict_head(visual_output)
            for key, key_info in self.visual_losses.items():
                label, mask_conf = obj_labels[key]
                output_dim = key_info[""num""]
                loss_fct_name = key_info[""loss""]
                label_shape = key_info[""shape""]
                weight = self.visual_loss_normalizer
                visn_loss_fct = self.loss_fcts[loss_fct_name]
                visn_prediction_scores = visn_prediction_scores_dict[key]
                visn_loss = visn_loss_fct(
                    tf.reshape(label, label_shape),
                    tf.reshape(visn_prediction_scores, [-1, output_dim]),
                )

                if visn_loss.ndim > 1:  # Regression Losses
                    visn_loss = tf.reduce_mean(visn_loss)
                visn_loss = tf.reduce_mean(visn_loss * tf.cast(tf.reshape(mask_conf, [-1]), visn_loss.dtype)) * weight
                total_visn_loss += visn_loss
                losses += (visn_loss,)
            total_loss += total_visn_loss
        if ans is not None and self.task_qa:
            answer_loss = self.loss_fcts[""ce""](
                tf.reshape(ans, [-1]), tf.reshape(answer_score, [-1, self.num_qa_labels])
            )
            # exclude ""*2"" here to match the effect of QA losses.
            # Previous: (loss *0) for 6 epochs, (loss *2) for 6 epochs.   (Used 10 instead of 6 in EMNLP paper)
            # Now     : (loss *1) for 12 epochs
            #
            # * 2       # Multiply by 2 because > half of the data will not have label
            total_loss += answer_loss
            losses += (answer_loss,)
        # return total_loss, tf.stack(losses)[tf.new_axis, ...], answer_score.detach()

        if not return_dict:
            output = (
                lang_prediction_scores,
                cross_relationship_score,
                answer_score,
            ) + lxmert_output[3:]
            return ((total_loss,) + output) if total_loss is not None else output

        return TFLxmertForPreTrainingOutput(
            loss=total_loss,
            prediction_logits=lang_prediction_scores,
            cross_relationship_score=cross_relationship_score,
            question_answering_score=answer_score,
            language_hidden_states=lxmert_output.language_hidden_states,
            vision_hidden_states=lxmert_output.vision_hidden_states,
            language_attentions=lxmert_output.language_attentions,
            vision_attentions=lxmert_output.vision_attentions,
            cross_encoder_attentions=lxmert_output.cross_encoder_attentions,
        )",masked_lm_labels is None and matched_label is None and (obj_labels is None) and (ans is None),masked_lm_labels is None and obj_labels is None and (matched_label is None is ans),Cannot refactor,2,1,,,,,
torch-light,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torch-light/yolo-v3/layer.py,https://github.com/ne7ermore/torch-light/tree/master/yolo-v3/layer.py,BasicPred,build_targets$95,"def build_targets(pred_boxes, pred_conf, pred_cls, target, anchors, num_anchors, num_classes, grid_size, ignore_thres, img_dim):
            """"""
            target - [bsz, max_obj, 5]
            """"""
            nB = target.size(0)
            nA = num_anchors
            nC = num_classes
            nG = grid_size
            mask = torch.zeros(nB, nA, nG, nG)
            conf_mask = torch.ones(nB, nA, nG, nG)
            tx = torch.zeros(nB, nA, nG, nG)
            ty = torch.zeros(nB, nA, nG, nG)
            tw = torch.zeros(nB, nA, nG, nG)
            th = torch.zeros(nB, nA, nG, nG)
            tconf = torch.ByteTensor(nB, nA, nG, nG).fill_(0)
            tcls = torch.ByteTensor(nB, nA, nG, nG, nC).fill_(0)

            nGT = 0
            nCorrect = 0
            for b in range(nB):
                for t in range(target.shape[1]):
                    if target[b, t].sum() == 0:
                        # pad
                        continue
                    nGT += 1
                    # Convert to position relative to box
                    gx = target[b, t, 1] * nG
                    gy = target[b, t, 2] * nG
                    gw = target[b, t, 3] * nG
                    gh = target[b, t, 4] * nG
                    # Get grid box indices
                    gi = int(gx)
                    gj = int(gy)
                    # Get shape of gt box
                    gt_box = torch.FloatTensor(
                        np.array([0, 0, gw, gh])).unsqueeze(0)
                    # Get shape of anchor box
                    anchor_shapes = torch.FloatTensor(np.concatenate(
                        (np.zeros((len(anchors), 2)), np.array(anchors)), 1))

                    # Calculate iou between gt and anchor shapes
                    # 1 on 3
                    anch_ious = bbox_iou(gt_box, anchor_shapes)
                    # Where the overlap is larger than threshold set mask to zero (ignore)
                    conf_mask[b, anch_ious > ignore_thres, gj, gi] = 0
                    # Find the best matching anchor box

                    best_n = np.argmax(anch_ious)
                    # Get ground truth box
                    gt_box = torch.FloatTensor(
                        np.array([gx, gy, gw, gh])).unsqueeze(0)
                    # Get the best prediction
                    pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)
                    # Masks
                    mask[b, best_n, gj, gi] = 1
                    conf_mask[b, best_n, gj, gi] = 1
                    # Coordinates
                    tx[b, best_n, gj, gi] = gx - gi
                    ty[b, best_n, gj, gi] = gy - gj
                    # Width and height
                    tw[b, best_n, gj, gi] = math.log(
                        gw / anchors[best_n][0] + 1e-16)
                    th[b, best_n, gj, gi] = math.log(
                        gh / anchors[best_n][1] + 1e-16)
                    # One-hot encoding of label
                    target_label = int(target[b, t, 0])
                    tcls[b, best_n, gj, gi, target_label] = 1
                    tconf[b, best_n, gj, gi] = 1

                    # Calculate iou between ground truth and best matching prediction
                    iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)
                    pred_label = torch.argmax(pred_cls[b, best_n, gj, gi])
                    score = pred_conf[b, best_n, gj, gi]
                    if iou > 0.5 and pred_label == target_label and score > 0.5:
                        nCorrect += 1

            return nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls",iou > 0.5 and pred_label == target_label and (score > 0.5),pred_label == target_label and score > 0.5 < iou,Cannot refactor,2,1,,,,,
rlkit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rlkit/rlkit/envs/pearl_envs/rand_param_envs/walker2d_rand_params.py,https://github.com/rail-berkeley/rlkit/tree/master/rlkit/envs/pearl_envs/rand_param_envs/walker2d_rand_params.py,Walker2DRandParamsEnv,_step$11,"def _step(self, a):
        # import ipdb; ipdb.set_trace()
        # posbefore = self.model.data.qpos[0, 0]
        posbefore = self.sim.data.qpos[0]
        self.do_simulation(a, self.frame_skip)
        # posafter, height, ang = self.model.data.qpos[0:3, 0]
        posafter, height, ang = self.sim.data.qpos[0:3]
        alive_bonus = 1.0
        reward = ((posafter - posbefore) / self.dt)
        reward += alive_bonus
        reward -= 1e-3 * np.square(a).sum()
        done = not (height > 0.8 and height < 2.0 and
                    ang > -1.0 and ang < 1.0)
        ob = self._get_obs()
        return ob, reward, done, {}",height > 0.8 and height < 2.0 and (ang > -1.0) and (ang < 1.0),height > 0.8 and height < 2.0 and (ang > -1.0) and (-1.0 < ang < 1.0),Cannot refactor,2,1,,,,,
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/unittest/test_tir_transform_compact_buffer_region.py,https://github.com/apache/tvm/tree/master/tests/python/unittest/test_tir_transform_compact_buffer_region.py,,compacted_padding_pattern_func$409,"def compacted_padding_pattern_func(a: T.handle, c: T.handle) -> None:
    A = T.match_buffer(a, [16, 16], dtype=""float32"")
    C = T.match_buffer(c, [20, 20], dtype=""float32"")
    with T.block():
        B = T.alloc_buffer([16, 16], dtype=""float32"")
        for i, j in T.grid(16, 16):
            with T.block():
                B[i, j] = A[i, j]
        for i, j in T.grid(20, 20):
            with T.block():
                C[i, j] = T.if_then_else(
                    2 <= i and i < 18 and 2 <= j and j < 18, B[i - 2, j - 2], 0.0, dtype=""float32""
                )",2 <= i and i < 18 and (2 <= j) and (j < 18),i < 18 and i >= 2 <= j and (j < 18),Cannot refactor,2,1,,,,,
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/unittest/test_tir_transform_compact_buffer_region.py,https://github.com/apache/tvm/tree/master/tests/python/unittest/test_tir_transform_compact_buffer_region.py,,compacted_padding_pattern_func$409,"def compacted_padding_pattern_func(a: T.handle, c: T.handle) -> None:
    A = T.match_buffer(a, [16, 16], dtype=""float32"")
    C = T.match_buffer(c, [20, 20], dtype=""float32"")
    with T.block():
        B = T.alloc_buffer([16, 16], dtype=""float32"")
        for i, j in T.grid(16, 16):
            with T.block():
                B[i, j] = A[i, j]
        for i, j in T.grid(20, 20):
            with T.block():
                C[i, j] = T.if_then_else(
                    2 <= i and i < 18 and 2 <= j and j < 18, B[i - 2, j - 2], 0.0, dtype=""float32""
                )",2 <= i and i < 18 and (2 <= j) and (j < 18),2 <= i and 2 <= j and (j < 18 > i),Cannot refactor,2,1,,,,,
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/unittest/test_tir_transform_compact_buffer_region.py,https://github.com/apache/tvm/tree/master/tests/python/unittest/test_tir_transform_compact_buffer_region.py,,compacted_padding_pattern_func$409,"def compacted_padding_pattern_func(a: T.handle, c: T.handle) -> None:
    A = T.match_buffer(a, [16, 16], dtype=""float32"")
    C = T.match_buffer(c, [20, 20], dtype=""float32"")
    with T.block():
        B = T.alloc_buffer([16, 16], dtype=""float32"")
        for i, j in T.grid(16, 16):
            with T.block():
                B[i, j] = A[i, j]
        for i, j in T.grid(20, 20):
            with T.block():
                C[i, j] = T.if_then_else(
                    2 <= i and i < 18 and 2 <= j and j < 18, B[i - 2, j - 2], 0.0, dtype=""float32""
                )",2 <= i and i < 18 and (2 <= j) and (j < 18),2 <= i and i < 18 and (2 <= j) and (2 <= j < 18),Cannot refactor,2,1,,,,,
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/utils/voc_eval.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/utils/voc_eval.py,,prune_zero_padding$113,"def prune_zero_padding(gt_box, gt_label, difficult=None):
    valid_cnt = 0
    for i in range(len(gt_box)):
        if gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and \
                gt_box[i, 2] == 0 and gt_box[i, 3] == 0:
            break
        valid_cnt += 1
    return (gt_box[:valid_cnt], gt_label[:valid_cnt], difficult[:valid_cnt]
            if difficult is not None else None)","gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and (gt_box[i, 2] == 0) and (gt_box[i, 3] == 0)","gt_box[i, 1] == 0 and gt_box[i, 2] == 0 == gt_box[i, 0] and (gt_box[i, 3] == 0)",Cannot refactor,2,1,,,,,
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/utils/voc_eval.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/utils/voc_eval.py,,prune_zero_padding$113,"def prune_zero_padding(gt_box, gt_label, difficult=None):
    valid_cnt = 0
    for i in range(len(gt_box)):
        if gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and \
                gt_box[i, 2] == 0 and gt_box[i, 3] == 0:
            break
        valid_cnt += 1
    return (gt_box[:valid_cnt], gt_label[:valid_cnt], difficult[:valid_cnt]
            if difficult is not None else None)","gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and (gt_box[i, 2] == 0) and (gt_box[i, 3] == 0)","gt_box[i, 1] == 0 and gt_box[i, 2] == 0 and (gt_box[i, 3] == 0 == gt_box[i, 0])",Cannot refactor,2,1,,,,,
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/utils/voc_eval.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/utils/voc_eval.py,,prune_zero_padding$113,"def prune_zero_padding(gt_box, gt_label, difficult=None):
    valid_cnt = 0
    for i in range(len(gt_box)):
        if gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and \
                gt_box[i, 2] == 0 and gt_box[i, 3] == 0:
            break
        valid_cnt += 1
    return (gt_box[:valid_cnt], gt_label[:valid_cnt], difficult[:valid_cnt]
            if difficult is not None else None)","gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and (gt_box[i, 2] == 0) and (gt_box[i, 3] == 0)","gt_box[i, 0] == 0 and gt_box[i, 2] == 0 == gt_box[i, 1] and (gt_box[i, 3] == 0)",Cannot refactor,2,1,,,,,
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/utils/voc_eval.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/utils/voc_eval.py,,prune_zero_padding$113,"def prune_zero_padding(gt_box, gt_label, difficult=None):
    valid_cnt = 0
    for i in range(len(gt_box)):
        if gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and \
                gt_box[i, 2] == 0 and gt_box[i, 3] == 0:
            break
        valid_cnt += 1
    return (gt_box[:valid_cnt], gt_label[:valid_cnt], difficult[:valid_cnt]
            if difficult is not None else None)","gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and (gt_box[i, 2] == 0) and (gt_box[i, 3] == 0)","gt_box[i, 0] == 0 and gt_box[i, 2] == 0 and (gt_box[i, 3] == 0 == gt_box[i, 1])",Cannot refactor,2,1,,,,,
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/ppdet/utils/voc_eval.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/utils/voc_eval.py,,prune_zero_padding$113,"def prune_zero_padding(gt_box, gt_label, difficult=None):
    valid_cnt = 0
    for i in range(len(gt_box)):
        if gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and \
                gt_box[i, 2] == 0 and gt_box[i, 3] == 0:
            break
        valid_cnt += 1
    return (gt_box[:valid_cnt], gt_label[:valid_cnt], difficult[:valid_cnt]
            if difficult is not None else None)","gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and (gt_box[i, 2] == 0) and (gt_box[i, 3] == 0)","gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and (gt_box[i, 2] == 0) and (gt_box[i, 3] == 0 == gt_box[i, 2])",Cannot refactor,2,1,,,,,
nlg-eval,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlg-eval/nlgeval/pycocoevalcap/cider/cider_scorer.py,https://github.com/Maluuba/nlg-eval/tree/master/nlgeval/pycocoevalcap/cider/cider_scorer.py,CiderScorer,sim$135,"def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):
            '''
            Compute the cosine similarity of two vectors.
            :param vec_hyp: array of dictionary for vector corresponding to hypothesis
            :param vec_ref: array of dictionary for vector corresponding to reference
            :param norm_hyp: array of float for vector corresponding to hypothesis
            :param norm_ref: array of float for vector corresponding to reference
            :param length_hyp: int containing length of hypothesis
            :param length_ref: int containing length of reference
            :return: array of score for each n-grams cosine similarity
            '''
            delta = float(length_hyp - length_ref)
            # measure consine similarity
            val = np.array([0.0 for _ in range(self.n)])
            for n in range(self.n):
                # ngram
                for (ngram,count) in six.iteritems(vec_hyp[n]):
                    # vrama91 : added clipping
                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]

                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):
                    val[n] /= (norm_hyp[n]*norm_ref[n])

                assert(not math.isnan(val[n]))
                # vrama91: added a length based gaussian penalty
                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))
            return val",norm_hyp[n] != 0 and norm_ref[n] != 0,norm_ref[n] != 0 != norm_hyp[n],Cannot refactor,2,1,,,,,
khard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/khard/khard/carddav_object.py,https://github.com/scheibler/khard/tree/master/khard/carddav_object.py,YAMLEditable,_format_date_object$928,"def _format_date_object(date: Optional[Date], localize: bool) -> str:
        if not date:
            return """"
        if isinstance(date, str):
            return date
        if date.year == 1900 and date.month != 0 and date.day != 0 \
                and date.hour == 0 and date.minute == 0 and date.second == 0:
            return date.strftime(""--%m-%d"")
        tz = date.tzname()
        if (tz and tz[3:]) or (date.hour != 0 or date.minute != 0
                               or date.second != 0):
            if localize:
                return date.strftime(locale.nl_langinfo(locale.D_T_FMT))
            utc_offset = -time.timezone / 60 / 60
            return date.strftime(""%FT%T+{:02}:00"".format(int(utc_offset)))
        if localize:
            return date.strftime(locale.nl_langinfo(locale.D_FMT))
        return date.strftime(""%F"")",date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0),date.year == 1900 and date.day != 0 and (date.hour == 0 != date.month) and (date.minute == 0) and (date.second == 0),Cannot refactor,2,1,,,,,
khard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/khard/khard/carddav_object.py,https://github.com/scheibler/khard/tree/master/khard/carddav_object.py,YAMLEditable,_format_date_object$928,"def _format_date_object(date: Optional[Date], localize: bool) -> str:
        if not date:
            return """"
        if isinstance(date, str):
            return date
        if date.year == 1900 and date.month != 0 and date.day != 0 \
                and date.hour == 0 and date.minute == 0 and date.second == 0:
            return date.strftime(""--%m-%d"")
        tz = date.tzname()
        if (tz and tz[3:]) or (date.hour != 0 or date.minute != 0
                               or date.second != 0):
            if localize:
                return date.strftime(locale.nl_langinfo(locale.D_T_FMT))
            utc_offset = -time.timezone / 60 / 60
            return date.strftime(""%FT%T+{:02}:00"".format(int(utc_offset)))
        if localize:
            return date.strftime(locale.nl_langinfo(locale.D_FMT))
        return date.strftime(""%F"")",date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0),date.year == 1900 and date.day != 0 and (date.hour == 0) and (date.minute == 0 != date.month) and (date.second == 0),Cannot refactor,2,1,,,,,
khard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/khard/khard/carddav_object.py,https://github.com/scheibler/khard/tree/master/khard/carddav_object.py,YAMLEditable,_format_date_object$928,"def _format_date_object(date: Optional[Date], localize: bool) -> str:
        if not date:
            return """"
        if isinstance(date, str):
            return date
        if date.year == 1900 and date.month != 0 and date.day != 0 \
                and date.hour == 0 and date.minute == 0 and date.second == 0:
            return date.strftime(""--%m-%d"")
        tz = date.tzname()
        if (tz and tz[3:]) or (date.hour != 0 or date.minute != 0
                               or date.second != 0):
            if localize:
                return date.strftime(locale.nl_langinfo(locale.D_T_FMT))
            utc_offset = -time.timezone / 60 / 60
            return date.strftime(""%FT%T+{:02}:00"".format(int(utc_offset)))
        if localize:
            return date.strftime(locale.nl_langinfo(locale.D_FMT))
        return date.strftime(""%F"")",date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0),date.year == 1900 and date.day != 0 and (date.hour == 0) and (date.minute == 0) and (date.second == 0 != date.month),Cannot refactor,2,1,,,,,
khard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/khard/khard/carddav_object.py,https://github.com/scheibler/khard/tree/master/khard/carddav_object.py,YAMLEditable,_format_date_object$928,"def _format_date_object(date: Optional[Date], localize: bool) -> str:
        if not date:
            return """"
        if isinstance(date, str):
            return date
        if date.year == 1900 and date.month != 0 and date.day != 0 \
                and date.hour == 0 and date.minute == 0 and date.second == 0:
            return date.strftime(""--%m-%d"")
        tz = date.tzname()
        if (tz and tz[3:]) or (date.hour != 0 or date.minute != 0
                               or date.second != 0):
            if localize:
                return date.strftime(locale.nl_langinfo(locale.D_T_FMT))
            utc_offset = -time.timezone / 60 / 60
            return date.strftime(""%FT%T+{:02}:00"".format(int(utc_offset)))
        if localize:
            return date.strftime(locale.nl_langinfo(locale.D_FMT))
        return date.strftime(""%F"")",date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0),date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0 != date.day) and (date.second == 0),Cannot refactor,2,1,,,,,
Sprytile,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Sprytile/sprytile_modal.py,https://github.com/Sprytile/Sprytile/tree/master//sprytile_modal.py,VIEW3D_OP_SprytileModalTool,get_face_tiledata$107,"def get_face_tiledata(bmesh, face):
        grid_id_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_INDEX)
        tile_id_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_TILE_ID)
        if grid_id_layer is None or tile_id_layer is None:
            return None, None, None, None, None

        grid_id = face[grid_id_layer]
        tile_packed_id = face[tile_id_layer]

        width = 1
        width_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_SEL_WIDTH)
        if width_layer is not None:
            width = face[width_layer]
            if width is None:
                width = 1

        height = 1
        height_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_SEL_HEIGHT)
        if height_layer is not None:
            height = face[height_layer]
            if height is None:
                height = 1

        origin = -1
        origin_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_SEL_ORIGIN)
        if origin_layer is not None:
            origin = face[origin_layer]
            if origin is None:
                origin = -1

        # For backwards compatibility. Origin/width/height
        # did not exist before 0.4.2
        if origin == 0 and height == 0 and width == 0:
            origin = tile_packed_id
        height = max(1, height)
        width = max(1, width)

        # print(""get tile data - grid:{0}, tile_id:{1}, w:{2}, h:{3}, o:{4}""
        #       .format(grid_id, tile_packed_id, width, height, origin))
        return grid_id, tile_packed_id, width, height, origin",origin == 0 and height == 0 and (width == 0),height == 0 and width == 0 == origin,Cannot refactor,2,1,,,,,
Sprytile,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Sprytile/sprytile_modal.py,https://github.com/Sprytile/Sprytile/tree/master//sprytile_modal.py,VIEW3D_OP_SprytileModalTool,get_face_tiledata$107,"def get_face_tiledata(bmesh, face):
        grid_id_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_INDEX)
        tile_id_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_TILE_ID)
        if grid_id_layer is None or tile_id_layer is None:
            return None, None, None, None, None

        grid_id = face[grid_id_layer]
        tile_packed_id = face[tile_id_layer]

        width = 1
        width_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_SEL_WIDTH)
        if width_layer is not None:
            width = face[width_layer]
            if width is None:
                width = 1

        height = 1
        height_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_SEL_HEIGHT)
        if height_layer is not None:
            height = face[height_layer]
            if height is None:
                height = 1

        origin = -1
        origin_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_SEL_ORIGIN)
        if origin_layer is not None:
            origin = face[origin_layer]
            if origin is None:
                origin = -1

        # For backwards compatibility. Origin/width/height
        # did not exist before 0.4.2
        if origin == 0 and height == 0 and width == 0:
            origin = tile_packed_id
        height = max(1, height)
        width = max(1, width)

        # print(""get tile data - grid:{0}, tile_id:{1}, w:{2}, h:{3}, o:{4}""
        #       .format(grid_id, tile_packed_id, width, height, origin))
        return grid_id, tile_packed_id, width, height, origin",origin == 0 and height == 0 and (width == 0),origin == 0 and width == 0 == height,Cannot refactor,2,1,,,,,
PINCE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PINCE/PINCE.py,https://github.com/korcankaraokcu/PINCE/tree/master//PINCE.py,ReferencedStringsWidgetForm,__init__$5219,"def __init__(self, parent=None):
        super().__init__()
        self.setupUi(self)
        GuiUtils.fill_value_combobox(self.comboBox_ValueType, type_defs.VALUE_INDEX.INDEX_STRING_UTF8)
        self.parent = lambda: parent
        global instances
        instances.append(self)
        GuiUtils.center_to_parent(self)
        self.setWindowFlags(Qt.Window)
        self.tableWidget_References.setColumnWidth(REF_STR_ADDR_COL, 150)
        self.tableWidget_References.setColumnWidth(REF_STR_COUNT_COL, 80)
        self.splitter.setStretchFactor(0, 1)
        self.listWidget_Referrers.resize(400, self.listWidget_Referrers.height())
        self.hex_len = 16 if GDB_Engine.inferior_arch == type_defs.INFERIOR_ARCH.ARCH_64 else 8
        str_dict, jmp_dict, call_dict = GDB_Engine.get_dissect_code_data()
        str_dict_len, jmp_dict_len, call_dict_len = len(str_dict), len(jmp_dict), len(call_dict)
        str_dict.close()
        jmp_dict.close()
        call_dict.close()
        if str_dict_len == 0 and jmp_dict_len == 0 and call_dict_len == 0:
            confirm_dialog = InputDialogForm(item_list=[(""You need to dissect code first\nProceed?"",)])
            if confirm_dialog.exec_():
                dissect_code_dialog = DissectCodeDialogForm()
                dissect_code_dialog.scan_finished_signal.connect(dissect_code_dialog.accept)
                dissect_code_dialog.exec_()
        self.refresh_table()
        self.tableWidget_References.sortByColumn(REF_STR_ADDR_COL, Qt.AscendingOrder)
        self.tableWidget_References.selectionModel().currentChanged.connect(self.tableWidget_References_current_changed)
        self.listWidget_Referrers.itemDoubleClicked.connect(self.listWidget_Referrers_item_double_clicked)
        self.tableWidget_References.itemDoubleClicked.connect(self.tableWidget_References_item_double_clicked)
        self.tableWidget_References.contextMenuEvent = self.tableWidget_References_context_menu_event
        self.listWidget_Referrers.contextMenuEvent = self.listWidget_Referrers_context_menu_event
        self.pushButton_Search.clicked.connect(self.refresh_table)
        self.comboBox_ValueType.currentIndexChanged.connect(self.refresh_table)
        self.shortcut_search = QShortcut(QKeySequence(""Return""), self)
        self.shortcut_search.activated.connect(self.refresh_table)",str_dict_len == 0 and jmp_dict_len == 0 and (call_dict_len == 0),jmp_dict_len == 0 and call_dict_len == 0 == str_dict_len,Cannot refactor,2,1,,,,,
PINCE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PINCE/PINCE.py,https://github.com/korcankaraokcu/PINCE/tree/master//PINCE.py,ReferencedStringsWidgetForm,__init__$5219,"def __init__(self, parent=None):
        super().__init__()
        self.setupUi(self)
        GuiUtils.fill_value_combobox(self.comboBox_ValueType, type_defs.VALUE_INDEX.INDEX_STRING_UTF8)
        self.parent = lambda: parent
        global instances
        instances.append(self)
        GuiUtils.center_to_parent(self)
        self.setWindowFlags(Qt.Window)
        self.tableWidget_References.setColumnWidth(REF_STR_ADDR_COL, 150)
        self.tableWidget_References.setColumnWidth(REF_STR_COUNT_COL, 80)
        self.splitter.setStretchFactor(0, 1)
        self.listWidget_Referrers.resize(400, self.listWidget_Referrers.height())
        self.hex_len = 16 if GDB_Engine.inferior_arch == type_defs.INFERIOR_ARCH.ARCH_64 else 8
        str_dict, jmp_dict, call_dict = GDB_Engine.get_dissect_code_data()
        str_dict_len, jmp_dict_len, call_dict_len = len(str_dict), len(jmp_dict), len(call_dict)
        str_dict.close()
        jmp_dict.close()
        call_dict.close()
        if str_dict_len == 0 and jmp_dict_len == 0 and call_dict_len == 0:
            confirm_dialog = InputDialogForm(item_list=[(""You need to dissect code first\nProceed?"",)])
            if confirm_dialog.exec_():
                dissect_code_dialog = DissectCodeDialogForm()
                dissect_code_dialog.scan_finished_signal.connect(dissect_code_dialog.accept)
                dissect_code_dialog.exec_()
        self.refresh_table()
        self.tableWidget_References.sortByColumn(REF_STR_ADDR_COL, Qt.AscendingOrder)
        self.tableWidget_References.selectionModel().currentChanged.connect(self.tableWidget_References_current_changed)
        self.listWidget_Referrers.itemDoubleClicked.connect(self.listWidget_Referrers_item_double_clicked)
        self.tableWidget_References.itemDoubleClicked.connect(self.tableWidget_References_item_double_clicked)
        self.tableWidget_References.contextMenuEvent = self.tableWidget_References_context_menu_event
        self.listWidget_Referrers.contextMenuEvent = self.listWidget_Referrers_context_menu_event
        self.pushButton_Search.clicked.connect(self.refresh_table)
        self.comboBox_ValueType.currentIndexChanged.connect(self.refresh_table)
        self.shortcut_search = QShortcut(QKeySequence(""Return""), self)
        self.shortcut_search.activated.connect(self.refresh_table)",str_dict_len == 0 and jmp_dict_len == 0 and (call_dict_len == 0),str_dict_len == 0 and call_dict_len == 0 == jmp_dict_len,Cannot refactor,2,1,,,,,
rasa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/rasa/shared/nlu/training_data/formats/readerwriter.py,https://github.com/RasaHQ/rasa/tree/master/rasa/shared/nlu/training_data/formats/readerwriter.py,TrainingDataWriter,generate_entity_attributes$151,"def generate_entity_attributes(
        text: Text, entity: Dict[Text, Any], short_allowed: bool = True
    ) -> Text:
        """"""Generates text for the entity attributes.

        Args:
            text: The text that is annotated with the entity
            entity: Entity data
            short_allowed: If `True`, allow shorthand annotation with parenthesis

        Returns:
            The annotation text that should follow the given text
        """"""
        entity_text = text
        entity_type = entity.get(ENTITY_ATTRIBUTE_TYPE)
        entity_value = entity.get(ENTITY_ATTRIBUTE_VALUE)
        entity_role = entity.get(ENTITY_ATTRIBUTE_ROLE)
        entity_group = entity.get(ENTITY_ATTRIBUTE_GROUP)

        if entity_value and entity_value == entity_text:
            entity_value = None

        use_short_syntax = (
            short_allowed
            and entity_value is None
            and entity_role is None
            and entity_group is None
        )

        if use_short_syntax:
            return f""({entity_type})""
        else:
            entity_dict = OrderedDict(
                [
                    (ENTITY_ATTRIBUTE_TYPE, entity_type),
                    (ENTITY_ATTRIBUTE_ROLE, entity_role),
                    (ENTITY_ATTRIBUTE_GROUP, entity_group),
                    (ENTITY_ATTRIBUTE_VALUE, entity_value),
                ]
            )
            entity_dict = OrderedDict(
                [(k, v) for k, v in entity_dict.items() if v is not None]
            )

            return f""{json.dumps(entity_dict)}""",short_allowed and entity_value is None and (entity_role is None) and (entity_group is None),short_allowed and entity_role is None and (entity_group is None is entity_value),Cannot refactor,2,1,,,,,
DeepKE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/example/re/document/run.py,https://github.com/zjunlp/DeepKE/tree/master/example/re/document/run.py,,train$17,"def train(args, model, train_features, dev_features, test_features):
    def logging(s, print_=True, log_=True):
        if print_:
            print(s)
        if log_ and args.log_dir != '':
            with open(args.log_dir, 'a+') as f_log:
                f_log.write(s + '\n')
    def finetune(features, optimizer, num_epoch, num_steps, model):
        cur_model = model.module if hasattr(model, 'module') else model
        device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
        if args.train_from_saved_model != '':
            best_score = torch.load(args.train_from_saved_model)[""best_f1""]
            epoch_delta = torch.load(args.train_from_saved_model)[""epoch""] + 1
        else:
            epoch_delta = 0
            best_score = -1
        train_dataloader = DataLoader(features, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)
        train_iterator = [epoch + epoch_delta for epoch in range(num_epoch)]
        total_steps = int(len(train_dataloader) * num_epoch // args.gradient_accumulation_steps)
        warmup_steps = int(total_steps * args.warmup_ratio)
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)
        print(""Total steps: {}"".format(total_steps))
        print(""Warmup steps: {}"".format(warmup_steps))
        global_step = 0
        log_step = 100
        total_loss = 0
        


        #scaler = GradScaler()
        for epoch in train_iterator:
            start_time = time.time()
            optimizer.zero_grad()

            for step, batch in enumerate(train_dataloader):
                model.train()

                inputs = {'input_ids': batch[0].to(device),
                          'attention_mask': batch[1].to(device),
                          'labels': batch[2],
                          'entity_pos': batch[3],
                          'hts': batch[4],
                          }
                #with autocast():
                outputs = model(**inputs)
                loss = outputs[0] / args.gradient_accumulation_steps
                total_loss += loss.item()
                #    scaler.scale(loss).backward()
               

                loss.backward()

                if step % args.gradient_accumulation_steps == 0:
                    #scaler.unscale_(optimizer)
                    if args.max_grad_norm > 0:
                        # torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)
                        torch.nn.utils.clip_grad_norm_(cur_model.parameters(), args.max_grad_norm)
                    #scaler.step(optimizer)
                    #scaler.update()
                    #scheduler.step()
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()
                    global_step += 1
                    num_steps += 1
                    if global_step % log_step == 0:
                        cur_loss = total_loss / log_step
                        elapsed = time.time() - start_time
                        logging(
                            '| epoch {:2d} | step {:4d} | min/b {:5.2f} | lr {} | train loss {:5.3f}'.format(
                                epoch, global_step, elapsed / 60, scheduler.get_last_lr(), cur_loss * 1000))
                        total_loss = 0
                        start_time = time.time()

                        wandb.log({
                            ""train_loss"":cur_loss
                        })

                if (step + 1) == len(train_dataloader) - 1 or (args.evaluation_steps > 0 and num_steps % args.evaluation_steps == 0 and step % args.gradient_accumulation_steps == 0):
                # if step ==0:
                    logging('-' * 89)
                    eval_start_time = time.time()
                    dev_score, dev_output = evaluate(args, model, dev_features, tag=""dev"")

                    logging(
                        '| epoch {:3d} | time: {:5.2f}s | dev_result:{}'.format(epoch, time.time() - eval_start_time,
                                                                                dev_output))

                    wandb.log({
                            ""dev_result"":dev_output
                    })

                    logging('-' * 89)
                    if dev_score > best_score:
                        best_score = dev_score
                        logging(
                            '| epoch {:3d} | best_f1:{}'.format(epoch, best_score))

                        wandb.log({
                            ""best_f1"":best_score
                        })

                        if args.save_path != """":
                            torch.save({
                                'epoch': epoch,
                                'checkpoint': cur_model.state_dict(),
                                'best_f1': best_score,
                                'optimizer': optimizer.state_dict()
                            }, args.save_path
                            , _use_new_zipfile_serialization=False)
                            logging(
                                '| successfully save model at: {}'.format(args.save_path))
                            logging('-' * 89)
        return num_steps

    cur_model = model.module if hasattr(model, 'module') else model
    extract_layer = [""extractor"", ""bilinear""]
    bert_layer = ['bert_model']
    optimizer_grouped_parameters = [
        {""params"": [p for n, p in cur_model.named_parameters() if any(nd in n for nd in bert_layer)], ""lr"": args.bert_lr},
        {""params"": [p for n, p in cur_model.named_parameters() if any(nd in n for nd in extract_layer)], ""lr"": 1e-4},
        {""params"": [p for n, p in cur_model.named_parameters() if not any(nd in n for nd in extract_layer + bert_layer)]},
    ]

    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
    if args.train_from_saved_model != '':
        optimizer.load_state_dict(torch.load(args.train_from_saved_model)[""optimizer""])
        print(""load saved optimizer from {}."".format(args.train_from_saved_model))
    

    num_steps = 0
    set_seed(args)
    model.zero_grad()
    finetune(train_features, optimizer, args.num_train_epochs, num_steps, model)",args.evaluation_steps > 0 and num_steps % args.evaluation_steps == 0 and (step % args.gradient_accumulation_steps == 0),num_steps % args.evaluation_steps == 0 and step % args.gradient_accumulation_steps == 0 < args.evaluation_steps,Cannot refactor,2,1,,,,,
DeepKE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/example/re/document/run.py,https://github.com/zjunlp/DeepKE/tree/master/example/re/document/run.py,,train$17,"def train(args, model, train_features, dev_features, test_features):
    def logging(s, print_=True, log_=True):
        if print_:
            print(s)
        if log_ and args.log_dir != '':
            with open(args.log_dir, 'a+') as f_log:
                f_log.write(s + '\n')
    def finetune(features, optimizer, num_epoch, num_steps, model):
        cur_model = model.module if hasattr(model, 'module') else model
        device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
        if args.train_from_saved_model != '':
            best_score = torch.load(args.train_from_saved_model)[""best_f1""]
            epoch_delta = torch.load(args.train_from_saved_model)[""epoch""] + 1
        else:
            epoch_delta = 0
            best_score = -1
        train_dataloader = DataLoader(features, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)
        train_iterator = [epoch + epoch_delta for epoch in range(num_epoch)]
        total_steps = int(len(train_dataloader) * num_epoch // args.gradient_accumulation_steps)
        warmup_steps = int(total_steps * args.warmup_ratio)
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)
        print(""Total steps: {}"".format(total_steps))
        print(""Warmup steps: {}"".format(warmup_steps))
        global_step = 0
        log_step = 100
        total_loss = 0
        


        #scaler = GradScaler()
        for epoch in train_iterator:
            start_time = time.time()
            optimizer.zero_grad()

            for step, batch in enumerate(train_dataloader):
                model.train()

                inputs = {'input_ids': batch[0].to(device),
                          'attention_mask': batch[1].to(device),
                          'labels': batch[2],
                          'entity_pos': batch[3],
                          'hts': batch[4],
                          }
                #with autocast():
                outputs = model(**inputs)
                loss = outputs[0] / args.gradient_accumulation_steps
                total_loss += loss.item()
                #    scaler.scale(loss).backward()
               

                loss.backward()

                if step % args.gradient_accumulation_steps == 0:
                    #scaler.unscale_(optimizer)
                    if args.max_grad_norm > 0:
                        # torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)
                        torch.nn.utils.clip_grad_norm_(cur_model.parameters(), args.max_grad_norm)
                    #scaler.step(optimizer)
                    #scaler.update()
                    #scheduler.step()
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()
                    global_step += 1
                    num_steps += 1
                    if global_step % log_step == 0:
                        cur_loss = total_loss / log_step
                        elapsed = time.time() - start_time
                        logging(
                            '| epoch {:2d} | step {:4d} | min/b {:5.2f} | lr {} | train loss {:5.3f}'.format(
                                epoch, global_step, elapsed / 60, scheduler.get_last_lr(), cur_loss * 1000))
                        total_loss = 0
                        start_time = time.time()

                        wandb.log({
                            ""train_loss"":cur_loss
                        })

                if (step + 1) == len(train_dataloader) - 1 or (args.evaluation_steps > 0 and num_steps % args.evaluation_steps == 0 and step % args.gradient_accumulation_steps == 0):
                # if step ==0:
                    logging('-' * 89)
                    eval_start_time = time.time()
                    dev_score, dev_output = evaluate(args, model, dev_features, tag=""dev"")

                    logging(
                        '| epoch {:3d} | time: {:5.2f}s | dev_result:{}'.format(epoch, time.time() - eval_start_time,
                                                                                dev_output))

                    wandb.log({
                            ""dev_result"":dev_output
                    })

                    logging('-' * 89)
                    if dev_score > best_score:
                        best_score = dev_score
                        logging(
                            '| epoch {:3d} | best_f1:{}'.format(epoch, best_score))

                        wandb.log({
                            ""best_f1"":best_score
                        })

                        if args.save_path != """":
                            torch.save({
                                'epoch': epoch,
                                'checkpoint': cur_model.state_dict(),
                                'best_f1': best_score,
                                'optimizer': optimizer.state_dict()
                            }, args.save_path
                            , _use_new_zipfile_serialization=False)
                            logging(
                                '| successfully save model at: {}'.format(args.save_path))
                            logging('-' * 89)
        return num_steps

    cur_model = model.module if hasattr(model, 'module') else model
    extract_layer = [""extractor"", ""bilinear""]
    bert_layer = ['bert_model']
    optimizer_grouped_parameters = [
        {""params"": [p for n, p in cur_model.named_parameters() if any(nd in n for nd in bert_layer)], ""lr"": args.bert_lr},
        {""params"": [p for n, p in cur_model.named_parameters() if any(nd in n for nd in extract_layer)], ""lr"": 1e-4},
        {""params"": [p for n, p in cur_model.named_parameters() if not any(nd in n for nd in extract_layer + bert_layer)]},
    ]

    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
    if args.train_from_saved_model != '':
        optimizer.load_state_dict(torch.load(args.train_from_saved_model)[""optimizer""])
        print(""load saved optimizer from {}."".format(args.train_from_saved_model))
    

    num_steps = 0
    set_seed(args)
    model.zero_grad()
    finetune(train_features, optimizer, args.num_train_epochs, num_steps, model)",args.evaluation_steps > 0 and num_steps % args.evaluation_steps == 0 and (step % args.gradient_accumulation_steps == 0),args.evaluation_steps > 0 and step % args.gradient_accumulation_steps == 0 == num_steps % args.evaluation_steps,Cannot refactor,2,1,,,,,
WordOps,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WordOps/wo/cli/plugins/stack_pref.py,https://github.com/WordOps/WordOps/tree/master/wo/cli/plugins/stack_pref.py,,pre_stack$1954,"def pre_stack(self):
    """"""Inital server configuration and tweak""""""
    # remove old sysctl tweak
    if os.path.isfile('/etc/sysctl.d/60-ubuntu-nginx-web-server.conf'):
        WOFileUtils.rm(
            self, '/etc/sysctl.d/60-ubuntu-nginx-web-server.conf')
    # check if version.txt exist
    if os.path.exists('/var/lib/wo/version.txt'):
        with open('/var/lib/wo/version.txt',
                  mode='r', encoding='utf-8') as wo_ver:
            # check version written in version.txt
            wo_check = bool(wo_ver.read().strip() ==
                            '{0}'.format(WOVar.wo_version))
    else:
        wo_check = False
    if wo_check is False:
        # wo sysctl tweaks
        # check system type
        wo_arch = bool((os.uname()[4]) == 'x86_64')
        if os.path.isfile('/proc/1/environ'):
            # detect lxc containers
            wo_lxc = WOFileUtils.grepcheck(
                self, '/proc/1/environ', 'container=lxc')
            # detect wsl
            wo_wsl = WOFileUtils.grepcheck(
                self, '/proc/1/environ', 'wsl')
        else:
            wo_wsl = True
            wo_lxc = True

        if (wo_lxc is not True) and (wo_wsl is not True) and (wo_arch is True):
            data = dict()
            WOTemplate.deploy(
                self, '/etc/sysctl.d/60-wo-tweaks.conf',
                'sysctl.mustache', data, True)
            # use tcp_bbr congestion algorithm only on new kernels
            if (WOVar.wo_platform_codename == 'bionic' or
                WOVar.wo_platform_codename == 'focal' or
                WOVar.wo_platform_codename == 'buster' or
                WOVar.wo_platform_codename == 'jammy' or
                    WOVar.wo_platform_codename == 'bullseye'):
                try:
                    WOShellExec.cmd_exec(
                        self, 'modprobe tcp_bbr')
                    with open(
                        ""/etc/modules-load.d/bbr.conf"",
                            encoding='utf-8', mode='w') as bbr_file:
                        bbr_file.write('tcp_bbr')
                    with open(
                        ""/etc/sysctl.d/60-wo-tweaks.conf"",
                            encoding='utf-8', mode='a') as sysctl_file:
                        sysctl_file.write(
                            '\nnet.ipv4.tcp_congestion_control = bbr'
                            '\nnet.ipv4.tcp_notsent_lowat = 16384')
                except OSError as e:
                    Log.debug(self, str(e))
                    Log.warn(self, ""failed to tweak sysctl"")
            else:
                try:
                    WOShellExec.cmd_exec(
                        self, 'modprobe tcp_htcp')
                    with open(
                        ""/etc/modules-load.d/htcp.conf"",
                            encoding='utf-8', mode='w') as bbr_file:
                        bbr_file.write('tcp_htcp')
                    with open(
                        ""/etc/sysctl.d/60-wo-tweaks.conf"",
                            encoding='utf-8', mode='a') as sysctl_file:
                        sysctl_file.write(
                            '\nnet.ipv4.tcp_congestion_control = htcp')
                except OSError as e:
                    Log.debug(self, str(e))
                    Log.warn(self, ""failed to tweak sysctl"")

            # apply sysctl tweaks
            WOShellExec.cmd_exec(
                self, 'sysctl -eq -p /etc/sysctl.d/60-wo-tweaks.conf')

        # sysctl tweak service
        data = dict()
        if not os.path.isfile('/opt/wo-kernel.sh'):
            WOTemplate.deploy(self, '/opt/wo-kernel.sh',
                              'wo-kernel-script.mustache', data)
        WOFileUtils.chmod(self, '/opt/wo-kernel.sh', 0o700)
        if not os.path.isfile('/lib/systemd/system/wo-kernel.service'):
            WOTemplate.deploy(
                self, '/lib/systemd/system/wo-kernel.service',
                'wo-kernel-service.mustache', data)
            WOShellExec.cmd_exec(self, 'systemctl enable wo-kernel.service')
            WOService.start_service(self, 'wo-kernel')
        # open_files_limit tweak
        if not WOFileUtils.grepcheck(self,
                                     '/etc/security/limits.conf', '500000'):
            with open(""/etc/security/limits.conf"",
                      encoding='utf-8', mode='a') as limit_file:
                limit_file.write(
                    '*         hard    nofile      500000\n'
                    '*         soft    nofile      500000\n'
                    'root      hard    nofile      500000\n'
                    'root      soft    nofile      500000\n')
        # custom motd-news
        data = dict()
        # check if update-motd.d directory exist
        if os.path.isdir('/etc/update-motd.d/'):
            # render custom motd template
            WOTemplate.deploy(
                self, '/etc/update-motd.d/98-wo-update',
                'wo-update.mustache', data)
            WOFileUtils.chmod(
                self, ""/etc/update-motd.d/98-wo-update"", 0o755)
        with open('/var/lib/wo/version.txt',
                  mode='w', encoding='utf-8') as wo_ver:
            wo_ver.write('{0}'.format(WOVar.wo_version))",wo_lxc is not True and wo_wsl is not True and (wo_arch is True),wo_wsl is not True and wo_arch is True is not wo_lxc,Cannot refactor,2,1,,,,,
WordOps,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WordOps/wo/cli/plugins/stack_pref.py,https://github.com/WordOps/WordOps/tree/master/wo/cli/plugins/stack_pref.py,,pre_stack$1954,"def pre_stack(self):
    """"""Inital server configuration and tweak""""""
    # remove old sysctl tweak
    if os.path.isfile('/etc/sysctl.d/60-ubuntu-nginx-web-server.conf'):
        WOFileUtils.rm(
            self, '/etc/sysctl.d/60-ubuntu-nginx-web-server.conf')
    # check if version.txt exist
    if os.path.exists('/var/lib/wo/version.txt'):
        with open('/var/lib/wo/version.txt',
                  mode='r', encoding='utf-8') as wo_ver:
            # check version written in version.txt
            wo_check = bool(wo_ver.read().strip() ==
                            '{0}'.format(WOVar.wo_version))
    else:
        wo_check = False
    if wo_check is False:
        # wo sysctl tweaks
        # check system type
        wo_arch = bool((os.uname()[4]) == 'x86_64')
        if os.path.isfile('/proc/1/environ'):
            # detect lxc containers
            wo_lxc = WOFileUtils.grepcheck(
                self, '/proc/1/environ', 'container=lxc')
            # detect wsl
            wo_wsl = WOFileUtils.grepcheck(
                self, '/proc/1/environ', 'wsl')
        else:
            wo_wsl = True
            wo_lxc = True

        if (wo_lxc is not True) and (wo_wsl is not True) and (wo_arch is True):
            data = dict()
            WOTemplate.deploy(
                self, '/etc/sysctl.d/60-wo-tweaks.conf',
                'sysctl.mustache', data, True)
            # use tcp_bbr congestion algorithm only on new kernels
            if (WOVar.wo_platform_codename == 'bionic' or
                WOVar.wo_platform_codename == 'focal' or
                WOVar.wo_platform_codename == 'buster' or
                WOVar.wo_platform_codename == 'jammy' or
                    WOVar.wo_platform_codename == 'bullseye'):
                try:
                    WOShellExec.cmd_exec(
                        self, 'modprobe tcp_bbr')
                    with open(
                        ""/etc/modules-load.d/bbr.conf"",
                            encoding='utf-8', mode='w') as bbr_file:
                        bbr_file.write('tcp_bbr')
                    with open(
                        ""/etc/sysctl.d/60-wo-tweaks.conf"",
                            encoding='utf-8', mode='a') as sysctl_file:
                        sysctl_file.write(
                            '\nnet.ipv4.tcp_congestion_control = bbr'
                            '\nnet.ipv4.tcp_notsent_lowat = 16384')
                except OSError as e:
                    Log.debug(self, str(e))
                    Log.warn(self, ""failed to tweak sysctl"")
            else:
                try:
                    WOShellExec.cmd_exec(
                        self, 'modprobe tcp_htcp')
                    with open(
                        ""/etc/modules-load.d/htcp.conf"",
                            encoding='utf-8', mode='w') as bbr_file:
                        bbr_file.write('tcp_htcp')
                    with open(
                        ""/etc/sysctl.d/60-wo-tweaks.conf"",
                            encoding='utf-8', mode='a') as sysctl_file:
                        sysctl_file.write(
                            '\nnet.ipv4.tcp_congestion_control = htcp')
                except OSError as e:
                    Log.debug(self, str(e))
                    Log.warn(self, ""failed to tweak sysctl"")

            # apply sysctl tweaks
            WOShellExec.cmd_exec(
                self, 'sysctl -eq -p /etc/sysctl.d/60-wo-tweaks.conf')

        # sysctl tweak service
        data = dict()
        if not os.path.isfile('/opt/wo-kernel.sh'):
            WOTemplate.deploy(self, '/opt/wo-kernel.sh',
                              'wo-kernel-script.mustache', data)
        WOFileUtils.chmod(self, '/opt/wo-kernel.sh', 0o700)
        if not os.path.isfile('/lib/systemd/system/wo-kernel.service'):
            WOTemplate.deploy(
                self, '/lib/systemd/system/wo-kernel.service',
                'wo-kernel-service.mustache', data)
            WOShellExec.cmd_exec(self, 'systemctl enable wo-kernel.service')
            WOService.start_service(self, 'wo-kernel')
        # open_files_limit tweak
        if not WOFileUtils.grepcheck(self,
                                     '/etc/security/limits.conf', '500000'):
            with open(""/etc/security/limits.conf"",
                      encoding='utf-8', mode='a') as limit_file:
                limit_file.write(
                    '*         hard    nofile      500000\n'
                    '*         soft    nofile      500000\n'
                    'root      hard    nofile      500000\n'
                    'root      soft    nofile      500000\n')
        # custom motd-news
        data = dict()
        # check if update-motd.d directory exist
        if os.path.isdir('/etc/update-motd.d/'):
            # render custom motd template
            WOTemplate.deploy(
                self, '/etc/update-motd.d/98-wo-update',
                'wo-update.mustache', data)
            WOFileUtils.chmod(
                self, ""/etc/update-motd.d/98-wo-update"", 0o755)
        with open('/var/lib/wo/version.txt',
                  mode='w', encoding='utf-8') as wo_ver:
            wo_ver.write('{0}'.format(WOVar.wo_version))",wo_lxc is not True and wo_wsl is not True and (wo_arch is True),wo_lxc is not True and wo_wsl is not True is wo_arch,Cannot refactor,2,1,,,,,
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/inference/vmp/nodes/gaussian_markov_chain.py,https://github.com/bayespy/bayespy/tree/master/bayespy/inference/vmp/nodes/gaussian_markov_chain.py,VaryingGaussianMarkovChain,_constructor$1331,"def _constructor(cls, mu, Lambda, B, S, v, n=None, **kwargs):
        """"""
        Constructs distribution and moments objects.

        Compute the dimensions of phi and u.

        The plates and dimensions of the parents should be:
        mu:     (...)                    and D-dimensional
        Lambda: (...)                    and D-dimensional
        B:      (...,D)                  and (D,K)-dimensional
        S:      (...,N-1)                and K-dimensional
        v:      (...,1,D) or (...,N-1,D) and 0-dimensional
        N:      ()                       and 0-dimensional (dummy parent)

        Check that the dimensionalities of the parents are proper.
        """"""

        mu = cls._ensure_moments(mu, GaussianMoments, ndim=1)
        Lambda = cls._ensure_moments(Lambda, WishartMoments, ndim=1)
        B = cls._ensure_moments(B, GaussianMoments, ndim=2)
        S = cls._ensure_moments(S, GaussianMoments, ndim=1)
        v = cls._ensure_moments(v, GammaMoments)

        (D, K) = B.dims[0]

        parent_moments = (
            GaussianMoments((D,)),
            WishartMoments((D,)),
            GaussianMoments((D, K)),
            GaussianMoments((K,)),
            GammaMoments()
        )

        # A dummy wrapper for the number of time instances.
        n_S = 1
        if len(S.plates) >= 1:
            n_S = S.plates[-1]
        n_v = 1
        if len(v.plates) >= 2:
            n_v = v.plates[-2]
        if n_v != n_S and n_v != 1 and n_S != 1:
            raise Exception(
                ""Plates of A and v are giving different number of time ""
                ""instances"")
        n_S = max(n_v, n_S)
        if n is None:
            if n_S == 1:
                raise Exception(
                    ""The number of time instances could not be determined ""
                    ""automatically. Give the number of time instances."")

            n = n_S + 1
        elif n_S != 1 and n_S+1 != n:
            raise Exception(
                ""The number of time instances must match the number of last ""
                ""plates of parents:"" ""%d != %d+1""
                % (n, n_S))

        D = mu.dims[0][0]
        K = B.dims[0][-1]
        M = n #N.get_moments()[0]

        # Check mu
        if mu.dims != ( (D,), (D,D) ):
            raise ValueError(""First parent has wrong dimensionality"")
        # Check Lambda
        if Lambda.dims != ( (D,D), () ):
            raise ValueError(""Second parent has wrong dimensionality"")
        # Check B
        if B.dims != ( (D,K), (D,K,D,K) ):
            raise ValueError(""Third parent has wrong dimensionality {0}. Should be {1}."".format(B.dims[0], (D,K)))
        if len(B.plates) == 0 or B.plates[-1] != D:
            raise ValueError(""Third parent should have a last plate ""
                             ""equal to the dimensionality of the ""
                             ""system."")
        if S.dims != ( (K,), (K,K) ):
            raise ValueError(""Fourth parent has wrong dimensionality %s, ""
                             ""should be %s""
                             % (S.dims, ( (K,), (K,K) )))
        if (len(S.plates) >= 1
            and S.plates[-1] != 1
            and S.plates[-1] != M-1):
            raise ValueError(""The last plate of the fourth ""
                             ""parent should have length equal to one or ""
                             ""N-1, where N is the number of time ""
                             ""instances."")
        # Check v
        if v.dims != ( (), () ):
            raise Exception(""Fifth parent has wrong dimensionality"")
        if len(v.plates) == 0 or v.plates[-1] != D:
            raise Exception(""Fifth parent should have a last plate ""
                            ""equal to the dimensionality of the ""
                            ""system."")
        if (len(v.plates) >= 2
            and v.plates[-2] != 1
            and v.plates[-2] != M-1):
            raise ValueError(""The second last plate of the fifth ""
                             ""parent should have length equal to one or ""
                             ""N-1 where N is the number of time ""
                             ""instances."")

        distribution = VaryingGaussianMarkovChainDistribution(M, D)
        moments = GaussianMarkovChainMoments(M, D)

        parents = [mu, Lambda, B, S, v]

        dims = ( (M,D), (M,D,D), (M-1,D,D) )

        return (parents,
                kwargs,
                dims,
                cls._total_plates(kwargs.get('plates'),
                                  distribution.plates_from_parent(0, mu.plates),
                                  distribution.plates_from_parent(1, Lambda.plates),
                                  distribution.plates_from_parent(2, B.plates),
                                  distribution.plates_from_parent(3, S.plates),
                                  distribution.plates_from_parent(4, v.plates)),
                distribution,
                moments,
                parent_moments)",n_v != n_S and n_v != 1 and (n_S != 1),n_v != 1 and n_v != n_S != 1,Cannot refactor,2,1,,,,,
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/inference/vmp/nodes/gaussian_markov_chain.py,https://github.com/bayespy/bayespy/tree/master/bayespy/inference/vmp/nodes/gaussian_markov_chain.py,VaryingGaussianMarkovChain,_constructor$1331,"def _constructor(cls, mu, Lambda, B, S, v, n=None, **kwargs):
        """"""
        Constructs distribution and moments objects.

        Compute the dimensions of phi and u.

        The plates and dimensions of the parents should be:
        mu:     (...)                    and D-dimensional
        Lambda: (...)                    and D-dimensional
        B:      (...,D)                  and (D,K)-dimensional
        S:      (...,N-1)                and K-dimensional
        v:      (...,1,D) or (...,N-1,D) and 0-dimensional
        N:      ()                       and 0-dimensional (dummy parent)

        Check that the dimensionalities of the parents are proper.
        """"""

        mu = cls._ensure_moments(mu, GaussianMoments, ndim=1)
        Lambda = cls._ensure_moments(Lambda, WishartMoments, ndim=1)
        B = cls._ensure_moments(B, GaussianMoments, ndim=2)
        S = cls._ensure_moments(S, GaussianMoments, ndim=1)
        v = cls._ensure_moments(v, GammaMoments)

        (D, K) = B.dims[0]

        parent_moments = (
            GaussianMoments((D,)),
            WishartMoments((D,)),
            GaussianMoments((D, K)),
            GaussianMoments((K,)),
            GammaMoments()
        )

        # A dummy wrapper for the number of time instances.
        n_S = 1
        if len(S.plates) >= 1:
            n_S = S.plates[-1]
        n_v = 1
        if len(v.plates) >= 2:
            n_v = v.plates[-2]
        if n_v != n_S and n_v != 1 and n_S != 1:
            raise Exception(
                ""Plates of A and v are giving different number of time ""
                ""instances"")
        n_S = max(n_v, n_S)
        if n is None:
            if n_S == 1:
                raise Exception(
                    ""The number of time instances could not be determined ""
                    ""automatically. Give the number of time instances."")

            n = n_S + 1
        elif n_S != 1 and n_S+1 != n:
            raise Exception(
                ""The number of time instances must match the number of last ""
                ""plates of parents:"" ""%d != %d+1""
                % (n, n_S))

        D = mu.dims[0][0]
        K = B.dims[0][-1]
        M = n #N.get_moments()[0]

        # Check mu
        if mu.dims != ( (D,), (D,D) ):
            raise ValueError(""First parent has wrong dimensionality"")
        # Check Lambda
        if Lambda.dims != ( (D,D), () ):
            raise ValueError(""Second parent has wrong dimensionality"")
        # Check B
        if B.dims != ( (D,K), (D,K,D,K) ):
            raise ValueError(""Third parent has wrong dimensionality {0}. Should be {1}."".format(B.dims[0], (D,K)))
        if len(B.plates) == 0 or B.plates[-1] != D:
            raise ValueError(""Third parent should have a last plate ""
                             ""equal to the dimensionality of the ""
                             ""system."")
        if S.dims != ( (K,), (K,K) ):
            raise ValueError(""Fourth parent has wrong dimensionality %s, ""
                             ""should be %s""
                             % (S.dims, ( (K,), (K,K) )))
        if (len(S.plates) >= 1
            and S.plates[-1] != 1
            and S.plates[-1] != M-1):
            raise ValueError(""The last plate of the fourth ""
                             ""parent should have length equal to one or ""
                             ""N-1, where N is the number of time ""
                             ""instances."")
        # Check v
        if v.dims != ( (), () ):
            raise Exception(""Fifth parent has wrong dimensionality"")
        if len(v.plates) == 0 or v.plates[-1] != D:
            raise Exception(""Fifth parent should have a last plate ""
                            ""equal to the dimensionality of the ""
                            ""system."")
        if (len(v.plates) >= 2
            and v.plates[-2] != 1
            and v.plates[-2] != M-1):
            raise ValueError(""The second last plate of the fifth ""
                             ""parent should have length equal to one or ""
                             ""N-1 where N is the number of time ""
                             ""instances."")

        distribution = VaryingGaussianMarkovChainDistribution(M, D)
        moments = GaussianMarkovChainMoments(M, D)

        parents = [mu, Lambda, B, S, v]

        dims = ( (M,D), (M,D,D), (M-1,D,D) )

        return (parents,
                kwargs,
                dims,
                cls._total_plates(kwargs.get('plates'),
                                  distribution.plates_from_parent(0, mu.plates),
                                  distribution.plates_from_parent(1, Lambda.plates),
                                  distribution.plates_from_parent(2, B.plates),
                                  distribution.plates_from_parent(3, S.plates),
                                  distribution.plates_from_parent(4, v.plates)),
                distribution,
                moments,
                parent_moments)",n_v != n_S and n_v != 1 and (n_S != 1),n_v != n_S and n_S != 1 != n_v,Cannot refactor,2,1,,,,,
bayespy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bayespy/bayespy/inference/vmp/nodes/gaussian_markov_chain.py,https://github.com/bayespy/bayespy/tree/master/bayespy/inference/vmp/nodes/gaussian_markov_chain.py,VaryingGaussianMarkovChain,_constructor$1331,"def _constructor(cls, mu, Lambda, B, S, v, n=None, **kwargs):
        """"""
        Constructs distribution and moments objects.

        Compute the dimensions of phi and u.

        The plates and dimensions of the parents should be:
        mu:     (...)                    and D-dimensional
        Lambda: (...)                    and D-dimensional
        B:      (...,D)                  and (D,K)-dimensional
        S:      (...,N-1)                and K-dimensional
        v:      (...,1,D) or (...,N-1,D) and 0-dimensional
        N:      ()                       and 0-dimensional (dummy parent)

        Check that the dimensionalities of the parents are proper.
        """"""

        mu = cls._ensure_moments(mu, GaussianMoments, ndim=1)
        Lambda = cls._ensure_moments(Lambda, WishartMoments, ndim=1)
        B = cls._ensure_moments(B, GaussianMoments, ndim=2)
        S = cls._ensure_moments(S, GaussianMoments, ndim=1)
        v = cls._ensure_moments(v, GammaMoments)

        (D, K) = B.dims[0]

        parent_moments = (
            GaussianMoments((D,)),
            WishartMoments((D,)),
            GaussianMoments((D, K)),
            GaussianMoments((K,)),
            GammaMoments()
        )

        # A dummy wrapper for the number of time instances.
        n_S = 1
        if len(S.plates) >= 1:
            n_S = S.plates[-1]
        n_v = 1
        if len(v.plates) >= 2:
            n_v = v.plates[-2]
        if n_v != n_S and n_v != 1 and n_S != 1:
            raise Exception(
                ""Plates of A and v are giving different number of time ""
                ""instances"")
        n_S = max(n_v, n_S)
        if n is None:
            if n_S == 1:
                raise Exception(
                    ""The number of time instances could not be determined ""
                    ""automatically. Give the number of time instances."")

            n = n_S + 1
        elif n_S != 1 and n_S+1 != n:
            raise Exception(
                ""The number of time instances must match the number of last ""
                ""plates of parents:"" ""%d != %d+1""
                % (n, n_S))

        D = mu.dims[0][0]
        K = B.dims[0][-1]
        M = n #N.get_moments()[0]

        # Check mu
        if mu.dims != ( (D,), (D,D) ):
            raise ValueError(""First parent has wrong dimensionality"")
        # Check Lambda
        if Lambda.dims != ( (D,D), () ):
            raise ValueError(""Second parent has wrong dimensionality"")
        # Check B
        if B.dims != ( (D,K), (D,K,D,K) ):
            raise ValueError(""Third parent has wrong dimensionality {0}. Should be {1}."".format(B.dims[0], (D,K)))
        if len(B.plates) == 0 or B.plates[-1] != D:
            raise ValueError(""Third parent should have a last plate ""
                             ""equal to the dimensionality of the ""
                             ""system."")
        if S.dims != ( (K,), (K,K) ):
            raise ValueError(""Fourth parent has wrong dimensionality %s, ""
                             ""should be %s""
                             % (S.dims, ( (K,), (K,K) )))
        if (len(S.plates) >= 1
            and S.plates[-1] != 1
            and S.plates[-1] != M-1):
            raise ValueError(""The last plate of the fourth ""
                             ""parent should have length equal to one or ""
                             ""N-1, where N is the number of time ""
                             ""instances."")
        # Check v
        if v.dims != ( (), () ):
            raise Exception(""Fifth parent has wrong dimensionality"")
        if len(v.plates) == 0 or v.plates[-1] != D:
            raise Exception(""Fifth parent should have a last plate ""
                            ""equal to the dimensionality of the ""
                            ""system."")
        if (len(v.plates) >= 2
            and v.plates[-2] != 1
            and v.plates[-2] != M-1):
            raise ValueError(""The second last plate of the fifth ""
                             ""parent should have length equal to one or ""
                             ""N-1 where N is the number of time ""
                             ""instances."")

        distribution = VaryingGaussianMarkovChainDistribution(M, D)
        moments = GaussianMarkovChainMoments(M, D)

        parents = [mu, Lambda, B, S, v]

        dims = ( (M,D), (M,D,D), (M-1,D,D) )

        return (parents,
                kwargs,
                dims,
                cls._total_plates(kwargs.get('plates'),
                                  distribution.plates_from_parent(0, mu.plates),
                                  distribution.plates_from_parent(1, Lambda.plates),
                                  distribution.plates_from_parent(2, B.plates),
                                  distribution.plates_from_parent(3, S.plates),
                                  distribution.plates_from_parent(4, v.plates)),
                distribution,
                moments,
                parent_moments)",len(S.plates) >= 1 and S.plates[-1] != 1 and (S.plates[-1] != M - 1),len(S.plates) >= 1 and 1 != S.plates[-1] != M - 1,Cannot refactor,2,1,,,,,
model-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/model-analysis/tensorflow_model_analysis/metrics/confusion_matrix_metrics.py,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/metrics/confusion_matrix_metrics.py,DiagnosticOddsRatio,result$2067,"def result(self, tp: float, tn: float, fp: float, fn: float) -> float:
    if fn > 0.0 and fp > 0.0 and tn > 0.0:
      return (tp / fn) / (fp / tn)
    else:
      return float('nan')",fn > 0.0 and fp > 0.0 and (tn > 0.0),fp > 0.0 and tn > 0.0 < fn,Cannot refactor,2,1,,,,,
model-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/model-analysis/tensorflow_model_analysis/metrics/confusion_matrix_metrics.py,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/metrics/confusion_matrix_metrics.py,DiagnosticOddsRatio,result$2067,"def result(self, tp: float, tn: float, fp: float, fn: float) -> float:
    if fn > 0.0 and fp > 0.0 and tn > 0.0:
      return (tp / fn) / (fp / tn)
    else:
      return float('nan')",fn > 0.0 and fp > 0.0 and (tn > 0.0),fn > 0.0 and tn > 0.0 < fp,Cannot refactor,2,1,,,,,
coding-interview-gym,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coding-interview-gym/leetcode.com/python/490_The_Maze.py,https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/490_The_Maze.py,Solution,hasPath$4,"def hasPath(self, maze, start, destination):
        """"""
        :type maze: List[List[int]]
        :type start: List[int]
        :type destination: List[int]
        :rtype: bool
        """"""
        if not maze or not start or not destination:
            # bad input
            return False
        if start == destination:
            # input start and destination were the same
            return True

        q = deque([(start[0], start[1])])
        # using a deque is important when used as a queue
        # stack here would be DFS
        visited = set()
        # a set will provide constant time access, we will never have duplicates
        directions_to_go = [(0, 1), (0, -1), (1, 0), (-1, 0)]
        # we can always roll north, south, east, or west

        while q:
            current = q.popleft()
            # first in first out (swap to pop here with stack for DFS)
            if current[0] == destination[0] and current[1] == destination[1]:
                return True
            for direction in directions_to_go:
                # move in a direction
                x = current[0] + direction[0]
                y = current[1] + direction[1]
                while 0 <= x < len(maze) and 0 <= y < len(maze[0]) and maze[x][y] == 0:
                    # keep moving until ONE PAST where you can't move (roll) anymore
                    x += direction[0]
                    y += direction[1]
                # roll back one so that you're actually where you should be
                rolled_to_x = x - direction[0]
                rolled_to_y = y - direction[1]
                if (rolled_to_x, rolled_to_y) not in visited:
                    visited.add((rolled_to_x, rolled_to_y))
                    # add this position to be searched from as well
                    q.append((rolled_to_x, rolled_to_y))
        # if you're here no solution was found and everything has been visited
        return False",0 <= x < len(maze) and 0 <= y < len(maze[0]) and (maze[x][y] == 0),0 <= x < len(maze) and len(maze[0]) > y >= 0 == maze[x][y],Cannot refactor,2,1,,,,,
ParlAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/select_sentences_tfidf.py,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/select_sentences_tfidf.py,,select_pars$23,"def select_pars(qa_dct, docs_list, word_freqs, n_sents=100, n_context=3):
    question = qa_dct['title'][0]
    split_docs = [sentence_split(doc['text'][0], max_len=64) for doc in docs_list]
    q_ti_dct = tf_idf_vec(question, word_freqs['title'][0], word_freqs['title'][1])
    split_docs_pre = [
        (i, j, sen, tf_idf_vec(sen, word_freqs['doc'][0], word_freqs['doc'][1]))
        for i, doc in enumerate(split_docs)
        for j, sen in enumerate(doc)
    ]
    split_docs_sc = [
        (i, j, tf_idf_dist(q_ti_dct, dct))
        for k, (i, j, sen, dct) in enumerate(split_docs_pre)
        if len(sen.split()) >= 4 and sen not in [x[2] for x in split_docs_pre[:k]]
    ]
    split_docs_sort = sorted(split_docs_sc, key=lambda x: x[-1], reverse=True)[:n_sents]
    select_ids = sorted([(i, j) for i, j, _ in split_docs_sort])
    par_ids = []
    this_par = []
    last_seen = (-1, -1)
    for i, j in select_ids:
        if i > last_seen[0]:
            par_ids += [this_par]
            this_par = []
            for k in range(-n_context, n_context + 1):
                if j + k >= 0 and j + k < len(split_docs[i]):
                    this_par += [(i, j + k)]
                    last_seen = (i, j + k)
        else:
            if j - n_context > last_seen[1] + 1:
                par_ids += [this_par]
                this_par = []
            for k in range(-n_context, n_context + 1):
                if j + k > last_seen[1] and j + k >= 0 and j + k < len(split_docs[i]):
                    this_par += [(i, j + k)]
                    last_seen = (i, j + k)
    par_ids = par_ids[1:] + [this_par]
    extract_doc = ' <P> '.join(
        [''] + [' '.join([split_docs[i][j] for i, j in par]) for par in par_ids]
    ).strip()
    return extract_doc",j + k > last_seen[1] and j + k >= 0 and (j + k < len(split_docs[i])),j + k >= 0 and last_seen[1] < j + k < len(split_docs[i]),Cannot refactor,2,1,,,,,
ParlAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/select_sentences_tfidf.py,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/select_sentences_tfidf.py,,select_pars$23,"def select_pars(qa_dct, docs_list, word_freqs, n_sents=100, n_context=3):
    question = qa_dct['title'][0]
    split_docs = [sentence_split(doc['text'][0], max_len=64) for doc in docs_list]
    q_ti_dct = tf_idf_vec(question, word_freqs['title'][0], word_freqs['title'][1])
    split_docs_pre = [
        (i, j, sen, tf_idf_vec(sen, word_freqs['doc'][0], word_freqs['doc'][1]))
        for i, doc in enumerate(split_docs)
        for j, sen in enumerate(doc)
    ]
    split_docs_sc = [
        (i, j, tf_idf_dist(q_ti_dct, dct))
        for k, (i, j, sen, dct) in enumerate(split_docs_pre)
        if len(sen.split()) >= 4 and sen not in [x[2] for x in split_docs_pre[:k]]
    ]
    split_docs_sort = sorted(split_docs_sc, key=lambda x: x[-1], reverse=True)[:n_sents]
    select_ids = sorted([(i, j) for i, j, _ in split_docs_sort])
    par_ids = []
    this_par = []
    last_seen = (-1, -1)
    for i, j in select_ids:
        if i > last_seen[0]:
            par_ids += [this_par]
            this_par = []
            for k in range(-n_context, n_context + 1):
                if j + k >= 0 and j + k < len(split_docs[i]):
                    this_par += [(i, j + k)]
                    last_seen = (i, j + k)
        else:
            if j - n_context > last_seen[1] + 1:
                par_ids += [this_par]
                this_par = []
            for k in range(-n_context, n_context + 1):
                if j + k > last_seen[1] and j + k >= 0 and j + k < len(split_docs[i]):
                    this_par += [(i, j + k)]
                    last_seen = (i, j + k)
    par_ids = par_ids[1:] + [this_par]
    extract_doc = ' <P> '.join(
        [''] + [' '.join([split_docs[i][j] for i, j in par]) for par in par_ids]
    ).strip()
    return extract_doc",j + k > last_seen[1] and j + k >= 0 and (j + k < len(split_docs[i])),j + k > last_seen[1] and 0 <= j + k < len(split_docs[i]),Cannot refactor,2,1,,,,,
dagster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/dagster-graphql/dagster_graphql/cli.py,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-graphql/dagster_graphql/cli.py,,ui$183,"def ui(text, file, predefined, variables, remote, output, ephemeral_instance, **kwargs):
    query = None
    if text is not None and file is None and predefined is None:
        query = text.strip(""'\"" \n\t"")
    elif file is not None and text is None and predefined is None:
        query = file.read()
    elif predefined is not None and text is None and file is None:
        query = PREDEFINED_QUERIES[predefined]
    else:
        raise click.UsageError(
            ""Must select one and only one of text (-t), file (-f), or predefined (-p) ""
            ""to select GraphQL document to execute.""
        )

    if remote:
        res = execute_query_against_remote(remote, query, variables)
        print(res)  # pylint: disable=print-call
    else:
        instance = DagsterInstance.ephemeral() if ephemeral_instance else DagsterInstance.get()
        with get_workspace_process_context_from_kwargs(
            instance, version=__version__, read_only=False, kwargs=kwargs
        ) as workspace_process_context:
            execute_query_from_cli(
                workspace_process_context,
                query,
                variables,
                output,
            )",text is not None and file is None and (predefined is None),file is None and text is not None is predefined,Cannot refactor,2,1,,,,,
dagster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/dagster-graphql/dagster_graphql/cli.py,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-graphql/dagster_graphql/cli.py,,ui$183,"def ui(text, file, predefined, variables, remote, output, ephemeral_instance, **kwargs):
    query = None
    if text is not None and file is None and predefined is None:
        query = text.strip(""'\"" \n\t"")
    elif file is not None and text is None and predefined is None:
        query = file.read()
    elif predefined is not None and text is None and file is None:
        query = PREDEFINED_QUERIES[predefined]
    else:
        raise click.UsageError(
            ""Must select one and only one of text (-t), file (-f), or predefined (-p) ""
            ""to select GraphQL document to execute.""
        )

    if remote:
        res = execute_query_against_remote(remote, query, variables)
        print(res)  # pylint: disable=print-call
    else:
        instance = DagsterInstance.ephemeral() if ephemeral_instance else DagsterInstance.get()
        with get_workspace_process_context_from_kwargs(
            instance, version=__version__, read_only=False, kwargs=kwargs
        ) as workspace_process_context:
            execute_query_from_cli(
                workspace_process_context,
                query,
                variables,
                output,
            )",text is not None and file is None and (predefined is None),text is not None and predefined is None is file,Cannot refactor,2,1,,,,,
dagster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/dagster-graphql/dagster_graphql/cli.py,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-graphql/dagster_graphql/cli.py,,ui$183,"def ui(text, file, predefined, variables, remote, output, ephemeral_instance, **kwargs):
    query = None
    if text is not None and file is None and predefined is None:
        query = text.strip(""'\"" \n\t"")
    elif file is not None and text is None and predefined is None:
        query = file.read()
    elif predefined is not None and text is None and file is None:
        query = PREDEFINED_QUERIES[predefined]
    else:
        raise click.UsageError(
            ""Must select one and only one of text (-t), file (-f), or predefined (-p) ""
            ""to select GraphQL document to execute.""
        )

    if remote:
        res = execute_query_against_remote(remote, query, variables)
        print(res)  # pylint: disable=print-call
    else:
        instance = DagsterInstance.ephemeral() if ephemeral_instance else DagsterInstance.get()
        with get_workspace_process_context_from_kwargs(
            instance, version=__version__, read_only=False, kwargs=kwargs
        ) as workspace_process_context:
            execute_query_from_cli(
                workspace_process_context,
                query,
                variables,
                output,
            )",file is not None and text is None and (predefined is None),text is None and file is not None is predefined,Cannot refactor,2,1,,,,,
dagster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/dagster-graphql/dagster_graphql/cli.py,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-graphql/dagster_graphql/cli.py,,ui$183,"def ui(text, file, predefined, variables, remote, output, ephemeral_instance, **kwargs):
    query = None
    if text is not None and file is None and predefined is None:
        query = text.strip(""'\"" \n\t"")
    elif file is not None and text is None and predefined is None:
        query = file.read()
    elif predefined is not None and text is None and file is None:
        query = PREDEFINED_QUERIES[predefined]
    else:
        raise click.UsageError(
            ""Must select one and only one of text (-t), file (-f), or predefined (-p) ""
            ""to select GraphQL document to execute.""
        )

    if remote:
        res = execute_query_against_remote(remote, query, variables)
        print(res)  # pylint: disable=print-call
    else:
        instance = DagsterInstance.ephemeral() if ephemeral_instance else DagsterInstance.get()
        with get_workspace_process_context_from_kwargs(
            instance, version=__version__, read_only=False, kwargs=kwargs
        ) as workspace_process_context:
            execute_query_from_cli(
                workspace_process_context,
                query,
                variables,
                output,
            )",file is not None and text is None and (predefined is None),file is not None and predefined is None is text,Cannot refactor,2,1,,,,,
dagster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/dagster-graphql/dagster_graphql/cli.py,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-graphql/dagster_graphql/cli.py,,ui$183,"def ui(text, file, predefined, variables, remote, output, ephemeral_instance, **kwargs):
    query = None
    if text is not None and file is None and predefined is None:
        query = text.strip(""'\"" \n\t"")
    elif file is not None and text is None and predefined is None:
        query = file.read()
    elif predefined is not None and text is None and file is None:
        query = PREDEFINED_QUERIES[predefined]
    else:
        raise click.UsageError(
            ""Must select one and only one of text (-t), file (-f), or predefined (-p) ""
            ""to select GraphQL document to execute.""
        )

    if remote:
        res = execute_query_against_remote(remote, query, variables)
        print(res)  # pylint: disable=print-call
    else:
        instance = DagsterInstance.ephemeral() if ephemeral_instance else DagsterInstance.get()
        with get_workspace_process_context_from_kwargs(
            instance, version=__version__, read_only=False, kwargs=kwargs
        ) as workspace_process_context:
            execute_query_from_cli(
                workspace_process_context,
                query,
                variables,
                output,
            )",predefined is not None and text is None and (file is None),text is None and file is None is not predefined,Cannot refactor,2,1,,,,,
dagster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/dagster-graphql/dagster_graphql/cli.py,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-graphql/dagster_graphql/cli.py,,ui$183,"def ui(text, file, predefined, variables, remote, output, ephemeral_instance, **kwargs):
    query = None
    if text is not None and file is None and predefined is None:
        query = text.strip(""'\"" \n\t"")
    elif file is not None and text is None and predefined is None:
        query = file.read()
    elif predefined is not None and text is None and file is None:
        query = PREDEFINED_QUERIES[predefined]
    else:
        raise click.UsageError(
            ""Must select one and only one of text (-t), file (-f), or predefined (-p) ""
            ""to select GraphQL document to execute.""
        )

    if remote:
        res = execute_query_against_remote(remote, query, variables)
        print(res)  # pylint: disable=print-call
    else:
        instance = DagsterInstance.ephemeral() if ephemeral_instance else DagsterInstance.get()
        with get_workspace_process_context_from_kwargs(
            instance, version=__version__, read_only=False, kwargs=kwargs
        ) as workspace_process_context:
            execute_query_from_cli(
                workspace_process_context,
                query,
                variables,
                output,
            )",predefined is not None and text is None and (file is None),predefined is not None and text is None is file,Cannot refactor,2,1,,,,,
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/stats/_hypotests.py,https://github.com/scipy/scipy/tree/master/scipy/stats/_hypotests.py,TukeyHSDResult,confidence_interval$1462,"def confidence_interval(self, confidence_level=.95):
        """"""Compute the confidence interval for the specified confidence level.

        Parameters
        ----------
        confidence_level : float, optional
            Confidence level for the computed confidence interval
            of the estimated proportion. Default is .95.

        Returns
        -------
        ci : ``ConfidenceInterval`` object
            The object has attributes ``low`` and ``high`` that hold the
            lower and upper bounds of the confidence intervals for each
            comparison. The high and low values are accessible for each
            comparison at index ``(i, j)`` between groups ``i`` and ``j``.

        References
        ----------
        .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, ""7.4.7.1.
               Tukey's Method.""
               https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,
               28 November 2020.

        Examples
        --------
        >>> from scipy.stats import tukey_hsd
        >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]
        >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]
        >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]
        >>> result = tukey_hsd(group0, group1, group2)
        >>> ci = result.confidence_interval()
        >>> ci.low
        array([[-3.649159, -8.249159, -3.909159],
               [ 0.950841, -3.649159,  0.690841],
               [-3.389159, -7.989159, -3.649159]])
        >>> ci.high
        array([[ 3.649159, -0.950841,  3.389159],
               [ 8.249159,  3.649159,  7.989159],
               [ 3.909159, -0.690841,  3.649159]])
        """"""
        # check to see if the supplied confidence level matches that of the
        # previously computed CI.
        if (self._ci is not None and self._ci_cl is not None and
                confidence_level == self._ci_cl):
            return self._ci

        if not 0 < confidence_level < 1:
            raise ValueError(""Confidence level must be between 0 and 1."")
        # determine the critical value of the studentized range using the
        # appropriate confidence level, number of treatments, and degrees
        # of freedom as determined by the number of data less the number of
        # treatments. (""Confidence limits for Tukey's method"")[1]. Note that
        # in the cases of unequal sample sizes there will be a criterion for
        # each group comparison.
        params = (confidence_level, self._nobs, self._ntreatments - self._nobs)
        srd = distributions.studentized_range.ppf(*params)
        # also called maximum critical value, the Tukey criterion is the
        # studentized range critical value * the square root of mean square
        # error over the sample size.
        tukey_criterion = srd * self._stand_err
        # the confidence levels are determined by the
        # `mean_differences` +- `tukey_criterion`
        upper_conf = self.statistic + tukey_criterion
        lower_conf = self.statistic - tukey_criterion
        self._ci = ConfidenceInterval(low=lower_conf, high=upper_conf)
        self._ci_cl = confidence_level
        return self._ci",self._ci is not None and self._ci_cl is not None and (confidence_level == self._ci_cl),self._ci is not None and None is not self._ci_cl == confidence_level,Cannot refactor,2,1,,,,,
MotionPlanning,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MotionPlanning/CurvesGenerator/reeds_shepp.py,https://github.com/zhm-real/MotionPlanning/tree/master/CurvesGenerator/reeds_shepp.py,,LRSR$313,"def LRSR(x, y, phi):
    xi = x + math.sin(phi)
    eta = y - 1.0 - math.cos(phi)
    rho, theta = R(-eta, xi)

    if rho >= 2.0:
        t = theta
        u = 2.0 - rho
        v = M(t + 0.5 * PI - phi)
        if t >= 0.0 and u <= 0.0 and v <= 0.0:
            return True, t, u, v

    return False, 0.0, 0.0, 0.0",t >= 0.0 and u <= 0.0 and (v <= 0.0),u <= 0.0 and v <= 0.0 <= t,Cannot refactor,2,1,,,,,
MotionPlanning,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MotionPlanning/CurvesGenerator/reeds_shepp.py,https://github.com/zhm-real/MotionPlanning/tree/master/CurvesGenerator/reeds_shepp.py,,LRSR$313,"def LRSR(x, y, phi):
    xi = x + math.sin(phi)
    eta = y - 1.0 - math.cos(phi)
    rho, theta = R(-eta, xi)

    if rho >= 2.0:
        t = theta
        u = 2.0 - rho
        v = M(t + 0.5 * PI - phi)
        if t >= 0.0 and u <= 0.0 and v <= 0.0:
            return True, t, u, v

    return False, 0.0, 0.0, 0.0",t >= 0.0 and u <= 0.0 and (v <= 0.0),t >= 0.0 and v <= 0.0 >= u,Cannot refactor,2,1,,,,,
no_find,,,,,,,,,,,,,,,
PyBoy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyBoy/pyboy/plugins/debug.py,https://github.com/Baekalfen/PyBoy/tree/master/pyboy/plugins/debug.py,BaseDebugWindow,mark_tile$426,"def mark_tile(self, x, y, color, height, width, grid):
        tw = width # Tile width
        th = height # Tile height
        if grid:
            xx = x - (x%tw)
            yy = y - (y%th)
        else:
            xx = x
            yy = y
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx < self.width:
                self.buf0[yy + i][xx] = color
        for i in range(tw):
            if 0 <= (yy) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy][xx + i] = color
        for i in range(tw):
            if 0 <= (yy + th - 1) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy + th - 1][xx + i] = color
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx + tw - 1 < self.width:
                self.buf0[yy + i][xx + tw - 1] = color",0 <= yy + i < self.height and 0 <= xx < self.width,self.height > yy + i >= 0 <= xx < self.width,0,,,,,,,
PyBoy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyBoy/pyboy/plugins/debug.py,https://github.com/Baekalfen/PyBoy/tree/master/pyboy/plugins/debug.py,BaseDebugWindow,mark_tile$426,"def mark_tile(self, x, y, color, height, width, grid):
        tw = width # Tile width
        th = height # Tile height
        if grid:
            xx = x - (x%tw)
            yy = y - (y%th)
        else:
            xx = x
            yy = y
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx < self.width:
                self.buf0[yy + i][xx] = color
        for i in range(tw):
            if 0 <= (yy) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy][xx + i] = color
        for i in range(tw):
            if 0 <= (yy + th - 1) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy + th - 1][xx + i] = color
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx + tw - 1 < self.width:
                self.buf0[yy + i][xx + tw - 1] = color",0 <= yy < self.height and 0 <= xx + i < self.width,self.height > yy >= 0 <= xx + i < self.width,0,,,,,,,
PyBoy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyBoy/pyboy/plugins/debug.py,https://github.com/Baekalfen/PyBoy/tree/master/pyboy/plugins/debug.py,BaseDebugWindow,mark_tile$426,"def mark_tile(self, x, y, color, height, width, grid):
        tw = width # Tile width
        th = height # Tile height
        if grid:
            xx = x - (x%tw)
            yy = y - (y%th)
        else:
            xx = x
            yy = y
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx < self.width:
                self.buf0[yy + i][xx] = color
        for i in range(tw):
            if 0 <= (yy) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy][xx + i] = color
        for i in range(tw):
            if 0 <= (yy + th - 1) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy + th - 1][xx + i] = color
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx + tw - 1 < self.width:
                self.buf0[yy + i][xx + tw - 1] = color",0 <= yy + i < self.height and 0 <= xx + tw - 1 < self.width,self.height > yy + i >= 0 <= xx + tw - 1 < self.width,0,,,,,,,
SMAC3,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMAC3/smac/runhistory/runhistory.py,https://github.com/automl/SMAC3/tree/master/smac/runhistory/runhistory.py,RunHistory,add$191,"def add(
        self,
        config: Configuration,
        cost: float,
        time: float,
        status: StatusType,
        instance_id: typing.Optional[str] = None,
        seed: typing.Optional[int] = None,
        budget: float = 0.0,
        starttime: float = 0.0,
        endtime: float = 0.0,
        additional_info: typing.Optional[typing.Dict] = None,
        origin: DataOrigin = DataOrigin.INTERNAL,
        force_update: bool = False,
    ) -> None:
        """"""Adds a data of a new target algorithm (TA) run;
        it will update data if the same key values are used
        (config, instance_id, seed)

        Parameters
        ----------
            config : dict (or other type -- depending on config space module)
                Parameter configuration
            cost: float
                Cost of TA run (will be minimized)
            time: float
                Runtime of TA run
            status: str
                Status in {SUCCESS, TIMEOUT, CRASHED, ABORT, MEMOUT}
            instance_id: str
                String representing an instance (default: None)
            seed: int
                Random seed used by TA (default: None)
            budget: float
                budget (cutoff) used in intensifier to limit TA (default: 0)
            starttime: float
                starting timestamp of TA evaluation
            endtime: float
                ending timestamp of TA evaluation
            additional_info: dict
                Additional run infos (could include further returned
                information from TA or fields such as start time and host_id)
            origin: DataOrigin
                Defines how data will be used.
            force_update: bool (default: False)
                Forces the addition of a config to the history
        """"""

        if config is None:
            raise TypeError('Configuration to add to the runhistory must not be None')
        elif not isinstance(config, Configuration):
            raise TypeError(
                'Configuration to add to the runhistory is not of type Configuration, but %s' % type(config)
            )

        # Get the config id
        config_id_tmp = self.config_ids.get(config)
        if config_id_tmp is None:
            self._n_id += 1
            self.config_ids[config] = self._n_id
            config_id = typing.cast(int, self.config_ids.get(config))
            self.ids_config[self._n_id] = config
        else:
            config_id = typing.cast(int, config_id_tmp)

        k = RunKey(config_id, instance_id, seed, budget)
        v = RunValue(cost, time, status, starttime, endtime, additional_info)
        # Construct keys and values for the data dictionary
        for key, value in (
            ('config', config.get_dictionary()),
            ('config_id', config_id),
            ('instance_id', instance_id),
            ('seed', seed),
            ('budget', budget),
            ('cost', cost),
            ('time', time),
            ('status', status),
            ('starttime', starttime),
            ('endtime', endtime),
            ('additional_info', additional_info),
            ('origin', config.origin),
        ):
            self._check_json_serializable(key, value, EnumEncoder, k, v)

        # Each runkey is supposed to be used only once. Repeated tries to add
        # the same runkey will be ignored silently if not capped.
        if self.overwrite_existing_runs or force_update or self.data.get(k) is None:
            self._add(k, v, status, origin)
        elif status != StatusType.CAPPED and self.data[k].status == StatusType.CAPPED:
            # overwrite capped runs with uncapped runs
            self._add(k, v, status, origin)
        elif status == StatusType.CAPPED and self.data[k].status == StatusType.CAPPED and cost > self.data[k].cost:
            # overwrite if censored with a larger cutoff
            self._add(k, v, status, origin)",status == StatusType.CAPPED and self.data[k].status == StatusType.CAPPED and (cost > self.data[k].cost),status == StatusType.CAPPED == self.data[k].status and cost > self.data[k].cost,0,,,,,,,
open_model_zoo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open_model_zoo/tools/accuracy_checker/openvino/tools/accuracy_checker/adapters/pose_estimation_associative_embedding.py,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/adapters/pose_estimation_associative_embedding.py,AssociativeEmbeddingDecoder,refine$274,"def refine(heatmap, tag, keypoints, pose_tag=None):
        K, H, W = heatmap.shape
        if len(tag.shape) == 3:
            tag = tag[..., None]

        if pose_tag is not None:
            prev_tag = pose_tag
        else:
            tags = []
            for i in range(K):
                if keypoints[i, 2] > 0:
                    x, y = keypoints[i][:2].astype(int)
                    tags.append(tag[i, y, x])
            prev_tag = np.mean(tags, axis=0)

        for i, (_heatmap, _tag) in enumerate(zip(heatmap, tag)):
            if keypoints[i, 2] > 0:
                continue
            # Get position with the closest tag value to the pose tag.
            diff = np.abs(_tag[..., 0] - prev_tag) + 0.5
            diff = diff.astype(np.int32).astype(_heatmap.dtype)
            diff -= _heatmap
            idx = diff.argmin()
            y, x = np.divmod(idx, _heatmap.shape[-1])
            # Corresponding keypoint detection score.
            val = _heatmap[y, x]
            if val > 0:
                keypoints[i, :3] = x, y, val
                if 1 < x < W - 1 and 1 < y < H - 1:
                    diff = np.array([
                        _heatmap[y, x + 1] - _heatmap[y, x - 1],
                        _heatmap[y + 1, x] - _heatmap[y - 1, x]
                    ])
                    keypoints[i, :2] += np.sign(diff) * .25

        return keypoints",1 < x < W - 1 and 1 < y < H - 1,W - 1 > x > 1 < y < H - 1,0,,,,,,,
rpaframework,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rpaframework/packages/main/src/RPA/Desktop/Windows.py,https://github.com/robocorp/rpaframework/tree/master/packages/main/src/RPA/Desktop/Windows.py,Windows,get_element_rich_text$1132,"def get_element_rich_text(self, locator: str) -> Any:
        """"""Get value of element `rich text` attribute.

        :param locator: element locator
        :return: `rich_text` value if found, else False

        Example:

        .. code-block:: robotframework

            ${text}  Get Element Rich Text  CalculatorResults

        """"""
        element = self.get_element(locator)
        if element is not False and ""rich_text"" in element:
            return element[""rich_text""]
        elif element is False:
            self.logger.info(""Did not find element with locator: %s"", locator)
            return False
        else:
            self.logger.info(
                ""Element for locator %s does not have 'rich_text' attribute"", locator
            )
            return False",element is not False and 'rich_text' in element,'rich_text' in element is not False,0,,,,,,,
scipy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/special/tests/test_hyp2f1.py,https://github.com/scipy/scipy/tree/master/scipy/special/tests/test_hyp2f1.py,TestHyp2f1,test_a_b_negative_int$390,"def test_a_b_negative_int(self, hyp2f1_test_case):
        a, b, c, z, expected, rtol = hyp2f1_test_case
        assert a == int(a) and a < 0 or b == int(b) and b < 0  # Tests the test
        assert_allclose(hyp2f1(a, b, c, z), expected, rtol=rtol)",b == int(b) and b < 0,int(b) == b < 0,0,,,,,,,
torch-light,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torch-light/yolo-v3/layer.py,https://github.com/ne7ermore/torch-light/tree/master/yolo-v3/layer.py,BasicPred,build_targets$95,"def build_targets(pred_boxes, pred_conf, pred_cls, target, anchors, num_anchors, num_classes, grid_size, ignore_thres, img_dim):
            """"""
            target - [bsz, max_obj, 5]
            """"""
            nB = target.size(0)
            nA = num_anchors
            nC = num_classes
            nG = grid_size
            mask = torch.zeros(nB, nA, nG, nG)
            conf_mask = torch.ones(nB, nA, nG, nG)
            tx = torch.zeros(nB, nA, nG, nG)
            ty = torch.zeros(nB, nA, nG, nG)
            tw = torch.zeros(nB, nA, nG, nG)
            th = torch.zeros(nB, nA, nG, nG)
            tconf = torch.ByteTensor(nB, nA, nG, nG).fill_(0)
            tcls = torch.ByteTensor(nB, nA, nG, nG, nC).fill_(0)

            nGT = 0
            nCorrect = 0
            for b in range(nB):
                for t in range(target.shape[1]):
                    if target[b, t].sum() == 0:
                        # pad
                        continue
                    nGT += 1
                    # Convert to position relative to box
                    gx = target[b, t, 1] * nG
                    gy = target[b, t, 2] * nG
                    gw = target[b, t, 3] * nG
                    gh = target[b, t, 4] * nG
                    # Get grid box indices
                    gi = int(gx)
                    gj = int(gy)
                    # Get shape of gt box
                    gt_box = torch.FloatTensor(
                        np.array([0, 0, gw, gh])).unsqueeze(0)
                    # Get shape of anchor box
                    anchor_shapes = torch.FloatTensor(np.concatenate(
                        (np.zeros((len(anchors), 2)), np.array(anchors)), 1))

                    # Calculate iou between gt and anchor shapes
                    # 1 on 3
                    anch_ious = bbox_iou(gt_box, anchor_shapes)
                    # Where the overlap is larger than threshold set mask to zero (ignore)
                    conf_mask[b, anch_ious > ignore_thres, gj, gi] = 0
                    # Find the best matching anchor box

                    best_n = np.argmax(anch_ious)
                    # Get ground truth box
                    gt_box = torch.FloatTensor(
                        np.array([gx, gy, gw, gh])).unsqueeze(0)
                    # Get the best prediction
                    pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)
                    # Masks
                    mask[b, best_n, gj, gi] = 1
                    conf_mask[b, best_n, gj, gi] = 1
                    # Coordinates
                    tx[b, best_n, gj, gi] = gx - gi
                    ty[b, best_n, gj, gi] = gy - gj
                    # Width and height
                    tw[b, best_n, gj, gi] = math.log(
                        gw / anchors[best_n][0] + 1e-16)
                    th[b, best_n, gj, gi] = math.log(
                        gh / anchors[best_n][1] + 1e-16)
                    # One-hot encoding of label
                    target_label = int(target[b, t, 0])
                    tcls[b, best_n, gj, gi, target_label] = 1
                    tconf[b, best_n, gj, gi] = 1

                    # Calculate iou between ground truth and best matching prediction
                    iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)
                    pred_label = torch.argmax(pred_cls[b, best_n, gj, gi])
                    score = pred_conf[b, best_n, gj, gi]
                    if iou > 0.5 and pred_label == target_label and score > 0.5:
                        nCorrect += 1

            return nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls",iou > 0.5 < score and pred_label == target_label and (score > 0.5),iou > 0.5 < score > 0.5 and pred_label == target_label,0,,,,,,,
metaworld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/metaworld/metaworld/envs/mujoco/sawyer_xyz/v1/sawyer_assembly_peg.py,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/sawyer_xyz/v1/sawyer_assembly_peg.py,SawyerNutAssemblyEnv,compute_reward$105,"def compute_reward(self, actions, obs):
        graspPos = obs[3:6]
        objPos = self.get_body_com('RoundNut')

        rightFinger, leftFinger = self._get_site_pos('rightEndEffector'), self._get_site_pos('leftEndEffector')
        fingerCOM  =  (rightFinger + leftFinger)/2

        heightTarget = self.heightTarget
        placingGoal = self._target_pos

        reachDist = np.linalg.norm(graspPos - fingerCOM)

        placingDist = np.linalg.norm(objPos[:2] - placingGoal[:2])
        placingDistFinal = np.abs(objPos[-1] - self.objHeight)

        def reachReward():
            reachRew = -reachDist
            reachDistxy = np.linalg.norm(graspPos[:-1] - fingerCOM[:-1])
            zRew = np.linalg.norm(fingerCOM[-1] - self.init_fingerCOM[-1])
            if reachDistxy < 0.04:
                reachRew = -reachDist
            else:
                reachRew =  -reachDistxy - zRew

            # incentive to close fingers when reachDist is small
            if reachDist < 0.04:
                reachRew = -reachDist + max(actions[-1],0)/50
            return reachRew, reachDist

        def pickCompletionCriteria():
            tolerance = 0.01
            if objPos[2] >= (heightTarget - tolerance) and reachDist < 0.03:
                return True
            else:
                return False

        if pickCompletionCriteria():
            self.pickCompleted = True

        def objDropped():
            return (objPos[2] < (self.objHeight + 0.005)) and (placingDist >0.02) and (reachDist > 0.02)

        def placeCompletionCriteria():
            if abs(objPos[0] - placingGoal[0]) < 0.03 and \
                abs(objPos[1] - placingGoal[1]) < 0.03:
                return True
            else:
                return False

        if placeCompletionCriteria():
            self.placeCompleted = True
        else:
            self.placeCompleted = False

        def orig_pickReward():
            hScale = 100
            if self.placeCompleted or (self.pickCompleted and not(objDropped())):
                return hScale*heightTarget
            elif (reachDist < 0.04) and (objPos[2]> (self.objHeight + 0.005)) :
                return hScale* min(heightTarget, objPos[2])
            else:
                return 0

        def placeRewardMove():
            c1 = 1000 ; c2 = 0.01 ; c3 = 0.001
            placeRew = 1000*(self.maxPlacingDist - placingDist) + c1*(np.exp(-(placingDist**2)/c2) + np.exp(-(placingDist**2)/c3))
            if self.placeCompleted:
                c4 = 2000; c5 = 0.003; c6 = 0.0003
                placeRew += 2000*(heightTarget - placingDistFinal) + c4*(np.exp(-(placingDistFinal**2)/c5) + np.exp(-(placingDistFinal**2)/c6))
            placeRew = max(placeRew,0)
            cond = self.placeCompleted or (self.pickCompleted and (reachDist < 0.04) and not(objDropped()))
            if cond:
                return [placeRew, placingDist, placingDistFinal]
            else:
                return [0, placingDist, placingDistFinal]

        reachRew, reachDist = reachReward()
        pickRew = orig_pickReward()
        placeRew , placingDist, placingDistFinal = placeRewardMove()
        assert ((placeRew >=0) and (pickRew>=0))
        reward = reachRew + pickRew + placeRew
        success = (abs(objPos[0] - placingGoal[0]) < 0.03 and abs(objPos[1] - placingGoal[1]) < 0.03 and placingDistFinal <= 0.04)
        return [reward, reachRew, reachDist, pickRew, placeRew, placingDist, placingDistFinal, success]",abs(objPos[0] - placingGoal[0]) < 0.03 and abs(objPos[1] - placingGoal[1]) < 0.03 and (placingDistFinal <= 0.04),abs(objPos[0] - placingGoal[0]) < 0.03 > abs(objPos[1] - placingGoal[1]) and placingDistFinal <= 0.04,0,,,,,,,
metaworld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/metaworld/metaworld/envs/mujoco/sawyer_xyz/v1/sawyer_assembly_peg.py,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/sawyer_xyz/v1/sawyer_assembly_peg.py,SawyerNutAssemblyEnv,compute_reward$105,"def compute_reward(self, actions, obs):
        graspPos = obs[3:6]
        objPos = self.get_body_com('RoundNut')

        rightFinger, leftFinger = self._get_site_pos('rightEndEffector'), self._get_site_pos('leftEndEffector')
        fingerCOM  =  (rightFinger + leftFinger)/2

        heightTarget = self.heightTarget
        placingGoal = self._target_pos

        reachDist = np.linalg.norm(graspPos - fingerCOM)

        placingDist = np.linalg.norm(objPos[:2] - placingGoal[:2])
        placingDistFinal = np.abs(objPos[-1] - self.objHeight)

        def reachReward():
            reachRew = -reachDist
            reachDistxy = np.linalg.norm(graspPos[:-1] - fingerCOM[:-1])
            zRew = np.linalg.norm(fingerCOM[-1] - self.init_fingerCOM[-1])
            if reachDistxy < 0.04:
                reachRew = -reachDist
            else:
                reachRew =  -reachDistxy - zRew

            # incentive to close fingers when reachDist is small
            if reachDist < 0.04:
                reachRew = -reachDist + max(actions[-1],0)/50
            return reachRew, reachDist

        def pickCompletionCriteria():
            tolerance = 0.01
            if objPos[2] >= (heightTarget - tolerance) and reachDist < 0.03:
                return True
            else:
                return False

        if pickCompletionCriteria():
            self.pickCompleted = True

        def objDropped():
            return (objPos[2] < (self.objHeight + 0.005)) and (placingDist >0.02) and (reachDist > 0.02)

        def placeCompletionCriteria():
            if abs(objPos[0] - placingGoal[0]) < 0.03 and \
                abs(objPos[1] - placingGoal[1]) < 0.03:
                return True
            else:
                return False

        if placeCompletionCriteria():
            self.placeCompleted = True
        else:
            self.placeCompleted = False

        def orig_pickReward():
            hScale = 100
            if self.placeCompleted or (self.pickCompleted and not(objDropped())):
                return hScale*heightTarget
            elif (reachDist < 0.04) and (objPos[2]> (self.objHeight + 0.005)) :
                return hScale* min(heightTarget, objPos[2])
            else:
                return 0

        def placeRewardMove():
            c1 = 1000 ; c2 = 0.01 ; c3 = 0.001
            placeRew = 1000*(self.maxPlacingDist - placingDist) + c1*(np.exp(-(placingDist**2)/c2) + np.exp(-(placingDist**2)/c3))
            if self.placeCompleted:
                c4 = 2000; c5 = 0.003; c6 = 0.0003
                placeRew += 2000*(heightTarget - placingDistFinal) + c4*(np.exp(-(placingDistFinal**2)/c5) + np.exp(-(placingDistFinal**2)/c6))
            placeRew = max(placeRew,0)
            cond = self.placeCompleted or (self.pickCompleted and (reachDist < 0.04) and not(objDropped()))
            if cond:
                return [placeRew, placingDist, placingDistFinal]
            else:
                return [0, placingDist, placingDistFinal]

        reachRew, reachDist = reachReward()
        pickRew = orig_pickReward()
        placeRew , placingDist, placingDistFinal = placeRewardMove()
        assert ((placeRew >=0) and (pickRew>=0))
        reward = reachRew + pickRew + placeRew
        success = (abs(objPos[0] - placingGoal[0]) < 0.03 and abs(objPos[1] - placingGoal[1]) < 0.03 and placingDistFinal <= 0.04)
        return [reward, reachRew, reachDist, pickRew, placeRew, placingDist, placingDistFinal, success]",abs(objPos[0] - placingGoal[0]) < 0.03 and abs(objPos[1] - placingGoal[1]) < 0.03,abs(objPos[0] - placingGoal[0]) < 0.03 > abs(objPos[1] - placingGoal[1]),0,,,,,,,
nlg-eval,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlg-eval/nlgeval/pycocoevalcap/cider/cider_scorer.py,https://github.com/Maluuba/nlg-eval/tree/master/nlgeval/pycocoevalcap/cider/cider_scorer.py,CiderScorer,sim$135,"def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):
            '''
            Compute the cosine similarity of two vectors.
            :param vec_hyp: array of dictionary for vector corresponding to hypothesis
            :param vec_ref: array of dictionary for vector corresponding to reference
            :param norm_hyp: array of float for vector corresponding to hypothesis
            :param norm_ref: array of float for vector corresponding to reference
            :param length_hyp: int containing length of hypothesis
            :param length_ref: int containing length of reference
            :return: array of score for each n-grams cosine similarity
            '''
            delta = float(length_hyp - length_ref)
            # measure consine similarity
            val = np.array([0.0 for _ in range(self.n)])
            for n in range(self.n):
                # ngram
                for (ngram,count) in six.iteritems(vec_hyp[n]):
                    # vrama91 : added clipping
                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]

                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):
                    val[n] /= (norm_hyp[n]*norm_ref[n])

                assert(not math.isnan(val[n]))
                # vrama91: added a length based gaussian penalty
                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))
            return val",norm_hyp[n] != 0 != norm_ref[n] and norm_ref[n] != 0,norm_hyp[n] != 0 != norm_ref[n] != 0,0,,,,,,,
R-Drop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/huggingface_transformer_src/src/transformers/generation_beam_search.py,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/generation_beam_search.py,BeamSearchScorer,process$199,"def process(
        self,
        input_ids: torch.LongTensor,
        next_scores: torch.FloatTensor,
        next_tokens: torch.LongTensor,
        next_indices: torch.LongTensor,
        pad_token_id: Optional[int] = None,
        eos_token_id: Optional[int] = None,
    ) -> Tuple[torch.Tensor]:
        cur_len = input_ids.shape[-1]
        batch_size = len(self._beam_hyps)
        assert batch_size == (input_ids.shape[0] // self.group_size)

        device = input_ids.device
        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)
        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)
        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)

        for batch_idx, beam_hyp in enumerate(self._beam_hyps):
            if self._done[batch_idx]:
                assert (
                    len(beam_hyp) >= self.num_beams
                ), f""Batch can only be done if at least {self.num_beams} beams have been generated""
                assert (
                    eos_token_id is not None and pad_token_id is not None
                ), ""generated beams >= num_beams -> eos_token_id and pad_token have to be defined""
                # pad the batch
                next_beam_scores[batch_idx, :] = 0
                next_beam_tokens[batch_idx, :] = pad_token_id
                next_beam_indices[batch_idx, :] = 0
                continue

            # next tokens for this sentence
            beam_idx = 0
            for beam_token_rank, (next_token, next_score, next_index) in enumerate(
                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])
            ):
                batch_beam_idx = batch_idx * self.group_size + next_index
                # add to generated hypotheses if end of sentence
                if (eos_token_id is not None) and (next_token.item() == eos_token_id):
                    # if beam_token does not belong to top num_beams tokens, it should not be added
                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size
                    if is_beam_token_worse_than_top_num_beams:
                        continue
                    beam_hyp.add(
                        input_ids[batch_beam_idx].clone(),
                        next_score.item(),
                    )
                else:
                    # add next predicted token since it is not eos_token
                    next_beam_scores[batch_idx, beam_idx] = next_score
                    next_beam_tokens[batch_idx, beam_idx] = next_token
                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx
                    beam_idx += 1

                # once the beam for next step is full, don't add more tokens to it.
                if beam_idx == self.group_size:
                    break

            if beam_idx < self.group_size:
                raise ValueError(
                    f""At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id: {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.""
                )

            # Check if we are done so that we can save a pad step if all(done)
            self._done[batch_idx] = self._done[batch_idx] or beam_hyp.is_done(
                next_scores[batch_idx].max().item(), cur_len
            )

        return UserDict(
            {
                ""next_beam_scores"": next_beam_scores.view(-1),
                ""next_beam_tokens"": next_beam_tokens.view(-1),
                ""next_beam_indices"": next_beam_indices.view(-1),
            }
        )",eos_token_id is not None and next_token.item() == eos_token_id,None is not eos_token_id == next_token.item(),0,,,,,,,
highway-env,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/highway-env/highway_env/envs/common/observation.py,https://github.com/eleurent/highway-env/tree/master/highway_env/envs/common/observation.py,OccupancyGridObservation,fill_road_layer_by_lanes$373,"def fill_road_layer_by_lanes(self, layer_index: int, lane_perception_distance: float = 100) -> None:
        """"""
        A layer to encode the onroad (1) / offroad (0) information

        Here, we iterate over lanes and regularly placed waypoints on these lanes to fill the corresponding cells.
        This approach is faster if the grid is large and the road network is small.

        :param layer_index: index of the layer in the grid
        :param lane_perception_distance: lanes are rendered +/- this distance from vehicle location
        """"""
        lane_waypoints_spacing = np.amin(self.grid_step)
        road = self.env.road

        for _from in road.network.graph.keys():
            for _to in road.network.graph[_from].keys():
                for lane in road.network.graph[_from][_to]:
                    origin, _ = lane.local_coordinates(self.observer_vehicle.position)
                    waypoints = np.arange(origin - lane_perception_distance,
                                            origin + lane_perception_distance,
                                            lane_waypoints_spacing).clip(0, lane.length)
                    for waypoint in waypoints:
                        cell = self.pos_to_index(lane.position(waypoint, 0))
                        if 0 <= cell[1] < self.grid.shape[-2] and 0 <= cell[0] < self.grid.shape[-1]:
                            self.grid[layer_index, cell[1], cell[0]] = 1",0 <= cell[1] < self.grid.shape[-2] and 0 <= cell[0] < self.grid.shape[-1],self.grid.shape[-2] > cell[1] >= 0 <= cell[0] < self.grid.shape[-1],0,,,,,,,
free-python-games,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/free-python-games/freegames/cannon.py,https://github.com/grantjenks/free-python-games/tree/master/freegames/cannon.py,,inside$30,"def inside(xy):
    """"""Return True if xy within screen.""""""
    return -200 < xy.x < 200 and -200 < xy.y < 200",-200 < xy.x < 200 and -200 < xy.y < 200,#NAME?,0,,,,,,,
