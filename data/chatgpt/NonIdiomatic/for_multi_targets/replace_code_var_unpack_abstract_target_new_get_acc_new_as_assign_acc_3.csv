repo_name,file_path,file_html,class_name,me_name,me_code,old_code,new_code,bool_code,chatGPT_code,if_correct,,,reversed_code,non_replace_var_refactored_code,refactored_code,acc,instruction,sys_msg,exam_msg,user_msg
natlas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/natlas/natlas-server/app/api/routes.py,https://github.com/MJL85/natlas/tree/master/natlas-server/app/api/routes.py,,submit$94,"def submit():
    status_code = None
    response_body = None
    data = request.get_json()
    newhost = {}
    newhost = json.loads(data)
    newhost[""ctime""] = dt.now(tz.utc)
    if newhost[""scan_reason""] == ""requested"":
        mark_scan_completed(newhost[""ip""], newhost[""scan_id""])

    try:
        nmap = NmapParser.parse(newhost.get(""xml_data"", None))
        # If there's more or less than 1 host in the xml data, reject it (for now)
        if nmap.hosts_total != 1:
            status_code = 400
            response_body = json.dumps(
                {
                    ""status"": status_code,
                    ""message"": ""XML had too many hosts in it"",
                    ""retry"": False,
                }
            )

        # If it's not an acceptable target, tell the agent it's out of scope
        elif len(nmap.hosts) == 1 and not current_app.ScopeManager.is_acceptable_target(
            nmap.hosts[0].address
        ):
            status_code = 400
            response_body = json.dumps(
                {
                    ""status"": status_code,
                    ""message"": ""Out of scope: "" + nmap.hosts[0].address,
                    ""retry"": False,
                }
            )

        # If there's no further processing to do, store the host and prepare the response
        elif not newhost[""is_up""] or (newhost[""is_up""] and newhost[""port_count""] == 0):
            current_app.elastic.new_result(newhost)
            status_code = 200
            response_body = json.dumps(
                {""status"": status_code, ""message"": ""Received: "" + newhost[""ip""]}
            )
    except NmapParserException:
        status_code = 400
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": ""Invalid nmap xml data provided"",
                ""retry"": False,
            }
        )

    # If status_code and response_body have been set by this point, return a response.
    if status_code and response_body:
        response = Response(
            response=response_body, status=status_code, content_type=json_content
        )
        return response

    if newhost[""scan_start""] and newhost[""scan_stop""]:
        elapsed = dateutil.parser.parse(newhost[""scan_stop""]) - dateutil.parser.parse(
            newhost[""scan_start""]
        )
        newhost[""elapsed""] = elapsed.seconds

    newhost[""ip""] = nmap.hosts[0].address
    if len(nmap.hosts[0].hostnames) > 0:
        newhost[""hostname""] = nmap.hosts[0].hostnames[0]

    tmpports = []
    newhost[""ports""] = []

    for port in nmap.hosts[0].get_open_ports():
        tmpports.append(str(port[0]))
        srv = nmap.hosts[0].get_service(port[0], port[1])
        portinfo = srv.get_dict()
        portinfo[""service""] = srv.service_dict
        portinfo[""scripts""] = []
        for script in srv.scripts_results:
            scriptsave = {""id"": script[""id""], ""output"": script[""output""]}
            portinfo[""scripts""].append(scriptsave)
            if script[""id""] == ""ssl-cert"":
                portinfo[""ssl""] = parse_ssl_data(script)

        newhost[""ports""].append(portinfo)

    newhost[""port_str""] = "", "".join(tmpports)

    if ""screenshots"" in newhost and newhost[""screenshots""]:
        newhost[""screenshots""], newhost[""num_screenshots""] = process_screenshots(
            newhost[""screenshots""]
        )

    if len(newhost[""ports""]) == 0:
        status_code = 200
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": f""Expected open ports but didn't find any for {newhost['ip']}"",
            }
        )
    elif len(newhost[""ports""]) > 500:
        status_code = 200
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": ""More than 500 ports found, throwing data out"",
            }
        )
    else:
        status_code = 200
        current_app.elastic.new_result(newhost)
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": f""Received {len(newhost['ports'])} ports for {newhost['ip']}"",
            }
        )

    response = Response(
        response=response_body, status=status_code, content_type=json_content
    )
    return response","for port in nmap.hosts[0].get_open_ports():
    tmpports.append(str(port[0]))
    srv = nmap.hosts[0].get_service(port[0], port[1])
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)","for port in nmap.hosts[0].get_open_ports():
    (port_0, port_1, *_) = port
    tmpports.append(str(port[0]))
    srv = nmap.hosts[0].get_service(port[0], port[1])
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
natlas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/natlas/natlas-server/app/api/routes.py,https://github.com/MJL85/natlas/tree/master/natlas-server/app/api/routes.py,,submit$94,"def submit():
    status_code = None
    response_body = None
    data = request.get_json()
    newhost = {}
    newhost = json.loads(data)
    newhost[""ctime""] = dt.now(tz.utc)
    if newhost[""scan_reason""] == ""requested"":
        mark_scan_completed(newhost[""ip""], newhost[""scan_id""])

    try:
        nmap = NmapParser.parse(newhost.get(""xml_data"", None))
        # If there's more or less than 1 host in the xml data, reject it (for now)
        if nmap.hosts_total != 1:
            status_code = 400
            response_body = json.dumps(
                {
                    ""status"": status_code,
                    ""message"": ""XML had too many hosts in it"",
                    ""retry"": False,
                }
            )

        # If it's not an acceptable target, tell the agent it's out of scope
        elif len(nmap.hosts) == 1 and not current_app.ScopeManager.is_acceptable_target(
            nmap.hosts[0].address
        ):
            status_code = 400
            response_body = json.dumps(
                {
                    ""status"": status_code,
                    ""message"": ""Out of scope: "" + nmap.hosts[0].address,
                    ""retry"": False,
                }
            )

        # If there's no further processing to do, store the host and prepare the response
        elif not newhost[""is_up""] or (newhost[""is_up""] and newhost[""port_count""] == 0):
            current_app.elastic.new_result(newhost)
            status_code = 200
            response_body = json.dumps(
                {""status"": status_code, ""message"": ""Received: "" + newhost[""ip""]}
            )
    except NmapParserException:
        status_code = 400
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": ""Invalid nmap xml data provided"",
                ""retry"": False,
            }
        )

    # If status_code and response_body have been set by this point, return a response.
    if status_code and response_body:
        response = Response(
            response=response_body, status=status_code, content_type=json_content
        )
        return response

    if newhost[""scan_start""] and newhost[""scan_stop""]:
        elapsed = dateutil.parser.parse(newhost[""scan_stop""]) - dateutil.parser.parse(
            newhost[""scan_start""]
        )
        newhost[""elapsed""] = elapsed.seconds

    newhost[""ip""] = nmap.hosts[0].address
    if len(nmap.hosts[0].hostnames) > 0:
        newhost[""hostname""] = nmap.hosts[0].hostnames[0]

    tmpports = []
    newhost[""ports""] = []

    for port in nmap.hosts[0].get_open_ports():
        tmpports.append(str(port[0]))
        srv = nmap.hosts[0].get_service(port[0], port[1])
        portinfo = srv.get_dict()
        portinfo[""service""] = srv.service_dict
        portinfo[""scripts""] = []
        for script in srv.scripts_results:
            scriptsave = {""id"": script[""id""], ""output"": script[""output""]}
            portinfo[""scripts""].append(scriptsave)
            if script[""id""] == ""ssl-cert"":
                portinfo[""ssl""] = parse_ssl_data(script)

        newhost[""ports""].append(portinfo)

    newhost[""port_str""] = "", "".join(tmpports)

    if ""screenshots"" in newhost and newhost[""screenshots""]:
        newhost[""screenshots""], newhost[""num_screenshots""] = process_screenshots(
            newhost[""screenshots""]
        )

    if len(newhost[""ports""]) == 0:
        status_code = 200
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": f""Expected open ports but didn't find any for {newhost['ip']}"",
            }
        )
    elif len(newhost[""ports""]) > 500:
        status_code = 200
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": ""More than 500 ports found, throwing data out"",
            }
        )
    else:
        status_code = 200
        current_app.elastic.new_result(newhost)
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": f""Received {len(newhost['ports'])} ports for {newhost['ip']}"",
            }
        )

    response = Response(
        response=response_body, status=status_code, content_type=json_content
    )
    return response","for script in srv.scripts_results:
    scriptsave = {'id': script['id'], 'output': script['output']}
    portinfo['scripts'].append(scriptsave)
    if script['id'] == 'ssl-cert':
        portinfo['ssl'] = parse_ssl_data(script)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked elements e['id'] and e['output'] are dictionary values that can be accessed directly using the keys 'id' and 'output'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
python-driver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-driver/tests/integration/standard/test_row_factories.py,https://github.com/datastax/python-driver/tree/master/tests/integration/standard/test_row_factories.py,RowFactoryTests,test_tuple_factory$93,"def test_tuple_factory(self):
        result = self._results_from_row_factory(tuple_factory)
        self.assertIsInstance(result, ResultSet)
        self.assertIsInstance(result[0], tuple)

        for row in result:
            self.assertEqual(row[0], row[1])

        self.assertEqual(result[0][0], result[0][1])
        self.assertEqual(result[0][0], 1)
        self.assertEqual(result[1][0], result[1][1])
        self.assertEqual(result[1][0], 2)","for row in result:
    self.assertEqual(row[0], row[1])","for row in result:
    (row_0, row_1, *_) = row
    self.assertEqual(row[0], row[1])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
WechatSogou,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WechatSogou/wechatsogou/tools.py,https://github.com/chyroc/WechatSogou/tree/master/wechatsogou/tools.py,,_replace_str_html$73,"def _replace_str_html(s):
    """"""替换html‘&quot;’等转义内容为正常内容

    Args:
        s: 文字内容

    Returns:
        s: 处理反转义后的文字
    """"""
    html_str_list = [
        ('&#39;', '\''),
        ('&quot;', '""'),
        ('&amp;', '&'),
        ('&yen;', '¥'),
        ('amp;', ''),
        ('&lt;', '<'),
        ('&gt;', '>'),
        ('&nbsp;', ' '),
        ('\\', '')
    ]
    for i in html_str_list:
        s = s.replace(i[0], i[1])
    return s","for i in html_str_list:
    s = s.replace(i[0], i[1])","for i in html_str_list:
    (i_0, i_1, *_) = i
    s = s.replace(i[0], i[1])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/time/tests/test_basic.py,https://github.com/astropy/astropy/tree/master/astropy/time/tests/test_basic.py,TestSubFormat,test_fits_scale$963,"def test_fits_scale(self):
        """"""Test that the previous FITS-string formatting can still be handled
        but with a DeprecationWarning.""""""
        for inputs in (
            (""2000-01-02(TAI)"", ""tai""),
            (""1999-01-01T00:00:00.123(ET(NIST))"", ""tt""),
            (""2014-12-12T01:00:44.1(UTC)"", ""utc""),
        ):
            with pytest.warns(AstropyDeprecationWarning):
                t = Time(inputs[0])
            assert t.scale == inputs[1]

            # Create Time using normal ISOT syntax and compare with FITS
            t2 = Time(inputs[0][: inputs[0].index(""("")], format=""isot"", scale=inputs[1])
            assert t == t2

        # Explicit check that conversions still work despite warning
        with pytest.warns(AstropyDeprecationWarning):
            t = Time(""1999-01-01T00:00:00.123456789(UTC)"")
        t = t.tai
        assert t.isot == ""1999-01-01T00:00:32.123""

        with pytest.warns(AstropyDeprecationWarning):
            t = Time(""1999-01-01T00:00:32.123456789(TAI)"")
        t = t.utc
        assert t.isot == ""1999-01-01T00:00:00.123""

        # Check scale consistency
        with pytest.warns(AstropyDeprecationWarning):
            t = Time(""1999-01-01T00:00:32.123456789(TAI)"", scale=""tai"")
        assert t.scale == ""tai""
        with pytest.warns(AstropyDeprecationWarning):
            t = Time(""1999-01-01T00:00:32.123456789(ET)"", scale=""tt"")
        assert t.scale == ""tt""
        with pytest.raises(ValueError), pytest.warns(AstropyDeprecationWarning):
            t = Time(""1999-01-01T00:00:32.123456789(TAI)"", scale=""utc"")","for inputs in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(inputs[0])
    assert t.scale == inputs[1]
    t2 = Time(inputs[0][:inputs[0].index('(')], format='isot', scale=inputs[1])
    assert t == t2","for inputs in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    ((inputs_0_0, inputs_0_1, *inputs_0_rinputsmaining), inputs_1, *inputs_rinputsmaining) = inputs
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(inputs[0])
    assert t.scale == inputs[1]
    t2 = Time(inputs[0][:inputs[0].index('(')], format='isot', scale=inputs[1])
    assert t == t2","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",0,,,"Answer: Yes
Iterable Unpacking: (e_0_0, e_0_1, *e_0_remaining), e_1, *e_remaining = e
variable mapping:
e_0_0: e[0]
e_0_1: e[0][:e[0].index('(')]
e_1: e[1]",,,,,,,
nicotine-plus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nicotine-plus/pynicotine/gtkgui/userbrowse.py,https://github.com/nicotine-plus/nicotine-plus/tree/master/pynicotine/gtkgui/userbrowse.py,UserBrowse,download_directory$557,"def download_directory(self, folder, prefix="""", recurse=False):

        if folder is None:
            return

        # Remember custom download location
        self.frame.np.transfers.requested_folders[self.user][folder] = prefix

        # Get final download destination
        destination = self.frame.np.transfers.get_folder_destination(self.user, folder)

        files = self.shares.get(folder)

        if files:
            if config.sections[""transfers""][""reverseorder""]:
                files.sort(key=lambda x: x[1], reverse=True)

            for file_data in files:
                virtualpath = ""\\"".join([folder, file_data[1]])
                size = file_data[2]
                h_bitrate, bitrate, h_length, length = get_result_bitrate_length(size, file_data[4])

                self.frame.np.transfers.get_file(
                    self.user, virtualpath, destination,
                    size=size, bitrate=h_bitrate, length=h_length)

        if not recurse:
            return

        for subdir, subf in self.shares.items():
            if folder in subdir and folder != subdir:
                self.download_directory(subdir, prefix=os.path.join(destination, """"))","for file_data in files:
    virtualpath = '\\'.join([folder, file_data[1]])
    size = file_data[2]
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, file_data[4])
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)","for file_data in files:
    (_, file_data_1, file_data_2, _, file_data_4, *_) = file_data
    virtualpath = '\\'.join([folder, file_data[1]])
    size = file_data[2]
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, file_data[4])
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, _, e_4 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_4: e[4]",,,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/deps/brew_exts.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/deps/brew_exts.py,,commit_for_version$237,"def commit_for_version(recipe_context, package, version):
    tap_path = recipe_context.tap_path
    commit = None
    with brew_head_at_commit(""master"", tap_path):
        version_to_commit = brew_versions_info(package, tap_path)
        if version is None:
            version = version_to_commit[0][0]
            commit = version_to_commit[0][1]
        else:
            for mapping in version_to_commit:
                if mapping[0] == version:
                    commit = mapping[1]
    if commit is None:
        raise Exception(f""Failed to find commit for version {version}"")
    return commit","for mapping in version_to_commit:
    if mapping[0] == version:
        commit = mapping[1]","for mapping in version_to_commit:
    (mapping_0, mapping_1, *_) = mapping
    if mapping[0] == version:
        commit = mapping[1]","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
aeneas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aeneas/aeneas/tests/test_globalfunctions.py,https://github.com/readbeyond/aeneas/tree/master/aeneas/tests/test_globalfunctions.py,TestGlobalFunctions,test_safe_get$143,"def test_safe_get(self):
        tests = [
            (None, None, u""default"", u""default""),
            (None, u""key"", u""default"", u""default""),
            ({}, None, u""default"", u""default""),
            ({}, u""key"", u""default"", u""default""),
            ([], u""key"", u""default"", u""default""),
            ({u""key"": u""value""}, None, u""default"", u""default""),
            ({u""key"": u""value""}, u""key"", u""default"", u""value""),
        ]
        for test in tests:
            self.assertEqual(gf.safe_get(test[0], test[1], test[2]), test[3])","for test in tests:
    self.assertEqual(gf.safe_get(test[0], test[1], test[2]), test[3])","for test in tests:
    (test_0, test_1, test_2, test_3, *_) = test
    self.assertEqual(gf.safe_get(test[0], test[1], test[2]), test[3])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
CANalyzat0r,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CANalyzat0r/src/ManagerTab.py,https://github.com/schutzwerk/CANalyzat0r/tree/master/src/ManagerTab.py,ManagerTab,saveToFile$1058,"def saveToFile(self):
        """"""
        Save the packets in the GUI table to a file in SocketCAN format.
        """"""

        # Convert raw data to SocketCAN format
        socketCANPackets = []

        packetsToSave = self.rawData
        if len(packetsToSave) == 0:
            return

        for packet in packetsToSave:
            socketCANPacket = SocketCANPacket(
                packet[3], Globals.CANData.ifaceName if
                Globals.CANData is not None else ""can0"", packet[0], packet[1])

            socketCANPackets.append(socketCANPacket)
            self.logger.debug(Strings.snifferTabElementSocketCANConvertOK)

        # A tuple is returned --> only use the first element which represents the absolute file path
        filePath = Toolbox.Toolbox.getSaveFileName(Strings.saveDialogTitle)
        if filePath:
            CANData.writeCANFile(filePath, socketCANPackets)
            self.logger.info(Strings.dataWritten + "" "" +
                             str(len(socketCANPackets)))
        else:
            self.logger.info(Strings.dataNotWritten)","for packet in packetsToSave:
    socketCANPacket = SocketCANPacket(packet[3], Globals.CANData.ifaceName if Globals.CANData is not None else 'can0', packet[0], packet[1])
    socketCANPackets.append(socketCANPacket)
    self.logger.debug(Strings.snifferTabElementSocketCANConvertOK)","for packet in packetsToSave:
    (packet_0, packet_1, _, packet_3, *packet_rpacketmaining) = packet
    socketCANPacket = SocketCANPacket(packet[3], Globals.CANData.ifaceName if Globals.CANData is not None else 'can0', packet[0], packet[1])
    socketCANPackets.append(socketCANPacket)
    self.logger.debug(Strings.snifferTabElementSocketCANConvertOK)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, _, e_3, *e_remaining = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_3: e[3]",,,,,,,
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2020/med_test.py,https://github.com/3b1b/videos/tree/master/_2020/med_test.py,SamplePopulationBreastCancer,construct$820,"def construct(self):
        # Introduce population
        title = TexText(
            ""Sample of "", ""$1{,}000$"", "" women"",
            font_size=72,
        )
        title.add(Underline(title, color=GREY_B))
        title.to_edge(UP, buff=MED_SMALL_BUFF)
        self.add(title)

        woman = WomanIcon()
        globals()['woman'] = woman
        population = VGroup(*[woman.copy() for x in range(1000)])
        population.arrange_in_grid(
            25, 40,
            buff=LARGE_BUFF,
            fill_rows_first=False,
        )
        population.set_height(6)
        population.next_to(title, DOWN)

        counter = Integer(1000, edge_to_fix=UL)
        counter.replace(title[1])
        counter.set_value(0)

        title[1].set_opacity(0)
        self.play(
            ShowIncreasingSubsets(population),
            ChangeDecimalToValue(counter, 1000),
            run_time=5
        )
        self.remove(counter)
        title[1].set_opacity(1)
        self.wait()

        # Show true positives
        rects = VGroup(Rectangle(), Rectangle())
        rects.set_height(6)
        rects[0].set_width(4, stretch=True)
        rects[1].set_width(8, stretch=True)
        rects[0].set_stroke(YELLOW, 3)
        rects[1].set_stroke(GREY, 3)
        rects.arrange(RIGHT)
        rects.center().to_edge(DOWN, buff=MED_SMALL_BUFF)

        positive_cases = population[:10]
        negative_cases = population[10:]

        positive_cases.generate_target()
        positive_cases.target.move_to(rects[0])
        positive_cases.target.set_color(YELLOW)

        negative_cases.generate_target()
        negative_cases.target.set_height(rects[1].get_height() * 0.8)
        negative_cases.target.move_to(rects[1])

        positive_words = TexText(r""1\% "", ""Have breast cancer"", font_size=36)
        positive_words.set_color(YELLOW)
        positive_words.next_to(rects[0], UP, SMALL_BUFF)

        negative_words = TexText(r""99\% "", ""Do not have cancer"", font_size=36)
        negative_words.set_color(GREY_B)
        negative_words.next_to(rects[1], UP, SMALL_BUFF)

        self.play(
            MoveToTarget(positive_cases),
            MoveToTarget(negative_cases),
            Write(positive_words, run_time=1),
            Write(negative_words, run_time=1),
            FadeIn(rects),
        )
        self.wait()

        # Show screening
        scan_lines = VGroup(*(
            Line(
                # FRAME_WIDTH * LEFT / 2,
                FRAME_HEIGHT * DOWN / 2,
                icon.get_center(),
                stroke_width=1,
                stroke_color=interpolate_color(BLUE, GREEN, random.random())
            )
            for icon in population
        ))
        self.play(
            LaggedStartMap(
                ShowCreationThenFadeOut, scan_lines,
                lag_ratio=1 / len(scan_lines),
                run_time=3,
            )
        )
        self.wait()

        # Test results on cancer population
        tpr_words = TexText(""9 True positives"", font_size=36)
        fnr_words = TexText(""1 False negative"", font_size=36)
        tnr_words = TexText(""901 True negatives"", font_size=36)
        fpr_words = TexText(""89 False positives"", font_size=36)

        tpr_words.set_color(GREEN_B)
        fnr_words.set_color(RED_D)
        tnr_words.set_color(RED_B)
        fpr_words.set_color(GREEN_D)

        tp_cases = positive_cases[:9]
        fn_cases = positive_cases[9:]

        tpr_words.next_to(tp_cases, UP)
        fnr_words.next_to(fn_cases, DOWN)

        signs = VGroup()
        for woman in tp_cases:
            sign = Tex(""+"")
            sign.set_color(GREEN_B)
            sign.match_height(woman)
            sign.next_to(woman, RIGHT, SMALL_BUFF)
            woman.sign = sign
            signs.add(sign)
        for woman in fn_cases:
            sign = Tex(""-"")
            sign.set_color(RED)
            sign.match_width(signs[0])
            sign.next_to(woman, RIGHT, SMALL_BUFF)
            woman.sign = sign
            signs.add(sign)

        boxes = VGroup()
        for n, woman in enumerate(positive_cases):
            box = SurroundingRectangle(woman, buff=0)
            box.set_stroke(width=2)
            if woman in tp_cases:
                box.set_color(GREEN)
            else:
                box.set_color(RED)
            woman.box = box
            boxes.add(box)

        self.play(
            FadeIn(tpr_words, shift=0.2 * UP),
            ShowIncreasingSubsets(signs[:9]),
            ShowIncreasingSubsets(boxes[:9]),
        )
        self.wait()
        self.play(
            FadeIn(fnr_words, shift=0.2 * DOWN),
            Write(signs[9:]),
            ShowCreation(boxes[9:]),
        )
        self.wait()

        # Test results on cancer-free population
        negative_cases.sort(lambda p: -p[1])

        num_fp = int(len(negative_cases) * 0.09)
        fp_cases = negative_cases[:num_fp]
        tn_cases = negative_cases[num_fp:]

        new_boxes = VGroup()
        for n, woman in enumerate(negative_cases):
            box = SurroundingRectangle(woman, buff=0)
            box.set_stroke(width=2)
            if woman in fp_cases:
                box.set_color(GREEN)
            else:
                box.set_color(RED)
            woman.box = box
            new_boxes.add(box)

        fpr_words.next_to(fp_cases, UP, buff=SMALL_BUFF)
        tnr_words.next_to(tn_cases, DOWN, buff=0.2)

        self.play(
            FadeIn(fpr_words, shift=0.2 * UP),
            ShowIncreasingSubsets(new_boxes[:num_fp])
        )
        self.wait()
        self.play(
            FadeIn(tnr_words, shift=0.2 * DOWN),
            ShowIncreasingSubsets(new_boxes[num_fp:])
        )
        self.wait()

        # Consolidate boxes
        self.remove(boxes, new_boxes, population)
        for woman in population:
            woman.add(woman.box)
        self.add(population)

        # Limit view to positive cases
        for cases, nr, rect in zip([tp_cases, fp_cases], [3, 7], rects):
            cases.save_state()
            cases.generate_target()
            for case in cases.target:
                case[-1].set_stroke(width=3)
                case[-1].scale(1.1)
            cases.target.arrange_in_grid(
                n_rows=nr,
                buff=0.5 * cases[0].get_width()
            )
            cases.target.scale(0.5 / cases.target[0].get_height())
            cases.target.move_to(rect)

        fp_cases.target.shift(0.4 * DOWN)
        positive_words.save_state()
        negative_words.save_state()
        tpr_words.save_state()
        fpr_words.save_state()

        self.play(
            MoveToTarget(tp_cases),
            MoveToTarget(fp_cases),
            tpr_words.next_to, tp_cases.target, UP,
            fpr_words.next_to, fp_cases.target, UP,
            FadeOut(signs),
            positive_words[0].set_opacity, 0,
            negative_words[0].set_opacity, 0,
            positive_words[1].match_x, rects[0],
            negative_words[1].match_x, rects[1],
            LaggedStart(
                FadeOut(fn_cases, shift=DOWN),
                FadeOut(fnr_words, shift=DOWN),
                FadeOut(tn_cases, shift=DOWN),
                FadeOut(tnr_words, shift=DOWN),
            ),
        )
        self.wait()

        # Emphasize groups counts
        self.play(
            ShowCreationThenFadeOut(SurroundingRectangle(
                tpr_words[0][:1],
                stroke_width=2,
                stroke_color=WHITE,
                buff=0.05,
            )),
            LaggedStartMap(Indicate, tp_cases, color=YELLOW, lag_ratio=0.3, run_time=1),
        )
        self.wait()
        self.play(
            ShowCreationThenFadeOut(SurroundingRectangle(
                fpr_words[0][:2],
                stroke_width=2,
                stroke_color=WHITE,
                buff=0.05,
            )),
            LaggedStartMap(
                Indicate, fp_cases,
                color=GREEN_A,
                lag_ratio=0.05,
                run_time=3
            )
        )
        self.wait()

        # Final equation
        equation = Tex(
            ""P("",
            ""\\text{Have cancer }"",
            ""|"",
            ""\\text{ positive test})"",
            ""\\approx"",
            ""\\frac{9}{9 + 89}"",
            ""\\approx \\frac{1}{11}""
        )
        equation.set_color_by_tex(""cancer"", YELLOW)
        equation.set_color_by_tex(""positive"", GREEN)
        equation.to_edge(UP, buff=SMALL_BUFF)

        self.play(
            FadeIn(equation[:-1], shift=UP),
            FadeOut(title, shift=UP),
        )
        self.wait()
        self.play(Write(equation[-1]))
        self.wait()

        # Label PPV
        frame = self.camera.frame
        frame.save_state()

        ppv_words = TexText(
            ""Positive\\\\"",
            ""Predictive\\\\"",
            ""Value\\\\"",
            alignment="""",
        )
        ppv_words.next_to(equation, RIGHT, LARGE_BUFF, DOWN)
        for word in ppv_words:
            word[0].set_color(BLUE)

        ppv_rhs = Tex(
            ""={\\text{TP} \\over \\text{TP} + \\text{FP}}"",
            tex_to_color_map={
                ""\\text{TP}"": GREEN_B,
                ""\\text{FP}"": GREEN_C,
            }
        )
        ppv_rhs.next_to(ppv_words, RIGHT)
        ppv_rhs.shift(1.5 * LEFT)

        self.play(frame.scale, 1.1, {""about_edge"": DL})
        self.play(ShowIncreasingSubsets(ppv_words))
        self.wait()

        self.play(
            equation.shift, 1.5 * LEFT + 0.5 * UP,
            ppv_words.shift, 1.5 * LEFT,
            FadeIn(ppv_rhs, lag_ratio=0.1),
            frame.scale, 1.1, {""about_edge"": DL},
        )
        self.wait()

        # Go back to earlier state
        self.play(
            frame.restore,
            frame.shift, 0.5 * DOWN,
            LaggedStartMap(FadeOut, VGroup(equation, ppv_words, ppv_rhs)),
            LaggedStartMap(Restore, VGroup(
                tpr_words, tp_cases,
                fpr_words, fp_cases,
            )),
            run_time=3,
        )
        self.play(
            LaggedStartMap(FadeIn, VGroup(
                fnr_words, fn_cases,
                tnr_words, tn_cases,
            )),
        )
        self.wait()

        # Fade rects
        fade_rects = VGroup(*(
            BackgroundRectangle(
                VGroup(rect, words),
                fill_opacity=0.9,
                fill_color=BLACK,
                buff=SMALL_BUFF,
            )
            for rect, words in zip(rects, [positive_words, negative_words])
        ))

        # Sensitivity
        sens_eq = Tex(
            ""\\text{Sensitivity}"",
            ""= {9 \\over 10}"",
            ""= 90\\%""
        )
        sens_eq.next_to(rects[0], LEFT, MED_LARGE_BUFF, aligned_edge=UP)
        sens_eq.shift(DOWN)

        fnr_eq = Tex(
            ""\\text{False Negative Rate}"", ""= 10\\%""
        )
        fnr_eq.set_color(RED)
        fnr_eq.scale(0.9)
        equiv = Tex(""\\Leftrightarrow"")
        equiv.scale(1.5)
        equiv.rotate(90 * DEGREES)
        equiv.next_to(sens_eq, DOWN, MED_LARGE_BUFF)
        fnr_eq.next_to(equiv, DOWN, MED_LARGE_BUFF)

        self.play(
            frame.shift, 5 * LEFT,
            FadeIn(fade_rects[1]),
            Write(sens_eq[0]),
        )
        self.wait()
        self.play(
            TransformFromCopy(tpr_words[0][0], sens_eq[1][1]),
            Write(sens_eq[1][0]),
            Write(sens_eq[1][2:]),
        )
        self.play(Write(sens_eq[2]))
        self.wait()

        self.play(
            FadeIn(equiv, shift=0.5 * DOWN),
            FadeIn(fnr_eq, shift=1.0 * DOWN),
        )
        self.wait()

        # Transition to right side
        fade_rects[0].stretch(5, 0, about_edge=RIGHT)
        self.play(
            ApplyMethod(frame.shift, 10 * RIGHT, run_time=4),
            FadeIn(fade_rects[0], run_time=2),
            FadeOut(fade_rects[1], run_time=2),
        )

        # Specificity
        spec_eq = Tex(
            ""\\text{Specificity}"",
            ""= {901 \\over 990}"",
            ""\\approx 91\\%""
        )
        spec_eq.next_to(rects[1], RIGHT, MED_LARGE_BUFF, aligned_edge=DOWN)
        spec_eq.shift(UP)

        fpr_eq = Tex(
            ""\\text{False Positive Rate}"", ""= 9\\%""
        )
        fpr_eq.set_color(GREEN)
        fpr_eq.scale(0.9)
        equiv2 = Tex(""\\Leftrightarrow"")
        equiv2.scale(1.5)
        equiv2.rotate(90 * DEGREES)
        equiv2.next_to(spec_eq, UP, MED_LARGE_BUFF)
        fpr_eq.next_to(equiv2, UP, MED_LARGE_BUFF)

        self.play(Write(spec_eq[0]))
        self.wait()
        self.play(
            Write(spec_eq[1][0]),
            TransformFromCopy(
                tnr_words[0][:3],
                spec_eq[1][1:4],
                run_time=2,
                path_arc=30 * DEGREES,
            ),
            Write(spec_eq[1][4:]),
        )
        self.wait()
        self.play(Write(spec_eq[2]))
        self.wait()

        self.play(
            FadeIn(equiv2, shift=0.5 * UP),
            FadeIn(fpr_eq, shift=1.0 * UP),
        )
        self.wait()

        # Reset to show both kinds of accuracy
        eqs = [sens_eq, spec_eq]
        for eq, word in zip(eqs, [positive_words, negative_words]):
            eq.generate_target()
            eq.target[1].set_opacity(0)
            eq.target[2].move_to(eq.target[1], LEFT),
            eq.target.next_to(word, UP, buff=0.3)

        self.play(
            FadeOut(fade_rects[0]),
            frame.shift, 5 * LEFT,
            frame.scale, 1.1, {""about_edge"": DOWN},
            MoveToTarget(sens_eq),
            MoveToTarget(spec_eq),
            *map(FadeOut, (fnr_eq, fpr_eq, equiv, equiv2)),
            run_time=2,
        )
        self.wait()

        self.play(
            VGroup(
                fn_cases, fnr_words,
                fp_cases, fpr_words,
            ).set_opacity, 0.2,
            rate_func=there_and_back_with_pause,
            run_time=3
        )","for (cases, nr, rect) in zip([tp_cases, fp_cases], [3, 7], rects):
    cases.save_state()
    cases.generate_target()
    for case in cases.target:
        case[-1].set_stroke(width=3)
        case[-1].scale(1.1)
    cases.target.arrange_in_grid(n_rows=nr, buff=0.5 * cases[0].get_width())
    cases.target.scale(0.5 / cases.target[0].get_height())
    cases.target.move_to(rect)","for (cases, nr, rect) in zip([tp_cases, fp_cases], [3, 7], rects):
    (cases_0, *cases_rcasesmaining) = cases
    cases.save_state()
    cases.generate_target()
    for case in cases.target:
        case[-1].set_stroke(width=3)
        case[-1].scale(1.1)
    cases.target.arrange_in_grid(n_rows=nr, buff=0.5 * cases[0].get_width())
    cases.target.scale(0.5 / cases.target[0].get_height())
    cases.target.move_to(rect)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2020/med_test.py,https://github.com/3b1b/videos/tree/master/_2020/med_test.py,SamplePopulationBreastCancer,construct$820,"def construct(self):
        # Introduce population
        title = TexText(
            ""Sample of "", ""$1{,}000$"", "" women"",
            font_size=72,
        )
        title.add(Underline(title, color=GREY_B))
        title.to_edge(UP, buff=MED_SMALL_BUFF)
        self.add(title)

        woman = WomanIcon()
        globals()['woman'] = woman
        population = VGroup(*[woman.copy() for x in range(1000)])
        population.arrange_in_grid(
            25, 40,
            buff=LARGE_BUFF,
            fill_rows_first=False,
        )
        population.set_height(6)
        population.next_to(title, DOWN)

        counter = Integer(1000, edge_to_fix=UL)
        counter.replace(title[1])
        counter.set_value(0)

        title[1].set_opacity(0)
        self.play(
            ShowIncreasingSubsets(population),
            ChangeDecimalToValue(counter, 1000),
            run_time=5
        )
        self.remove(counter)
        title[1].set_opacity(1)
        self.wait()

        # Show true positives
        rects = VGroup(Rectangle(), Rectangle())
        rects.set_height(6)
        rects[0].set_width(4, stretch=True)
        rects[1].set_width(8, stretch=True)
        rects[0].set_stroke(YELLOW, 3)
        rects[1].set_stroke(GREY, 3)
        rects.arrange(RIGHT)
        rects.center().to_edge(DOWN, buff=MED_SMALL_BUFF)

        positive_cases = population[:10]
        negative_cases = population[10:]

        positive_cases.generate_target()
        positive_cases.target.move_to(rects[0])
        positive_cases.target.set_color(YELLOW)

        negative_cases.generate_target()
        negative_cases.target.set_height(rects[1].get_height() * 0.8)
        negative_cases.target.move_to(rects[1])

        positive_words = TexText(r""1\% "", ""Have breast cancer"", font_size=36)
        positive_words.set_color(YELLOW)
        positive_words.next_to(rects[0], UP, SMALL_BUFF)

        negative_words = TexText(r""99\% "", ""Do not have cancer"", font_size=36)
        negative_words.set_color(GREY_B)
        negative_words.next_to(rects[1], UP, SMALL_BUFF)

        self.play(
            MoveToTarget(positive_cases),
            MoveToTarget(negative_cases),
            Write(positive_words, run_time=1),
            Write(negative_words, run_time=1),
            FadeIn(rects),
        )
        self.wait()

        # Show screening
        scan_lines = VGroup(*(
            Line(
                # FRAME_WIDTH * LEFT / 2,
                FRAME_HEIGHT * DOWN / 2,
                icon.get_center(),
                stroke_width=1,
                stroke_color=interpolate_color(BLUE, GREEN, random.random())
            )
            for icon in population
        ))
        self.play(
            LaggedStartMap(
                ShowCreationThenFadeOut, scan_lines,
                lag_ratio=1 / len(scan_lines),
                run_time=3,
            )
        )
        self.wait()

        # Test results on cancer population
        tpr_words = TexText(""9 True positives"", font_size=36)
        fnr_words = TexText(""1 False negative"", font_size=36)
        tnr_words = TexText(""901 True negatives"", font_size=36)
        fpr_words = TexText(""89 False positives"", font_size=36)

        tpr_words.set_color(GREEN_B)
        fnr_words.set_color(RED_D)
        tnr_words.set_color(RED_B)
        fpr_words.set_color(GREEN_D)

        tp_cases = positive_cases[:9]
        fn_cases = positive_cases[9:]

        tpr_words.next_to(tp_cases, UP)
        fnr_words.next_to(fn_cases, DOWN)

        signs = VGroup()
        for woman in tp_cases:
            sign = Tex(""+"")
            sign.set_color(GREEN_B)
            sign.match_height(woman)
            sign.next_to(woman, RIGHT, SMALL_BUFF)
            woman.sign = sign
            signs.add(sign)
        for woman in fn_cases:
            sign = Tex(""-"")
            sign.set_color(RED)
            sign.match_width(signs[0])
            sign.next_to(woman, RIGHT, SMALL_BUFF)
            woman.sign = sign
            signs.add(sign)

        boxes = VGroup()
        for n, woman in enumerate(positive_cases):
            box = SurroundingRectangle(woman, buff=0)
            box.set_stroke(width=2)
            if woman in tp_cases:
                box.set_color(GREEN)
            else:
                box.set_color(RED)
            woman.box = box
            boxes.add(box)

        self.play(
            FadeIn(tpr_words, shift=0.2 * UP),
            ShowIncreasingSubsets(signs[:9]),
            ShowIncreasingSubsets(boxes[:9]),
        )
        self.wait()
        self.play(
            FadeIn(fnr_words, shift=0.2 * DOWN),
            Write(signs[9:]),
            ShowCreation(boxes[9:]),
        )
        self.wait()

        # Test results on cancer-free population
        negative_cases.sort(lambda p: -p[1])

        num_fp = int(len(negative_cases) * 0.09)
        fp_cases = negative_cases[:num_fp]
        tn_cases = negative_cases[num_fp:]

        new_boxes = VGroup()
        for n, woman in enumerate(negative_cases):
            box = SurroundingRectangle(woman, buff=0)
            box.set_stroke(width=2)
            if woman in fp_cases:
                box.set_color(GREEN)
            else:
                box.set_color(RED)
            woman.box = box
            new_boxes.add(box)

        fpr_words.next_to(fp_cases, UP, buff=SMALL_BUFF)
        tnr_words.next_to(tn_cases, DOWN, buff=0.2)

        self.play(
            FadeIn(fpr_words, shift=0.2 * UP),
            ShowIncreasingSubsets(new_boxes[:num_fp])
        )
        self.wait()
        self.play(
            FadeIn(tnr_words, shift=0.2 * DOWN),
            ShowIncreasingSubsets(new_boxes[num_fp:])
        )
        self.wait()

        # Consolidate boxes
        self.remove(boxes, new_boxes, population)
        for woman in population:
            woman.add(woman.box)
        self.add(population)

        # Limit view to positive cases
        for cases, nr, rect in zip([tp_cases, fp_cases], [3, 7], rects):
            cases.save_state()
            cases.generate_target()
            for case in cases.target:
                case[-1].set_stroke(width=3)
                case[-1].scale(1.1)
            cases.target.arrange_in_grid(
                n_rows=nr,
                buff=0.5 * cases[0].get_width()
            )
            cases.target.scale(0.5 / cases.target[0].get_height())
            cases.target.move_to(rect)

        fp_cases.target.shift(0.4 * DOWN)
        positive_words.save_state()
        negative_words.save_state()
        tpr_words.save_state()
        fpr_words.save_state()

        self.play(
            MoveToTarget(tp_cases),
            MoveToTarget(fp_cases),
            tpr_words.next_to, tp_cases.target, UP,
            fpr_words.next_to, fp_cases.target, UP,
            FadeOut(signs),
            positive_words[0].set_opacity, 0,
            negative_words[0].set_opacity, 0,
            positive_words[1].match_x, rects[0],
            negative_words[1].match_x, rects[1],
            LaggedStart(
                FadeOut(fn_cases, shift=DOWN),
                FadeOut(fnr_words, shift=DOWN),
                FadeOut(tn_cases, shift=DOWN),
                FadeOut(tnr_words, shift=DOWN),
            ),
        )
        self.wait()

        # Emphasize groups counts
        self.play(
            ShowCreationThenFadeOut(SurroundingRectangle(
                tpr_words[0][:1],
                stroke_width=2,
                stroke_color=WHITE,
                buff=0.05,
            )),
            LaggedStartMap(Indicate, tp_cases, color=YELLOW, lag_ratio=0.3, run_time=1),
        )
        self.wait()
        self.play(
            ShowCreationThenFadeOut(SurroundingRectangle(
                fpr_words[0][:2],
                stroke_width=2,
                stroke_color=WHITE,
                buff=0.05,
            )),
            LaggedStartMap(
                Indicate, fp_cases,
                color=GREEN_A,
                lag_ratio=0.05,
                run_time=3
            )
        )
        self.wait()

        # Final equation
        equation = Tex(
            ""P("",
            ""\\text{Have cancer }"",
            ""|"",
            ""\\text{ positive test})"",
            ""\\approx"",
            ""\\frac{9}{9 + 89}"",
            ""\\approx \\frac{1}{11}""
        )
        equation.set_color_by_tex(""cancer"", YELLOW)
        equation.set_color_by_tex(""positive"", GREEN)
        equation.to_edge(UP, buff=SMALL_BUFF)

        self.play(
            FadeIn(equation[:-1], shift=UP),
            FadeOut(title, shift=UP),
        )
        self.wait()
        self.play(Write(equation[-1]))
        self.wait()

        # Label PPV
        frame = self.camera.frame
        frame.save_state()

        ppv_words = TexText(
            ""Positive\\\\"",
            ""Predictive\\\\"",
            ""Value\\\\"",
            alignment="""",
        )
        ppv_words.next_to(equation, RIGHT, LARGE_BUFF, DOWN)
        for word in ppv_words:
            word[0].set_color(BLUE)

        ppv_rhs = Tex(
            ""={\\text{TP} \\over \\text{TP} + \\text{FP}}"",
            tex_to_color_map={
                ""\\text{TP}"": GREEN_B,
                ""\\text{FP}"": GREEN_C,
            }
        )
        ppv_rhs.next_to(ppv_words, RIGHT)
        ppv_rhs.shift(1.5 * LEFT)

        self.play(frame.scale, 1.1, {""about_edge"": DL})
        self.play(ShowIncreasingSubsets(ppv_words))
        self.wait()

        self.play(
            equation.shift, 1.5 * LEFT + 0.5 * UP,
            ppv_words.shift, 1.5 * LEFT,
            FadeIn(ppv_rhs, lag_ratio=0.1),
            frame.scale, 1.1, {""about_edge"": DL},
        )
        self.wait()

        # Go back to earlier state
        self.play(
            frame.restore,
            frame.shift, 0.5 * DOWN,
            LaggedStartMap(FadeOut, VGroup(equation, ppv_words, ppv_rhs)),
            LaggedStartMap(Restore, VGroup(
                tpr_words, tp_cases,
                fpr_words, fp_cases,
            )),
            run_time=3,
        )
        self.play(
            LaggedStartMap(FadeIn, VGroup(
                fnr_words, fn_cases,
                tnr_words, tn_cases,
            )),
        )
        self.wait()

        # Fade rects
        fade_rects = VGroup(*(
            BackgroundRectangle(
                VGroup(rect, words),
                fill_opacity=0.9,
                fill_color=BLACK,
                buff=SMALL_BUFF,
            )
            for rect, words in zip(rects, [positive_words, negative_words])
        ))

        # Sensitivity
        sens_eq = Tex(
            ""\\text{Sensitivity}"",
            ""= {9 \\over 10}"",
            ""= 90\\%""
        )
        sens_eq.next_to(rects[0], LEFT, MED_LARGE_BUFF, aligned_edge=UP)
        sens_eq.shift(DOWN)

        fnr_eq = Tex(
            ""\\text{False Negative Rate}"", ""= 10\\%""
        )
        fnr_eq.set_color(RED)
        fnr_eq.scale(0.9)
        equiv = Tex(""\\Leftrightarrow"")
        equiv.scale(1.5)
        equiv.rotate(90 * DEGREES)
        equiv.next_to(sens_eq, DOWN, MED_LARGE_BUFF)
        fnr_eq.next_to(equiv, DOWN, MED_LARGE_BUFF)

        self.play(
            frame.shift, 5 * LEFT,
            FadeIn(fade_rects[1]),
            Write(sens_eq[0]),
        )
        self.wait()
        self.play(
            TransformFromCopy(tpr_words[0][0], sens_eq[1][1]),
            Write(sens_eq[1][0]),
            Write(sens_eq[1][2:]),
        )
        self.play(Write(sens_eq[2]))
        self.wait()

        self.play(
            FadeIn(equiv, shift=0.5 * DOWN),
            FadeIn(fnr_eq, shift=1.0 * DOWN),
        )
        self.wait()

        # Transition to right side
        fade_rects[0].stretch(5, 0, about_edge=RIGHT)
        self.play(
            ApplyMethod(frame.shift, 10 * RIGHT, run_time=4),
            FadeIn(fade_rects[0], run_time=2),
            FadeOut(fade_rects[1], run_time=2),
        )

        # Specificity
        spec_eq = Tex(
            ""\\text{Specificity}"",
            ""= {901 \\over 990}"",
            ""\\approx 91\\%""
        )
        spec_eq.next_to(rects[1], RIGHT, MED_LARGE_BUFF, aligned_edge=DOWN)
        spec_eq.shift(UP)

        fpr_eq = Tex(
            ""\\text{False Positive Rate}"", ""= 9\\%""
        )
        fpr_eq.set_color(GREEN)
        fpr_eq.scale(0.9)
        equiv2 = Tex(""\\Leftrightarrow"")
        equiv2.scale(1.5)
        equiv2.rotate(90 * DEGREES)
        equiv2.next_to(spec_eq, UP, MED_LARGE_BUFF)
        fpr_eq.next_to(equiv2, UP, MED_LARGE_BUFF)

        self.play(Write(spec_eq[0]))
        self.wait()
        self.play(
            Write(spec_eq[1][0]),
            TransformFromCopy(
                tnr_words[0][:3],
                spec_eq[1][1:4],
                run_time=2,
                path_arc=30 * DEGREES,
            ),
            Write(spec_eq[1][4:]),
        )
        self.wait()
        self.play(Write(spec_eq[2]))
        self.wait()

        self.play(
            FadeIn(equiv2, shift=0.5 * UP),
            FadeIn(fpr_eq, shift=1.0 * UP),
        )
        self.wait()

        # Reset to show both kinds of accuracy
        eqs = [sens_eq, spec_eq]
        for eq, word in zip(eqs, [positive_words, negative_words]):
            eq.generate_target()
            eq.target[1].set_opacity(0)
            eq.target[2].move_to(eq.target[1], LEFT),
            eq.target.next_to(word, UP, buff=0.3)

        self.play(
            FadeOut(fade_rects[0]),
            frame.shift, 5 * LEFT,
            frame.scale, 1.1, {""about_edge"": DOWN},
            MoveToTarget(sens_eq),
            MoveToTarget(spec_eq),
            *map(FadeOut, (fnr_eq, fpr_eq, equiv, equiv2)),
            run_time=2,
        )
        self.wait()

        self.play(
            VGroup(
                fn_cases, fnr_words,
                fp_cases, fpr_words,
            ).set_opacity, 0.2,
            rate_func=there_and_back_with_pause,
            run_time=3
        )","for word in ppv_words:
    word[0].set_color(BLUE)","for word in ppv_words:
    (word_0, *word_rwordmaining) = word
    word[0].set_color(BLUE)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2020/med_test.py,https://github.com/3b1b/videos/tree/master/_2020/med_test.py,SamplePopulationBreastCancer,construct$820,"def construct(self):
        # Introduce population
        title = TexText(
            ""Sample of "", ""$1{,}000$"", "" women"",
            font_size=72,
        )
        title.add(Underline(title, color=GREY_B))
        title.to_edge(UP, buff=MED_SMALL_BUFF)
        self.add(title)

        woman = WomanIcon()
        globals()['woman'] = woman
        population = VGroup(*[woman.copy() for x in range(1000)])
        population.arrange_in_grid(
            25, 40,
            buff=LARGE_BUFF,
            fill_rows_first=False,
        )
        population.set_height(6)
        population.next_to(title, DOWN)

        counter = Integer(1000, edge_to_fix=UL)
        counter.replace(title[1])
        counter.set_value(0)

        title[1].set_opacity(0)
        self.play(
            ShowIncreasingSubsets(population),
            ChangeDecimalToValue(counter, 1000),
            run_time=5
        )
        self.remove(counter)
        title[1].set_opacity(1)
        self.wait()

        # Show true positives
        rects = VGroup(Rectangle(), Rectangle())
        rects.set_height(6)
        rects[0].set_width(4, stretch=True)
        rects[1].set_width(8, stretch=True)
        rects[0].set_stroke(YELLOW, 3)
        rects[1].set_stroke(GREY, 3)
        rects.arrange(RIGHT)
        rects.center().to_edge(DOWN, buff=MED_SMALL_BUFF)

        positive_cases = population[:10]
        negative_cases = population[10:]

        positive_cases.generate_target()
        positive_cases.target.move_to(rects[0])
        positive_cases.target.set_color(YELLOW)

        negative_cases.generate_target()
        negative_cases.target.set_height(rects[1].get_height() * 0.8)
        negative_cases.target.move_to(rects[1])

        positive_words = TexText(r""1\% "", ""Have breast cancer"", font_size=36)
        positive_words.set_color(YELLOW)
        positive_words.next_to(rects[0], UP, SMALL_BUFF)

        negative_words = TexText(r""99\% "", ""Do not have cancer"", font_size=36)
        negative_words.set_color(GREY_B)
        negative_words.next_to(rects[1], UP, SMALL_BUFF)

        self.play(
            MoveToTarget(positive_cases),
            MoveToTarget(negative_cases),
            Write(positive_words, run_time=1),
            Write(negative_words, run_time=1),
            FadeIn(rects),
        )
        self.wait()

        # Show screening
        scan_lines = VGroup(*(
            Line(
                # FRAME_WIDTH * LEFT / 2,
                FRAME_HEIGHT * DOWN / 2,
                icon.get_center(),
                stroke_width=1,
                stroke_color=interpolate_color(BLUE, GREEN, random.random())
            )
            for icon in population
        ))
        self.play(
            LaggedStartMap(
                ShowCreationThenFadeOut, scan_lines,
                lag_ratio=1 / len(scan_lines),
                run_time=3,
            )
        )
        self.wait()

        # Test results on cancer population
        tpr_words = TexText(""9 True positives"", font_size=36)
        fnr_words = TexText(""1 False negative"", font_size=36)
        tnr_words = TexText(""901 True negatives"", font_size=36)
        fpr_words = TexText(""89 False positives"", font_size=36)

        tpr_words.set_color(GREEN_B)
        fnr_words.set_color(RED_D)
        tnr_words.set_color(RED_B)
        fpr_words.set_color(GREEN_D)

        tp_cases = positive_cases[:9]
        fn_cases = positive_cases[9:]

        tpr_words.next_to(tp_cases, UP)
        fnr_words.next_to(fn_cases, DOWN)

        signs = VGroup()
        for woman in tp_cases:
            sign = Tex(""+"")
            sign.set_color(GREEN_B)
            sign.match_height(woman)
            sign.next_to(woman, RIGHT, SMALL_BUFF)
            woman.sign = sign
            signs.add(sign)
        for woman in fn_cases:
            sign = Tex(""-"")
            sign.set_color(RED)
            sign.match_width(signs[0])
            sign.next_to(woman, RIGHT, SMALL_BUFF)
            woman.sign = sign
            signs.add(sign)

        boxes = VGroup()
        for n, woman in enumerate(positive_cases):
            box = SurroundingRectangle(woman, buff=0)
            box.set_stroke(width=2)
            if woman in tp_cases:
                box.set_color(GREEN)
            else:
                box.set_color(RED)
            woman.box = box
            boxes.add(box)

        self.play(
            FadeIn(tpr_words, shift=0.2 * UP),
            ShowIncreasingSubsets(signs[:9]),
            ShowIncreasingSubsets(boxes[:9]),
        )
        self.wait()
        self.play(
            FadeIn(fnr_words, shift=0.2 * DOWN),
            Write(signs[9:]),
            ShowCreation(boxes[9:]),
        )
        self.wait()

        # Test results on cancer-free population
        negative_cases.sort(lambda p: -p[1])

        num_fp = int(len(negative_cases) * 0.09)
        fp_cases = negative_cases[:num_fp]
        tn_cases = negative_cases[num_fp:]

        new_boxes = VGroup()
        for n, woman in enumerate(negative_cases):
            box = SurroundingRectangle(woman, buff=0)
            box.set_stroke(width=2)
            if woman in fp_cases:
                box.set_color(GREEN)
            else:
                box.set_color(RED)
            woman.box = box
            new_boxes.add(box)

        fpr_words.next_to(fp_cases, UP, buff=SMALL_BUFF)
        tnr_words.next_to(tn_cases, DOWN, buff=0.2)

        self.play(
            FadeIn(fpr_words, shift=0.2 * UP),
            ShowIncreasingSubsets(new_boxes[:num_fp])
        )
        self.wait()
        self.play(
            FadeIn(tnr_words, shift=0.2 * DOWN),
            ShowIncreasingSubsets(new_boxes[num_fp:])
        )
        self.wait()

        # Consolidate boxes
        self.remove(boxes, new_boxes, population)
        for woman in population:
            woman.add(woman.box)
        self.add(population)

        # Limit view to positive cases
        for cases, nr, rect in zip([tp_cases, fp_cases], [3, 7], rects):
            cases.save_state()
            cases.generate_target()
            for case in cases.target:
                case[-1].set_stroke(width=3)
                case[-1].scale(1.1)
            cases.target.arrange_in_grid(
                n_rows=nr,
                buff=0.5 * cases[0].get_width()
            )
            cases.target.scale(0.5 / cases.target[0].get_height())
            cases.target.move_to(rect)

        fp_cases.target.shift(0.4 * DOWN)
        positive_words.save_state()
        negative_words.save_state()
        tpr_words.save_state()
        fpr_words.save_state()

        self.play(
            MoveToTarget(tp_cases),
            MoveToTarget(fp_cases),
            tpr_words.next_to, tp_cases.target, UP,
            fpr_words.next_to, fp_cases.target, UP,
            FadeOut(signs),
            positive_words[0].set_opacity, 0,
            negative_words[0].set_opacity, 0,
            positive_words[1].match_x, rects[0],
            negative_words[1].match_x, rects[1],
            LaggedStart(
                FadeOut(fn_cases, shift=DOWN),
                FadeOut(fnr_words, shift=DOWN),
                FadeOut(tn_cases, shift=DOWN),
                FadeOut(tnr_words, shift=DOWN),
            ),
        )
        self.wait()

        # Emphasize groups counts
        self.play(
            ShowCreationThenFadeOut(SurroundingRectangle(
                tpr_words[0][:1],
                stroke_width=2,
                stroke_color=WHITE,
                buff=0.05,
            )),
            LaggedStartMap(Indicate, tp_cases, color=YELLOW, lag_ratio=0.3, run_time=1),
        )
        self.wait()
        self.play(
            ShowCreationThenFadeOut(SurroundingRectangle(
                fpr_words[0][:2],
                stroke_width=2,
                stroke_color=WHITE,
                buff=0.05,
            )),
            LaggedStartMap(
                Indicate, fp_cases,
                color=GREEN_A,
                lag_ratio=0.05,
                run_time=3
            )
        )
        self.wait()

        # Final equation
        equation = Tex(
            ""P("",
            ""\\text{Have cancer }"",
            ""|"",
            ""\\text{ positive test})"",
            ""\\approx"",
            ""\\frac{9}{9 + 89}"",
            ""\\approx \\frac{1}{11}""
        )
        equation.set_color_by_tex(""cancer"", YELLOW)
        equation.set_color_by_tex(""positive"", GREEN)
        equation.to_edge(UP, buff=SMALL_BUFF)

        self.play(
            FadeIn(equation[:-1], shift=UP),
            FadeOut(title, shift=UP),
        )
        self.wait()
        self.play(Write(equation[-1]))
        self.wait()

        # Label PPV
        frame = self.camera.frame
        frame.save_state()

        ppv_words = TexText(
            ""Positive\\\\"",
            ""Predictive\\\\"",
            ""Value\\\\"",
            alignment="""",
        )
        ppv_words.next_to(equation, RIGHT, LARGE_BUFF, DOWN)
        for word in ppv_words:
            word[0].set_color(BLUE)

        ppv_rhs = Tex(
            ""={\\text{TP} \\over \\text{TP} + \\text{FP}}"",
            tex_to_color_map={
                ""\\text{TP}"": GREEN_B,
                ""\\text{FP}"": GREEN_C,
            }
        )
        ppv_rhs.next_to(ppv_words, RIGHT)
        ppv_rhs.shift(1.5 * LEFT)

        self.play(frame.scale, 1.1, {""about_edge"": DL})
        self.play(ShowIncreasingSubsets(ppv_words))
        self.wait()

        self.play(
            equation.shift, 1.5 * LEFT + 0.5 * UP,
            ppv_words.shift, 1.5 * LEFT,
            FadeIn(ppv_rhs, lag_ratio=0.1),
            frame.scale, 1.1, {""about_edge"": DL},
        )
        self.wait()

        # Go back to earlier state
        self.play(
            frame.restore,
            frame.shift, 0.5 * DOWN,
            LaggedStartMap(FadeOut, VGroup(equation, ppv_words, ppv_rhs)),
            LaggedStartMap(Restore, VGroup(
                tpr_words, tp_cases,
                fpr_words, fp_cases,
            )),
            run_time=3,
        )
        self.play(
            LaggedStartMap(FadeIn, VGroup(
                fnr_words, fn_cases,
                tnr_words, tn_cases,
            )),
        )
        self.wait()

        # Fade rects
        fade_rects = VGroup(*(
            BackgroundRectangle(
                VGroup(rect, words),
                fill_opacity=0.9,
                fill_color=BLACK,
                buff=SMALL_BUFF,
            )
            for rect, words in zip(rects, [positive_words, negative_words])
        ))

        # Sensitivity
        sens_eq = Tex(
            ""\\text{Sensitivity}"",
            ""= {9 \\over 10}"",
            ""= 90\\%""
        )
        sens_eq.next_to(rects[0], LEFT, MED_LARGE_BUFF, aligned_edge=UP)
        sens_eq.shift(DOWN)

        fnr_eq = Tex(
            ""\\text{False Negative Rate}"", ""= 10\\%""
        )
        fnr_eq.set_color(RED)
        fnr_eq.scale(0.9)
        equiv = Tex(""\\Leftrightarrow"")
        equiv.scale(1.5)
        equiv.rotate(90 * DEGREES)
        equiv.next_to(sens_eq, DOWN, MED_LARGE_BUFF)
        fnr_eq.next_to(equiv, DOWN, MED_LARGE_BUFF)

        self.play(
            frame.shift, 5 * LEFT,
            FadeIn(fade_rects[1]),
            Write(sens_eq[0]),
        )
        self.wait()
        self.play(
            TransformFromCopy(tpr_words[0][0], sens_eq[1][1]),
            Write(sens_eq[1][0]),
            Write(sens_eq[1][2:]),
        )
        self.play(Write(sens_eq[2]))
        self.wait()

        self.play(
            FadeIn(equiv, shift=0.5 * DOWN),
            FadeIn(fnr_eq, shift=1.0 * DOWN),
        )
        self.wait()

        # Transition to right side
        fade_rects[0].stretch(5, 0, about_edge=RIGHT)
        self.play(
            ApplyMethod(frame.shift, 10 * RIGHT, run_time=4),
            FadeIn(fade_rects[0], run_time=2),
            FadeOut(fade_rects[1], run_time=2),
        )

        # Specificity
        spec_eq = Tex(
            ""\\text{Specificity}"",
            ""= {901 \\over 990}"",
            ""\\approx 91\\%""
        )
        spec_eq.next_to(rects[1], RIGHT, MED_LARGE_BUFF, aligned_edge=DOWN)
        spec_eq.shift(UP)

        fpr_eq = Tex(
            ""\\text{False Positive Rate}"", ""= 9\\%""
        )
        fpr_eq.set_color(GREEN)
        fpr_eq.scale(0.9)
        equiv2 = Tex(""\\Leftrightarrow"")
        equiv2.scale(1.5)
        equiv2.rotate(90 * DEGREES)
        equiv2.next_to(spec_eq, UP, MED_LARGE_BUFF)
        fpr_eq.next_to(equiv2, UP, MED_LARGE_BUFF)

        self.play(Write(spec_eq[0]))
        self.wait()
        self.play(
            Write(spec_eq[1][0]),
            TransformFromCopy(
                tnr_words[0][:3],
                spec_eq[1][1:4],
                run_time=2,
                path_arc=30 * DEGREES,
            ),
            Write(spec_eq[1][4:]),
        )
        self.wait()
        self.play(Write(spec_eq[2]))
        self.wait()

        self.play(
            FadeIn(equiv2, shift=0.5 * UP),
            FadeIn(fpr_eq, shift=1.0 * UP),
        )
        self.wait()

        # Reset to show both kinds of accuracy
        eqs = [sens_eq, spec_eq]
        for eq, word in zip(eqs, [positive_words, negative_words]):
            eq.generate_target()
            eq.target[1].set_opacity(0)
            eq.target[2].move_to(eq.target[1], LEFT),
            eq.target.next_to(word, UP, buff=0.3)

        self.play(
            FadeOut(fade_rects[0]),
            frame.shift, 5 * LEFT,
            frame.scale, 1.1, {""about_edge"": DOWN},
            MoveToTarget(sens_eq),
            MoveToTarget(spec_eq),
            *map(FadeOut, (fnr_eq, fpr_eq, equiv, equiv2)),
            run_time=2,
        )
        self.wait()

        self.play(
            VGroup(
                fn_cases, fnr_words,
                fp_cases, fpr_words,
            ).set_opacity, 0.2,
            rate_func=there_and_back_with_pause,
            run_time=3
        )","for case in cases.target:
    case[-1].set_stroke(width=3)
    case[-1].scale(1.1)","for case in cases.target:
    (*case_rcasemaining, case_last) = case
    case[-1].set_stroke(width=3)
    case[-1].scale(1.1)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_last = e
variable mapping:
e_last: e[-1]",,,,,,,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/tools/verify_headers.py,https://github.com/Qiskit/qiskit-terra/tree/master/tools/verify_headers.py,,_main$81,"def _main():
    default_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))), ""qiskit""
    )
    parser = argparse.ArgumentParser(description=""Check file headers."")
    parser.add_argument(
        ""paths"",
        type=str,
        nargs=""*"",
        default=[default_path],
        help=""Paths to scan by default uses ../qiskit from the script"",
    )
    args = parser.parse_args()
    files = discover_files(args.paths)
    with multiprocessing.Pool() as pool:
        res = pool.map(validate_header, files)
    failed_files = [x for x in res if x[1] is False]
    if len(failed_files) > 0:
        for failed_file in failed_files:
            sys.stderr.write(""%s failed header check because:\n"" % failed_file[0])
            sys.stderr.write(""%s\n\n"" % failed_file[2])
        sys.exit(1)
    sys.exit(0)","for failed_file in failed_files:
    sys.stderr.write('%s failed header check because:\n' % failed_file[0])
    sys.stderr.write('%s\n\n' % failed_file[2])","for failed_file in failed_files:
    (failed_file_0, _, failed_file_2, *failed_file_rfailed_filemaining) = failed_file
    sys.stderr.write('%s failed header check because:\n' % failed_file[0])
    sys.stderr.write('%s\n\n' % failed_file[2])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, _, e_2, *e_remaining = e
variable mapping:
e_0: e[0]
e_2: e[2]",,,,,,,
lbry-sdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lbry-sdk/lbry/error/generate.py,https://github.com/lbryio/lbry-sdk/tree/master/lbry/error/generate.py,,analyze$127,"def analyze():
    errors = {e.class_name: [] for e in get_errors() if e.is_leaf}
    here = Path(__file__).absolute().parents[0]
    module = here.parent
    for file_path in module.glob('**/*.py'):
        if here in file_path.parents:
            continue
        with open(file_path) as src_file:
            src = src_file.read()
            for error in errors.keys():
                found = src.count(error)
                if found > 0:
                    errors[error].append((file_path, found))

    print('Unused Errors:\n')
    for error, used in errors.items():
        if used:
            print(f' - {error}')
            for use in used:
                print(f'   {use[0].relative_to(module.parent)} {use[1]}')
            print('')

    print('')
    print('Unused Errors:')
    for error, used in errors.items():
        if not used:
            print(f' - {error}')","for use in used:
    print(f'   {use[0].relative_to(module.parent)} {use[1]}')","for use in used:
    (use_0, use_1, *_) = use
    print(f'   {use[0].relative_to(module.parent)} {use[1]}')","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
superpaper,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/superpaper/superpaper/gui.py,https://github.com/hhannine/superpaper/tree/master/superpaper/gui.py,WallpaperPreviewPanel,export_offsets$1993,"def export_offsets(self, display_sys):
        """"""Read dragged preview positions, normalize them to be positive,
        and scale sizes up to old canvas size.""""""
        # DragShape sizes and positions need to be scaled up by true_canvas_old_w/preview_canv_w
        prev_canv_w = self.st_bmp_canvas.GetSize()[0]
        true_canv_w = self.get_canvas(display_sys.get_disp_list(True))[0]
        scaling = true_canv_w / prev_canv_w
        sanitzed_offs = self.sanitize_shape_offs()
        ppi_norm_offsets = []
        for off in sanitzed_offs:
            ppi_norm_offsets.append(
                (
                    off[0]*scaling,
                    off[1]*scaling
                )
            )
        display_sys.update_ppinorm_offsets(ppi_norm_offsets, bezels_included=False)","for off in sanitzed_offs:
    ppi_norm_offsets.append((off[0] * scaling, off[1] * scaling))","for off in sanitzed_offs:
    (off_0, off_1, *_) = off
    ppi_norm_offsets.append((off[0] * scaling, off[1] * scaling))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
napalm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/napalm/napalm/junos/junos.py,https://github.com/napalm-automation/napalm/tree/master/napalm/junos/junos.py,JunOSDriver,get_snmp_information$1884,"def get_snmp_information(self):
        """"""Return the SNMP configuration.""""""
        snmp_information = {}

        snmp_config = junos_views.junos_snmp_config_table(self.device)
        snmp_config.get(options=self.junos_config_options)
        snmp_items = snmp_config.items()

        if not snmp_items:
            return snmp_information

        snmp_information = {
            str(ele[0]): ele[1] if ele[1] else """" for ele in snmp_items[0][1]
        }

        snmp_information[""community""] = {}
        communities_table = snmp_information.pop(""communities_table"")
        if not communities_table:
            return snmp_information

        for community in communities_table.items():
            community_name = str(community[0])
            community_details = {""acl"": """"}
            community_details.update(
                {
                    str(ele[0]): str(
                        ele[1]
                        if ele[0] != ""mode""
                        else C.SNMP_AUTHORIZATION_MODE_MAP.get(ele[1])
                    )
                    for ele in community[1]
                }
            )
            snmp_information[""community""][community_name] = community_details

        return snmp_information","for community in communities_table.items():
    community_name = str(community[0])
    community_details = {'acl': ''}
    community_details.update({str(ele[0]): str(ele[1] if ele[0] != 'mode' else C.SNMP_AUTHORIZATION_MODE_MAP.get(ele[1])) for ele in community[1]})
    snmp_information['community'][community_name] = community_details","for community in communities_table.items():
    (community_0, community_1, *_) = community
    community_name = str(community[0])
    community_details = {'acl': ''}
    community_details.update({str(ele[0]): str(ele[1] if ele[0] != 'mode' else C.SNMP_AUTHORIZATION_MODE_MAP.get(ele[1])) for ele in community[1]})
    snmp_information['community'][community_name] = community_details","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
moviepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moviepy/tests/test_compositing.py,https://github.com/Zulko/moviepy/tree/master/tests/test_compositing.py,,test_slide_out$173,"def test_slide_out():
    duration = 0.1
    size = (11, 1)
    fps = 10
    color = (255, 0, 0)

    # left and right sides
    clip = ColorClip(
        color=color,
        duration=duration,
        size=size,
    ).with_fps(fps)

    for side in [""left"", ""right""]:
        new_clip = CompositeVideoClip([slide_out(clip, duration, side)])

        for t in np.arange(0, duration, duration / fps):
            n_reds, n_reds_expected = (0, round(11 - t * 100, 6))

            if t:
                assert n_reds_expected

            for r, g, b in new_clip.get_frame(t)[0]:
                if r == color[0] and g == color[1] and g == color[2]:
                    n_reds += 1

            assert n_reds == n_reds_expected

    # top and bottom sides
    clip = ColorClip(
        color=color,
        duration=duration,
        size=(size[1], size[0]),
    ).with_fps(fps)

    for side in [""top"", ""bottom""]:
        new_clip = CompositeVideoClip([slide_out(clip, duration, side)])
        for t in np.arange(0, duration, duration / fps):
            n_reds, n_reds_expected = (0, round(11 - t * 100, 6))

            if t:
                assert n_reds_expected

            for row in new_clip.get_frame(t):
                r, g, b = row[0]

                if r == color[0] and g == color[1] and g == color[2]:
                    n_reds += 1

            assert n_reds == n_reds_expected","for row in new_clip.get_frame(t):
    (r, g, b) = row[0]
    if r == color[0] and g == color[1] and (g == color[2]):
        n_reds += 1","for row in new_clip.get_frame(t):
    (row_0, *row_rrowmaining) = row
    (r, g, b) = row[0]
    if r == color[0] and g == color[1] and (g == color[2]):
        n_reds += 1","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
PaddleVideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleVideo/data/ucf101/build_ucf101_file_list.py,https://github.com/PaddlePaddle/PaddleVideo/tree/master/data/ucf101/build_ucf101_file_list.py,,build_set_list$49,"def build_set_list(set_list):
        rgb_list = list()
        for item in set_list:
            if item[0] not in frame_info:
                continue
            elif frame_info[item[0]][1] > 0:
                rgb_cnt = frame_info[item[0]][1]
                rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
            else:
                rgb_list.append('{} {}\n'.format(item[0], item[1]))
        if shuffle:
            random.shuffle(rgb_list)
        return rgb_list","for item in set_list:
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))","for item in set_list:
    (item_0, item_1, *_) = item
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
quay,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quay/test/test_ldap.py,https://github.com/quay/quay/tree/master/test/test_ldap.py,TestLDAP,test_iterate_group_members$556,"def test_iterate_group_members(self):
        with mock_ldap() as ldap:
            (it, err) = ldap.iterate_group_members(
                {""group_dn"": ""cn=AwesomeFolk""}, disable_pagination=True
            )
            self.assertIsNone(err)

            results = list(it)
            self.assertEqual(4, len(results))

            first = results[0][0]
            second = results[1][0]
            third = results[2][0]
            fourth = results[3][0]

            assert all(
                x in [u[0].id for u in results]
                for x in [""testy"", ""someuser"", ""somesuperuser"", ""somerestricteduser""]
            )

            for i in results:
                u = i[0]
                if u.id == ""testy"":
                    self.assertEqual(""testy"", u.id)
                    self.assertEqual(""testy"", u.username)
                    self.assertEqual(""bar@baz.com"", u.email)

                if u.id == ""someuser"":
                    self.assertEqual(""someuser"", u.id)
                    self.assertEqual(""someuser"", u.username)
                    self.assertEqual(""foo@bar.com"", u.email)

                if u.id == ""somesuperuser"":
                    self.assertEqual(""somesuperuser"", u.id)
                    self.assertEqual(""somesuperuser"", u.username)
                    self.assertEqual(""superfoo@bar.com"", u.email)

                if u.id == ""somerestricteduser"":
                    self.assertEqual(""somerestricteduser"", u.id)
                    self.assertEqual(""somerestricteduser"", u.username)
                    self.assertEqual(""restrictedfoo@bar.com"", u.email)","for i in results:
    u = i[0]
    if u.id == 'testy':
        self.assertEqual('testy', u.id)
        self.assertEqual('testy', u.username)
        self.assertEqual('bar@baz.com', u.email)
    if u.id == 'someuser':
        self.assertEqual('someuser', u.id)
        self.assertEqual('someuser', u.username)
        self.assertEqual('foo@bar.com', u.email)
    if u.id == 'somesuperuser':
        self.assertEqual('somesuperuser', u.id)
        self.assertEqual('somesuperuser', u.username)
        self.assertEqual('superfoo@bar.com', u.email)
    if u.id == 'somerestricteduser':
        self.assertEqual('somerestricteduser', u.id)
        self.assertEqual('somerestricteduser', u.username)
        self.assertEqual('restrictedfoo@bar.com', u.email)","for i in results:
    (i_0, *i_rimaining) = i
    u = i[0]
    if u.id == 'testy':
        self.assertEqual('testy', u.id)
        self.assertEqual('testy', u.username)
        self.assertEqual('bar@baz.com', u.email)
    if u.id == 'someuser':
        self.assertEqual('someuser', u.id)
        self.assertEqual('someuser', u.username)
        self.assertEqual('foo@bar.com', u.email)
    if u.id == 'somesuperuser':
        self.assertEqual('somesuperuser', u.id)
        self.assertEqual('somesuperuser', u.username)
        self.assertEqual('superfoo@bar.com', u.email)
    if u.id == 'somerestricteduser':
        self.assertEqual('somerestricteduser', u.id)
        self.assertEqual('somerestricteduser', u.username)
        self.assertEqual('restrictedfoo@bar.com', u.email)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
edx-platform,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/lms/djangoapps/grades/management/commands/compute_grades.py,https://github.com/edx/edx-platform/tree/master/lms/djangoapps/grades/management/commands/compute_grades.py,Command,_shuffled_task_kwargs$92,"def _shuffled_task_kwargs(self, options):
        """"""
        Iterate over all task keyword arguments in random order.

        Randomizing them will help even out the load on the task workers,
        though it will not entirely prevent the possibility of spikes.  It will
        also make the overall time to completion more predictable.
        """"""
        all_args = []
        estimate_first_attempted = options['estimate_first_attempted']
        for course_key in self._get_course_keys(options):
            # This is a tuple to reduce memory consumption.
            # The dictionaries with their extra overhead will be created
            # and consumed one at a time.
            for task_arg_tuple in tasks._course_task_args(course_key, **options):  # lint-amnesty, pylint: disable=protected-access
                all_args.append(task_arg_tuple)

        all_args.sort(key=lambda x: hashlib.md5(f'{x!r}'.encode('utf-8')).digest())

        for args in all_args:
            yield {
                'course_key': args[0],
                'offset': args[1],
                'batch_size': args[2],
                'estimate_first_attempted': estimate_first_attempted,
            }","for args in all_args:
    yield {'course_key': args[0], 'offset': args[1], 'batch_size': args[2], 'estimate_first_attempted': estimate_first_attempted}","for args in all_args:
    (args_0, args_1, args_2, *_) = args
    yield {'course_key': args[0], 'offset': args[1], 'batch_size': args[2], 'estimate_first_attempted': estimate_first_attempted}","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
dynaconf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dynaconf/dynaconf/vendor_src/ruamel/yaml/resolver.py,https://github.com/rochacbruno/dynaconf/tree/master/dynaconf/vendor_src/ruamel/yaml/resolver.py,VersionedResolver,versioned_resolver$345,"def versioned_resolver(self):
        # type: () -> Any
        """"""
        select the resolver based on the version we are parsing
        """"""
        version = self.processing_version
        if version not in self._version_implicit_resolver:
            for x in implicit_resolvers:
                if version in x[0]:
                    self.add_version_implicit_resolver(version, x[1], x[2], x[3])
        return self._version_implicit_resolver[version]","for x in implicit_resolvers:
    if version in x[0]:
        self.add_version_implicit_resolver(version, x[1], x[2], x[3])","for x in implicit_resolvers:
    (x_0, x_1, x_2, x_3, *_) = x
    if version in x[0]:
        self.add_version_implicit_resolver(version, x[1], x[2], x[3])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
DeepKE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/name_entity_re/few_shot/module/datasets.py,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/name_entity_re/few_shot/module/datasets.py,ConllNERDataset,collate_fn$205,"def collate_fn(self, batch):
        src_tokens, src_seq_len, first  = [], [], []
        tgt_tokens, tgt_seq_len, target_span = [], [], []
        if self.mode == ""test"":
            raw_words = []
            for tup in batch:
                src_tokens.append(tup[0])
                src_seq_len.append(tup[1])
                first.append(tup[2])
                raw_words.append(tup[3])
            src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
            first = pad_sequence(first, batch_first=True, padding_value=0)
            return src_tokens, torch.stack(src_seq_len, 0), first, raw_words

        for tup in batch:
            src_tokens.append(tup[0])
            tgt_tokens.append(tup[1])
            src_seq_len.append(tup[2])
            tgt_seq_len.append(tup[3])
            first.append(tup[4])
            target_span.append(tup[5])
        src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
        tgt_tokens = pad_sequence(tgt_tokens, batch_first=True, padding_value=1)
        first = pad_sequence(first, batch_first=True, padding_value=0)
        return src_tokens, tgt_tokens, torch.stack(src_seq_len, 0), torch.stack(tgt_seq_len, 0), first, target_span","for tup in batch:
    src_tokens.append(tup[0])
    tgt_tokens.append(tup[1])
    src_seq_len.append(tup[2])
    tgt_seq_len.append(tup[3])
    first.append(tup[4])
    target_span.append(tup[5])","for tup in batch:
    (tup_0, tup_1, tup_2, tup_3, tup_4, tup_5, *_) = tup
    src_tokens.append(tup[0])
    tgt_tokens.append(tup[1])
    src_seq_len.append(tup[2])
    tgt_seq_len.append(tup[3])
    first.append(tup[4])
    target_span.append(tup[5])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4, e_5 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]
e_5: e[5]",,,,,,,
DeepKE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/name_entity_re/few_shot/module/datasets.py,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/name_entity_re/few_shot/module/datasets.py,ConllNERDataset,collate_fn$205,"def collate_fn(self, batch):
        src_tokens, src_seq_len, first  = [], [], []
        tgt_tokens, tgt_seq_len, target_span = [], [], []
        if self.mode == ""test"":
            raw_words = []
            for tup in batch:
                src_tokens.append(tup[0])
                src_seq_len.append(tup[1])
                first.append(tup[2])
                raw_words.append(tup[3])
            src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
            first = pad_sequence(first, batch_first=True, padding_value=0)
            return src_tokens, torch.stack(src_seq_len, 0), first, raw_words

        for tup in batch:
            src_tokens.append(tup[0])
            tgt_tokens.append(tup[1])
            src_seq_len.append(tup[2])
            tgt_seq_len.append(tup[3])
            first.append(tup[4])
            target_span.append(tup[5])
        src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
        tgt_tokens = pad_sequence(tgt_tokens, batch_first=True, padding_value=1)
        first = pad_sequence(first, batch_first=True, padding_value=0)
        return src_tokens, tgt_tokens, torch.stack(src_seq_len, 0), torch.stack(tgt_seq_len, 0), first, target_span","for tup in batch:
    src_tokens.append(tup[0])
    src_seq_len.append(tup[1])
    first.append(tup[2])
    raw_words.append(tup[3])","for tup in batch:
    (tup_0, tup_1, tup_2, tup_3, *_) = tup
    src_tokens.append(tup[0])
    src_seq_len.append(tup[1])
    first.append(tup[2])
    raw_words.append(tup[3])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
PyQt5-Apps,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyQt5-Apps/words-recorder/main.py,https://github.com/taseikyo/PyQt5-Apps/tree/master/words-recorder/main.py,MainWindow,query$155,"def query(self, w):
        origin = w.input_origin.text().replace(' ', '')
        if origin:
            if db:
                try:
                    sql = ""SELECT origin, trans FROM words WHERE origin = '%s'"" % origin
                    print(sql)
                    num = cursor.execute(sql)
                    if num:
                        for r in cursor:
                            w.input_trans.setText(r[1])
                            # self.messageBox(""%s: %s""%(r))
                except Exception as e:
                    self.messageBox(""insert data failed!\nerror msg: %s""%(e.args[1]))
            else:
                self.messageBox(""connect to the database first!\nclick the button 'File-connect'"")","for r in cursor:
    w.input_trans.setText(r[1])","for r in cursor:
    (_, r_1, *r_rrmaining) = r
    w.input_trans.setText(r[1])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/mysql.py,https://github.com/saltstack/salt/tree/master/salt/modules/mysql.py,,__do_query_into_hash$2673,"def __do_query_into_hash(conn, sql_str):
    """"""
    Perform the query that is passed to it (sql_str).

    Returns:
       results in a dict.

    """"""
    mod = sys._getframe().f_code.co_name
    log.debug(""%s<--(%s)"", mod, sql_str)

    rtn_results = []

    try:
        cursor = conn.cursor()
    except MySQLdb.MySQLError:
        log.error(""%s: Can't get cursor for SQL->%s"", mod, sql_str)
        cursor.close()
        log.debug(""%s-->"", mod)
        return rtn_results

    try:
        _execute(cursor, sql_str)
    except MySQLdb.MySQLError:
        log.error(""%s: try to execute : SQL->%s"", mod, sql_str)
        cursor.close()
        log.debug(""%s-->"", mod)
        return rtn_results

    qrs = cursor.fetchall()

    for row_data in qrs:
        col_cnt = 0
        row = {}
        for col_data in cursor.description:
            col_name = col_data[0]
            row[col_name] = row_data[col_cnt]
            col_cnt += 1

        rtn_results.append(row)

    cursor.close()
    log.debug(""%s-->"", mod)
    return rtn_results","for row_data in qrs:
    col_cnt = 0
    row = {}
    for col_data in cursor.description:
        col_name = col_data[0]
        row[col_name] = row_data[col_cnt]
        col_cnt += 1
    rtn_results.append(row)","for row_data in qrs:
    (row_data_col_cnt, *row_data_rrow_datamaining) = row_data
    col_cnt = 0
    row = {}
    for col_data in cursor.description:
        col_name = col_data[0]
        row[col_name] = row_data[col_cnt]
        col_cnt += 1
    rtn_results.append(row)",Cannot refactor,Cannot refactor,2,,,"Answer: It depends on the value of col_cnt and the type of iterable object ""e"".
Iterable Unpacking: If the iterable object ""e"" is a sequence type like a list or tuple and col_cnt is an integer type constant that is within the range of the length of ""e"", then the answer is Yes. In this case, the Python code to unpack ""e"" to get unpacked elements and the corresponding variable mapping for each unpacked element is as follows:
e_col_cnt, *e_remaining = e
variable mapping:
e_col_cnt: e[col_cnt]
Otherwise, if the iterable object ""e"" is not a sequence type or col_cnt is not an integer type constant or it is out of range, then the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/mysql.py,https://github.com/saltstack/salt/tree/master/salt/modules/mysql.py,,__do_query_into_hash$2673,"def __do_query_into_hash(conn, sql_str):
    """"""
    Perform the query that is passed to it (sql_str).

    Returns:
       results in a dict.

    """"""
    mod = sys._getframe().f_code.co_name
    log.debug(""%s<--(%s)"", mod, sql_str)

    rtn_results = []

    try:
        cursor = conn.cursor()
    except MySQLdb.MySQLError:
        log.error(""%s: Can't get cursor for SQL->%s"", mod, sql_str)
        cursor.close()
        log.debug(""%s-->"", mod)
        return rtn_results

    try:
        _execute(cursor, sql_str)
    except MySQLdb.MySQLError:
        log.error(""%s: try to execute : SQL->%s"", mod, sql_str)
        cursor.close()
        log.debug(""%s-->"", mod)
        return rtn_results

    qrs = cursor.fetchall()

    for row_data in qrs:
        col_cnt = 0
        row = {}
        for col_data in cursor.description:
            col_name = col_data[0]
            row[col_name] = row_data[col_cnt]
            col_cnt += 1

        rtn_results.append(row)

    cursor.close()
    log.debug(""%s-->"", mod)
    return rtn_results","for col_data in cursor.description:
    col_name = col_data[0]
    row[col_name] = row_data[col_cnt]
    col_cnt += 1","for col_data in cursor.description:
    (col_data_0, *col_data_rcol_datamaining) = col_data
    col_name = col_data[0]
    row[col_name] = row_data[col_cnt]
    col_cnt += 1","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
hydra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydra/hydra/_internal/hydra.py,https://github.com/facebookresearch/hydra/tree/master/hydra/_internal/hydra.py,Hydra,_print_plugins_profiling_info$449,"def _print_plugins_profiling_info(self, top_n: int) -> None:
        assert log is not None
        stats = Plugins.instance().get_stats()
        if stats is None:
            return

        items = list(stats.modules_import_time.items())
        # hide anything that took less than 5ms
        filtered = filter(lambda x: x[1] > 0.0005, items)
        sorted_items = sorted(filtered, key=lambda x: x[1], reverse=True)

        top_n = max(len(sorted_items), top_n)
        box: List[List[str]] = [[""Module"", ""Sec""]]

        for item in sorted_items[0:top_n]:
            box.append([item[0], f""{item[1]:.3f}""])
        padding = get_column_widths(box)

        log.debug("""")
        self._log_header(header=""Profiling information"", filler=""*"")
        self._log_header(
            header=f""Total plugins scan time : {stats.total_time:.3f} seconds"",
            filler=""-"",
        )

        header = f""| {box[0][0].ljust(padding[0])} | {box[0][1].ljust(padding[1])} |""
        self._log_header(header=header, filler=""-"")
        del box[0]

        for row in box:
            a = row[0].ljust(padding[0])
            b = row[1].ljust(padding[1])
            log.debug(f""| {a} | {b} |"")

        self._log_footer(header=header, filler=""-"")","for item in sorted_items[0:top_n]:
    box.append([item[0], f'{item[1]:.3f}'])","for item in sorted_items[0:top_n]:
    (item_0, item_1, *_) = item
    box.append([item[0], f'{item[1]:.3f}'])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
hydra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydra/hydra/_internal/hydra.py,https://github.com/facebookresearch/hydra/tree/master/hydra/_internal/hydra.py,Hydra,_print_plugins_profiling_info$449,"def _print_plugins_profiling_info(self, top_n: int) -> None:
        assert log is not None
        stats = Plugins.instance().get_stats()
        if stats is None:
            return

        items = list(stats.modules_import_time.items())
        # hide anything that took less than 5ms
        filtered = filter(lambda x: x[1] > 0.0005, items)
        sorted_items = sorted(filtered, key=lambda x: x[1], reverse=True)

        top_n = max(len(sorted_items), top_n)
        box: List[List[str]] = [[""Module"", ""Sec""]]

        for item in sorted_items[0:top_n]:
            box.append([item[0], f""{item[1]:.3f}""])
        padding = get_column_widths(box)

        log.debug("""")
        self._log_header(header=""Profiling information"", filler=""*"")
        self._log_header(
            header=f""Total plugins scan time : {stats.total_time:.3f} seconds"",
            filler=""-"",
        )

        header = f""| {box[0][0].ljust(padding[0])} | {box[0][1].ljust(padding[1])} |""
        self._log_header(header=header, filler=""-"")
        del box[0]

        for row in box:
            a = row[0].ljust(padding[0])
            b = row[1].ljust(padding[1])
            log.debug(f""| {a} | {b} |"")

        self._log_footer(header=header, filler=""-"")","for row in box:
    a = row[0].ljust(padding[0])
    b = row[1].ljust(padding[1])
    log.debug(f'| {a} | {b} |')","for row in box:
    (row_0, row_1, *_) = row
    a = row[0].ljust(padding[0])
    b = row[1].ljust(padding[1])
    log.debug(f'| {a} | {b} |')","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
chartpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chartpy/chartpy/engine.py,https://github.com/cuemacro/chartpy/tree/master/chartpy/engine.py,EnginePlotly,plot_chart$1458,"def plot_chart(self, data_frame, style, chart_type):

        # Special case if we have a pre-created Plotly object
        if isinstance(data_frame, Figure):
            return self.publish_plot(data_frame, style)

        mode = 'lines'

        if style is None: style = Style()

        marker_size = 1

        x = '';
        y = '';
        z = ''

        scale = 1

        try:
            # Adjust sizing if offline_html format
            if (
                    style.plotly_plot_mode == 'offline_html' and style.scale_factor > 0):
                scale = float(2.0 / 3.0)
        except:
            pass

        if style.plotly_webgl:
            plotly.graph_objs.Scatter = plotly.graph_objs.Scattergl

        # Check other plots implemented by Cufflinks
        cm = ColorMaster()

        # Create figure
        data_frame_list = self.split_data_frame_to_list(data_frame, style)
        fig_list = []
        cols = []

        # If we provide a list of Figures this will get ignored
        try:
            for data_frame in data_frame_list:
                cols.append(data_frame.columns)

            cols = list(numpy.array(cols).flat)

            # Get all the correct colors (and construct gradients if necessary eg. from 'Blues')
            # need to change to strings for cufflinks
            color_list = cm.create_color_list(style, [], cols=cols)
            color_spec = []

            # If no colors are specified then just use our default color set from chart constants
            if color_list == [None] * len(color_list):
                color_spec = [None] * len(color_list)

                for i in range(0, len(color_list)):

                    # Get the color
                    if color_spec[i] is None:
                        color_spec[i] = self.get_color_list(i)

                    try:
                        color_spec[i] = matplotlib.colors.rgb2hex(
                            color_spec[i])
                    except:
                        pass

            else:
                # Otherwise assume all the colors are rgba
                for color in color_list:
                    color = 'rgba' + str(color)
                    color_spec.append(color)
        except Exception as e:
            pass

        start = 0

        title_list = style.title

        if not (isinstance(title_list, list)):
            title_list = [style.title] * len(data_frame_list)

        # Go through each data_frame in the list and plot
        for i in range(0, len(data_frame_list)):
            data_frame = data_frame_list[i]
            title = title_list[i]

            if isinstance(data_frame, Figure):
                fig = data_frame
            else:

                if style.drop_na:
                    data_frame = data_frame.dropna()

                if isinstance(chart_type, list):
                    chart_type_ord = chart_type[i]
                else:
                    chart_type_ord = chart_type

                end = start + len(data_frame.columns)
                color_spec1 = color_spec[start:start + end]
                start = end

                # Special call for choropleth (uses Plotly API directly)
                # Special case for map/choropleth which has yet to be implemented in Cufflinks
                # will likely remove this in the future
                if chart_type_ord == 'choropleth':

                    for col in data_frame.columns:
                        try:
                            data_frame[col] = data_frame[col].astype(str)
                        except:
                            pass

                    if style.color != []:
                        color = style.color
                    else:
                        color = [[0.0, 'rgb(242,240,247)'],
                                 [0.2, 'rgb(218,218,235)'],
                                 [0.4, 'rgb(188,189,220)'], \
                                 [0.6, 'rgb(158,154,200)'],
                                 [0.8, 'rgb(117,107,177)'],
                                 [1.0, 'rgb(84,39,143)']]

                    text = ''

                    if 'text' in data_frame.columns:
                        text = data_frame['Text']

                    data = [dict(
                        type='choropleth',
                        colorscale=color,
                        autocolorscale=False,
                        locations=data_frame['Code'],
                        z=data_frame[style.plotly_choropleth_field].astype(
                            float),
                        locationmode=style.plotly_location_mode,
                        text=text,
                        marker=dict(
                            line=dict(
                                color='rgb(255,255,255)',
                                width=1
                            )
                        ),
                        colorbar=dict(
                            title=style.units
                        )
                    )]

                    layout = dict(
                        title=title,
                        geo=dict(
                            scope=style.plotly_scope,
                            projection=dict(type=style.plotly_projection),
                            showlakes=True,
                            lakecolor='rgb(255, 255, 255)',
                        ),
                    )

                    fig = dict(data=data, layout=layout)

                # Otherwise underlying Cufflinks library underneath
                elif style.plotly_helper == 'cufflinks':

                    # NOTE: we use cufflinks library, which simplifies plotting DataFrames in plotly
                    if chart_type_ord == 'surface':
                        fig = data_frame.iplot(kind=chart_type,
                                               title=title,
                                               xTitle=style.x_title,
                                               yTitle=style.y_title,
                                               zTitle=style.z_title,
                                               x=x, y=y, z=z,
                                               mode=mode,
                                               size=marker_size,
                                               sharing=style.plotly_sharing,
                                               theme=style.plotly_theme,
                                               bestfit=style.line_of_best_fit,
                                               legend=style.display_legend,
                                               colorscale=style.color,
                                               dimensions=(style.width * abs(
                                                   style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                               asFigure=True)

                        # Setting axis is different with a surface
                        if style.x_axis_range is not None:
                            fig.update_layout(scene=dict(
                                xaxis=dict(range=style.x_axis_range)))

                        if style.y_axis_range is not None:
                            fig.update_layout(scene=dict(
                                xaxis=dict(range=style.y_axis_range)))

                        if style.z_axis_range is not None:
                            fig.update_layout(scene=dict(
                                xaxis=dict(range=style.z_axis_range)))

                    elif chart_type_ord == 'heatmap':
                        fig = data_frame.iplot(kind=chart_type,
                                               title=title,
                                               xTitle=style.x_title,
                                               yTitle=style.y_title,
                                               x=x, y=y,
                                               mode=mode,
                                               size=marker_size,
                                               sharing=style.plotly_sharing,
                                               theme=style.plotly_theme,
                                               bestfit=style.line_of_best_fit,
                                               legend=style.display_legend,
                                               colorscale=style.color,
                                               dimensions=(style.width * abs(
                                                   style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                               asFigure=True)

                    elif chart_type_ord == ""annotated-heatmap"":

                        if style.color == []:
                            color = None
                        else:
                            color = style.color

                        fig = px.imshow(data_frame, text_auto=True,
                                        title=style.title,
                                        color_continuous_scale=color,
                                        width=(style.width *
                                               abs(style.scale_factor) * scale),
                                        height= style.height *
                                                abs(style.scale_factor) * scale)

                    # Otherwise we have a line plot (or similar such as a scatter plot, or bar chart etc)
                    else:

                        full_line = style.connect_line_gaps

                        if chart_type_ord == 'line':
                            full_line = True

                            # chart_type_ord = 'scatter'
                            mode = 'lines'
                        elif chart_type_ord in ['dash', 'dashdot', 'dot']:
                            chart_type_ord = 'scatter'

                        elif chart_type_ord == 'line+markers':
                            full_line = True

                            chart_type_ord = 'line'
                            mode = 'lines+markers'
                            marker_size = 5
                        elif chart_type_ord == 'scatter':
                            mode = 'markers'
                            marker_size = 5
                        elif chart_type_ord == 'bubble':
                            chart_type_ord = 'scatter'

                            mode = 'markers'

                        # TODO check this!
                        # Can have issues calling cufflinks with a theme which is None, so split up the cases
                        if style.plotly_theme is None:
                            plotly_theme = 'pearl'
                        else:
                            plotly_theme = style.plotly_theme

                        m = 0

                        y_axis_2_series = [x for x in style.y_axis_2_series if
                                           x in data_frame.columns]

                        vspan = None

                        if style.x_shade_dates is not None:
                            vspan = {
                                'x0': data_frame.index[0].strftime(""%Y-%m-%d""),
                                'x1': data_frame.index[-1].strftime(
                                    ""%Y-%m-%d""), 'color': 'rgba(30,30,30,0.3)',
                                'fill': True, 'opacity': .4}

                        # Sometimes Plotly has issues generating figures in dash, so if fails first, try again
                        while m < 10:

                            if True:
                                if vspan is None:
                                    fig = data_frame.iplot(kind=chart_type_ord,
                                                           title=title,
                                                           xTitle=style.x_title,
                                                           yTitle=style.y_title,
                                                           x=x, y=y, z=z,
                                                           subplots=False,
                                                           sharing=style.plotly_sharing,
                                                           mode=mode,
                                                           secondary_y=y_axis_2_series,
                                                           size=marker_size,
                                                           theme=plotly_theme,
                                                           colorscale='dflt',
                                                           bestfit=style.line_of_best_fit,
                                                           legend=style.display_legend,
                                                           width=style.linewidth,
                                                           color=color_spec1,
                                                           dimensions=(
                                                           style.width * abs(
                                                               style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                                           asFigure=True)
                                else:
                                    fig = data_frame.iplot(kind=chart_type_ord,
                                                           title=title,
                                                           xTitle=style.x_title,
                                                           yTitle=style.y_title,
                                                           x=x, y=y, z=z,
                                                           subplots=False,
                                                           sharing=style.plotly_sharing,
                                                           mode=mode,
                                                           secondary_y=y_axis_2_series,
                                                           size=marker_size,
                                                           theme=plotly_theme,
                                                           colorscale='dflt',
                                                           bestfit=style.line_of_best_fit,
                                                           legend=style.display_legend,
                                                           width=style.linewidth,
                                                           color=color_spec1,
                                                           dimensions=(
                                                           style.width * abs(
                                                               style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                                           vspan=vspan,
                                                           asFigure=True)

                                m = 10;
                                break
                                # except Exception as e:
                                print(""Will attempt to re-render: "" + str(e))

                                import time
                                time.sleep(0.3)

                            m = m + 1

                        # For lines set the property of connectgaps (cannot specify directly in cufflinks)
                        if full_line:
                            for z in range(0, len(fig['data'])):
                                fig['data'][
                                    z].connectgaps = style.connect_line_gaps

                                for k in range(0, len(fig['data'])):
                                    if full_line:
                                        fig['data'][
                                            k].connectgaps = style.connect_line_gaps

                        if style.line_shape != None:
                            if isinstance(style.line_shape, str):
                                line_shape = [style.line_shape] * len(
                                    fig['data'])
                            else:
                                line_shape = style.line_shape

                            for k in range(0, len(fig['data'])):
                                fig['data'][k].line.shape = line_shape[k]

                        #if style.plotly_webgl:
                        #    for k in range(0, len(fig['data'])):
                        #        if fig['data'][k].type == 'scatter':
                        #            fig.update_traces(type=""scattergl"")
                                        #selector=dict(type=""bar""))
                                    #fig['data'][k].type = 'scattergl'

                        if style.stackgroup is not None:

                            if isinstance(style.stackgroup, list):
                                stackgroup = style.stackgroup
                            else:
                                stackgroup = ['A'] * len(fig['data'])

                            for k in range(0, len(fig['data'])):
                                fig['data'][k].stackgroup = stackgroup[k]


                # Use plotly express (not implemented yet)
                elif style.plotly_helper == 'plotly_express':
                    # TODO
                    pass

            # Common properties
            # Override other properties, which cannot be set directly by cufflinks/or you want to reset later
            if style.title is not None:
                try:
                    fig.update_layout(title=style.title)
                except:
                    pass

            # Add second y axis title
            if style.y_2_title is not None:
                if style.y_2_title != '':
                    try:
                        fig['layout'].update(
                            yaxis2=dict(title=style.y_2_title))
                    except:
                        pass

            if style.x_axis_range is not None:
                try:
                    fig['layout'].update(
                        xaxis=dict(range=style.x_axis_range, autorange=False))
                except:
                    pass

            if style.y_axis_range is not None:

                try:
                    fig['layout'].update(
                        yaxis=dict(range=style.y_axis_range, autorange=False))
                except:
                    pass

            if style.y_axis_2_range is not None:
                try:
                    fig['layout'].update(
                        yaxis2=dict(range=style.y_axis_2_range,
                                    autorange=False))
                except:
                    pass

            if style.z_axis_range is not None:
                try:
                    fig['layout'].update(
                        zaxis=dict(range=style.z_axis_range, autorange=False))
                except:
                    pass

            if style.font_family is not None:
                try:
                    fig.update_layout(font_family=style.font_family)
                except:
                    pass

            if style.x_axis_type is not None:
                try:
                    fig.update_xaxes(type=style.x_axis_type)
                except:
                    pass

            if style.y_axis_type is not None:
                try:
                    fig.update_yaxes(type=style.y_axis_type)
                except:
                    pass

            if style.x_dtick is not None:
                try:
                    fig.update_layout(
                        xaxis=dict(tickmode='linear', dtick=style.x_dtick))
                except:
                    pass

            if style.y_dtick is not None:
                try:
                    fig.update_layout(
                        yaxis=dict(tickmode='linear', dtick=style.y_dtick))
                except:
                    pass

            # Add shaded regions
            fig = self._multi_shade(fig, style)

            # Legend Properties
            if style.legend_x_anchor is not None:
                try:
                    fig.update_layout(
                        legend=dict(xanchor=style.legend_x_anchor))
                except:
                    pass

            if style.legend_y_anchor is not None:
                try:
                    fig.update_layout(
                        legend=dict(yanchor=style.legend_y_anchor))
                except:
                    pass

            if style.legend_x_pos is not None:
                try:
                    fig.update_layout(legend=dict(x=style.legend_x_pos))
                except:
                    pass

            if style.legend_y_pos is not None:
                try:
                    fig.update_layout(legend=dict(y=style.legend_y_pos))
                except:
                    pass

            if style.legend_bgcolor is not None:
                try:
                    fig.update_layout(
                        legend=dict(bgcolor=style.legend_bgcolor))
                except:
                    pass

            if style.legend_orientation is not None:
                try:
                    fig.update_layout(
                        legend=dict(orientation=style.legend_orientation))
                except:
                    pass

            if style.barmode is not None:
                try:
                    fig.update_layout(barmode=style.barmode)
                except:
                    pass

            fig_list.append(fig)

        #### Plotted all the lines

        # Create subplots if more than one figure
        if len(fig_list) > 1 and style.animate_figure == False \
                and style.subplots == True:
            from plotly.subplots import make_subplots

            if style.subplot_titles:
                fig = make_subplots(rows=len(fig_list), cols=1,
                                    subplot_titles=style.subplot_titles)
            else:
                fig = make_subplots(rows=len(fig_list), cols=1)

            # layout = fig_list[0]['layout']

            # fig.layout = fig_list[0].layout

            # for k, v in list(layout.items()):
            #     if 'xaxis' not in k and 'yaxis' not in k:
            #         fig['layout'].update({k: v})

            for i, f in enumerate(fig_list):
                f = f.data[0]
                f.update(legendgroup=i)
                fig.add_trace(f, row=i + 1, col=1)

            # fig = cf.subplots(fig_list, base_layout=fig_list[0].to_dict()['layout'], shape=(len(fig_list), 1),
            #                  shared_xaxes=False, shared_yaxes=False)

            if not (isinstance(style.title, list)):
                fig['layout'].update(title=style.title)

            fig.update_layout(
                height=style.height * abs(style.scale_factor),
                width=style.width * abs(style.scale_factor),
                showlegend=style.display_legend
            )

        elif style.animate_figure:

            fig = fig_list[0]

            # Add buttons to play/pause
            fig[""layout""][""updatemenus""] = [
                {
                    ""buttons"": [
                        {
                            ""args"": [None, {
                                ""frame"": {""duration"": style.animate_frame_ms,
                                          ""redraw"": True},
                                ""fromcurrent"": True, ""transition"": {
                                    ""duration"": style.animate_frame_ms,
                                    ""easing"": ""quadratic-in-out""}}],
                            ""label"": ""Play"",
                            ""method"": ""animate""
                        },
                        {
                            ""args"": [[None],
                                     {""frame"": {""duration"": 0, ""redraw"": True},
                                      ""mode"": ""immediate"",
                                      ""transition"": {""duration"": 0}}],
                            ""label"": ""Pause"",
                            ""method"": ""animate""
                        }
                    ],
                    ""direction"": ""left"",
                    ""pad"": {""r"": 10, ""t"": 87},
                    ""showactive"": False,
                    ""type"": ""buttons"",
                    ""x"": 0.1,
                    ""xanchor"": ""right"",
                    ""y"": 0,
                    ""yanchor"": ""top""
                }
            ]

            if style.animate_titles is not None:
                animate_titles = style.animate_titles
            else:
                animate_titles = list(range(0, len(fig_list)))

            # Add an animation frame for each data frame
            frames = []

            for fig_temp, title_temp in zip(fig_list, animate_titles):
                frames.append(go.Frame(data=fig_temp['data'],
                                       name=str(title_temp),
                                       layout=go.Layout(
                                           title=str(title_temp))))

            fig.update(frames=frames)

            # Add a slider, with the frame labels
            sliders_dict = {
                ""active"": 0,
                ""yanchor"": ""top"",
                ""xanchor"": ""left"",
                ""currentvalue"": {
                    ""visible"": True,
                    ""xanchor"": ""right""
                },
                ""transition"": {""duration"": style.animate_frame_ms,
                               ""easing"": ""cubic-in-out""},
                ""pad"": {""b"": 10, ""t"": 50},
                ""len"": 0.9,
                ""x"": 0.1,
                ""y"": 0,
                ""steps"": []
            }

            for i in range(0, len(fig_list)):
                slider_step = {""args"": [
                    [animate_titles[i]],
                    {""frame"": {""duration"": style.animate_frame_ms,
                               ""redraw"": True},
                     ""mode"": ""immediate"",
                     ""transition"": {""duration"": style.animate_frame_ms}}
                ],
                    ""label"": str(animate_titles[i]),
                    ""method"": ""animate""}

                sliders_dict[""steps""].append(slider_step)

            fig[""layout""][""sliders""] = [sliders_dict]

            # else:
            # Add an animation frame for each data frame
            #    fig.update(frames=[go.Frame(data=fig_temp['data']) for fig_temp in fig_list])

        else:
            fig = fig_list[0]

        fig.update(dict(layout=dict(legend=dict(
            x=0.05,
            y=1
        ))))

        # Adjust margins
        if style.thin_margin:
            fig.update(dict(layout=dict(margin=go.layout.Margin(
                l=20,
                r=20,
                b=40,
                t=40,
                pad=0
            ))))

        # Change background color
        fig.update(dict(layout=dict(paper_bgcolor='rgba(0,0,0,0)')))
        fig.update(dict(layout=dict(plot_bgcolor='rgba(0,0,0,0)')))

        # Deal with grids
        if (not (style.x_axis_showgrid)): fig.update(
            dict(layout=dict(xaxis=dict(showgrid=style.x_axis_showgrid))))
        if (not (style.y_axis_showgrid)): fig.update(
            dict(layout=dict(yaxis=dict(showgrid=style.y_axis_showgrid))))
        if (not (style.y_axis_2_showgrid)): fig.update(
            dict(layout=dict(yaxis2=dict(showgrid=style.y_axis_2_showgrid))))

        # Override properties, which cannot be set directly by cufflinks

        # For the type of line (ie. line or scatter)
        # For making the lined dashed, dotted etc.
        if style.subplots == False and isinstance(chart_type, list):
            for j in range(0, len(fig['data'])):
                mode = None;
                dash = None;
                line_shape = None;

                if chart_type[j] == 'line':
                    mode = 'lines'
                elif chart_type[j] == 'line+markers':
                    mode = 'lines+markers'
                elif chart_type[j] == 'scatter':
                    mode = 'markers'
                elif chart_type[j] in ['dash', 'dashdot', 'dot']:
                    dash = chart_type[j]
                    mode = 'lines'
                elif chart_type[j] in ['hv', 'vh', 'vhv', 'spline', 'linear']:
                    line_shape = chart_type[j]
                    mode = 'lines'
                elif chart_type[j] == 'bubble':
                    mode = 'markers'

                    bubble_series = style.bubble_series[cols[j]]
                    bubble_series = bubble_series.fillna(0)

                    # dash = chart_type[j]
                    # data_frame[bubble_series.name] = bubble_series
                    scale = float(bubble_series.max())

                    fig['data'][j].marker.size = \
                        (style.bubble_size_scalar * (
                                    bubble_series.values / scale)).tolist()

                if mode is not None:
                    fig['data'][j].mode = mode

                if dash is not None:
                    fig['data'][j].line.dash = dash

                if line_shape is not None:
                    fig['data'][j].line.shape = line_shape

        # If candlestick specified add that (needed to be appended on top of the Plotly figure's data
        if style.candlestick_series is not None and not (style.plotly_webgl):

            # self.logger.debug(""About to create candlesticks"")

            if isinstance(style.candlestick_series, Figure):
                fig_candle = style.candlestick_series
            else:
                # from plotly.tools import FigureFactory as FF
                fig_candle = create_candlestick(
                    style.candlestick_series['open'],
                    style.candlestick_series['high'],
                    style.candlestick_series['low'],
                    style.candlestick_series['close'],
                    dates=style.candlestick_series['close'].index
                    )

            if style.candlestick_increasing_color is not None:
                # Increasing
                fig_candle['data'][0].fillcolor = cm.get_color_code(
                    style.candlestick_increasing_color)
                fig_candle['data'][0].line.color = cm.get_color_code(
                    style.candlestick_increasing_line_color)

            if style.candlestick_decreasing_color is not None:
                # Decreasing
                fig_candle['data'][1].fillcolor = cm.get_color_code(
                    style.candlestick_decreasing_color)
                fig_candle['data'][1].line.color = cm.get_color_code(
                    style.candlestick_decreasing_line_color)

            try:
                # Append the data to the existing Plotly figure, plotted earlier
                fig.data.append(fig_candle.data[0]);
                fig.data.append(fig_candle.data[1])
            except:
                # Later version of Plotly
                fig.add_trace(fig_candle.data[0])
                fig.add_trace(fig_candle.data[1])

        # Overlay other Plotly figures on top of
        if style.overlay_fig is not None:

            for d in style.overlay_fig.data:
                fig.add_trace(d)

        x_y_line_list = []

        # fig.layout.yrange
        # add x-line:
        for x_y_line in style.x_y_line:
            start = x_y_line[0]
            finish = x_y_line[1]

            x_y_line_list.append(
                {
                    'type': 'line',
                    'x0': start[0],
                    'y0': start[1],
                    'x1': finish[0],
                    'y1': finish[1],
                    'line': {
                        'color': 'black',
                        'width': 0.5,
                        'dash': 'dot',
                    },
                }
            )

        # x_y_line_list =  [{
        #     'type': 'line',
        #     'x0': 1,
        #     'y0': 0,
        #     'x1': 1,
        #     'y1': 2,
        #     'li","for x_y_line in style.x_y_line:
    start = x_y_line[0]
    finish = x_y_line[1]
    x_y_line_list.append({'type': 'line', 'x0': start[0], 'y0': start[1], 'x1': finish[0], 'y1': finish[1], 'line': {'color': 'black', 'width': 0.5, 'dash': 'dot'}})","for x_y_line in style.x_y_line:
    (x_y_line_0, x_y_line_1, *_) = x_y_line
    start = x_y_line[0]
    finish = x_y_line[1]
    x_y_line_list.append({'type': 'line', 'x0': start[0], 'y0': start[1], 'x1': finish[0], 'y1': finish[1], 'line': {'color': 'black', 'width': 0.5, 'dash': 'dot'}})","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
chartpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chartpy/chartpy/engine.py,https://github.com/cuemacro/chartpy/tree/master/chartpy/engine.py,EnginePlotly,plot_chart$1458,"def plot_chart(self, data_frame, style, chart_type):

        # Special case if we have a pre-created Plotly object
        if isinstance(data_frame, Figure):
            return self.publish_plot(data_frame, style)

        mode = 'lines'

        if style is None: style = Style()

        marker_size = 1

        x = '';
        y = '';
        z = ''

        scale = 1

        try:
            # Adjust sizing if offline_html format
            if (
                    style.plotly_plot_mode == 'offline_html' and style.scale_factor > 0):
                scale = float(2.0 / 3.0)
        except:
            pass

        if style.plotly_webgl:
            plotly.graph_objs.Scatter = plotly.graph_objs.Scattergl

        # Check other plots implemented by Cufflinks
        cm = ColorMaster()

        # Create figure
        data_frame_list = self.split_data_frame_to_list(data_frame, style)
        fig_list = []
        cols = []

        # If we provide a list of Figures this will get ignored
        try:
            for data_frame in data_frame_list:
                cols.append(data_frame.columns)

            cols = list(numpy.array(cols).flat)

            # Get all the correct colors (and construct gradients if necessary eg. from 'Blues')
            # need to change to strings for cufflinks
            color_list = cm.create_color_list(style, [], cols=cols)
            color_spec = []

            # If no colors are specified then just use our default color set from chart constants
            if color_list == [None] * len(color_list):
                color_spec = [None] * len(color_list)

                for i in range(0, len(color_list)):

                    # Get the color
                    if color_spec[i] is None:
                        color_spec[i] = self.get_color_list(i)

                    try:
                        color_spec[i] = matplotlib.colors.rgb2hex(
                            color_spec[i])
                    except:
                        pass

            else:
                # Otherwise assume all the colors are rgba
                for color in color_list:
                    color = 'rgba' + str(color)
                    color_spec.append(color)
        except Exception as e:
            pass

        start = 0

        title_list = style.title

        if not (isinstance(title_list, list)):
            title_list = [style.title] * len(data_frame_list)

        # Go through each data_frame in the list and plot
        for i in range(0, len(data_frame_list)):
            data_frame = data_frame_list[i]
            title = title_list[i]

            if isinstance(data_frame, Figure):
                fig = data_frame
            else:

                if style.drop_na:
                    data_frame = data_frame.dropna()

                if isinstance(chart_type, list):
                    chart_type_ord = chart_type[i]
                else:
                    chart_type_ord = chart_type

                end = start + len(data_frame.columns)
                color_spec1 = color_spec[start:start + end]
                start = end

                # Special call for choropleth (uses Plotly API directly)
                # Special case for map/choropleth which has yet to be implemented in Cufflinks
                # will likely remove this in the future
                if chart_type_ord == 'choropleth':

                    for col in data_frame.columns:
                        try:
                            data_frame[col] = data_frame[col].astype(str)
                        except:
                            pass

                    if style.color != []:
                        color = style.color
                    else:
                        color = [[0.0, 'rgb(242,240,247)'],
                                 [0.2, 'rgb(218,218,235)'],
                                 [0.4, 'rgb(188,189,220)'], \
                                 [0.6, 'rgb(158,154,200)'],
                                 [0.8, 'rgb(117,107,177)'],
                                 [1.0, 'rgb(84,39,143)']]

                    text = ''

                    if 'text' in data_frame.columns:
                        text = data_frame['Text']

                    data = [dict(
                        type='choropleth',
                        colorscale=color,
                        autocolorscale=False,
                        locations=data_frame['Code'],
                        z=data_frame[style.plotly_choropleth_field].astype(
                            float),
                        locationmode=style.plotly_location_mode,
                        text=text,
                        marker=dict(
                            line=dict(
                                color='rgb(255,255,255)',
                                width=1
                            )
                        ),
                        colorbar=dict(
                            title=style.units
                        )
                    )]

                    layout = dict(
                        title=title,
                        geo=dict(
                            scope=style.plotly_scope,
                            projection=dict(type=style.plotly_projection),
                            showlakes=True,
                            lakecolor='rgb(255, 255, 255)',
                        ),
                    )

                    fig = dict(data=data, layout=layout)

                # Otherwise underlying Cufflinks library underneath
                elif style.plotly_helper == 'cufflinks':

                    # NOTE: we use cufflinks library, which simplifies plotting DataFrames in plotly
                    if chart_type_ord == 'surface':
                        fig = data_frame.iplot(kind=chart_type,
                                               title=title,
                                               xTitle=style.x_title,
                                               yTitle=style.y_title,
                                               zTitle=style.z_title,
                                               x=x, y=y, z=z,
                                               mode=mode,
                                               size=marker_size,
                                               sharing=style.plotly_sharing,
                                               theme=style.plotly_theme,
                                               bestfit=style.line_of_best_fit,
                                               legend=style.display_legend,
                                               colorscale=style.color,
                                               dimensions=(style.width * abs(
                                                   style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                               asFigure=True)

                        # Setting axis is different with a surface
                        if style.x_axis_range is not None:
                            fig.update_layout(scene=dict(
                                xaxis=dict(range=style.x_axis_range)))

                        if style.y_axis_range is not None:
                            fig.update_layout(scene=dict(
                                xaxis=dict(range=style.y_axis_range)))

                        if style.z_axis_range is not None:
                            fig.update_layout(scene=dict(
                                xaxis=dict(range=style.z_axis_range)))

                    elif chart_type_ord == 'heatmap':
                        fig = data_frame.iplot(kind=chart_type,
                                               title=title,
                                               xTitle=style.x_title,
                                               yTitle=style.y_title,
                                               x=x, y=y,
                                               mode=mode,
                                               size=marker_size,
                                               sharing=style.plotly_sharing,
                                               theme=style.plotly_theme,
                                               bestfit=style.line_of_best_fit,
                                               legend=style.display_legend,
                                               colorscale=style.color,
                                               dimensions=(style.width * abs(
                                                   style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                               asFigure=True)

                    elif chart_type_ord == ""annotated-heatmap"":

                        if style.color == []:
                            color = None
                        else:
                            color = style.color

                        fig = px.imshow(data_frame, text_auto=True,
                                        title=style.title,
                                        color_continuous_scale=color,
                                        width=(style.width *
                                               abs(style.scale_factor) * scale),
                                        height= style.height *
                                                abs(style.scale_factor) * scale)

                    # Otherwise we have a line plot (or similar such as a scatter plot, or bar chart etc)
                    else:

                        full_line = style.connect_line_gaps

                        if chart_type_ord == 'line':
                            full_line = True

                            # chart_type_ord = 'scatter'
                            mode = 'lines'
                        elif chart_type_ord in ['dash', 'dashdot', 'dot']:
                            chart_type_ord = 'scatter'

                        elif chart_type_ord == 'line+markers':
                            full_line = True

                            chart_type_ord = 'line'
                            mode = 'lines+markers'
                            marker_size = 5
                        elif chart_type_ord == 'scatter':
                            mode = 'markers'
                            marker_size = 5
                        elif chart_type_ord == 'bubble':
                            chart_type_ord = 'scatter'

                            mode = 'markers'

                        # TODO check this!
                        # Can have issues calling cufflinks with a theme which is None, so split up the cases
                        if style.plotly_theme is None:
                            plotly_theme = 'pearl'
                        else:
                            plotly_theme = style.plotly_theme

                        m = 0

                        y_axis_2_series = [x for x in style.y_axis_2_series if
                                           x in data_frame.columns]

                        vspan = None

                        if style.x_shade_dates is not None:
                            vspan = {
                                'x0': data_frame.index[0].strftime(""%Y-%m-%d""),
                                'x1': data_frame.index[-1].strftime(
                                    ""%Y-%m-%d""), 'color': 'rgba(30,30,30,0.3)',
                                'fill': True, 'opacity': .4}

                        # Sometimes Plotly has issues generating figures in dash, so if fails first, try again
                        while m < 10:

                            if True:
                                if vspan is None:
                                    fig = data_frame.iplot(kind=chart_type_ord,
                                                           title=title,
                                                           xTitle=style.x_title,
                                                           yTitle=style.y_title,
                                                           x=x, y=y, z=z,
                                                           subplots=False,
                                                           sharing=style.plotly_sharing,
                                                           mode=mode,
                                                           secondary_y=y_axis_2_series,
                                                           size=marker_size,
                                                           theme=plotly_theme,
                                                           colorscale='dflt',
                                                           bestfit=style.line_of_best_fit,
                                                           legend=style.display_legend,
                                                           width=style.linewidth,
                                                           color=color_spec1,
                                                           dimensions=(
                                                           style.width * abs(
                                                               style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                                           asFigure=True)
                                else:
                                    fig = data_frame.iplot(kind=chart_type_ord,
                                                           title=title,
                                                           xTitle=style.x_title,
                                                           yTitle=style.y_title,
                                                           x=x, y=y, z=z,
                                                           subplots=False,
                                                           sharing=style.plotly_sharing,
                                                           mode=mode,
                                                           secondary_y=y_axis_2_series,
                                                           size=marker_size,
                                                           theme=plotly_theme,
                                                           colorscale='dflt',
                                                           bestfit=style.line_of_best_fit,
                                                           legend=style.display_legend,
                                                           width=style.linewidth,
                                                           color=color_spec1,
                                                           dimensions=(
                                                           style.width * abs(
                                                               style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                                           vspan=vspan,
                                                           asFigure=True)

                                m = 10;
                                break
                                # except Exception as e:
                                print(""Will attempt to re-render: "" + str(e))

                                import time
                                time.sleep(0.3)

                            m = m + 1

                        # For lines set the property of connectgaps (cannot specify directly in cufflinks)
                        if full_line:
                            for z in range(0, len(fig['data'])):
                                fig['data'][
                                    z].connectgaps = style.connect_line_gaps

                                for k in range(0, len(fig['data'])):
                                    if full_line:
                                        fig['data'][
                                            k].connectgaps = style.connect_line_gaps

                        if style.line_shape != None:
                            if isinstance(style.line_shape, str):
                                line_shape = [style.line_shape] * len(
                                    fig['data'])
                            else:
                                line_shape = style.line_shape

                            for k in range(0, len(fig['data'])):
                                fig['data'][k].line.shape = line_shape[k]

                        #if style.plotly_webgl:
                        #    for k in range(0, len(fig['data'])):
                        #        if fig['data'][k].type == 'scatter':
                        #            fig.update_traces(type=""scattergl"")
                                        #selector=dict(type=""bar""))
                                    #fig['data'][k].type = 'scattergl'

                        if style.stackgroup is not None:

                            if isinstance(style.stackgroup, list):
                                stackgroup = style.stackgroup
                            else:
                                stackgroup = ['A'] * len(fig['data'])

                            for k in range(0, len(fig['data'])):
                                fig['data'][k].stackgroup = stackgroup[k]


                # Use plotly express (not implemented yet)
                elif style.plotly_helper == 'plotly_express':
                    # TODO
                    pass

            # Common properties
            # Override other properties, which cannot be set directly by cufflinks/or you want to reset later
            if style.title is not None:
                try:
                    fig.update_layout(title=style.title)
                except:
                    pass

            # Add second y axis title
            if style.y_2_title is not None:
                if style.y_2_title != '':
                    try:
                        fig['layout'].update(
                            yaxis2=dict(title=style.y_2_title))
                    except:
                        pass

            if style.x_axis_range is not None:
                try:
                    fig['layout'].update(
                        xaxis=dict(range=style.x_axis_range, autorange=False))
                except:
                    pass

            if style.y_axis_range is not None:

                try:
                    fig['layout'].update(
                        yaxis=dict(range=style.y_axis_range, autorange=False))
                except:
                    pass

            if style.y_axis_2_range is not None:
                try:
                    fig['layout'].update(
                        yaxis2=dict(range=style.y_axis_2_range,
                                    autorange=False))
                except:
                    pass

            if style.z_axis_range is not None:
                try:
                    fig['layout'].update(
                        zaxis=dict(range=style.z_axis_range, autorange=False))
                except:
                    pass

            if style.font_family is not None:
                try:
                    fig.update_layout(font_family=style.font_family)
                except:
                    pass

            if style.x_axis_type is not None:
                try:
                    fig.update_xaxes(type=style.x_axis_type)
                except:
                    pass

            if style.y_axis_type is not None:
                try:
                    fig.update_yaxes(type=style.y_axis_type)
                except:
                    pass

            if style.x_dtick is not None:
                try:
                    fig.update_layout(
                        xaxis=dict(tickmode='linear', dtick=style.x_dtick))
                except:
                    pass

            if style.y_dtick is not None:
                try:
                    fig.update_layout(
                        yaxis=dict(tickmode='linear', dtick=style.y_dtick))
                except:
                    pass

            # Add shaded regions
            fig = self._multi_shade(fig, style)

            # Legend Properties
            if style.legend_x_anchor is not None:
                try:
                    fig.update_layout(
                        legend=dict(xanchor=style.legend_x_anchor))
                except:
                    pass

            if style.legend_y_anchor is not None:
                try:
                    fig.update_layout(
                        legend=dict(yanchor=style.legend_y_anchor))
                except:
                    pass

            if style.legend_x_pos is not None:
                try:
                    fig.update_layout(legend=dict(x=style.legend_x_pos))
                except:
                    pass

            if style.legend_y_pos is not None:
                try:
                    fig.update_layout(legend=dict(y=style.legend_y_pos))
                except:
                    pass

            if style.legend_bgcolor is not None:
                try:
                    fig.update_layout(
                        legend=dict(bgcolor=style.legend_bgcolor))
                except:
                    pass

            if style.legend_orientation is not None:
                try:
                    fig.update_layout(
                        legend=dict(orientation=style.legend_orientation))
                except:
                    pass

            if style.barmode is not None:
                try:
                    fig.update_layout(barmode=style.barmode)
                except:
                    pass

            fig_list.append(fig)

        #### Plotted all the lines

        # Create subplots if more than one figure
        if len(fig_list) > 1 and style.animate_figure == False \
                and style.subplots == True:
            from plotly.subplots import make_subplots

            if style.subplot_titles:
                fig = make_subplots(rows=len(fig_list), cols=1,
                                    subplot_titles=style.subplot_titles)
            else:
                fig = make_subplots(rows=len(fig_list), cols=1)

            # layout = fig_list[0]['layout']

            # fig.layout = fig_list[0].layout

            # for k, v in list(layout.items()):
            #     if 'xaxis' not in k and 'yaxis' not in k:
            #         fig['layout'].update({k: v})

            for i, f in enumerate(fig_list):
                f = f.data[0]
                f.update(legendgroup=i)
                fig.add_trace(f, row=i + 1, col=1)

            # fig = cf.subplots(fig_list, base_layout=fig_list[0].to_dict()['layout'], shape=(len(fig_list), 1),
            #                  shared_xaxes=False, shared_yaxes=False)

            if not (isinstance(style.title, list)):
                fig['layout'].update(title=style.title)

            fig.update_layout(
                height=style.height * abs(style.scale_factor),
                width=style.width * abs(style.scale_factor),
                showlegend=style.display_legend
            )

        elif style.animate_figure:

            fig = fig_list[0]

            # Add buttons to play/pause
            fig[""layout""][""updatemenus""] = [
                {
                    ""buttons"": [
                        {
                            ""args"": [None, {
                                ""frame"": {""duration"": style.animate_frame_ms,
                                          ""redraw"": True},
                                ""fromcurrent"": True, ""transition"": {
                                    ""duration"": style.animate_frame_ms,
                                    ""easing"": ""quadratic-in-out""}}],
                            ""label"": ""Play"",
                            ""method"": ""animate""
                        },
                        {
                            ""args"": [[None],
                                     {""frame"": {""duration"": 0, ""redraw"": True},
                                      ""mode"": ""immediate"",
                                      ""transition"": {""duration"": 0}}],
                            ""label"": ""Pause"",
                            ""method"": ""animate""
                        }
                    ],
                    ""direction"": ""left"",
                    ""pad"": {""r"": 10, ""t"": 87},
                    ""showactive"": False,
                    ""type"": ""buttons"",
                    ""x"": 0.1,
                    ""xanchor"": ""right"",
                    ""y"": 0,
                    ""yanchor"": ""top""
                }
            ]

            if style.animate_titles is not None:
                animate_titles = style.animate_titles
            else:
                animate_titles = list(range(0, len(fig_list)))

            # Add an animation frame for each data frame
            frames = []

            for fig_temp, title_temp in zip(fig_list, animate_titles):
                frames.append(go.Frame(data=fig_temp['data'],
                                       name=str(title_temp),
                                       layout=go.Layout(
                                           title=str(title_temp))))

            fig.update(frames=frames)

            # Add a slider, with the frame labels
            sliders_dict = {
                ""active"": 0,
                ""yanchor"": ""top"",
                ""xanchor"": ""left"",
                ""currentvalue"": {
                    ""visible"": True,
                    ""xanchor"": ""right""
                },
                ""transition"": {""duration"": style.animate_frame_ms,
                               ""easing"": ""cubic-in-out""},
                ""pad"": {""b"": 10, ""t"": 50},
                ""len"": 0.9,
                ""x"": 0.1,
                ""y"": 0,
                ""steps"": []
            }

            for i in range(0, len(fig_list)):
                slider_step = {""args"": [
                    [animate_titles[i]],
                    {""frame"": {""duration"": style.animate_frame_ms,
                               ""redraw"": True},
                     ""mode"": ""immediate"",
                     ""transition"": {""duration"": style.animate_frame_ms}}
                ],
                    ""label"": str(animate_titles[i]),
                    ""method"": ""animate""}

                sliders_dict[""steps""].append(slider_step)

            fig[""layout""][""sliders""] = [sliders_dict]

            # else:
            # Add an animation frame for each data frame
            #    fig.update(frames=[go.Frame(data=fig_temp['data']) for fig_temp in fig_list])

        else:
            fig = fig_list[0]

        fig.update(dict(layout=dict(legend=dict(
            x=0.05,
            y=1
        ))))

        # Adjust margins
        if style.thin_margin:
            fig.update(dict(layout=dict(margin=go.layout.Margin(
                l=20,
                r=20,
                b=40,
                t=40,
                pad=0
            ))))

        # Change background color
        fig.update(dict(layout=dict(paper_bgcolor='rgba(0,0,0,0)')))
        fig.update(dict(layout=dict(plot_bgcolor='rgba(0,0,0,0)')))

        # Deal with grids
        if (not (style.x_axis_showgrid)): fig.update(
            dict(layout=dict(xaxis=dict(showgrid=style.x_axis_showgrid))))
        if (not (style.y_axis_showgrid)): fig.update(
            dict(layout=dict(yaxis=dict(showgrid=style.y_axis_showgrid))))
        if (not (style.y_axis_2_showgrid)): fig.update(
            dict(layout=dict(yaxis2=dict(showgrid=style.y_axis_2_showgrid))))

        # Override properties, which cannot be set directly by cufflinks

        # For the type of line (ie. line or scatter)
        # For making the lined dashed, dotted etc.
        if style.subplots == False and isinstance(chart_type, list):
            for j in range(0, len(fig['data'])):
                mode = None;
                dash = None;
                line_shape = None;

                if chart_type[j] == 'line':
                    mode = 'lines'
                elif chart_type[j] == 'line+markers':
                    mode = 'lines+markers'
                elif chart_type[j] == 'scatter':
                    mode = 'markers'
                elif chart_type[j] in ['dash', 'dashdot', 'dot']:
                    dash = chart_type[j]
                    mode = 'lines'
                elif chart_type[j] in ['hv', 'vh', 'vhv', 'spline', 'linear']:
                    line_shape = chart_type[j]
                    mode = 'lines'
                elif chart_type[j] == 'bubble':
                    mode = 'markers'

                    bubble_series = style.bubble_series[cols[j]]
                    bubble_series = bubble_series.fillna(0)

                    # dash = chart_type[j]
                    # data_frame[bubble_series.name] = bubble_series
                    scale = float(bubble_series.max())

                    fig['data'][j].marker.size = \
                        (style.bubble_size_scalar * (
                                    bubble_series.values / scale)).tolist()

                if mode is not None:
                    fig['data'][j].mode = mode

                if dash is not None:
                    fig['data'][j].line.dash = dash

                if line_shape is not None:
                    fig['data'][j].line.shape = line_shape

        # If candlestick specified add that (needed to be appended on top of the Plotly figure's data
        if style.candlestick_series is not None and not (style.plotly_webgl):

            # self.logger.debug(""About to create candlesticks"")

            if isinstance(style.candlestick_series, Figure):
                fig_candle = style.candlestick_series
            else:
                # from plotly.tools import FigureFactory as FF
                fig_candle = create_candlestick(
                    style.candlestick_series['open'],
                    style.candlestick_series['high'],
                    style.candlestick_series['low'],
                    style.candlestick_series['close'],
                    dates=style.candlestick_series['close'].index
                    )

            if style.candlestick_increasing_color is not None:
                # Increasing
                fig_candle['data'][0].fillcolor = cm.get_color_code(
                    style.candlestick_increasing_color)
                fig_candle['data'][0].line.color = cm.get_color_code(
                    style.candlestick_increasing_line_color)

            if style.candlestick_decreasing_color is not None:
                # Decreasing
                fig_candle['data'][1].fillcolor = cm.get_color_code(
                    style.candlestick_decreasing_color)
                fig_candle['data'][1].line.color = cm.get_color_code(
                    style.candlestick_decreasing_line_color)

            try:
                # Append the data to the existing Plotly figure, plotted earlier
                fig.data.append(fig_candle.data[0]);
                fig.data.append(fig_candle.data[1])
            except:
                # Later version of Plotly
                fig.add_trace(fig_candle.data[0])
                fig.add_trace(fig_candle.data[1])

        # Overlay other Plotly figures on top of
        if style.overlay_fig is not None:

            for d in style.overlay_fig.data:
                fig.add_trace(d)

        x_y_line_list = []

        # fig.layout.yrange
        # add x-line:
        for x_y_line in style.x_y_line:
            start = x_y_line[0]
            finish = x_y_line[1]

            x_y_line_list.append(
                {
                    'type': 'line',
                    'x0': start[0],
                    'y0': start[1],
                    'x1': finish[0],
                    'y1': finish[1],
                    'line': {
                        'color': 'black',
                        'width': 0.5,
                        'dash': 'dot',
                    },
                }
            )

        # x_y_line_list =  [{
        #     'type': 'line',
        #     'x0': 1,
        #     'y0': 0,
        #     'x1': 1,
        #     'y1': 2,
        #     'li","for (fig_temp, title_temp) in zip(fig_list, animate_titles):
    frames.append(go.Frame(data=fig_temp['data'], name=str(title_temp), layout=go.Layout(title=str(title_temp))))",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['data'] is a dictionary value that can be accessed directly using the key 'data'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/homeassistant/components/nzbget/coordinator.py,https://github.com/home-assistant/core/tree/master/homeassistant/components/nzbget/coordinator.py,NZBGetDataUpdateCoordinator,_check_completed_downloads$51,"def _check_completed_downloads(self, history):
        """"""Check history for newly completed downloads.""""""
        actual_completed_downloads = {
            (x[""Name""], x[""Category""], x[""Status""]) for x in history
        }

        if self._completed_downloads_init:
            tmp_completed_downloads = list(
                actual_completed_downloads.difference(self._completed_downloads)
            )

            for download in tmp_completed_downloads:
                self.hass.bus.fire(
                    ""nzbget_download_complete"",
                    {
                        ""name"": download[0],
                        ""category"": download[1],
                        ""status"": download[2],
                    },
                )

        self._completed_downloads = actual_completed_downloads
        self._completed_downloads_init = True","for download in tmp_completed_downloads:
    self.hass.bus.fire('nzbget_download_complete', {'name': download[0], 'category': download[1], 'status': download[2]})","for download in tmp_completed_downloads:
    (download_0, download_1, download_2, *_) = download
    self.hass.bus.fire('nzbget_download_complete', {'name': download[0], 'category': download[1], 'status': download[2]})","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
pithos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pithos/pithos/pithos.py,https://github.com/pithos/pithos/tree/master/pithos/pithos.py,PithosWindow,station_added$1002,"def station_added(self, station, user_data):
        music_type, description = user_data
        for existing_station in self.stations_model:
            if existing_station[0].id == station.id:
                self.station_already_exists(existing_station[0], description, music_type, self)
                return
        # We shouldn't actually add the station to the pandora stations list
        # until we know it's not a duplicate.
        self.pandora.stations.append(station)
        self.stations_model.insert_with_valuesv(0, (0, 1, 2), (station, station.name, 0))
        self.emit('station-added', station)
        self.station_changed(station)","for existing_station in self.stations_model:
    if existing_station[0].id == station.id:
        self.station_already_exists(existing_station[0], description, music_type, self)
        return","for existing_station in self.stations_model:
    (existing_station_0, *existing_station_rexisting_stationmaining) = existing_station
    if existing_station[0].id == station.id:
        self.station_already_exists(existing_station[0], description, music_type, self)
        return","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
transitions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transitions/transitions/extensions/diagrams_pygraphviz.py,https://github.com/pytransitions/transitions/tree/master/transitions/extensions/diagrams_pygraphviz.py,Graph,get_graph$55,"def get_graph(self, title=None, roi_state=None):
        if title:
            self.fsm_graph.graph_attr['label'] = title
        if roi_state:
            filtered = _copy_agraph(self.fsm_graph)
            kept_nodes = set()
            active_state = roi_state.name if hasattr(roi_state, 'name') else roi_state
            if not filtered.has_node(roi_state):
                active_state += '_anchor'
            kept_nodes.add(active_state)

            # remove all edges that have no connection to the currently active state
            for edge in filtered.edges():
                if active_state not in edge:
                    filtered.delete_edge(edge)

            # find the ingoing edge by color; remove the rest
            for edge in filtered.in_edges(active_state):
                if edge.attr['color'] == self.fsm_graph.style_attributes['edge']['previous']['color']:
                    kept_nodes.add(edge[0])
                else:
                    filtered.delete_edge(edge)

            # remove outgoing edges from children
            for edge in filtered.out_edges_iter(active_state):
                kept_nodes.add(edge[1])

            for node in filtered.nodes():
                if node not in kept_nodes:
                    filtered.delete_node(node)

            return filtered
        return self.fsm_graph","for edge in filtered.in_edges(active_state):
    if edge.attr['color'] == self.fsm_graph.style_attributes['edge']['previous']['color']:
        kept_nodes.add(edge[0])
    else:
        filtered.delete_edge(edge)","for edge in filtered.in_edges(active_state):
    (edge_0, *edge_redgemaining) = edge
    if edge.attr['color'] == self.fsm_graph.style_attributes['edge']['previous']['color']:
        kept_nodes.add(edge[0])
    else:
        filtered.delete_edge(edge)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
transitions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transitions/transitions/extensions/diagrams_pygraphviz.py,https://github.com/pytransitions/transitions/tree/master/transitions/extensions/diagrams_pygraphviz.py,Graph,get_graph$55,"def get_graph(self, title=None, roi_state=None):
        if title:
            self.fsm_graph.graph_attr['label'] = title
        if roi_state:
            filtered = _copy_agraph(self.fsm_graph)
            kept_nodes = set()
            active_state = roi_state.name if hasattr(roi_state, 'name') else roi_state
            if not filtered.has_node(roi_state):
                active_state += '_anchor'
            kept_nodes.add(active_state)

            # remove all edges that have no connection to the currently active state
            for edge in filtered.edges():
                if active_state not in edge:
                    filtered.delete_edge(edge)

            # find the ingoing edge by color; remove the rest
            for edge in filtered.in_edges(active_state):
                if edge.attr['color'] == self.fsm_graph.style_attributes['edge']['previous']['color']:
                    kept_nodes.add(edge[0])
                else:
                    filtered.delete_edge(edge)

            # remove outgoing edges from children
            for edge in filtered.out_edges_iter(active_state):
                kept_nodes.add(edge[1])

            for node in filtered.nodes():
                if node not in kept_nodes:
                    filtered.delete_node(node)

            return filtered
        return self.fsm_graph","for edge in filtered.out_edges_iter(active_state):
    kept_nodes.add(edge[1])","for edge in filtered.out_edges_iter(active_state):
    (edge_0, edge_1, *edge_redgemaining) = edge
    kept_nodes.add(edge[1])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
django-notifications,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-notifications/notifications/tests/tests.py,https://github.com/django-notifications/django-notifications/tree/master/notifications/tests/tests.py,NotificationManagersTest,test_notify_send_return_val_group$102,"def test_notify_send_return_val_group(self):  # pylint: disable=invalid-name
        results = notify.send(self.from_user, recipient=self.to_group, verb='commented', action_object=self.from_user)
        for result in results:
            if result[0] is notify_handler:
                self.assertEqual(len(result[1]), self.to_group.user_set.count())
                for notification in result[1]:
                    # only check types for now
                    self.assertEqual(type(notification), Notification)","for result in results:
    if result[0] is notify_handler:
        self.assertEqual(len(result[1]), self.to_group.user_set.count())
        for notification in result[1]:
            self.assertEqual(type(notification), Notification)","for result in results:
    (result_0, result_1, *_) = result
    if result[0] is notify_handler:
        self.assertEqual(len(result[1]), self.to_group.user_set.count())
        for notification in result[1]:
            self.assertEqual(type(notification), Notification)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pgmpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgmpy/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py,TestCustomDistributionMethods,test_copy$443,"def test_copy(self):
        copy1 = self.phi1.copy()
        copy2 = self.phi3.copy()

        copy4 = copy1.copy()
        copy5 = copy2.copy()

        self.assertEqual(copy1.variables, copy4.variables)
        self.assertEqual(copy1._pdf, copy4._pdf)
        self.assertEqual(copy2.variables, copy5.variables)
        self.assertEqual(copy2._pdf, copy5._pdf)

        copy1.variables = [""A"", ""B""]
        self.assertEqual(copy4.variables, self.phi1.variables)

        def pdf(a, b):
            return (a + b) / (a * a + b * b)

        copy1._pdf = pdf
        copy1_pdf = pdf
        self.assertEqual(copy4._pdf, self.phi1._pdf)
        copy4.variables = [""X"", ""Y""]
        self.assertEqual(copy1.variables, [""A"", ""B""])
        copy4._pdf = lambda a, b: a + b
        for inp in np.random.rand(4, 2):
            self.assertEqual(copy1._pdf(inp[0], inp[1]), copy1_pdf(inp[0], inp[1]))

        copy2.reduce([(""x"", 7.7)])

        def reduced_pdf(y, z):
            return z * (np.power(7.7, 1) * np.power(y, 2)) / beta(7.7, y)

        self.assertEqual(copy5.variables, self.phi3.variables)
        self.assertEqual(copy5._pdf, self.phi3._pdf)
        copy5.reduce([(""x"", 11), (""z"", 13)])
        self.assertEqual(copy2.variables, [""y"", ""z""])
        for inp in np.random.rand(4, 2):
            self.assertEqual(copy2._pdf(inp[0], inp[1]), reduced_pdf(inp[0], inp[1]))","for inp in np.random.rand(4, 2):
    self.assertEqual(copy1._pdf(inp[0], inp[1]), copy1_pdf(inp[0], inp[1]))","for inp in np.random.rand(4, 2):
    (inp_0, inp_1, *_) = inp
    self.assertEqual(copy1._pdf(inp[0], inp[1]), copy1_pdf(inp[0], inp[1]))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pgmpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgmpy/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py,TestCustomDistributionMethods,test_copy$443,"def test_copy(self):
        copy1 = self.phi1.copy()
        copy2 = self.phi3.copy()

        copy4 = copy1.copy()
        copy5 = copy2.copy()

        self.assertEqual(copy1.variables, copy4.variables)
        self.assertEqual(copy1._pdf, copy4._pdf)
        self.assertEqual(copy2.variables, copy5.variables)
        self.assertEqual(copy2._pdf, copy5._pdf)

        copy1.variables = [""A"", ""B""]
        self.assertEqual(copy4.variables, self.phi1.variables)

        def pdf(a, b):
            return (a + b) / (a * a + b * b)

        copy1._pdf = pdf
        copy1_pdf = pdf
        self.assertEqual(copy4._pdf, self.phi1._pdf)
        copy4.variables = [""X"", ""Y""]
        self.assertEqual(copy1.variables, [""A"", ""B""])
        copy4._pdf = lambda a, b: a + b
        for inp in np.random.rand(4, 2):
            self.assertEqual(copy1._pdf(inp[0], inp[1]), copy1_pdf(inp[0], inp[1]))

        copy2.reduce([(""x"", 7.7)])

        def reduced_pdf(y, z):
            return z * (np.power(7.7, 1) * np.power(y, 2)) / beta(7.7, y)

        self.assertEqual(copy5.variables, self.phi3.variables)
        self.assertEqual(copy5._pdf, self.phi3._pdf)
        copy5.reduce([(""x"", 11), (""z"", 13)])
        self.assertEqual(copy2.variables, [""y"", ""z""])
        for inp in np.random.rand(4, 2):
            self.assertEqual(copy2._pdf(inp[0], inp[1]), reduced_pdf(inp[0], inp[1]))","for inp in np.random.rand(4, 2):
    self.assertEqual(copy2._pdf(inp[0], inp[1]), reduced_pdf(inp[0], inp[1]))","for inp in np.random.rand(4, 2):
    (inp_0, inp_1, *_) = inp
    self.assertEqual(copy2._pdf(inp[0], inp[1]), reduced_pdf(inp[0], inp[1]))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Vulmap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Vulmap/Vulmap-Linux/vulmap-linux.py,https://github.com/vulmon/Vulmap/tree/master/Vulmap-Linux/vulmap-linux.py,,ReadFromFile$207,"def ReadFromFile(InventoryOutFile):
	count = 0
	print(""Reading software inventory from ""+InventoryOutFile)
	with open(InventoryOutFile) as json_file:
		products = json.load(json_file)
	for a in products:
		if count == 0:
			queryData = '['
		queryData += '{'
		queryData += '""product"": ""' + a[0] + '"",'
		queryData += '""version"": ""' + a[1] + '"",'
		queryData += '""arc"": ""' + a[2] + '""'
		queryData += '},'
		count += 1
		if count == 100:
			count = 0
			outResults(queryData)
	outResults(queryData)","for a in products:
    if count == 0:
        queryData = '['
    queryData += '{'
    queryData += '""product"": ""' + a[0] + '"",'
    queryData += '""version"": ""' + a[1] + '"",'
    queryData += '""arc"": ""' + a[2] + '""'
    queryData += '},'
    count += 1
    if count == 100:
        count = 0
        outResults(queryData)","for a in products:
    (a_0, a_1, a_2, *_) = a
    if count == 0:
        queryData = '['
    queryData += '{'
    queryData += '""product"": ""' + a[0] + '"",'
    queryData += '""version"": ""' + a[1] + '"",'
    queryData += '""arc"": ""' + a[2] + '""'
    queryData += '},'
    count += 1
    if count == 100:
        count = 0
        outResults(queryData)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
OpenCore-Legacy-Patcher,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenCore-Legacy-Patcher/resources/cli_menu.py,https://github.com/dortania/OpenCore-Legacy-Patcher/tree/master/resources/cli_menu.py,MenuOptions,patcher_settings_security$968,"def patcher_settings_security(self):
        response = None
        while not (response and response == -1):
            title = [""Adjust Security Settings""]
            menu = utilities.TUIMenu(title, ""Please select an option: "", auto_number=True, top_level=True)
            options = [
                # [
                #     f""Set Apple Mobile File Integrity (AMFI):\tCurrently {self.constants.amfi_status}"",
                #     MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).set_amfi,
                # ],
                [
                    f""Set System Integrity Protection (SIP):\tCurrently {self.constants.custom_sip_value or self.constants.sip_status}"",
                    MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_sip,
                ],
                [
                    f""Set Secure Boot Model (SBM):\t\tCurrently {self.constants.secure_status}"",
                    MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_sbm,
                ],
                [f""Set Vault Mode:\t\t\t\tCurrently {self.constants.vault}"", MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_vault],
            ]

            for option in options:
                menu.add_menu_option(option[0], function=option[1])

            response = menu.start()","for option in options:
    menu.add_menu_option(option[0], function=option[1])","for option in options:
    (option_0, option_1, *_) = option
    menu.add_menu_option(option[0], function=option[1])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/CreatHtml.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/CreatHtml.py,CreatHtml,api_detail_html$1072,"def api_detail_html(self):
        api_id = 1
        whole_html = """"

        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from api_tree where success = 1 or success = 2""
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            api_info_html = """"""<div id=""api_{api_id}"" class=""modal fade"" role=""dialog"">
            	<div class=""modal-dialog"">
            		<div class=""modal-content"">
            			<div class=""modal-header"">
            				<h6 class=""modal-title"">%s...</h4>
            			</div>
            			<div class=""modal-body"">
            				<div class=""box-body no-padding"">
            					<p>
            						%s：{api_name}<br><br>
                        %s：{api_type}<br><br>
                        %s <a href=""{api_path}"">{api_path}</a><br><br>
                        %s {api_js}<br><br>
                        %s <code>{api_res}</code><br>
            					</p>
            				</div>
            			</div>
            		</div>
            	</div>
            </div>""""""%(Utils().getMyWord(""{api_detail}""),Utils().getMyWord(""{api_name}""),Utils().getMyWord(""{r_type}""),Utils().getMyWord(""{r_api_addr}""),Utils().getMyWord(""{r_api_js}""),Utils().getMyWord(""{r_api_res}""))
            sql = ""select path from js_file where id='%s'"" % (api_info[6])
            cursor.execute(sql)
            js_path = cursor.fetchone()
            if api_info[5] == 2:
                api_type = Utils().getMyWord(""{r_post}"")
            else:
                api_type = Utils().getMyWord(""{r_get}"")
            if api_info[4] == None:
                api_res = """"
            else:
                api_res = api_info[4]
            api_info_html = api_info_html.replace(""{api_id}"", str(api_id))
            api_info_html = api_info_html.replace(""{api_type}"", str(api_type))
            api_info_html = api_info_html.replace(""{api_name}"", api_info[2])
            api_info_html = api_info_html.replace(""{api_path}"", api_info[1])
            api_info_html = api_info_html.replace(""{api_js}"", js_path[0])
            api_info_html = api_info_html.replace(""{api_res}"", api_res)
            api_id = api_id + 1
            whole_html = whole_html + api_info_html
        return whole_html","for api_info in api_infos:
    api_info_html = '<div id=""api_{api_id}"" class=""modal fade"" role=""dialog"">\n            \t<div class=""modal-dialog"">\n            \t\t<div class=""modal-content"">\n            \t\t\t<div class=""modal-header"">\n            \t\t\t\t<h6 class=""modal-title"">%s...</h4>\n            \t\t\t</div>\n            \t\t\t<div class=""modal-body"">\n            \t\t\t\t<div class=""box-body no-padding"">\n            \t\t\t\t\t<p>\n            \t\t\t\t\t\t%s：{api_name}<br><br>\n                        %s：{api_type}<br><br>\n                        %s <a href=""{api_path}"">{api_path}</a><br><br>\n                        %s {api_js}<br><br>\n                        %s <code>{api_res}</code><br>\n            \t\t\t\t\t</p>\n            \t\t\t\t</div>\n            \t\t\t</div>\n            \t\t</div>\n            \t</div>\n            </div>' % (Utils().getMyWord('{api_detail}'), Utils().getMyWord('{api_name}'), Utils().getMyWord('{r_type}'), Utils().getMyWord('{r_api_addr}'), Utils().getMyWord('{r_api_js}'), Utils().getMyWord('{r_api_res}'))
    sql = ""select path from js_file where id='%s'"" % api_info[6]
    cursor.execute(sql)
    js_path = cursor.fetchone()
    if api_info[5] == 2:
        api_type = Utils().getMyWord('{r_post}')
    else:
        api_type = Utils().getMyWord('{r_get}')
    if api_info[4] == None:
        api_res = ''
    else:
        api_res = api_info[4]
    api_info_html = api_info_html.replace('{api_id}', str(api_id))
    api_info_html = api_info_html.replace('{api_type}', str(api_type))
    api_info_html = api_info_html.replace('{api_name}', api_info[2])
    api_info_html = api_info_html.replace('{api_path}', api_info[1])
    api_info_html = api_info_html.replace('{api_js}', js_path[0])
    api_info_html = api_info_html.replace('{api_res}', api_res)
    api_id = api_id + 1
    whole_html = whole_html + api_info_html","for api_info in api_infos:
    (_, api_info_1, api_info_2, _, api_info_4, api_info_5, api_info_6, *_) = api_info
    api_info_html = '<div id=""api_{api_id}"" class=""modal fade"" role=""dialog"">\n            \t<div class=""modal-dialog"">\n            \t\t<div class=""modal-content"">\n            \t\t\t<div class=""modal-header"">\n            \t\t\t\t<h6 class=""modal-title"">%s...</h4>\n            \t\t\t</div>\n            \t\t\t<div class=""modal-body"">\n            \t\t\t\t<div class=""box-body no-padding"">\n            \t\t\t\t\t<p>\n            \t\t\t\t\t\t%s：{api_name}<br><br>\n                        %s：{api_type}<br><br>\n                        %s <a href=""{api_path}"">{api_path}</a><br><br>\n                        %s {api_js}<br><br>\n                        %s <code>{api_res}</code><br>\n            \t\t\t\t\t</p>\n            \t\t\t\t</div>\n            \t\t\t</div>\n            \t\t</div>\n            \t</div>\n            </div>' % (Utils().getMyWord('{api_detail}'), Utils().getMyWord('{api_name}'), Utils().getMyWord('{r_type}'), Utils().getMyWord('{r_api_addr}'), Utils().getMyWord('{r_api_js}'), Utils().getMyWord('{r_api_res}'))
    sql = ""select path from js_file where id='%s'"" % api_info[6]
    cursor.execute(sql)
    js_path = cursor.fetchone()
    if api_info[5] == 2:
        api_type = Utils().getMyWord('{r_post}')
    else:
        api_type = Utils().getMyWord('{r_get}')
    if api_info[4] == None:
        api_res = ''
    else:
        api_res = api_info[4]
    api_info_html = api_info_html.replace('{api_id}', str(api_id))
    api_info_html = api_info_html.replace('{api_type}', str(api_type))
    api_info_html = api_info_html.replace('{api_name}', api_info[2])
    api_info_html = api_info_html.replace('{api_path}', api_info[1])
    api_info_html = api_info_html.replace('{api_js}', js_path[0])
    api_info_html = api_info_html.replace('{api_res}', api_res)
    api_id = api_id + 1
    whole_html = whole_html + api_info_html","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, _, e_4, e_5, e_6 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_4: e[4]
e_5: e[5]
e_6: e[6]",,,,,,,
sparkup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparkup/sparkup.py,https://github.com/rstacruz/sparkup/tree/master//sparkup.py,Parser,_textmatify$404,"def _textmatify(self, output):
        """"""Returns a version of the output with TextMate placeholders in it.
        """"""

        matches = re.findall(r'(></)|("""")|(\n\s+)\n|(.|\s)', output)
        output = ''
        n = 1
        for i in matches:
            if i[0]:
                output += '>$%i</' % n
                n += 1
            elif i[1]:
                output += '""$%i""' % n
                n += 1
            elif i[2]:
                output += i[2] + '$%i\n' % n
                n += 1
            elif i[3]:
                output += i[3]
        output += ""$0""
        return output","for i in matches:
    if i[0]:
        output += '>$%i</' % n
        n += 1
    elif i[1]:
        output += '""$%i""' % n
        n += 1
    elif i[2]:
        output += i[2] + '$%i\n' % n
        n += 1
    elif i[3]:
        output += i[3]","for i in matches:
    (i_0, i_1, i_2, i_3, *_) = i
    if i[0]:
        output += '>$%i</' % n
        n += 1
    elif i[1]:
        output += '""$%i""' % n
        n += 1
    elif i[2]:
        output += i[2] + '$%i\n' % n
        n += 1
    elif i[3]:
        output += i[3]","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
tartube,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tartube/tartube/config.py,https://github.com/axcore/tartube/tree/master/tartube/config.py,SystemPrefWin,on_data_dir_move_up_button_clicked$28440,"def on_data_dir_move_up_button_clicked(self, button, treeview, liststore,
    button2):

        """"""Called from callback in self.setup_files_database_tab().

        Moves the selected data directory up one position in the list of
        alternative data directories.

        Args:

            button (Gtk.Button): The widget that was clicked (the up button)

            treeview (Gtk.TreeView): The widget in which a line was selected

            liststore (Gtk.ListStore): The treeview's liststore

            button2 (Gtk.Button): The down button

        """"""

        selection = treeview.get_selection()
        (model, path_list) = selection.get_selected_rows()
        if not path_list:

            # Nothing selected
            return

        # (Keeping track of the first/last selected items helps us to
        #   (de)sensitise buttons, in a moment)
        first_item = None
        last_item = None

        # Move the selected items up
        for path in path_list:

            this_iter = model.get_iter(path)
            last_item = model[this_iter][0]
            if first_item is None:
                first_item = model[this_iter][0]

            if model.iter_previous(this_iter):

                liststore.move_before(
                    this_iter,
                    model.iter_previous(this_iter),
                )

            else:

                # If the first item won't move up, then successive items will
                #   be moved above this one (which is not what we want)
                break

        # Update the IV
        dir_list = []
        for row in liststore:
            dir_list.append(row[0])

        self.app_obj.set_data_dir_alt_list(dir_list)

        # (De)sensitise the button(s), if required
        if dir_list.index(first_item) == 0:
            button.set_sensitive(False)
        else:
            button.set_sensitive(True)

        if dir_list.index(last_item) == (len(dir_list) - 1):
            button2.set_sensitive(False)
        else:
            button2.set_sensitive(True)","for row in liststore:
    dir_list.append(row[0])","for row in liststore:
    (row_0, *row_rrowmaining) = row
    dir_list.append(row[0])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
3DDFA_V2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/3DDFA_V2/FaceBoxes/FaceBoxes.py,https://github.com/cleardusk/3DDFA_V2/tree/master/FaceBoxes/FaceBoxes.py,FaceBoxes,__call__$58,"def __call__(self, img_):
        img_raw = img_.copy()

        # scaling to speed up
        scale = 1
        if scale_flag:
            h, w = img_raw.shape[:2]
            if h > HEIGHT:
                scale = HEIGHT / h
            if w * scale > WIDTH:
                scale *= WIDTH / (w * scale)
            # print(scale)
            if scale == 1:
                img_raw_scale = img_raw
            else:
                h_s = int(scale * h)
                w_s = int(scale * w)
                # print(h_s, w_s)
                img_raw_scale = cv2.resize(img_raw, dsize=(w_s, h_s))
                # print(img_raw_scale.shape)

            img = np.float32(img_raw_scale)
        else:
            img = np.float32(img_raw)

        # forward
        _t = {'forward_pass': Timer(), 'misc': Timer()}
        im_height, im_width, _ = img.shape
        scale_bbox = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])
        img -= (104, 117, 123)
        img = img.transpose(2, 0, 1)
        img = torch.from_numpy(img).unsqueeze(0)

        _t['forward_pass'].tic()
        loc, conf = self.net(img)  # forward pass
        _t['forward_pass'].toc()
        _t['misc'].tic()
        priorbox = PriorBox(image_size=(im_height, im_width))
        priors = priorbox.forward()
        prior_data = priors.data
        boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])
        if scale_flag:
            boxes = boxes * scale_bbox / scale / resize
        else:
            boxes = boxes * scale_bbox / resize

        boxes = boxes.cpu().numpy()
        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]

        # ignore low scores
        inds = np.where(scores > confidence_threshold)[0]
        boxes = boxes[inds]
        scores = scores[inds]

        # keep top-K before NMS
        order = scores.argsort()[::-1][:top_k]
        boxes = boxes[order]
        scores = scores[order]

        # do NMS
        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)
        keep = nms(dets, nms_threshold)
        dets = dets[keep, :]

        # keep top-K faster NMS
        dets = dets[:keep_top_k, :]
        _t['misc'].toc()

        if self.timer_flag:
            print('Detection: {:d}/{:d} forward_pass_time: {:.4f}s misc: {:.4f}s'.format(1, 1, _t[
                'forward_pass'].average_time, _t['misc'].average_time))

        # filter using vis_thres
        det_bboxes = []
        for b in dets:
            if b[4] > vis_thres:
                xmin, ymin, xmax, ymax, score = b[0], b[1], b[2], b[3], b[4]
                bbox = [xmin, ymin, xmax, ymax, score]
                det_bboxes.append(bbox)

        return det_bboxes","for b in dets:
    if b[4] > vis_thres:
        (xmin, ymin, xmax, ymax, score) = (b[0], b[1], b[2], b[3], b[4])
        bbox = [xmin, ymin, xmax, ymax, score]
        det_bboxes.append(bbox)","for b in dets:
    (b_0, b_1, b_2, b_3, b_4, *_) = b
    if b[4] > vis_thres:
        (xmin, ymin, xmax, ymax, score) = (b[0], b[1], b[2], b[3], b[4])
        bbox = [xmin, ymin, xmax, ymax, score]
        det_bboxes.append(bbox)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,
supervisor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/supervisor/supervisor/xmlrpc.py,https://github.com/home-assistant/supervisor/tree/master/supervisor/xmlrpc.py,SystemNamespaceRPCInterface,methodSignature$197,"def methodSignature(self, name):
        """""" Return an array describing the method signature in the
        form [rtype, ptype, ptype...] where rtype is the return data type
        of the method, and ptypes are the parameter data types that the
        method accepts in method argument order.

        @param string name  The name of the method.
        @return array result  The result.
        """"""
        methods = self._listMethods()
        for method in methods:
            if method == name:
                rtype = None
                ptypes = []
                parsed = gettags(methods[method])
                for thing in parsed:
                    if thing[1] == 'return': # tag name
                        rtype = thing[2] # datatype
                    elif thing[1] == 'param': # tag name
                        ptypes.append(thing[2]) # datatype
                if rtype is None:
                    raise RPCError(Faults.SIGNATURE_UNSUPPORTED)
                return [rtype] + ptypes
        raise RPCError(Faults.SIGNATURE_UNSUPPORTED)","for thing in parsed:
    if thing[1] == 'return':
        rtype = thing[2]
    elif thing[1] == 'param':
        ptypes.append(thing[2])","for thing in parsed:
    (_, thing_1, thing_2, *thing_rthingmaining) = thing
    if thing[1] == 'return':
        rtype = thing[2]
    elif thing[1] == 'param':
        ptypes.append(thing[2])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
Vxscan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Vxscan/analyzer.py,https://github.com/al0ne/Vxscan/tree/master//analyzer.py,,gener$97,"def gener():
    f = open('output.log', 'a', encoding='utf-8')
    webinfo = Sqldb(dbname).query('select domain,ipaddr,title,server,apps,waf,os from webinfo')
    for i in webinfo:
        domain, ipaddr, title, server, apps, waf, os = i
        print('\n' + '*' * 40 + ' ' + domain + ' ' + '*' * 40)
        f.write('\n' + '*' * 40 + ' ' + domain + ' ' + '*' * 40 + '\n')
        print('{}|{}|{}|{}|{}'.format(domain, ipaddr, title, server, waf))
        f.write('{}|{}|{}|{}|{}'.format(domain, ipaddr, title, server, waf) + '\n')
        print('指纹：' + str(apps))
        f.write('指纹：' + str(apps) + '\n')
        print('操作系统：' + str(os))
        f.write('操作系统：' + str(os) + '\n')
        ports = Sqldb(dbname).query(f""select ipaddr,service,port from ports where ipaddr = '{domain}'"")
        for port in ports:
            domain, server, port = port
            print(domain, server, port)
            f.write('{}\t{}\t{}'.format(domain, server, port) + '\n')
        urls = Sqldb(dbname).query(f""select title,url,contype,rsp_len,rsp_code from urls where domain = '{domain}'"")
        for url in urls:
            title, url, contype, rsp_len, rsp_code = url
            print('{}\t{}\t{}\t{}t{}'.format(title, url, contype, rsp_len, rsp_code))
            f.write('{}\t{}\t{}\t{}t{}'.format(title, url, contype, rsp_len, rsp_code) + '\n')
        vulns = Sqldb(dbname).query(f""select vuln from vuln where domain = '{ipaddr}'"")
        for vuln in vulns:
            print(vuln[0])
            f.write(vuln[0] + '\n')","for vuln in vulns:
    print(vuln[0])
    f.write(vuln[0] + '\n')","for vuln in vulns:
    (vuln_0, *vuln_rvulnmaining) = vuln
    print(vuln[0])
    f.write(vuln[0] + '\n')","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
electricitymap-contrib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electricitymap-contrib/parsers/CL.py,https://github.com/tmrowco/electricitymap-contrib/tree/master/parsers/CL.py,,production_processor_live$45,"def production_processor_live(json_tot, json_ren):
    """"""
    Extracts generation data and timestamp into dictionary.
    Returns a list of dictionaries for all of the available ""live"" data, usually that day.
    """"""

    gen_total = json_tot[""data""][0][""values""]

    if json_ren[""data""][1][""key""] == ""ENERGÍA SOLAR"":
        rawgen_sol = json_ren[""data""][1][""values""]
    else:
        raise RuntimeError(
            f""Unexpected data label. Expected 'ENERGÍA SOLAR' and got {json_ren['data'][1]['key']}""
        )

    if json_ren[""data""][0][""key""] == ""ENERGÍA EÓLICA"":
        rawgen_wind = json_ren[""data""][0][""values""]
    else:
        raise RuntimeError(
            f""Unexpected data label. Expected 'ENERGÍA EÓLICA' and got {json_ren['data'][0]['key']}""
        )

    mapped_totals = []

    for total in gen_total:
        datapoint = {}

        dt = total[0]
        for pair in rawgen_sol:
            if pair[0] == dt:
                solar = pair[1]
                break
        for pair in rawgen_wind:
            if pair[0] == dt:
                wind = pair[1]
                break

        datapoint[""datetime""] = arrow.get(
            dt / 1000, tzinfo=""Chile/Continental""
        ).datetime
        datapoint[""unknown""] = total[1] - wind - solar
        datapoint[""wind""] = wind
        datapoint[""solar""] = solar
        mapped_totals.append(datapoint)

    return mapped_totals","for total in gen_total:
    datapoint = {}
    dt = total[0]
    for pair in rawgen_sol:
        if pair[0] == dt:
            solar = pair[1]
            break
    for pair in rawgen_wind:
        if pair[0] == dt:
            wind = pair[1]
            break
    datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime
    datapoint['unknown'] = total[1] - wind - solar
    datapoint['wind'] = wind
    datapoint['solar'] = solar
    mapped_totals.append(datapoint)","for total in gen_total:
    (total_0, total_1, *_) = total
    datapoint = {}
    dt = total[0]
    for pair in rawgen_sol:
        if pair[0] == dt:
            solar = pair[1]
            break
    for pair in rawgen_wind:
        if pair[0] == dt:
            wind = pair[1]
            break
    datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime
    datapoint['unknown'] = total[1] - wind - solar
    datapoint['wind'] = wind
    datapoint['solar'] = solar
    mapped_totals.append(datapoint)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
electricitymap-contrib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electricitymap-contrib/parsers/CL.py,https://github.com/tmrowco/electricitymap-contrib/tree/master/parsers/CL.py,,production_processor_live$45,"def production_processor_live(json_tot, json_ren):
    """"""
    Extracts generation data and timestamp into dictionary.
    Returns a list of dictionaries for all of the available ""live"" data, usually that day.
    """"""

    gen_total = json_tot[""data""][0][""values""]

    if json_ren[""data""][1][""key""] == ""ENERGÍA SOLAR"":
        rawgen_sol = json_ren[""data""][1][""values""]
    else:
        raise RuntimeError(
            f""Unexpected data label. Expected 'ENERGÍA SOLAR' and got {json_ren['data'][1]['key']}""
        )

    if json_ren[""data""][0][""key""] == ""ENERGÍA EÓLICA"":
        rawgen_wind = json_ren[""data""][0][""values""]
    else:
        raise RuntimeError(
            f""Unexpected data label. Expected 'ENERGÍA EÓLICA' and got {json_ren['data'][0]['key']}""
        )

    mapped_totals = []

    for total in gen_total:
        datapoint = {}

        dt = total[0]
        for pair in rawgen_sol:
            if pair[0] == dt:
                solar = pair[1]
                break
        for pair in rawgen_wind:
            if pair[0] == dt:
                wind = pair[1]
                break

        datapoint[""datetime""] = arrow.get(
            dt / 1000, tzinfo=""Chile/Continental""
        ).datetime
        datapoint[""unknown""] = total[1] - wind - solar
        datapoint[""wind""] = wind
        datapoint[""solar""] = solar
        mapped_totals.append(datapoint)

    return mapped_totals","for pair in rawgen_sol:
    if pair[0] == dt:
        solar = pair[1]
        break","for pair in rawgen_sol:
    (pair_0, pair_1, *_) = pair
    if pair[0] == dt:
        solar = pair[1]
        break","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
electricitymap-contrib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electricitymap-contrib/parsers/CL.py,https://github.com/tmrowco/electricitymap-contrib/tree/master/parsers/CL.py,,production_processor_live$45,"def production_processor_live(json_tot, json_ren):
    """"""
    Extracts generation data and timestamp into dictionary.
    Returns a list of dictionaries for all of the available ""live"" data, usually that day.
    """"""

    gen_total = json_tot[""data""][0][""values""]

    if json_ren[""data""][1][""key""] == ""ENERGÍA SOLAR"":
        rawgen_sol = json_ren[""data""][1][""values""]
    else:
        raise RuntimeError(
            f""Unexpected data label. Expected 'ENERGÍA SOLAR' and got {json_ren['data'][1]['key']}""
        )

    if json_ren[""data""][0][""key""] == ""ENERGÍA EÓLICA"":
        rawgen_wind = json_ren[""data""][0][""values""]
    else:
        raise RuntimeError(
            f""Unexpected data label. Expected 'ENERGÍA EÓLICA' and got {json_ren['data'][0]['key']}""
        )

    mapped_totals = []

    for total in gen_total:
        datapoint = {}

        dt = total[0]
        for pair in rawgen_sol:
            if pair[0] == dt:
                solar = pair[1]
                break
        for pair in rawgen_wind:
            if pair[0] == dt:
                wind = pair[1]
                break

        datapoint[""datetime""] = arrow.get(
            dt / 1000, tzinfo=""Chile/Continental""
        ).datetime
        datapoint[""unknown""] = total[1] - wind - solar
        datapoint[""wind""] = wind
        datapoint[""solar""] = solar
        mapped_totals.append(datapoint)

    return mapped_totals","for pair in rawgen_wind:
    if pair[0] == dt:
        wind = pair[1]
        break","for pair in rawgen_wind:
    (pair_0, pair_1, *_) = pair
    if pair[0] == dt:
        wind = pair[1]
        break","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
dulwich,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dulwich/dulwich/tests/compat/test_client.py,https://github.com/dulwich/dulwich/tree/master/dulwich/tests/compat/test_client.py,DulwichClientTestBase,test_fetch_pack_no_side_band_64k$303,"def test_fetch_pack_no_side_band_64k(self):
        c = self._client()
        c._fetch_capabilities.remove(b""side-band-64k"")
        with repo.Repo(os.path.join(self.gitroot, ""dest"")) as dest:
            result = c.fetch(self._build_path(""/server_new.export""), dest)
            for r in result.refs.items():
                dest.refs.set_if_equals(r[0], None, r[1])
            self.assertDestEqualsSrc()","for r in result.refs.items():
    dest.refs.set_if_equals(r[0], None, r[1])","for r in result.refs.items():
    (r_0, r_1, *_) = r
    dest.refs.set_if_equals(r[0], None, r[1])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pytorch_HMR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch_HMR/src/dataloader/AICH_dataloader.py,https://github.com/MandyMo/pytorch_HMR/tree/master/src/dataloader/AICH_dataloader.py,AICH_dataloader,_collide_heavily$89,"def _collide_heavily(box, boxs):
            for it in boxs:
                if get_rectangle_intersect_ratio(box[0], box[1], it[0], it[1]) > self.max_intersec_ratio:
                    return True
            return False","for it in boxs:
    if get_rectangle_intersect_ratio(box[0], box[1], it[0], it[1]) > self.max_intersec_ratio:
        return True","for it in boxs:
    (it_0, it_1, *_) = it
    if get_rectangle_intersect_ratio(box[0], box[1], it[0], it[1]) > self.max_intersec_ratio:
        return True","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
fuzzbench,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fuzzbench/common/gcloud.py,https://github.com/google/fuzzbench/tree/master/common/gcloud.py,,create_instance_template$139,"def create_instance_template(template_name, docker_image, env, project, zone):
    """"""Returns a ProcessResult from running the command to create an instance
    template.""""""
    # Creating an instance template cannot be done using the GCE API because
    # there is no public API for handling some docker related functionality that
    # we need.
    command = [
        'gcloud', 'compute', '--project', project, 'instance-templates',
        'create-with-container', template_name, '--no-address',
        '--image-family=cos-stable', '--image-project=cos-cloud',
        f'--region={zone}', '--scopes=cloud-platform',
        f'--machine-type={MEASURER_WORKER_MACHINE_TYPE}',
        f'--boot-disk-size={MEASURER_WORKER_BOOT_DISK_SIZE}', '--preemptible',
        '--container-image', docker_image
    ]
    for item in env.items():
        command.extend(['--container-env', f'{item[0]}={item[1]}'])
    new_process.execute(command)
    return posixpath.join('https://www.googleapis.com/compute/v1/projects/',
                          project, 'global', 'instanceTemplates', template_name)","for item in env.items():
    command.extend(['--container-env', f'{item[0]}={item[1]}'])","for item in env.items():
    (item_0, item_1, *_) = item
    command.extend(['--container-env', f'{item[0]}={item[1]}'])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
mysql-connector-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mysql-connector-python/examples/dates.py,https://github.com/mysql/mysql-connector-python/tree/master/examples/dates.py,,main$56,"def main(config):
    output = []
    db = mysql.connector.Connect(**config)
    cursor = db.cursor()

    tbl = 'myconnpy_dates'

    cursor.execute('SET sql_mode = """"')

    # Drop table if exists, and create it new
    stmt_drop = ""DROP TABLE IF EXISTS {0}"".format(tbl)
    cursor.execute(stmt_drop)

    stmt_create = (
        ""CREATE TABLE {0} ( ""
        ""  `id` tinyint(4) NOT NULL AUTO_INCREMENT, ""
        ""  `c1` date DEFAULT NULL, ""
        ""  `c2` datetime NOT NULL, ""
        ""  `c3` time DEFAULT NULL, ""
        ""  `changed` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ""
        ""    ON UPDATE CURRENT_TIMESTAMP, ""
        ""PRIMARY KEY (`id`))""
    ).format(tbl)
    cursor.execute(stmt_create)

    # not using executemany to handle errors better
    stmt_insert = (""INSERT INTO {0} (c1,c2,c3) VALUES ""
                   ""(%s,%s,%s)"".format(tbl))
    for data in DATA:
        try:
            cursor.execute(stmt_insert, data)
        except (mysql.connector.errors.Error, TypeError) as exc:
            output.append(""Failed inserting {0}\nError: {1}\n"".format(
                data, exc))
            cursor.execute(stmt_drop)
            raise

    # Read the names again and print them
    stmt_select = ""SELECT * FROM {0} ORDER BY id"".format(tbl)
    cursor.execute(stmt_select)

    for row in cursor.fetchall():
        output.append(""%3s | %10s | %19s | %8s |"" % (
            row[0],
            row[1],
            row[2],
            row[3],
        ))

    # Cleaning up, dropping the table again
    cursor.execute(stmt_drop)

    cursor.close()
    db.close()
    return output","for row in cursor.fetchall():
    output.append('%3s | %10s | %19s | %8s |' % (row[0], row[1], row[2], row[3]))","for row in cursor.fetchall():
    (row_0, row_1, row_2, row_3, *_) = row
    output.append('%3s | %10s | %19s | %8s |' % (row[0], row[1], row[2], row[3]))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
rotki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rotki/rotkehlchen/db/dbhandler.py,https://github.com/rotki/rotki/tree/master/rotkehlchen/db/dbhandler.py,DBHandler,get_tags$2597,"def get_tags(self, cursor: 'DBCursor') -> Dict[str, Tag]:
        tags_mapping: Dict[str, Tag] = {}
        cursor.execute(
            'SELECT name, description, background_color, foreground_color FROM tags;',
        )
        for result in cursor:
            name = result[0]
            description = result[1]

            if description is not None and not isinstance(description, str):
                self.msg_aggregator.add_warning(
                    f'Tag {name} with invalid description found in the DB. Skipping tag',
                )
                continue

            try:
                background_color = deserialize_hex_color_code(result[2])
                foreground_color = deserialize_hex_color_code(result[3])
            except DeserializationError as e:
                self.msg_aggregator.add_warning(
                    f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag',
                )
                continue

            tags_mapping[name] = Tag(
                name=name,
                description=description,
                background_color=background_color,
                foreground_color=foreground_color,
            )

        return tags_mapping","for result in cursor:
    name = result[0]
    description = result[1]
    if description is not None and (not isinstance(description, str)):
        self.msg_aggregator.add_warning(f'Tag {name} with invalid description found in the DB. Skipping tag')
        continue
    try:
        background_color = deserialize_hex_color_code(result[2])
        foreground_color = deserialize_hex_color_code(result[3])
    except DeserializationError as e:
        self.msg_aggregator.add_warning(f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag')
        continue
    tags_mapping[name] = Tag(name=name, description=description, background_color=background_color, foreground_color=foreground_color)","for result in cursor:
    (result_0, result_1, result_2, result_3, *_) = result
    name = result[0]
    description = result[1]
    if description is not None and (not isinstance(description, str)):
        self.msg_aggregator.add_warning(f'Tag {name} with invalid description found in the DB. Skipping tag')
        continue
    try:
        background_color = deserialize_hex_color_code(result[2])
        foreground_color = deserialize_hex_color_code(result[3])
    except DeserializationError as e:
        self.msg_aggregator.add_warning(f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag')
        continue
    tags_mapping[name] = Tag(name=name, description=description, background_color=background_color, foreground_color=foreground_color)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/smooth_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/smooth_uv.py,MUV_OT_SmoothUV,__smooth_wo_transmission$107,"def __smooth_wo_transmission(self, loop_seqs, uv_layer):
        # calculate path length
        loops = []
        for hseq in loop_seqs:
            loops.extend([hseq[0][0], hseq[0][1]])
        full_vlen = 0
        accm_vlens = [0.0]
        full_uvlen = 0
        accm_uvlens = [0.0]
        orig_uvs = [loop_seqs[0][0][0][uv_layer].uv.copy()]
        for l1, l2 in zip(loops[:-1], loops[1:]):
            diff_v = l2.vert.co - l1.vert.co
            full_vlen = full_vlen + diff_v.length
            accm_vlens.append(full_vlen)
            diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
            full_uvlen = full_uvlen + diff_uv.length
            accm_uvlens.append(full_uvlen)
            orig_uvs.append(l2[uv_layer].uv.copy())

        for hidx, hseq in enumerate(loop_seqs):
            pair = hseq[0]
            for pidx, l in enumerate(pair):
                if self.select:
                    l[uv_layer].select = True

                # ignore start/end loop
                if (hidx == 0 and pidx == 0) or\
                   ((hidx == len(loop_seqs) - 1) and (pidx == len(pair) - 1)):
                    continue

                # calculate target path length
                # target = no influenced * (1 - infl) + influenced * infl
                tgt_noinfl = full_uvlen * (hidx + pidx) / (len(loop_seqs))
                tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
                target_length = tgt_noinfl * (1 - self.mesh_infl) + \
                    tgt_infl * self.mesh_infl

                # get target UV
                for i in range(len(accm_uvlens[:-1])):
                    # get line segment which UV will be placed
                    if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                        tgt_seg_len = target_length - accm_uvlens[i]
                        seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                        uv1 = orig_uvs[i]
                        uv2 = orig_uvs[i + 1]
                        target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                        break
                else:
                    self.report({'ERROR'}, ""Failed to get target UV"")
                    return {'CANCELLED'}

                # update UV
                l[uv_layer].uv = target_uv","for hseq in loop_seqs:
    loops.extend([hseq[0][0], hseq[0][1]])","for hseq in loop_seqs:
    (((hseq_0_0, hseq_0_1), *hseq_0_rhseqmaining), *hseq_rhseqmaining) = hseq
    loops.extend([hseq[0][0], hseq[0][1]])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",0,,,"Answer: Yes
Iterable Unpacking: ((e_0_0, e_0_1), *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]
e_0_1: e[0][1]",,,,,,,
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/smooth_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/smooth_uv.py,MUV_OT_SmoothUV,__smooth_wo_transmission$107,"def __smooth_wo_transmission(self, loop_seqs, uv_layer):
        # calculate path length
        loops = []
        for hseq in loop_seqs:
            loops.extend([hseq[0][0], hseq[0][1]])
        full_vlen = 0
        accm_vlens = [0.0]
        full_uvlen = 0
        accm_uvlens = [0.0]
        orig_uvs = [loop_seqs[0][0][0][uv_layer].uv.copy()]
        for l1, l2 in zip(loops[:-1], loops[1:]):
            diff_v = l2.vert.co - l1.vert.co
            full_vlen = full_vlen + diff_v.length
            accm_vlens.append(full_vlen)
            diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
            full_uvlen = full_uvlen + diff_uv.length
            accm_uvlens.append(full_uvlen)
            orig_uvs.append(l2[uv_layer].uv.copy())

        for hidx, hseq in enumerate(loop_seqs):
            pair = hseq[0]
            for pidx, l in enumerate(pair):
                if self.select:
                    l[uv_layer].select = True

                # ignore start/end loop
                if (hidx == 0 and pidx == 0) or\
                   ((hidx == len(loop_seqs) - 1) and (pidx == len(pair) - 1)):
                    continue

                # calculate target path length
                # target = no influenced * (1 - infl) + influenced * infl
                tgt_noinfl = full_uvlen * (hidx + pidx) / (len(loop_seqs))
                tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
                target_length = tgt_noinfl * (1 - self.mesh_infl) + \
                    tgt_infl * self.mesh_infl

                # get target UV
                for i in range(len(accm_uvlens[:-1])):
                    # get line segment which UV will be placed
                    if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                        tgt_seg_len = target_length - accm_uvlens[i]
                        seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                        uv1 = orig_uvs[i]
                        uv2 = orig_uvs[i + 1]
                        target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                        break
                else:
                    self.report({'ERROR'}, ""Failed to get target UV"")
                    return {'CANCELLED'}

                # update UV
                l[uv_layer].uv = target_uv","for (l1, l2) in zip(loops[:-1], loops[1:]):
    diff_v = l2.vert.co - l1.vert.co
    full_vlen = full_vlen + diff_v.length
    accm_vlens.append(full_vlen)
    diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
    full_uvlen = full_uvlen + diff_uv.length
    accm_uvlens.append(full_uvlen)
    orig_uvs.append(l2[uv_layer].uv.copy())",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e[uv_layer], uv_layer is a variable that cannot be accessed directly using an integer type constant. Therefore, iterable unpacking is not applicable in this case as index is not an integer type constant.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.***************
Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e[uv_layer], uv_layer is a variable that cannot be accessed directly using an integer type constant. Therefore, iterable unpacking is not applicable in this case as index is not an integer type constant.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/smooth_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/smooth_uv.py,MUV_OT_SmoothUV,__smooth_wo_transmission$107,"def __smooth_wo_transmission(self, loop_seqs, uv_layer):
        # calculate path length
        loops = []
        for hseq in loop_seqs:
            loops.extend([hseq[0][0], hseq[0][1]])
        full_vlen = 0
        accm_vlens = [0.0]
        full_uvlen = 0
        accm_uvlens = [0.0]
        orig_uvs = [loop_seqs[0][0][0][uv_layer].uv.copy()]
        for l1, l2 in zip(loops[:-1], loops[1:]):
            diff_v = l2.vert.co - l1.vert.co
            full_vlen = full_vlen + diff_v.length
            accm_vlens.append(full_vlen)
            diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
            full_uvlen = full_uvlen + diff_uv.length
            accm_uvlens.append(full_uvlen)
            orig_uvs.append(l2[uv_layer].uv.copy())

        for hidx, hseq in enumerate(loop_seqs):
            pair = hseq[0]
            for pidx, l in enumerate(pair):
                if self.select:
                    l[uv_layer].select = True

                # ignore start/end loop
                if (hidx == 0 and pidx == 0) or\
                   ((hidx == len(loop_seqs) - 1) and (pidx == len(pair) - 1)):
                    continue

                # calculate target path length
                # target = no influenced * (1 - infl) + influenced * infl
                tgt_noinfl = full_uvlen * (hidx + pidx) / (len(loop_seqs))
                tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
                target_length = tgt_noinfl * (1 - self.mesh_infl) + \
                    tgt_infl * self.mesh_infl

                # get target UV
                for i in range(len(accm_uvlens[:-1])):
                    # get line segment which UV will be placed
                    if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                        tgt_seg_len = target_length - accm_uvlens[i]
                        seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                        uv1 = orig_uvs[i]
                        uv2 = orig_uvs[i + 1]
                        target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                        break
                else:
                    self.report({'ERROR'}, ""Failed to get target UV"")
                    return {'CANCELLED'}

                # update UV
                l[uv_layer].uv = target_uv","for (hidx, hseq) in enumerate(loop_seqs):
    pair = hseq[0]
    for (pidx, l) in enumerate(pair):
        if self.select:
            l[uv_layer].select = True
        if hidx == 0 and pidx == 0 or (hidx == len(loop_seqs) - 1 and pidx == len(pair) - 1):
            continue
        tgt_noinfl = full_uvlen * (hidx + pidx) / len(loop_seqs)
        tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
        target_length = tgt_noinfl * (1 - self.mesh_infl) + tgt_infl * self.mesh_infl
        for i in range(len(accm_uvlens[:-1])):
            if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                tgt_seg_len = target_length - accm_uvlens[i]
                seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                uv1 = orig_uvs[i]
                uv2 = orig_uvs[i + 1]
                target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                break
        else:
            self.report({'ERROR'}, 'Failed to get target UV')
            return {'CANCELLED'}
        l[uv_layer].uv = target_uv","for (hidx, hseq) in enumerate(loop_seqs):
    (hseq_0, *hseq_rhseqmaining) = hseq
    pair = hseq[0]
    for (pidx, l) in enumerate(pair):
        if self.select:
            l[uv_layer].select = True
        if hidx == 0 and pidx == 0 or (hidx == len(loop_seqs) - 1 and pidx == len(pair) - 1):
            continue
        tgt_noinfl = full_uvlen * (hidx + pidx) / len(loop_seqs)
        tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
        target_length = tgt_noinfl * (1 - self.mesh_infl) + tgt_infl * self.mesh_infl
        for i in range(len(accm_uvlens[:-1])):
            if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                tgt_seg_len = target_length - accm_uvlens[i]
                seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                uv1 = orig_uvs[i]
                uv2 = orig_uvs[i + 1]
                target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                break
        else:
            self.report({'ERROR'}, 'Failed to get target UV')
            return {'CANCELLED'}
        l[uv_layer].uv = target_uv",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/smooth_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/smooth_uv.py,MUV_OT_SmoothUV,__smooth_wo_transmission$107,"def __smooth_wo_transmission(self, loop_seqs, uv_layer):
        # calculate path length
        loops = []
        for hseq in loop_seqs:
            loops.extend([hseq[0][0], hseq[0][1]])
        full_vlen = 0
        accm_vlens = [0.0]
        full_uvlen = 0
        accm_uvlens = [0.0]
        orig_uvs = [loop_seqs[0][0][0][uv_layer].uv.copy()]
        for l1, l2 in zip(loops[:-1], loops[1:]):
            diff_v = l2.vert.co - l1.vert.co
            full_vlen = full_vlen + diff_v.length
            accm_vlens.append(full_vlen)
            diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
            full_uvlen = full_uvlen + diff_uv.length
            accm_uvlens.append(full_uvlen)
            orig_uvs.append(l2[uv_layer].uv.copy())

        for hidx, hseq in enumerate(loop_seqs):
            pair = hseq[0]
            for pidx, l in enumerate(pair):
                if self.select:
                    l[uv_layer].select = True

                # ignore start/end loop
                if (hidx == 0 and pidx == 0) or\
                   ((hidx == len(loop_seqs) - 1) and (pidx == len(pair) - 1)):
                    continue

                # calculate target path length
                # target = no influenced * (1 - infl) + influenced * infl
                tgt_noinfl = full_uvlen * (hidx + pidx) / (len(loop_seqs))
                tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
                target_length = tgt_noinfl * (1 - self.mesh_infl) + \
                    tgt_infl * self.mesh_infl

                # get target UV
                for i in range(len(accm_uvlens[:-1])):
                    # get line segment which UV will be placed
                    if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                        tgt_seg_len = target_length - accm_uvlens[i]
                        seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                        uv1 = orig_uvs[i]
                        uv2 = orig_uvs[i + 1]
                        target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                        break
                else:
                    self.report({'ERROR'}, ""Failed to get target UV"")
                    return {'CANCELLED'}

                # update UV
                l[uv_layer].uv = target_uv","for (pidx, l) in enumerate(pair):
    if self.select:
        l[uv_layer].select = True
    if hidx == 0 and pidx == 0 or (hidx == len(loop_seqs) - 1 and pidx == len(pair) - 1):
        continue
    tgt_noinfl = full_uvlen * (hidx + pidx) / len(loop_seqs)
    tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
    target_length = tgt_noinfl * (1 - self.mesh_infl) + tgt_infl * self.mesh_infl
    for i in range(len(accm_uvlens[:-1])):
        if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
            tgt_seg_len = target_length - accm_uvlens[i]
            seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
            uv1 = orig_uvs[i]
            uv2 = orig_uvs[i + 1]
            target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
            break
    else:
        self.report({'ERROR'}, 'Failed to get target UV')
        return {'CANCELLED'}
    l[uv_layer].uv = target_uv",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: The given unpacked element e[uv_eayer] is not valid as the variable name ""uv_eayer"" is not defined. Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
upvote_py2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/upvote_py2/upvote/monitoring/metrics.py,https://github.com/google/upvote_py2/tree/master/upvote/monitoring/metrics.py,Namespace,__init__$35,"def __init__(self, prefix, tuples):

    self.metrics = []

    for t in tuples:
      metric = Metric(prefix + t[0], t[1])
      setattr(self, t[0].upper(), metric)
      self.metrics.append(metric)

    setattr(self, 'ALL', self.metrics)","for t in tuples:
    metric = Metric(prefix + t[0], t[1])
    setattr(self, t[0].upper(), metric)
    self.metrics.append(metric)","for t in tuples:
    (t_0, t_1, *_) = t
    metric = Metric(prefix + t[0], t[1])
    setattr(self, t[0].upper(), metric)
    self.metrics.append(metric)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
clearml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clearml/clearml/utilities/gpu/pynvml.py,https://github.com/allegroai/clearml/tree/master/clearml/utilities/gpu/pynvml.py,,nvmlStructToFriendlyObject$754,"def nvmlStructToFriendlyObject(struct):
    d = {}
    for x in struct._fields_:
        key = x[0]
        value = getattr(struct, key)
        d[key] = value
    obj = nvmlFriendlyObject(d)
    return obj","for x in struct._fields_:
    key = x[0]
    value = getattr(struct, key)
    d[key] = value","for x in struct._fields_:
    (x_0, *x_rxmaining) = x
    key = x[0]
    value = getattr(struct, key)
    d[key] = value","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
mifthtools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mifthtools/blender/addons/2.8/mira_tools/mi_curve_stretch.py,https://github.com/mifth/mifthtools/tree/master/blender/addons/2.8/mira_tools/mi_curve_stretch.py,MI_OT_CurveStretch,start_tool$82,"def start_tool(self, context):
        # the arguments we pass the the callbackection
        args = (self, context)
        # Add the region OpenGL drawing callback
        # draw in view space with 'POST_VIEW' and 'PRE_VIEW'

        cur_stretch_settings = context.scene.mi_cur_stretch_settings
        curve_settings = context.scene.mi_settings

        active_obj = context.active_object
        bm = bmesh.from_edit_mesh(active_obj.data)


        self.manipulator = context.space_data.show_gizmo
        context.space_data.show_gizmo = False


        for loop in self.loops:
            loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in loop[0]]
            loop_line = cur_main.pass_line(loop_verts, loop[1])
            new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, loop[1])

            # set closed curve
            if loop[1] is True:
                new_curve.closed = True

            self.all_curves.append(new_curve)
            self.active_curve = new_curve

            cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)

            self.original_verts_data.append( cur_main.pass_line([bm.verts[i].co.copy() for i in loop[0]] , loop[1]) )

            # move point to the curve
            for curve in self.all_curves:
                update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])

            # get meshes for snapping
            if curve_settings.surface_snap is True:
                meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)
                if meshes_array:
                    self.picked_meshes = meshes_array

        self.mi_deform_handle_3d = bpy.types.SpaceView3D.draw_handler_add(mi_curve_draw_3d, args, 'WINDOW', 'POST_VIEW')
        self.mi_deform_handle_2d = bpy.types.SpaceView3D.draw_handler_add(mi_curve_draw_2d, args, 'WINDOW', 'POST_PIXEL')
        self.gh_circle_select_handle = bpy.types.SpaceView3D.draw_handler_add(gh_circle_draw_2d, args, 'WINDOW', 'POST_PIXEL')

        bm.normal_update()
        bmesh.update_edit_mesh(active_obj.data)","for loop in self.loops:
    loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in loop[0]]
    loop_line = cur_main.pass_line(loop_verts, loop[1])
    new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, loop[1])
    if loop[1] is True:
        new_curve.closed = True
    self.all_curves.append(new_curve)
    self.active_curve = new_curve
    cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)
    self.original_verts_data.append(cur_main.pass_line([bm.verts[i].co.copy() for i in loop[0]], loop[1]))
    for curve in self.all_curves:
        update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])
    if curve_settings.surface_snap is True:
        meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)
        if meshes_array:
            self.picked_meshes = meshes_array","for loop in self.loops:
    (loop_0, loop_1, *_) = loop
    loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in loop[0]]
    loop_line = cur_main.pass_line(loop_verts, loop[1])
    new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, loop[1])
    if loop[1] is True:
        new_curve.closed = True
    self.all_curves.append(new_curve)
    self.active_curve = new_curve
    cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)
    self.original_verts_data.append(cur_main.pass_line([bm.verts[i].co.copy() for i in loop[0]], loop[1]))
    for curve in self.all_curves:
        update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])
    if curve_settings.surface_snap is True:
        meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)
        if meshes_array:
            self.picked_meshes = meshes_array","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/azurearm_network.py,https://github.com/saltstack/salt/tree/master/salt/modules/azurearm_network.py,,security_rule_create_or_update$263,"def security_rule_create_or_update(
    name,
    access,
    direction,
    priority,
    protocol,
    security_group,
    resource_group,
    source_address_prefix=None,
    destination_address_prefix=None,
    source_port_range=None,
    destination_port_range=None,
    source_address_prefixes=None,
    destination_address_prefixes=None,
    source_port_ranges=None,
    destination_port_ranges=None,
    **kwargs
):
    """"""
    .. versionadded:: 2019.2.0

    Create or update a security rule within a specified network security group.

    :param name: The name of the security rule to create.

    :param access:
        'allow' or 'deny'

    :param direction:
        'inbound' or 'outbound'

    :param priority:
        Integer between 100 and 4096 used for ordering rule application.

    :param protocol:
        'tcp', 'udp', or '*'

    :param destination_address_prefix:
        The CIDR or destination IP range. Asterix '*' can also be used to match all destination IPs.
        Default tags such as 'VirtualNetwork', 'AzureLoadBalancer' and 'Internet' can also be used.
        If this is an ingress rule, specifies where network traffic originates from.

    :param destination_port_range:
        The destination port or range. Integer or range between 0 and 65535. Asterix '*'
        can also be used to match all ports.

    :param source_address_prefix:
        The CIDR or source IP range. Asterix '*' can also be used to match all source IPs.
        Default tags such as 'VirtualNetwork', 'AzureLoadBalancer' and 'Internet' can also be used.
        If this is an ingress rule, specifies where network traffic originates from.

    :param source_port_range:
        The source port or range. Integer or range between 0 and 65535. Asterix '*'
        can also be used to match all ports.

    :param destination_address_prefixes:
        A list of destination_address_prefix values. This parameter overrides destination_address_prefix
        and will cause any value entered there to be ignored.

    :param destination_port_ranges:
        A list of destination_port_range values. This parameter overrides destination_port_range
        and will cause any value entered there to be ignored.

    :param source_address_prefixes:
        A list of source_address_prefix values. This parameter overrides source_address_prefix
        and will cause any value entered there to be ignored.

    :param source_port_ranges:
        A list of source_port_range values. This parameter overrides source_port_range
        and will cause any value entered there to be ignored.

    :param security_group: The network security group containing the
        security rule.

    :param resource_group: The resource group name assigned to the
        network security group.

    CLI Example:

    .. code-block:: bash

        salt-call azurearm_network.security_rule_create_or_update testrule1 allow outbound 101 tcp testnsg testgroup \
                  source_address_prefix='*' destination_address_prefix=internet source_port_range='*' \
                  destination_port_range='1-1024'

    """"""
    exclusive_params = [
        (""source_port_ranges"", ""source_port_range""),
        (""source_address_prefixes"", ""source_address_prefix""),
        (""destination_port_ranges"", ""destination_port_range""),
        (""destination_address_prefixes"", ""destination_address_prefix""),
    ]

    for params in exclusive_params:
        # pylint: disable=eval-used
        if not eval(params[0]) and not eval(params[1]):
            log.error(
                ""Either the %s or %s parameter must be provided!"", params[0], params[1]
            )
            return False
        # pylint: disable=eval-used
        if eval(params[0]):
            # pylint: disable=exec-used
            exec(""{} = None"".format(params[1]))

    netconn = __utils__[""azurearm.get_client""](""network"", **kwargs)

    try:
        rulemodel = __utils__[""azurearm.create_object_model""](
            ""network"",
            ""SecurityRule"",
            name=name,
            access=access,
            direction=direction,
            priority=priority,
            protocol=protocol,
            source_port_ranges=source_port_ranges,
            source_port_range=source_port_range,
            source_address_prefixes=source_address_prefixes,
            source_address_prefix=source_address_prefix,
            destination_port_ranges=destination_port_ranges,
            destination_port_range=destination_port_range,
            destination_address_prefixes=destination_address_prefixes,
            destination_address_prefix=destination_address_prefix,
            **kwargs
        )
    except TypeError as exc:
        result = {""error"": ""The object model could not be built. ({})"".format(str(exc))}
        return result

    try:
        secrule = netconn.security_rules.create_or_update(
            resource_group_name=resource_group,
            network_security_group_name=security_group,
            security_rule_name=name,
            security_rule_parameters=rulemodel,
        )
        secrule.wait()
        secrule_result = secrule.result()
        result = secrule_result.as_dict()
    except CloudError as exc:
        __utils__[""azurearm.log_cloud_error""](""network"", str(exc), **kwargs)
        result = {""error"": str(exc)}
    except SerializationError as exc:
        result = {
            ""error"": ""The object model could not be parsed. ({})"".format(str(exc))
        }

    return result","for params in exclusive_params:
    if not eval(params[0]) and (not eval(params[1])):
        log.error('Either the %s or %s parameter must be provided!', params[0], params[1])
        return False
    if eval(params[0]):
        exec('{} = None'.format(params[1]))","for params in exclusive_params:
    (params_0, params_1, *_) = params
    if not eval(params[0]) and (not eval(params[1])):
        log.error('Either the %s or %s parameter must be provided!', params[0], params[1])
        return False
    if eval(params[0]):
        exec('{} = None'.format(params[1]))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
mvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mvt/mvt/ios/modules/mixed/chrome_favicon.py,https://github.com/mvt-project/mvt/tree/master/mvt/ios/modules/mixed/chrome_favicon.py,ChromeFavicon,run$48,"def run(self):
        self._find_ios_database(backup_ids=CHROME_FAVICON_BACKUP_IDS,
                                root_paths=CHROME_FAVICON_ROOT_PATHS)
        self.log.info(""Found Chrome favicon cache database at path: %s"", self.file_path)

        conn = sqlite3.connect(self.file_path)

        # Fetch icon cache
        cur = conn.cursor()
        cur.execute(""""""
            SELECT
                icon_mapping.page_url,
                favicons.url,
                favicon_bitmaps.last_updated,
                favicon_bitmaps.last_requested
            FROM icon_mapping
            JOIN favicon_bitmaps ON icon_mapping.icon_id = favicon_bitmaps.icon_id
            JOIN favicons ON icon_mapping.icon_id = favicons.id
            ORDER BY icon_mapping.id;
        """""")

        records = []
        for row in cur:
            last_timestamp = int(row[2]) or int(row[3])
            records.append({
                ""url"": row[0],
                ""icon_url"": row[1],
                ""timestamp"": last_timestamp,
                ""isodate"": convert_timestamp_to_iso(convert_chrometime_to_unix(last_timestamp)),
            })

        cur.close()
        conn.close()

        self.log.info(""Extracted a total of %d favicon records"", len(records))
        self.results = sorted(records, key=lambda row: row[""isodate""])","for row in cur:
    last_timestamp = int(row[2]) or int(row[3])
    records.append({'url': row[0], 'icon_url': row[1], 'timestamp': last_timestamp, 'isodate': convert_timestamp_to_iso(convert_chrometime_to_unix(last_timestamp))})","for row in cur:
    (row_0, row_1, row_2, row_3, *_) = row
    last_timestamp = int(row[2]) or int(row[3])
    records.append({'url': row[0], 'icon_url': row[1], 'timestamp': last_timestamp, 'isodate': convert_timestamp_to_iso(convert_chrometime_to_unix(last_timestamp))})","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
deluge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deluge/deluge/ui/gtk3/addtorrentdialog.py,https://github.com/deluge-torrent/deluge/tree/master/deluge/ui/gtk3/addtorrentdialog.py,AddTorrentDialog,_on_filename_edited$986,"def _on_filename_edited(self, renderer, path, new_text):
        index = self.files_treestore[path][3]

        new_text = new_text.strip(os.path.sep).strip()

        # Return if the text hasn't changed
        if new_text == self.files_treestore[path][1]:
            return

        # Get the tree iter
        itr = self.files_treestore.get_iter(path)

        # Get the torrent_id
        (model, row) = self.listview_torrents.get_selection().get_selected()
        torrent_id = model[row][0]

        if 'mapped_files' not in self.options[torrent_id]:
            self.options[torrent_id]['mapped_files'] = {}

        if index > -1:
            # We're renaming a file! Yay! That's easy!
            if not new_text:
                return
            parent = self.files_treestore.iter_parent(itr)
            file_path = os.path.join(self.get_file_path(parent), new_text)
            # Don't rename if filename exists
            if parent:
                for row in self.files_treestore[parent].iterchildren():
                    if new_text == row[1]:
                        return
            if os.path.sep in new_text:
                # There are folders in this path, so we need to create them
                # and then move the file iter to top
                split_text = new_text.split(os.path.sep)
                for s in split_text[:-1]:
                    parent = self.files_treestore.append(
                        parent, [True, s, 0, -1, False, 'folder-symbolic']
                    )

                self.files_treestore[itr][1] = split_text[-1]
                reparent_iter(self.files_treestore, itr, parent)
            else:
                # Update the row's text
                self.files_treestore[itr][1] = new_text

            # Update the mapped_files dict in the options with the index and new
            # file path.
            # We'll send this to the core when adding the torrent so it knows
            # what to rename before adding.
            self.options[torrent_id]['mapped_files'][index] = file_path
            self.files[torrent_id][index]['path'] = file_path
        else:
            # Folder!
            def walk_tree(row):
                if not row:
                    return

                # Get the file path base once, since it will be the same for
                # all siblings
                file_path_base = self.get_file_path(
                    self.files_treestore.iter_parent(row)
                )

                # Iterate through all the siblings at this level
                while row:
                    # We recurse if there are children
                    if self.files_treestore.iter_has_child(row):
                        walk_tree(self.files_treestore.iter_children(row))

                    index = self.files_treestore[row][3]

                    if index > -1:
                        # Get the new full path for this file
                        file_path = file_path_base + self.files_treestore[row][1]

                        # Update the file path in the mapped_files dict
                        self.options[torrent_id]['mapped_files'][index] = file_path
                        self.files[torrent_id][index]['path'] = file_path

                    # Get the next siblings iter
                    row = self.files_treestore.iter_next(row)

            # Update the treestore row first so that when walking the tree
            # we can construct the new proper paths

            # We need to check if this folder has been split
            if os.path.sep in new_text:
                # It's been split, so we need to add new folders and then re-parent
                # itr.
                parent = self.files_treestore.iter_parent(itr)
                split_text = new_text.split(os.path.sep)
                for s in split_text[:-1]:
                    # We don't iterate over the last item because we'll just use
                    # the existing itr and change the text
                    parent = self.files_treestore.append(
                        parent, [True, s + os.path.sep, 0, -1, False, 'folder-symbolic']
                    )

                self.files_treestore[itr][1] = split_text[-1] + os.path.sep

                # Now re-parent itr to parent
                reparent_iter(self.files_treestore, itr, parent)
                itr = parent

                # We need to re-expand the view because it might contracted
                # if we change the root iter
                root = Gtk.TreePath.new_first()
                self.listview_files.expand_row(root, False)
            else:
                # This was a simple folder rename without any splits, so just
                # change the path for itr
                self.files_treestore[itr][1] = new_text + os.path.sep

            # Walk through the tree from 'itr' and add all the new file paths
            # to the 'mapped_files' option
            walk_tree(itr)","for row in self.files_treestore[parent].iterchildren():
    if new_text == row[1]:
        return","for row in self.files_treestore[parent].iterchildren():
    (_, row_1, *row_rrowmaining) = row
    if new_text == row[1]:
        return","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/scripts/datasets/hmdb51.py,https://github.com/dmlc/gluon-cv/tree/master/scripts/datasets/hmdb51.py,,build_split_list$292,"def build_split_list(split, frame_info, shuffle=False):

    def build_set_list(set_list):
        rgb_list, flow_list = list(), list()
        for item in set_list:
            if item[0] not in frame_info:
                # print(""item:"", item)
                continue
            elif frame_info[item[0]][1] > 0:
                rgb_cnt = frame_info[item[0]][1]
                flow_cnt = frame_info[item[0]][2]
                rgb_list.append('{} {} {}\n'.format(
                    item[0], rgb_cnt, item[1]))
                flow_list.append('{} {} {}\n'.format(
                    item[0], flow_cnt, item[1]))
            else:
                rgb_list.append('{} {}\n'.format(
                    item[0], item[1]))
                flow_list.append('{} {}\n'.format(
                    item[0], item[1]))
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return rgb_list, flow_list

    train_rgb_list, train_flow_list = build_set_list(split[0])
    test_rgb_list, test_flow_list = build_set_list(split[1])
    return (train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list)","for item in set_list:
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))
        flow_list.append('{} {}\n'.format(item[0], item[1]))","for item in set_list:
    (item_0, item_1, *_) = item
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))
        flow_list.append('{} {}\n'.format(item[0], item[1]))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
arcade,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/arcade/arcade/draw_commands.py,https://github.com/pythonarcade/arcade/tree/master/arcade/draw_commands.py,,draw_arc_filled$41,"def draw_arc_filled(center_x: float, center_y: float,
                    width: float, height: float,
                    color: Color,
                    start_angle: float, end_angle: float,
                    tilt_angle: float = 0,
                    num_segments: int = 128):
    """"""
    Draw a filled in arc. Useful for drawing pie-wedges, or Pac-Man.

    :param float center_x: x position that is the center of the arc.
    :param float center_y: y position that is the center of the arc.
    :param float width: width of the arc.
    :param float height: height of the arc.
    :param Color color: color, specified in a list of 3 or 4 bytes in RGB or
         RGBA format.
    :param float start_angle: start angle of the arc in degrees.
    :param float end_angle: end angle of the arc in degrees.
    :param float tilt_angle: angle the arc is tilted.
    :param float num_segments: Number of line segments used to draw arc.
    """"""
    unrotated_point_list = [[0.0, 0.0]]

    start_segment = int(start_angle / 360 * num_segments)
    end_segment = int(end_angle / 360 * num_segments)

    for segment in range(start_segment, end_segment + 1):
        theta = 2.0 * 3.1415926 * segment / num_segments

        x = width * math.cos(theta) / 2
        y = height * math.sin(theta) / 2

        unrotated_point_list.append([x, y])

    if tilt_angle == 0:
        uncentered_point_list = unrotated_point_list
    else:
        uncentered_point_list = []
        for point in unrotated_point_list:
            uncentered_point_list.append(rotate_point(point[0], point[1], 0, 0, tilt_angle))

    point_list = []
    for point in uncentered_point_list:
        point_list.append((point[0] + center_x, point[1] + center_y))

    _generic_draw_line_strip(point_list, color, gl.GL_TRIANGLE_FAN)","for point in uncentered_point_list:
    point_list.append((point[0] + center_x, point[1] + center_y))","for point in uncentered_point_list:
    (point_0, point_1, *_) = point
    point_list.append((point[0] + center_x, point[1] + center_y))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
arcade,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/arcade/arcade/draw_commands.py,https://github.com/pythonarcade/arcade/tree/master/arcade/draw_commands.py,,draw_arc_filled$41,"def draw_arc_filled(center_x: float, center_y: float,
                    width: float, height: float,
                    color: Color,
                    start_angle: float, end_angle: float,
                    tilt_angle: float = 0,
                    num_segments: int = 128):
    """"""
    Draw a filled in arc. Useful for drawing pie-wedges, or Pac-Man.

    :param float center_x: x position that is the center of the arc.
    :param float center_y: y position that is the center of the arc.
    :param float width: width of the arc.
    :param float height: height of the arc.
    :param Color color: color, specified in a list of 3 or 4 bytes in RGB or
         RGBA format.
    :param float start_angle: start angle of the arc in degrees.
    :param float end_angle: end angle of the arc in degrees.
    :param float tilt_angle: angle the arc is tilted.
    :param float num_segments: Number of line segments used to draw arc.
    """"""
    unrotated_point_list = [[0.0, 0.0]]

    start_segment = int(start_angle / 360 * num_segments)
    end_segment = int(end_angle / 360 * num_segments)

    for segment in range(start_segment, end_segment + 1):
        theta = 2.0 * 3.1415926 * segment / num_segments

        x = width * math.cos(theta) / 2
        y = height * math.sin(theta) / 2

        unrotated_point_list.append([x, y])

    if tilt_angle == 0:
        uncentered_point_list = unrotated_point_list
    else:
        uncentered_point_list = []
        for point in unrotated_point_list:
            uncentered_point_list.append(rotate_point(point[0], point[1], 0, 0, tilt_angle))

    point_list = []
    for point in uncentered_point_list:
        point_list.append((point[0] + center_x, point[1] + center_y))

    _generic_draw_line_strip(point_list, color, gl.GL_TRIANGLE_FAN)","for point in unrotated_point_list:
    uncentered_point_list.append(rotate_point(point[0], point[1], 0, 0, tilt_angle))","for point in unrotated_point_list:
    (point_0, point_1, *_) = point
    uncentered_point_list.append(rotate_point(point[0], point[1], 0, 0, tilt_angle))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Listed-company-news-crawl-and-text-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Listed-company-news-crawl-and-text-analysis/Crawler/crawler_nbd.py,https://github.com/DemonDamon/Listed-company-news-crawl-and-text-analysis/tree/master/Crawler/crawler_nbd.py,WebCrawlFromNBD,multi_threads_run$268,"def multi_threads_run(self):
        '''Multi-threading running.
        '''
        page_ranges_lst = self.GenPagesLst()
        th_lst = []
        for page_range in page_ranges_lst:
            thread = threading.Thread(target=self.CrawlCompanyNews,\
                                      args=(page_range[0],page_range[1]))
            th_lst.append(thread)
        for thread in th_lst:
            thread.start()
        for thread in th_lst:
            thread.join()
        return self.url_lst_withoutNews","for page_range in page_ranges_lst:
    thread = threading.Thread(target=self.CrawlCompanyNews, args=(page_range[0], page_range[1]))
    th_lst.append(thread)","for page_range in page_ranges_lst:
    (page_range_0, page_range_1, *_) = page_range
    thread = threading.Thread(target=self.CrawlCompanyNews, args=(page_range[0], page_range[1]))
    th_lst.append(thread)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
textflint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/textflint/textflint/common/utils/overlap_templates.py,https://github.com/textflint/textflint/tree/master/textflint/common/utils/overlap_templates.py,,template_filler$205,"def template_filler(template_list):
    probs = []
    templates = []

    for template_pair in template_list:
        probs.append(template_pair[0])
        templates.append(template_pair[1])

    template_index = np.random.choice(range(len(templates)), p=probs)
    template_tuple = templates[template_index]
    template = template_tuple[0]
    hypothesis_template = template_tuple[1]
    template_tag = template_tuple[2]

    premise_list = []
    index_dict = {}

    for (index, element) in template:
        if element == ""VP"":
            vp, vp_parse = generate_vp()
            premise_list.append(vp)
            index_dict[index] = vp

        elif element == ""RC"":
            rc = generate_rc()
            premise_list.append(rc)
            index_dict[index] = rc

        elif ""vobj"" in element:
            obj = random.choice(object_dict[index_dict[int(element[-1])]])
            premise_list.append(obj)
            index_dict[index] = obj

        elif isinstance(element, str):
            premise_list.append(element)
            index_dict[index] = element

        else:
            word = random.choice(element)
            premise_list.append(word)
            index_dict[index] = word

    hypothesis_list = [index_dict[ind] for ind in hypothesis_template]

    return postprocess("" "".join(premise_list)), postprocess(
        "" "".join(hypothesis_list)), template_tag","for template_pair in template_list:
    probs.append(template_pair[0])
    templates.append(template_pair[1])","for template_pair in template_list:
    (template_pair_0, template_pair_1, *_) = template_pair
    probs.append(template_pair[0])
    templates.append(template_pair[1])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
few-shot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/few-shot/tests/test_utils.py,https://github.com/oscarknagg/few-shot/tree/master/tests/test_utils.py,TestAutogradGraphRetrieval,test_retrieval$89,"def test_retrieval(self):
        """"""Create a simple autograd graph and check that the output is what is expected""""""
        x = torch.ones(2, 2, requires_grad=True)
        y = x + 2
        # The operation on the next line will create two edges because the y variable is
        # y variable is used twice.
        z = y * y
        out = z.mean()

        nodes, edges = autograd_graph(out)

        # This is quite a brittle test as it will break if the names of the autograd Functions change
        # TODO: Less brittle test

        expected_nodes = [
            'MeanBackward1',
            'MulBackward0',
            'AddBackward0',
            'AccumulateGrad',
        ]

        self.assertEqual(
            set(expected_nodes),
            set(n.__class__.__name__ for n in nodes),
            'autograd_graph() must return all nodes in the autograd graph.'
        )

        # Check for the existence of the expected edges
        expected_edges = [
            ('MulBackward0', 'MeanBackward1'),  # z = y * y, out = z.mean()
            ('AddBackward0', 'MulBackward0'),    # y = x + 2, z = y * y
            ('AccumulateGrad', 'AddBackward0'),   # x = torch.ones(2, 2, requires_grad=True), y = x + 2
        ]
        for e in edges:
            self.assertIn(
                (e[0].__class__.__name__, e[1].__class__.__name__),
                expected_edges,
                'autograd_graph() must return all edges in the autograd graph.'
            )

        # Check for two edges between the AddBackward node and the ThMulBackward node
        num_y_squared_edges = 0
        for e in edges:
            if e[0].__class__.__name__ == 'AddBackward0' and e[1].__class__.__name__ == 'MulBackward0':
                num_y_squared_edges += 1

        self.assertEqual(
            num_y_squared_edges,
            2,
            'autograd_graph() must return multiple edges between nodes if they exist.'
        )","for e in edges:
    self.assertIn((e[0].__class__.__name__, e[1].__class__.__name__), expected_edges, 'autograd_graph() must return all edges in the autograd graph.')","for e in edges:
    (e_0, e_1, *_) = e
    self.assertIn((e_0.__class__.__name__, e_1.__class__.__name__), expected_edges, 'autograd_graph() must return all edges in the autograd graph.')","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
few-shot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/few-shot/tests/test_utils.py,https://github.com/oscarknagg/few-shot/tree/master/tests/test_utils.py,TestAutogradGraphRetrieval,test_retrieval$89,"def test_retrieval(self):
        """"""Create a simple autograd graph and check that the output is what is expected""""""
        x = torch.ones(2, 2, requires_grad=True)
        y = x + 2
        # The operation on the next line will create two edges because the y variable is
        # y variable is used twice.
        z = y * y
        out = z.mean()

        nodes, edges = autograd_graph(out)

        # This is quite a brittle test as it will break if the names of the autograd Functions change
        # TODO: Less brittle test

        expected_nodes = [
            'MeanBackward1',
            'MulBackward0',
            'AddBackward0',
            'AccumulateGrad',
        ]

        self.assertEqual(
            set(expected_nodes),
            set(n.__class__.__name__ for n in nodes),
            'autograd_graph() must return all nodes in the autograd graph.'
        )

        # Check for the existence of the expected edges
        expected_edges = [
            ('MulBackward0', 'MeanBackward1'),  # z = y * y, out = z.mean()
            ('AddBackward0', 'MulBackward0'),    # y = x + 2, z = y * y
            ('AccumulateGrad', 'AddBackward0'),   # x = torch.ones(2, 2, requires_grad=True), y = x + 2
        ]
        for e in edges:
            self.assertIn(
                (e[0].__class__.__name__, e[1].__class__.__name__),
                expected_edges,
                'autograd_graph() must return all edges in the autograd graph.'
            )

        # Check for two edges between the AddBackward node and the ThMulBackward node
        num_y_squared_edges = 0
        for e in edges:
            if e[0].__class__.__name__ == 'AddBackward0' and e[1].__class__.__name__ == 'MulBackward0':
                num_y_squared_edges += 1

        self.assertEqual(
            num_y_squared_edges,
            2,
            'autograd_graph() must return multiple edges between nodes if they exist.'
        )","for e in edges:
    if e[0].__class__.__name__ == 'AddBackward0' and e[1].__class__.__name__ == 'MulBackward0':
        num_y_squared_edges += 1","for e in edges:
    (e_0, e_1, *_) = e
    if e_0.__class__.__name__ == 'AddBackward0' and e_1.__class__.__name__ == 'MulBackward0':
        num_y_squared_edges += 1","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
cocoNLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cocoNLP/dist/cocoNLP-0.0.9/cocoNLP/config/phrase/rake.py,https://github.com/fighting41love/cocoNLP/tree/master/dist/cocoNLP-0.0.9/cocoNLP/config/phrase/rake.py,Rake,_get_phrase_list_from_words$218,"def _get_phrase_list_from_words(self, word_list, min_len, max_len):
        """"""Method to create contender phrases from the list of words that form
        a sentence by dropping stopwords and punctuations and grouping the left
        words into phrases. Only phrases in the given length range (both limits
        inclusive) would be considered to build co-occurrence matrix. Ex:

        Sentence: Red apples, are good in flavour.
        List of words: ['red', 'apples', "","", 'are', 'good', 'in', 'flavour']
        List after dropping punctuations and stopwords.
        List of words: ['red', 'apples', *, *, good, *, 'flavour']
        List of phrases: [('red', 'apples'), ('good',), ('flavour',)]

        List of phrases with a correct length:
        For the range [1, 2]: [('red', 'apples'), ('good',), ('flavour',)]
        For the range [1, 1]: [('good',), ('flavour',)]
        For the range [2, 2]: [('red', 'apples')]

        :param word_list: List of words which form a sentence when joined in
                          the same order.
        :return: List of contender phrases that are formed after dropping
                 stopwords and punctuations.
        """"""
        groups = groupby(word_list, lambda x: x not in self.to_ignore)
        phrases = []
        for group in groups:
            tmp = tuple(group[1])
            len_g1 = len(list(tmp))
            if group[0] and len_g1>=min_len and len_g1<=max_len: # restrict the length of the phrase
                phrases.append(tuple(tmp))

        return list(
            filter(
                lambda x: self.min_length <= len(x) <= self.max_length, phrases
            )
        )","for group in groups:
    tmp = tuple(group[1])
    len_g1 = len(list(tmp))
    if group[0] and len_g1 >= min_len and (len_g1 <= max_len):
        phrases.append(tuple(tmp))","for group in groups:
    (group_0, group_1, *_) = group
    tmp = tuple(group[1])
    len_g1 = len(list(tmp))
    if group[0] and len_g1 >= min_len and (len_g1 <= max_len):
        phrases.append(tuple(tmp))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
sphinx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sphinx/sphinx/ext/autodoc/typehints.py,https://github.com/sphinx-doc/sphinx/tree/master/sphinx/ext/autodoc/typehints.py,,modify_field_list$90,"def modify_field_list(node: nodes.field_list, annotations: Dict[str, str],
                      suppress_rtype: bool = False) -> None:
    arguments: Dict[str, Dict[str, bool]] = {}
    fields = cast(Iterable[nodes.field], node)
    for field in fields:
        field_name = field[0].astext()
        parts = re.split(' +', field_name)
        if parts[0] == 'param':
            if len(parts) == 2:
                # :param xxx:
                arg = arguments.setdefault(parts[1], {})
                arg['param'] = True
            elif len(parts) > 2:
                # :param xxx yyy:
                name = ' '.join(parts[2:])
                arg = arguments.setdefault(name, {})
                arg['param'] = True
                arg['type'] = True
        elif parts[0] == 'type':
            name = ' '.join(parts[1:])
            arg = arguments.setdefault(name, {})
            arg['type'] = True
        elif parts[0] == 'rtype':
            arguments['return'] = {'type': True}

    for name, annotation in annotations.items():
        if name == 'return':
            continue

        if '*' + name in arguments:
            name = '*' + name
            arguments.get(name)
        elif '**' + name in arguments:
            name = '**' + name
            arguments.get(name)
        else:
            arg = arguments.get(name, {})

        if not arg.get('type'):
            field = nodes.field()
            field += nodes.field_name('', 'type ' + name)
            field += nodes.field_body('', nodes.paragraph('', annotation))
            node += field
        if not arg.get('param'):
            field = nodes.field()
            field += nodes.field_name('', 'param ' + name)
            field += nodes.field_body('', nodes.paragraph('', ''))
            node += field

    if 'return' in annotations and 'return' not in arguments:
        annotation = annotations['return']
        if annotation == 'None' and suppress_rtype:
            return

        field = nodes.field()
        field += nodes.field_name('', 'rtype')
        field += nodes.field_body('', nodes.paragraph('', annotation))
        node += field","for field in fields:
    field_name = field[0].astext()
    parts = re.split(' +', field_name)
    if parts[0] == 'param':
        if len(parts) == 2:
            arg = arguments.setdefault(parts[1], {})
            arg['param'] = True
        elif len(parts) > 2:
            name = ' '.join(parts[2:])
            arg = arguments.setdefault(name, {})
            arg['param'] = True
            arg['type'] = True
    elif parts[0] == 'type':
        name = ' '.join(parts[1:])
        arg = arguments.setdefault(name, {})
        arg['type'] = True
    elif parts[0] == 'rtype':
        arguments['return'] = {'type': True}","for field in fields:
    (field_0, *field_rfieldmaining) = field
    field_name = field[0].astext()
    parts = re.split(' +', field_name)
    if parts[0] == 'param':
        if len(parts) == 2:
            arg = arguments.setdefault(parts[1], {})
            arg['param'] = True
        elif len(parts) > 2:
            name = ' '.join(parts[2:])
            arg = arguments.setdefault(name, {})
            arg['param'] = True
            arg['type'] = True
    elif parts[0] == 'type':
        name = ' '.join(parts[1:])
        arg = arguments.setdefault(name, {})
        arg['type'] = True
    elif parts[0] == 'rtype':
        arguments['return'] = {'type': True}","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
deluge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deluge/deluge/log.py,https://github.com/deluge-torrent/deluge/tree/master/deluge/log.py,_BackwardsCompatibleLOG,__getattribute__$318,"def __getattribute__(self, name):
        import warnings

        logger_name = 'deluge'
        stack = inspect.stack()
        stack.pop(0)  # The logging call from this module
        module_stack = stack.pop(0)  # The module that called the log function
        caller_module = inspect.getmodule(module_stack[0])
        # In some weird cases caller_module might be None, try to continue
        caller_module_name = getattr(caller_module, '__name__', '')
        warnings.warn_explicit(
            DEPRECATION_WARNING,
            DeprecationWarning,
            module_stack[1],
            module_stack[2],
            caller_module_name,
        )
        if caller_module:
            for member in stack:
                module = inspect.getmodule(member[0])
                if not module:
                    continue
                if module.__name__ in (
                    'deluge.plugins.pluginbase',
                    'deluge.plugins.init',
                ):
                    logger_name += '.plugin.%s' % caller_module_name
                    # Monkey Patch The Plugin Module
                    caller_module.log = logging.getLogger(logger_name)
                    break
        else:
            logging.getLogger(logger_name).warning(
                ""Unable to monkey-patch the calling module's `log` attribute! ""
                'You should really update and rebuild your plugins...'
            )
        return getattr(logging.getLogger(logger_name), name)","for member in stack:
    module = inspect.getmodule(member[0])
    if not module:
        continue
    if module.__name__ in ('deluge.plugins.pluginbase', 'deluge.plugins.init'):
        logger_name += '.plugin.%s' % caller_module_name
        caller_module.log = logging.getLogger(logger_name)
        break","for member in stack:
    (member_0, *member_rmembermaining) = member
    module = inspect.getmodule(member[0])
    if not module:
        continue
    if module.__name__ in ('deluge.plugins.pluginbase', 'deluge.plugins.init'):
        logger_name += '.plugin.%s' % caller_module_name
        caller_module.log = logging.getLogger(logger_name)
        break","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
MarkdownEditing,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/lint.py,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/lint.py,MdeMarkdownLintCommand,run$76,"def run(self, edit):
        mddef = globals()[""mddef""]
        text = self.view.substr(sublime.Region(0, self.view.size()))
        st = self.view.settings().get(""mde.lint"", {})
        uselist = []
        disablelist = st[""disable""]
        for cl in mddef.__subclasses__():
            if cl.__name__ not in disablelist:
                uselist.append(cl)
        result = []
        for mddef in uselist:
            r = self.test(
                mddef(st[mddef.__name__] if mddef.__name__ in st else None, self.view), text
            )
            result.extend(r)
        window = self.view.window() or sublime.active_window()
        if len(result) > 0:
            sublime.status_message(""MarkdownLint: %d error(s) found"" % len(result))
            result = sorted(result, key=lambda t: t[0])
            outputtxt = """"
            for t in result:
                (row, col) = self.view.rowcol(t[0])
                outputtxt += ""line %d: %s, %s\n"" % (row + 1, t[1], t[2])
            output = window.create_output_panel(""mde"")
            output.run_command(""insert"", {""characters"": outputtxt})
            window.run_command(""show_panel"", {""panel"": ""output.mde""})
        else:
            sublime.status_message(""MarkdownLint: no errors found"")
            window.destroy_output_panel(""mde"")","for t in result:
    (row, col) = self.view.rowcol(t[0])
    outputtxt += 'line %d: %s, %s\n' % (row + 1, t[1], t[2])","for t in result:
    (t_0, t_1, t_2, *_) = t
    (row, col) = self.view.rowcol(t[0])
    outputtxt += 'line %d: %s, %s\n' % (row + 1, t[1], t[2])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
scanpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scanpy/scanpy/external/exporting.py,https://github.com/theislab/scanpy/tree/master/scanpy/external/exporting.py,,_write_edges$313,"def _write_edges(filename, edges):
    with open(filename, 'w') as f:
        for e in edges:
            f.write('%i;%i\n' % (e[0], e[1]))","for e in edges:
    f.write('%i;%i\n' % (e[0], e[1]))","for e in edges:
    (e_0, e_1, *_) = e
    f.write('%i;%i\n' % (e_0, e_1))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
EasyOCR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyOCR/trainer/utils.py,https://github.com/JaidedAI/EasyOCR/tree/master/trainer/utils.py,,word_segmentation$176,"def word_segmentation(mat, separator_idx =  {'th': [1,2],'en': [3,4]}, separator_idx_list = [1,2,3,4]):
    result = []
    sep_list = []
    start_idx = 0
    for sep_idx in separator_idx_list:
        if sep_idx % 2 == 0: mode ='first'
        else: mode ='last'
        a = consecutive( np.argwhere(mat == sep_idx).flatten(), mode)
        new_sep = [ [item, sep_idx] for item in a]
        sep_list += new_sep
    sep_list = sorted(sep_list, key=lambda x: x[0])

    for sep in sep_list:
        for lang in separator_idx.keys():
            if sep[1] == separator_idx[lang][0]: # start lang
                sep_lang = lang
                sep_start_idx = sep[0]
            elif sep[1] == separator_idx[lang][1]: # end lang
                if sep_lang == lang: # check if last entry if the same start lang
                    new_sep_pair = [lang, [sep_start_idx+1, sep[0]-1]]
                    if sep_start_idx > start_idx:
                        result.append( ['', [start_idx, sep_start_idx-1] ] )
                    start_idx = sep[0]+1
                    result.append(new_sep_pair)
                else: # reset
                    sep_lang = ''

    if start_idx <= len(mat)-1:
        result.append( ['', [start_idx, len(mat)-1] ] )
    return result","for sep in sep_list:
    for lang in separator_idx.keys():
        if sep[1] == separator_idx[lang][0]:
            sep_lang = lang
            sep_start_idx = sep[0]
        elif sep[1] == separator_idx[lang][1]:
            if sep_lang == lang:
                new_sep_pair = [lang, [sep_start_idx + 1, sep[0] - 1]]
                if sep_start_idx > start_idx:
                    result.append(['', [start_idx, sep_start_idx - 1]])
                start_idx = sep[0] + 1
                result.append(new_sep_pair)
            else:
                sep_lang = ''","for sep in sep_list:
    (sep_0, sep_1, *_) = sep
    for lang in separator_idx.keys():
        if sep[1] == separator_idx[lang][0]:
            sep_lang = lang
            sep_start_idx = sep[0]
        elif sep[1] == separator_idx[lang][1]:
            if sep_lang == lang:
                new_sep_pair = [lang, [sep_start_idx + 1, sep[0] - 1]]
                if sep_start_idx > start_idx:
                    result.append(['', [start_idx, sep_start_idx - 1]])
                start_idx = sep[0] + 1
                result.append(new_sep_pair)
            else:
                sep_lang = ''","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Archery,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Archery/sql/engines/oracle.py,https://github.com/hhyo/Archery/tree/master/sql/engines/oracle.py,OracleEngine,get_kill_command$1429,"def get_kill_command(self, thread_ids):
        """"""由传入的sid+serial#列表生成kill命令""""""
        # 校验传参，thread_ids格式：[[sid, serial#], [sid, serial#]]
        if [
            k
            for k in [[j for j in i if not isinstance(j, int)] for i in thread_ids]
            if k
        ]:
            return None
        sql = """"""select 'alter system kill session ' || '''' || s.sid || ',' || s.serial# || '''' || ' immediate' || ';'
                 from v$process p, v$session s, v$sqlarea q
                 where p.addr = s.paddr
                 and s.sql_hash_value = q.hash_value
                 and s.sid || ',' || s.serial# in ({});"""""".format(
            "","".join(f""'{str(tid[0])},{str(tid[1])}'"" for tid in thread_ids)
        )
        all_kill_sql = self.query(sql=sql)
        kill_sql = """"
        for row in all_kill_sql.rows:
            kill_sql = kill_sql + row[0]

        return kill_sql","for row in all_kill_sql.rows:
    kill_sql = kill_sql + row[0]","for row in all_kill_sql.rows:
    (row_0, *row_rrowmaining) = row
    kill_sql = kill_sql + row[0]","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
nlpcda,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlpcda/nlpcda/tools/Ner.py,https://github.com/425776024/nlpcda/tree/master/nlpcda/tools/Ner.py,Ner,__data_augment_one$78,"def __data_augment_one(self, org_data):

        new_data = []
        for di in org_data:
            t_tag, t_ner_sentence = di[0], di[1]
            if t_tag in self.data_augument_tag_list and t_tag in self.tag_map:
                rdm_select_ner = self.__get_random_ner(t_tag)
                new_data.append([t_tag, rdm_select_ner])
            else:
                new_data.append([t_tag, t_ner_sentence])
        return new_data","for di in org_data:
    (t_tag, t_ner_sentence) = (di[0], di[1])
    if t_tag in self.data_augument_tag_list and t_tag in self.tag_map:
        rdm_select_ner = self.__get_random_ner(t_tag)
        new_data.append([t_tag, rdm_select_ner])
    else:
        new_data.append([t_tag, t_ner_sentence])","for di in org_data:
    (di_0, di_1, *_) = di
    (t_tag, t_ner_sentence) = (di[0], di[1])
    if t_tag in self.data_augument_tag_list and t_tag in self.tag_map:
        rdm_select_ner = self.__get_random_ner(t_tag)
        new_data.append([t_tag, rdm_select_ner])
    else:
        new_data.append([t_tag, t_ner_sentence])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
detectron2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/detectron2/tests/data/test_sampler.py,https://github.com/facebookresearch/detectron2/tree/master/tests/data/test_sampler.py,TestGroupedBatchSampler,test_groups$30,"def test_groups(self):
        sampler = SequentialSampler(list(range(100)))
        group_ids = [1, 0] * 50
        samples = GroupedBatchSampler(sampler, group_ids, 2)

        for mini_batch in samples:
            self.assertEqual((mini_batch[0] + mini_batch[1]) % 2, 0)","for mini_batch in samples:
    self.assertEqual((mini_batch[0] + mini_batch[1]) % 2, 0)","for mini_batch in samples:
    (mini_batch_0, mini_batch_1, *_) = mini_batch
    self.assertEqual((mini_batch[0] + mini_batch[1]) % 2, 0)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
dcos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dcos/packages/dcos-integration-test/extra/test_metrics.py,https://github.com/dcos/dcos/tree/master/packages/dcos-integration-test/extra/test_metrics.py,,_check_calico_metrics$212,"def _check_calico_metrics() -> None:
        response = get_metrics_prom(dcos_api_session, node)
        for family in text_string_to_metric_families(response.text):
            for sample in family.samples:
                if sample[0].startswith('felix') and sample[1].get('dcos_component_name') == 'DC/OS Calico':
                    return
        raise Exception('Expected DC/OS Calico felix* metric on agent nodes not found')","for sample in family.samples:
    if sample[0].startswith('felix') and sample[1].get('dcos_component_name') == 'DC/OS Calico':
        return","for sample in family.samples:
    (sample_0, sample_1, *_) = sample
    if sample[0].startswith('felix') and sample[1].get('dcos_component_name') == 'DC/OS Calico':
        return","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
qutip,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutip/qutip/legacy/bloch_redfield.py,https://github.com/qutip/qutip/tree/master/qutip/legacy/bloch_redfield.py,,bloch_redfield_tensor$21,"def bloch_redfield_tensor(H, a_ops, spectra_cb=None, c_ops=[], use_secular=True, sec_cutoff=0.1):
    """"""
    Calculate the Bloch-Redfield tensor for a system given a set of operators
    and corresponding spectral functions that describes the system's coupling
    to its environment.

    .. note::

        This tensor generation requires a time-independent Hamiltonian.

    Parameters
    ----------

    H : :class:`qutip.qobj`
        System Hamiltonian.

    a_ops : list of :class:`qutip.qobj`
        List of system operators that couple to the environment.

    spectra_cb : list of callback functions
        List of callback functions that evaluate the noise power spectrum
        at a given frequency.

    c_ops : list of :class:`qutip.qobj`
        List of system collapse operators.

    use_secular : bool
        Flag (True of False) that indicates if the secular approximation should
        be used.
    
    sec_cutoff : float {0.1}
        Threshold for secular approximation.

    Returns
    -------

    R, kets: :class:`qutip.Qobj`, list of :class:`qutip.Qobj`

        R is the Bloch-Redfield tensor and kets is a list eigenstates of the
        Hamiltonian.

    """"""
    
    if not (spectra_cb is None):
        warnings.warn(""The use of spectra_cb is depreciated."", DeprecationWarning)
        _a_ops = []
        for kk, a in enumerate(a_ops):
            _a_ops.append([a,spectra_cb[kk]])
        a_ops = _a_ops
    
    # Sanity checks for input parameters
    if not isinstance(H, Qobj):
        raise TypeError(""H must be an instance of Qobj"")

    for a in a_ops:
        if not isinstance(a[0], Qobj) or not a[0].isherm:
            raise TypeError(""Operators in a_ops must be Hermitian Qobj."")

    if c_ops is None:
        c_ops = []

    # use the eigenbasis
    evals, ekets = H.eigenstates()

    N = len(evals)
    K = len(a_ops)
    
    #only Lindblad collapse terms
    if K==0:
        Heb = qdiags(evals,0,dims=H.dims)
        L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])
        return L, ekets
    
    
    A = np.array([a_ops[k][0].transform(ekets).full() for k in range(K)])
    Jw = np.zeros((K, N, N), dtype=complex)

    # pre-calculate matrix elements and spectral densities
    # W[m,n] = real(evals[m] - evals[n])
    W = np.real(evals[:,np.newaxis] - evals[np.newaxis,:])

    for k in range(K):
        # do explicit loops here in case spectra_cb[k] can not deal with array arguments
        for n in range(N):
            for m in range(N):
                Jw[k, n, m] = a_ops[k][1](W[n, m])

    dw_min = np.abs(W[W.nonzero()]).min()

    # pre-calculate mapping between global index I and system indices a,b
    Iabs = np.empty((N*N,3),dtype=int)
    for I, Iab in enumerate(Iabs):
        # important: use [:] to change array values, instead of creating new variable Iab
        Iab[0]  = I
        Iab[1:] = vec2mat_index(N, I)

    # unitary part + dissipation from c_ops (if given):
    Heb = qdiags(evals,0,dims=H.dims)
    L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])
    
    # dissipative part:
    rows = []
    cols = []
    data = []
    for I, a, b in Iabs:
        # only check use_secular once per I
        if use_secular:
            # only loop over those indices J which actually contribute
            Jcds = Iabs[np.where(np.abs(W[a, b] - W[Iabs[:,1], Iabs[:,2]]) < dw_min * sec_cutoff)]
        else:
            Jcds = Iabs
        for J, c, d in Jcds:
            elem = 0+0j
            # summed over k, i.e., each operator coupling the system to the environment
            elem += 0.5 * np.sum(A[:, a, c] * A[:, d, b] * (Jw[:, c, a] + Jw[:, d, b]))
            if b==d:
                #                  sum_{k,n} A[k, a, n] * A[k, n, c] * Jw[k, c, n])
                elem -= 0.5 * np.sum(A[:, a, :] * A[:, :, c] * Jw[:, c, :])
            if a==c:
                #                  sum_{k,n} A[k, d, n] * A[k, n, b] * Jw[k, d, n])
                elem -= 0.5 * np.sum(A[:, d, :] * A[:, :, b] * Jw[:, d, :])
            if elem != 0:
                rows.append(I)
                cols.append(J)
                data.append(elem)

    R = arr_coo2fast(np.array(data, dtype=complex),
                    np.array(rows, dtype=np.int32),
                    np.array(cols, dtype=np.int32), N**2, N**2)
    
    L.data = L.data + R
    
    return L, ekets","for a in a_ops:
    if not isinstance(a[0], Qobj) or not a[0].isherm:
        raise TypeError('Operators in a_ops must be Hermitian Qobj.')","for a in a_ops:
    (a_0, *a_ramaining) = a
    if not isinstance(a[0], Qobj) or not a[0].isherm:
        raise TypeError('Operators in a_ops must be Hermitian Qobj.')","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
qutip,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutip/qutip/legacy/bloch_redfield.py,https://github.com/qutip/qutip/tree/master/qutip/legacy/bloch_redfield.py,,bloch_redfield_tensor$21,"def bloch_redfield_tensor(H, a_ops, spectra_cb=None, c_ops=[], use_secular=True, sec_cutoff=0.1):
    """"""
    Calculate the Bloch-Redfield tensor for a system given a set of operators
    and corresponding spectral functions that describes the system's coupling
    to its environment.

    .. note::

        This tensor generation requires a time-independent Hamiltonian.

    Parameters
    ----------

    H : :class:`qutip.qobj`
        System Hamiltonian.

    a_ops : list of :class:`qutip.qobj`
        List of system operators that couple to the environment.

    spectra_cb : list of callback functions
        List of callback functions that evaluate the noise power spectrum
        at a given frequency.

    c_ops : list of :class:`qutip.qobj`
        List of system collapse operators.

    use_secular : bool
        Flag (True of False) that indicates if the secular approximation should
        be used.
    
    sec_cutoff : float {0.1}
        Threshold for secular approximation.

    Returns
    -------

    R, kets: :class:`qutip.Qobj`, list of :class:`qutip.Qobj`

        R is the Bloch-Redfield tensor and kets is a list eigenstates of the
        Hamiltonian.

    """"""
    
    if not (spectra_cb is None):
        warnings.warn(""The use of spectra_cb is depreciated."", DeprecationWarning)
        _a_ops = []
        for kk, a in enumerate(a_ops):
            _a_ops.append([a,spectra_cb[kk]])
        a_ops = _a_ops
    
    # Sanity checks for input parameters
    if not isinstance(H, Qobj):
        raise TypeError(""H must be an instance of Qobj"")

    for a in a_ops:
        if not isinstance(a[0], Qobj) or not a[0].isherm:
            raise TypeError(""Operators in a_ops must be Hermitian Qobj."")

    if c_ops is None:
        c_ops = []

    # use the eigenbasis
    evals, ekets = H.eigenstates()

    N = len(evals)
    K = len(a_ops)
    
    #only Lindblad collapse terms
    if K==0:
        Heb = qdiags(evals,0,dims=H.dims)
        L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])
        return L, ekets
    
    
    A = np.array([a_ops[k][0].transform(ekets).full() for k in range(K)])
    Jw = np.zeros((K, N, N), dtype=complex)

    # pre-calculate matrix elements and spectral densities
    # W[m,n] = real(evals[m] - evals[n])
    W = np.real(evals[:,np.newaxis] - evals[np.newaxis,:])

    for k in range(K):
        # do explicit loops here in case spectra_cb[k] can not deal with array arguments
        for n in range(N):
            for m in range(N):
                Jw[k, n, m] = a_ops[k][1](W[n, m])

    dw_min = np.abs(W[W.nonzero()]).min()

    # pre-calculate mapping between global index I and system indices a,b
    Iabs = np.empty((N*N,3),dtype=int)
    for I, Iab in enumerate(Iabs):
        # important: use [:] to change array values, instead of creating new variable Iab
        Iab[0]  = I
        Iab[1:] = vec2mat_index(N, I)

    # unitary part + dissipation from c_ops (if given):
    Heb = qdiags(evals,0,dims=H.dims)
    L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])
    
    # dissipative part:
    rows = []
    cols = []
    data = []
    for I, a, b in Iabs:
        # only check use_secular once per I
        if use_secular:
            # only loop over those indices J which actually contribute
            Jcds = Iabs[np.where(np.abs(W[a, b] - W[Iabs[:,1], Iabs[:,2]]) < dw_min * sec_cutoff)]
        else:
            Jcds = Iabs
        for J, c, d in Jcds:
            elem = 0+0j
            # summed over k, i.e., each operator coupling the system to the environment
            elem += 0.5 * np.sum(A[:, a, c] * A[:, d, b] * (Jw[:, c, a] + Jw[:, d, b]))
            if b==d:
                #                  sum_{k,n} A[k, a, n] * A[k, n, c] * Jw[k, c, n])
                elem -= 0.5 * np.sum(A[:, a, :] * A[:, :, c] * Jw[:, c, :])
            if a==c:
                #                  sum_{k,n} A[k, d, n] * A[k, n, b] * Jw[k, d, n])
                elem -= 0.5 * np.sum(A[:, d, :] * A[:, :, b] * Jw[:, d, :])
            if elem != 0:
                rows.append(I)
                cols.append(J)
                data.append(elem)

    R = arr_coo2fast(np.array(data, dtype=complex),
                    np.array(rows, dtype=np.int32),
                    np.array(cols, dtype=np.int32), N**2, N**2)
    
    L.data = L.data + R
    
    return L, ekets","for (I, Iab) in enumerate(Iabs):
    Iab[0] = I
    Iab[1:] = vec2mat_index(N, I)","for (I, Iab) in enumerate(Iabs):
    (Iab_0, *Iab_1) = Iab
    Iab[0] = I
    Iab[1:] = vec2mat_index(N, I)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, *e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1:]",,,,,,,
taurus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taurus/bzt/modules/aggregator.py,https://github.com/Blazemeter/taurus/tree/master/bzt/modules/aggregator.py,KPISet,items$351,"def items(self):
        for item in super(KPISet, self).items():
            yield (item[0], self.__getitem__(item[0]))","for item in super(KPISet, self).items():
    yield (item[0], self.__getitem__(item[0]))","for item in super(KPISet, self).items():
    (item_0, *item_ritemmaining) = item
    yield (item[0], self.__getitem__(item[0]))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Pyro4,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pyro4/tests/PyroTests/test_httpgateway.py,https://github.com/irmen/Pyro4/tree/master/tests/PyroTests/test_httpgateway.py,WSGITestBase,_start_response$72,"def _start_response(self, status, headers):
        """"""A callback passed into the application, to simulate a wsgi
        environment.

        @param status: The response status of the application (""200"", ""404"", etc)
        @param headers: Any headers to begin the response with.
        """"""
        assert not self.response_started
        self.response_started = True
        self.status = status
        self.headers = headers
        for header in headers:
            # Parse out any cookies and save them to send with later requests.
            if header[0] == 'Set-Cookie':
                var = header[1].split(';', 1)
                if len(var) > 1 and var[1][0:9] == ' Max-Age=':
                    if int(var[1][9:]) > 0:
                        # An approximation, since our cookies never expire unless
                        # explicitly deleted (by setting Max-Age=0).
                        self.cookies.append(var[0])
                    else:
                        index = self.cookies.index(var[0])
                        self.cookies.pop(index)","for header in headers:
    if header[0] == 'Set-Cookie':
        var = header[1].split(';', 1)
        if len(var) > 1 and var[1][0:9] == ' Max-Age=':
            if int(var[1][9:]) > 0:
                self.cookies.append(var[0])
            else:
                index = self.cookies.index(var[0])
                self.cookies.pop(index)","for header in headers:
    (header_0, header_1, *_) = header
    if header[0] == 'Set-Cookie':
        var = header[1].split(';', 1)
        if len(var) > 1 and var[1][0:9] == ' Max-Age=':
            if int(var[1][9:]) > 0:
                self.cookies.append(var[0])
            else:
                index = self.cookies.index(var[0])
                self.cookies.pop(index)","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
mycroft-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mycroft-core/mycroft/tts/mimic2_tts.py,https://github.com/MycroftAI/mycroft-core/tree/master/mycroft/tts/mimic2_tts.py,Mimic2,viseme$189,"def viseme(self, phonemes):
        """"""Maps phonemes to appropriate viseme encoding

        Args:
            phonemes (list): list of tuples (phoneme, time_start)

        Returns:
            list: list of tuples (viseme_encoding, time_start)
        """"""
        visemes = []
        for pair in phonemes:
            if pair[0]:
                phone = pair[0].lower()
            else:
                # if phoneme doesn't exist use
                # this as placeholder since it
                # is the most common one ""3""
                phone = 'z'
            vis = VISIMES.get(phone)
            vis_dur = float(pair[1])
            visemes.append((vis, vis_dur))
        return visemes","for pair in phonemes:
    if pair[0]:
        phone = pair[0].lower()
    else:
        phone = 'z'
    vis = VISIMES.get(phone)
    vis_dur = float(pair[1])
    visemes.append((vis, vis_dur))","for pair in phonemes:
    (pair_0, pair_1, *_) = pair
    if pair[0]:
        phone = pair[0].lower()
    else:
        phone = 'z'
    vis = VISIMES.get(phone)
    vis_dur = float(pair[1])
    visemes.append((vis, vis_dur))","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
thonny,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/thonny/thonny/plugins/find_replace.py,https://github.com/thonny/thonny/tree/master/thonny/plugins/find_replace.py,FindDialog,_remove_all_tags$358,"def _remove_all_tags(self):
        for tag in self.passive_found_tags:
            self.codeview.text.tag_remove(""found"", tag[0], tag[1])  # removes the passive tags

        if self.active_found_tag is not None:
            self.codeview.text.tag_remove(
                ""current_found"", self.active_found_tag[0], self.active_found_tag[1]
            )  # removes the currently active tag

        self.active_found_tag = None
        self.replace_and_find_button.config(state=""disabled"")
        self.replace_button.config(state=""disabled"")","for tag in self.passive_found_tags:
    self.codeview.text.tag_remove('found', tag[0], tag[1])","for tag in self.passive_found_tags:
    (tag_0, tag_1, *_) = tag
    self.codeview.text.tag_remove('found', tag[0], tag[1])","(data, data, *data)","for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():
    tmpports.append(str(
    port_0))
    srv = nmap.hosts[0].get_service(
    port_0, 
    port_1)
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
tensorlayer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorlayer/tensorlayer/prepro.py,https://github.com/tensorlayer/tensorlayer/tree/master/tensorlayer/prepro.py,,resize_image$3684,"def resize_image(image, annos, mask, target_width, target_height):
        """"""Reszie image

        Parameters
        -----------
        image : 3 channel image
            The given image.
        annos : list of list of floats
            Keypoints of people
        mask : single channel image or None
            The mask if available.
        target_width : int
            Expected width of returned image.
        target_height : int
            Expected height of returned image.

        Returns
        ----------
        preprocessed input image, annos, mask

        """"""
        y, x, _ = np.shape(image)

        ratio_y = target_height / y
        ratio_x = target_width / x

        new_joints = []
        # update meta
        for people in annos:
            new_keypoints = []
            for keypoints in people:
                if keypoints[0] < 0 or keypoints[1] < 0:
                    new_keypoints.append((-1000, -1000))
                    continue
                pts = (int(keypoints[0] * ratio_x + 0.5), int(keypoints[1] * ratio_y + 0.5))
                if pts[0] > target_width - 1 or pts[1] > target_height - 1:
                    new_keypoints.append((-1000, -1000))
                    continue

                new_keypoints.append(pts)
            new_joints.append(new_keypoints)
        annos = new_joints

        new_image = cv2.resize(image, (target_width, target_height), interpolation=cv2.INTER_AREA)
        if mask is not None:
            new_mask = cv2.resize(mask, (target_width, target_height), interpolation=cv2.INTER_AREA)
            return new_image, annos, new_mask
        else:
            return new_image, annos, None","for keypoints in people:
    if keypoints[0] < 0 or keypoints[1] < 0:
        new_keypoints.append((-1000, -1000))
        continue
    pts = (int(keypoints[0] * ratio_x + 0.5), int(keypoints[1] * ratio_y + 0.5))
    if pts[0] > target_width - 1 or pts[1] > target_height - 1:
        new_keypoints.append((-1000, -1000))
        continue
    new_keypoints.append(pts)","for keypoints in people:
    (keypoints_0, keypoints_1, *_) = keypoints
    if keypoints[0] < 0 or keypoints[1] < 0:
        new_keypoints.append((-1000, -1000))
        continue
    pts = (int(keypoints[0] * ratio_x + 0.5), int(keypoints[1] * ratio_y + 0.5))
    if pts[0] > target_width - 1 or pts[1] > target_height - 1:
        new_keypoints.append((-1000, -1000))
        continue
    new_keypoints.append(pts)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
FIR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FIR/fir_email/views.py,https://github.com/certsocietegenerale/FIR/tree/master/fir_email/views.py,,smime_configuration$11,"def smime_configuration(request):
    form = SMIMECertificateForm(request.POST, user=request.user)
    if form.is_valid():
        form.save()
    else:
        for error in form.errors.items():
            messages.error(request, error[1])
    return redirect('user:profile')","for error in form.errors.items():
    messages.error(request, error[1])","for error in form.errors.items():
    (_, error_1, *error_rerrormaining) = error
    messages.error(request, error[1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/cfg/cfg_emulated.py,https://github.com/angr/angr/tree/master/angr/analyses/cfg/cfg_emulated.py,CFGEmulated,_is_address_executable$3365,"def _is_address_executable(self, address):
        """"""
        Check if the specific address is in one of the executable ranges.

        :param int address: The address
        :return: True if it's in an executable range, False otherwise
        """"""

        for r in self._executable_address_ranges:
            if r[0] <= address < r[1]:
                return True
        return False","for r in self._executable_address_ranges:
    if r[0] <= address < r[1]:
        return True","for r in self._executable_address_ranges:
    (r_0, r_1, *_) = r
    if r[0] <= address < r[1]:
        return True","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
dnsrecon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dnsrecon/tests/test_dnshelper.py,https://github.com/darkoperator/dnsrecon/tree/master/tests/test_dnshelper.py,Test_Lib_dnshelper,test_get_srv$71,"def test_get_srv(self):
        helper = DnsHelper('nsztm1.digi.ninja')
        records = helper.get_srv('_sip._tcp.zonetransfer.me')
        for record in records:
            assert record[0] == 'SRV'","for record in records:
    assert record[0] == 'SRV'","for record in records:
    (record_0, *record_rrecordmaining) = record
    assert record[0] == 'SRV'","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
DeepNER,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepNER/src/utils/evaluator.py,https://github.com/z814081807/DeepNER/tree/master/src/utils/evaluator.py,,mrc_evaluation$240,"def mrc_evaluation(model, dev_info, device):
    dev_loader, (dev_callback_info, type_weight) = dev_info

    start_logits, end_logits = None, None

    model.eval()

    for tmp_pred in get_base_out(model, dev_loader, device):
        tmp_start_logits = tmp_pred[0].cpu().numpy()
        tmp_end_logits = tmp_pred[1].cpu().numpy()

        if start_logits is None:
            start_logits = tmp_start_logits
            end_logits = tmp_end_logits
        else:
            start_logits = np.append(start_logits, tmp_start_logits, axis=0)
            end_logits = np.append(end_logits, tmp_end_logits, axis=0)

    assert len(start_logits) == len(end_logits) == len(dev_callback_info)

    role_metric = np.zeros([13, 3])

    mirco_metrics = np.zeros(3)

    id2ent = {x: i for i, x in enumerate(ENTITY_TYPES)}

    for tmp_start_logits, tmp_end_logits, tmp_callback \
            in zip(start_logits, end_logits, dev_callback_info):

        text, text_offset, ent_type, gt_entities = tmp_callback

        tmp_start_logits = tmp_start_logits[text_offset:text_offset+len(text)]
        tmp_end_logits = tmp_end_logits[text_offset:text_offset+len(text)]

        pred_entities = mrc_decode(tmp_start_logits, tmp_end_logits, text)

        role_metric[id2ent[ent_type]] += calculate_metric(gt_entities, pred_entities)

    for idx, _type in enumerate(ENTITY_TYPES):
        temp_metric = get_p_r_f(role_metric[idx][0], role_metric[idx][1], role_metric[idx][2])

        mirco_metrics += temp_metric * type_weight[_type]

    metric_str = f'[MIRCO] precision: {mirco_metrics[0]:.4f}, ' \
                  f'recall: {mirco_metrics[1]:.4f}, f1: {mirco_metrics[2]:.4f}'

    return metric_str, mirco_metrics[2]","for tmp_pred in get_base_out(model, dev_loader, device):
    tmp_start_logits = tmp_pred[0].cpu().numpy()
    tmp_end_logits = tmp_pred[1].cpu().numpy()
    if start_logits is None:
        start_logits = tmp_start_logits
        end_logits = tmp_end_logits
    else:
        start_logits = np.append(start_logits, tmp_start_logits, axis=0)
        end_logits = np.append(end_logits, tmp_end_logits, axis=0)","for tmp_pred in get_base_out(model, dev_loader, device):
    (tmp_pred_0, tmp_pred_1, *_) = tmp_pred
    tmp_start_logits = tmp_pred[0].cpu().numpy()
    tmp_end_logits = tmp_pred[1].cpu().numpy()
    if start_logits is None:
        start_logits = tmp_start_logits
        end_logits = tmp_end_logits
    else:
        start_logits = np.append(start_logits, tmp_start_logits, axis=0)
        end_logits = np.append(end_logits, tmp_end_logits, axis=0)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
DeepNER,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepNER/src/utils/evaluator.py,https://github.com/z814081807/DeepNER/tree/master/src/utils/evaluator.py,,mrc_evaluation$240,"def mrc_evaluation(model, dev_info, device):
    dev_loader, (dev_callback_info, type_weight) = dev_info

    start_logits, end_logits = None, None

    model.eval()

    for tmp_pred in get_base_out(model, dev_loader, device):
        tmp_start_logits = tmp_pred[0].cpu().numpy()
        tmp_end_logits = tmp_pred[1].cpu().numpy()

        if start_logits is None:
            start_logits = tmp_start_logits
            end_logits = tmp_end_logits
        else:
            start_logits = np.append(start_logits, tmp_start_logits, axis=0)
            end_logits = np.append(end_logits, tmp_end_logits, axis=0)

    assert len(start_logits) == len(end_logits) == len(dev_callback_info)

    role_metric = np.zeros([13, 3])

    mirco_metrics = np.zeros(3)

    id2ent = {x: i for i, x in enumerate(ENTITY_TYPES)}

    for tmp_start_logits, tmp_end_logits, tmp_callback \
            in zip(start_logits, end_logits, dev_callback_info):

        text, text_offset, ent_type, gt_entities = tmp_callback

        tmp_start_logits = tmp_start_logits[text_offset:text_offset+len(text)]
        tmp_end_logits = tmp_end_logits[text_offset:text_offset+len(text)]

        pred_entities = mrc_decode(tmp_start_logits, tmp_end_logits, text)

        role_metric[id2ent[ent_type]] += calculate_metric(gt_entities, pred_entities)

    for idx, _type in enumerate(ENTITY_TYPES):
        temp_metric = get_p_r_f(role_metric[idx][0], role_metric[idx][1], role_metric[idx][2])

        mirco_metrics += temp_metric * type_weight[_type]

    metric_str = f'[MIRCO] precision: {mirco_metrics[0]:.4f}, ' \
                  f'recall: {mirco_metrics[1]:.4f}, f1: {mirco_metrics[2]:.4f}'

    return metric_str, mirco_metrics[2]","for (tmp_start_logits, tmp_end_logits, tmp_callback) in zip(start_logits, end_logits, dev_callback_info):
    (text, text_offset, ent_type, gt_entities) = tmp_callback
    tmp_start_logits = tmp_start_logits[text_offset:text_offset + len(text)]
    tmp_end_logits = tmp_end_logits[text_offset:text_offset + len(text)]
    pred_entities = mrc_decode(tmp_start_logits, tmp_end_logits, text)
    role_metric[id2ent[ent_type]] += calculate_metric(gt_entities, pred_entities)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e[text_offset:text_offset + len(text)] is a slice of the iterable object ""e"". However, iterable unpacking is not applicable in this case as the slice is not a fixed number of elements that can be unpacked.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.***************
Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e[text_offset:text_offset + len(text)] is a slice of the iterable object ""e"". However, iterable unpacking is not applicable in this case as the slice is not a fixed number of elements that can be unpacked.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
abseil-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/abseil-py/absl/testing/tests/parameterized_test.py,https://github.com/abseil/abseil-py/tree/master/absl/testing/tests/parameterized_test.py,,decorator$51,"def decorator(test_method):
    # If decorating result of another dict_decorator
    if isinstance(test_method, abc.Iterable):
      actual_tests = []
      for old_test in test_method.testcases:
        # each test is a ('test_suffix', dict) tuple
        new_dict = old_test[1].copy()
        new_dict[key] = value
        test_suffix = '%s_%s_%s' % (old_test[0], key, value)
        actual_tests.append((test_suffix, new_dict))

      test_method.testcases = actual_tests
      return test_method
    else:
      test_suffix = ('_%s_%s') % (key, value)
      tests_to_make = ((test_suffix, {key: value}),)
      # 'test_method' here is the original test method
      return parameterized.named_parameters(*tests_to_make)(test_method)","for old_test in test_method.testcases:
    new_dict = old_test[1].copy()
    new_dict[key] = value
    test_suffix = '%s_%s_%s' % (old_test[0], key, value)
    actual_tests.append((test_suffix, new_dict))","for old_test in test_method.testcases:
    (old_test_0, old_test_1, *_) = old_test
    new_dict = old_test[1].copy()
    new_dict[key] = value
    test_suffix = '%s_%s_%s' % (old_test[0], key, value)
    actual_tests.append((test_suffix, new_dict))","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
security_monkey,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/security_monkey/security_monkey/cloudaux_watcher.py,https://github.com/Netflix/security_monkey/tree/master/security_monkey/cloudaux_watcher.py,CloudAuxWatcher,_flatten_iter_response$59,"def _flatten_iter_response(self, response):
        """"""
        The cloudaux iter_account_region decorator returns a list of tuples.
        Each tuple contains two members.  1) The result. 2) The exception map.
        This method combines that list of tuples into a single result list and a single exception map.
        """"""
        items = list()
        exception_map = dict()
        for result in response:
            items.extend(result[0])
            exception_map.update(result[1])
        return items, exception_map","for result in response:
    items.extend(result[0])
    exception_map.update(result[1])","for result in response:
    (result_0, result_1, *_) = result
    items.extend(result[0])
    exception_map.update(result[1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pandas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandas/pandas/core/internals/concat.py,https://github.com/pandas-dev/pandas/tree/master/pandas/core/internals/concat.py,,_combine_concat_plans$704,"def _combine_concat_plans(plans, concat_axis: AxisInt):
    """"""
    Combine multiple concatenation plans into one.

    existing_plan is updated in-place.
    """"""
    if len(plans) == 1:
        for p in plans[0]:
            yield p[0], [p[1]]

    elif concat_axis == 0:
        offset = 0
        for plan in plans:
            last_plc = None

            for plc, unit in plan:
                yield plc.add(offset), [unit]
                last_plc = plc

            if last_plc is not None:
                offset += last_plc.as_slice.stop

    else:
        # singleton list so we can modify it as a side-effect within _next_or_none
        num_ended = [0]

        def _next_or_none(seq):
            retval = next(seq, None)
            if retval is None:
                num_ended[0] += 1
            return retval

        plans = list(map(iter, plans))
        next_items = list(map(_next_or_none, plans))

        while num_ended[0] != len(next_items):
            if num_ended[0] > 0:
                raise ValueError(""Plan shapes are not aligned"")

            placements, units = zip(*next_items)

            lengths = list(map(len, placements))
            min_len, max_len = min(lengths), max(lengths)

            if min_len == max_len:
                yield placements[0], units
                next_items[:] = map(_next_or_none, plans)
            else:
                yielded_placement = None
                yielded_units = [None] * len(next_items)
                for i, (plc, unit) in enumerate(next_items):
                    yielded_units[i] = unit
                    if len(plc) > min_len:
                        # _trim_join_unit updates unit in place, so only
                        # placement needs to be sliced to skip min_len.
                        next_items[i] = (plc[min_len:], _trim_join_unit(unit, min_len))
                    else:
                        yielded_placement = plc
                        next_items[i] = _next_or_none(plans[i])

                yield yielded_placement, yielded_units","for p in plans[0]:
    yield (p[0], [p[1]])","for p in plans[0]:
    (p_0, p_1, *_) = p
    yield (p[0], [p[1]])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pandas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandas/pandas/core/internals/concat.py,https://github.com/pandas-dev/pandas/tree/master/pandas/core/internals/concat.py,,_combine_concat_plans$704,"def _combine_concat_plans(plans, concat_axis: AxisInt):
    """"""
    Combine multiple concatenation plans into one.

    existing_plan is updated in-place.
    """"""
    if len(plans) == 1:
        for p in plans[0]:
            yield p[0], [p[1]]

    elif concat_axis == 0:
        offset = 0
        for plan in plans:
            last_plc = None

            for plc, unit in plan:
                yield plc.add(offset), [unit]
                last_plc = plc

            if last_plc is not None:
                offset += last_plc.as_slice.stop

    else:
        # singleton list so we can modify it as a side-effect within _next_or_none
        num_ended = [0]

        def _next_or_none(seq):
            retval = next(seq, None)
            if retval is None:
                num_ended[0] += 1
            return retval

        plans = list(map(iter, plans))
        next_items = list(map(_next_or_none, plans))

        while num_ended[0] != len(next_items):
            if num_ended[0] > 0:
                raise ValueError(""Plan shapes are not aligned"")

            placements, units = zip(*next_items)

            lengths = list(map(len, placements))
            min_len, max_len = min(lengths), max(lengths)

            if min_len == max_len:
                yield placements[0], units
                next_items[:] = map(_next_or_none, plans)
            else:
                yielded_placement = None
                yielded_units = [None] * len(next_items)
                for i, (plc, unit) in enumerate(next_items):
                    yielded_units[i] = unit
                    if len(plc) > min_len:
                        # _trim_join_unit updates unit in place, so only
                        # placement needs to be sliced to skip min_len.
                        next_items[i] = (plc[min_len:], _trim_join_unit(unit, min_len))
                    else:
                        yielded_placement = plc
                        next_items[i] = _next_or_none(plans[i])

                yield yielded_placement, yielded_units","for (i, (plc, unit)) in enumerate(next_items):
    yielded_units[i] = unit
    if len(plc) > min_len:
        next_items[i] = (plc[min_len:], _trim_join_unit(unit, min_len))
    else:
        yielded_placement = plc
        next_items[i] = _next_or_none(plans[i])",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_min_len, = e[min_len:]
variable mapping:
e[min_len:]: e[min_len:]",,,,,,,
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/policy.py,https://github.com/google/capirca/tree/master/capirca/lib/policy.py,Term,CheckPortIsContained$1379,"def CheckPortIsContained(self, superset, subset):
    """"""Check if the given list of ports is wholly contained.

    Args:
      superset: list of port tuples
      subset: list of port tuples

    Returns:
      bool: True if subset is contained in superset, false otherwise
    """"""
    if not superset:
      return True
    if not subset:
      return False

    for sub_port in subset:
      not_contains = True
      for sup_port in superset:
        if (int(sub_port[0]) >= int(sup_port[0])
            and int(sub_port[1]) <= int(sup_port[1])):
          not_contains = False
          break
      if not_contains:
        return False
    return True","for sub_port in subset:
    not_contains = True
    for sup_port in superset:
        if int(sub_port[0]) >= int(sup_port[0]) and int(sub_port[1]) <= int(sup_port[1]):
            not_contains = False
            break
    if not_contains:
        return False","for sub_port in subset:
    (sub_port_0, sub_port_1, *_) = sub_port
    not_contains = True
    for sup_port in superset:
        if int(sub_port[0]) >= int(sup_port[0]) and int(sub_port[1]) <= int(sup_port[1]):
            not_contains = False
            break
    if not_contains:
        return False","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/policy.py,https://github.com/google/capirca/tree/master/capirca/lib/policy.py,Term,CheckPortIsContained$1379,"def CheckPortIsContained(self, superset, subset):
    """"""Check if the given list of ports is wholly contained.

    Args:
      superset: list of port tuples
      subset: list of port tuples

    Returns:
      bool: True if subset is contained in superset, false otherwise
    """"""
    if not superset:
      return True
    if not subset:
      return False

    for sub_port in subset:
      not_contains = True
      for sup_port in superset:
        if (int(sub_port[0]) >= int(sup_port[0])
            and int(sub_port[1]) <= int(sup_port[1])):
          not_contains = False
          break
      if not_contains:
        return False
    return True","for sup_port in superset:
    if int(sub_port[0]) >= int(sup_port[0]) and int(sub_port[1]) <= int(sup_port[1]):
        not_contains = False
        break","for sup_port in superset:
    (sup_port_0, sup_port_1, *_) = sup_port
    if int(sub_port[0]) >= int(sup_port[0]) and int(sub_port[1]) <= int(sup_port[1]):
        not_contains = False
        break","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
CrackMapExec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CrackMapExec/cme/protocols/smb/db_navigator.py,https://github.com/byt3bl33d3r/CrackMapExec/tree/master/cme/protocols/smb/db_navigator.py,navigator,display_groups$29,"def display_groups(self, groups):

        data = [['GroupID', 'Domain', 'Name', 'Members']]

        for group in groups:
            groupID = group[0]
            domain = group[1]
            name = group[2]
            members = len(self.db.get_group_relations(groupID=groupID))

            data.append([groupID, domain, name, members])

        self.print_table(data, title='Groups')","for group in groups:
    groupID = group[0]
    domain = group[1]
    name = group[2]
    members = len(self.db.get_group_relations(groupID=groupID))
    data.append([groupID, domain, name, members])","for group in groups:
    (group_0, group_1, group_2, *_) = group
    groupID = group[0]
    domain = group[1]
    name = group[2]
    members = len(self.db.get_group_relations(groupID=groupID))
    data.append([groupID, domain, name, members])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
strictyaml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/strictyaml/strictyaml/ruamel/comments.py,https://github.com/crdoconnor/strictyaml/tree/master/strictyaml/ruamel/comments.py,CommentedMap,__getitem__$757,"def __getitem__(self, key):
        # type: (Any) -> Any
        try:
            return ordereddict.__getitem__(self, key)
        except KeyError:
            for merged in getattr(self, merge_attrib, []):
                if key in merged[1]:
                    return merged[1][key]
            raise","for merged in getattr(self, merge_attrib, []):
    if key in merged[1]:
        return merged[1][key]",It cannot be refactored by var unpacking,"(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",0,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e[1][key], the key is not a constant integer value. Therefore, iterable unpacking is not applicable in this case. Instead, we can access the value using dictionary indexing as e[1]['key'].
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
django-js-reverse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-js-reverse/django_js_reverse/core.py,https://github.com/ierror/django-js-reverse/tree/master/django_js_reverse/core.py,,prepare_url_list$37,"def prepare_url_list(urlresolver, namespace_path='', namespace=''):
    """"""
    returns list of tuples [(<url_name>, <url_patern_tuple> ), ...]
    """"""
    exclude_ns = getattr(settings, 'JS_REVERSE_EXCLUDE_NAMESPACES', JS_EXCLUDE_NAMESPACES)
    include_only_ns = getattr(settings, 'JS_REVERSE_INCLUDE_ONLY_NAMESPACES', JS_INCLUDE_ONLY_NAMESPACES)

    if exclude_ns and include_only_ns:
        raise ImproperlyConfigured(
            'Neither use JS_REVERSE_EXCLUDE_NAMESPACES nor JS_REVERSE_INCLUDE_ONLY_NAMESPACES setting')

    if namespace[:-1] in exclude_ns:
        return

    include_only_allow = True  # include_only state variable

    if include_only_ns != []:
        # True mean that ns passed the test
        in_on_empty_ns = False
        in_on_is_in_list = False
        in_on_null = False

        # Test urls without ns
        if namespace == '' and '' in include_only_ns:
            in_on_empty_ns = True

        # check if nestead ns isn't subns of include_only ns
        # e.g. ns = ""foo:bar"" include_only = [""foo""] -> this ns will be used
        # works for ns = ""lorem:ipsum:dolor"" include_only = [""lorem:ipsum""]
        # ns ""lorem"" will be ignored but ""lorem:ipsum"" & ""lorem:ipsum:.."" won't
        for ns in include_only_ns:
            if ns != """" and namespace[:-1].startswith(ns):
                in_on_is_in_list = True
                break

        # Test if isn't used ""\0"" flag
        # use ""foo\0"" to add urls just from ""foo"" not from subns ""foo:bar""
        if namespace[:-1] + '\0' in include_only_ns:
            in_on_null = True

        include_only_allow = in_on_empty_ns or in_on_is_in_list or in_on_null

    if include_only_allow:
        for url_name in urlresolver.reverse_dict.keys():
            if isinstance(url_name, (text_type, str)):
                url_patterns = []
                for url_pattern in urlresolver.reverse_dict.getlist(url_name):
                    url_patterns += [
                        [namespace_path + pat[0], pat[1]] for pat in url_pattern[0]]
                yield [namespace + url_name, url_patterns]

    for inner_ns, (inner_ns_path, inner_urlresolver) in \
            urlresolver.namespace_dict.items():
        inner_ns_path = namespace_path + inner_ns_path
        inner_ns = namespace + inner_ns + ':'

        # if we have inner_ns_path, reconstruct a new resolver so that we can
        # handle regex substitutions within the regex of a namespace.
        if inner_ns_path:
            args = [inner_ns_path, inner_urlresolver]

            # https://github.com/ierror/django-js-reverse/issues/65
            if LooseVersion(django.get_version()) >= LooseVersion(""2.0.6""):
                args.append(tuple(urlresolver.pattern.converters.items()))

            inner_urlresolver = urlresolvers.get_ns_resolver(*args)
            inner_ns_path = ''

        for x in prepare_url_list(inner_urlresolver, inner_ns_path, inner_ns):
            yield x","for url_pattern in urlresolver.reverse_dict.getlist(url_name):
    url_patterns += [[namespace_path + pat[0], pat[1]] for pat in url_pattern[0]]","for url_pattern in urlresolver.reverse_dict.getlist(url_name):
    (url_pattern_0, *url_pattern_rurl_patternmaining) = url_pattern
    url_patterns += [[namespace_path + pat[0], pat[1]] for pat in url_pattern[0]]","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/utils/snuba.py,https://github.com/getsentry/sentry/tree/master/src/sentry/utils/snuba.py,,get_query_params_to_update_for_projects$441,"def get_query_params_to_update_for_projects(query_params, with_org=False):
    """"""
    Get the project ID and query params that need to be updated for project
    based datasets, before we send the query to Snuba.
    """"""
    if ""project_id"" in query_params.filter_keys:
        # If we are given a set of project ids, use those directly.
        project_ids = list(set(query_params.filter_keys[""project_id""]))
    elif query_params.filter_keys:
        # Otherwise infer the project_ids from any related models
        with timer(""get_related_project_ids""):
            ids = [
                set(get_related_project_ids(k, query_params.filter_keys[k]))
                for k in query_params.filter_keys
            ]
            project_ids = list(set.union(*ids))
    elif query_params.conditions:
        project_ids = []
        for cond in query_params.conditions:
            if cond[0] == ""project_id"":
                project_ids = [cond[2]] if cond[1] == ""="" else cond[2]
    else:
        project_ids = []

    if not project_ids:
        raise UnqualifiedQueryError(
            ""No project_id filter, or none could be inferred from other filters.""
        )

    # any project will do, as they should all be from the same organization
    try:
        # Most of the time the project should exist, so get from cache to keep it fast
        organization_id = Project.objects.get_from_cache(pk=project_ids[0]).organization_id
    except Project.DoesNotExist:
        # But in the case the first project doesn't exist, grab the first non deleted project
        project = Project.objects.filter(pk__in=project_ids).values(""organization_id"").first()
        if project is None:
            raise UnqualifiedQueryError(""All project_ids from the filter no longer exist"")
        organization_id = project.get(""organization_id"")

    params = {""project"": project_ids}
    if with_org:
        params[""organization""] = organization_id

    return organization_id, params","for cond in query_params.conditions:
    if cond[0] == 'project_id':
        project_ids = [cond[2]] if cond[1] == '=' else cond[2]","for cond in query_params.conditions:
    (cond_0, cond_1, cond_2, *_) = cond
    if cond[0] == 'project_id':
        project_ids = [cond[2]] if cond[1] == '=' else cond[2]","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
GeneticAlgorithmsWithPython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GeneticAlgorithmsWithPython/es/ch16/pruebas.py,https://github.com/handcraftsman/GeneticAlgorithmsWithPython/tree/master/es/ch16/pruebas.py,,obtener_aptitud$26,"def obtener_aptitud(genes, reglas, entradas):
    circuito = nodos_a_circuito(genes)[0]
    etiquetasDeFuente = ""ABCD""
    reglasExitosas = 0
    for regla in reglas:
        entradas.clear()
        entradas.update(zip(etiquetasDeFuente, regla[0]))
        if circuito.obtener_salida() == regla[1]:
            reglasExitosas += 1
    return reglasExitosas","for regla in reglas:
    entradas.clear()
    entradas.update(zip(etiquetasDeFuente, regla[0]))
    if circuito.obtener_salida() == regla[1]:
        reglasExitosas += 1","for regla in reglas:
    (regla_0, regla_1, *_) = regla
    entradas.clear()
    entradas.update(zip(etiquetasDeFuente, regla[0]))
    if circuito.obtener_salida() == regla[1]:
        reglasExitosas += 1","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_es.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_es.py,Num2WordsESTest,test_currency_khr$2476,"def test_currency_khr(self):
        for test in TEST_CASES_TO_CURRENCY_KHR:
            self.assertEqual(
                num2words(test[0], lang='es', to='currency', currency='KHR'),
                test[1]
            )","for test in TEST_CASES_TO_CURRENCY_KHR:
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='KHR'), test[1])","for test in TEST_CASES_TO_CURRENCY_KHR:
    (test_0, test_1, *_) = test
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='KHR'), test[1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
unrpyc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unrpyc/decompiler/util.py,https://github.com/CensoredUsername/unrpyc/tree/master/decompiler/util.py,,reconstruct_paraminfo$177,"def reconstruct_paraminfo(paraminfo):
    if paraminfo is None:
        return """"

    rv = [""(""]

    sep = First("""", "", "")
    positional = [i for i in paraminfo.parameters if i[0] in paraminfo.positional]
    nameonly = [i for i in paraminfo.parameters if i not in positional]
    for parameter in positional:
        rv.append(sep())
        rv.append(parameter[0])
        if parameter[1] is not None:
            rv.append(""=%s"" % parameter[1])
    if paraminfo.extrapos:
        rv.append(sep())
        rv.append(""*%s"" % paraminfo.extrapos)
    if nameonly:
        if not paraminfo.extrapos:
            rv.append(sep())
            rv.append(""*"")
        for parameter in nameonly:
            rv.append(sep())
            rv.append(parameter[0])
            if parameter[1] is not None:
                rv.append(""=%s"" % parameter[1])
    if paraminfo.extrakw:
        rv.append(sep())
        rv.append(""**%s"" % paraminfo.extrakw)

    rv.append("")"")

    return """".join(rv)","for parameter in positional:
    rv.append(sep())
    rv.append(parameter[0])
    if parameter[1] is not None:
        rv.append('=%s' % parameter[1])","for parameter in positional:
    (parameter_0, parameter_1, *_) = parameter
    rv.append(sep())
    rv.append(parameter[0])
    if parameter[1] is not None:
        rv.append('=%s' % parameter[1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
unrpyc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unrpyc/decompiler/util.py,https://github.com/CensoredUsername/unrpyc/tree/master/decompiler/util.py,,reconstruct_paraminfo$177,"def reconstruct_paraminfo(paraminfo):
    if paraminfo is None:
        return """"

    rv = [""(""]

    sep = First("""", "", "")
    positional = [i for i in paraminfo.parameters if i[0] in paraminfo.positional]
    nameonly = [i for i in paraminfo.parameters if i not in positional]
    for parameter in positional:
        rv.append(sep())
        rv.append(parameter[0])
        if parameter[1] is not None:
            rv.append(""=%s"" % parameter[1])
    if paraminfo.extrapos:
        rv.append(sep())
        rv.append(""*%s"" % paraminfo.extrapos)
    if nameonly:
        if not paraminfo.extrapos:
            rv.append(sep())
            rv.append(""*"")
        for parameter in nameonly:
            rv.append(sep())
            rv.append(parameter[0])
            if parameter[1] is not None:
                rv.append(""=%s"" % parameter[1])
    if paraminfo.extrakw:
        rv.append(sep())
        rv.append(""**%s"" % paraminfo.extrakw)

    rv.append("")"")

    return """".join(rv)","for parameter in nameonly:
    rv.append(sep())
    rv.append(parameter[0])
    if parameter[1] is not None:
        rv.append('=%s' % parameter[1])","for parameter in nameonly:
    (parameter_0, parameter_1, *_) = parameter
    rv.append(sep())
    rv.append(parameter[0])
    if parameter[1] is not None:
        rv.append('=%s' % parameter[1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,,taper_off_qubits$404,"def taper_off_qubits(operator,
                     stabilizers,
                     manual_input=False,
                     fixed_positions=None,
                     output_tapered_positions=False):
    r""""""
    Remove qubits from given operator.

    Qubits are removed by eliminating an equivalent number of
    stabilizer conditions. Which qubits that are can either be determined
    automatically or their positions can be set manually.

    Qubits can be disregarded from the Hamiltonian when the effect of all its
    terms on them is rendered trivial. This algorithm employs a stabilizers
    like $\pm X \otimes p$ to fix the action of every Pauli
    string on the first qubit to $Z$ or the identity. A string
    $X \otimes h$ would for instance be multiplied with the stabilizer
    to obtain $1 \otimes (\pm h\cdot p)$ while a string
    $Z \otimes h^\prime$ would pass without correction. The first
    qubit can subsequently be removed as it must be in the computational basis
    in Hamiltonian eigenstates.
    For stabilizers acting as $Y$ ($Z$) on selected qubits,
    the algorithm would fix the action of every Hamiltonian string to
    $Z$ ($X$). Updating also the list of remaining stabilizer
    generators, the algorithm is run iteratively.

    Args:
        operator (QubitOperator): Operator of which qubits will be removed.
        stabilizers (QubitOperator): Stabilizer generators for the tapering.
                                     Can also be passed as a list of
                                     QubitOperator.
        manual_input (Boolean): Option to pass the list of fixed qubits
                                positions manually. Set to False by default.
        fixed_positions (list): (optional) List of fixed qubit positions.
                                Passing a list is only effective if
                                manual_input is True.
        output_tapered_positions (Boolean): Option to output the positions of
                                            qubits that have been removed.
    Returns:
        skimmed_operator (QubitOperator): Operator with fewer qubits.
        removed_positions (list): (optional) List of removed qubit positions.
                                  For the qubits to be gone in the qubit count,
                                  the remaining qubits have been moved up to
                                  those indices.
    """"""
    if isinstance(stabilizers, (list, tuple, numpy.ndarray)):
        n_qbits_stabs = 0
        for ent in stabilizers:
            if op_utils.count_qubits(ent) > n_qbits_stabs:
                n_qbits_stabs = op_utils.count_qubits(ent)
    else:
        n_qbits_stabs = op_utils.count_qubits(stabilizers)

    n_qbits = max(op_utils.count_qubits(operator), n_qbits_stabs)

    (ham_to_update,
     qbts_to_rm) = reduce_number_of_terms(operator,
                                          stabilizers,
                                          maintain_length=False,
                                          manual_input=manual_input,
                                          fixed_positions=fixed_positions,
                                          output_fixed_positions=True)

    # Gets a list of the order of the qubits after tapering
    qbit_order = list(numpy.arange(n_qbits - len(qbts_to_rm), dtype=int))
    # Save the original list before it gets ordered
    removed_positions = qbts_to_rm
    qbts_to_rm.sort()
    for x in qbts_to_rm:
        qbit_order.insert(x, 'remove')

    # Remove the qubits
    skimmed_operator = QubitOperator()
    for term, coef in ham_to_update.terms.items():
        if term == ():
            skimmed_operator += QubitOperator('', coef)
            continue
        tap_tpls = []
        for p in term:
            if qbit_order[p[0]] != 'remove':
                tap_tpls.append((qbit_order[p[0]].item(), p[1]))

        skimmed_operator += QubitOperator(tuple(tap_tpls), coef)

    if output_tapered_positions:
        return skimmed_operator, removed_positions
    else:
        return skimmed_operator","for p in term:
    if qbit_order[p[0]] != 'remove':
        tap_tpls.append((qbit_order[p[0]].item(), p[1]))","for p in term:
    (p_0, p_1, *p_rpmaining) = p
    if qbit_order[p[0]] != 'remove':
        tap_tpls.append((qbit_order[p[0]].item(), p[1]))","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
FASPell,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FASPell/bert_modified/modeling.py,https://github.com/iqiyi/FASPell/tree/master/bert_modified/modeling.py,,get_assignment_map_from_checkpoint$318,"def get_assignment_map_from_checkpoint(tvars, init_checkpoint):
  """"""Compute the union of the current variables and checkpoint variables.""""""
  assignment_map = {}
  initialized_variable_names = {}

  name_to_variable = collections.OrderedDict()
  for var in tvars:
    name = var.name
    m = re.match(""^(.*):\\d+$"", name)
    if m is not None:
      name = m.group(1)
    name_to_variable[name] = var

  init_vars = tf.train.list_variables(init_checkpoint)

  assignment_map = collections.OrderedDict()
  for x in init_vars:
    (name, var) = (x[0], x[1])
    if name not in name_to_variable:
      continue
    assignment_map[name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[name + "":0""] = 1

  return (assignment_map, initialized_variable_names)","for x in init_vars:
    (name, var) = (x[0], x[1])
    if name not in name_to_variable:
        continue
    assignment_map[name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[name + ':0'] = 1","for x in init_vars:
    (x_0, x_1, *_) = x
    (name, var) = (x[0], x[1])
    if name not in name_to_variable:
        continue
    assignment_map[name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[name + ':0'] = 1","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pylearn2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pylearn2/pylearn2/scripts/tutorials/jobman_demo/utils.py,https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/scripts/tutorials/jobman_demo/utils.py,,parse_results$44,"def parse_results(cwd):
    optimal_dd = None
    optimal_measure = numpy.inf

    for tup in tools.find_conf_files(cwd):
        dd = tup[1]
        if 'results.train_y_misclass' in dd:
            if dd['results.train_y_misclass'] < optimal_measure:
                optimal_measure = dd['results.train_y_misclass']
                optimal_dd = dd

    print(""Optimal results.train_y_misclass:"", str(optimal_measure))
    for key, value in optimal_dd.items():
        if 'hyper_parameters' in key:
            print(key + "": "" + str(value))","for tup in tools.find_conf_files(cwd):
    dd = tup[1]
    if 'results.train_y_misclass' in dd:
        if dd['results.train_y_misclass'] < optimal_measure:
            optimal_measure = dd['results.train_y_misclass']
            optimal_dd = dd","for tup in tools.find_conf_files(cwd):
    (_, tup_1, *tup_rtupmaining) = tup
    dd = tup[1]
    if 'results.train_y_misclass' in dd:
        if dd['results.train_y_misclass'] < optimal_measure:
            optimal_measure = dd['results.train_y_misclass']
            optimal_dd = dd","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
PythonStdioGames,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonStdioGames/src/gamesbyexample/pygame_games/memorypuzzle.py,https://github.com/asweigart/PythonStdioGames/tree/master/src/gamesbyexample/pygame_games/memorypuzzle.py,,drawBoxCovers$209,"def drawBoxCovers(board, boxes, coverage):
    # Draws boxes being covered/revealed. ""boxes"" is a list
    # of two-item lists, which have the x & y spot of the box.
    for box in boxes:
        left, top = leftTopCoordsOfBox(box[0], box[1])
        pygame.draw.rect(DISPLAYSURF, BGCOLOR, (left, top, BOXSIZE, BOXSIZE))
        shape, color = getShapeAndColor(board, box[0], box[1])
        drawIcon(shape, color, box[0], box[1])
        if coverage > 0: # only draw the cover if there is an coverage
            pygame.draw.rect(DISPLAYSURF, BOXCOLOR, (left, top, coverage, BOXSIZE))
    pygame.display.update()
    FPSCLOCK.tick(FPS)","for box in boxes:
    (left, top) = leftTopCoordsOfBox(box[0], box[1])
    pygame.draw.rect(DISPLAYSURF, BGCOLOR, (left, top, BOXSIZE, BOXSIZE))
    (shape, color) = getShapeAndColor(board, box[0], box[1])
    drawIcon(shape, color, box[0], box[1])
    if coverage > 0:
        pygame.draw.rect(DISPLAYSURF, BOXCOLOR, (left, top, coverage, BOXSIZE))","for box in boxes:
    (box_0, box_1, *_) = box
    (left, top) = leftTopCoordsOfBox(box[0], box[1])
    pygame.draw.rect(DISPLAYSURF, BGCOLOR, (left, top, BOXSIZE, BOXSIZE))
    (shape, color) = getShapeAndColor(board, box[0], box[1])
    drawIcon(shape, color, box[0], box[1])
    if coverage > 0:
        pygame.draw.rect(DISPLAYSURF, BOXCOLOR, (left, top, coverage, BOXSIZE))","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pynguin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pynguin/pynguin/testcase/statement.py,https://github.com/se2p/pynguin/tree/master/pynguin/testcase/statement.py,DictStatement,get_variable_references$681,"def get_variable_references(self) -> Set[vr.VariableReference]:
        references = set()
        references.add(self.ret_val)
        for entry in self._elements:
            references.add(entry[0])
            references.add(entry[1])
        return references","for entry in self._elements:
    references.add(entry[0])
    references.add(entry[1])","for entry in self._elements:
    (entry_0, entry_1, *_) = entry
    references.add(entry[0])
    references.add(entry[1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/doctype/sales_invoice/test_sales_invoice.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/doctype/sales_invoice/test_sales_invoice.py,TestSalesInvoice,test_rounding_adjustment_3$2029,"def test_rounding_adjustment_3(self):
		from erpnext.accounts.doctype.accounting_dimension.test_accounting_dimension import (
			create_dimension,
			disable_dimension,
		)

		create_dimension()

		si = create_sales_invoice(do_not_save=True)
		si.items = []
		for d in [(1122, 2), (1122.01, 1), (1122.01, 1)]:
			si.append(
				""items"",
				{
					""item_code"": ""_Test Item"",
					""gst_hsn_code"": ""999800"",
					""warehouse"": ""_Test Warehouse - _TC"",
					""qty"": d[1],
					""rate"": d[0],
					""income_account"": ""Sales - _TC"",
					""cost_center"": ""_Test Cost Center - _TC"",
				},
			)
		for tax_account in [""_Test Account VAT - _TC"", ""_Test Account Service Tax - _TC""]:
			si.append(
				""taxes"",
				{
					""charge_type"": ""On Net Total"",
					""account_head"": tax_account,
					""description"": tax_account,
					""rate"": 6,
					""cost_center"": ""_Test Cost Center - _TC"",
					""included_in_print_rate"": 1,
				},
			)

		si.cost_center = ""_Test Cost Center 2 - _TC""
		si.location = ""Block 1""

		si.save()
		si.submit()
		self.assertEqual(si.net_total, 4007.16)
		self.assertEqual(si.grand_total, 4488.02)
		self.assertEqual(si.total_taxes_and_charges, 480.86)
		self.assertEqual(si.rounding_adjustment, -0.02)

		expected_values = dict(
			(d[0], d)
			for d in [
				[si.debit_to, 4488.0, 0.0],
				[""_Test Account Service Tax - _TC"", 0.0, 240.43],
				[""_Test Account VAT - _TC"", 0.0, 240.43],
				[""Sales - _TC"", 0.0, 4007.15],
				[""Round Off - _TC"", 0.01, 0],
			]
		)

		gl_entries = frappe.db.sql(
			""""""select account, debit, credit
			from `tabGL Entry` where voucher_type='Sales Invoice' and voucher_no=%s
			order by account asc"""""",
			si.name,
			as_dict=1,
		)

		debit_credit_diff = 0
		for gle in gl_entries:
			self.assertEqual(expected_values[gle.account][0], gle.account)
			self.assertEqual(expected_values[gle.account][1], gle.debit)
			self.assertEqual(expected_values[gle.account][2], gle.credit)
			debit_credit_diff += gle.debit - gle.credit

		self.assertEqual(debit_credit_diff, 0)

		round_off_gle = frappe.db.get_value(
			""GL Entry"",
			{""voucher_type"": ""Sales Invoice"", ""voucher_no"": si.name, ""account"": ""Round Off - _TC""},
			[""cost_center"", ""location""],
			as_dict=1,
		)

		self.assertEqual(round_off_gle.cost_center, ""_Test Cost Center 2 - _TC"")
		self.assertEqual(round_off_gle.location, ""Block 1"")

		disable_dimension()","for d in [(1122, 2), (1122.01, 1), (1122.01, 1)]:
    si.append('items', {'item_code': '_Test Item', 'gst_hsn_code': '999800', 'warehouse': '_Test Warehouse - _TC', 'qty': d[1], 'rate': d[0], 'income_account': 'Sales - _TC', 'cost_center': '_Test Cost Center - _TC'})","for d in [(1122, 2), (1122.01, 1), (1122.01, 1)]:
    (d_0, d_1, *_) = d
    si.append('items', {'item_code': '_Test Item', 'gst_hsn_code': '999800', 'warehouse': '_Test Warehouse - _TC', 'qty': d[1], 'rate': d[0], 'income_account': 'Sales - _TC', 'cost_center': '_Test Cost Center - _TC'})","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
djongo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/djongo/tests/django_tests/tests/v22/tests/template_tests/test_custom.py,https://github.com/nesdis/djongo/tree/master/tests/django_tests/tests/v22/tests/template_tests/test_custom.py,InclusionTagTests,test_inclusion_tags$159,"def test_inclusion_tags(self):
        c = Context({'value': 42})

        templates = [
            ('{% load inclusion %}{% inclusion_no_params %}', 'inclusion_no_params - Expected result\n'),
            ('{% load inclusion %}{% inclusion_one_param 37 %}', 'inclusion_one_param - Expected result: 37\n'),
            ('{% load inclusion %}{% inclusion_explicit_no_context 37 %}',
                'inclusion_explicit_no_context - Expected result: 37\n'),
            ('{% load inclusion %}{% inclusion_no_params_with_context %}',
                'inclusion_no_params_with_context - Expected result (context value: 42)\n'),
            ('{% load inclusion %}{% inclusion_params_and_context 37 %}',
                'inclusion_params_and_context - Expected result (context value: 42): 37\n'),
            ('{% load inclusion %}{% inclusion_two_params 37 42 %}',
                'inclusion_two_params - Expected result: 37, 42\n'),
            (
                '{% load inclusion %}{% inclusion_one_default 37 %}',
                'inclusion_one_default - Expected result: 37, hi\n'
            ),
            ('{% load inclusion %}{% inclusion_one_default 37 two=""hello"" %}',
                'inclusion_one_default - Expected result: 37, hello\n'),
            ('{% load inclusion %}{% inclusion_one_default one=99 two=""hello"" %}',
                'inclusion_one_default - Expected result: 99, hello\n'),
            ('{% load inclusion %}{% inclusion_one_default 37 42 %}',
                'inclusion_one_default - Expected result: 37, 42\n'),
            ('{% load inclusion %}{% inclusion_unlimited_args 37 %}',
                'inclusion_unlimited_args - Expected result: 37, hi\n'),
            ('{% load inclusion %}{% inclusion_unlimited_args 37 42 56 89 %}',
                'inclusion_unlimited_args - Expected result: 37, 42, 56, 89\n'),
            ('{% load inclusion %}{% inclusion_only_unlimited_args %}',
                'inclusion_only_unlimited_args - Expected result: \n'),
            ('{% load inclusion %}{% inclusion_only_unlimited_args 37 42 56 89 %}',
                'inclusion_only_unlimited_args - Expected result: 37, 42, 56, 89\n'),
            ('{% load inclusion %}{% inclusion_unlimited_args_kwargs 37 40|add:2 56 eggs=""scrambled"" four=1|add:3 %}',
                'inclusion_unlimited_args_kwargs - Expected result: 37, 42, 56 / eggs=scrambled, four=4\n'),
        ]

        for entry in templates:
            t = self.engine.from_string(entry[0])
            self.assertEqual(t.render(c), entry[1])","for entry in templates:
    t = self.engine.from_string(entry[0])
    self.assertEqual(t.render(c), entry[1])","for entry in templates:
    (entry_0, entry_1, *_) = entry
    t = self.engine.from_string(entry[0])
    self.assertEqual(t.render(c), entry[1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/CreatHtml.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/CreatHtml.py,CreatHtml,vuln_list$27,"def vuln_list(self):
        global creat_vuln_num
        creat_vuln_num = 1
        tr_whole_api_list = """"

        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        for vuln_info in vuln_infos:
            if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    tr_api_list = """"""
                    <tr>
                      <td>{vuln_num}</td>
                      <td>{vuln_api_name}</td>
                      <td>{vuln_url}</td>
                      <td>{vuln_url_type}</td>
                      <td>{vuln_risk}</td>
                      <td>{vuln_length}</td>
                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>
                    </tr>""""""
                    if api_info[5] == 1:
                        vuln_url_type = Utils().getMyWord(""{r_get}"")
                    else:
                        vuln_url_type = Utils().getMyWord(""{r_post}"")
                    tr_api_list = tr_api_list.replace(""{vuln_num}"", str(self.creat_api_num))
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    try:
                        api_length_info = len(api_info[4])
                    except:
                        api_length_info = 0
                    for js_path in js_paths:
                        tr_api_list = tr_api_list.replace(""{vuln_api_name}"",api_info[2])
                        tr_api_list = tr_api_list.replace(""{vuln_url}"", api_info[1])
                        tr_api_list = tr_api_list.replace(""{vuln_url_type}"", vuln_url_type)
                        tr_api_list = tr_api_list.replace(""{vuln_length}"", str(api_length_info))
                        tr_api_list = tr_api_list.replace(""{vuln_risk}"", Utils().getMyWord(""{r_l_m}""))
                        tr_api_list = tr_api_list.replace(""{vuln_id}"", ""vuln_"" + str(creat_vuln_num))
                        self.creat_api_num = self.creat_api_num + 1
                        creat_vuln_num = creat_vuln_num + 1
                        tr_whole_api_list = tr_whole_api_list + tr_api_list
            elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    tr_api_list = """"""
                    <tr>
                      <td>{vuln_num}</td>
                      <td>{vuln_api_name}</td>
                      <td>{vuln_url}</td>
                      <td>{vuln_url_type}</td>
                      <td>{vuln_risk}</td>
                      <td>{vuln_length}</td>
                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>
                    </tr>""""""
                    if api_info[5] == 1:
                        vuln_url_type = Utils().getMyWord(""{r_get}"")
                    else:
                        vuln_url_type = Utils().getMyWord(""{r_post}"")
                    tr_api_list = tr_api_list.replace(""{vuln_num}"", str(self.creat_api_num))
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    try:
                        api_length_info = len(api_info[4])
                    except:
                        api_length_info = 0
                    for js_path in js_paths:
                        tr_api_list = tr_api_list.replace(""{vuln_api_name}"", api_info[2])
                        tr_api_list = tr_api_list.replace(""{vuln_url}"", api_info[1])
                        tr_api_list = tr_api_list.replace(""{vuln_url_type}"", vuln_url_type)
                        tr_api_list = tr_api_list.replace(""{vuln_length}"", str(api_length_info))
                        tr_api_list = tr_api_list.replace(""{vuln_risk}"", Utils().getMyWord(""{r_l_l}"") + ""，"" + Utils().getMyWord(""{r_vuln_maybe}""))
                        tr_api_list = tr_api_list.replace(""{vuln_id}"", ""vuln_"" + str(creat_vuln_num))
                        self.creat_api_num = self.creat_api_num + 1
                        creat_vuln_num = creat_vuln_num + 1
                        tr_whole_api_list = tr_whole_api_list + tr_api_list
        return tr_whole_api_list","for vuln_info in vuln_infos:
    if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list
    elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '，' + Utils().getMyWord('{r_vuln_maybe}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list","for vuln_info in vuln_infos:
    (vuln_info_1, vuln_info_2, vuln_info_3, vuln_info_4, *_) = vuln_info
    if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list
    elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '，' + Utils().getMyWord('{r_vuln_maybe}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",0,,,"Answer: Yes
Iterable Unpacking: e_1, e_2, e_3, e_4 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/CreatHtml.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/CreatHtml.py,CreatHtml,vuln_list$27,"def vuln_list(self):
        global creat_vuln_num
        creat_vuln_num = 1
        tr_whole_api_list = """"

        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        for vuln_info in vuln_infos:
            if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    tr_api_list = """"""
                    <tr>
                      <td>{vuln_num}</td>
                      <td>{vuln_api_name}</td>
                      <td>{vuln_url}</td>
                      <td>{vuln_url_type}</td>
                      <td>{vuln_risk}</td>
                      <td>{vuln_length}</td>
                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>
                    </tr>""""""
                    if api_info[5] == 1:
                        vuln_url_type = Utils().getMyWord(""{r_get}"")
                    else:
                        vuln_url_type = Utils().getMyWord(""{r_post}"")
                    tr_api_list = tr_api_list.replace(""{vuln_num}"", str(self.creat_api_num))
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    try:
                        api_length_info = len(api_info[4])
                    except:
                        api_length_info = 0
                    for js_path in js_paths:
                        tr_api_list = tr_api_list.replace(""{vuln_api_name}"",api_info[2])
                        tr_api_list = tr_api_list.replace(""{vuln_url}"", api_info[1])
                        tr_api_list = tr_api_list.replace(""{vuln_url_type}"", vuln_url_type)
                        tr_api_list = tr_api_list.replace(""{vuln_length}"", str(api_length_info))
                        tr_api_list = tr_api_list.replace(""{vuln_risk}"", Utils().getMyWord(""{r_l_m}""))
                        tr_api_list = tr_api_list.replace(""{vuln_id}"", ""vuln_"" + str(creat_vuln_num))
                        self.creat_api_num = self.creat_api_num + 1
                        creat_vuln_num = creat_vuln_num + 1
                        tr_whole_api_list = tr_whole_api_list + tr_api_list
            elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    tr_api_list = """"""
                    <tr>
                      <td>{vuln_num}</td>
                      <td>{vuln_api_name}</td>
                      <td>{vuln_url}</td>
                      <td>{vuln_url_type}</td>
                      <td>{vuln_risk}</td>
                      <td>{vuln_length}</td>
                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>
                    </tr>""""""
                    if api_info[5] == 1:
                        vuln_url_type = Utils().getMyWord(""{r_get}"")
                    else:
                        vuln_url_type = Utils().getMyWord(""{r_post}"")
                    tr_api_list = tr_api_list.replace(""{vuln_num}"", str(self.creat_api_num))
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    try:
                        api_length_info = len(api_info[4])
                    except:
                        api_length_info = 0
                    for js_path in js_paths:
                        tr_api_list = tr_api_list.replace(""{vuln_api_name}"", api_info[2])
                        tr_api_list = tr_api_list.replace(""{vuln_url}"", api_info[1])
                        tr_api_list = tr_api_list.replace(""{vuln_url_type}"", vuln_url_type)
                        tr_api_list = tr_api_list.replace(""{vuln_length}"", str(api_length_info))
                        tr_api_list = tr_api_list.replace(""{vuln_risk}"", Utils().getMyWord(""{r_l_l}"") + ""，"" + Utils().getMyWord(""{r_vuln_maybe}""))
                        tr_api_list = tr_api_list.replace(""{vuln_id}"", ""vuln_"" + str(creat_vuln_num))
                        self.creat_api_num = self.creat_api_num + 1
                        creat_vuln_num = creat_vuln_num + 1
                        tr_whole_api_list = tr_whole_api_list + tr_api_list
        return tr_whole_api_list","for api_info in api_infos:
    tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
    if api_info[5] == 1:
        vuln_url_type = Utils().getMyWord('{r_get}')
    else:
        vuln_url_type = Utils().getMyWord('{r_post}')
    tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    try:
        api_length_info = len(api_info[4])
    except:
        api_length_info = 0
    for js_path in js_paths:
        tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
        tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
        tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
        tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
        tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
        tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
        self.creat_api_num = self.creat_api_num + 1
        creat_vuln_num = creat_vuln_num + 1
        tr_whole_api_list = tr_whole_api_list + tr_api_list","for api_info in api_infos:
    (_, api_info_1, api_info_2, _, api_info_4, api_info_5, *_) = api_info
    tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
    if api_info[5] == 1:
        vuln_url_type = Utils().getMyWord('{r_get}')
    else:
        vuln_url_type = Utils().getMyWord('{r_post}')
    tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    try:
        api_length_info = len(api_info[4])
    except:
        api_length_info = 0
    for js_path in js_paths:
        tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
        tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
        tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
        tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
        tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
        tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
        self.creat_api_num = self.creat_api_num + 1
        creat_vuln_num = creat_vuln_num + 1
        tr_whole_api_list = tr_whole_api_list + tr_api_list","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, _, e_4, e_5 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_4: e[4]
e_5: e[5]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/CreatHtml.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/CreatHtml.py,CreatHtml,vuln_list$27,"def vuln_list(self):
        global creat_vuln_num
        creat_vuln_num = 1
        tr_whole_api_list = """"

        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        for vuln_info in vuln_infos:
            if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    tr_api_list = """"""
                    <tr>
                      <td>{vuln_num}</td>
                      <td>{vuln_api_name}</td>
                      <td>{vuln_url}</td>
                      <td>{vuln_url_type}</td>
                      <td>{vuln_risk}</td>
                      <td>{vuln_length}</td>
                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>
                    </tr>""""""
                    if api_info[5] == 1:
                        vuln_url_type = Utils().getMyWord(""{r_get}"")
                    else:
                        vuln_url_type = Utils().getMyWord(""{r_post}"")
                    tr_api_list = tr_api_list.replace(""{vuln_num}"", str(self.creat_api_num))
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    try:
                        api_length_info = len(api_info[4])
                    except:
                        api_length_info = 0
                    for js_path in js_paths:
                        tr_api_list = tr_api_list.replace(""{vuln_api_name}"",api_info[2])
                        tr_api_list = tr_api_list.replace(""{vuln_url}"", api_info[1])
                        tr_api_list = tr_api_list.replace(""{vuln_url_type}"", vuln_url_type)
                        tr_api_list = tr_api_list.replace(""{vuln_length}"", str(api_length_info))
                        tr_api_list = tr_api_list.replace(""{vuln_risk}"", Utils().getMyWord(""{r_l_m}""))
                        tr_api_list = tr_api_list.replace(""{vuln_id}"", ""vuln_"" + str(creat_vuln_num))
                        self.creat_api_num = self.creat_api_num + 1
                        creat_vuln_num = creat_vuln_num + 1
                        tr_whole_api_list = tr_whole_api_list + tr_api_list
            elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    tr_api_list = """"""
                    <tr>
                      <td>{vuln_num}</td>
                      <td>{vuln_api_name}</td>
                      <td>{vuln_url}</td>
                      <td>{vuln_url_type}</td>
                      <td>{vuln_risk}</td>
                      <td>{vuln_length}</td>
                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>
                    </tr>""""""
                    if api_info[5] == 1:
                        vuln_url_type = Utils().getMyWord(""{r_get}"")
                    else:
                        vuln_url_type = Utils().getMyWord(""{r_post}"")
                    tr_api_list = tr_api_list.replace(""{vuln_num}"", str(self.creat_api_num))
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    try:
                        api_length_info = len(api_info[4])
                    except:
                        api_length_info = 0
                    for js_path in js_paths:
                        tr_api_list = tr_api_list.replace(""{vuln_api_name}"", api_info[2])
                        tr_api_list = tr_api_list.replace(""{vuln_url}"", api_info[1])
                        tr_api_list = tr_api_list.replace(""{vuln_url_type}"", vuln_url_type)
                        tr_api_list = tr_api_list.replace(""{vuln_length}"", str(api_length_info))
                        tr_api_list = tr_api_list.replace(""{vuln_risk}"", Utils().getMyWord(""{r_l_l}"") + ""，"" + Utils().getMyWord(""{r_vuln_maybe}""))
                        tr_api_list = tr_api_list.replace(""{vuln_id}"", ""vuln_"" + str(creat_vuln_num))
                        self.creat_api_num = self.creat_api_num + 1
                        creat_vuln_num = creat_vuln_num + 1
                        tr_whole_api_list = tr_whole_api_list + tr_api_list
        return tr_whole_api_list","for api_info in api_infos:
    tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
    if api_info[5] == 1:
        vuln_url_type = Utils().getMyWord('{r_get}')
    else:
        vuln_url_type = Utils().getMyWord('{r_post}')
    tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    try:
        api_length_info = len(api_info[4])
    except:
        api_length_info = 0
    for js_path in js_paths:
        tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
        tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
        tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
        tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
        tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '，' + Utils().getMyWord('{r_vuln_maybe}'))
        tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
        self.creat_api_num = self.creat_api_num + 1
        creat_vuln_num = creat_vuln_num + 1
        tr_whole_api_list = tr_whole_api_list + tr_api_list","for api_info in api_infos:
    (_, api_info_1, api_info_2, _, api_info_4, api_info_5, *_) = api_info
    tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
    if api_info[5] == 1:
        vuln_url_type = Utils().getMyWord('{r_get}')
    else:
        vuln_url_type = Utils().getMyWord('{r_post}')
    tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    try:
        api_length_info = len(api_info[4])
    except:
        api_length_info = 0
    for js_path in js_paths:
        tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
        tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
        tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
        tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
        tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '，' + Utils().getMyWord('{r_vuln_maybe}'))
        tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
        self.creat_api_num = self.creat_api_num + 1
        creat_vuln_num = creat_vuln_num + 1
        tr_whole_api_list = tr_whole_api_list + tr_api_list","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, _, e_4, e_5 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_4: e[4]
e_5: e[5]",,,,,,,
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/setup/doctype/company/company.py,https://github.com/frappe/erpnext/tree/master/erpnext/setup/doctype/company/company.py,Company,validate_default_accounts$88,"def validate_default_accounts(self):
		accounts = [
			[""Default Bank Account"", ""default_bank_account""],
			[""Default Cash Account"", ""default_cash_account""],
			[""Default Receivable Account"", ""default_receivable_account""],
			[""Default Payable Account"", ""default_payable_account""],
			[""Default Expense Account"", ""default_expense_account""],
			[""Default Income Account"", ""default_income_account""],
			[""Stock Received But Not Billed Account"", ""stock_received_but_not_billed""],
			[""Stock Adjustment Account"", ""stock_adjustment_account""],
			[""Expense Included In Valuation Account"", ""expenses_included_in_valuation""],
		]

		for account in accounts:
			if self.get(account[1]):
				for_company = frappe.db.get_value(""Account"", self.get(account[1]), ""company"")
				if for_company != self.name:
					frappe.throw(
						_(""Account {0} does not belong to company: {1}"").format(self.get(account[1]), self.name)
					)

				if get_account_currency(self.get(account[1])) != self.default_currency:
					error_message = _(
						""{0} currency must be same as company's default currency. Please select another account.""
					).format(frappe.bold(account[0]))
					frappe.throw(error_message)","for account in accounts:
    if self.get(account[1]):
        for_company = frappe.db.get_value('Account', self.get(account[1]), 'company')
        if for_company != self.name:
            frappe.throw(_('Account {0} does not belong to company: {1}').format(self.get(account[1]), self.name))
        if get_account_currency(self.get(account[1])) != self.default_currency:
            error_message = _(""{0} currency must be same as company's default currency. Please select another account."").format(frappe.bold(account[0]))
            frappe.throw(error_message)","for account in accounts:
    (account_0, account_1, *_) = account
    if self.get(account[1]):
        for_company = frappe.db.get_value('Account', self.get(account[1]), 'company')
        if for_company != self.name:
            frappe.throw(_('Account {0} does not belong to company: {1}').format(self.get(account[1]), self.name))
        if get_account_currency(self.get(account[1])) != self.default_currency:
            error_message = _(""{0} currency must be same as company's default currency. Please select another account."").format(frappe.bold(account[0]))
            frappe.throw(error_message)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pyglet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglet/contrib/aseprite_codec/aseprite.py,https://github.com/pyglet/pyglet/tree/master/contrib/aseprite_codec/aseprite.py,Frame,_convert_to_rgba$193,"def _convert_to_rgba(self, cel):
        if self.color_depth == 8:
            global PALETTE_INDEX
            pixel_array = []
            for pixel in cel.pixel_data:
                if pixel == PALETTE_INDEX:
                    pixel_array.extend([0, 0, 0, 0])
                else:
                    pixel_array.extend(PALETTE_DICT[pixel])
            cel.pixel_data = bytes(pixel_array)
            return cel

        elif self.color_depth == 16:
            greyscale_iter = _chunked_iter(cel.pixel_data, 2)
            pixel_array = []
            for pixel in greyscale_iter:
                rgba = (pixel[0] * 3) + pixel[1]
                pixel_array.append(rgba)
            cel.pixel_data = bytes(pixel_array)
            return cel

        else:
            return cel","for pixel in greyscale_iter:
    rgba = pixel[0] * 3 + pixel[1]
    pixel_array.append(rgba)","for pixel in greyscale_iter:
    (pixel_0, pixel_1, *_) = pixel
    rgba = pixel[0] * 3 + pixel[1]
    pixel_array.append(rgba)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
PornHub-downloader-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PornHub-downloader-python/functions.py,https://github.com/mariosemes/PornHub-downloader-python/tree/master//functions.py,,dl_all_items$121,"def dl_all_items(conn):
    c = conn.cursor()
    try:
        c.execute(""SELECT * FROM ph_items"")
    except Error as e:
        print(e)
        sys.exit()

    rows = c.fetchall()

    for row in rows:
        if row[1] == ""model"":
            url_after = ""/videos/upload""
        # elif row[1] == ""pornstar"":
        #     url_after = ""/""
        elif row[1] == ""users"":
            url_after = ""/videos/public""
        elif row[1] == ""channels"":
            url_after = ""/videos""
        else:
            url_after = """"

        print(""-----------------------------"")
        print(row[1])
        print(row[2])
        print(""https://www.pornhub.com/"" + str(row[1]) + ""/"" + str(row[2]) + url_after)
        print(""-----------------------------"")

        # Find more available options here: https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/YoutubeDL.py#L129-L279
        outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
        ydl_opts_start = {
            'format': 'best',
            'playliststart:': 1,
            'playlistend': 4,
            'outtmpl': outtmpl,
            'nooverwrites': True,
            'no_warnings': False,
            'ignoreerrors': True,
        }

        url = ""https://www.pornhub.com/"" + str(row[1]) + ""/"" + str(row[2] + url_after)
        with youtube_dl.YoutubeDL(ydl_opts_start) as ydl:
            ydl.download([url])

        try:
            c.execute(""UPDATE ph_items SET lastchecked=CURRENT_TIMESTAMP WHERE url_name = ?"", (row[2],))
            conn.commit()
        except Error as e:
            print(e)
            sys.exit()","for row in rows:
    if row[1] == 'model':
        url_after = '/videos/upload'
    elif row[1] == 'users':
        url_after = '/videos/public'
    elif row[1] == 'channels':
        url_after = '/videos'
    else:
        url_after = ''
    print('-----------------------------')
    print(row[1])
    print(row[2])
    print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
    print('-----------------------------')
    outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
    ydl_opts_start = {'format': 'best', 'playliststart:': 1, 'playlistend': 4, 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
    url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2] + url_after)
    with youtube_dl.YoutubeDL(ydl_opts_start) as ydl:
        ydl.download([url])
    try:
        c.execute('UPDATE ph_items SET lastchecked=CURRENT_TIMESTAMP WHERE url_name = ?', (row[2],))
        conn.commit()
    except Error as e:
        print(e)
        sys.exit()","for row in rows:
    (_, row_1, row_2, row_3, *_) = row
    if row[1] == 'model':
        url_after = '/videos/upload'
    elif row[1] == 'users':
        url_after = '/videos/public'
    elif row[1] == 'channels':
        url_after = '/videos'
    else:
        url_after = ''
    print('-----------------------------')
    print(row[1])
    print(row[2])
    print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
    print('-----------------------------')
    outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
    ydl_opts_start = {'format': 'best', 'playliststart:': 1, 'playlistend': 4, 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
    url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2] + url_after)
    with youtube_dl.YoutubeDL(ydl_opts_start) as ydl:
        ydl.download([url])
    try:
        c.execute('UPDATE ph_items SET lastchecked=CURRENT_TIMESTAMP WHERE url_name = ?', (row[2],))
        conn.commit()
    except Error as e:
        print(e)
        sys.exit()","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, e_3 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/hamiltonians/jellium_hf_state.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/hamiltonians/jellium_hf_state.py,,hartree_fock_state_jellium$45,"def hartree_fock_state_jellium(grid,
                               n_electrons,
                               spinless=True,
                               plane_wave=False):
    """"""Give the Hartree-Fock state of jellium.

    Args:
        grid (Grid): The discretization to use.
        n_electrons (int): Number of electrons in the system.
        spinless (bool): Whether to use the spinless model or not.
        plane_wave (bool): Whether to return the Hartree-Fock state in
                           the plane wave (True) or dual basis (False).

    Notes:
        The jellium model is built up by filling the lowest-energy
        single-particle states in the plane-wave Hamiltonian until
        n_electrons states are filled.
    """"""

    # Get the jellium Hamiltonian in the plane wave basis.
    # For determining the Hartree-Fock state in the PW basis, only the
    # kinetic energy terms matter.
    hamiltonian = plane_wave_kinetic(grid, spinless=spinless)
    hamiltonian = normal_ordered(hamiltonian)
    hamiltonian.compress()

    # The number of occupied single-particle states is the number of electrons.
    # Those states with the lowest single-particle energies are occupied first.
    occupied_states = lowest_single_particle_energy_states(
        hamiltonian, n_electrons)
    occupied_states = numpy.array(occupied_states)

    if plane_wave:
        # In the plane wave basis the HF state is a single determinant.
        hartree_fock_state_index = numpy.sum(2**occupied_states)
        hartree_fock_state = numpy.zeros(2**count_qubits(hamiltonian),
                                         dtype=complex)
        hartree_fock_state[hartree_fock_state_index] = 1.0

    else:
        # Inverse Fourier transform the creation operators for the state to get
        # to the dual basis state, then use that to get the dual basis state.
        hartree_fock_state_creation_operator = FermionOperator.identity()
        for state in occupied_states[::-1]:
            hartree_fock_state_creation_operator *= (FermionOperator(
                ((int(state), 1),)))
        dual_basis_hf_creation_operator = inverse_fourier_transform(
            hartree_fock_state_creation_operator, grid, spinless)

        dual_basis_hf_creation = normal_ordered(dual_basis_hf_creation_operator)

        # Initialize the HF state.
        hartree_fock_state = numpy.zeros(2**count_qubits(hamiltonian),
                                         dtype=complex)

        # Populate the elements of the HF state in the dual basis.
        for term in dual_basis_hf_creation.terms:
            index = 0
            for operator in term:
                index += 2**operator[0]
            hartree_fock_state[index] = dual_basis_hf_creation.terms[term]

    return hartree_fock_state","for operator in term:
    index += 2 ** operator[0]","for operator in term:
    (operator_0, *operator_roperatormaining) = operator
    index += 2 ** operator[0]","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
microk8s,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/microk8s/scripts/wrappers/status.py,https://github.com/ubuntu/microk8s/tree/master/scripts/wrappers/status.py,,print_pretty$33,"def print_pretty(isReady, enabled_addons, disabled_addons):
    console_formatter = ""{:>3} {:<20} # ({}) {}""
    if isReady:
        print(""microk8s is running"")
        if not is_ha_enabled():
            print(""high-availability: no"")
        else:
            info = get_dqlite_info()
            if ha_cluster_formed(info):
                print(""high-availability: yes"")
            else:
                print(""high-availability: no"")

            masters = ""none""
            standby = ""none""
            for node in info:
                if node[1] == ""voter"":
                    if masters == ""none"":
                        masters = ""{}"".format(node[0])
                    else:
                        masters = ""{} {}"".format(masters, node[0])
                if node[1] == ""standby"":
                    if standby == ""none"":
                        standby = ""{}"".format(node[0])
                    else:
                        standby = ""{} {}"".format(standby, node[0])

            print(""{:>2}{} {}"".format("""", ""datastore master nodes:"", masters))
            print(""{:>2}{} {}"".format("""", ""datastore standby nodes:"", standby))

        print(""addons:"")
        if enabled_addons and len(enabled_addons) > 0:
            print(""{:>2}{}"".format("""", ""enabled:""))
            for enabled in enabled_addons:
                print(
                    console_formatter.format(
                        """", enabled[""name""], enabled[""repository""], enabled[""description""]
                    )
                )
        if disabled_addons and len(disabled_addons) > 0:
            print(""{:>2}{}"".format("""", ""disabled:""))
            for disabled in disabled_addons:
                print(
                    console_formatter.format(
                        """", disabled[""name""], disabled[""repository""], disabled[""description""]
                    )
                )
    else:
        print(""microk8s is not running. Use microk8s inspect for a deeper inspection."")","for node in info:
    if node[1] == 'voter':
        if masters == 'none':
            masters = '{}'.format(node[0])
        else:
            masters = '{} {}'.format(masters, node[0])
    if node[1] == 'standby':
        if standby == 'none':
            standby = '{}'.format(node[0])
        else:
            standby = '{} {}'.format(standby, node[0])","for node in info:
    (node_0, node_1, *_) = node
    if node[1] == 'voter':
        if masters == 'none':
            masters = '{}'.format(node[0])
        else:
            masters = '{} {}'.format(masters, node[0])
    if node[1] == 'standby':
        if standby == 'none':
            standby = '{}'.format(node[0])
        else:
            standby = '{} {}'.format(standby, node[0])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
microk8s,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/microk8s/scripts/wrappers/status.py,https://github.com/ubuntu/microk8s/tree/master/scripts/wrappers/status.py,,print_pretty$33,"def print_pretty(isReady, enabled_addons, disabled_addons):
    console_formatter = ""{:>3} {:<20} # ({}) {}""
    if isReady:
        print(""microk8s is running"")
        if not is_ha_enabled():
            print(""high-availability: no"")
        else:
            info = get_dqlite_info()
            if ha_cluster_formed(info):
                print(""high-availability: yes"")
            else:
                print(""high-availability: no"")

            masters = ""none""
            standby = ""none""
            for node in info:
                if node[1] == ""voter"":
                    if masters == ""none"":
                        masters = ""{}"".format(node[0])
                    else:
                        masters = ""{} {}"".format(masters, node[0])
                if node[1] == ""standby"":
                    if standby == ""none"":
                        standby = ""{}"".format(node[0])
                    else:
                        standby = ""{} {}"".format(standby, node[0])

            print(""{:>2}{} {}"".format("""", ""datastore master nodes:"", masters))
            print(""{:>2}{} {}"".format("""", ""datastore standby nodes:"", standby))

        print(""addons:"")
        if enabled_addons and len(enabled_addons) > 0:
            print(""{:>2}{}"".format("""", ""enabled:""))
            for enabled in enabled_addons:
                print(
                    console_formatter.format(
                        """", enabled[""name""], enabled[""repository""], enabled[""description""]
                    )
                )
        if disabled_addons and len(disabled_addons) > 0:
            print(""{:>2}{}"".format("""", ""disabled:""))
            for disabled in disabled_addons:
                print(
                    console_formatter.format(
                        """", disabled[""name""], disabled[""repository""], disabled[""description""]
                    )
                )
    else:
        print(""microk8s is not running. Use microk8s inspect for a deeper inspection."")","for enabled in enabled_addons:
    print(console_formatter.format('', enabled['name'], enabled['repository'], enabled['description']))",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_description, e_name, e_repository = e['description'], e['name'], e['repository']
variable mapping:
e_description: e['description']
e_name: e['name']
e_repository: e['repository']",,,,,,,
microk8s,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/microk8s/scripts/wrappers/status.py,https://github.com/ubuntu/microk8s/tree/master/scripts/wrappers/status.py,,print_pretty$33,"def print_pretty(isReady, enabled_addons, disabled_addons):
    console_formatter = ""{:>3} {:<20} # ({}) {}""
    if isReady:
        print(""microk8s is running"")
        if not is_ha_enabled():
            print(""high-availability: no"")
        else:
            info = get_dqlite_info()
            if ha_cluster_formed(info):
                print(""high-availability: yes"")
            else:
                print(""high-availability: no"")

            masters = ""none""
            standby = ""none""
            for node in info:
                if node[1] == ""voter"":
                    if masters == ""none"":
                        masters = ""{}"".format(node[0])
                    else:
                        masters = ""{} {}"".format(masters, node[0])
                if node[1] == ""standby"":
                    if standby == ""none"":
                        standby = ""{}"".format(node[0])
                    else:
                        standby = ""{} {}"".format(standby, node[0])

            print(""{:>2}{} {}"".format("""", ""datastore master nodes:"", masters))
            print(""{:>2}{} {}"".format("""", ""datastore standby nodes:"", standby))

        print(""addons:"")
        if enabled_addons and len(enabled_addons) > 0:
            print(""{:>2}{}"".format("""", ""enabled:""))
            for enabled in enabled_addons:
                print(
                    console_formatter.format(
                        """", enabled[""name""], enabled[""repository""], enabled[""description""]
                    )
                )
        if disabled_addons and len(disabled_addons) > 0:
            print(""{:>2}{}"".format("""", ""disabled:""))
            for disabled in disabled_addons:
                print(
                    console_formatter.format(
                        """", disabled[""name""], disabled[""repository""], disabled[""description""]
                    )
                )
    else:
        print(""microk8s is not running. Use microk8s inspect for a deeper inspection."")","for disabled in disabled_addons:
    print(console_formatter.format('', disabled['name'], disabled['repository'], disabled['description']))",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_description, e_name, e_repository = e['description'], e['name'], e['repository']
variable mapping:
e_description: e['description']
e_name: e['name']
e_repository: e['repository']",,,,,,,
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_es.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_es.py,Num2WordsESTest,test_currency_zar$2028,"def test_currency_zar(self):
        for test in TEST_CASES_TO_CURRENCY_ZAR:
            self.assertEqual(
                num2words(test[0], lang='es', to='currency', currency='ZAR'),
                test[1]
            )","for test in TEST_CASES_TO_CURRENCY_ZAR:
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='ZAR'), test[1])","for test in TEST_CASES_TO_CURRENCY_ZAR:
    (test_0, test_1, *_) = test
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='ZAR'), test[1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/tests/unit/lib/test_get_client.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/tests/unit/lib/test_get_client.py,TestGetClientCase,test_get_client_from_request_by_forwarded$200,"def test_get_client_from_request_by_forwarded(self):
        """"""
         according to the spec the old expression is the same as the
         new one:

             X-Forwarded-For: 192.0.2.43, 2001:db8:cafe::17
        becomes:
            Forwarded: for=192.0.2.43, for=""[2001:db8:cafe::17]

        """"""
        global LinConfig

        forward_test_strings = [
            (
                'for=192.0.2.43,for=""[2001:db8:cafe::17]"",for=unknown',
                ""192.0.2.43"",
            ),
            ('for=""_gazonk""', ""_gazonk""),
            ('for=""_gazonk:800""', ""_gazonk""),
            ('For=""[2001:db8:cafe::17]:4711""', ""2001:db8:cafe::17""),
            (""for=192.0.2.60;proto=http;by=203.0.113.43"", ""192.0.2.60""),
            (""for=192.0.2.43, for=198.51.100.17"", ""192.0.2.43""),
        ]

        LinConfig = {
            ""client.FORWARDED"": ""true"",
            ""client.FORWARDED_PROXY"": ""121.121.121.121, 123.234.123.234"",
        }

        environ = {
            ""REMOTE_ADDR"": ""123.234.123.234"",  # the last requester, the proxy
        }

        for forward_test_string in forward_test_strings:

            environ[""Forwarded""] = forward_test_string[0]

            request = Request(environ)
            client = _get_client_from_request(request)

            assert client == forward_test_string[1], client","for forward_test_string in forward_test_strings:
    environ['Forwarded'] = forward_test_string[0]
    request = Request(environ)
    client = _get_client_from_request(request)
    assert client == forward_test_string[1], client","for forward_test_string in forward_test_strings:
    (forward_test_string_0, forward_test_string_1, *_) = forward_test_string
    environ['Forwarded'] = forward_test_string[0]
    request = Request(environ)
    client = _get_client_from_request(request)
    assert client == forward_test_string[1], client","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
spaCy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spaCy/spacy/tests/pipeline/test_tok2vec.py,https://github.com/explosion/spaCy/tree/master/spacy/tests/pipeline/test_tok2vec.py,,test_tok2vec_listener$143,"def test_tok2vec_listener():
    orig_config = Config().from_str(cfg_string)
    nlp = util.load_model_from_config(orig_config, auto_fill=True, validate=True)
    assert nlp.pipe_names == [""tok2vec"", ""tagger""]
    tagger = nlp.get_pipe(""tagger"")
    tok2vec = nlp.get_pipe(""tok2vec"")
    tagger_tok2vec = tagger.model.get_ref(""tok2vec"")
    assert isinstance(tok2vec, Tok2Vec)
    assert isinstance(tagger_tok2vec, Tok2VecListener)
    train_examples = []
    for t in TRAIN_DATA:
        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
        for tag in t[1][""tags""]:
            tagger.add_label(tag)

    # Check that the Tok2Vec component finds it listeners
    assert tok2vec.listeners == []
    optimizer = nlp.initialize(lambda: train_examples)
    assert tok2vec.listeners == [tagger_tok2vec]

    for i in range(5):
        losses = {}
        nlp.update(train_examples, sgd=optimizer, losses=losses)

    doc = nlp(""Running the pipeline as a whole."")
    doc_tensor = tagger_tok2vec.predict([doc])[0]
    ops = get_current_ops()
    assert_array_equal(ops.to_numpy(doc.tensor), ops.to_numpy(doc_tensor))

    # TODO: should this warn or error?
    nlp.select_pipes(disable=""tok2vec"")
    assert nlp.pipe_names == [""tagger""]
    nlp(""Running the pipeline with the Tok2Vec component disabled."")","for t in TRAIN_DATA:
    train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
    for tag in t[1]['tags']:
        tagger.add_label(tag)","for t in TRAIN_DATA:
    (t_0, t_1, _, _, _, t_1_tags, *t_rtmaining) = t
    train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
    for tag in t[1]['tags']:
        tagger.add_label(tag)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",0,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, _, _, _, e_1_eags, *e_remaining = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_1_eags: e[1]['eags']",,,,,,,
dnsrecon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dnsrecon/tools/parser.py,https://github.com/darkoperator/dnsrecon/tree/master/tools/parser.py,,extract_hostnames$177,"def extract_hostnames(file):
    host_names = []
    hostname_pattern = re.compile(""(^[^.]*)"")
    file_type = detect_type(file)
    if file_type == ""xml"":
        for event, elem in cElementTree.iterparse(file):
            # Check if it is a record
            if elem.tag == ""record"":
                # Check that it is a RR Type that has an IP Address
                if ""address"" in elem.attrib:
                    # Process A, AAAA and PTR Records
                    if re.search(r'PTR|^[A]$|AAAA', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['name']).group(1))

                    # Process NS Records
                    elif re.search(r'NS', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['target']).group(1))

                    # Process SOA Records
                    elif re.search(r'SOA', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['mname']).group(1))

                    # Process MX Records
                    elif re.search(r'MX', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['exchange']).group(1))

                    # Process SRV Records
                    elif re.search(r'SRV', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['target']).group(1))

    elif file_type == ""csv"":
        reader = csv.reader(open(file, 'r'), delimiter=',')
        reader.next()
        for row in reader:
            host_names.append(re.search(hostname_pattern, row[1]).group(1))

    host_names = list(set(host_names))
    # Return list with no empty values
    return filter(None, host_names)","for row in reader:
    host_names.append(re.search(hostname_pattern, row[1]).group(1))","for row in reader:
    (row_0, row_1, *row_rrowmaining) = row
    host_names.append(re.search(hostname_pattern, row[1]).group(1))","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/pedestrian_detection/data_provider_farm/reformat_caltech.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/pedestrian_detection/data_provider_farm/reformat_caltech.py,,generate_data_list$8,"def generate_data_list():
    annotation_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_new_annotations/anno_test_1xnew'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_data/extracted_data'

    list_file_path = './data_folder/data_list_caltech_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fout = open(list_file_path, 'w')

    counter = 0
    for parent, dirnames, filenames in os.walk(annotation_root):
        for filename in filenames:
            if not filename.endswith('.txt'):
                continue

            filename_splits = filename[:-4].split('_')
            set_name = filename_splits[0]
            seq_name = filename_splits[1]
            img_name = filename_splits[2]

            img_path = os.path.join(image_root, set_name, seq_name, 'images', img_name)
            if not os.path.exists(img_path):
                print('The corresponding image does not exist! [%s]' % img_path)
                continue

            line = img_path

            fin_anno = open(os.path.join(parent, filename), 'r')

            bbox_list = []
            for i, anno in enumerate(fin_anno):
                if i == 0:
                    continue
                anno = anno.strip('\n').split(' ')
                if anno[0] != 'person':
                    continue
                x = math.floor(float(anno[1]))
                y = math.floor(float(anno[2]))
                width = math.ceil(float(anno[3]))
                height = math.ceil(float(anno[4]))

                width_vis = math.ceil(float(anno[8]))
                height_vis = math.ceil(float(anno[9]))

                if (width_vis*height_vis)/(width*height) < 0.2:
                    continue

                bbox_list.append((x, y, width, height))
            if len(bbox_list) == 0:
                line += ',0,0'
                fout.write(line + '\n')
            else:
                bbox_line = ''
                for bbox in bbox_list:
                    bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
                line += ',1,' + str(len(bbox_list)) + bbox_line
                fout.write(line + '\n')
            counter += 1
            print(counter)

    fout.close()","for filename in filenames:
    if not filename.endswith('.txt'):
        continue
    filename_splits = filename[:-4].split('_')
    set_name = filename_splits[0]
    seq_name = filename_splits[1]
    img_name = filename_splits[2]
    img_path = os.path.join(image_root, set_name, seq_name, 'images', img_name)
    if not os.path.exists(img_path):
        print('The corresponding image does not exist! [%s]' % img_path)
        continue
    line = img_path
    fin_anno = open(os.path.join(parent, filename), 'r')
    bbox_list = []
    for (i, anno) in enumerate(fin_anno):
        if i == 0:
            continue
        anno = anno.strip('\n').split(' ')
        if anno[0] != 'person':
            continue
        x = math.floor(float(anno[1]))
        y = math.floor(float(anno[2]))
        width = math.ceil(float(anno[3]))
        height = math.ceil(float(anno[4]))
        width_vis = math.ceil(float(anno[8]))
        height_vis = math.ceil(float(anno[9]))
        if width_vis * height_vis / (width * height) < 0.2:
            continue
        bbox_list.append((x, y, width, height))
    if len(bbox_list) == 0:
        line += ',0,0'
        fout.write(line + '\n')
    else:
        bbox_line = ''
        for bbox in bbox_list:
            bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
        line += ',1,' + str(len(bbox_list)) + bbox_line
        fout.write(line + '\n')
    counter += 1
    print(counter)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_unpacked, *_ = e[:-4]
variable mapping:
e_unpacked: e[:-4]
Explanation: The unpacked elements e[:-4] represent a slice of the iterable object ""e"" that can be accessed using the slice operator. Therefore, iterable unpacking is applicable in this case and we can use the slice operator to get the unpacked elements.",,,,,,,
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/pedestrian_detection/data_provider_farm/reformat_caltech.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/pedestrian_detection/data_provider_farm/reformat_caltech.py,,generate_data_list$8,"def generate_data_list():
    annotation_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_new_annotations/anno_test_1xnew'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_data/extracted_data'

    list_file_path = './data_folder/data_list_caltech_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fout = open(list_file_path, 'w')

    counter = 0
    for parent, dirnames, filenames in os.walk(annotation_root):
        for filename in filenames:
            if not filename.endswith('.txt'):
                continue

            filename_splits = filename[:-4].split('_')
            set_name = filename_splits[0]
            seq_name = filename_splits[1]
            img_name = filename_splits[2]

            img_path = os.path.join(image_root, set_name, seq_name, 'images', img_name)
            if not os.path.exists(img_path):
                print('The corresponding image does not exist! [%s]' % img_path)
                continue

            line = img_path

            fin_anno = open(os.path.join(parent, filename), 'r')

            bbox_list = []
            for i, anno in enumerate(fin_anno):
                if i == 0:
                    continue
                anno = anno.strip('\n').split(' ')
                if anno[0] != 'person':
                    continue
                x = math.floor(float(anno[1]))
                y = math.floor(float(anno[2]))
                width = math.ceil(float(anno[3]))
                height = math.ceil(float(anno[4]))

                width_vis = math.ceil(float(anno[8]))
                height_vis = math.ceil(float(anno[9]))

                if (width_vis*height_vis)/(width*height) < 0.2:
                    continue

                bbox_list.append((x, y, width, height))
            if len(bbox_list) == 0:
                line += ',0,0'
                fout.write(line + '\n')
            else:
                bbox_line = ''
                for bbox in bbox_list:
                    bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
                line += ',1,' + str(len(bbox_list)) + bbox_line
                fout.write(line + '\n')
            counter += 1
            print(counter)

    fout.close()","for (i, anno) in enumerate(fin_anno):
    if i == 0:
        continue
    anno = anno.strip('\n').split(' ')
    if anno[0] != 'person':
        continue
    x = math.floor(float(anno[1]))
    y = math.floor(float(anno[2]))
    width = math.ceil(float(anno[3]))
    height = math.ceil(float(anno[4]))
    width_vis = math.ceil(float(anno[8]))
    height_vis = math.ceil(float(anno[9]))
    if width_vis * height_vis / (width * height) < 0.2:
        continue
    bbox_list.append((x, y, width, height))","for (i, anno) in enumerate(fin_anno):
    (anno_0, anno_1, anno_2, anno_3, anno_4, *_, anno_8, anno_9) = anno
    if i == 0:
        continue
    anno = anno.strip('\n').split(' ')
    if anno[0] != 'person':
        continue
    x = math.floor(float(anno[1]))
    y = math.floor(float(anno[2]))
    width = math.ceil(float(anno[3]))
    height = math.ceil(float(anno[4]))
    width_vis = math.ceil(float(anno[8]))
    height_vis = math.ceil(float(anno[9]))
    if width_vis * height_vis / (width * height) < 0.2:
        continue
    bbox_list.append((x, y, width, height))",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4, *_, e_8, e_9 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]
e_8: e[8]
e_9: e[9]",,,,,,,
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/pedestrian_detection/data_provider_farm/reformat_caltech.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/pedestrian_detection/data_provider_farm/reformat_caltech.py,,generate_data_list$8,"def generate_data_list():
    annotation_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_new_annotations/anno_test_1xnew'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_data/extracted_data'

    list_file_path = './data_folder/data_list_caltech_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fout = open(list_file_path, 'w')

    counter = 0
    for parent, dirnames, filenames in os.walk(annotation_root):
        for filename in filenames:
            if not filename.endswith('.txt'):
                continue

            filename_splits = filename[:-4].split('_')
            set_name = filename_splits[0]
            seq_name = filename_splits[1]
            img_name = filename_splits[2]

            img_path = os.path.join(image_root, set_name, seq_name, 'images', img_name)
            if not os.path.exists(img_path):
                print('The corresponding image does not exist! [%s]' % img_path)
                continue

            line = img_path

            fin_anno = open(os.path.join(parent, filename), 'r')

            bbox_list = []
            for i, anno in enumerate(fin_anno):
                if i == 0:
                    continue
                anno = anno.strip('\n').split(' ')
                if anno[0] != 'person':
                    continue
                x = math.floor(float(anno[1]))
                y = math.floor(float(anno[2]))
                width = math.ceil(float(anno[3]))
                height = math.ceil(float(anno[4]))

                width_vis = math.ceil(float(anno[8]))
                height_vis = math.ceil(float(anno[9]))

                if (width_vis*height_vis)/(width*height) < 0.2:
                    continue

                bbox_list.append((x, y, width, height))
            if len(bbox_list) == 0:
                line += ',0,0'
                fout.write(line + '\n')
            else:
                bbox_line = ''
                for bbox in bbox_list:
                    bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
                line += ',1,' + str(len(bbox_list)) + bbox_line
                fout.write(line + '\n')
            counter += 1
            print(counter)

    fout.close()","for bbox in bbox_list:
    bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])","for bbox in bbox_list:
    (bbox_0, bbox_1, bbox_2, bbox_3, *_) = bbox
    bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/database/database.py,https://github.com/frappe/frappe/tree/master/frappe/database/database.py,Database,commit$915,"def commit(self):
		""""""Commit current transaction. Calls SQL `COMMIT`.""""""
		for method in frappe.local.before_commit:
			frappe.call(method[0], *(method[1] or []), **(method[2] or {}))

		self.sql(""commit"")
		self.begin()  # explicitly start a new transaction

		frappe.local.rollback_observers = []
		self.flush_realtime_log()
		enqueue_jobs_after_commit()
		flush_local_link_count()","for method in frappe.local.before_commit:
    frappe.call(method[0], *(method[1] or []), **method[2] or {})","for method in frappe.local.before_commit:
    (method_0, method_1, method_2, *_) = method
    frappe.call(method[0], *(method[1] or []), **method[2] or {})","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
azure-devops-cli-extension,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-devops-cli-extension/scripts/findEmptyHelpTexts.py,https://github.com/Azure/azure-devops-cli-extension/tree/master/scripts/findEmptyHelpTexts.py,,print_missing_help_files$63,"def print_missing_help_files(help_files):
    missing_help_text = False
    missing_help_list = []
    for help_file in help_files:
        if help_file.short_summary == '' and help_file.command != '':
            is_command = isinstance(help_file, CliCommandHelpFile)
            command_type = 'command' if is_command else 'group'
            missing_help_list.append((command_type, help_file.command))
            missing_help_text = True
    if not missing_help_text:
        print('No missing help texts found.')
    else:
        print('\n\nNo help texts were found for below command(s):')
        for text in missing_help_list :
            print('{} : {}'.format(text[0], text[1]))
        raise Exception('Please update the help text(s).')","for text in missing_help_list:
    print('{} : {}'.format(text[0], text[1]))","for text in missing_help_list:
    (text_0, text_1, *text_rtextmaining) = text
    print('{} : {}'.format(text[0], text[1]))","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
AIDungeon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AIDungeon/story/utils.py,https://github.com/Latitude-Archives/AIDungeon/tree/master/story/utils.py,,first_to_second_person$273,"def first_to_second_person(text):
    text = "" "" + text
    text = standardize_punctuation(text)
    for pair in first_to_second_mappings:
        variations = mapping_variation_pairs(pair)
        for variation in variations:
            text = replace_outside_quotes(text, variation[0], variation[1])

    return capitalize_first_letters(text[1:])","for variation in variations:
    text = replace_outside_quotes(text, variation[0], variation[1])","for variation in variations:
    (variation_0, variation_1, *_) = variation
    text = replace_outside_quotes(text, variation[0], variation[1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Mycodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/api/measurement.py,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/api/measurement.py,MeasurementsPast,get$232,"def get(self, unique_id, unit, channel, past_seconds):
        """"""
        Return a list of measurements found within a duration from the past to the present
        """"""
        if not utils_general.user_has_permission('view_settings'):
            abort(403)

        if unit not in add_custom_units(Unit.query.all()):
            abort(422, custom='Unit ID not found')
        if channel < 0:
            abort(422, custom='channel must be >= 0')
        if past_seconds < 1:
            abort(422, custom='past_seconds must be >= 1')

        try:
            return_ = read_influxdb_list(
                unique_id, unit, channel, duration_sec=past_seconds)
            if return_ and len(return_) > 0:
                dict_return = {'measurements': []}
                for each_set in return_:
                    dict_return['measurements'].append(
                        {'time': each_set[0], 'value': each_set[1]})
                return dict_return, 200
            else:
                return return_, 200
        except Exception:
            abort(500,
                  message='An exception occurred',
                  error=traceback.format_exc())","for each_set in return_:
    dict_return['measurements'].append({'time': each_set[0], 'value': each_set[1]})","for each_set in return_:
    (each_set_0, each_set_1, *_) = each_set
    dict_return['measurements'].append({'time': each_set[0], 'value': each_set[1]})","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
OnlyFans,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OnlyFans/modules/onlyfans.py,https://github.com/DIGITALCRIMINAL/OnlyFans/tree/master/modules/onlyfans.py,,scrape_choice$166,"def scrape_choice(authed: create_auth, subscription):
    user_id = subscription.id
    post_count = subscription.postsCount
    archived_count = subscription.archivedPostsCount
    message = ""Scrape: 0 = All | 1 = Images | 2 = Videos | 3 = Audios | 4 = Texts""
    media_types = [
        [["""", ""All""], ["""", ""Images""], ["""", ""Videos""], ["""", ""Audios""], ["""", ""Texts""]],
        message,
    ]
    choice_list = main_helper.choose_option(media_types, auto_media_choice)
    user_api = OnlyFans.endpoint_links(user_id).users
    message_api = OnlyFans.endpoint_links(user_id).message_api
    # mass_messages_api = OnlyFans.endpoint_links().mass_messages_api
    stories_api = OnlyFans.endpoint_links(user_id).stories_api
    list_highlights = OnlyFans.endpoint_links(user_id).list_highlights
    post_api = OnlyFans.endpoint_links(user_id).post_api
    archived_api = OnlyFans.endpoint_links(user_id).archived_posts
    # ARGUMENTS
    only_links = False
    mandatory = [download_directory, only_links]
    y = [""photo"", ""video"", ""stream"", ""gif"", ""audio"", ""text""]
    u_array = [
        ""You have chosen to scrape {}"",
        [user_api, media_types, *mandatory, post_count],
        ""Profile"",
    ]
    s_array = [
        ""You have chosen to scrape {}"",
        [stories_api, media_types, *mandatory, post_count],
        ""Stories"",
    ]
    h_array = [
        ""You have chosen to scrape {}"",
        [list_highlights, media_types, *mandatory, post_count],
        ""Highlights"",
    ]
    p_array = [
        ""You have chosen to scrape {}"",
        [post_api, media_types, *mandatory, post_count],
        ""Posts"",
    ]
    m_array = [
        ""You have chosen to scrape {}"",
        [message_api, media_types, *mandatory, post_count],
        ""Messages"",
    ]
    a_array = [
        ""You have chosen to scrape {}"",
        [archived_api, media_types, *mandatory, archived_count],
        ""Archived"",
    ]
    array = [u_array, s_array, p_array, a_array, m_array]
    # array = [u_array, s_array, p_array, a_array, m_array]
    # array = [s_array, h_array, p_array, a_array, m_array]
    # array = [s_array]
    # array = [u_array]
    # array = [p_array]
    # array = [a_array]
    # array = [m_array]
    new_array = []
    valid_input = True
    for xxx in array:
        if xxx[2] == ""Mass Messages"":
            if not subscription.is_me():
                continue
        new_item = dict()
        new_item[""api_message""] = xxx[0]
        new_item[""api_array""] = {}
        new_item[""api_array""][""api_link""] = xxx[1][0]
        new_item[""api_array""][""media_types""] = xxx[1][1]
        new_item[""api_array""][""directory""] = xxx[1][2]
        new_item[""api_array""][""only_links""] = xxx[1][3]
        new_item[""api_array""][""post_count""] = xxx[1][4]
        formatted = format_media_types()
        final_format = []
        for choice in choice_list:
            choice = choice[1]
            final_format.extend([result for result in formatted if result[0] == choice])
        new_item[""api_array""][""media_types""] = final_format
        new_item[""api_type""] = xxx[2]
        if valid_input:
            new_array.append(new_item)
    return new_array","for xxx in array:
    if xxx[2] == 'Mass Messages':
        if not subscription.is_me():
            continue
    new_item = dict()
    new_item['api_message'] = xxx[0]
    new_item['api_array'] = {}
    new_item['api_array']['api_link'] = xxx[1][0]
    new_item['api_array']['media_types'] = xxx[1][1]
    new_item['api_array']['directory'] = xxx[1][2]
    new_item['api_array']['only_links'] = xxx[1][3]
    new_item['api_array']['post_count'] = xxx[1][4]
    formatted = format_media_types()
    final_format = []
    for choice in choice_list:
        choice = choice[1]
        final_format.extend([result for result in formatted if result[0] == choice])
    new_item['api_array']['media_types'] = final_format
    new_item['api_type'] = xxx[2]
    if valid_input:
        new_array.append(new_item)","for xxx in array:
    (xxx_0, (xxx_1_0, xxx_1_1, xxx_1_2, xxx_1_3, xxx_1_4), xxx_2, *xxx_rxxxmaining) = xxx
    if xxx[2] == 'Mass Messages':
        if not subscription.is_me():
            continue
    new_item = dict()
    new_item['api_message'] = xxx[0]
    new_item['api_array'] = {}
    new_item['api_array']['api_link'] = xxx[1][0]
    new_item['api_array']['media_types'] = xxx[1][1]
    new_item['api_array']['directory'] = xxx[1][2]
    new_item['api_array']['only_links'] = xxx[1][3]
    new_item['api_array']['post_count'] = xxx[1][4]
    formatted = format_media_types()
    final_format = []
    for choice in choice_list:
        choice = choice[1]
        final_format.extend([result for result in formatted if result[0] == choice])
    new_item['api_array']['media_types'] = final_format
    new_item['api_type'] = xxx[2]
    if valid_input:
        new_array.append(new_item)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",0,,,"Answer: Yes
Iterable Unpacking: e_0, (e_1_0, e_1_1, e_1_2, e_1_3, e_1_4), e_2, *e_remaining = e
variable mapping:
e_0: e[0]
e_1_0: e[1][0]
e_1_1: e[1][1]
e_1_2: e[1][2]
e_1_3: e[1][3]
e_1_4: e[1][4]
e_2: e[2]",,,,,,,
OnlyFans,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OnlyFans/modules/onlyfans.py,https://github.com/DIGITALCRIMINAL/OnlyFans/tree/master/modules/onlyfans.py,,scrape_choice$166,"def scrape_choice(authed: create_auth, subscription):
    user_id = subscription.id
    post_count = subscription.postsCount
    archived_count = subscription.archivedPostsCount
    message = ""Scrape: 0 = All | 1 = Images | 2 = Videos | 3 = Audios | 4 = Texts""
    media_types = [
        [["""", ""All""], ["""", ""Images""], ["""", ""Videos""], ["""", ""Audios""], ["""", ""Texts""]],
        message,
    ]
    choice_list = main_helper.choose_option(media_types, auto_media_choice)
    user_api = OnlyFans.endpoint_links(user_id).users
    message_api = OnlyFans.endpoint_links(user_id).message_api
    # mass_messages_api = OnlyFans.endpoint_links().mass_messages_api
    stories_api = OnlyFans.endpoint_links(user_id).stories_api
    list_highlights = OnlyFans.endpoint_links(user_id).list_highlights
    post_api = OnlyFans.endpoint_links(user_id).post_api
    archived_api = OnlyFans.endpoint_links(user_id).archived_posts
    # ARGUMENTS
    only_links = False
    mandatory = [download_directory, only_links]
    y = [""photo"", ""video"", ""stream"", ""gif"", ""audio"", ""text""]
    u_array = [
        ""You have chosen to scrape {}"",
        [user_api, media_types, *mandatory, post_count],
        ""Profile"",
    ]
    s_array = [
        ""You have chosen to scrape {}"",
        [stories_api, media_types, *mandatory, post_count],
        ""Stories"",
    ]
    h_array = [
        ""You have chosen to scrape {}"",
        [list_highlights, media_types, *mandatory, post_count],
        ""Highlights"",
    ]
    p_array = [
        ""You have chosen to scrape {}"",
        [post_api, media_types, *mandatory, post_count],
        ""Posts"",
    ]
    m_array = [
        ""You have chosen to scrape {}"",
        [message_api, media_types, *mandatory, post_count],
        ""Messages"",
    ]
    a_array = [
        ""You have chosen to scrape {}"",
        [archived_api, media_types, *mandatory, archived_count],
        ""Archived"",
    ]
    array = [u_array, s_array, p_array, a_array, m_array]
    # array = [u_array, s_array, p_array, a_array, m_array]
    # array = [s_array, h_array, p_array, a_array, m_array]
    # array = [s_array]
    # array = [u_array]
    # array = [p_array]
    # array = [a_array]
    # array = [m_array]
    new_array = []
    valid_input = True
    for xxx in array:
        if xxx[2] == ""Mass Messages"":
            if not subscription.is_me():
                continue
        new_item = dict()
        new_item[""api_message""] = xxx[0]
        new_item[""api_array""] = {}
        new_item[""api_array""][""api_link""] = xxx[1][0]
        new_item[""api_array""][""media_types""] = xxx[1][1]
        new_item[""api_array""][""directory""] = xxx[1][2]
        new_item[""api_array""][""only_links""] = xxx[1][3]
        new_item[""api_array""][""post_count""] = xxx[1][4]
        formatted = format_media_types()
        final_format = []
        for choice in choice_list:
            choice = choice[1]
            final_format.extend([result for result in formatted if result[0] == choice])
        new_item[""api_array""][""media_types""] = final_format
        new_item[""api_type""] = xxx[2]
        if valid_input:
            new_array.append(new_item)
    return new_array","for choice in choice_list:
    choice = choice[1]
    final_format.extend([result for result in formatted if result[0] == choice])","for choice in choice_list:
    (choice_0, choice_1, *choice_rchoicemaining) = choice
    choice = choice[1]
    final_format.extend([result for result in formatted if result[0] == choice])",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
dl-4-tsc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dl-4-tsc/classifiers/mcnn.py,https://github.com/hfawaz/dl-4-tsc/tree/master/classifiers/mcnn.py,Classifier_MCNN,split_input_for_model$410,"def split_input_for_model(self, x, input_shapes):
        res = []
        indx = 0 
        for input_shape in input_shapes:
            res.append(x[:,indx:indx+input_shape[0],:])
            indx = indx + input_shape[0]
        return res","for input_shape in input_shapes:
    res.append(x[:, indx:indx + input_shape[0], :])
    indx = indx + input_shape[0]","for input_shape in input_shapes:
    (input_shape_0, *input_shape_rinput_shapemaining) = input_shape
    res.append(x[:, indx:indx + input_shape[0], :])
    indx = indx + input_shape[0]","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/packages/app/galaxy/queue_worker.py,https://github.com/ansible/galaxy/tree/master/packages/app/galaxy/queue_worker.py,,__job_rule_module_names$279,"def __job_rule_module_names(app):
    rules_module_names = {'galaxy.jobs.rules'}
    if app.job_config.dynamic_params is not None:
        module_name = app.job_config.dynamic_params.get('rules_module')
        if module_name:
            rules_module_names.add(module_name)
    # Also look for destination level rules_module overrides
    for dest_tuple in app.job_config.destinations.values():
        module_name = dest_tuple[0].params.get('rules_module')
        if module_name:
            rules_module_names.add(module_name)
    return rules_module_names","for dest_tuple in app.job_config.destinations.values():
    module_name = dest_tuple[0].params.get('rules_module')
    if module_name:
        rules_module_names.add(module_name)","for dest_tuple in app.job_config.destinations.values():
    (dest_tuple_0, *dest_tuple_rdest_tuplemaining) = dest_tuple
    module_name = dest_tuple[0].params.get('rules_module')
    if module_name:
        rules_module_names.add(module_name)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
TauonMusicBox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TauonMusicBox/t_modules/t_main.py,https://github.com/Taiko2k/TauonMusicBox/tree/master/t_modules/t_main.py,,update_set$2149,"def update_set():  # This is used to scale columns when windows is resized or items added/removed

    wid = gui.plw - round(16 * gui.scale)
    if gui.tracklist_center_mode:
        wid = gui.tracklist_highlight_width - round(16 * gui.scale)

    total = 0
    for item in gui.pl_st:
        if item[2] is False:
            total += item[1]
        else:
            wid -= item[1]

    if wid <= 75:
        wid = 75

    for i in range(len(gui.pl_st)):
        if gui.pl_st[i][2] is False and total:
            gui.pl_st[i][1] = int(round((gui.pl_st[i][1] / total) * wid))","for item in gui.pl_st:
    if item[2] is False:
        total += item[1]
    else:
        wid -= item[1]","for item in gui.pl_st:
    (_, item_1, item_2, *item_ritemmaining) = item
    if item[2] is False:
        total += item[1]
    else:
        wid -= item[1]","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/gluoncv/auto/estimators/yolo/yolo.py,https://github.com/dmlc/gluon-cv/tree/master/gluoncv/auto/estimators/yolo/yolo.py,YOLOv3Estimator,_evaluate$241,"def _evaluate(self, val_data):
        """"""Evaluate the current model on dataset.""""""
        if not isinstance(val_data, gluon.data.DataLoader):
            if hasattr(val_data, 'to_mxnet'):
                val_data = val_data.to_mxnet()
            val_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))
            val_data = gluon.data.DataLoader(
                val_data.transform(YOLO3DefaultValTransform(self._cfg.yolo3.data_shape, self._cfg.yolo3.data_shape)),
                self._cfg.valid.batch_size, False, batchify_fn=val_batchify_fn, last_batch='keep',
                num_workers=self._cfg.num_workers)
        if self._cfg.valid.metric == 'voc07':
            eval_metric = VOC07MApMetric(iou_thresh=self._cfg.valid.iou_thresh, class_names=self.classes)
        elif self._cfg.valid.metric == 'voc':
            eval_metric = VOCMApMetric(iou_thresh=self._cfg.valid.iou_thresh, class_names=self.classes)
        else:
            raise ValueError(f'Invalid metric type: {self._cfg.valid.metric}')
        self.net.collect_params().reset_ctx(self.ctx)
        # set nms threshold and topk constraint
        self.net.set_nms(nms_thresh=self._cfg.yolo3.nms_thresh, nms_topk=self._cfg.yolo3.nms_topk)
        mx.nd.waitall()
        self.net.hybridize()
        for batch in val_data:
            val_ctx = self.ctx
            if batch[0].shape[0] < len(val_ctx):
                val_ctx = val_ctx[:batch[0].shape[0]]
            data = gluon.utils.split_and_load(batch[0], ctx_list=val_ctx, batch_axis=0, even_split=False)
            label = gluon.utils.split_and_load(batch[1], ctx_list=val_ctx, batch_axis=0, even_split=False)
            det_bboxes = []
            det_ids = []
            det_scores = []
            gt_bboxes = []
            gt_ids = []
            gt_difficults = []
            for x, y in zip(data, label):
                # get prediction results
                ids, scores, bboxes = self.net(x)
                det_ids.append(ids)
                det_scores.append(scores)
                # clip to image size
                det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
                # split ground truths
                gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
                gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
                gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)

            # update metric
            eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)
        return eval_metric.get()","for batch in val_data:
    val_ctx = self.ctx
    if batch[0].shape[0] < len(val_ctx):
        val_ctx = val_ctx[:batch[0].shape[0]]
    data = gluon.utils.split_and_load(batch[0], ctx_list=val_ctx, batch_axis=0, even_split=False)
    label = gluon.utils.split_and_load(batch[1], ctx_list=val_ctx, batch_axis=0, even_split=False)
    det_bboxes = []
    det_ids = []
    det_scores = []
    gt_bboxes = []
    gt_ids = []
    gt_difficults = []
    for (x, y) in zip(data, label):
        (ids, scores, bboxes) = self.net(x)
        det_ids.append(ids)
        det_scores.append(scores)
        det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)","for batch in val_data:
    (batch_0, batch_1, *_) = batch
    val_ctx = self.ctx
    if batch[0].shape[0] < len(val_ctx):
        val_ctx = val_ctx[:batch[0].shape[0]]
    data = gluon.utils.split_and_load(batch[0], ctx_list=val_ctx, batch_axis=0, even_split=False)
    label = gluon.utils.split_and_load(batch[1], ctx_list=val_ctx, batch_axis=0, even_split=False)
    det_bboxes = []
    det_ids = []
    det_scores = []
    gt_bboxes = []
    gt_ids = []
    gt_difficults = []
    for (x, y) in zip(data, label):
        (ids, scores, bboxes) = self.net(x)
        det_ids.append(ids)
        det_scores.append(scores)
        det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
seahub,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/seahub/seahub/api2/endpoints/group_members.py,https://github.com/haiwen/seahub/tree/master/seahub/api2/endpoints/group_members.py,GroupMembersImport,post$391,"def post(self, request, group_id):

        """""" Import members from xlsx file

        Permission checking:
        1. group admin or owner.
        """"""

        xlsx_file = request.FILES.get('file', None)
        if not xlsx_file:
            error_msg = 'file can not be found.'
            return api_error(status.HTTP_400_BAD_REQUEST, error_msg)

        file_type, ext = get_file_type_and_ext(xlsx_file.name)
        if ext != 'xlsx':
            error_msg = file_type_error_msg(ext, 'xlsx')
            return api_error(status.HTTP_400_BAD_REQUEST, error_msg)

        # recourse check
        group_id = int(group_id)
        group = ccnet_api.get_group(group_id)
        if not group:
            error_msg = _('Group does not exist')
            return api_error(status.HTTP_404_NOT_FOUND, error_msg)

        # check permission
        # only group owner/admin can add group members
        username = request.user.username
        if not is_group_admin_or_owner(group_id, username):
            error_msg = 'Permission denied.'
            return api_error(status.HTTP_403_FORBIDDEN, error_msg)

        content = xlsx_file.read()

        try:
            fs = BytesIO(content)
            wb = load_workbook(filename=fs, read_only=True)
        except Exception as e:
            logger.error(e)

        # example file is like:
        # Email
        # a@a.com
        # b@b.com

        rows = wb.worksheets[0].rows
        records = []
        # skip first row(head field).
        next(rows)
        for row in rows:
            records.append([col.value for col in row])

        emails_list = []
        for record in records:
            if record[0]:
                email = record[0].strip().lower()
                emails_list.append(email)

        result = {}
        result['failed'] = []
        result['success'] = []
        emails_need_add = []

        org_id = None
        if is_org_context(request):
            org_id = request.user.org.org_id

        for email in emails_list:
            email_name = email2nickname(email)
            try:
                User.objects.get(email=email)
            except User.DoesNotExist:
                result['failed'].append({
                    'email': email,
                    'email_name': email_name,
                    'error_msg': 'User %s not found.' % email_name
                    })
                continue

            if is_group_member(group_id, email, in_structure=False):
                result['failed'].append({
                    'email': email,
                    'email_name': email_name,
                    'error_msg': _('User %s is already a group member.') % email_name
                    })
                continue

            # Can only invite organization users to group
            if org_id and not ccnet_api.org_user_exists(org_id, email):
                result['failed'].append({
                    'email': email,
                    'email_name': email_name,
                    'error_msg': _('User %s not found in organization.') % email_name
                    })
                continue

            if not org_id and is_org_user(email):
                result['failed'].append({
                    'email': email,
                    'email_name': email_name,
                    'error_msg': _('User %s is an organization user.') % email_name
                    })
                continue

            emails_need_add.append(email)

        # Add user to group.
        for email in emails_need_add:
            try:
                ccnet_api.group_add_member(group_id, username, email)
                member_info = get_group_member_info(request, group_id, email)
                result['success'].append(member_info)
            except SearpcError as e:
                logger.error(e)
                result['failed'].append({
                    'email': email,
                    'error_msg': 'Internal Server Error'
                    })

            add_user_to_group.send(sender=None,
                                   group_staff=username,
                                   group_id=group_id,
                                   added_user=email)
        return Response(result)","for record in records:
    if record[0]:
        email = record[0].strip().lower()
        emails_list.append(email)","for record in records:
    (record_0, *record_rrecordmaining) = record
    if record[0]:
        email = record[0].strip().lower()
        emails_list.append(email)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
nfstream,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nfstream/nfstream/utils.py,https://github.com/nfstream/nfstream/tree/master/nfstream/utils.py,,update_performances$83,"def update_performances(performances, is_linux, flows_count):
    """""" Update performance report and check platform for consistency """"""
    drops = 0
    processed = 0
    ignored = 0
    load = []
    for meter in performances:
        if is_linux:
            drops += meter[0].value
            ignored += meter[2].value
        else:
            drops = max(meter[0].value, drops)
            ignored = max(meter[2].value, ignored)
        processed += meter[1].value
        load.append(meter[1].value)
    print(json.dumps({""flows_expired"": flows_count.value,
                      ""packets_processed"": processed,
                      ""packets_ignored"": ignored,
                      ""packets_dropped_filtered_by_kernel"": drops,
                      ""meters_packets_processing_balance"": load}))","for meter in performances:
    if is_linux:
        drops += meter[0].value
        ignored += meter[2].value
    else:
        drops = max(meter[0].value, drops)
        ignored = max(meter[2].value, ignored)
    processed += meter[1].value
    load.append(meter[1].value)","for meter in performances:
    (meter_0, meter_1, meter_2, *_) = meter
    if is_linux:
        drops += meter[0].value
        ignored += meter[2].value
    else:
        drops = max(meter[0].value, drops)
        ignored = max(meter[2].value, ignored)
    processed += meter[1].value
    load.append(meter[1].value)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for note in encoding:
    max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
    if 0 <= note[3] < 128:
        note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))","for note in encoding:
    (_, note_1, _, note_3, note_4, note_5, *_) = note
    max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
    if 0 <= note[3] < 128:
        note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, _, e_3, e_4, e_5 = e
variable mapping:
e_1: e[1]
e_3: e[3]
e_4: e[4]
e_5: e[5]",,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for i in encoding:
    if i[2] < 128:
        pitch_sum[i[-1]] += i[3]
        note_cnt[i[-1]] += 1","for i in encoding:
    (_, _, i_2, i_3, *_) = i
    if i[2] < 128:
        pitch_sum[i[-1]] += i[3]
        note_cnt[i[-1]] += 1",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, _, e_2, e_3 = e
variable mapping:
e_2: e[2]
e_3: e[3]",,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for note in encoding:
    if note[6] != allowed_ts:
        continue
    if note[3] < 0 or note[3] > 127:
        continue
    if len(tmp):
        cur_pos = bar_to_pos[note[0]] + note[1]
        last_note = tmp[-1]
        last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
        if cur_pos - last_pos >= 0:
            tmp.append(note)
    else:
        tmp.append(note)","for note in encoding:
    (_, note_1, _, note_3, _, _, note_6, *note_rnotemaining) = note
    if note[6] != allowed_ts:
        continue
    if note[3] < 0 or note[3] > 127:
        continue
    if len(tmp):
        cur_pos = bar_to_pos[note[0]] + note[1]
        last_note = tmp[-1]
        last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
        if cur_pos - last_pos >= 0:
            tmp.append(note)
    else:
        tmp.append(note)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, e_1, _, e_3, _, _, e_6, *e_remaining = e
variable mapping:
e_1: e[1]
e_3: e[3]
e_6: e[6]",,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for note in encoding:
    max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
    if 0 <= note[3] < 128:
        lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))","for note in encoding:
    (_, note_1, _, note_3, note_4, note_5, *_) = note
    max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
    if 0 <= note[3] < 128:
        lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, _, e_3, e_4, e_5 = e
variable mapping:
e_1: e[1]
e_3: e[3]
e_4: e[4]
e_5: e[5]",,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for enc in encoding:
    if boundry_idx >= len(boundry):
        break
    cur_pos = bar_to_pos[enc[0]] + enc[1]
    while cur_pos >= boundry[boundry_idx]:
        boundry_idx += 1
        if boundry_idx >= len(boundry):
            break
        segments.append([])
    if len(segments):
        segments[-1].append(enc)","for enc in encoding:
    (_, enc_1, *enc_rencmaining) = enc
    if boundry_idx >= len(boundry):
        break
    cur_pos = bar_to_pos[enc[0]] + enc[1]
    while cur_pos >= boundry[boundry_idx]:
        boundry_idx += 1
        if boundry_idx >= len(boundry):
            break
        segments.append([])
    if len(segments):
        segments[-1].append(enc)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for note in raw_notes:
    if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
        notes_list.append([])
    notes_list[-1].append(note)
    cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])","for note in raw_notes:
    (_, note_1, *_, note_4) = note
    if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
        notes_list.append([])
    notes_list[-1].append(note)
    cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, e_1, *_, e_4 = e
variable mapping:
e_1: e[1]
e_4: e[4]",,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for notes in notes_list:
    if len(notes) < min_notes or len(notes) > max_notes:
        continue
    src_words = []
    if is_major:
        src_words.append('MAJ')
    else:
        src_words.append('MIN')
    tgt_words = []
    first_note = notes[0]
    min_bar = first_note[0]
    for (note_idx, note) in enumerate(notes):
        cur_pos = bar_to_pos[note[0]] + note[1]
        chord_idx = 2 * note[0]
        if note[1] >= 2 * pos_resolution:
            chord_idx += 1
        cur_chord = lead_chords[chord_idx]
        src_words.append(f'Chord_{cur_chord}')
        if note_idx != len(notes) - 1:
            nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
            if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                pitch_type = note[3] % 12
                if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                    src_words.append('AUT')
                else:
                    src_words.append('HALF')
            else:
                src_words.append('NOT')
        elif dec_dur(note[4]) >= pos_resolution:
            pitch_type = note[3] % 12
            if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                src_words.append('AUT')
            else:
                src_words.append('HALF')
        else:
            src_words.append('NOT')
        beat_idx = note[1] // pos_resolution
        beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
        src_words.append(f'BEAT_{beat_idx}')
        tgt_words.append(f'Bar_{note[0] - min_bar}')
        tgt_words.append(f'Pos_{note[1]}')
        tgt_words.append(f'Pitch_{note[3]}')
        tgt_words.append(f'Dur_{note[4]}')
    src_strs.append(' '.join(src_words))
    tgt_strs.append(' '.join(tgt_words))","for notes in notes_list:
    (notes_0, *notes_rnotesmaining) = notes
    if len(notes) < min_notes or len(notes) > max_notes:
        continue
    src_words = []
    if is_major:
        src_words.append('MAJ')
    else:
        src_words.append('MIN')
    tgt_words = []
    first_note = notes[0]
    min_bar = first_note[0]
    for (note_idx, note) in enumerate(notes):
        cur_pos = bar_to_pos[note[0]] + note[1]
        chord_idx = 2 * note[0]
        if note[1] >= 2 * pos_resolution:
            chord_idx += 1
        cur_chord = lead_chords[chord_idx]
        src_words.append(f'Chord_{cur_chord}')
        if note_idx != len(notes) - 1:
            nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
            if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                pitch_type = note[3] % 12
                if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                    src_words.append('AUT')
                else:
                    src_words.append('HALF')
            else:
                src_words.append('NOT')
        elif dec_dur(note[4]) >= pos_resolution:
            pitch_type = note[3] % 12
            if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                src_words.append('AUT')
            else:
                src_words.append('HALF')
        else:
            src_words.append('NOT')
        beat_idx = note[1] // pos_resolution
        beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
        src_words.append(f'BEAT_{beat_idx}')
        tgt_words.append(f'Bar_{note[0] - min_bar}')
        tgt_words.append(f'Pos_{note[1]}')
        tgt_words.append(f'Pitch_{note[3]}')
        tgt_words.append(f'Dur_{note[4]}')
    src_strs.append(' '.join(src_words))
    tgt_strs.append(' '.join(tgt_words))",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
_, e_note_1, *e_note_remaining = e[note_idx + 1]
_, e_note_1_1, *e_note_1_remaining = e_note_1
variable mapping:
e_0: e[0]
e_note_1_1: e[note_idx + 1][1]
",,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for (note_idx, note) in enumerate(notes):
    cur_pos = bar_to_pos[note[0]] + note[1]
    chord_idx = 2 * note[0]
    if note[1] >= 2 * pos_resolution:
        chord_idx += 1
    cur_chord = lead_chords[chord_idx]
    src_words.append(f'Chord_{cur_chord}')
    if note_idx != len(notes) - 1:
        nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
        if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
            pitch_type = note[3] % 12
            if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                src_words.append('AUT')
            else:
                src_words.append('HALF')
        else:
            src_words.append('NOT')
    elif dec_dur(note[4]) >= pos_resolution:
        pitch_type = note[3] % 12
        if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
            src_words.append('AUT')
        else:
            src_words.append('HALF')
    else:
        src_words.append('NOT')
    beat_idx = note[1] // pos_resolution
    beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
    src_words.append(f'BEAT_{beat_idx}')
    tgt_words.append(f'Bar_{note[0] - min_bar}')
    tgt_words.append(f'Pos_{note[1]}')
    tgt_words.append(f'Pitch_{note[3]}')
    tgt_words.append(f'Dur_{note[4]}')","for (note_idx, note) in enumerate(notes):
    (note_0, note_1, *_, note_3, note_4) = note
    cur_pos = bar_to_pos[note[0]] + note[1]
    chord_idx = 2 * note[0]
    if note[1] >= 2 * pos_resolution:
        chord_idx += 1
    cur_chord = lead_chords[chord_idx]
    src_words.append(f'Chord_{cur_chord}')
    if note_idx != len(notes) - 1:
        nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
        if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
            pitch_type = note[3] % 12
            if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                src_words.append('AUT')
            else:
                src_words.append('HALF')
        else:
            src_words.append('NOT')
    elif dec_dur(note[4]) >= pos_resolution:
        pitch_type = note[3] % 12
        if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
            src_words.append('AUT')
        else:
            src_words.append('HALF')
    else:
        src_words.append('NOT')
    beat_idx = note[1] // pos_resolution
    beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
    src_words.append(f'BEAT_{beat_idx}')
    tgt_words.append(f'Bar_{note[0] - min_bar}')
    tgt_words.append(f'Pos_{note[1]}')
    tgt_words.append(f'Pitch_{note[3]}')
    tgt_words.append(f'Dur_{note[4]}')",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *_, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_3: e[3]
e_4: e[4]",,,,,,,
jieba,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jieba/test/jieba_test.py,https://github.com/fxsjy/jieba/tree/master/test/jieba_test.py,JiebaTestCase,testTokenize_NOHMM$185,"def testTokenize_NOHMM(self):
        for content in test_contents:
            result = jieba.tokenize(content,HMM=False)
            assert isinstance(result, types.GeneratorType), ""Test Tokenize Generator error""
            result = list(result)
            assert isinstance(result, list), ""Test Tokenize error on content: %s"" % content
            for tk in result:
                print(""word %s\t\t start: %d \t\t end:%d"" % (tk[0],tk[1],tk[2]), file=sys.stderr)
        print(""testTokenize_NOHMM"", file=sys.stderr)","for tk in result:
    print('word %s\t\t start: %d \t\t end:%d' % (tk[0], tk[1], tk[2]), file=sys.stderr)","for tk in result:
    (tk_0, tk_1, tk_2, *_) = tk
    print('word %s\t\t start: %d \t\t end:%d' % (tk[0], tk[1], tk[2]), file=sys.stderr)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
AIDungeon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AIDungeon/story/utils.py,https://github.com/Latitude-Archives/AIDungeon/tree/master/story/utils.py,,is_first_person$151,"def is_first_person(text):

    count = 0
    for pair in first_to_second_mappings:
        variations = mapping_variation_pairs(pair)
        for variation in variations:
            reg_expr = re.compile(variation[0] + '(?=([^""]*""[^""]*"")*[^""]*$)')
            matches = re.findall(reg_expr, text)
            count += len(matches)

    if count > 3:
        return True
    else:
        return False","for variation in variations:
    reg_expr = re.compile(variation[0] + '(?=([^""]*""[^""]*"")*[^""]*$)')
    matches = re.findall(reg_expr, text)
    count += len(matches)","for variation in variations:
    (variation_0, *variation_rvariationmaining) = variation
    reg_expr = re.compile(variation[0] + '(?=([^""]*""[^""]*"")*[^""]*$)')
    matches = re.findall(reg_expr, text)
    count += len(matches)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
kivy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy/kivy/atlas.py,https://github.com/kivy/kivy/tree/master/kivy/atlas.py,Atlas,create$229,"def create(outname, filenames, size, padding=2, use_path=False):
        '''This method can be used to create an atlas manually from a set of
        images.

        :Parameters:
            `outname`: str
                Basename to use for ``.atlas`` creation and ``-<idx>.png``
                associated images.
            `filenames`: list
                List of filenames to put in the atlas.
            `size`: int or list (width, height)
                Size of the atlas image. If the size is not large enough to
                fit all of the source images, more atlas images will created
                as required.
            `padding`: int, defaults to 2
                Padding to put around each image.

                Be careful. If you're using a padding < 2, you might have
                issues with the borders of the images. Because of the OpenGL
                linearization, it might use the pixels of the adjacent image.

                If you're using a padding >= 2, we'll automatically generate a
                ""border"" of 1px around your image. If you look at
                the result, don't be scared if the image inside is not
                exactly the same as yours :).

            `use_path`: bool, defaults to False
                If True, the relative path of the source png
                file names will be included in the atlas ids rather
                that just in the file names. Leading dots and slashes will be
                excluded and all other slashes in the path will be replaced
                with underscores. For example, if `use_path` is False
                (the default) and the file name is
                ``../data/tiles/green_grass.png``, the id will be
                ``green_grass``. If `use_path` is True, it will be
                ``data_tiles_green_grass``.

            .. versionchanged:: 1.8.0
                Parameter use_path added
        '''
        # Thanks to
        # omnisaurusgames.com/2011/06/texture-atlas-generation-using-python/
        # for its initial implementation.
        try:
            from PIL import Image
        except ImportError:
            Logger.critical('Atlas: Imaging/PIL are missing')
            raise

        if isinstance(size, (tuple, list)):
            size_w, size_h = list(map(int, size))
        else:
            size_w = size_h = int(size)

        # open all of the images
        ims = list()
        for f in filenames:
            fp = open(f, 'rb')
            im = Image.open(fp)
            im.load()
            fp.close()
            ims.append((f, im))

        # sort by image area
        ims = sorted(ims, key=lambda im: im[1].size[0] * im[1].size[1],
                     reverse=True)

        # free boxes are empty space in our output image set
        # the freebox tuple format is: outidx, x, y, w, h
        freeboxes = [(0, 0, 0, size_w, size_h)]
        numoutimages = 1

        # full boxes are areas where we have placed images in the atlas
        # the full box tuple format is: image, outidx, x, y, w, h, filename
        fullboxes = []

        # do the actual atlasing by sticking the largest images we can
        # have into the smallest valid free boxes
        for imageinfo in ims:
            im = imageinfo[1]
            imw, imh = im.size
            imw += padding
            imh += padding
            if imw > size_w or imh > size_h:
                Logger.error(
                    'Atlas: image %s (%d by %d) is larger than the atlas size!'
                    % (imageinfo[0], imw, imh))
                return

            inserted = False
            while not inserted:
                for idx, fb in enumerate(freeboxes):
                    # find the smallest free box that will contain this image
                    if fb[3] >= imw and fb[4] >= imh:
                        # we found a valid spot! Remove the current
                        # freebox, and split the leftover space into (up to)
                        # two new freeboxes
                        del freeboxes[idx]
                        if fb[3] > imw:
                            freeboxes.append((
                                fb[0], fb[1] + imw, fb[2],
                                fb[3] - imw, imh))

                        if fb[4] > imh:
                            freeboxes.append((
                                fb[0], fb[1], fb[2] + imh,
                                fb[3], fb[4] - imh))

                        # keep this sorted!
                        freeboxes = sorted(freeboxes,
                                           key=lambda fb: fb[3] * fb[4])
                        fullboxes.append((im,
                                          fb[0], fb[1] + padding,
                                          fb[2] + padding, imw - padding,
                                          imh - padding, imageinfo[0]))
                        inserted = True
                        break

                if not inserted:
                    # oh crap - there isn't room in any of our free
                    # boxes, so we have to add a new output image
                    freeboxes.append((numoutimages, 0, 0, size_w, size_h))
                    numoutimages += 1

        # now that we've figured out where everything goes, make the output
        # images and blit the source images to the appropriate locations
        Logger.info('Atlas: create an {0}x{1} rgba image'.format(size_w,
                                                                 size_h))
        outimages = [Image.new('RGBA', (size_w, size_h))
                     for i in range(0, int(numoutimages))]
        for fb in fullboxes:
            x, y = fb[2], fb[3]
            out = outimages[fb[1]]
            out.paste(fb[0], (fb[2], fb[3]))
            w, h = fb[0].size
            if padding > 1:
                out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
                out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
                out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
                out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))

        # save the output images
        for idx, outimage in enumerate(outimages):
            outimage.save('%s-%d.png' % (outname, idx))

        # write out an json file that says where everything ended up
        meta = {}
        for fb in fullboxes:
            fn = '%s-%d.png' % (basename(outname), fb[1])
            if fn not in meta:
                d = meta[fn] = {}
            else:
                d = meta[fn]

            # fb[6] contain the filename
            if use_path:
                # use the path with separators replaced by _
                # example '../data/tiles/green_grass.png' becomes
                # 'data_tiles_green_grass'
                uid = splitext(fb[6])[0]
                # remove leading dots and slashes
                uid = uid.lstrip('./\\')
                # replace remaining slashes with _
                uid = uid.replace('/', '_').replace('\\', '_')
            else:
                # for example, '../data/tiles/green_grass.png'
                # just get only 'green_grass' as the uniq id.
                uid = splitext(basename(fb[6]))[0]

            x, y, w, h = fb[2:6]
            d[uid] = x, size_h - y - h, w, h

        outfn = '%s.atlas' % outname
        with open(outfn, 'w') as fd:
            json.dump(meta, fd)

        return outfn, meta","for imageinfo in ims:
    im = imageinfo[1]
    (imw, imh) = im.size
    imw += padding
    imh += padding
    if imw > size_w or imh > size_h:
        Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (imageinfo[0], imw, imh))
        return
    inserted = False
    while not inserted:
        for (idx, fb) in enumerate(freeboxes):
            if fb[3] >= imw and fb[4] >= imh:
                del freeboxes[idx]
                if fb[3] > imw:
                    freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
                if fb[4] > imh:
                    freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
                freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
                fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
                inserted = True
                break
        if not inserted:
            freeboxes.append((numoutimages, 0, 0, size_w, size_h))
            numoutimages += 1","for imageinfo in ims:
    (imageinfo_0, imageinfo_1, *_) = imageinfo
    im = imageinfo[1]
    (imw, imh) = im.size
    imw += padding
    imh += padding
    if imw > size_w or imh > size_h:
        Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (imageinfo[0], imw, imh))
        return
    inserted = False
    while not inserted:
        for (idx, fb) in enumerate(freeboxes):
            if fb[3] >= imw and fb[4] >= imh:
                del freeboxes[idx]
                if fb[3] > imw:
                    freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
                if fb[4] > imh:
                    freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
                freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
                fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
                inserted = True
                break
        if not inserted:
            freeboxes.append((numoutimages, 0, 0, size_w, size_h))
            numoutimages += 1","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
kivy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy/kivy/atlas.py,https://github.com/kivy/kivy/tree/master/kivy/atlas.py,Atlas,create$229,"def create(outname, filenames, size, padding=2, use_path=False):
        '''This method can be used to create an atlas manually from a set of
        images.

        :Parameters:
            `outname`: str
                Basename to use for ``.atlas`` creation and ``-<idx>.png``
                associated images.
            `filenames`: list
                List of filenames to put in the atlas.
            `size`: int or list (width, height)
                Size of the atlas image. If the size is not large enough to
                fit all of the source images, more atlas images will created
                as required.
            `padding`: int, defaults to 2
                Padding to put around each image.

                Be careful. If you're using a padding < 2, you might have
                issues with the borders of the images. Because of the OpenGL
                linearization, it might use the pixels of the adjacent image.

                If you're using a padding >= 2, we'll automatically generate a
                ""border"" of 1px around your image. If you look at
                the result, don't be scared if the image inside is not
                exactly the same as yours :).

            `use_path`: bool, defaults to False
                If True, the relative path of the source png
                file names will be included in the atlas ids rather
                that just in the file names. Leading dots and slashes will be
                excluded and all other slashes in the path will be replaced
                with underscores. For example, if `use_path` is False
                (the default) and the file name is
                ``../data/tiles/green_grass.png``, the id will be
                ``green_grass``. If `use_path` is True, it will be
                ``data_tiles_green_grass``.

            .. versionchanged:: 1.8.0
                Parameter use_path added
        '''
        # Thanks to
        # omnisaurusgames.com/2011/06/texture-atlas-generation-using-python/
        # for its initial implementation.
        try:
            from PIL import Image
        except ImportError:
            Logger.critical('Atlas: Imaging/PIL are missing')
            raise

        if isinstance(size, (tuple, list)):
            size_w, size_h = list(map(int, size))
        else:
            size_w = size_h = int(size)

        # open all of the images
        ims = list()
        for f in filenames:
            fp = open(f, 'rb')
            im = Image.open(fp)
            im.load()
            fp.close()
            ims.append((f, im))

        # sort by image area
        ims = sorted(ims, key=lambda im: im[1].size[0] * im[1].size[1],
                     reverse=True)

        # free boxes are empty space in our output image set
        # the freebox tuple format is: outidx, x, y, w, h
        freeboxes = [(0, 0, 0, size_w, size_h)]
        numoutimages = 1

        # full boxes are areas where we have placed images in the atlas
        # the full box tuple format is: image, outidx, x, y, w, h, filename
        fullboxes = []

        # do the actual atlasing by sticking the largest images we can
        # have into the smallest valid free boxes
        for imageinfo in ims:
            im = imageinfo[1]
            imw, imh = im.size
            imw += padding
            imh += padding
            if imw > size_w or imh > size_h:
                Logger.error(
                    'Atlas: image %s (%d by %d) is larger than the atlas size!'
                    % (imageinfo[0], imw, imh))
                return

            inserted = False
            while not inserted:
                for idx, fb in enumerate(freeboxes):
                    # find the smallest free box that will contain this image
                    if fb[3] >= imw and fb[4] >= imh:
                        # we found a valid spot! Remove the current
                        # freebox, and split the leftover space into (up to)
                        # two new freeboxes
                        del freeboxes[idx]
                        if fb[3] > imw:
                            freeboxes.append((
                                fb[0], fb[1] + imw, fb[2],
                                fb[3] - imw, imh))

                        if fb[4] > imh:
                            freeboxes.append((
                                fb[0], fb[1], fb[2] + imh,
                                fb[3], fb[4] - imh))

                        # keep this sorted!
                        freeboxes = sorted(freeboxes,
                                           key=lambda fb: fb[3] * fb[4])
                        fullboxes.append((im,
                                          fb[0], fb[1] + padding,
                                          fb[2] + padding, imw - padding,
                                          imh - padding, imageinfo[0]))
                        inserted = True
                        break

                if not inserted:
                    # oh crap - there isn't room in any of our free
                    # boxes, so we have to add a new output image
                    freeboxes.append((numoutimages, 0, 0, size_w, size_h))
                    numoutimages += 1

        # now that we've figured out where everything goes, make the output
        # images and blit the source images to the appropriate locations
        Logger.info('Atlas: create an {0}x{1} rgba image'.format(size_w,
                                                                 size_h))
        outimages = [Image.new('RGBA', (size_w, size_h))
                     for i in range(0, int(numoutimages))]
        for fb in fullboxes:
            x, y = fb[2], fb[3]
            out = outimages[fb[1]]
            out.paste(fb[0], (fb[2], fb[3]))
            w, h = fb[0].size
            if padding > 1:
                out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
                out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
                out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
                out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))

        # save the output images
        for idx, outimage in enumerate(outimages):
            outimage.save('%s-%d.png' % (outname, idx))

        # write out an json file that says where everything ended up
        meta = {}
        for fb in fullboxes:
            fn = '%s-%d.png' % (basename(outname), fb[1])
            if fn not in meta:
                d = meta[fn] = {}
            else:
                d = meta[fn]

            # fb[6] contain the filename
            if use_path:
                # use the path with separators replaced by _
                # example '../data/tiles/green_grass.png' becomes
                # 'data_tiles_green_grass'
                uid = splitext(fb[6])[0]
                # remove leading dots and slashes
                uid = uid.lstrip('./\\')
                # replace remaining slashes with _
                uid = uid.replace('/', '_').replace('\\', '_')
            else:
                # for example, '../data/tiles/green_grass.png'
                # just get only 'green_grass' as the uniq id.
                uid = splitext(basename(fb[6]))[0]

            x, y, w, h = fb[2:6]
            d[uid] = x, size_h - y - h, w, h

        outfn = '%s.atlas' % outname
        with open(outfn, 'w') as fd:
            json.dump(meta, fd)

        return outfn, meta","for fb in fullboxes:
    (x, y) = (fb[2], fb[3])
    out = outimages[fb[1]]
    out.paste(fb[0], (fb[2], fb[3]))
    (w, h) = fb[0].size
    if padding > 1:
        out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
        out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
        out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
        out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))","for fb in fullboxes:
    (fb_0, _, fb_2, fb_3, *_) = fb
    (x, y) = (fb[2], fb[3])
    out = outimages[fb[1]]
    out.paste(fb[0], (fb[2], fb[3]))
    (w, h) = fb[0].size
    if padding > 1:
        out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
        out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
        out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
        out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, _, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_2: e[2]
e_3: e[3]",,,,,,,
kivy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy/kivy/atlas.py,https://github.com/kivy/kivy/tree/master/kivy/atlas.py,Atlas,create$229,"def create(outname, filenames, size, padding=2, use_path=False):
        '''This method can be used to create an atlas manually from a set of
        images.

        :Parameters:
            `outname`: str
                Basename to use for ``.atlas`` creation and ``-<idx>.png``
                associated images.
            `filenames`: list
                List of filenames to put in the atlas.
            `size`: int or list (width, height)
                Size of the atlas image. If the size is not large enough to
                fit all of the source images, more atlas images will created
                as required.
            `padding`: int, defaults to 2
                Padding to put around each image.

                Be careful. If you're using a padding < 2, you might have
                issues with the borders of the images. Because of the OpenGL
                linearization, it might use the pixels of the adjacent image.

                If you're using a padding >= 2, we'll automatically generate a
                ""border"" of 1px around your image. If you look at
                the result, don't be scared if the image inside is not
                exactly the same as yours :).

            `use_path`: bool, defaults to False
                If True, the relative path of the source png
                file names will be included in the atlas ids rather
                that just in the file names. Leading dots and slashes will be
                excluded and all other slashes in the path will be replaced
                with underscores. For example, if `use_path` is False
                (the default) and the file name is
                ``../data/tiles/green_grass.png``, the id will be
                ``green_grass``. If `use_path` is True, it will be
                ``data_tiles_green_grass``.

            .. versionchanged:: 1.8.0
                Parameter use_path added
        '''
        # Thanks to
        # omnisaurusgames.com/2011/06/texture-atlas-generation-using-python/
        # for its initial implementation.
        try:
            from PIL import Image
        except ImportError:
            Logger.critical('Atlas: Imaging/PIL are missing')
            raise

        if isinstance(size, (tuple, list)):
            size_w, size_h = list(map(int, size))
        else:
            size_w = size_h = int(size)

        # open all of the images
        ims = list()
        for f in filenames:
            fp = open(f, 'rb')
            im = Image.open(fp)
            im.load()
            fp.close()
            ims.append((f, im))

        # sort by image area
        ims = sorted(ims, key=lambda im: im[1].size[0] * im[1].size[1],
                     reverse=True)

        # free boxes are empty space in our output image set
        # the freebox tuple format is: outidx, x, y, w, h
        freeboxes = [(0, 0, 0, size_w, size_h)]
        numoutimages = 1

        # full boxes are areas where we have placed images in the atlas
        # the full box tuple format is: image, outidx, x, y, w, h, filename
        fullboxes = []

        # do the actual atlasing by sticking the largest images we can
        # have into the smallest valid free boxes
        for imageinfo in ims:
            im = imageinfo[1]
            imw, imh = im.size
            imw += padding
            imh += padding
            if imw > size_w or imh > size_h:
                Logger.error(
                    'Atlas: image %s (%d by %d) is larger than the atlas size!'
                    % (imageinfo[0], imw, imh))
                return

            inserted = False
            while not inserted:
                for idx, fb in enumerate(freeboxes):
                    # find the smallest free box that will contain this image
                    if fb[3] >= imw and fb[4] >= imh:
                        # we found a valid spot! Remove the current
                        # freebox, and split the leftover space into (up to)
                        # two new freeboxes
                        del freeboxes[idx]
                        if fb[3] > imw:
                            freeboxes.append((
                                fb[0], fb[1] + imw, fb[2],
                                fb[3] - imw, imh))

                        if fb[4] > imh:
                            freeboxes.append((
                                fb[0], fb[1], fb[2] + imh,
                                fb[3], fb[4] - imh))

                        # keep this sorted!
                        freeboxes = sorted(freeboxes,
                                           key=lambda fb: fb[3] * fb[4])
                        fullboxes.append((im,
                                          fb[0], fb[1] + padding,
                                          fb[2] + padding, imw - padding,
                                          imh - padding, imageinfo[0]))
                        inserted = True
                        break

                if not inserted:
                    # oh crap - there isn't room in any of our free
                    # boxes, so we have to add a new output image
                    freeboxes.append((numoutimages, 0, 0, size_w, size_h))
                    numoutimages += 1

        # now that we've figured out where everything goes, make the output
        # images and blit the source images to the appropriate locations
        Logger.info('Atlas: create an {0}x{1} rgba image'.format(size_w,
                                                                 size_h))
        outimages = [Image.new('RGBA', (size_w, size_h))
                     for i in range(0, int(numoutimages))]
        for fb in fullboxes:
            x, y = fb[2], fb[3]
            out = outimages[fb[1]]
            out.paste(fb[0], (fb[2], fb[3]))
            w, h = fb[0].size
            if padding > 1:
                out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
                out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
                out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
                out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))

        # save the output images
        for idx, outimage in enumerate(outimages):
            outimage.save('%s-%d.png' % (outname, idx))

        # write out an json file that says where everything ended up
        meta = {}
        for fb in fullboxes:
            fn = '%s-%d.png' % (basename(outname), fb[1])
            if fn not in meta:
                d = meta[fn] = {}
            else:
                d = meta[fn]

            # fb[6] contain the filename
            if use_path:
                # use the path with separators replaced by _
                # example '../data/tiles/green_grass.png' becomes
                # 'data_tiles_green_grass'
                uid = splitext(fb[6])[0]
                # remove leading dots and slashes
                uid = uid.lstrip('./\\')
                # replace remaining slashes with _
                uid = uid.replace('/', '_').replace('\\', '_')
            else:
                # for example, '../data/tiles/green_grass.png'
                # just get only 'green_grass' as the uniq id.
                uid = splitext(basename(fb[6]))[0]

            x, y, w, h = fb[2:6]
            d[uid] = x, size_h - y - h, w, h

        outfn = '%s.atlas' % outname
        with open(outfn, 'w') as fd:
            json.dump(meta, fd)

        return outfn, meta","for fb in fullboxes:
    fn = '%s-%d.png' % (basename(outname), fb[1])
    if fn not in meta:
        d = meta[fn] = {}
    else:
        d = meta[fn]
    if use_path:
        uid = splitext(fb[6])[0]
        uid = uid.lstrip('./\\')
        uid = uid.replace('/', '_').replace('\\', '_')
    else:
        uid = splitext(basename(fb[6]))[0]
    (x, y, w, h) = fb[2:6]
    d[uid] = (x, size_h - y - h, w, h)","for fb in fullboxes:
    (_, fb_1, *fb_2_to_6, *fb_rfbmaining) = fb
    fn = '%s-%d.png' % (basename(outname), fb[1])
    if fn not in meta:
        d = meta[fn] = {}
    else:
        d = meta[fn]
    if use_path:
        uid = splitext(fb[6])[0]
        uid = uid.lstrip('./\\')
        uid = uid.replace('/', '_').replace('\\', '_')
    else:
        uid = splitext(basename(fb[6]))[0]
    (x, y, w, h) = fb[2:6]
    d[uid] = (x, size_h - y - h, w, h)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_2_to_6, *e_remaining = e
variable mapping:
e_1: e[1]
e_2_to_6: e[2:6]",,,,,,,
kivy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy/kivy/atlas.py,https://github.com/kivy/kivy/tree/master/kivy/atlas.py,Atlas,create$229,"def create(outname, filenames, size, padding=2, use_path=False):
        '''This method can be used to create an atlas manually from a set of
        images.

        :Parameters:
            `outname`: str
                Basename to use for ``.atlas`` creation and ``-<idx>.png``
                associated images.
            `filenames`: list
                List of filenames to put in the atlas.
            `size`: int or list (width, height)
                Size of the atlas image. If the size is not large enough to
                fit all of the source images, more atlas images will created
                as required.
            `padding`: int, defaults to 2
                Padding to put around each image.

                Be careful. If you're using a padding < 2, you might have
                issues with the borders of the images. Because of the OpenGL
                linearization, it might use the pixels of the adjacent image.

                If you're using a padding >= 2, we'll automatically generate a
                ""border"" of 1px around your image. If you look at
                the result, don't be scared if the image inside is not
                exactly the same as yours :).

            `use_path`: bool, defaults to False
                If True, the relative path of the source png
                file names will be included in the atlas ids rather
                that just in the file names. Leading dots and slashes will be
                excluded and all other slashes in the path will be replaced
                with underscores. For example, if `use_path` is False
                (the default) and the file name is
                ``../data/tiles/green_grass.png``, the id will be
                ``green_grass``. If `use_path` is True, it will be
                ``data_tiles_green_grass``.

            .. versionchanged:: 1.8.0
                Parameter use_path added
        '''
        # Thanks to
        # omnisaurusgames.com/2011/06/texture-atlas-generation-using-python/
        # for its initial implementation.
        try:
            from PIL import Image
        except ImportError:
            Logger.critical('Atlas: Imaging/PIL are missing')
            raise

        if isinstance(size, (tuple, list)):
            size_w, size_h = list(map(int, size))
        else:
            size_w = size_h = int(size)

        # open all of the images
        ims = list()
        for f in filenames:
            fp = open(f, 'rb')
            im = Image.open(fp)
            im.load()
            fp.close()
            ims.append((f, im))

        # sort by image area
        ims = sorted(ims, key=lambda im: im[1].size[0] * im[1].size[1],
                     reverse=True)

        # free boxes are empty space in our output image set
        # the freebox tuple format is: outidx, x, y, w, h
        freeboxes = [(0, 0, 0, size_w, size_h)]
        numoutimages = 1

        # full boxes are areas where we have placed images in the atlas
        # the full box tuple format is: image, outidx, x, y, w, h, filename
        fullboxes = []

        # do the actual atlasing by sticking the largest images we can
        # have into the smallest valid free boxes
        for imageinfo in ims:
            im = imageinfo[1]
            imw, imh = im.size
            imw += padding
            imh += padding
            if imw > size_w or imh > size_h:
                Logger.error(
                    'Atlas: image %s (%d by %d) is larger than the atlas size!'
                    % (imageinfo[0], imw, imh))
                return

            inserted = False
            while not inserted:
                for idx, fb in enumerate(freeboxes):
                    # find the smallest free box that will contain this image
                    if fb[3] >= imw and fb[4] >= imh:
                        # we found a valid spot! Remove the current
                        # freebox, and split the leftover space into (up to)
                        # two new freeboxes
                        del freeboxes[idx]
                        if fb[3] > imw:
                            freeboxes.append((
                                fb[0], fb[1] + imw, fb[2],
                                fb[3] - imw, imh))

                        if fb[4] > imh:
                            freeboxes.append((
                                fb[0], fb[1], fb[2] + imh,
                                fb[3], fb[4] - imh))

                        # keep this sorted!
                        freeboxes = sorted(freeboxes,
                                           key=lambda fb: fb[3] * fb[4])
                        fullboxes.append((im,
                                          fb[0], fb[1] + padding,
                                          fb[2] + padding, imw - padding,
                                          imh - padding, imageinfo[0]))
                        inserted = True
                        break

                if not inserted:
                    # oh crap - there isn't room in any of our free
                    # boxes, so we have to add a new output image
                    freeboxes.append((numoutimages, 0, 0, size_w, size_h))
                    numoutimages += 1

        # now that we've figured out where everything goes, make the output
        # images and blit the source images to the appropriate locations
        Logger.info('Atlas: create an {0}x{1} rgba image'.format(size_w,
                                                                 size_h))
        outimages = [Image.new('RGBA', (size_w, size_h))
                     for i in range(0, int(numoutimages))]
        for fb in fullboxes:
            x, y = fb[2], fb[3]
            out = outimages[fb[1]]
            out.paste(fb[0], (fb[2], fb[3]))
            w, h = fb[0].size
            if padding > 1:
                out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
                out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
                out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
                out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))

        # save the output images
        for idx, outimage in enumerate(outimages):
            outimage.save('%s-%d.png' % (outname, idx))

        # write out an json file that says where everything ended up
        meta = {}
        for fb in fullboxes:
            fn = '%s-%d.png' % (basename(outname), fb[1])
            if fn not in meta:
                d = meta[fn] = {}
            else:
                d = meta[fn]

            # fb[6] contain the filename
            if use_path:
                # use the path with separators replaced by _
                # example '../data/tiles/green_grass.png' becomes
                # 'data_tiles_green_grass'
                uid = splitext(fb[6])[0]
                # remove leading dots and slashes
                uid = uid.lstrip('./\\')
                # replace remaining slashes with _
                uid = uid.replace('/', '_').replace('\\', '_')
            else:
                # for example, '../data/tiles/green_grass.png'
                # just get only 'green_grass' as the uniq id.
                uid = splitext(basename(fb[6]))[0]

            x, y, w, h = fb[2:6]
            d[uid] = x, size_h - y - h, w, h

        outfn = '%s.atlas' % outname
        with open(outfn, 'w') as fd:
            json.dump(meta, fd)

        return outfn, meta","for (idx, fb) in enumerate(freeboxes):
    if fb[3] >= imw and fb[4] >= imh:
        del freeboxes[idx]
        if fb[3] > imw:
            freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
        if fb[4] > imh:
            freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
        freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
        fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
        inserted = True
        break","for (idx, fb) in enumerate(freeboxes):
    (fb_0, fb_1, fb_2, fb_3, fb_4, *_) = fb
    if fb[3] >= imw and fb[4] >= imh:
        del freeboxes[idx]
        if fb[3] > imw:
            freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
        if fb[4] > imh:
            freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
        freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
        fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
        inserted = True
        break",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,
ShuiZe_0x727,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ShuiZe_0x727/Plugins/infoGather/subdomain/Spider/Baidu/baidu.py,https://github.com/0x727/ShuiZe_0x727/tree/master/Plugins/infoGather/subdomain/Spider/Baidu/baidu.py,BaiduSpider,get_subdomain$41,"def get_subdomain(self, each_wd, i):
        for page in range(1, self.PAGES+1):
            wd = 'site:{} {}'.format(self.domain, each_wd)
            print('[{}] -> [page: {}]'.format(wd, page))
            wd = quote(wd)
            bd_link_titles = self.keyword(wd=wd, page=page)
            if bd_link_titles:
                for bd_link_title in bd_link_titles:
                    title, link = bd_link_title[0], bd_link_title[1]
                    subdomain = self.location(each_wd, link, title)
                    # subdomain = map(lambda x: 'http://{}'.format(urlparse(x).netloc), map(self.location, [each_wd] * len(retList), retList))
                    # for _ in subdomain:
                    self.bdSubdomains.append(urlparse(subdomain).netloc)","for bd_link_title in bd_link_titles:
    (title, link) = (bd_link_title[0], bd_link_title[1])
    subdomain = self.location(each_wd, link, title)
    self.bdSubdomains.append(urlparse(subdomain).netloc)","for bd_link_title in bd_link_titles:
    (bd_link_title_0, bd_link_title_1, *_) = bd_link_title
    (title, link) = (bd_link_title[0], bd_link_title[1])
    subdomain = self.location(each_wd, link, title)
    self.bdSubdomains.append(urlparse(subdomain).netloc)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
osxphotos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/osxphotos/osxphotos/cli.py,https://github.com/RhetTbull/osxphotos/tree/master/osxphotos/cli.py,,export$1069,"def export(
    ctx,
    cli_obj,
    db,
    photos_library,
    keyword,
    person,
    album,
    folder,
    uuid,
    name,
    uuid_from_file,
    title,
    no_title,
    description,
    no_description,
    uti,
    ignore_case,
    edited,
    external_edit,
    favorite,
    not_favorite,
    hidden,
    not_hidden,
    shared,
    not_shared,
    from_date,
    to_date,
    from_time,
    to_time,
    verbose,
    missing,
    update,
    ignore_signature,
    only_new,
    dry_run,
    export_as_hardlink,
    touch_file,
    overwrite,
    retry,
    export_by_date,
    skip_edited,
    skip_original_if_edited,
    skip_bursts,
    skip_live,
    skip_raw,
    person_keyword,
    album_keyword,
    keyword_template,
    replace_keywords,
    description_template,
    finder_tag_template,
    finder_tag_keywords,
    xattr_template,
    current_name,
    convert_to_jpeg,
    jpeg_quality,
    sidecar,
    sidecar_drop_ext,
    only_photos,
    only_movies,
    burst,
    not_burst,
    live,
    not_live,
    download_missing,
    dest,
    exiftool,
    exiftool_path,
    exiftool_option,
    exiftool_merge_keywords,
    exiftool_merge_persons,
    ignore_date_modified,
    portrait,
    not_portrait,
    screenshot,
    not_screenshot,
    slow_mo,
    not_slow_mo,
    time_lapse,
    not_time_lapse,
    hdr,
    not_hdr,
    selfie,
    not_selfie,
    panorama,
    not_panorama,
    has_raw,
    directory,
    filename_template,
    jpeg_ext,
    strip,
    edited_suffix,
    original_suffix,
    place,
    no_place,
    location,
    no_location,
    has_comment,
    no_comment,
    has_likes,
    no_likes,
    label,
    deleted,
    deleted_only,
    use_photos_export,
    use_photokit,
    report,
    cleanup,
    add_exported_to_album,
    add_skipped_to_album,
    add_missing_to_album,
    exportdb,
    load_config,
    save_config,
    is_reference,
    beta,
    in_album,
    not_in_album,
    min_size,
    max_size,
    regex,
    selected,
    query_eval,
    query_function,
    duplicate,
    post_command,
    post_function,
    preview,
    preview_suffix,
    preview_if_missing,
):
    """"""Export photos from the Photos database.
    Export path DEST is required.
    Optionally, query the Photos database using 1 or more search options;
    if more than one option is provided, they are treated as ""AND""
    (e.g. search for photos matching all options).
    If no query options are provided, all photos will be exported.
    By default, all versions of all photos will be exported including edited
    versions, live photo movies, burst photos, and associated raw images.
    See --skip-edited, --skip-live, --skip-bursts, and --skip-raw options
    to modify this behavior.
    """"""

    # NOTE: because of the way ConfigOptions works, Click options must not
    # set defaults which are not None or False. If defaults need to be set
    # do so below after load_config and save_config are handled.
    cfg = ConfigOptions(
        ""export"",
        locals(),
        ignore=[""ctx"", ""cli_obj"", ""dest"", ""load_config"", ""save_config""],
    )

    global VERBOSE
    VERBOSE = bool(verbose)

    if load_config:
        try:
            cfg.load_from_file(load_config)
        except ConfigOptionsLoadError as e:
            click.echo(
                click.style(
                    f""Error parsing {load_config} config file: {e.message}"",
                    fg=CLI_COLOR_ERROR,
                ),
                err=True,
            )
            raise click.Abort()

        # re-set the local vars to the corresponding config value
        # this isn't elegant but avoids having to rewrite this function to use cfg.varname for every parameter
        db = cfg.db
        photos_library = cfg.photos_library
        keyword = cfg.keyword
        person = cfg.person
        album = cfg.album
        folder = cfg.folder
        name = cfg.name
        uuid = cfg.uuid
        uuid_from_file = cfg.uuid_from_file
        title = cfg.title
        no_title = cfg.no_title
        description = cfg.description
        no_description = cfg.no_description
        uti = cfg.uti
        ignore_case = cfg.ignore_case
        edited = cfg.edited
        external_edit = cfg.external_edit
        favorite = cfg.favorite
        not_favorite = cfg.not_favorite
        hidden = cfg.hidden
        not_hidden = cfg.not_hidden
        shared = cfg.shared
        not_shared = cfg.not_shared
        from_date = cfg.from_date
        to_date = cfg.to_date
        from_time = cfg.from_time
        to_time = cfg.to_time
        verbose = cfg.verbose
        missing = cfg.missing
        update = cfg.update
        ignore_signature = cfg.ignore_signature
        dry_run = cfg.dry_run
        export_as_hardlink = cfg.export_as_hardlink
        touch_file = cfg.touch_file
        overwrite = cfg.overwrite
        retry = cfg.retry
        export_by_date = cfg.export_by_date
        skip_edited = cfg.skip_edited
        skip_original_if_edited = cfg.skip_original_if_edited
        skip_bursts = cfg.skip_bursts
        skip_live = cfg.skip_live
        skip_raw = cfg.skip_raw
        person_keyword = cfg.person_keyword
        album_keyword = cfg.album_keyword
        keyword_template = cfg.keyword_template
        replace_keywords = cfg.replace_keywords
        description_template = cfg.description_template
        finder_tag_template = cfg.finder_tag_template
        finder_tag_keywords = cfg.finder_tag_keywords
        xattr_template = cfg.xattr_template
        current_name = cfg.current_name
        convert_to_jpeg = cfg.convert_to_jpeg
        jpeg_quality = cfg.jpeg_quality
        sidecar = cfg.sidecar
        sidecar_drop_ext = cfg.sidecar_drop_ext
        only_photos = cfg.only_photos
        only_movies = cfg.only_movies
        burst = cfg.burst
        not_burst = cfg.not_burst
        live = cfg.live
        not_live = cfg.not_live
        download_missing = cfg.download_missing
        exiftool = cfg.exiftool
        exiftool_path = cfg.exiftool_path
        exiftool_option = cfg.exiftool_option
        exiftool_merge_keywords = cfg.exiftool_merge_keywords
        exiftool_merge_persons = cfg.exiftool_merge_persons
        ignore_date_modified = cfg.ignore_date_modified
        portrait = cfg.portrait
        not_portrait = cfg.not_portrait
        screenshot = cfg.screenshot
        not_screenshot = cfg.not_screenshot
        slow_mo = cfg.slow_mo
        not_slow_mo = cfg.not_slow_mo
        time_lapse = cfg.time_lapse
        not_time_lapse = cfg.not_time_lapse
        hdr = cfg.hdr
        not_hdr = cfg.not_hdr
        selfie = cfg.selfie
        not_selfie = cfg.not_selfie
        panorama = cfg.panorama
        not_panorama = cfg.not_panorama
        has_raw = cfg.has_raw
        directory = cfg.directory
        filename_template = cfg.filename_template
        jpeg_ext = cfg.jpeg_ext
        strip = cfg.strip
        edited_suffix = cfg.edited_suffix
        original_suffix = cfg.original_suffix
        place = cfg.place
        no_place = cfg.no_place
        location = cfg.location
        no_location = cfg.no_location
        has_comment = cfg.has_comment
        no_comment = cfg.no_comment
        has_likes = cfg.has_likes
        no_likes = cfg.no_likes
        label = cfg.label
        deleted = cfg.deleted
        deleted_only = cfg.deleted_only
        use_photos_export = cfg.use_photos_export
        use_photokit = cfg.use_photokit
        report = cfg.report
        cleanup = cfg.cleanup
        add_exported_to_album = cfg.add_exported_to_album
        add_skipped_to_album = cfg.add_skipped_to_album
        add_missing_to_album = cfg.add_missing_to_album
        exportdb = cfg.exportdb
        beta = cfg.beta
        only_new = cfg.only_new
        in_album = cfg.in_album
        not_in_album = cfg.not_in_album
        min_size = cfg.min_size
        max_size = cfg.max_size
        regex = cfg.regex
        selected = cfg.selected
        query_eval = cfg.query_eval
        query_function = cfg.query_function
        duplicate = cfg.duplicate
        post_command = cfg.post_command
        post_function = cfg.post_function
        preview = cfg.preview
        preview_suffix = cfg.preview_suffix
        preview_if_missing = cfg.preview_if_missing

        # config file might have changed verbose
        VERBOSE = bool(verbose)
        verbose_(f""Loaded options from file {load_config}"")

    verbose_(f""osxphotos version {__version__}"")

    # validate options
    exclusive_options = [
        (""favorite"", ""not_favorite""),
        (""hidden"", ""not_hidden""),
        (""title"", ""no_title""),
        (""description"", ""no_description""),
        (""only_photos"", ""only_movies""),
        (""burst"", ""not_burst""),
        (""live"", ""not_live""),
        (""portrait"", ""not_portrait""),
        (""screenshot"", ""not_screenshot""),
        (""slow_mo"", ""not_slow_mo""),
        (""time_lapse"", ""not_time_lapse""),
        (""hdr"", ""not_hdr""),
        (""selfie"", ""not_selfie""),
        (""panorama"", ""not_panorama""),
        (""export_by_date"", ""directory""),
        (""export_as_hardlink"", ""exiftool""),
        (""place"", ""no_place""),
        (""deleted"", ""deleted_only""),
        (""skip_edited"", ""skip_original_if_edited""),
        (""export_as_hardlink"", ""convert_to_jpeg""),
        (""export_as_hardlink"", ""download_missing""),
        (""shared"", ""not_shared""),
        (""has_comment"", ""no_comment""),
        (""has_likes"", ""no_likes""),
        (""in_album"", ""not_in_album""),
        (""location"", ""no_location""),
    ]
    dependent_options = [
        (""missing"", (""download_missing"", ""use_photos_export"")),
        (""jpeg_quality"", (""convert_to_jpeg"")),
        (""ignore_signature"", (""update"")),
        (""only_new"", (""update"")),
        (""exiftool_option"", (""exiftool"")),
        (""exiftool_merge_keywords"", (""exiftool"", ""sidecar"")),
        (""exiftool_merge_persons"", (""exiftool"", ""sidecar"")),
    ]
    try:
        cfg.validate(exclusive=exclusive_options, dependent=dependent_options, cli=True)
    except ConfigOptionsInvalidError as e:
        click.echo(
            click.style(
                f""Incompatible export options: {e.message}"", fg=CLI_COLOR_ERROR
            ),
            err=True,
        )
        raise click.Abort()

    if all(x in [s.lower() for s in sidecar] for x in [""json"", ""exiftool""]):
        click.echo(
            click.style(
                ""Cannot use --sidecar json with --sidecar exiftool due to name collisions"",
                fg=CLI_COLOR_ERROR,
            ),
            err=True,
        )
        raise click.Abort()

    if xattr_template:
        for attr, _ in xattr_template:
            if attr not in EXTENDED_ATTRIBUTE_NAMES:
                click.echo(
                    click.style(
                        f""Invalid attribute '{attr}' for --xattr-template; ""
                        f""valid values are {', '.join(EXTENDED_ATTRIBUTE_NAMES_QUOTED)}"",
                        fg=CLI_COLOR_ERROR,
                    ),
                    err=True,
                )
                raise click.Abort()

    if save_config:
        verbose_(f""Saving options to file {save_config}"")
        cfg.write_to_file(save_config)

    # set defaults for options that need them
    jpeg_quality = DEFAULT_JPEG_QUALITY if jpeg_quality is None else jpeg_quality
    edited_suffix = DEFAULT_EDITED_SUFFIX if edited_suffix is None else edited_suffix
    original_suffix = (
        DEFAULT_ORIGINAL_SUFFIX if original_suffix is None else original_suffix
    )
    preview_suffix = (
        DEFAULT_PREVIEW_SUFFIX if preview_suffix is None else preview_suffix
    )
    retry = 0 if not retry else retry

    if not os.path.isdir(dest):
        click.echo(
            click.style(f""DEST {dest} must be valid path"", fg=CLI_COLOR_ERROR), err=True
        )
        raise click.Abort()

    dest = str(pathlib.Path(dest).resolve())

    if report and os.path.isdir(report):
        click.echo(
            click.style(
                f""report is a directory, must be file name"", fg=CLI_COLOR_ERROR
            ),
            err=True,
        )
        raise click.Abort()

    # if use_photokit and not check_photokit_authorization():
    #     click.echo(
    #         ""Requesting access to use your Photos library. Click 'OK' on the dialog box to grant access.""
    #     )
    #     request_photokit_authorization()
    #     click.confirm(""Have you granted access?"")
    #     if not check_photokit_authorization():
    #         click.echo(
    #             ""Failed to get access to the Photos library which is needed with `--use-photokit`.""
    #         )
    #         return

    # initialize export flags
    # by default, will export all versions of photos unless skip flag is set
    (export_edited, export_bursts, export_live, export_raw) = [
        not x for x in [skip_edited, skip_bursts, skip_live, skip_raw]
    ]

    # verify exiftool installed and in path if path not provided and exiftool will be used
    # NOTE: this won't catch use of {exiftool:} in a template
    # but those will raise error during template eval if exiftool path not set
    if (
        any([exiftool, exiftool_merge_keywords, exiftool_merge_persons])
        and not exiftool_path
    ):
        try:
            exiftool_path = get_exiftool_path()
        except FileNotFoundError:
            click.echo(
                click.style(
                    ""Could not find exiftool. Please download and install""
                    "" from https://exiftool.org/"",
                    fg=CLI_COLOR_ERROR,
                ),
                err=True,
            )
            ctx.exit(2)

    if any([exiftool, exiftool_merge_keywords, exiftool_merge_persons]):
        verbose_(f""exiftool path: {exiftool_path}"")

    photos = movies = True  # default searches for everything
    if only_movies:
        photos = False
    if only_photos:
        movies = False

    # load UUIDs if necessary and append to any uuids passed with --uuid
    if uuid_from_file:
        uuid_list = list(uuid)  # Click option is a tuple
        uuid_list.extend(load_uuid_from_file(uuid_from_file))
        uuid = tuple(uuid_list)

    # below needed for to make CliRunner work for testing
    cli_db = cli_obj.db if cli_obj is not None else None
    db = get_photos_db(*photos_library, db, cli_db)
    if db is None:
        click.echo(cli.commands[""export""].get_help(ctx), err=True)
        click.echo(""\n\nLocated the following Photos library databases: "", err=True)
        _list_libraries()
        return

    # sanity check exportdb
    if exportdb and exportdb != OSXPHOTOS_EXPORT_DB:
        if ""/"" in exportdb:
            click.echo(
                click.style(
                    f""Error: --exportdb must be specified as filename not path; ""
                    + f""export database will saved in export directory '{dest}'."",
                    fg=CLI_COLOR_ERROR,
                )
            )
            raise click.Abort()
        elif pathlib.Path(pathlib.Path(dest) / OSXPHOTOS_EXPORT_DB).exists():
            click.echo(
                click.style(
                    f""Warning: export database is '{exportdb}' but found '{OSXPHOTOS_EXPORT_DB}' in {dest}; using '{exportdb}'"",
                    fg=CLI_COLOR_WARNING,
                )
            )

    # open export database and assign copy/link/unlink functions
    export_db_path = os.path.join(dest, exportdb or OSXPHOTOS_EXPORT_DB)

    # check that export isn't in the parent or child of a previously exported library
    other_db_files = find_files_in_branch(dest, OSXPHOTOS_EXPORT_DB)
    if other_db_files:
        click.echo(
            click.style(
                ""WARNING: found other export database files in this destination directory branch.  ""
                + ""This likely means you are attempting to export files into a directory ""
                + ""that is either the parent or a child directory of a previous export. ""
                + ""Proceeding may cause your exported files to be overwritten."",
                fg=CLI_COLOR_WARNING,
            ),
            err=True,
        )
        click.echo(
            f""You are exporting to {dest}, found {OSXPHOTOS_EXPORT_DB} files in:""
        )
        for other_db in other_db_files:
            click.echo(f""{other_db}"")
        click.confirm(""Do you want to continue?"", abort=True)

    if dry_run:
        export_db = ExportDBInMemory(export_db_path)
        fileutil = FileUtilNoOp
    else:
        export_db = ExportDB(export_db_path)
        fileutil = FileUtil

    if verbose_:
        if export_db.was_created:
            verbose_(f""Created export database {export_db_path}"")
        else:
            verbose_(f""Using export database {export_db_path}"")
        upgraded = export_db.was_upgraded
        if upgraded:
            verbose_(
                f""Upgraded export database {export_db_path} from version {upgraded[0]} to {upgraded[1]}""
            )

    photosdb = osxphotos.PhotosDB(dbfile=db, verbose=verbose_, exiftool=exiftool_path)

    # enable beta features if requested
    photosdb._beta = beta

    query_options = QueryOptions(
        keyword=keyword,
        person=person,
        album=album,
        folder=folder,
        uuid=uuid,
        title=title,
        no_title=no_title,
        description=description,
        no_description=no_description,
        ignore_case=ignore_case,
        edited=edited,
        external_edit=external_edit,
        favorite=favorite,
        not_favorite=not_favorite,
        hidden=hidden,
        not_hidden=not_hidden,
        missing=missing,
        not_missing=None,
        shared=shared,
        not_shared=not_shared,
        photos=photos,
        movies=movies,
        uti=uti,
        burst=burst,
        not_burst=not_burst,
        live=live,
        not_live=not_live,
        cloudasset=False,
        not_cloudasset=False,
        incloud=False,
        not_incloud=False,
        from_date=from_date,
        to_date=to_date,
        from_time=from_time,
        to_time=to_time,
        portrait=portrait,
        not_portrait=not_portrait,
        screenshot=screenshot,
        not_screenshot=not_screenshot,
        slow_mo=slow_mo,
        not_slow_mo=not_slow_mo,
        time_lapse=time_lapse,
        not_time_lapse=not_time_lapse,
        hdr=hdr,
        not_hdr=not_hdr,
        selfie=selfie,
        not_selfie=not_selfie,
        panorama=panorama,
        not_panorama=not_panorama,
        has_raw=has_raw,
        place=place,
        no_place=no_place,
        location=location,
        no_location=no_location,
        label=label,
        deleted=deleted,
        deleted_only=deleted_only,
        has_comment=has_comment,
        no_comment=no_comment,
        has_likes=has_likes,
        no_likes=no_likes,
        is_reference=is_reference,
        in_album=in_album,
        not_in_album=not_in_album,
        burst_photos=export_bursts,
        # skip missing bursts if using --download-missing by itself as AppleScript otherwise causes errors
        missing_bursts=(download_missing and use_photokit) or not download_missing,
        name=name,
        min_size=min_size,
        max_size=max_size,
        regex=regex,
        selected=selected,
        query_eval=query_eval,
        function=query_function,
        duplicate=duplicate,
    )

    try:
        photos = photosdb.query(query_options)
    except ValueError as e:
        if ""Invalid query_eval CRITERIA:"" in str(e):
            msg = str(e).split("":"")[1]
            raise click.BadOptionUsage(
                ""query_eval"", f""Invalid query-eval CRITERIA: {msg}""
            )
        else:
            raise ValueError(e)

    if photos and only_new:
        # ignore previously exported files
        previous_uuids = {uuid: 1 for uuid in export_db.get_previous_uuids()}
        photos = [p for p in photos if p.uuid not in previous_uuids]

    # store results of export
    results = ExportResults()

    if photos:
        num_photos = len(photos)
        # TODO: photos or photo appears several times, pull into a separate function
        photo_str = ""photos"" if num_photos > 1 else ""photo""
        click.echo(f""Exporting {num_photos} {photo_str} to {dest}..."")
        start_time = time.perf_counter()
        # though the command line option is current_name, internally all processing
        # logic uses original_name which is the boolean inverse of current_name
        # because the original code used --original-name as an option
        original_name = not current_name

        # set up for --add-export-to-album if needed
        album_export = (
            PhotosAlbum(add_exported_to_album, verbose=verbose_)
            if add_exported_to_album
            else None
        )
        album_skipped = (
            PhotosAlbum(add_skipped_to_album, verbose=verbose_)
            if add_skipped_to_album
            else None
        )
        album_missing = (
            PhotosAlbum(add_missing_to_album, verbose=verbose_)
            if add_missing_to_album
            else None
        )

        # send progress bar output to /dev/null if verbose to hide the progress bar
        fp = open(os.devnull, ""w"") if verbose else None
        with click.progressbar(photos, file=fp) as bar:
            for p in bar:
                export_results = export_photo(
                    photo=p,
                    dest=dest,
                    verbose=verbose,
                    export_by_date=export_by_date,
                    sidecar=sidecar,
                    sidecar_drop_ext=sidecar_drop_ext,
                    update=update,
                    ignore_signature=ignore_signature,
                    export_as_hardlink=export_as_hardlink,
                    overwrite=overwrite,
                    export_edited=export_edited,
                    skip_original_if_edited=skip_original_if_edited,
                    original_name=original_name,
                    export_live=export_live,
                    download_missing=download_missing,
                    exiftool=exiftool,
                    exiftool_merge_keywords=exiftool_merge_keywords,
                    exiftool_merge_persons=exiftool_merge_persons,
                    directory=directory,
                    filename_template=filename_template,
                    export_raw=export_raw,
                    album_keyword=album_keyword,
                    person_keyword=person_keyword,
                    keyword_template=keyword_template,
                    description_template=description_template,
                    export_db=export_db,
                    fileutil=fileutil,
                    dry_run=dry_run,
                    touch_file=touch_file,
                    edited_suffix=edited_suffix,
                    original_suffix=original_suffix,
                    use_photos_export=use_photos_export,
                    convert_to_jpeg=convert_to_jpeg,
                    jpeg_quality=jpeg_quality,
                    ignore_date_modified=ignore_date_modified,
                    use_photokit=use_photokit,
                    exiftool_option=exiftool_option,
                    strip=strip,
                    jpeg_ext=jpeg_ext,
                    replace_keywords=replace_keywords,
                    retry=retry,
                    export_dir=dest,
                    export_preview=preview,
                    preview_suffix=preview_suffix,
                    preview_if_missing=preview_if_missing,
                )

                if post_function:
                    for function in post_function:
                        # post function is tuple of (function, filename.py::function_name)
                        verbose_(f""Calling post-function {function[1]}"")
                        if not dry_run:
                            try:
                                function[0](p, export_results, verbose_)
                            except Exception as e:
                                click.secho(
                                    f""Error running post-function {function[1]}: {e}"",
                                    fg=CLI_COLOR_ERROR,
                                    err=True,
                                )

                run_post_command(
                    photo=p,
                    post_command=post_command,
                    export_results=export_results,
                    export_dir=dest,
                    dry_run=dry_run,
                    exiftool_path=exiftool_path,
                    export_db=export_db,
                )

                if album_export and export_results.exported:
                    try:
                        album_export.add(p)
                        export_results.exported_album = [
                            (filename, album_export.name)
                            for filename in export_results.exported
                        ]
                    except Exception as e:
                        click.secho(
                            f""Error adding photo {p.original_filename} ({p.uuid}) to album {album_export.name}: {e}"",
                            fg=CLI_COLOR_ERROR,
                            err=True,
                        )

                if album_skipped and export_results.skipped:
                    try:
                        album_skipped.add(p)
                        export_results.skipped_album = [
                            (filename, album_skipped.name)
                            for filename in export_results.skipped
                        ]
                    except Exception as e:
                        click.secho(
                            f""Error adding photo {p.original_filename} ({p.uuid}) to album {album_skipped.name}: {e}"",
                            fg=CLI_COLOR_ERROR,
                            err=True,
                        )

                if album_missing and export_results.missing:
                    try:
                        album_missing.add(p)
                        export_results.missing_album = [
                            (filename, album_missing.name)
                            for filename in export_results.missing
                        ]
                    except Exception as e:
                        click.secho(
                            f""Error adding photo {p.original_filename} ({p.uuid}) to album {album_missing.name}: {e}"",
                            fg=CLI_COLOR_ERROR,
                            err=True,
                        )

                results += export_results

                # all photo files (not including sidecars) that are part of this export set
                # used below for applying Finder tags, etc.
                photo_files = set(
                    export_results.exported
                    + export_results.new
                    + export_results.updated
                    + export_results.exif_updated
                    + export_results.converted_to_jpeg
                    + export_results.skipped
                )

                if finder_tag_keywords or finder_tag_template:
                    tags_written, tags_skipped = write_finder_tags(
                        p,
                        photo_files,
                        keywords=finder_tag_keywords,
                        keyword_template=keyword_template,
                        album_keyword=album_keyword,
                        person_keyword=person_keyword,
                        exiftool_merge_keywords=exiftool_merge_keywords,
                        finder_tag_template=finder_tag_template,
                        strip=strip,
                        export_dir=dest,
                        export_db=export_db,
                    )
                    results.xattr_written.extend(tags_written)
                    results.xattr_skipped.extend(tags_skipped)

                if xattr_template:
                    xattr_written, xattr_skipped = write_extended_attributes(
                        p,
                        photo_files,
                        xattr_template,
                        strip=strip,
                        export_dir=dest,
                        export_db=export_db,
                    )
                    results.xattr_written.extend(xattr_written)
                    results.xattr_skipped.extend(xattr_skipped)

        if fp is not None:
            fp.close()

        photo_str_total = ""photos"" if len(photos) != 1 else ""photo""
        if update:
            summary = (
                f""Processed: {len(photos)} {photo_str_total}, ""
                f""exported: {len(results.new)}, ""
                f""updated: {len(results.updated)}, ""
                f""skipped: {len(results.skipped)}, ""
                f""updated EXIF data: {len(results.exif_updated)}, ""
            )
        else:
            summary = (
                f""Processed: {len(photos)} {photo_str_total}, ""
                f""exported: {len(results.exported)}, ""
            )
        summary += f""missing: {len(results.missing)}, ""
        summary += f""error: {len(results.error)}""
        if touch_file:
            summary += f"", touched date: {len(results.touched)}""
        click.echo(summary)
        stop_time = time.perf_counter()
        click.echo(f""Elapsed time: {(stop_time-start_time):.3f} seconds"")
    else:
        click.echo(""Did not find any photos to export"")

    # cleanup files and do report if needed
    if cleanup:
        all_files = (
            results.exported
            + results.skipped
            + results.exif_updated
            + results.touched
            + results.converted_to_jpeg
            + results.sidecar_json_written
            + results.sidecar_json_skipped
            + results.sidecar_exiftool_written
            + results.sidecar_exiftool_skipped
            + results.sidecar_xmp_written
            + results.sidecar_xmp_skipped
            # include missing so a file that was already in export directory
            # but was missing on --update doesn't get deleted
            # (better to have old version than none)
            + results.missing
            # include files that have error in case they exist from previous export
            + [r[0] for r in results.error]
            + [str(pathlib.Path(export_db_path).resolve())]
        )
        click.echo(f""Cleaning up {dest}"")
        cleaned_files, cleaned_dirs = cleanup_files(dest, all_files, fileutil)
        file_str = ""files"" if len(cleaned_files) != 1 else ""file""
        dir_str = ""directories"" if len(cleaned_dirs) != 1 else ""directory""
        click.echo(
            f""Deleted: {len(cleaned_files)} {file_str}, {len(cleaned_dirs)} {dir_str}""
        )
        results.deleted_files = cleaned_files
        results.deleted_directories = cleaned_dirs

    if report:
        verbose_(f""Writing export report to {report}"")
        write_export_report(report, results)

    export_db.close()","for function in post_function:
    verbose_(f'Calling post-function {function[1]}')
    if not dry_run:
        try:
            function[0](p, export_results, verbose_)
        except Exception as e:
            click.secho(f'Error running post-function {function[1]}: {e}', fg=CLI_COLOR_ERROR, err=True)","for function in post_function:
    (function_0, function_1, *_) = function
    verbose_(f'Calling post-function {function[1]}')
    if not dry_run:
        try:
            function[0](p, export_results, verbose_)
        except Exception as e:
            click.secho(f'Error running post-function {function[1]}: {e}', fg=CLI_COLOR_ERROR, err=True)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
faker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faker/tests/providers/test_ssn.py,https://github.com/joke2k/faker/tree/master/tests/providers/test_ssn.py,TestTrTr,first_part_non_zero$1041,"def first_part_non_zero(self):
        for sample in self.samples:
            assert sample[0] != 0","for sample in self.samples:
    assert sample[0] != 0","for sample in self.samples:
    (sample_0, *sample_rsamplemaining) = sample
    assert sample[0] != 0","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
rtfm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rtfm/rtfm.py,https://github.com/leostat/rtfm/tree/master//rtfm.py,,dbInsertCmd$386,"def dbInsertCmd(conn, cmds):
	cur = conn.cursor()
	cur.execute(""SELECT max(cmdid) from tblcommand"")
	max_id = cur.fetchall()
	if options.debug:
		for cmd in cmds:
			debug(""I: INSERT INTO tblcommand VALUES (NULL, '""+str(cmd[0])+""', '""+str(cmd[1]) \
				+""', '""+str(cmd[2])+""', ""+""date('now'))"")
	cur.executemany('INSERT INTO tblcommand VALUES (NULL, ?, ?, ?, date(""now""));', cmds)
	conn.commit()
	ok(""Added Rows : "" + str(cur.rowcount))
	cur.execute(""SELECT max(cmdid) FROM tblcommand"")
	new_max_id = cur.fetchall()
	ok(""New Top ID : "" + str(new_max_id[0][0]) + "" | Number of CMD's Added : "" \
		+ str(new_max_id[0][0]-max_id[0][0]))","for cmd in cmds:
    debug(""I: INSERT INTO tblcommand VALUES (NULL, '"" + str(cmd[0]) + ""', '"" + str(cmd[1]) + ""', '"" + str(cmd[2]) + ""', "" + ""date('now'))"")","for cmd in cmds:
    (cmd_0, cmd_1, cmd_2, *_) = cmd
    debug(""I: INSERT INTO tblcommand VALUES (NULL, '"" + str(cmd[0]) + ""', '"" + str(cmd[1]) + ""', '"" + str(cmd[2]) + ""', "" + ""date('now'))"")","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/base/plugins/agent_based/esx_vsphere_counters.py,https://github.com/tribe29/checkmk/tree/master/cmk/base/plugins/agent_based/esx_vsphere_counters.py,,_max_latency$252,"def _max_latency(latencies: SubSectionCounter) -> Optional[int]:
    all_latencies: List[int] = []
    for data in latencies.values():
        multivalues, _unit = data[0]
        all_latencies.extend(map(int, multivalues))
    return max(all_latencies) if all_latencies else None","for data in latencies.values():
    (multivalues, _unit) = data[0]
    all_latencies.extend(map(int, multivalues))","for data in latencies.values():
    (data_0, *data_rdatamaining) = data
    (multivalues, _unit) = data[0]
    all_latencies.extend(map(int, multivalues))","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
dash,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dash/dash/testing/application_runners.py,https://github.com/plotly/dash/tree/master/dash/testing/application_runners.py,JuliaRunner,start$340,"def start(self, app, start_timeout=30, cwd=None):
        """"""Start the server with subprocess and julia.""""""

        if os.path.isfile(app) and os.path.exists(app):
            # app is already a file in a dir - use that as cwd
            if not cwd:
                cwd = os.path.dirname(app)
                logger.info(""JuliaRunner inferred cwd from app path: %s"", cwd)
        else:
            # app is a string chunk, we make a temporary folder to store app.jl
            # and its relevant assets
            self._tmp_app_path = os.path.join(
                ""/tmp"" if not self.is_windows else os.getenv(""TEMP""), uuid.uuid4().hex
            )
            try:
                os.mkdir(self.tmp_app_path)
            except OSError:
                logger.exception(""cannot make temporary folder %s"", self.tmp_app_path)
            path = os.path.join(self.tmp_app_path, ""app.jl"")

            logger.info(""JuliaRunner start => app is Julia code chunk"")
            logger.info(""make a temporary Julia file for execution => %s"", path)
            logger.debug(""content of the Dash.jl app"")
            logger.debug(""%s"", app)

            with open(path, ""w"") as fp:
                fp.write(app)

            app = path

            # try to find the path to the calling script to use as cwd
            if not cwd:
                for entry in inspect.stack():
                    if ""/dash/testing/"" not in entry[1].replace(""\\"", ""/""):
                        cwd = os.path.dirname(os.path.realpath(entry[1]))
                        logger.warning(""get cwd from inspect => %s"", cwd)
                        break
            if cwd:
                logger.info(
                    ""JuliaRunner inferred cwd from the Python call stack: %s"", cwd
                )

                # try copying all valid sub folders (i.e. assets) in cwd to tmp
                # note that the R assets folder name can be any valid folder name
                assets = [
                    os.path.join(cwd, _)
                    for _ in os.listdir(cwd)
                    if not _.startswith(""__"") and os.path.isdir(os.path.join(cwd, _))
                ]

                for asset in assets:
                    target = os.path.join(self.tmp_app_path, os.path.basename(asset))
                    if os.path.exists(target):
                        logger.debug(""delete existing target %s"", target)
                        shutil.rmtree(target)
                    logger.debug(""copying %s => %s"", asset, self.tmp_app_path)
                    shutil.copytree(asset, target)
                    logger.debug(""copied with %s"", os.listdir(target))

            else:
                logger.warning(
                    ""JuliaRunner found no cwd in the Python call stack. ""
                    ""You may wish to specify an explicit working directory ""
                    ""using something like: ""
                    ""dashjl.run_server(app, cwd=os.path.dirname(__file__))""
                )

        logger.info(""Run Dash.jl app with julia => %s"", app)

        args = shlex.split(
            ""julia {}"".format(os.path.realpath(app)),
            posix=not self.is_windows,
        )
        logger.debug(""start Dash.jl process with %s"", args)

        try:
            self.proc = subprocess.Popen(  # pylint: disable=consider-using-with
                args,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=self.tmp_app_path if self.tmp_app_path else cwd,
            )
            # wait until server is able to answer http request
            wait.until(lambda: self.accessible(self.url), timeout=start_timeout)

        except (OSError, ValueError):
            logger.exception(""process server has encountered an error"")
            self.started = False
            return

        self.started = True","for entry in inspect.stack():
    if '/dash/testing/' not in entry[1].replace('\\', '/'):
        cwd = os.path.dirname(os.path.realpath(entry[1]))
        logger.warning('get cwd from inspect => %s', cwd)
        break","for entry in inspect.stack():
    (entry_0, entry_1, *entry_rentrymaining) = entry
    if '/dash/testing/' not in entry[1].replace('\\', '/'):
        cwd = os.path.dirname(os.path.realpath(entry[1]))
        logger.warning('get cwd from inspect => %s', cwd)
        break","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
geojson,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/geojson/tests/test_utils.py,https://github.com/jazzband/geojson/tree/master/tests/test_utils.py,,check_polygon_bbox$24,"def check_polygon_bbox(polygon, bbox):
    min_lon, min_lat, max_lon, max_lat = bbox
    eps = 1e-3
    for linear_ring in polygon['coordinates']:
        for coordinate in linear_ring:
            if not (min_lon-eps <= coordinate[0] <= max_lon+eps
                    and min_lat-eps <= coordinate[1] <= max_lat+eps):
                return False
    return True","for coordinate in linear_ring:
    if not (min_lon - eps <= coordinate[0] <= max_lon + eps and min_lat - eps <= coordinate[1] <= max_lat + eps):
        return False","for coordinate in linear_ring:
    (coordinate_0, coordinate_1, *_) = coordinate
    if not (min_lon - eps <= coordinate[0] <= max_lon + eps and min_lat - eps <= coordinate[1] <= max_lat + eps):
        return False","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
cloudtracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloudtracker/cloudtracker/datasources/athena.py,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,__init__$164,"def __init__(self, config, account, start, end, args):
        # Mute boto except errors
        logging.getLogger(""botocore"").setLevel(logging.WARN)
        logging.info(
            ""Source of CloudTrail logs: s3://{bucket}/{path}"".format(
                bucket=config[""s3_bucket""], path=config[""path""]
            )
        )

        # Check start date is not older than a year, as we only create partitions for that far back
        if (
            datetime.datetime.now() - datetime.datetime.strptime(start, ""%Y-%m-%d"")
        ).days > 365:
            raise Exception(
                ""Start date is over a year old. CloudTracker does not create or use partitions over a year old.""
            )

        #
        # Create date filtering
        #
        month_restrictions = set()
        start = start.split(""-"")
        end = end.split(""-"")

        if start[0] == end[0]:
            for month in range(int(start[1]), int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
        else:
            # Add restrictions for months in start year
            for month in range(int(start[1]), 12 + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
            # Add restrictions for months in middle years
            for year in range(int(start[0]), int(end[0])):
                for month in (1, 12 + 1):
                    month_restrictions.add(
                        ""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month)
                    )
            # Add restrictions for months in final year
            for month in range(1, int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month)
                )

        # Combine date filters and add error filter
        self.search_filter = (
            ""(("" + "" or "".join(month_restrictions) + "") and errorcode IS NULL)""
        )

        self.table_name = ""cloudtrail_logs_{}"".format(account[""id""])

        #
        # Display the AWS identity (doubles as a check that boto creds are setup)
        #
        sts = boto3.client(""sts"")
        identity = sts.get_caller_identity()
        logging.info(""Using AWS identity: {}"".format(identity[""Arn""]))
        current_account_id = identity[""Account""]
        region = boto3.session.Session().region_name

        if ""output_s3_bucket"" in config:
            self.output_bucket = config[""output_s3_bucket""]
        else:
            self.output_bucket = ""s3://aws-athena-query-results-{}-{}"".format(
                current_account_id, region
            )
        logging.info(""Using output bucket: {}"".format(self.output_bucket))

        if ""workgroup"" in config:
            self.workgroup = config[""workgroup""]
        logging.info(""Using workgroup: {}"".format(self.workgroup))

        if not config.get('org_id'):
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], account_id=account[""id""]
            )
        else:
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], org_id=config[""org_id""], account_id=account[""id""]
            )

        logging.info(""Account cloudtrail log path: {}"".format(cloudtrail_log_path))

        # Open connections to needed AWS services
        self.athena = boto3.client(""athena"")
        self.s3 = boto3.client(""s3"")

        if args.skip_setup:
            logging.info(""Skipping initial table creation"")
            return

        # Check we can access the S3 bucket
        resp = self.s3.list_objects_v2(
            Bucket=config[""s3_bucket""], Prefix=config[""path""], MaxKeys=1
        )
        if ""Contents"" not in resp or len(resp[""Contents""]) == 0:
            exit(
                ""ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}"".format(
                    bucket=config[""s3_bucket""], path=config[""path""]
                )
            )

        # Ensure our database exists
        self.query_athena(
            ""CREATE DATABASE IF NOT EXISTS {db} {comment}"".format(
                db=self.database, comment=""COMMENT 'Created by CloudTracker'""
            ),
            context=None,
        )

        #
        # Set up table
        #
        query = """"""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (
            `eventversion` string COMMENT 'from deserializer', 
            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', 
            `eventtime` string COMMENT 'from deserializer', 
            `eventsource` string COMMENT 'from deserializer', 
            `eventname` string COMMENT 'from deserializer', 
            `awsregion` string COMMENT 'from deserializer', 
            `sourceipaddress` string COMMENT 'from deserializer', 
            `useragent` string COMMENT 'from deserializer', 
            `errorcode` string COMMENT 'from deserializer', 
            `errormessage` string COMMENT 'from deserializer', 
            `requestparameters` string COMMENT 'from deserializer', 
            `responseelements` string COMMENT 'from deserializer', 
            `additionaleventdata` string COMMENT 'from deserializer', 
            `requestid` string COMMENT 'from deserializer', 
            `eventid` string COMMENT 'from deserializer', 
            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', 
            `eventtype` string COMMENT 'from deserializer', 
            `apiversion` string COMMENT 'from deserializer', 
            `readonly` string COMMENT 'from deserializer', 
            `recipientaccountid` string COMMENT 'from deserializer', 
            `serviceeventdetails` string COMMENT 'from deserializer', 
            `sharedeventid` string COMMENT 'from deserializer', 
            `vpcendpointid` string COMMENT 'from deserializer')
            PARTITIONED BY (region string, year string, month string)
            ROW FORMAT SERDE 
            'com.amazon.emr.hive.serde.CloudTrailSerde' 
            STORED AS INPUTFORMAT 
            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' 
            OUTPUTFORMAT 
            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
            LOCATION '{cloudtrail_log_path}'"""""".format(
            table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path
        )
        self.query_athena(query)

        #
        # Create partitions
        #

        logging.info(
            ""Checking if all partitions for the past {} months exist"".format(
                NUM_MONTHS_FOR_PARTITIONS
            )
        )

        # Get list of current partitions
        query = ""SHOW PARTITIONS {table_name}"".format(table_name=self.table_name)
        partition_list = self.query_athena(query, skip_header=False)

        partition_set = set()
        for partition in partition_list:
            partition_set.add(partition[0])

        # Get region list. Using ec2 here just because it exists in all regions.
        regions = boto3.session.Session().get_available_regions(""ec2"")

        queries_to_make = set()

        # Iterate over every month for the past year and build queries to run to create partitions
        for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
            date_of_interest = datetime.datetime.now() - relativedelta(
                months=num_months_ago
            )
            year = date_of_interest.year
            month = ""{:0>2}"".format(date_of_interest.month)

            query = """"

            for region in regions:
                if (
                    ""region={region}/year={year}/month={month}"".format(
                        region=region, year=year, month=month
                    )
                    in partition_set
                ):
                    continue

                query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(
                    region=region,
                    year=year,
                    month=month,
                    cloudtrail_log_path=cloudtrail_log_path,
                )
            if query != """":
                queries_to_make.add(
                    ""ALTER TABLE {table_name} ADD "".format(table_name=self.table_name)
                    + query
                )

        # Run the queries
        query_count = len(queries_to_make)
        for query in queries_to_make:
            logging.info(""Partition groups remaining to create: {}"".format(query_count))
            self.query_athena(query)
            query_count -= 1","for partition in partition_list:
    partition_set.add(partition[0])","for partition in partition_list:
    (partition_0, *partition_rpartitionmaining) = partition
    partition_set.add(partition[0])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
redis-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/redis-py/redis/commands/timeseries/utils.py,https://github.com/redis/redis-py/tree/master/redis/commands/timeseries/utils.py,,parse_m_range$13,"def parse_m_range(response):
    """"""Parse multi range response. Used by TS.MRANGE and TS.MREVRANGE.""""""
    res = []
    for item in response:
        res.append({nativestr(item[0]): [list_to_dict(item[1]), parse_range(item[2])]})
    return sorted(res, key=lambda d: list(d.keys()))","for item in response:
    res.append({nativestr(item[0]): [list_to_dict(item[1]), parse_range(item[2])]})","for item in response:
    (item_0, item_1, item_2, *_) = item
    res.append({nativestr(item[0]): [list_to_dict(item[1]), parse_range(item[2])]})","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/examples/pathfinding/mob_graph_test_navmesh.py,https://github.com/kidscancode/pygame_tutorials/tree/master/examples/pathfinding/mob_graph_test_navmesh.py,WeightedMesh,draw$121,"def draw(self):
        for node in self.edges.keys():
            x = int(node[0] * TILESIZE + TILESIZE / 2)
            y = int(node[1] * TILESIZE + TILESIZE / 2)
            pg.draw.circle(screen, CYAN, (x, y), 10)
            for c in self.edges[node]:
                cx = c[0] * TILESIZE + TILESIZE / 2
                cy = c[1] * TILESIZE + TILESIZE / 2
                pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)","for node in self.edges.keys():
    x = int(node[0] * TILESIZE + TILESIZE / 2)
    y = int(node[1] * TILESIZE + TILESIZE / 2)
    pg.draw.circle(screen, CYAN, (x, y), 10)
    for c in self.edges[node]:
        cx = c[0] * TILESIZE + TILESIZE / 2
        cy = c[1] * TILESIZE + TILESIZE / 2
        pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)","for node in self.edges.keys():
    (node_0, node_1, *_) = node
    x = int(node[0] * TILESIZE + TILESIZE / 2)
    y = int(node[1] * TILESIZE + TILESIZE / 2)
    pg.draw.circle(screen, CYAN, (x, y), 10)
    for c in self.edges[node]:
        cx = c[0] * TILESIZE + TILESIZE / 2
        cy = c[1] * TILESIZE + TILESIZE / 2
        pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/examples/pathfinding/mob_graph_test_navmesh.py,https://github.com/kidscancode/pygame_tutorials/tree/master/examples/pathfinding/mob_graph_test_navmesh.py,WeightedMesh,draw$121,"def draw(self):
        for node in self.edges.keys():
            x = int(node[0] * TILESIZE + TILESIZE / 2)
            y = int(node[1] * TILESIZE + TILESIZE / 2)
            pg.draw.circle(screen, CYAN, (x, y), 10)
            for c in self.edges[node]:
                cx = c[0] * TILESIZE + TILESIZE / 2
                cy = c[1] * TILESIZE + TILESIZE / 2
                pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)","for c in self.edges[node]:
    cx = c[0] * TILESIZE + TILESIZE / 2
    cy = c[1] * TILESIZE + TILESIZE / 2
    pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)","for c in self.edges[node]:
    (c_0, c_1, *_) = c
    cx = c[0] * TILESIZE + TILESIZE / 2
    cy = c[1] * TILESIZE + TILESIZE / 2
    pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/datadog_checks_dev/tests/tooling/config_validator/test_config_block.py,https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/tests/tooling/config_validator/test_config_block.py,,test_is_comment$216,"def test_is_comment():
    dir = os.path.dirname(__file__)
    test_file = open(os.path.join(dir, 'test_config_block_5.yaml'), 'r')
    config_lines = test_file.read().split('\n')
    test_file.close()

    test_cases = [
        # idx, indent, expected
        (0, 0, False),
        (0, 2, False),
        (6, 0, True),
        (6, 2, True),
        (12, 0, True),
        (12, 2, True),
    ]

    test_cases_with_errors = [(12, 0, True, ""Comment block incorrectly indented"")]

    for c in test_cases:
        status = _is_comment(c[0], config_lines, c[1], [])
        assert status == c[2]

    for c in test_cases_with_errors:
        errors = []
        status = _is_comment(c[0], config_lines, c[1], errors)
        assert status == c[2]
        assert len(errors) == 1
        assert errors[0].error_str == c[3]","for c in test_cases:
    status = _is_comment(c[0], config_lines, c[1], [])
    assert status == c[2]","for c in test_cases:
    (c_0, c_1, c_2, *_) = c
    status = _is_comment(c[0], config_lines, c[1], [])
    assert status == c[2]","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/datadog_checks_dev/tests/tooling/config_validator/test_config_block.py,https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/tests/tooling/config_validator/test_config_block.py,,test_is_comment$216,"def test_is_comment():
    dir = os.path.dirname(__file__)
    test_file = open(os.path.join(dir, 'test_config_block_5.yaml'), 'r')
    config_lines = test_file.read().split('\n')
    test_file.close()

    test_cases = [
        # idx, indent, expected
        (0, 0, False),
        (0, 2, False),
        (6, 0, True),
        (6, 2, True),
        (12, 0, True),
        (12, 2, True),
    ]

    test_cases_with_errors = [(12, 0, True, ""Comment block incorrectly indented"")]

    for c in test_cases:
        status = _is_comment(c[0], config_lines, c[1], [])
        assert status == c[2]

    for c in test_cases_with_errors:
        errors = []
        status = _is_comment(c[0], config_lines, c[1], errors)
        assert status == c[2]
        assert len(errors) == 1
        assert errors[0].error_str == c[3]","for c in test_cases_with_errors:
    errors = []
    status = _is_comment(c[0], config_lines, c[1], errors)
    assert status == c[2]
    assert len(errors) == 1
    assert errors[0].error_str == c[3]","for c in test_cases_with_errors:
    (c_0, c_1, c_2, c_3, *_) = c
    errors = []
    status = _is_comment(c[0], config_lines, c[1], errors)
    assert status == c[2]
    assert len(errors) == 1
    assert errors[0].error_str == c[3]","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/tests/providers/aws/test_default.py,https://github.com/cloudtools/stacker/tree/master/stacker/tests/providers/aws/test_default.py,TestProviderInteractiveMode,test_select_update_method$945,"def test_select_update_method(self):
        for i in [[{'force_interactive': False,
                    'force_change_set': False},
                   self.provider.interactive_update_stack],
                  [{'force_interactive': True,
                    'force_change_set': False},
                   self.provider.interactive_update_stack],
                  [{'force_interactive': False,
                    'force_change_set': True},
                   self.provider.interactive_update_stack],
                  [{'force_interactive': True,
                    'force_change_set': True},
                   self.provider.interactive_update_stack]]:
            self.assertEquals(
                self.provider.select_update_method(**i[0]),
                i[1]
            )","for i in [[{'force_interactive': False, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
    self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","for i in [[{'force_interactive': False, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
    (i_0, i_1, *_) = i
    self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
ezdxf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/path/converter.py,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/path/converter.py,,to_bsplines_and_vertices$837,"def to_bsplines_and_vertices(
    path: Path, g1_tol: float = G1_TOL
) -> Iterator[PathParts]:
    """"""Convert a :class:`Path` object into multiple cubic B-splines and
    polylines as lists of vertices. Breaks adjacent Bèzier without G1
    continuity into separated B-splines.

    Args:
        path: :class:`Path` objects
        g1_tol: tolerance for G1 continuity check

    Returns:
        :class:`~ezdxf.math.BSpline` and lists of :class:`~ezdxf.math.Vec3`

    """"""
    from ezdxf.math import bezier_to_bspline

    def to_vertices():
        points = [polyline[0][0]]
        for line in polyline:
            points.append(line[1])
        return points

    def to_bspline():
        b1 = bezier[0]
        _g1_continuity_curves = [b1]
        for b2 in bezier[1:]:
            if have_bezier_curves_g1_continuity(b1, b2, g1_tol):
                _g1_continuity_curves.append(b2)
            else:
                yield bezier_to_bspline(_g1_continuity_curves)
                _g1_continuity_curves = [b2]
            b1 = b2

        if _g1_continuity_curves:
            yield bezier_to_bspline(_g1_continuity_curves)

    curves = []
    for path in tools.single_paths([path]):
        prev = path.start
        for cmd in path:
            if cmd.type == Command.CURVE3_TO:
                curve = Bezier3P([prev, cmd.ctrl, cmd.end])  # type: ignore
            elif cmd.type == Command.CURVE4_TO:
                curve = Bezier4P([prev, cmd.ctrl1, cmd.ctrl2, cmd.end])  # type: ignore
            elif cmd.type == Command.LINE_TO:
                curve = (prev, cmd.end)
            else:
                raise ValueError
            curves.append(curve)
            prev = cmd.end

    bezier: list = []
    polyline: list = []
    for curve in curves:
        if isinstance(curve, tuple):
            if bezier:
                yield from to_bspline()
                bezier.clear()
            polyline.append(curve)
        else:
            if polyline:
                yield to_vertices()
                polyline.clear()
            bezier.append(curve)

    if bezier:
        yield from to_bspline()
    if polyline:
        yield to_vertices()","for line in polyline:
    points.append(line[1])","for line in polyline:
    (_, line_1, *line_rlinemaining) = line
    points.append(line[1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
opentelemetry-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opentelemetry-python/propagator/opentelemetry-propagator-jaeger/tests/test_jaeger_propagator.py,https://github.com/open-telemetry/opentelemetry-python/tree/master/propagator/opentelemetry-propagator-jaeger/tests/test_jaeger_propagator.py,TestJaegerPropagator,test_fields$181,"def test_fields(self):
        tracer = trace.TracerProvider().get_tracer(""sdk_tracer_provider"")
        mock_setter = Mock()
        with tracer.start_as_current_span(""parent""):
            with tracer.start_as_current_span(""child""):
                FORMAT.inject({}, setter=mock_setter)
        inject_fields = set()
        for call in mock_setter.mock_calls:
            inject_fields.add(call[1][1])
        self.assertEqual(FORMAT.fields, inject_fields)","for call in mock_setter.mock_calls:
    inject_fields.add(call[1][1])","for call in mock_setter.mock_calls:
    (_, call_1, *call_rcallmaining) = call
    inject_fields.add(call[1][1])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",0,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1[1]: e[1][1]",,,,,,,
Source-Code-from-Tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Source-Code-from-Tutorials/Pygame/24_PythonGameDevelopment.py,https://github.com/buckyroberts/Source-Code-from-Tutorials/tree/master/Pygame/24_PythonGameDevelopment.py,,snake$27,"def snake(block_size, snakelist):
    for XnY in snakelist:
        pygame.draw.rect(gameDisplay, green, [XnY[0],XnY[1],block_size,block_size])","for XnY in snakelist:
    pygame.draw.rect(gameDisplay, green, [XnY[0], XnY[1], block_size, block_size])","for XnY in snakelist:
    (XnY_0, XnY_1, *_) = XnY
    pygame.draw.rect(gameDisplay, green, [XnY[0], XnY[1], block_size, block_size])","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
aws-parallelcluster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-parallelcluster/awsbatch-cli/src/awsbatch/awsbsub.py,https://github.com/aws/aws-parallelcluster/tree/master/awsbatch-cli/src/awsbatch/awsbsub.py,AWSBsubCommand,run$433,"def run(  # noqa: C901 FIXME
        self,
        job_definition,
        job_name,
        job_queue,
        command,
        nodes=None,
        vcpus=None,
        memory=None,
        array_size=None,
        retry_attempts=1,
        timeout=None,
        dependencies=None,
        env=None,
    ):
        """"""Submit the job.""""""
        try:
            # array properties
            array_properties = {}
            if array_size:
                array_properties.update(size=array_size)

            retry_strategy = {""attempts"": retry_attempts}

            depends_on = dependencies if dependencies else []

            # populate container overrides
            container_overrides = {""command"": command}
            if vcpus:
                container_overrides.update(vcpus=vcpus)
            if memory:
                container_overrides.update(memory=memory)
            # populate environment variables
            environment = []
            for env_var in env:
                environment.append({""name"": env_var[0], ""value"": env_var[1]})
            container_overrides.update(environment=environment)

            # common submission arguments
            submission_args = {
                ""jobName"": job_name,
                ""jobQueue"": job_queue,
                ""dependsOn"": depends_on,
                ""retryStrategy"": retry_strategy,
            }

            if nodes:
                submission_args.update({""jobDefinition"": job_definition})

                target_nodes = ""0:""
                # populate node overrides
                node_overrides = {
                    ""numNodes"": nodes,
                    ""nodePropertyOverrides"": [{""targetNodes"": target_nodes, ""containerOverrides"": container_overrides}],
                }
                submission_args.update({""nodeOverrides"": node_overrides})
                if timeout:
                    submission_args.update({""timeout"": {""attemptDurationSeconds"": timeout}})
            else:
                # Standard submission
                submission_args.update({""jobDefinition"": job_definition})
                submission_args.update({""containerOverrides"": container_overrides})
                submission_args.update({""arrayProperties"": array_properties})
                if timeout:
                    submission_args.update({""timeout"": {""attemptDurationSeconds"": timeout}})

            self.log.debug(""Job submission args: %s"", submission_args)
            response = self.batch_client.submit_job(**submission_args)
            print(""Job %s (%s) has been submitted."" % (response[""jobId""], response[""jobName""]))
        except Exception as e:
            fail(""Error submitting job to AWS Batch. Failed with exception: %s"" % e)","for env_var in env:
    environment.append({'name': env_var[0], 'value': env_var[1]})","for env_var in env:
    (env_var_0, env_var_1, *_) = env_var
    environment.append({'name': env_var[0], 'value': env_var[1]})","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
tmuxomatic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tmuxomatic/windowgram/windowgram_test.py,https://github.com/oxidane/tmuxomatic/tree/master/windowgram/windowgram_test.py,SenseTestCase,runTest$123,"def runTest(self):
        try:
            # Sense test methods
            methods = inspect.getmembers(self.__class__(), predicate=inspect.ismethod)
            methods = [ method for method in methods if method[0].startswith(""test_"") ]
            # Pair with line numbers
            methods = [ (method[1], inspect.getsourcelines(method[1])[1]) for method in methods ]
            # Sort by line numbers
            methods = sorted(methods, key=lambda tup: tup[1])
            # Execute tests in the order they appear
            for method in methods:
                method[0]()
        except AssertionError as e:
            raise e # Forward
        except Exception as e:
            error = ""An error occurred during testing: "" + repr(e) # Show the error in case of failure during test
            raise AssertionError( e )","for method in methods:
    method[0]()","for method in methods:
    (method_0, *method_rmethodmaining) = method
    method[0]()","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
yolov5-face,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yolov5-face/test_widerface.py,https://github.com/deepcam-cn/yolov5-face/tree/master//test_widerface.py,,if_main_my$113,"if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default='runs/train/exp5/weights/last.pt', help='model.pt path(s)')
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.02, help='object confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
    parser.add_argument('--project', default='runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--save_folder', default='./widerface_evaluate/widerface_txt/', type=str, help='Dir to save txt results')
    parser.add_argument('--dataset_folder', default='../WiderFace/val/images/', type=str, help='dataset path')
    parser.add_argument('--folder_pict', default='/yolov5-face/data/widerface/val/wider_val.txt', type=str, help='folder_pict')
    opt = parser.parse_args()
    print(opt)

    # changhy : read folder_pict
    pict_folder = {}
    with open(opt.folder_pict, 'r') as f:
        lines = f.readlines()
        for line in lines:
            line = line.strip().split('/')
            pict_folder[line[-1]] = line[-2]

    # Load model
    device = select_device(opt.device)
    model = attempt_load(opt.weights, map_location=device)  # load FP32 model
    with torch.no_grad():
        # testing dataset
        testset_folder = opt.dataset_folder

        for image_path in tqdm(glob.glob(os.path.join(testset_folder, '*'))):
            if image_path.endswith('.txt'):
                continue
            img0 = cv2.imread(image_path)  # BGR
            if img0 is None:
                print(f'ignore : {image_path}')
                continue
            boxes = detect(model, img0)
            # --------------------------------------------------------------------
            image_name = os.path.basename(image_path)
            txt_name = os.path.splitext(image_name)[0] + "".txt""
            save_name = os.path.join(opt.save_folder, pict_folder[image_name], txt_name)
            dirname = os.path.dirname(save_name)
            if not os.path.isdir(dirname):
                os.makedirs(dirname)
            with open(save_name, ""w"") as fd:
                file_name = os.path.basename(save_name)[:-4] + ""\n""            
                bboxs_num = str(len(boxes)) + ""\n""
                fd.write(file_name)
                fd.write(bboxs_num)
                for box in boxes:
                    fd.write('%d %d %d %d %.03f' % (box[0], box[1], box[2], box[3], box[4] if box[4] <= 1 else 1) + '\n')
        print('done.')","for line in lines:
    line = line.strip().split('/')
    pict_folder[line[-1]] = line[-2]","for line in lines:
    (*line_rlinemaining, line_nlineg2) = line
    line = line.strip().split('/')
    pict_folder[line[-1]] = line[-2]",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg2 = e
variable mapping:
e_neg2: e[-2]",,,,,,,
yolov5-face,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yolov5-face/test_widerface.py,https://github.com/deepcam-cn/yolov5-face/tree/master//test_widerface.py,,if_main_my$113,"if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default='runs/train/exp5/weights/last.pt', help='model.pt path(s)')
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.02, help='object confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
    parser.add_argument('--project', default='runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--save_folder', default='./widerface_evaluate/widerface_txt/', type=str, help='Dir to save txt results')
    parser.add_argument('--dataset_folder', default='../WiderFace/val/images/', type=str, help='dataset path')
    parser.add_argument('--folder_pict', default='/yolov5-face/data/widerface/val/wider_val.txt', type=str, help='folder_pict')
    opt = parser.parse_args()
    print(opt)

    # changhy : read folder_pict
    pict_folder = {}
    with open(opt.folder_pict, 'r') as f:
        lines = f.readlines()
        for line in lines:
            line = line.strip().split('/')
            pict_folder[line[-1]] = line[-2]

    # Load model
    device = select_device(opt.device)
    model = attempt_load(opt.weights, map_location=device)  # load FP32 model
    with torch.no_grad():
        # testing dataset
        testset_folder = opt.dataset_folder

        for image_path in tqdm(glob.glob(os.path.join(testset_folder, '*'))):
            if image_path.endswith('.txt'):
                continue
            img0 = cv2.imread(image_path)  # BGR
            if img0 is None:
                print(f'ignore : {image_path}')
                continue
            boxes = detect(model, img0)
            # --------------------------------------------------------------------
            image_name = os.path.basename(image_path)
            txt_name = os.path.splitext(image_name)[0] + "".txt""
            save_name = os.path.join(opt.save_folder, pict_folder[image_name], txt_name)
            dirname = os.path.dirname(save_name)
            if not os.path.isdir(dirname):
                os.makedirs(dirname)
            with open(save_name, ""w"") as fd:
                file_name = os.path.basename(save_name)[:-4] + ""\n""            
                bboxs_num = str(len(boxes)) + ""\n""
                fd.write(file_name)
                fd.write(bboxs_num)
                for box in boxes:
                    fd.write('%d %d %d %d %.03f' % (box[0], box[1], box[2], box[3], box[4] if box[4] <= 1 else 1) + '\n')
        print('done.')","for box in boxes:
    fd.write('%d %d %d %d %.03f' % (box[0], box[1], box[2], box[3], box[4] if box[4] <= 1 else 1) + '\n')","for box in boxes:
    (box_0, box_1, box_2, box_3, box_4, *_) = box
    fd.write('%d %d %d %d %.03f' % (box[0], box[1], box[2], box[3], box[4] if box[4] <= 1 else 1) + '\n')","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,
workalendar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/workalendar/workalendar/asia/china.py,https://github.com/workalendar/workalendar/tree/master/workalendar/asia/china.py,China,__init__$97,"def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.extra_working_days = []
        for year, data in workdays.items():
            for holiday_name, day_list in data.items():
                for v in day_list:
                    self.extra_working_days.append(date(year, v[0], v[1]))","for v in day_list:
    self.extra_working_days.append(date(year, v[0], v[1]))","for v in day_list:
    (v_0, v_1, *_) = v
    self.extra_working_days.append(date(year, v[0], v[1]))","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
RootTheBox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/handlers/AdminHandlers/AdminGameObjectHandlers.py,https://github.com/moloch--/RootTheBox/tree/master/handlers/AdminHandlers/AdminGameObjectHandlers.py,AdminViewHandler,post$403,"def post(self, *args, **kwargs):
        if args[0] == ""statistics"" or args[0] == ""game_objects"":
            uri = {
                ""game_objects"": ""admin/view/game_objects.html"",
                ""statistics"": ""admin/view/statistics.html"",
            }
            flag_uuid = self.get_argument(""flag_uuid"", """")
            team_uuid = self.get_argument(""team_uuid"", """")
            user_uuid = self.get_argument(""user_uuid"", """")
            flag = Flag.by_uuid(flag_uuid)
            team = Team.by_uuid(team_uuid)
            user = User.by_uuid(user_uuid)
            errors = []
            success = []
            if flag:
                point_restore = self.get_argument(""point_restore"", """")
                accept_answer = self.get_argument(""accept_answer"", """")
                answer_token = self.get_argument(""answer_token"", """")
                if point_restore == ""on"" and team:
                    if options.penalize_flag_value:
                        penalty = Penalty.by_team_token(flag, team, answer_token)
                        if penalty:
                            value = penalty.cost()
                            if value > 0:
                                team.money += value
                                if user:
                                    user.money += value
                                    self.dbsession.add(user)
                                self.dbsession.add(team)
                                self.event_manager.admin_score_update(
                                    team,
                                    ""%s penalty reversed - score has been updated.""
                                    % team.name,
                                    value,
                                )
                            self.dbsession.delete(penalty)
                            self.dbsession.commit()
                    if flag not in team.flags:
                        flag_value = flag.dynamic_value(team)
                        if (
                            self.config.dynamic_flag_value
                            and self.config.dynamic_flag_type == ""decay_all""
                        ):
                            for item in Flag.team_captures(flag.id):
                                tm = Team.by_id(item[0])
                                deduction = flag.dynamic_value(tm) - flag_value
                                tm.money = int(tm.money - deduction)
                                self.dbsession.add(tm)
                                self.event_manager.flag_decayed(tm, flag)
                        team.money += flag_value
                        if user:
                            user.money += flag_value
                            user.flags.append(flag)
                            self.dbsession.add(user)
                        team.flags.append(flag)
                        self.dbsession.add(team)
                        self.dbsession.commit()
                        BoxHandler.success_capture(self, user, flag, flag_value)
                        self._check_level(flag, team)
                        self.event_manager.flag_captured(team, flag)
                        if options.banking:
                            price = ""$"" + str(flag_value)
                        else:
                            price = str(flag_value) + "" points""
                        success.append(""%s awarded flag and %s"" % (team.name, price))
                if (
                    accept_answer == ""on""
                    and (flag.type == ""static"" or flag.type == ""regex"")
                    and not flag.capture(answer_token)
                ):
                    flag.type = ""regex""
                    if flag.token.startswith(""("") and flag.token.endwith("")""):
                        token = ""%s|(%s)"" % (flag.token, answer_token)
                    else:
                        token = ""(%s)|(%s)"" % (flag.token, answer_token)
                    if len(token) < 256:
                        flag.token = token
                        self.dbsession.add(flag)
                        self.dbsession.commit()
                        success.append(
                            ""Token successfully added for Flag %s"" % flag.name
                        )
                    else:
                        errors.append(""Flag token too long. Can not expand token."")
            self.render(uri[args[0]], errors=errors, success=success)
        else:
            self.render(""public/404.html"")","for item in Flag.team_captures(flag.id):
    tm = Team.by_id(item[0])
    deduction = flag.dynamic_value(tm) - flag_value
    tm.money = int(tm.money - deduction)
    self.dbsession.add(tm)
    self.event_manager.flag_decayed(tm, flag)","for item in Flag.team_captures(flag.id):
    (item_0, *item_ritemmaining) = item
    tm = Team.by_id(item[0])
    deduction = flag.dynamic_value(tm) - flag_value
    tm.money = int(tm.money - deduction)
    self.dbsession.add(tm)
    self.event_manager.flag_decayed(tm, flag)","(data, data, *data)","for (row_0, row_1, *row_len) in result:
    self.assertEqual(
    row_0, 
    row_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
frigate,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frigate/frigate/video.py,https://github.com/blakeblackshear/frigate/tree/master/frigate/video.py,,detect$404,"def detect(
    object_detector, frame, model_shape, region, objects_to_track, object_filters
):
    tensor_input = create_tensor_input(frame, model_shape, region)

    detections = []
    region_detections = object_detector.detect(tensor_input)
    for d in region_detections:
        box = d[2]
        size = region[2] - region[0]
        x_min = int((box[1] * size) + region[0])
        y_min = int((box[0] * size) + region[1])
        x_max = int((box[3] * size) + region[0])
        y_max = int((box[2] * size) + region[1])
        det = (
            d[0],
            d[1],
            (x_min, y_min, x_max, y_max),
            (x_max - x_min) * (y_max - y_min),
            region,
        )
        # apply object filters
        if filtered(det, objects_to_track, object_filters):
            continue
        detections.append(det)
    return detections","for d in region_detections:
    box = d[2]
    size = region[2] - region[0]
    x_min = int(box[1] * size + region[0])
    y_min = int(box[0] * size + region[1])
    x_max = int(box[3] * size + region[0])
    y_max = int(box[2] * size + region[1])
    det = (d[0], d[1], (x_min, y_min, x_max, y_max), (x_max - x_min) * (y_max - y_min), region)
    if filtered(det, objects_to_track, object_filters):
        continue
    detections.append(det)","for d in region_detections:
    (d_0, d_1, d_2, *_) = d
    box = d[2]
    size = region[2] - region[0]
    x_min = int(box[1] * size + region[0])
    y_min = int(box[0] * size + region[1])
    x_max = int(box[3] * size + region[0])
    y_max = int(box[2] * size + region[1])
    det = (d[0], d[1], (x_min, y_min, x_max, y_max), (x_max - x_min) * (y_max - y_min), region)
    if filtered(det, objects_to_track, object_filters):
        continue
    detections.append(det)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
sublime_debugger,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sublime_debugger/debugger/interfaces/debugger_model.py,https://github.com/shuky19/sublime_debugger/tree/master/debugger/interfaces/debugger_model.py,DebuggerModel,referesh_data$123,"def referesh_data(self):
		# Refresh autoreferesh data
		for command in DebuggerModel.REFRESHABLE_COMMANDS:
			self.debugger.run_command(command)

		# Refresh watch
		for watch_exp in self.data[DebuggerModel.DATA_WATCH]:
			self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp[0], watch_exp[0])","for watch_exp in self.data[DebuggerModel.DATA_WATCH]:
    self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp[0], watch_exp[0])","for watch_exp in self.data[DebuggerModel.DATA_WATCH]:
    (watch_exp_0, *watch_exp_rwatch_expmaining) = watch_exp
    self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp[0], watch_exp[0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
ASoulCnki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ASoulCnki/app/spider/reply/generate_refresh_like_spider.py,https://github.com/ASoulCnki/ASoulCnki/tree/master/app/spider/reply/generate_refresh_like_spider.py,,send_refresh_like_spider$6,"def send_refresh_like_spider(min_time):
    session = sqla['session']
    res = session.query(Reply.type_id, Reply.oid, Reply.dynamic_id,Reply.uid, min_time). \
        filter(Reply.ctime > min_time).distinct(Reply.oid).all()

    for r in res:
        param = (r[0], r[1], r[2], r[3],r[4])
        tasks.refresh_like_num_task.apply_async(param, queue=""reply_task_low_priority"", routing_key='reply_low')","for r in res:
    param = (r[0], r[1], r[2], r[3], r[4])
    tasks.refresh_like_num_task.apply_async(param, queue='reply_task_low_priority', routing_key='reply_low')","for r in res:
    (r_0, r_1, r_2, r_3, r_4, *_) = r
    param = (r[0], r[1], r[2], r[3], r[4])
    tasks.refresh_like_num_task.apply_async(param, queue='reply_task_low_priority', routing_key='reply_low')","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for edge in edges:
    points = GKPoints()
    if 'shape' in edge:
        for p in edge['shape']:
            new_point = GKPoint()
            new_point.set(p[0], p[1], 0)
            points.append(new_point)
        cmd = model.createNewCmd(model.getType('GKSection'))
        cmd.setPoints(edge['numLanes'], lane_width, points)
        model.getCommander().addCommand(cmd)
        section = cmd.createdObject()
        section.setName(edge['id'])
        edge_aimsun = model.getCatalog().findByName(edge['id'], type_section)
        edge_aimsun.setSpeed(edge['speed'] * 3.6)
    else:
        (first_node, last_node) = get_edge_nodes(edge, nodes)
        theta = get_edge_angle(first_node, last_node)
        first_node_offset = [0, 0]
        last_node_offset = [0, 0]
        if 'radius' in first_node:
            first_node_offset[0] = first_node['radius'] * np.cos(theta * np.pi / 180)
            first_node_offset[1] = first_node['radius'] * np.sin(theta * np.pi / 180)
        if 'radius' in last_node:
            last_node_offset[0] = -last_node['radius'] * np.cos(theta * np.pi / 180)
            last_node_offset[1] = -last_node['radius'] * np.sin(theta * np.pi / 180)
        edges_shared_node = [edg for edg in edges if first_node['id'] == edg['to'] or last_node['id'] == edg['from']]
        for new_edge in edges_shared_node:
            (new_first_node, new_last_node) = get_edge_nodes(new_edge, nodes)
            new_theta = get_edge_angle(new_first_node, new_last_node)
            if new_theta == theta - 180 or new_theta == theta + 180:
                first_node_offset[0] += lane_width * 0.5 * np.sin(theta * np.pi / 180)
                first_node_offset[1] -= lane_width * 0.5 * np.cos(theta * np.pi / 180)
                last_node_offset[0] += lane_width * 0.5 * np.sin(theta * np.pi / 180)
                last_node_offset[1] -= lane_width * 0.5 * np.cos(theta * np.pi / 180)
                break
        new_point = GKPoint()
        new_point.set(first_node['x'] + first_node_offset[0], first_node['y'] + first_node_offset[1], 0)
        points.append(new_point)
        new_point = GKPoint()
        new_point.set(last_node['x'] + last_node_offset[0], last_node['y'] + last_node_offset[1], 0)
        points.append(new_point)
        cmd = model.createNewCmd(type_section)
        cmd.setPoints(edge['numLanes'], lane_width, points)
        model.getCommander().addCommand(cmd)
        section = cmd.createdObject()
        section.setName(edge['id'])
        edge_aimsun = model.getCatalog().findByName(edge['id'], type_section)
        edge_aimsun.setSpeed(edge['speed'] * 3.6)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_id, e_numLanes, e_shape, e_speed = e.values()
variable mapping:
e_id: e['id']
e_numLanes: e['numLanes']
e_shape: e['shape']
e_speed: e['speed']",,,,,,,
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for node in nodes:
    node_pos = GKPoint()
    node_pos.set(node['x'], node['y'], 0)
    cmd = model.createNewCmd(type_node)
    cmd.setPosition(node_pos)
    model.getCommander().addCommand(cmd)
    new_node = cmd.createdObject()
    new_node.setName(node['id'])
    from_edges = [edge['id'] for edge in edges if edge['from'] == node['id']]
    to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]
    if len(to_edges) > 1 and len(from_edges) > 1 and (connections[node['id']] is not None):
        for connection in connections[node['id']]:
            cmd = model.createNewCmd(type_turn)
            from_section = model.getCatalog().findByName(connection['from'], type_section, True)
            to_section = model.getCatalog().findByName(connection['to'], type_section, True)
            cmd.setTurning(from_section, to_section)
            model.getCommander().addCommand(cmd)
            turn = cmd.createdObject()
            turn_name = '{}_to_{}'.format(connection['from'], connection['to'])
            turn.setName(turn_name)
            existing_node = turn.getNode()
            if existing_node is not None:
                existing_node.removeTurning(turn)
            new_node.addTurning(turn, False, True)
    else:
        for i in range(len(from_edges)):
            for j in range(len(to_edges)):
                cmd = model.createNewCmd(type_turn)
                to_section = model.getCatalog().findByName(from_edges[i], type_section, True)
                from_section = model.getCatalog().findByName(to_edges[j], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = '{}_to_{}'.format(from_edges[i], to_edges[j])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                new_node.addTurning(turn, False, True)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked elements, e['id'], e['x'], and e['y'] are dictionary values that can be accessed directly using the keys 'id', 'x', and 'y'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for node in junctions:
    phases = tls_properties[node['id']]['phases']
    print(phases)
    create_node_meters(model, control_plan, node['id'], phases)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['id'] is a dictionary value that can be accessed directly using the key 'id'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for veh_type in veh_types:
    new_state = create_state(model, veh_type['veh_id'])
    veh_type = model.getCatalog().findByName(veh_type['veh_id'], model.getType('GKVehicle'))
    new_state.setVehicle(veh_type)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['veh_id'] is a dictionary value that can be accessed directly using the key 'veh_id'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for inflow in inflows:
    traffic_state_aimsun = model.getCatalog().findByName(inflow['vtype'], type_traffic_state)
    edge_aimsun = model.getCatalog().findByName(inflow['edge'], type_section)
    traffic_state_aimsun.setEntranceFlow(edge_aimsun, None, inflow['vehsPerHour'])",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e['edge'], e['vehsPerHour'], e['vtype']
variable mapping:
e_0: e['edge']
e_1: e['vehsPerHour']
e_2: e['vtype']",,,,,,,
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for veh_type in veh_types:
    state_car = model.getCatalog().findByName(veh_type['veh_id'], type_traffic_state)
    if demand is not None and demand.isA('GKTrafficDemand'):
        if state_car is not None and state_car.isA('GKTrafficState'):
            set_demand_item(model, demand, state_car)
        model.getCommander().addCommand(None)
    else:
        create_traffic_demand(model, veh_type['veh_id'])",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['veh_id'] is a dictionary value that can be accessed directly using the key 'veh_id'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for p in edge['shape']:
    new_point = GKPoint()
    new_point.set(p[0], p[1], 0)
    points.append(new_point)","for p in edge['shape']:
    (p_0, p_1, *_) = p
    new_point = GKPoint()
    new_point.set(p[0], p[1], 0)
    points.append(new_point)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for connection in connections[node['id']]:
    cmd = model.createNewCmd(type_turn)
    from_section = model.getCatalog().findByName(connection['from'], type_section, True)
    to_section = model.getCatalog().findByName(connection['to'], type_section, True)
    cmd.setTurning(from_section, to_section)
    model.getCommander().addCommand(cmd)
    turn = cmd.createdObject()
    turn_name = '{}_to_{}'.format(connection['from'], connection['to'])
    turn.setName(turn_name)
    existing_node = turn.getNode()
    if existing_node is not None:
        existing_node.removeTurning(turn)
    new_node.addTurning(turn, False, True)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_from, e_to = e['from'], e['to']
variable mapping:
e_from: e['from']
e_to: e['to']",,,,,,,
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for veh_type in veh_types:
    cmd = GKObjectDuplicateCmd()
    cmd.init(vehicle)
    model.getCommander().addCommand(cmd)
    new_veh = cmd.createdObject()
    new_veh.setName(veh_type['veh_id'])",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['veh_id'] is a dictionary value that can be accessed directly using the key 'veh_id'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
plover,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plover/plover/orthography.py,https://github.com/openstenoproject/plover/tree/master/plover/orthography.py,,make_candidates_from_rules$9,"def make_candidates_from_rules(word, suffix, check=lambda x: True):
    candidates = []
    for r in system.ORTHOGRAPHY_RULES:
        m = r[0].match(word + "" ^ "" + suffix)
        if m:   
            expanded = m.expand(r[1])
            if check(expanded):
                candidates.append(expanded)
    return candidates","for r in system.ORTHOGRAPHY_RULES:
    m = r[0].match(word + ' ^ ' + suffix)
    if m:
        expanded = m.expand(r[1])
        if check(expanded):
            candidates.append(expanded)","for r in system.ORTHOGRAPHY_RULES:
    (r_0, r_1, *_) = r
    m = r[0].match(word + ' ^ ' + suffix)
    if m:
        expanded = m.expand(r[1])
        if check(expanded):
            candidates.append(expanded)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/physics/vector/dyadic.py,https://github.com/sympy/sympy/tree/master/sympy/physics/vector/dyadic.py,Dyadic,simplify$496,"def simplify(self):
        """"""Returns a simplified Dyadic.""""""
        out = Dyadic(0)
        for v in self.args:
            out += Dyadic([(v[0].simplify(), v[1], v[2])])
        return out","for v in self.args:
    out += Dyadic([(v[0].simplify(), v[1], v[2])])","for v in self.args:
    (v_0, v_1, v_2, *_) = v
    out += Dyadic([(v[0].simplify(), v[1], v[2])])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
QRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/data/rating.py,https://github.com/Coder-Yu/QRec/tree/master/data/rating.py,Rating,matrix$158,"def matrix(self):
        m = np.zeros((len(self.user),len(self.item)))
        for u in self.user:
            k, v = self.userRated(u)
            vec = np.zeros(len(self.item))
            # print vec
            for pair in zip(k, v):
                iid = self.item[pair[0]]
                vec[iid] = pair[1]
            m[self.user[u]]=vec
        return m","for pair in zip(k, v):
    iid = self.item[pair[0]]
    vec[iid] = pair[1]","for pair in zip(k, v):
    (_, pair_1, *pair_rpairmaining) = pair
    iid = self.item[pair[0]]
    vec[iid] = pair[1]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_fr_ch.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_fr_ch.py,Num2WordsENTest,test_currency_frf$121,"def test_currency_frf(self):
        for test in TEST_CASES_TO_CURRENCY_FRF:
            self.assertEqual(
                num2words(test[0], lang='fr_CH', to='currency',
                          currency='FRF'),
                test[1]
            )","for test in TEST_CASES_TO_CURRENCY_FRF:
    self.assertEqual(num2words(test[0], lang='fr_CH', to='currency', currency='FRF'), test[1])","for test in TEST_CASES_TO_CURRENCY_FRF:
    (test_0, test_1, *_) = test
    self.assertEqual(num2words(test[0], lang='fr_CH', to='currency', currency='FRF'), test[1])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/visualization/data_providers/genome.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/visualization/data_providers/genome.py,BamDataProvider,convert_cigar$1053,"def convert_cigar(read, start_field, cigar_field, seq_field):
            '''
            Convert read cigar from pysam format to string format.
            '''
            cigar_ops = 'MIDNSHP=X'
            read_cigar = ''
            for op_tuple in read[cigar_field]:
                read_cigar += '%i%s' % (op_tuple[1], cigar_ops[op_tuple[0]])
            read[cigar_field] = read_cigar","for op_tuple in read[cigar_field]:
    read_cigar += '%i%s' % (op_tuple[1], cigar_ops[op_tuple[0]])","for op_tuple in read[cigar_field]:
    (op_tuple_0, op_tuple_1, *op_tuple_rop_tuplemaining) = op_tuple
    read_cigar += '%i%s' % (op_tuple[1], cigar_ops[op_tuple[0]])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
pororo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pororo/pororo/models/tts/utils/display.py,https://github.com/kakaobrain/pororo/tree/master/pororo/models/tts/utils/display.py,,simple_table$17,"def simple_table(item_tuples):
    border_pattern = ""+---------------------------------------""
    whitespace = ""                                            ""

    headings, cells, = (
        [],
        [],
    )

    for item in item_tuples:

        heading, cell = str(item[0]), str(item[1])

        pad_head = True if len(heading) < len(cell) else False

        pad = abs(len(heading) - len(cell))
        pad = whitespace[:pad]

        pad_left = pad[:len(pad) // 2]
        pad_right = pad[len(pad) // 2:]

        if pad_head:
            heading = pad_left + heading + pad_right
        else:
            cell = pad_left + cell + pad_right

        headings += [heading]
        cells += [cell]

    border, head, body = """", """", """"

    for i in range(len(item_tuples)):

        temp_head = f""| {headings[i]} ""
        temp_body = f""| {cells[i]} ""

        border += border_pattern[:len(temp_head)]
        head += temp_head
        body += temp_body

        if i == len(item_tuples) - 1:
            head += ""|""
            body += ""|""
            border += ""+""

    print(border)
    print(head)
    print(border)
    print(body)
    print(border)
    print("" "")","for item in item_tuples:
    (heading, cell) = (str(item[0]), str(item[1]))
    pad_head = True if len(heading) < len(cell) else False
    pad = abs(len(heading) - len(cell))
    pad = whitespace[:pad]
    pad_left = pad[:len(pad) // 2]
    pad_right = pad[len(pad) // 2:]
    if pad_head:
        heading = pad_left + heading + pad_right
    else:
        cell = pad_left + cell + pad_right
    headings += [heading]
    cells += [cell]","for item in item_tuples:
    (item_0, item_1, *_) = item
    (heading, cell) = (str(item[0]), str(item[1]))
    pad_head = True if len(heading) < len(cell) else False
    pad = abs(len(heading) - len(cell))
    pad = whitespace[:pad]
    pad_left = pad[:len(pad) // 2]
    pad_right = pad[len(pad) // 2:]
    if pad_head:
        heading = pad_left + heading + pad_right
    else:
        cell = pad_left + cell + pad_right
    headings += [heading]
    cells += [cell]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_fr_dz.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_fr_dz.py,Num2WordsPLTest,test_ordinal_num$60,"def test_ordinal_num(self):
        for test in test_fr.TEST_CASES_ORDINAL_NUM:
            self.assertEqual(
                num2words(test[0], lang='fr_DZ', to='ordinal_num'),
                test[1]
            )","for test in test_fr.TEST_CASES_ORDINAL_NUM:
    self.assertEqual(num2words(test[0], lang='fr_DZ', to='ordinal_num'), test[1])","for test in test_fr.TEST_CASES_ORDINAL_NUM:
    (test_0, test_1, *_) = test
    self.assertEqual(num2words(test[0], lang='fr_DZ', to='ordinal_num'), test[1])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
sfepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sfepy/sfepy/terms/terms.py,https://github.com/sfepy/sfepy/tree/master/sfepy/terms/terms.py,Term,get_material_names$676,"def get_material_names(self):
        out = []
        for aux in self.names.material:
            if aux[0] is not None:
                out.append(aux[0])
        return out","for aux in self.names.material:
    if aux[0] is not None:
        out.append(aux[0])","for aux in self.names.material:
    (aux_0, *aux_rauxmaining) = aux
    if aux[0] is not None:
        out.append(aux[0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
cogdl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cogdl/cogdl/models/emb/gatne.py,https://github.com/THUDM/cogdl/tree/master/cogdl/models/emb/gatne.py,,get_G_from_edges$335,"def get_G_from_edges(edges):
    edge_dict = dict()
    for edge in edges:
        edge_key = str(edge[0]) + ""_"" + str(edge[1])
        if edge_key not in edge_dict:
            edge_dict[edge_key] = 1
        else:
            edge_dict[edge_key] += 1
    tmp_G = nx.Graph()
    for edge_key in edge_dict:
        weight = edge_dict[edge_key]
        x = int(edge_key.split(""_"")[0])
        y = int(edge_key.split(""_"")[1])
        tmp_G.add_edge(x, y)
        tmp_G[x][y][""weight""] = weight
    return tmp_G","for edge in edges:
    edge_key = str(edge[0]) + '_' + str(edge[1])
    if edge_key not in edge_dict:
        edge_dict[edge_key] = 1
    else:
        edge_dict[edge_key] += 1","for edge in edges:
    (edge_0, edge_1, *_) = edge
    edge_key = str(edge[0]) + '_' + str(edge[1])
    if edge_key not in edge_dict:
        edge_dict[edge_key] = 1
    else:
        edge_dict[edge_key] += 1","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
cloudtracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloudtracker/cloudtracker/datasources/athena.py,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,get_performed_users$377,"def get_performed_users(self):
        """"""
        Returns the users that performed actions within the search filters
        """"""
        query = ""select distinct userIdentity.userName from {table_name} where {search_filter}"".format(
            table_name=self.table_name, search_filter=self.search_filter
        )
        response = self.query_athena(query)

        user_names = {}
        for row in response:
            user_name = row[0]
            if user_name == ""HIDDEN_DUE_TO_SECURITY_REASONS"":
                # This happens when a user logs in with the wrong username
                continue
            user_names[user_name] = True
        return user_names","for row in response:
    user_name = row[0]
    if user_name == 'HIDDEN_DUE_TO_SECURITY_REASONS':
        continue
    user_names[user_name] = True","for row in response:
    (row_0, *row_rrowmaining) = row
    user_name = row[0]
    if user_name == 'HIDDEN_DUE_TO_SECURITY_REASONS':
        continue
    user_names[user_name] = True","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
checkov,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkov/tests/terraform/parser/test_parser_var_blocks.py,https://github.com/bridgecrewio/checkov/tree/master/tests/terraform/parser/test_parser_var_blocks.py,TestParserInternals,test_split_merge_args$10,"def test_split_merge_args(self):
        cases: List[Tuple[str, List[str]]] = [
            (""local.one, local.two"",
             [""local.one"", ""local.two""]),
            (""{Tag4 = \""four\""}, {Tag5 = \""five\""}"",
             [""{Tag4 = \""four\""}"", ""{Tag5 = \""five\""}""]),
            (""{a=\""b\""}, {a=[1,2], c=\""z\""}, {d=3}"",
             [""{a=\""b\""}"", ""{a=[1,2], c=\""z\""}"", ""{d=3}""]),
            (""local.common_tags, merge({Tag4 = \""four\""}, {Tag5 = \""five\""})"",
             [""local.common_tags"", ""merge({Tag4 = \""four\""}, {Tag5 = \""five\""})""]),
            ("", "",
             None),
            ("""",
             None),
            ("", leading_comma"",
             [""leading_comma""]),
            (""kinda_maybe_shouldnt_work_but_we_will_roll_with_it, "",        # <-- trailing comma
             [""kinda_maybe_shouldnt_work_but_we_will_roll_with_it""]),
            (""local.one"",
             [""local.one""]),
            ('{""a"": ""}, evil""}',        # bracket inside string, should not be split
             ['{""a"": ""}, evil""}']),
            (""{'a': '}, evil'}"",        # bracket inside string, should not be split
             [""{'a': '}, evil'}""]),     # Note: these happen with native maps (see merge tests)
            ('${merge({\'a\': \'}, evil\'})}',
             ['${merge({\'a\': \'}, evil\'})}']),
            ('local.common_tags,,{\'Tag4\': \'four\'},,{\'Tag2\': \'Dev\'},',
             [""local.common_tags"", ""{\'Tag4\': \'four\'}"", ""{\'Tag2\': \'Dev\'}""])
        ]
        for case in cases:
            actual = split_merge_args(case[0])
            assert actual == case[1], f""Case \""{case[0]}\"" failed. Expected: {case[1]}  Actual: {actual}""","for case in cases:
    actual = split_merge_args(case[0])
    assert actual == case[1], f'Case ""{case[0]}"" failed. Expected: {case[1]}  Actual: {actual}'","for case in cases:
    (case_0, case_1, *_) = case
    actual = split_merge_args(case[0])
    assert actual == case[1], f'Case ""{case[0]}"" failed. Expected: {case[1]}  Actual: {actual}'","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
PaddleVideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleVideo/paddlevideo/utils/multigrid/multigrid.py,https://github.com/PaddlePaddle/PaddleVideo/tree/master/paddlevideo/utils/multigrid/multigrid.py,MultigridSchedule,get_long_cycle_schedule$110,"def get_long_cycle_schedule(self, cfg):
        """"""
        Based on multigrid hyperparameters, define the schedule of a long cycle.
        Args:
            cfg (configs): configs that contains training and multigrid specific
                hyperparameters.
        Returns:
            schedule (list): Specifies a list long cycle base shapes and their
                corresponding training epochs.
        """"""

        steps = cfg.OPTIMIZER.learning_rate.steps

        default_size = float(
            cfg.PIPELINE.train.decode_sampler.num_frames *
            cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size']**
            2)  # 32 * 224 * 224  C*H*W
        default_iters = steps[-1]  # 196

        # Get shapes and average batch size for each long cycle shape.
        avg_bs = []
        all_shapes = []
        #        for t_factor, s_factor in cfg.MULTIGRID.long_cycle_factors:
        for item in cfg.MULTIGRID.long_cycle_factors:
            t_factor, s_factor = item[""value""]
            base_t = int(
                round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
            base_s = int(
                round(
                    cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size']
                    * s_factor))
            if cfg.MULTIGRID.SHORT_CYCLE:
                shapes = [
                    [
                        base_t,
                        cfg.MULTIGRID.default_crop_size *
                        cfg.MULTIGRID.short_cycle_factors[0],
                    ],
                    [
                        base_t,
                        cfg.MULTIGRID.default_crop_size *
                        cfg.MULTIGRID.short_cycle_factors[1],
                    ],
                    [base_t, base_s],
                ]  #first two is short_cycle, last is the base long_cycle
            else:
                shapes = [[base_t, base_s]]

            # (T, S) -> (B, T, S)
            shapes = [[
                int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]
            ] for s in shapes]
            avg_bs.append(np.mean([s[0] for s in shapes]))
            all_shapes.append(shapes)

        # Get schedule regardless of cfg.MULTIGRID.epoch_factor.
        total_iters = 0
        schedule = []
        for step_index in range(len(steps) - 1):
            step_epochs = steps[step_index + 1] - steps[step_index]

            for long_cycle_index, shapes in enumerate(all_shapes):
                #ensure each of 4 sequences run the same num of iters
                cur_epochs = (step_epochs * avg_bs[long_cycle_index] /
                              sum(avg_bs))

                # get cur_iters from cur_epochs
                cur_iters = cur_epochs / avg_bs[long_cycle_index]
                total_iters += cur_iters
                schedule.append((step_index, shapes[-1], cur_epochs))

        iter_saving = default_iters / total_iters  # ratio between default iters and real iters

        final_step_epochs = cfg.OPTIMIZER.learning_rate.max_epoch - steps[-1]

        # We define the fine-tuning phase to have the same amount of iteration
        # saving as the rest of the training.
        #final_step_epochs / iter_saving make fine-tune having the same iters as training
        ft_epochs = final_step_epochs / iter_saving * avg_bs[-1]

        #        schedule.append((step_index + 1, all_shapes[-1][2], ft_epochs))
        schedule.append((step_index + 1, all_shapes[-1][-1], ft_epochs))

        # Obtrain final schedule given desired cfg.MULTIGRID.epoch_factor.
        x = (cfg.OPTIMIZER.learning_rate.max_epoch *
             cfg.MULTIGRID.epoch_factor / sum(s[-1] for s in schedule))

        final_schedule = []
        total_epochs = 0
        for s in schedule:
            epochs = s[2] * x
            total_epochs += epochs
            final_schedule.append((s[0], s[1], int(round(total_epochs))))
        print_schedule(final_schedule)
        return final_schedule","for item in cfg.MULTIGRID.long_cycle_factors:
    (t_factor, s_factor) = item['value']
    base_t = int(round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
    base_s = int(round(cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size'] * s_factor))
    if cfg.MULTIGRID.SHORT_CYCLE:
        shapes = [[base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[0]], [base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[1]], [base_t, base_s]]
    else:
        shapes = [[base_t, base_s]]
    shapes = [[int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]] for s in shapes]
    avg_bs.append(np.mean([s[0] for s in shapes]))
    all_shapes.append(shapes)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['value'] is a dictionary value that can be accessed directly using the key 'value'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
PaddleVideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleVideo/paddlevideo/utils/multigrid/multigrid.py,https://github.com/PaddlePaddle/PaddleVideo/tree/master/paddlevideo/utils/multigrid/multigrid.py,MultigridSchedule,get_long_cycle_schedule$110,"def get_long_cycle_schedule(self, cfg):
        """"""
        Based on multigrid hyperparameters, define the schedule of a long cycle.
        Args:
            cfg (configs): configs that contains training and multigrid specific
                hyperparameters.
        Returns:
            schedule (list): Specifies a list long cycle base shapes and their
                corresponding training epochs.
        """"""

        steps = cfg.OPTIMIZER.learning_rate.steps

        default_size = float(
            cfg.PIPELINE.train.decode_sampler.num_frames *
            cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size']**
            2)  # 32 * 224 * 224  C*H*W
        default_iters = steps[-1]  # 196

        # Get shapes and average batch size for each long cycle shape.
        avg_bs = []
        all_shapes = []
        #        for t_factor, s_factor in cfg.MULTIGRID.long_cycle_factors:
        for item in cfg.MULTIGRID.long_cycle_factors:
            t_factor, s_factor = item[""value""]
            base_t = int(
                round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
            base_s = int(
                round(
                    cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size']
                    * s_factor))
            if cfg.MULTIGRID.SHORT_CYCLE:
                shapes = [
                    [
                        base_t,
                        cfg.MULTIGRID.default_crop_size *
                        cfg.MULTIGRID.short_cycle_factors[0],
                    ],
                    [
                        base_t,
                        cfg.MULTIGRID.default_crop_size *
                        cfg.MULTIGRID.short_cycle_factors[1],
                    ],
                    [base_t, base_s],
                ]  #first two is short_cycle, last is the base long_cycle
            else:
                shapes = [[base_t, base_s]]

            # (T, S) -> (B, T, S)
            shapes = [[
                int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]
            ] for s in shapes]
            avg_bs.append(np.mean([s[0] for s in shapes]))
            all_shapes.append(shapes)

        # Get schedule regardless of cfg.MULTIGRID.epoch_factor.
        total_iters = 0
        schedule = []
        for step_index in range(len(steps) - 1):
            step_epochs = steps[step_index + 1] - steps[step_index]

            for long_cycle_index, shapes in enumerate(all_shapes):
                #ensure each of 4 sequences run the same num of iters
                cur_epochs = (step_epochs * avg_bs[long_cycle_index] /
                              sum(avg_bs))

                # get cur_iters from cur_epochs
                cur_iters = cur_epochs / avg_bs[long_cycle_index]
                total_iters += cur_iters
                schedule.append((step_index, shapes[-1], cur_epochs))

        iter_saving = default_iters / total_iters  # ratio between default iters and real iters

        final_step_epochs = cfg.OPTIMIZER.learning_rate.max_epoch - steps[-1]

        # We define the fine-tuning phase to have the same amount of iteration
        # saving as the rest of the training.
        #final_step_epochs / iter_saving make fine-tune having the same iters as training
        ft_epochs = final_step_epochs / iter_saving * avg_bs[-1]

        #        schedule.append((step_index + 1, all_shapes[-1][2], ft_epochs))
        schedule.append((step_index + 1, all_shapes[-1][-1], ft_epochs))

        # Obtrain final schedule given desired cfg.MULTIGRID.epoch_factor.
        x = (cfg.OPTIMIZER.learning_rate.max_epoch *
             cfg.MULTIGRID.epoch_factor / sum(s[-1] for s in schedule))

        final_schedule = []
        total_epochs = 0
        for s in schedule:
            epochs = s[2] * x
            total_epochs += epochs
            final_schedule.append((s[0], s[1], int(round(total_epochs))))
        print_schedule(final_schedule)
        return final_schedule","for s in schedule:
    epochs = s[2] * x
    total_epochs += epochs
    final_schedule.append((s[0], s[1], int(round(total_epochs))))","for s in schedule:
    (s_0, s_1, s_2, *_) = s
    epochs = s[2] * x
    total_epochs += epochs
    final_schedule.append((s[0], s[1], int(round(total_epochs))))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
PaddleVideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleVideo/paddlevideo/utils/multigrid/multigrid.py,https://github.com/PaddlePaddle/PaddleVideo/tree/master/paddlevideo/utils/multigrid/multigrid.py,MultigridSchedule,get_long_cycle_schedule$110,"def get_long_cycle_schedule(self, cfg):
        """"""
        Based on multigrid hyperparameters, define the schedule of a long cycle.
        Args:
            cfg (configs): configs that contains training and multigrid specific
                hyperparameters.
        Returns:
            schedule (list): Specifies a list long cycle base shapes and their
                corresponding training epochs.
        """"""

        steps = cfg.OPTIMIZER.learning_rate.steps

        default_size = float(
            cfg.PIPELINE.train.decode_sampler.num_frames *
            cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size']**
            2)  # 32 * 224 * 224  C*H*W
        default_iters = steps[-1]  # 196

        # Get shapes and average batch size for each long cycle shape.
        avg_bs = []
        all_shapes = []
        #        for t_factor, s_factor in cfg.MULTIGRID.long_cycle_factors:
        for item in cfg.MULTIGRID.long_cycle_factors:
            t_factor, s_factor = item[""value""]
            base_t = int(
                round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
            base_s = int(
                round(
                    cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size']
                    * s_factor))
            if cfg.MULTIGRID.SHORT_CYCLE:
                shapes = [
                    [
                        base_t,
                        cfg.MULTIGRID.default_crop_size *
                        cfg.MULTIGRID.short_cycle_factors[0],
                    ],
                    [
                        base_t,
                        cfg.MULTIGRID.default_crop_size *
                        cfg.MULTIGRID.short_cycle_factors[1],
                    ],
                    [base_t, base_s],
                ]  #first two is short_cycle, last is the base long_cycle
            else:
                shapes = [[base_t, base_s]]

            # (T, S) -> (B, T, S)
            shapes = [[
                int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]
            ] for s in shapes]
            avg_bs.append(np.mean([s[0] for s in shapes]))
            all_shapes.append(shapes)

        # Get schedule regardless of cfg.MULTIGRID.epoch_factor.
        total_iters = 0
        schedule = []
        for step_index in range(len(steps) - 1):
            step_epochs = steps[step_index + 1] - steps[step_index]

            for long_cycle_index, shapes in enumerate(all_shapes):
                #ensure each of 4 sequences run the same num of iters
                cur_epochs = (step_epochs * avg_bs[long_cycle_index] /
                              sum(avg_bs))

                # get cur_iters from cur_epochs
                cur_iters = cur_epochs / avg_bs[long_cycle_index]
                total_iters += cur_iters
                schedule.append((step_index, shapes[-1], cur_epochs))

        iter_saving = default_iters / total_iters  # ratio between default iters and real iters

        final_step_epochs = cfg.OPTIMIZER.learning_rate.max_epoch - steps[-1]

        # We define the fine-tuning phase to have the same amount of iteration
        # saving as the rest of the training.
        #final_step_epochs / iter_saving make fine-tune having the same iters as training
        ft_epochs = final_step_epochs / iter_saving * avg_bs[-1]

        #        schedule.append((step_index + 1, all_shapes[-1][2], ft_epochs))
        schedule.append((step_index + 1, all_shapes[-1][-1], ft_epochs))

        # Obtrain final schedule given desired cfg.MULTIGRID.epoch_factor.
        x = (cfg.OPTIMIZER.learning_rate.max_epoch *
             cfg.MULTIGRID.epoch_factor / sum(s[-1] for s in schedule))

        final_schedule = []
        total_epochs = 0
        for s in schedule:
            epochs = s[2] * x
            total_epochs += epochs
            final_schedule.append((s[0], s[1], int(round(total_epochs))))
        print_schedule(final_schedule)
        return final_schedule","for (long_cycle_index, shapes) in enumerate(all_shapes):
    cur_epochs = step_epochs * avg_bs[long_cycle_index] / sum(avg_bs)
    cur_iters = cur_epochs / avg_bs[long_cycle_index]
    total_iters += cur_iters
    schedule.append((step_index, shapes[-1], cur_epochs))","for (long_cycle_index, shapes) in enumerate(all_shapes):
    (*shapes_rshapesmaining, shapes_last) = shapes
    cur_epochs = step_epochs * avg_bs[long_cycle_index] / sum(avg_bs)
    cur_iters = cur_epochs / avg_bs[long_cycle_index]
    total_iters += cur_iters
    schedule.append((step_index, shapes[-1], cur_epochs))",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_last = e
variable mapping:
e_last: e[-1]",,,,,,,
WordOps,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WordOps/wo/cli/plugins/site_functions.py,https://github.com/WordOps/WordOps/tree/master/wo/cli/plugins/site_functions.py,,setupwordpress$243,"def setupwordpress(self, data, vhostonly=False):
    wo_domain_name = data['site_name']
    wo_site_webroot = data['webroot']
    if self.app.config.has_section('wordpress'):
        prompt_wpprefix = self.app.config.get('wordpress', 'prefix')
        wo_wp_user = self.app.config.get('wordpress', 'user')
        wo_wp_pass = self.app.config.get('wordpress', 'password')
        wo_wp_email = self.app.config.get('wordpress', 'email')
    else:
        prompt_wpprefix = False
        wo_wp_user = ''
        wo_wp_pass = ''
        wo_wp_email = ''
    # Random characters
    wo_random_pass = (''.join(random.sample(string.ascii_uppercase +
                                            string.ascii_lowercase +
                                            string.digits, 24)))
    wo_wp_prefix = ''
    # wo_wp_user = ''
    # wo_wp_pass = ''

    if 'wp-user' in data.keys() and data['wp-user']:
        wo_wp_user = data['wp-user']
    if 'wp-email' in data.keys() and data['wp-email']:
        wo_wp_email = data['wp-email']
    if 'wp-pass' in data.keys() and data['wp-pass']:
        wo_wp_pass = data['wp-pass']

    Log.info(self, ""Downloading WordPress \t\t"", end='')
    WOFileUtils.chdir(self, '{0}/htdocs/'.format(wo_site_webroot))
    try:
        if WOShellExec.cmd_exec(self, ""wp --allow-root core""
                                "" download""):
            pass
        else:
            Log.info(self, ""["" + Log.ENDC + Log.FAIL +
                     ""Fail"" + Log.OKBLUE + ""]"")
            raise SiteError(""download WordPress core failed"")
    except CommandExecutionError:
        Log.info(self, ""["" + Log.ENDC + Log.FAIL + ""Fail"" + Log.OKBLUE + ""]"")
        raise SiteError(""download WordPress core failed"")

    Log.info(self, ""["" + Log.ENDC + ""Done"" + Log.OKBLUE + ""]"")

    if not (data['wo_db_name'] and data['wo_db_user'] and data['wo_db_pass']):
        data = setupdatabase(self, data)
    if prompt_wpprefix == 'True' or prompt_wpprefix == 'true':
        try:
            wo_wp_prefix = input('Enter the WordPress table prefix [wp_]: ')
            while not re.match('^[A-Za-z0-9_]*$', wo_wp_prefix):
                Log.warn(self, ""table prefix can only ""
                         ""contain numbers, letters, and underscores"")
                wo_wp_prefix = input('Enter the WordPress table prefix [wp_]: '
                                     )
        except EOFError:
            raise SiteError(""input table prefix failed"")

    if not wo_wp_prefix:
        wo_wp_prefix = 'wp_'

    # Modify wp-config.php & move outside the webroot

    WOFileUtils.chdir(self, '{0}/htdocs/'.format(wo_site_webroot))
    Log.debug(self, ""Setting up wp-config file"")
    if not data['multisite']:
        Log.debug(self, ""Generating wp-config for WordPress Single site"")
        Log.debug(self, ""/bin/bash -c \""{0} --allow-root ""
                  .format(WOVar.wo_wpcli_path) +
                  ""config create "" +
                  ""--dbname=\'{0}\' --dbprefix=\'{1}\' --dbuser=\'{2}\' ""
                  ""--dbhost=\'{3}\' ""
                  .format(data['wo_db_name'], wo_wp_prefix,
                          data['wo_db_user'], data['wo_db_host']) +
                  ""--dbpass= ""
                  ""--extra-php<<PHP \n {0}\nPHP\""""
                  .format(""\n\ndefine(\'WP_DEBUG\', false);""))
        try:
            if WOShellExec.cmd_exec(self, ""/bin/bash -c \""{0} --allow-root""
                                    .format(WOVar.wo_wpcli_path) +
                                    "" config create "" +
                                    ""--dbname=\'{0}\' --dbprefix=\'{1}\' ""
                                    ""--dbuser=\'{2}\' --dbhost=\'{3}\' ""
                                    .format(data['wo_db_name'], wo_wp_prefix,
                                            data['wo_db_user'],
                                            data['wo_db_host']
                                            ) +
                                    ""--dbpass=\'{0}\'\""""
                                    .format(data['wo_db_pass']),
                                    log=False
                                    ):
                pass
            else:
                raise SiteError(""generate wp-config failed for wp single site"")
        except CommandExecutionError:
            raise SiteError(""generate wp-config failed for wp single site"")
    else:
        Log.debug(self, ""Generating wp-config for WordPress multisite"")
        Log.debug(self, ""/bin/bash -c \""{0} --allow-root ""
                  .format(WOVar.wo_wpcli_path) +
                  ""config create "" +
                  ""--dbname=\'{0}\' --dbprefix=\'{1}\' --dbhost=\'{2}\' ""
                  .format(data['wo_db_name'],
                          wo_wp_prefix, data['wo_db_host']) +
                  ""--dbuser=\'{0}\' --dbpass= ""
                  ""--extra-php<<PHP \n {1} {2} \nPHP\""""
                  .format(data['wo_db_user'],
                          ""\ndefine(\'WPMU_ACCEL_REDIRECT\',""
                          "" true);"",
                          ""\ndefine(\'CONCATENATE_SCRIPTS\',""
                          "" false);""))
        try:
            if WOShellExec.cmd_exec(self, ""/bin/bash -c \""{0} --allow-root""
                                    .format(WOVar.wo_wpcli_path) +
                                    "" config create "" +
                                    ""--dbname=\'{0}\' --dbprefix=\'{1}\' ""
                                    ""--dbhost=\'{2}\' ""
                                    .format(data['wo_db_name'], wo_wp_prefix,
                                            data['wo_db_host']) +
                                    ""--dbuser=\'{0}\' --dbpass=\'{1}\' ""
                                    ""--extra-php<<PHP \n ""
                                    ""\n{2} \nPHP\""""
                                    .format(data['wo_db_user'],
                                            data['wo_db_pass'],
                                            ""\ndefine(\'WPMU_ACCEL_REDIRECT\',""
                                            "" true);""),
                                    log=False
                                    ):
                pass
            else:
                raise SiteError(""generate wp-config failed for wp multi site"")
        except CommandExecutionError:
            raise SiteError(""generate wp-config failed for wp multi site"")

    # set all wp-config.php variables
    wp_conf_variables = [
        ['WP_REDIS_PREFIX', '{0}:'.format(wo_domain_name)],
        ['WP_MEMORY_LIMIT', '128M'],
        ['WP_MAX_MEMORY_LIMIT', '256M'],
        ['CONCATENATE_SCRIPTS', 'false'],
        ['WP_POST_REVISIONS', '10'],
        ['MEDIA_TRASH', 'true'],
        ['EMPTY_TRASH_DAYS', '15'],
        ['WP_AUTO_UPDATE_CORE', 'minor'],
        ['WP_REDIS_DISABLE_BANNERS', 'true']]
    Log.wait(self, ""Configuring WordPress"")
    for wp_conf in wp_conf_variables:
        wp_var = wp_conf[0]
        wp_val = wp_conf[1]
        var_raw = (bool(wp_val == 'true' or wp_val == 'false'))
        try:
            WOShellExec.cmd_exec(
                self, ""/bin/bash -c \""{0} --allow-root ""
                .format(WOVar.wo_wpcli_path) +
                ""config set {0} ""
                ""\'{1}\' {wp_raw}\""""
                .format(wp_var, wp_val,
                        wp_raw='--raw'
                        if var_raw is True else ''))
        except CommandExecutionError as e:
            Log.failed(self, ""Configuring WordPress"")
            Log.debug(self, str(e))
            Log.error(self, 'Unable to define wp-config.php variables')
    Log.valide(self, ""Configuring WordPress"")

    # WOFileUtils.mvfile(self, os.getcwd()+'/wp-config.php',
    #                   os.path.abspath(os.path.join(os.getcwd(), os.pardir)))

    try:

        Log.debug(self, ""Moving file from {0} to {1}"".format(os.getcwd(
        ) + '/wp-config.php', os.path.abspath(os.path.join(os.getcwd(),
                                                           os.pardir))))
        shutil.move(os.getcwd() + '/wp-config.php',
                    os.path.abspath(os.path.join(os.getcwd(), os.pardir)))
    except Exception as e:
        Log.debug(self, str(e))
        Log.error(self, 'Unable to move file from {0} to {1}'
                  .format(os.getcwd() + '/wp-config.php',
                          os.path.abspath(os.path.join(os.getcwd(),
                                                       os.pardir))), False)
        raise SiteError(""Unable to move wp-config.php"")

    if not wo_wp_user:
        wo_wp_user = WOVar.wo_user
        while not wo_wp_user:
            Log.warn(self, ""Username can have only alphanumeric""
                     ""characters, spaces, underscores, hyphens,""
                     ""periods and the @ symbol."")
            try:
                wo_wp_user = input('Enter WordPress username: ')
            except EOFError:
                raise SiteError(""input WordPress username failed"")
    if not wo_wp_pass:
        wo_wp_pass = wo_random_pass

    if not wo_wp_email:
        wo_wp_email = WOVar.wo_email
        while not wo_wp_email:
            try:
                wo_wp_email = input('Enter WordPress email: ')
            except EOFError:
                raise SiteError(""input WordPress username failed"")

    try:
        while not re.match(r""^[A-Za-z0-9\.\+_-]+@[A-Za-z0-9\._-]+\.[a-zA-Z]*$"",
                           wo_wp_email):
            Log.info(self, ""EMail not Valid in config, ""
                     ""Please provide valid email id"")
            wo_wp_email = input(""Enter your email: "")
    except EOFError:
        raise SiteError(""input WordPress user email failed"")

    Log.debug(self, ""Setting up WordPress tables"")
    Log.wait(self, ""Installing WordPress"")
    if not data['multisite']:
        Log.debug(self, ""Creating tables for WordPress Single site"")
        Log.debug(
            self, ""{0} --allow-root core install ""
                  .format(WOVar.wo_wpcli_path) +
                  ""--url=\'{0}\' --title=\'{0}\' --admin_name=\'{1}\' ""
                  .format(data['site_name'], wo_wp_user) +
                  ""--admin_password= --admin_email=\'{0}\'""
                  .format(wo_wp_email))
        try:
            if WOShellExec.cmd_exec(
                self, ""{0} --allow-root core ""
                .format(WOVar.wo_wpcli_path) +
                ""install --url=\'{0}\' --title=\'{0}\' ""
                ""--admin_name=\'{1}\' ""
                .format(data['site_name'], wo_wp_user) +
                ""--admin_password=\'{0}\' ""
                ""--admin_email=\'{1}\'""
                .format(wo_wp_pass, wo_wp_email),
                    log=False):
                pass
            else:
                Log.failed(self, ""Installing WordPress"")
                raise SiteError(
                    ""setup WordPress tables failed for single site"")
        except CommandExecutionError:
            raise SiteError(""setup WordPress tables failed for single site"")
    else:
        Log.debug(self, ""Creating tables for WordPress multisite"")
        Log.debug(self, ""{0} --allow-root ""
                  .format(WOVar.wo_wpcli_path) +
                  ""core multisite-install ""
                  ""--url=\'{0}\' --title=\'{0}\' --admin_name=\'{1}\' ""
                  .format(data['site_name'], wo_wp_user) +
                  ""--admin_password= --admin_email=\'{0}\' ""
                  ""{subdomains}""
                  .format(wo_wp_email,
                          subdomains='--subdomains'
                          if not data['wpsubdir'] else ''))
        try:
            if WOShellExec.cmd_exec(
                self, ""{0} --allow-root ""
                .format(WOVar.wo_wpcli_path) +
                ""core multisite-install ""
                ""--url=\'{0}\' --title=\'{0}\' ""
                ""--admin_name=\'{1}\' ""
                .format(data['site_name'], wo_wp_user) +
                ""--admin_password=\'{0}\' ""
                ""--admin_email=\'{1}\' ""
                ""{subdomains}""
                .format(wo_wp_pass, wo_wp_email,
                        subdomains='--subdomains'
                        if not data['wpsubdir'] else ''),
                    log=False):
                pass
            else:
                Log.failed(self, ""Installing WordPress"")
                raise SiteError(
                    ""setup WordPress tables failed for wp multi site"")
        except CommandExecutionError:
            raise SiteError(""setup WordPress tables failed for wp multi site"")
    Log.valide(self, ""Installing WordPress"")
    Log.debug(self, ""Updating WordPress permalink"")
    try:
        WOShellExec.cmd_exec(self, "" {0} --allow-root ""
                             .format(WOVar.wo_wpcli_path) +
                             ""rewrite structure ""
                             ""/%postname%/"")
    except CommandExecutionError as e:
        Log.debug(self, str(e))
        raise SiteError(""Update wordpress permalinks failed"")

    """"""Install nginx-helper plugin """"""
    installwp_plugin(self, 'nginx-helper', data)
    if data['wpfc']:
        plugin_data_object = {""log_level"": ""INFO"",
                              ""log_filesize"": 5,
                              ""enable_purge"": 1,
                              ""enable_map"": ""0"",
                              ""enable_log"": 0,
                              ""enable_stamp"": 1,
                              ""purge_homepage_on_new"": 1,
                              ""purge_homepage_on_edit"": 1,
                              ""purge_homepage_on_del"": 1,
                              ""purge_archive_on_new"": 1,
                              ""purge_archive_on_edit"": 1,
                              ""purge_archive_on_del"": 1,
                              ""purge_archive_on_new_comment"": 0,
                              ""purge_archive_on_deleted_comment"": 0,
                              ""purge_page_on_mod"": 1,
                              ""purge_page_on_new_comment"": 1,
                              ""purge_page_on_deleted_comment"": 1,
                              ""cache_method"": ""enable_fastcgi"",
                              ""purge_method"": ""get_request"",
                              ""redis_hostname"": ""127.0.0.1"",
                              ""redis_port"": ""6379"",
                              ""redis_prefix"": ""nginx-cache:""}
        plugin_data = json.dumps(plugin_data_object)
        setupwp_plugin(self, ""nginx-helper"",
                       ""rt_wp_nginx_helper_options"", plugin_data, data)
    elif data['wpredis']:
        plugin_data_object = {""log_level"": ""INFO"",
                              ""log_filesize"": 5,
                              ""enable_purge"": 1,
                              ""enable_map"": ""0"",
                              ""enable_log"": 0,
                              ""enable_stamp"": 1,
                              ""purge_homepage_on_new"": 1,
                              ""purge_homepage_on_edit"": 1,
                              ""purge_homepage_on_del"": 1,
                              ""purge_archive_on_new"": 1,
                              ""purge_archive_on_edit"": 1,
                              ""purge_archive_on_del"": 1,
                              ""purge_archive_on_new_comment"": 0,
                              ""purge_archive_on_deleted_comment"": 0,
                              ""purge_page_on_mod"": 1,
                              ""purge_page_on_new_comment"": 1,
                              ""purge_page_on_deleted_comment"": 1,
                              ""cache_method"": ""enable_redis"",
                              ""purge_method"": ""get_request"",
                              ""redis_hostname"": ""127.0.0.1"",
                              ""redis_port"": ""6379"",
                              ""redis_prefix"": ""nginx-cache:""}
        plugin_data = json.dumps(plugin_data_object)
        setupwp_plugin(self, 'nginx-helper',
                       'rt_wp_nginx_helper_options', plugin_data, data)

    """"""Install Wp Super Cache""""""
    if data['wpsc']:
        installwp_plugin(self, 'wp-super-cache', data)

    """"""Install Redis Cache""""""
    if data['wpredis']:
        installwp_plugin(self, 'redis-cache', data)

    """"""Install Cache-Enabler""""""
    if data['wpce']:
        installwp_plugin(self, 'cache-enabler', data)
        plugin_data_object = {""expires"": 24,
                              ""new_post"": 1,
                              ""new_comment"": 0,
                              ""webp"": 0,
                              ""clear_on_upgrade"": 1,
                              ""compress"": 0,
                              ""excl_ids"": """",
                              ""excl_regexp"": """",
                              ""excl_cookies"": """",
                              ""incl_attributes"": """",
                              ""minify_html"": 1}
        plugin_data = json.dumps(plugin_data_object)
        setupwp_plugin(self, 'cache-enabler', 'cache-enabler',
                       plugin_data, data)
        WOShellExec.cmd_exec(
            self, ""/bin/bash -c \""{0} --allow-root ""
            .format(WOVar.wo_wpcli_path) +
            ""config set WP_CACHE ""
            ""true --raw\"""")

    if vhostonly:
        try:
            WOShellExec.cmd_exec(self, ""/bin/bash -c \""{0} --allow-root ""
                                 .format(WOVar.wo_wpcli_path) +
                                 ""db clean --yes\"""")
            WOFileUtils.chdir(self, '{0}'.format(wo_site_webroot))
            WOFileUtils.rm(self, ""{0}/htdocs"".format(wo_site_webroot))
            WOFileUtils.mkdir(self, ""{0}/htdocs"".format(wo_site_webroot))
            WOFileUtils.chown(self, ""{0}/htdocs"".format(wo_site_webroot),
                              'www-data', 'www-data')
        except CommandExecutionError:
            raise SiteError(""Cleaning WordPress install failed"")

    wp_creds = dict(wp_user=wo_wp_user, wp_pass=wo_wp_pass,
                    wp_email=wo_wp_email)

    return(wp_creds)","for wp_conf in wp_conf_variables:
    wp_var = wp_conf[0]
    wp_val = wp_conf[1]
    var_raw = bool(wp_val == 'true' or wp_val == 'false')
    try:
        WOShellExec.cmd_exec(self, '/bin/bash -c ""{0} --allow-root '.format(WOVar.wo_wpcli_path) + 'config set {0} \'{1}\' {wp_raw}""'.format(wp_var, wp_val, wp_raw='--raw' if var_raw is True else ''))
    except CommandExecutionError as e:
        Log.failed(self, 'Configuring WordPress')
        Log.debug(self, str(e))
        Log.error(self, 'Unable to define wp-config.php variables')","for wp_conf in wp_conf_variables:
    (wp_conf_0, wp_conf_1, *_) = wp_conf
    wp_var = wp_conf[0]
    wp_val = wp_conf[1]
    var_raw = bool(wp_val == 'true' or wp_val == 'false')
    try:
        WOShellExec.cmd_exec(self, '/bin/bash -c ""{0} --allow-root '.format(WOVar.wo_wpcli_path) + 'config set {0} \'{1}\' {wp_raw}""'.format(wp_var, wp_val, wp_raw='--raw' if var_raw is True else ''))
    except CommandExecutionError as e:
        Log.failed(self, 'Configuring WordPress')
        Log.debug(self, str(e))
        Log.error(self, 'Unable to define wp-config.php variables')","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
UER-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UER-py/uer/utils/mask.py,https://github.com/dbiir/UER-py/tree/master/uer/utils/mask.py,,mask_seq$5,"def mask_seq(src, tokenizer, whole_word_masking, span_masking, span_geo_prob, span_max_length):
    vocab = tokenizer.vocab
    PAD_ID = vocab.get(PAD_TOKEN)
    for i in range(len(src) - 1, -1, -1):
        if src[i] != PAD_ID:
            break
    src_no_pad = src[:i + 1]

    tokens_index, src_no_pad = create_index(src_no_pad, tokenizer, whole_word_masking, span_masking, span_geo_prob, span_max_length)
    if len(src_no_pad) < len(src):
        src = src_no_pad + (len(src) - len(src_no_pad)) * [PAD_ID]
    else:
        src = src_no_pad

    random.shuffle(tokens_index)
    num_to_predict = max(1, int(round(len(src_no_pad) * 0.15)))
    tgt_mlm = []
    for index_set in tokens_index:
        if len(tgt_mlm) >= num_to_predict:
            break
        if whole_word_masking:
            i = index_set[0]
            mask_len = index_set[1]
            if len(tgt_mlm) + mask_len > num_to_predict:
                continue

            for j in range(mask_len):
                token = src[i + j]
                tgt_mlm.append((i + j, token))
                prob = random.random()
                if prob < 0.8:
                    src[i + j] = vocab.get(MASK_TOKEN)
                elif prob < 0.9:
                    while True:
                        rdi = random.randint(1, len(vocab) - 1)
                        if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                            break
                    src[i + j] = rdi
        elif span_masking:
            i = index_set[0]
            span_len = index_set[1]
            if len(tgt_mlm) + span_len > num_to_predict:
                continue

            for j in range(span_len):
                token = src[i + j]
                tgt_mlm.append((i + j, token))
            prob = random.random()
            if prob < 0.8:
                for j in range(span_len):
                    src[i + j] = vocab.get(MASK_TOKEN)
            elif prob < 0.9:
                for j in range(span_len):
                    while True:
                        rdi = random.randint(1, len(vocab) - 1)
                        if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                            break
                    src[i + j] = rdi
        else:
            i = index_set[0]
            token = src[i]
            tgt_mlm.append((i, token))
            prob = random.random()
            if prob < 0.8:
                src[i] = vocab.get(MASK_TOKEN)
            elif prob < 0.9:
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i] = rdi
    tgt_mlm = sorted(tgt_mlm, key=lambda x: x[0])
    return src, tgt_mlm","for index_set in tokens_index:
    if len(tgt_mlm) >= num_to_predict:
        break
    if whole_word_masking:
        i = index_set[0]
        mask_len = index_set[1]
        if len(tgt_mlm) + mask_len > num_to_predict:
            continue
        for j in range(mask_len):
            token = src[i + j]
            tgt_mlm.append((i + j, token))
            prob = random.random()
            if prob < 0.8:
                src[i + j] = vocab.get(MASK_TOKEN)
            elif prob < 0.9:
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i + j] = rdi
    elif span_masking:
        i = index_set[0]
        span_len = index_set[1]
        if len(tgt_mlm) + span_len > num_to_predict:
            continue
        for j in range(span_len):
            token = src[i + j]
            tgt_mlm.append((i + j, token))
        prob = random.random()
        if prob < 0.8:
            for j in range(span_len):
                src[i + j] = vocab.get(MASK_TOKEN)
        elif prob < 0.9:
            for j in range(span_len):
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i + j] = rdi
    else:
        i = index_set[0]
        token = src[i]
        tgt_mlm.append((i, token))
        prob = random.random()
        if prob < 0.8:
            src[i] = vocab.get(MASK_TOKEN)
        elif prob < 0.9:
            while True:
                rdi = random.randint(1, len(vocab) - 1)
                if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                    break
            src[i] = rdi","for index_set in tokens_index:
    (index_set_0, index_set_1, *_) = index_set
    if len(tgt_mlm) >= num_to_predict:
        break
    if whole_word_masking:
        i = index_set[0]
        mask_len = index_set[1]
        if len(tgt_mlm) + mask_len > num_to_predict:
            continue
        for j in range(mask_len):
            token = src[i + j]
            tgt_mlm.append((i + j, token))
            prob = random.random()
            if prob < 0.8:
                src[i + j] = vocab.get(MASK_TOKEN)
            elif prob < 0.9:
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i + j] = rdi
    elif span_masking:
        i = index_set[0]
        span_len = index_set[1]
        if len(tgt_mlm) + span_len > num_to_predict:
            continue
        for j in range(span_len):
            token = src[i + j]
            tgt_mlm.append((i + j, token))
        prob = random.random()
        if prob < 0.8:
            for j in range(span_len):
                src[i + j] = vocab.get(MASK_TOKEN)
        elif prob < 0.9:
            for j in range(span_len):
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i + j] = rdi
    else:
        i = index_set[0]
        token = src[i]
        tgt_mlm.append((i, token))
        prob = random.random()
        if prob < 0.8:
            src[i] = vocab.get(MASK_TOKEN)
        elif prob < 0.9:
            while True:
                rdi = random.randint(1, len(vocab) - 1)
                if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                    break
            src[i] = rdi","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
	dbg.createLogWindow()
	global currentArgs
	currentArgs = copy.copy(args)
	try:
		starttime = datetime.datetime.now()
		ptr_counter = 0
		
		# initialize list of commands
		commands = {}
		
		# ----- HELP ----- #
		def getBanner():
			banners = {}
			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                         __               __                      |\n""
			bannertext += ""    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n""
			bannertext += ""    |  / ___/ __ \/ ___/ _ \/ / __ `/ __ \   / __/ _ \/ __ `/ __ `__ \ |\n""
			bannertext += ""    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n""
			bannertext += ""    | \___/\____/_/   \___/_/\__,_/_/ /_/   \__/\___/\__,_/_/ /_/ /_/  |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |     https://www.corelan.be | https://www.corelan-training.com    |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[0] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""			
			bannertext += ""    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n""
			bannertext += ""    |       | '_ ` _ \  / _ \ | '_ \  / _` |   | '_ \ | | | |          |\n""
			bannertext += ""    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n""
			bannertext += ""    |       |_| |_| |_| \___/ |_| |_| \__,_|(_)| .__/  \__, |          |\n""
			bannertext += ""    |                                          |_|     |___/           |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""	
			banners[1] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |    _____ ___  ____  ____  ____ _                                 |\n""
			bannertext += ""    |    / __ `__ \/ __ \/ __ \/ __ `/  https://www.corelan.be         |\n""
			bannertext += ""    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n""
			bannertext += ""    |  /_/ /_/ /_/\____/_/ /_/\__,_/  #corelan (Freenode IRC)          |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[2] = bannertext

			bannertext = """"
			bannertext += ""\n    .##.....##..#######..##....##....###........########..##....##\n""
			bannertext += ""    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n""
			bannertext += ""    .####.####.##.....##.####..##..##...##......##.....##...####..\n""
			bannertext += ""    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n""
			bannertext += ""    .##.....##.##.....##.##..####.#########.....##...........##...\n""
			bannertext += ""    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n""
			bannertext += ""    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n""
			banners[3] = bannertext


			# pick random banner
			bannerlist = []
			for i in range (0, len(banners)):
				bannerlist.append(i)

			random.shuffle(bannerlist)
			return banners[bannerlist[0]]

		
		def procHelp(args):
			dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__,str(arch)))
			dbg.log(""     Plugin version : %s r%s"" % (__VERSION__,__REV__))
			dbg.log(""     Python version : %s"" % (getPythonVersion()))
			if __DEBUGGERAPP__ == ""WinDBG"":
				pykdversion = dbg.getPyKDVersionNr()
				dbg.log(""     PyKD version %s"" % pykdversion)
			dbg.log(""     Written by Corelan - https://www.corelan.be"")
			dbg.log(""     Project page : https://github.com/corelan/mona"")
			dbg.logLines(getBanner(),highlight=1)
			dbg.log(""Global options :"")
			dbg.log(""----------------"")
			dbg.log(""You can use one or more of the following global options on any command that will perform"")
			dbg.log(""a search in one or more modules, returning a list of pointers :"")
			dbg.log("" -n                     : Skip modules that start with a null byte. If this is too broad, use"")
			dbg.log(""                          option -cp nonull instead"")
			dbg.log("" -o                     : Ignore OS modules"")
			dbg.log("" -p <nr>                : Stop search after <nr> pointers."")
			dbg.log("" -m <module,module,...> : only query the given modules. Be sure what you are doing !"")
			dbg.log(""                          You can specify multiple modules (comma separated)"")
			dbg.log(""                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored"")
			dbg.log(""                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,"")
			dbg.log(""                          blah or *blah* = contains blah"")
			dbg.log("" -cm <crit,crit,...>    : Apply some additional criteria to the modules to query."")
			dbg.log(""                          You can use one or more of the following criteria :"")
			dbg.log(""                          aslr,safeseh,rebase,nx,os"")
			dbg.log(""                          You can enable or disable a certain criterium by setting it to true or false"")
			dbg.log(""                          Example :  -cm aslr=true,safeseh=false"")
			dbg.log(""                          Suppose you want to search for p/p/r in aslr enabled modules, you could call"")
			dbg.log(""                          !mona seh -cm aslr"")
			dbg.log("" -cp <crit,crit,...>    : Apply some criteria to the pointers to return"")
			dbg.log(""                          Available options are :"")
			dbg.log(""                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev"")
			dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
			dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
			dbg.log(""                          You can use .. to indicate a range of bytes (in between 2 bad chars)"")
			dbg.log("" -x <access>            : Specify desired access level of the returning pointers. If not specified,"")
			dbg.log(""                          only executable pointers will be returned."")
			dbg.log(""                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *"")
			
			if not args:
				args = []
			if len(args) > 1:
				thiscmd = args[1].lower().strip()
				if thiscmd in commands:
					dbg.log("""")
					dbg.log(""Usage of command '%s' :"" % thiscmd)
					dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
					dbg.logLines(commands[thiscmd].usage)
					dbg.log("""")
				else:
					aliasfound = False
					for cmd in commands:
						if commands[cmd].alias == thiscmd:
							dbg.log("""")
							dbg.log(""Usage of command '%s' :"" % thiscmd)
							dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
							dbg.logLines(commands[cmd].usage)
							dbg.log("""")
							aliasfound = True
					if not aliasfound:
						dbg.logLines(""\nCommand %s does not exist. Run !mona to get a list of available commands\n"" % thiscmd,highlight=1)
			else:
				dbg.logLines(""\nUsage :"")
				dbg.logLines(""-------\n"")
				dbg.log("" !mona <command> <parameter>"")
				dbg.logLines(""\nAvailable commands and parameters :\n"")

				items = commands.items()
				items.sort(key = itemgetter(0))
				for item in items:
					if commands[item[0]].usage != """":
						aliastxt = """"
						if commands[item[0]].alias != """":
							aliastxt = "" / "" + commands[item[0]].alias
						dbg.logLines(""%s | %s"" % (item[0] + aliastxt + ("" "" * (20 - len(item[0]+aliastxt))), commands[item[0]].description))
				dbg.log("""")
				dbg.log(""Want more info about a given command ?  Run !mona help <command>"",highlight=1)
				dbg.log("""")
		
		commands[""help""] = MnCommand(""help"", ""show help"", ""!mona help [command]"",procHelp)
		
		# ----- Config file management ----- #
		
		def procConfig(args):
			#did we specify -get, -set or -add?
			showerror = False
			if not ""set"" in args and not ""get"" in args and not ""add"" in args:
				showerror = True
				
			if ""set"" in args:
				if type(args[""set""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""set""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""add"" in args:
				if type(args[""add""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""add""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""get"" in args:
				if type(args[""get""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""get""].split("" "")
					if len(params) < 1:
						showerror = True
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(configUsage,highlight=1)
				return
			else:
				if ""get"" in args:
					dbg.log(""Reading value from configuration file"")
					monaConfig = MnConfig()
					thevalue = monaConfig.get(args[""get""])
					dbg.log(""Parameter %s = %s"" % (args[""get""],thevalue))
				
				if ""set"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""set""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = args[""set""][0+len(configparam):len(args[""set""])]
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
				if ""add"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""add""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = monaConfig.get(configparam).strip() + "","" + args[""add""][0+len(configparam):len(args[""add""])].strip()
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
		# ----- Jump to register ----- #
	
		def procFindJ(args):
			return procFindJMP(args)
		
		def procFindJMP(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			
			if (inspect.stack()[1][3] == ""procFindJ""):
				dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."",highlight=1)

			criteria={}
			all_opcodes={}
			
			global ptr_to_get
			ptr_to_get = -1
			
			distancestr = """"
			mindistance = 0
			maxdistance = 0
			
			#did user specify -r <reg> ?
			showerror = False
			if ""r"" in args:
				if type(args[""r""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#valid register ?
					thisreg = args[""r""].upper().strip()
					validregs = dbglib.Registers32BitsOrder
					if not thisreg in validregs:
						showerror = True
			else:
				showerror = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0		
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0						
			
			if maxdistance < mindistance:
				tmp = maxdistance
				maxdistance = mindistance
				mindistance = tmp
			
			criteria[""mindistance""] = mindistance
			criteria[""maxdistance""] = maxdistance
			
			
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(jmpUsage,highlight=1)
				return				
			else:
				modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
				# go for it !	
				all_opcodes=findJMP(modulecriteria,criteria,args[""r""].lower().strip())
			
			# write to log
			logfile = MnLog(""jmp.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog)
		
		# ----- Exception Handler Overwrites ----- #
		
					
		def procFindSEH(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""safeseh""] = False
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False

			criteria = {}
			specialcases = {}
			all_opcodes = {}
			
			global ptr_to_get
			ptr_to_get = -1
			
			#what is the caller function (backwards compatibility with pvefindaddr)
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if ""rop"" in args:
				criteria[""rop""] = True
			
			if ""all"" in args:
				criteria[""all""] = True
				specialcases[""maponly""] = True
			else:
				criteria[""all""] = False
				specialcases[""maponly""] = False
			
			# go for it !	
			all_opcodes = findSEH(modulecriteria,criteria)
			#report findings to log
			logfile = MnLog(""seh.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog,specialcases)
			
			
		# ----- MODULES ------ #
		def procShowMODULES(args):
			modulecriteria={}
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			modulestosearch = getModulesToQuery(modulecriteria)
			showModuleTable("""",modulestosearch)

		# ----- ROP ----- #
		def procFindROPFUNC(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			#modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			ropfuncs = {}
			ropfuncoffsets ={}
			ropfuncs,ropfuncoffsets = findROPFUNC(modulecriteria,criteria)
			#report findings to log
			dbg.log(""[+] Processing pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc.txt"")
			thislog = logfile.reset()
			processResults(ropfuncs,logfile,thislog)
			global silent
			silent = True
			dbg.log(""[+] Processing offsets to pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc_offset.txt"")
			thislog = logfile.reset()
			processResults(ropfuncoffsets,logfile,thislog)			
			
		def procStackPivots(args):
			procROP(args,""stackpivot"")
			
		def procROP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			maxoffset = 40
			thedistance = 8
			split = False
			fast = False
			sortedprint = False
			endingstr = """"
			endings = []
			technique = """"            
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass
			
			if ""offset"" in args:
				if type(args[""offset""]).__name__.lower() != ""bool"":
					try:
						maxoffset = int(args[""offset""])
					except:
						pass
			
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() != ""bool"":
					try:
						thedistance = args[""distance""]
					except:
						pass
			
			if ""split"" in args:
				if type(args[""split""]).__name__.lower() == ""bool"":
					split = args[""split""]

			if ""s"" in args:
				if type(args[""s""]).__name__.lower() != ""bool"":
					technique = args[""s""].replace(""'"","""").replace('""',"""").strip().lower()                   
					
			if ""fast"" in args:
				if type(args[""fast""]).__name__.lower() == ""bool"":
					fast = args[""fast""]
			
			if ""end"" in args:
				if type(args[""end""]).__name__.lower() == ""str"":
					endingstr = args[""end""].replace(""'"","""").replace('""',"""").strip()
					endings = endingstr.split(""#"")
					
			if ""f"" in args:
				if args[""f""] != """":
					criteria[""f""] = args[""f""]
			
			if ""sort"" in args:
				sortedprint = True
			
			if ""rva"" in args:
				criteria[""rva""] = True
			
			if mode == ""stackpivot"":
				fast = False
				endings = """"
				split = False
			else:
				mode = ""all""
			
			findROPGADGETS(modulecriteria,criteria,endings,maxoffset,depth,split,thedistance,fast,mode,sortedprint,technique)
			

		def procJseh(args):
			results = []
			showred=0
			showall=False
			if ""all"" in args:
				showall = True
			nrfound = 0
			dbg.log(""-----------------------------------------------------------------------"")
			dbg.log(""Search for jmp/call dword[ebp/esp+nn] (and other) combinations started "")
			dbg.log(""-----------------------------------------------------------------------"")
			opcodej=[""\xff\x54\x24\x08"", #call dword ptr [esp+08]
					""\xff\x64\x24\x08"", #jmp dword ptr [esp+08]
					""\xff\x54\x24\x14"", #call dword ptr [esp+14]
					""\xff\x54\x24\x14"", #jmp dword ptr [esp+14]
					""\xff\x54\x24\x1c"", #call dword ptr [esp+1c]
					""\xff\x54\x24\x1c"", #jmp dword ptr [esp+1c]
					""\xff\x54\x24\x2c"", #call dword ptr [esp+2c]
					""\xff\x54\x24\x2c"", #jmp dword ptr [esp+2c]
					""\xff\x54\x24\x44"", #call dword ptr [esp+44]
					""\xff\x54\x24\x44"", #jmp dword ptr [esp+44]
					""\xff\x54\x24\x50"", #call dword ptr [esp+50]
					""\xff\x54\x24\x50"", #jmp dword ptr [esp+50]
					""\xff\x55\x0c"",     #call dword ptr [ebp+0c]
					""\xff\x65\x0c"",     #jmp dword ptr [ebp+0c]
					""\xff\x55\x24"",     #call dword ptr [ebp+24]
					""\xff\x65\x24"",     #jmp dword ptr [ebp+24]
					""\xff\x55\x30"",     #call dword ptr [ebp+30]
					""\xff\x65\x30"",     #jmp dword ptr [ebp+30]
					""\xff\x55\xfc"",     #call dword ptr [ebp-04]
					""\xff\x65\xfc"",     #jmp dword ptr [ebp-04]
					""\xff\x55\xf4"",     #call dword ptr [ebp-0c]
					""\xff\x65\xf4"",     #jmp dword ptr [ebp-0c]
					""\xff\x55\xe8"",     #call dword ptr [ebp-18]
					""\xff\x65\xe8"",     #jmp dword ptr [ebp-18]
					""\x83\xc4\x08\xc3"", #add esp,8 + ret
					""\x83\xc4\x08\xc2""] #add esp,8 + ret X
			fakeptrcriteria = {}
			fakeptrcriteria[""accesslevel""] = ""*""
			for opjc in opcodej:
				addys = []
				addys = searchInRange( [[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
				results += addys
				for ptrtypes in addys:
					for ad1 in addys[ptrtypes]:
						ptr = MnPointer(ad1)
						module = ptr.belongsTo()
						if not module:
							module=""""
							page   = dbg.getMemoryPageByAddress( ad1 )
							access = page.getAccess( human = True )
							op = dbg.disasm( ad1 )
							opstring=op.getDisasm()
							dbg.log(""Found %s at 0x%08x - Access: (%s) - Outside of a loaded module"" % (opstring, ad1, access), address = ad1,highlight=1)
							nrfound+=1
						else:
							if showall:
								page   = dbg.getMemoryPageByAddress( ad1 )
								access = page.getAccess( human = True )
								op = dbg.disasm( ad1 )
								opstring=op.getDisasm()
								thismod = MnModule(module)
								if not thismod.isSafeSEH:
								#if ismodulenosafeseh(module[0])==1:
									extratext=""=== Safeseh : NO ===""
									showred=1
								else:
									extratext=""Safeseh protected""
									showred=0
								dbg.log(""Found %s at 0x%08x (%s) - Access: (%s) - %s"" % (opstring, ad1, module,access,extratext), address = ad1,highlight=showred)
								nrfound+=1
			dbg.log(""Search complete"")
			if results:
				dbg.log(""Found %d address(es)"" % nrfound)
				return ""Found %d address(es) (Check the log Windows for details)"" % nrfound
			else:
				dbg.log(""No addresses found"")
				return ""Sorry, no addresses found""

			
		def procJOP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass			
			findJOPGADGETS(modulecriteria,criteria,depth)			
			
			
		def procCreatePATTERN(args):
			size = 0
			pattern = """"
			if ""?"" in args and args[""?""] != """":
				try:
					if ""0x"" in args[""?""].lower():
						try:
							size = int(args[""?""],16)
						except:
							size = 0
					else:
						size = int(args[""?""])
				except:
					size = 0
			if size == 0:
				dbg.log(""Please enter a valid size"",highlight=1)
			else:
				pattern = createPattern(size,args)
				dbg.log(""Creating cyclic pattern of %d bytes"" % size)				
				dbg.log(pattern)
				global ignoremodules
				ignoremodules = True
				objpatternfile = MnLog(""pattern.txt"")
				patternfile = objpatternfile.reset()
				# ASCII
				objpatternfile.write(""\nPattern of "" + str(size) + "" bytes :\n"",patternfile)
				objpatternfile.write(""-"" * (19 + len(str(size))),patternfile)
				objpatternfile.write(""\nASCII:"",patternfile)
				objpatternfile.write(""\n"" + pattern,patternfile)
				# Hex
				patternhex = """"
				for patternchar in pattern:
					patternhex += str(hex(ord(patternchar))).replace(""0x"",""\\x"")
				objpatternfile.write(""\n\nHEX:\n"",patternfile)
				objpatternfile.write(patternhex,patternfile)
				# Javascript
				patternjs = str2js(pattern)
				objpatternfile.write(""\n\nJAVASCRIPT (unescape() friendly):\n"",patternfile)
				objpatternfile.write(patternjs,patternfile)
				if not silent:
					dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"",highlight=1)
					dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile,highlight=1)
				
				ignoremodules = False
			return


		def procOffsetPATTERN(args):
			egg = """"
			if ""?"" in args and args[""?""] != """":
				try:
					egg = args[""?""]
				except:
					egg = """"
			if egg == """":
				dbg.log(""Please enter a valid target"",highlight=1)
			else:
				findOffsetInPattern(egg,-1,args)
			return
		
		# ----- Comparing file output ----- #
		def procFileCOMPARE(args):
			modulecriteria={}
			criteria={}
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			allfiles=[]
			tomatch=""""
			checkstrict=True
			rangeval = 0
			fast = False
			if ""ptronly"" in args or ""ptrsonly"" in args:
				fast = True
			if ""f"" in args:
				if args[""f""] != """":
					rawfilenames=args[""f""].replace('""',"""")
					allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
					dbg.log(""[+] Number of files to be examined : %d "" % len(allfiles))
			if ""range"" in args:
				if not type(args[""range""]).__name__.lower() == ""bool"":
					strrange = args[""range""].lower()
					if strrange.startswith(""0x"") and len(strrange) > 2 :
						rangeval = int(strrange,16)
					else:
						try:
							rangeval = int(args[""range""])
						except:
							rangeval = 0
					if rangeval > 0:
						dbg.log(""[+] Find overlap using pointer +/- range, value %d"" % rangeval)
						dbg.log(""    Note : this will significantly slow down the comparison process !"")
				else:
					dbg.log(""Please provide a numeric value ^(> 0) with option -range"",highlight=1)
					return
			else:
				if ""contains"" in args:
					if type(args[""contains""]).__name__.lower() == ""str"":
						tomatch = args[""contains""].replace(""'"","""").replace('""',"""")
				if ""nostrict"" in args:
					if type(args[""nostrict""]).__name__.lower() == ""bool"":
						checkstrict = not args[""nostrict""]
						dbg.log(""[+] Instructions must match in all files ? %s"" % checkstrict)
			# maybe one of the arguments is a folder
			callfiles = allfiles
			allfiles = []
			for tfile in callfiles:
				if os.path.isdir(tfile):
					# folder, get all files from this folder
					for root,dirs,files in os.walk(tfile):
						for dfile in files:
							allfiles.append(os.path.join(root,dfile))
				else:
					allfiles.append(tfile)
			if len(allfiles) > 1:
				findFILECOMPARISON(modulecriteria,criteria,allfiles,tomatch,checkstrict,rangeval,fast)
			else:
				dbg.log(""Please specify at least 2 filenames to compare"",highlight=1)

		# ----- Find bytes in memory ----- #
		def procFind(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			base = 0
			offset = 0
			top  = TOP_USERLAND
			consecutive = False
			ftype = """"
			
			level = 0
			offsetlevel = 0			
			
			if not ""a"" in args:
				args[""a""] = ""*""

			ptronly = False

			if ""ptronly"" in args or ""ptrsonly"" in args:
				ptronly = True	
			
			#search for all pointers by default
			if not ""x"" in args:
				args[""x""] = ""*""
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			if criteria[""accesslevel""] == """":
				return
			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			if ""unicode"" in args:
				criteria[""unic""] = True

			if ""b"" in args:
				try:
					base = int(args[""b""],16)
				except:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return
			if ""t"" in args:
				try:
					top = int(args[""t""],16)
				except:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
			if ""offset"" in args:
				if not args[""offset""].__class__.__name__ == ""bool"":
					if ""0x"" in args[""offset""].lower():
						try:
							offset = 0 - int(args[""offset""],16)
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return
					else:	
						try:
							offset = 0 - int(args[""offset""])
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return	
				else:
					dbg.log(""invalid offset value"",highlight=1)
					return
					
			if ""level"" in args:
				try:
					level = int(args[""level""])
				except:
					dbg.log(""invalid level value"",highlight=1)
					return

			if ""offsetlevel"" in args:
				try:
					offsetlevel = int(args[""offsetlevel""])
				except:
					dbg.log(""invalid offsetlevel value"",highlight=1)
					return						
					
			if ""c"" in args:
				dbg.log(""    - Skipping consecutive pointers, showing size instead"")			
				consecutive = True
				
			if ""type"" in args:
				if not args[""type""] in [""bin"",""asc"",""ptr"",""instr"",""file""]:
					dbg.log(""Invalid search type : %s"" % args[""type""], highlight=1)
					return
				ftype = args[""type""] 
				if ftype == ""file"":
					filename = args[""s""].replace('""',"""").replace(""'"","""")
					#see if we can read the file
					if not os.path.isfile(filename):
						dbg.log(""Unable to find/read file %s"" % filename,highlight=1)
						return
			rangep2p = 0

			
			if ""p2p"" in args or level > 0:
				dbg.log(""    - Looking for pointers to pointers"")
				criteria[""p2p""] = True
				if ""r"" in args:	
					try:
						rangep2p = int(args[""r""])
					except:
						pass
					if rangep2p > 0:
						dbg.log(""    - Will search for close pointers (%d bytes backwards)"" % rangep2p)
				if ""p2p"" in args:
					level = 1
			
			
			if level > 0:
				dbg.log(""    - Recursive levels : %d"" % level)
			

			allpointers = findPattern(modulecriteria,criteria,pattern,ftype,base,top,consecutive,rangep2p,level,offset,offsetlevel)
				
			logfile = MnLog(""find.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog,{},ptronly)
			return
			
			
		# ---- Find instructions, wildcard search ----- #
		def procFindWild(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			patterntype = """"
			base = 0
			top  = TOP_USERLAND
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			patterntypes = [""bin"",""str""]
			if ""type"" in args:
				if type(args[""type""]).__name__.lower() != ""bool"":
					if args[""type""] in patterntypes:
						patterntype = args[""type""]
					else:
						dbg.log(""-type argument only takes one of these values: %s"" % patterntypes,highlight=1)
						return
				else:
					dbg.log(""Please specify a valid value for -type. Valid values are %s"" % patterntypes,highlight=1)
					return


			if patterntype == """":
				if ""\\x"" in pattern:
					patterntype = ""bin""
				else:
					patterntype = ""str""
			
			if ""b"" in args:
				base,addyok = getAddyArg(args[""b""])
				if not addyok:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return

			if ""t"" in args:
				top,addyok = getAddyArg(args[""t""])
				if not addyok:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
					
			if ""depth"" in args:
				try:
					criteria[""depth""] = int(args[""depth""])
				except:
					dbg.log(""invalid depth value"",highlight=1)
					return	

			if ""all"" in args:
				criteria[""all""] = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					dbg.log(""invalid distance value(s)"",highlight=1)
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0	
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0	
			
				if maxdistance < mindistance:
					tmp = maxdistance
					maxdistance = mindistance
					mindistance = tmp
				
				criteria[""mindistance""] = mindistance
				criteria[""maxdistance""] = maxdistance
						
			allpointers = findPatternWild(modulecriteria,criteria,pattern,base,top,patterntype)
				
			logfile = MnLog(""findwild.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog)		
			return
	
			
		# ----- assemble: assemble instructions to opcodes ----- #
		def procAssemble(args):
			opcodes = """"
			encoder = """"
			
			if not 's' in args:
				dbg.log(""Mandatory argument -s <opcodes> missing"", highlight=1)
				return
			opcodes = args['s']
			
			if 'e' in args:
				# TODO: implement encoder support
				dbg.log(""Encoder support not yet implemented"", highlight=1)
				return
				encoder = args['e'].lowercase()
				if encoder not in [""ascii""]:
					dbg.log(""Invalid encoder : %s"" % encoder, highlight=1)
					return
			
			assemble(opcodes,encoder)
			
		# ----- info: show information about an address ----- #
		def procInfo(args):
			if not ""a"" in args:
				dbg.log(""Missing mandatory argument -a"", highlight=1)
				return
			
			address,addyok = getAddyArg(args[""a""])
			if not addyok:
				dbg.log(""%s is an invalid address"" % args[""a""], highlight=1)
				return
			
			ptr = MnPointer(address)
			modname = ptr.belongsTo()
			modinfo = None
			if modname != """":
				modinfo = MnModule(modname)
			rebase = """"
			rva=0
			if modinfo :
				rva = address - modinfo.moduleBase
			procFlags(args)
			dbg.log("""")			
			dbg.log(""[+] Information about address 0x%s"" % toHex(address))
			dbg.log(""    %s"" % ptr.__str__())
			thepage = dbg.getMemoryPageByAddress(address)
			dbg.log(""    Address is part of page 0x%08x - 0x%08x"" % (thepage.getBaseAddress(),thepage.getBaseAddress()+thepage.getSize()))
			section = """"
			try:
				section = thepage.getSection()
			except:
				section = """"
			if section != """":
				dbg.log(""    Section : %s"" % section)
			
			if ptr.isOnStack():
				stacks = getStacks()
				stackref = """"
				for tid in stacks:
					currstack = stacks[tid]
					if currstack[0] <= address and address <= currstack[1]:
						stackref = "" (Thread 0x%08x, Stack Base : 0x%08x, Stack Top : 0x%08x)"" % (tid,currstack[0],currstack[1])
						break
				dbg.log(""    This address is in a stack segment %s"" % stackref)
			if modinfo:
				dbg.log(""    Address is part of a module:"")
				dbg.log(""    %s"" % modinfo.__str__())
				if rva != 0:
					dbg.log(""    Offset from module base: 0x%x"" % rva)
					if modinfo:
						eatlist = modinfo.getEAT()
						if address in eatlist:
							dbg.log(""    Address is start of function '%s' in %s"" % (eatlist[address],modname))
						else:
							iatlist = modinfo.getIAT()
							if address in iatlist:
								iatentry = iatlist[address]
								dbg.log(""    Address is part of IAT, and contains pointer to '%s'"" % iatentry)				
			else:
				output = """"
				if ptr.isInHeap():
					dbg.log(""    This address resides in the heap"")
					dbg.log("""")
					ptr.showHeapBlockInfo()
				else:
					dbg.log(""    Module: None"")					
			try:
				dbg.log("""")
				dbg.log(""[+] Disassembly:"")
				op = dbg.disasm(address)
				opstring=getDisasmInstruction(op)
				dbg.log(""    Instruction at %s : %s"" % (toHex(address),opstring))
			except:
				pass
			if __DEBUGGERAPP__ == ""WinDBG"":
				dbg.log("""")
				dbg.log(""Output of !address 0x%08x:"" % address)
				output = dbg.nativeCommand(""!address 0x%08","for word in arguments:
    if word[0] == '-':
        word = word.lstrip('-')
        opts[word] = True
        last = word
    elif last != '':
        if str(opts[last]) == 'True':
            opts[last] = word
        else:
            opts[last] = opts[last] + ' ' + word","for word in arguments:
    (word_0, *word_rwordmaining) = word
    if word[0] == '-':
        word = word.lstrip('-')
        opts[word] = True
        last = word
    elif last != '':
        if str(opts[last]) == 'True':
            opts[last] = word
        else:
            opts[last] = opts[last] + ' ' + word",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
	dbg.createLogWindow()
	global currentArgs
	currentArgs = copy.copy(args)
	try:
		starttime = datetime.datetime.now()
		ptr_counter = 0
		
		# initialize list of commands
		commands = {}
		
		# ----- HELP ----- #
		def getBanner():
			banners = {}
			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                         __               __                      |\n""
			bannertext += ""    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n""
			bannertext += ""    |  / ___/ __ \/ ___/ _ \/ / __ `/ __ \   / __/ _ \/ __ `/ __ `__ \ |\n""
			bannertext += ""    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n""
			bannertext += ""    | \___/\____/_/   \___/_/\__,_/_/ /_/   \__/\___/\__,_/_/ /_/ /_/  |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |     https://www.corelan.be | https://www.corelan-training.com    |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[0] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""			
			bannertext += ""    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n""
			bannertext += ""    |       | '_ ` _ \  / _ \ | '_ \  / _` |   | '_ \ | | | |          |\n""
			bannertext += ""    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n""
			bannertext += ""    |       |_| |_| |_| \___/ |_| |_| \__,_|(_)| .__/  \__, |          |\n""
			bannertext += ""    |                                          |_|     |___/           |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""	
			banners[1] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |    _____ ___  ____  ____  ____ _                                 |\n""
			bannertext += ""    |    / __ `__ \/ __ \/ __ \/ __ `/  https://www.corelan.be         |\n""
			bannertext += ""    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n""
			bannertext += ""    |  /_/ /_/ /_/\____/_/ /_/\__,_/  #corelan (Freenode IRC)          |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[2] = bannertext

			bannertext = """"
			bannertext += ""\n    .##.....##..#######..##....##....###........########..##....##\n""
			bannertext += ""    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n""
			bannertext += ""    .####.####.##.....##.####..##..##...##......##.....##...####..\n""
			bannertext += ""    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n""
			bannertext += ""    .##.....##.##.....##.##..####.#########.....##...........##...\n""
			bannertext += ""    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n""
			bannertext += ""    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n""
			banners[3] = bannertext


			# pick random banner
			bannerlist = []
			for i in range (0, len(banners)):
				bannerlist.append(i)

			random.shuffle(bannerlist)
			return banners[bannerlist[0]]

		
		def procHelp(args):
			dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__,str(arch)))
			dbg.log(""     Plugin version : %s r%s"" % (__VERSION__,__REV__))
			dbg.log(""     Python version : %s"" % (getPythonVersion()))
			if __DEBUGGERAPP__ == ""WinDBG"":
				pykdversion = dbg.getPyKDVersionNr()
				dbg.log(""     PyKD version %s"" % pykdversion)
			dbg.log(""     Written by Corelan - https://www.corelan.be"")
			dbg.log(""     Project page : https://github.com/corelan/mona"")
			dbg.logLines(getBanner(),highlight=1)
			dbg.log(""Global options :"")
			dbg.log(""----------------"")
			dbg.log(""You can use one or more of the following global options on any command that will perform"")
			dbg.log(""a search in one or more modules, returning a list of pointers :"")
			dbg.log("" -n                     : Skip modules that start with a null byte. If this is too broad, use"")
			dbg.log(""                          option -cp nonull instead"")
			dbg.log("" -o                     : Ignore OS modules"")
			dbg.log("" -p <nr>                : Stop search after <nr> pointers."")
			dbg.log("" -m <module,module,...> : only query the given modules. Be sure what you are doing !"")
			dbg.log(""                          You can specify multiple modules (comma separated)"")
			dbg.log(""                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored"")
			dbg.log(""                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,"")
			dbg.log(""                          blah or *blah* = contains blah"")
			dbg.log("" -cm <crit,crit,...>    : Apply some additional criteria to the modules to query."")
			dbg.log(""                          You can use one or more of the following criteria :"")
			dbg.log(""                          aslr,safeseh,rebase,nx,os"")
			dbg.log(""                          You can enable or disable a certain criterium by setting it to true or false"")
			dbg.log(""                          Example :  -cm aslr=true,safeseh=false"")
			dbg.log(""                          Suppose you want to search for p/p/r in aslr enabled modules, you could call"")
			dbg.log(""                          !mona seh -cm aslr"")
			dbg.log("" -cp <crit,crit,...>    : Apply some criteria to the pointers to return"")
			dbg.log(""                          Available options are :"")
			dbg.log(""                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev"")
			dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
			dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
			dbg.log(""                          You can use .. to indicate a range of bytes (in between 2 bad chars)"")
			dbg.log("" -x <access>            : Specify desired access level of the returning pointers. If not specified,"")
			dbg.log(""                          only executable pointers will be returned."")
			dbg.log(""                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *"")
			
			if not args:
				args = []
			if len(args) > 1:
				thiscmd = args[1].lower().strip()
				if thiscmd in commands:
					dbg.log("""")
					dbg.log(""Usage of command '%s' :"" % thiscmd)
					dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
					dbg.logLines(commands[thiscmd].usage)
					dbg.log("""")
				else:
					aliasfound = False
					for cmd in commands:
						if commands[cmd].alias == thiscmd:
							dbg.log("""")
							dbg.log(""Usage of command '%s' :"" % thiscmd)
							dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
							dbg.logLines(commands[cmd].usage)
							dbg.log("""")
							aliasfound = True
					if not aliasfound:
						dbg.logLines(""\nCommand %s does not exist. Run !mona to get a list of available commands\n"" % thiscmd,highlight=1)
			else:
				dbg.logLines(""\nUsage :"")
				dbg.logLines(""-------\n"")
				dbg.log("" !mona <command> <parameter>"")
				dbg.logLines(""\nAvailable commands and parameters :\n"")

				items = commands.items()
				items.sort(key = itemgetter(0))
				for item in items:
					if commands[item[0]].usage != """":
						aliastxt = """"
						if commands[item[0]].alias != """":
							aliastxt = "" / "" + commands[item[0]].alias
						dbg.logLines(""%s | %s"" % (item[0] + aliastxt + ("" "" * (20 - len(item[0]+aliastxt))), commands[item[0]].description))
				dbg.log("""")
				dbg.log(""Want more info about a given command ?  Run !mona help <command>"",highlight=1)
				dbg.log("""")
		
		commands[""help""] = MnCommand(""help"", ""show help"", ""!mona help [command]"",procHelp)
		
		# ----- Config file management ----- #
		
		def procConfig(args):
			#did we specify -get, -set or -add?
			showerror = False
			if not ""set"" in args and not ""get"" in args and not ""add"" in args:
				showerror = True
				
			if ""set"" in args:
				if type(args[""set""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""set""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""add"" in args:
				if type(args[""add""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""add""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""get"" in args:
				if type(args[""get""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""get""].split("" "")
					if len(params) < 1:
						showerror = True
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(configUsage,highlight=1)
				return
			else:
				if ""get"" in args:
					dbg.log(""Reading value from configuration file"")
					monaConfig = MnConfig()
					thevalue = monaConfig.get(args[""get""])
					dbg.log(""Parameter %s = %s"" % (args[""get""],thevalue))
				
				if ""set"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""set""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = args[""set""][0+len(configparam):len(args[""set""])]
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
				if ""add"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""add""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = monaConfig.get(configparam).strip() + "","" + args[""add""][0+len(configparam):len(args[""add""])].strip()
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
		# ----- Jump to register ----- #
	
		def procFindJ(args):
			return procFindJMP(args)
		
		def procFindJMP(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			
			if (inspect.stack()[1][3] == ""procFindJ""):
				dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."",highlight=1)

			criteria={}
			all_opcodes={}
			
			global ptr_to_get
			ptr_to_get = -1
			
			distancestr = """"
			mindistance = 0
			maxdistance = 0
			
			#did user specify -r <reg> ?
			showerror = False
			if ""r"" in args:
				if type(args[""r""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#valid register ?
					thisreg = args[""r""].upper().strip()
					validregs = dbglib.Registers32BitsOrder
					if not thisreg in validregs:
						showerror = True
			else:
				showerror = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0		
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0						
			
			if maxdistance < mindistance:
				tmp = maxdistance
				maxdistance = mindistance
				mindistance = tmp
			
			criteria[""mindistance""] = mindistance
			criteria[""maxdistance""] = maxdistance
			
			
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(jmpUsage,highlight=1)
				return				
			else:
				modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
				# go for it !	
				all_opcodes=findJMP(modulecriteria,criteria,args[""r""].lower().strip())
			
			# write to log
			logfile = MnLog(""jmp.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog)
		
		# ----- Exception Handler Overwrites ----- #
		
					
		def procFindSEH(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""safeseh""] = False
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False

			criteria = {}
			specialcases = {}
			all_opcodes = {}
			
			global ptr_to_get
			ptr_to_get = -1
			
			#what is the caller function (backwards compatibility with pvefindaddr)
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if ""rop"" in args:
				criteria[""rop""] = True
			
			if ""all"" in args:
				criteria[""all""] = True
				specialcases[""maponly""] = True
			else:
				criteria[""all""] = False
				specialcases[""maponly""] = False
			
			# go for it !	
			all_opcodes = findSEH(modulecriteria,criteria)
			#report findings to log
			logfile = MnLog(""seh.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog,specialcases)
			
			
		# ----- MODULES ------ #
		def procShowMODULES(args):
			modulecriteria={}
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			modulestosearch = getModulesToQuery(modulecriteria)
			showModuleTable("""",modulestosearch)

		# ----- ROP ----- #
		def procFindROPFUNC(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			#modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			ropfuncs = {}
			ropfuncoffsets ={}
			ropfuncs,ropfuncoffsets = findROPFUNC(modulecriteria,criteria)
			#report findings to log
			dbg.log(""[+] Processing pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc.txt"")
			thislog = logfile.reset()
			processResults(ropfuncs,logfile,thislog)
			global silent
			silent = True
			dbg.log(""[+] Processing offsets to pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc_offset.txt"")
			thislog = logfile.reset()
			processResults(ropfuncoffsets,logfile,thislog)			
			
		def procStackPivots(args):
			procROP(args,""stackpivot"")
			
		def procROP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			maxoffset = 40
			thedistance = 8
			split = False
			fast = False
			sortedprint = False
			endingstr = """"
			endings = []
			technique = """"            
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass
			
			if ""offset"" in args:
				if type(args[""offset""]).__name__.lower() != ""bool"":
					try:
						maxoffset = int(args[""offset""])
					except:
						pass
			
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() != ""bool"":
					try:
						thedistance = args[""distance""]
					except:
						pass
			
			if ""split"" in args:
				if type(args[""split""]).__name__.lower() == ""bool"":
					split = args[""split""]

			if ""s"" in args:
				if type(args[""s""]).__name__.lower() != ""bool"":
					technique = args[""s""].replace(""'"","""").replace('""',"""").strip().lower()                   
					
			if ""fast"" in args:
				if type(args[""fast""]).__name__.lower() == ""bool"":
					fast = args[""fast""]
			
			if ""end"" in args:
				if type(args[""end""]).__name__.lower() == ""str"":
					endingstr = args[""end""].replace(""'"","""").replace('""',"""").strip()
					endings = endingstr.split(""#"")
					
			if ""f"" in args:
				if args[""f""] != """":
					criteria[""f""] = args[""f""]
			
			if ""sort"" in args:
				sortedprint = True
			
			if ""rva"" in args:
				criteria[""rva""] = True
			
			if mode == ""stackpivot"":
				fast = False
				endings = """"
				split = False
			else:
				mode = ""all""
			
			findROPGADGETS(modulecriteria,criteria,endings,maxoffset,depth,split,thedistance,fast,mode,sortedprint,technique)
			

		def procJseh(args):
			results = []
			showred=0
			showall=False
			if ""all"" in args:
				showall = True
			nrfound = 0
			dbg.log(""-----------------------------------------------------------------------"")
			dbg.log(""Search for jmp/call dword[ebp/esp+nn] (and other) combinations started "")
			dbg.log(""-----------------------------------------------------------------------"")
			opcodej=[""\xff\x54\x24\x08"", #call dword ptr [esp+08]
					""\xff\x64\x24\x08"", #jmp dword ptr [esp+08]
					""\xff\x54\x24\x14"", #call dword ptr [esp+14]
					""\xff\x54\x24\x14"", #jmp dword ptr [esp+14]
					""\xff\x54\x24\x1c"", #call dword ptr [esp+1c]
					""\xff\x54\x24\x1c"", #jmp dword ptr [esp+1c]
					""\xff\x54\x24\x2c"", #call dword ptr [esp+2c]
					""\xff\x54\x24\x2c"", #jmp dword ptr [esp+2c]
					""\xff\x54\x24\x44"", #call dword ptr [esp+44]
					""\xff\x54\x24\x44"", #jmp dword ptr [esp+44]
					""\xff\x54\x24\x50"", #call dword ptr [esp+50]
					""\xff\x54\x24\x50"", #jmp dword ptr [esp+50]
					""\xff\x55\x0c"",     #call dword ptr [ebp+0c]
					""\xff\x65\x0c"",     #jmp dword ptr [ebp+0c]
					""\xff\x55\x24"",     #call dword ptr [ebp+24]
					""\xff\x65\x24"",     #jmp dword ptr [ebp+24]
					""\xff\x55\x30"",     #call dword ptr [ebp+30]
					""\xff\x65\x30"",     #jmp dword ptr [ebp+30]
					""\xff\x55\xfc"",     #call dword ptr [ebp-04]
					""\xff\x65\xfc"",     #jmp dword ptr [ebp-04]
					""\xff\x55\xf4"",     #call dword ptr [ebp-0c]
					""\xff\x65\xf4"",     #jmp dword ptr [ebp-0c]
					""\xff\x55\xe8"",     #call dword ptr [ebp-18]
					""\xff\x65\xe8"",     #jmp dword ptr [ebp-18]
					""\x83\xc4\x08\xc3"", #add esp,8 + ret
					""\x83\xc4\x08\xc2""] #add esp,8 + ret X
			fakeptrcriteria = {}
			fakeptrcriteria[""accesslevel""] = ""*""
			for opjc in opcodej:
				addys = []
				addys = searchInRange( [[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
				results += addys
				for ptrtypes in addys:
					for ad1 in addys[ptrtypes]:
						ptr = MnPointer(ad1)
						module = ptr.belongsTo()
						if not module:
							module=""""
							page   = dbg.getMemoryPageByAddress( ad1 )
							access = page.getAccess( human = True )
							op = dbg.disasm( ad1 )
							opstring=op.getDisasm()
							dbg.log(""Found %s at 0x%08x - Access: (%s) - Outside of a loaded module"" % (opstring, ad1, access), address = ad1,highlight=1)
							nrfound+=1
						else:
							if showall:
								page   = dbg.getMemoryPageByAddress( ad1 )
								access = page.getAccess( human = True )
								op = dbg.disasm( ad1 )
								opstring=op.getDisasm()
								thismod = MnModule(module)
								if not thismod.isSafeSEH:
								#if ismodulenosafeseh(module[0])==1:
									extratext=""=== Safeseh : NO ===""
									showred=1
								else:
									extratext=""Safeseh protected""
									showred=0
								dbg.log(""Found %s at 0x%08x (%s) - Access: (%s) - %s"" % (opstring, ad1, module,access,extratext), address = ad1,highlight=showred)
								nrfound+=1
			dbg.log(""Search complete"")
			if results:
				dbg.log(""Found %d address(es)"" % nrfound)
				return ""Found %d address(es) (Check the log Windows for details)"" % nrfound
			else:
				dbg.log(""No addresses found"")
				return ""Sorry, no addresses found""

			
		def procJOP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass			
			findJOPGADGETS(modulecriteria,criteria,depth)			
			
			
		def procCreatePATTERN(args):
			size = 0
			pattern = """"
			if ""?"" in args and args[""?""] != """":
				try:
					if ""0x"" in args[""?""].lower():
						try:
							size = int(args[""?""],16)
						except:
							size = 0
					else:
						size = int(args[""?""])
				except:
					size = 0
			if size == 0:
				dbg.log(""Please enter a valid size"",highlight=1)
			else:
				pattern = createPattern(size,args)
				dbg.log(""Creating cyclic pattern of %d bytes"" % size)				
				dbg.log(pattern)
				global ignoremodules
				ignoremodules = True
				objpatternfile = MnLog(""pattern.txt"")
				patternfile = objpatternfile.reset()
				# ASCII
				objpatternfile.write(""\nPattern of "" + str(size) + "" bytes :\n"",patternfile)
				objpatternfile.write(""-"" * (19 + len(str(size))),patternfile)
				objpatternfile.write(""\nASCII:"",patternfile)
				objpatternfile.write(""\n"" + pattern,patternfile)
				# Hex
				patternhex = """"
				for patternchar in pattern:
					patternhex += str(hex(ord(patternchar))).replace(""0x"",""\\x"")
				objpatternfile.write(""\n\nHEX:\n"",patternfile)
				objpatternfile.write(patternhex,patternfile)
				# Javascript
				patternjs = str2js(pattern)
				objpatternfile.write(""\n\nJAVASCRIPT (unescape() friendly):\n"",patternfile)
				objpatternfile.write(patternjs,patternfile)
				if not silent:
					dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"",highlight=1)
					dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile,highlight=1)
				
				ignoremodules = False
			return


		def procOffsetPATTERN(args):
			egg = """"
			if ""?"" in args and args[""?""] != """":
				try:
					egg = args[""?""]
				except:
					egg = """"
			if egg == """":
				dbg.log(""Please enter a valid target"",highlight=1)
			else:
				findOffsetInPattern(egg,-1,args)
			return
		
		# ----- Comparing file output ----- #
		def procFileCOMPARE(args):
			modulecriteria={}
			criteria={}
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			allfiles=[]
			tomatch=""""
			checkstrict=True
			rangeval = 0
			fast = False
			if ""ptronly"" in args or ""ptrsonly"" in args:
				fast = True
			if ""f"" in args:
				if args[""f""] != """":
					rawfilenames=args[""f""].replace('""',"""")
					allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
					dbg.log(""[+] Number of files to be examined : %d "" % len(allfiles))
			if ""range"" in args:
				if not type(args[""range""]).__name__.lower() == ""bool"":
					strrange = args[""range""].lower()
					if strrange.startswith(""0x"") and len(strrange) > 2 :
						rangeval = int(strrange,16)
					else:
						try:
							rangeval = int(args[""range""])
						except:
							rangeval = 0
					if rangeval > 0:
						dbg.log(""[+] Find overlap using pointer +/- range, value %d"" % rangeval)
						dbg.log(""    Note : this will significantly slow down the comparison process !"")
				else:
					dbg.log(""Please provide a numeric value ^(> 0) with option -range"",highlight=1)
					return
			else:
				if ""contains"" in args:
					if type(args[""contains""]).__name__.lower() == ""str"":
						tomatch = args[""contains""].replace(""'"","""").replace('""',"""")
				if ""nostrict"" in args:
					if type(args[""nostrict""]).__name__.lower() == ""bool"":
						checkstrict = not args[""nostrict""]
						dbg.log(""[+] Instructions must match in all files ? %s"" % checkstrict)
			# maybe one of the arguments is a folder
			callfiles = allfiles
			allfiles = []
			for tfile in callfiles:
				if os.path.isdir(tfile):
					# folder, get all files from this folder
					for root,dirs,files in os.walk(tfile):
						for dfile in files:
							allfiles.append(os.path.join(root,dfile))
				else:
					allfiles.append(tfile)
			if len(allfiles) > 1:
				findFILECOMPARISON(modulecriteria,criteria,allfiles,tomatch,checkstrict,rangeval,fast)
			else:
				dbg.log(""Please specify at least 2 filenames to compare"",highlight=1)

		# ----- Find bytes in memory ----- #
		def procFind(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			base = 0
			offset = 0
			top  = TOP_USERLAND
			consecutive = False
			ftype = """"
			
			level = 0
			offsetlevel = 0			
			
			if not ""a"" in args:
				args[""a""] = ""*""

			ptronly = False

			if ""ptronly"" in args or ""ptrsonly"" in args:
				ptronly = True	
			
			#search for all pointers by default
			if not ""x"" in args:
				args[""x""] = ""*""
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			if criteria[""accesslevel""] == """":
				return
			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			if ""unicode"" in args:
				criteria[""unic""] = True

			if ""b"" in args:
				try:
					base = int(args[""b""],16)
				except:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return
			if ""t"" in args:
				try:
					top = int(args[""t""],16)
				except:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
			if ""offset"" in args:
				if not args[""offset""].__class__.__name__ == ""bool"":
					if ""0x"" in args[""offset""].lower():
						try:
							offset = 0 - int(args[""offset""],16)
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return
					else:	
						try:
							offset = 0 - int(args[""offset""])
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return	
				else:
					dbg.log(""invalid offset value"",highlight=1)
					return
					
			if ""level"" in args:
				try:
					level = int(args[""level""])
				except:
					dbg.log(""invalid level value"",highlight=1)
					return

			if ""offsetlevel"" in args:
				try:
					offsetlevel = int(args[""offsetlevel""])
				except:
					dbg.log(""invalid offsetlevel value"",highlight=1)
					return						
					
			if ""c"" in args:
				dbg.log(""    - Skipping consecutive pointers, showing size instead"")			
				consecutive = True
				
			if ""type"" in args:
				if not args[""type""] in [""bin"",""asc"",""ptr"",""instr"",""file""]:
					dbg.log(""Invalid search type : %s"" % args[""type""], highlight=1)
					return
				ftype = args[""type""] 
				if ftype == ""file"":
					filename = args[""s""].replace('""',"""").replace(""'"","""")
					#see if we can read the file
					if not os.path.isfile(filename):
						dbg.log(""Unable to find/read file %s"" % filename,highlight=1)
						return
			rangep2p = 0

			
			if ""p2p"" in args or level > 0:
				dbg.log(""    - Looking for pointers to pointers"")
				criteria[""p2p""] = True
				if ""r"" in args:	
					try:
						rangep2p = int(args[""r""])
					except:
						pass
					if rangep2p > 0:
						dbg.log(""    - Will search for close pointers (%d bytes backwards)"" % rangep2p)
				if ""p2p"" in args:
					level = 1
			
			
			if level > 0:
				dbg.log(""    - Recursive levels : %d"" % level)
			

			allpointers = findPattern(modulecriteria,criteria,pattern,ftype,base,top,consecutive,rangep2p,level,offset,offsetlevel)
				
			logfile = MnLog(""find.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog,{},ptronly)
			return
			
			
		# ---- Find instructions, wildcard search ----- #
		def procFindWild(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			patterntype = """"
			base = 0
			top  = TOP_USERLAND
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			patterntypes = [""bin"",""str""]
			if ""type"" in args:
				if type(args[""type""]).__name__.lower() != ""bool"":
					if args[""type""] in patterntypes:
						patterntype = args[""type""]
					else:
						dbg.log(""-type argument only takes one of these values: %s"" % patterntypes,highlight=1)
						return
				else:
					dbg.log(""Please specify a valid value for -type. Valid values are %s"" % patterntypes,highlight=1)
					return


			if patterntype == """":
				if ""\\x"" in pattern:
					patterntype = ""bin""
				else:
					patterntype = ""str""
			
			if ""b"" in args:
				base,addyok = getAddyArg(args[""b""])
				if not addyok:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return

			if ""t"" in args:
				top,addyok = getAddyArg(args[""t""])
				if not addyok:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
					
			if ""depth"" in args:
				try:
					criteria[""depth""] = int(args[""depth""])
				except:
					dbg.log(""invalid depth value"",highlight=1)
					return	

			if ""all"" in args:
				criteria[""all""] = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					dbg.log(""invalid distance value(s)"",highlight=1)
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0	
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0	
			
				if maxdistance < mindistance:
					tmp = maxdistance
					maxdistance = mindistance
					mindistance = tmp
				
				criteria[""mindistance""] = mindistance
				criteria[""maxdistance""] = maxdistance
						
			allpointers = findPatternWild(modulecriteria,criteria,pattern,base,top,patterntype)
				
			logfile = MnLog(""findwild.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog)		
			return
	
			
		# ----- assemble: assemble instructions to opcodes ----- #
		def procAssemble(args):
			opcodes = """"
			encoder = """"
			
			if not 's' in args:
				dbg.log(""Mandatory argument -s <opcodes> missing"", highlight=1)
				return
			opcodes = args['s']
			
			if 'e' in args:
				# TODO: implement encoder support
				dbg.log(""Encoder support not yet implemented"", highlight=1)
				return
				encoder = args['e'].lowercase()
				if encoder not in [""ascii""]:
					dbg.log(""Invalid encoder : %s"" % encoder, highlight=1)
					return
			
			assemble(opcodes,encoder)
			
		# ----- info: show information about an address ----- #
		def procInfo(args):
			if not ""a"" in args:
				dbg.log(""Missing mandatory argument -a"", highlight=1)
				return
			
			address,addyok = getAddyArg(args[""a""])
			if not addyok:
				dbg.log(""%s is an invalid address"" % args[""a""], highlight=1)
				return
			
			ptr = MnPointer(address)
			modname = ptr.belongsTo()
			modinfo = None
			if modname != """":
				modinfo = MnModule(modname)
			rebase = """"
			rva=0
			if modinfo :
				rva = address - modinfo.moduleBase
			procFlags(args)
			dbg.log("""")			
			dbg.log(""[+] Information about address 0x%s"" % toHex(address))
			dbg.log(""    %s"" % ptr.__str__())
			thepage = dbg.getMemoryPageByAddress(address)
			dbg.log(""    Address is part of page 0x%08x - 0x%08x"" % (thepage.getBaseAddress(),thepage.getBaseAddress()+thepage.getSize()))
			section = """"
			try:
				section = thepage.getSection()
			except:
				section = """"
			if section != """":
				dbg.log(""    Section : %s"" % section)
			
			if ptr.isOnStack():
				stacks = getStacks()
				stackref = """"
				for tid in stacks:
					currstack = stacks[tid]
					if currstack[0] <= address and address <= currstack[1]:
						stackref = "" (Thread 0x%08x, Stack Base : 0x%08x, Stack Top : 0x%08x)"" % (tid,currstack[0],currstack[1])
						break
				dbg.log(""    This address is in a stack segment %s"" % stackref)
			if modinfo:
				dbg.log(""    Address is part of a module:"")
				dbg.log(""    %s"" % modinfo.__str__())
				if rva != 0:
					dbg.log(""    Offset from module base: 0x%x"" % rva)
					if modinfo:
						eatlist = modinfo.getEAT()
						if address in eatlist:
							dbg.log(""    Address is start of function '%s' in %s"" % (eatlist[address],modname))
						else:
							iatlist = modinfo.getIAT()
							if address in iatlist:
								iatentry = iatlist[address]
								dbg.log(""    Address is part of IAT, and contains pointer to '%s'"" % iatentry)				
			else:
				output = """"
				if ptr.isInHeap():
					dbg.log(""    This address resides in the heap"")
					dbg.log("""")
					ptr.showHeapBlockInfo()
				else:
					dbg.log(""    Module: None"")					
			try:
				dbg.log("""")
				dbg.log(""[+] Disassembly:"")
				op = dbg.disasm(address)
				opstring=getDisasmInstruction(op)
				dbg.log(""    Instruction at %s : %s"" % (toHex(address),opstring))
			except:
				pass
			if __DEBUGGERAPP__ == ""WinDBG"":
				dbg.log("""")
				dbg.log(""Output of !address 0x%08x:"" % address)
				output = dbg.nativeCommand(""!address 0x%08","for (name, value) in zip(registers_to_fill, values_to_generate_all_255_values):
    padding = ''
    if value < 16:
        padding = '0'
    if 'h' in name:
        prefix += 'mov e%sx,0x4100%s%s00; ' % (name[0], padding, hex(value)[2:])
        prefix += 'add [ebp],ch; '
        additionalLength += 8
    if 'l' in name:
        prefix += 'mov e%sx,0x4100%s%s00; ' % (buf_sig, padding, hex(value)[2:])
        prefix += 'add %s,%sh; ' % (name, buf_sig)
        prefix += 'add [ebp],ch; '
        additionalLength += 10","for (name, value) in zip(registers_to_fill, values_to_generate_all_255_values):
    (name_0, *name_rnamemaining) = name
    padding = ''
    if value < 16:
        padding = '0'
    if 'h' in name:
        prefix += 'mov e%sx,0x4100%s%s00; ' % (name[0], padding, hex(value)[2:])
        prefix += 'add [ebp],ch; '
        additionalLength += 8
    if 'l' in name:
        prefix += 'mov e%sx,0x4100%s%s00; ' % (buf_sig, padding, hex(value)[2:])
        prefix += 'add %s,%sh; ' % (name, buf_sig)
        prefix += 'add [ebp],ch; '
        additionalLength += 10",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
	dbg.createLogWindow()
	global currentArgs
	currentArgs = copy.copy(args)
	try:
		starttime = datetime.datetime.now()
		ptr_counter = 0
		
		# initialize list of commands
		commands = {}
		
		# ----- HELP ----- #
		def getBanner():
			banners = {}
			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                         __               __                      |\n""
			bannertext += ""    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n""
			bannertext += ""    |  / ___/ __ \/ ___/ _ \/ / __ `/ __ \   / __/ _ \/ __ `/ __ `__ \ |\n""
			bannertext += ""    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n""
			bannertext += ""    | \___/\____/_/   \___/_/\__,_/_/ /_/   \__/\___/\__,_/_/ /_/ /_/  |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |     https://www.corelan.be | https://www.corelan-training.com    |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[0] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""			
			bannertext += ""    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n""
			bannertext += ""    |       | '_ ` _ \  / _ \ | '_ \  / _` |   | '_ \ | | | |          |\n""
			bannertext += ""    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n""
			bannertext += ""    |       |_| |_| |_| \___/ |_| |_| \__,_|(_)| .__/  \__, |          |\n""
			bannertext += ""    |                                          |_|     |___/           |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""	
			banners[1] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |    _____ ___  ____  ____  ____ _                                 |\n""
			bannertext += ""    |    / __ `__ \/ __ \/ __ \/ __ `/  https://www.corelan.be         |\n""
			bannertext += ""    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n""
			bannertext += ""    |  /_/ /_/ /_/\____/_/ /_/\__,_/  #corelan (Freenode IRC)          |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[2] = bannertext

			bannertext = """"
			bannertext += ""\n    .##.....##..#######..##....##....###........########..##....##\n""
			bannertext += ""    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n""
			bannertext += ""    .####.####.##.....##.####..##..##...##......##.....##...####..\n""
			bannertext += ""    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n""
			bannertext += ""    .##.....##.##.....##.##..####.#########.....##...........##...\n""
			bannertext += ""    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n""
			bannertext += ""    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n""
			banners[3] = bannertext


			# pick random banner
			bannerlist = []
			for i in range (0, len(banners)):
				bannerlist.append(i)

			random.shuffle(bannerlist)
			return banners[bannerlist[0]]

		
		def procHelp(args):
			dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__,str(arch)))
			dbg.log(""     Plugin version : %s r%s"" % (__VERSION__,__REV__))
			dbg.log(""     Python version : %s"" % (getPythonVersion()))
			if __DEBUGGERAPP__ == ""WinDBG"":
				pykdversion = dbg.getPyKDVersionNr()
				dbg.log(""     PyKD version %s"" % pykdversion)
			dbg.log(""     Written by Corelan - https://www.corelan.be"")
			dbg.log(""     Project page : https://github.com/corelan/mona"")
			dbg.logLines(getBanner(),highlight=1)
			dbg.log(""Global options :"")
			dbg.log(""----------------"")
			dbg.log(""You can use one or more of the following global options on any command that will perform"")
			dbg.log(""a search in one or more modules, returning a list of pointers :"")
			dbg.log("" -n                     : Skip modules that start with a null byte. If this is too broad, use"")
			dbg.log(""                          option -cp nonull instead"")
			dbg.log("" -o                     : Ignore OS modules"")
			dbg.log("" -p <nr>                : Stop search after <nr> pointers."")
			dbg.log("" -m <module,module,...> : only query the given modules. Be sure what you are doing !"")
			dbg.log(""                          You can specify multiple modules (comma separated)"")
			dbg.log(""                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored"")
			dbg.log(""                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,"")
			dbg.log(""                          blah or *blah* = contains blah"")
			dbg.log("" -cm <crit,crit,...>    : Apply some additional criteria to the modules to query."")
			dbg.log(""                          You can use one or more of the following criteria :"")
			dbg.log(""                          aslr,safeseh,rebase,nx,os"")
			dbg.log(""                          You can enable or disable a certain criterium by setting it to true or false"")
			dbg.log(""                          Example :  -cm aslr=true,safeseh=false"")
			dbg.log(""                          Suppose you want to search for p/p/r in aslr enabled modules, you could call"")
			dbg.log(""                          !mona seh -cm aslr"")
			dbg.log("" -cp <crit,crit,...>    : Apply some criteria to the pointers to return"")
			dbg.log(""                          Available options are :"")
			dbg.log(""                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev"")
			dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
			dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
			dbg.log(""                          You can use .. to indicate a range of bytes (in between 2 bad chars)"")
			dbg.log("" -x <access>            : Specify desired access level of the returning pointers. If not specified,"")
			dbg.log(""                          only executable pointers will be returned."")
			dbg.log(""                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *"")
			
			if not args:
				args = []
			if len(args) > 1:
				thiscmd = args[1].lower().strip()
				if thiscmd in commands:
					dbg.log("""")
					dbg.log(""Usage of command '%s' :"" % thiscmd)
					dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
					dbg.logLines(commands[thiscmd].usage)
					dbg.log("""")
				else:
					aliasfound = False
					for cmd in commands:
						if commands[cmd].alias == thiscmd:
							dbg.log("""")
							dbg.log(""Usage of command '%s' :"" % thiscmd)
							dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
							dbg.logLines(commands[cmd].usage)
							dbg.log("""")
							aliasfound = True
					if not aliasfound:
						dbg.logLines(""\nCommand %s does not exist. Run !mona to get a list of available commands\n"" % thiscmd,highlight=1)
			else:
				dbg.logLines(""\nUsage :"")
				dbg.logLines(""-------\n"")
				dbg.log("" !mona <command> <parameter>"")
				dbg.logLines(""\nAvailable commands and parameters :\n"")

				items = commands.items()
				items.sort(key = itemgetter(0))
				for item in items:
					if commands[item[0]].usage != """":
						aliastxt = """"
						if commands[item[0]].alias != """":
							aliastxt = "" / "" + commands[item[0]].alias
						dbg.logLines(""%s | %s"" % (item[0] + aliastxt + ("" "" * (20 - len(item[0]+aliastxt))), commands[item[0]].description))
				dbg.log("""")
				dbg.log(""Want more info about a given command ?  Run !mona help <command>"",highlight=1)
				dbg.log("""")
		
		commands[""help""] = MnCommand(""help"", ""show help"", ""!mona help [command]"",procHelp)
		
		# ----- Config file management ----- #
		
		def procConfig(args):
			#did we specify -get, -set or -add?
			showerror = False
			if not ""set"" in args and not ""get"" in args and not ""add"" in args:
				showerror = True
				
			if ""set"" in args:
				if type(args[""set""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""set""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""add"" in args:
				if type(args[""add""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""add""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""get"" in args:
				if type(args[""get""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""get""].split("" "")
					if len(params) < 1:
						showerror = True
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(configUsage,highlight=1)
				return
			else:
				if ""get"" in args:
					dbg.log(""Reading value from configuration file"")
					monaConfig = MnConfig()
					thevalue = monaConfig.get(args[""get""])
					dbg.log(""Parameter %s = %s"" % (args[""get""],thevalue))
				
				if ""set"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""set""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = args[""set""][0+len(configparam):len(args[""set""])]
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
				if ""add"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""add""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = monaConfig.get(configparam).strip() + "","" + args[""add""][0+len(configparam):len(args[""add""])].strip()
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
		# ----- Jump to register ----- #
	
		def procFindJ(args):
			return procFindJMP(args)
		
		def procFindJMP(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			
			if (inspect.stack()[1][3] == ""procFindJ""):
				dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."",highlight=1)

			criteria={}
			all_opcodes={}
			
			global ptr_to_get
			ptr_to_get = -1
			
			distancestr = """"
			mindistance = 0
			maxdistance = 0
			
			#did user specify -r <reg> ?
			showerror = False
			if ""r"" in args:
				if type(args[""r""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#valid register ?
					thisreg = args[""r""].upper().strip()
					validregs = dbglib.Registers32BitsOrder
					if not thisreg in validregs:
						showerror = True
			else:
				showerror = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0		
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0						
			
			if maxdistance < mindistance:
				tmp = maxdistance
				maxdistance = mindistance
				mindistance = tmp
			
			criteria[""mindistance""] = mindistance
			criteria[""maxdistance""] = maxdistance
			
			
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(jmpUsage,highlight=1)
				return				
			else:
				modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
				# go for it !	
				all_opcodes=findJMP(modulecriteria,criteria,args[""r""].lower().strip())
			
			# write to log
			logfile = MnLog(""jmp.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog)
		
		# ----- Exception Handler Overwrites ----- #
		
					
		def procFindSEH(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""safeseh""] = False
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False

			criteria = {}
			specialcases = {}
			all_opcodes = {}
			
			global ptr_to_get
			ptr_to_get = -1
			
			#what is the caller function (backwards compatibility with pvefindaddr)
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if ""rop"" in args:
				criteria[""rop""] = True
			
			if ""all"" in args:
				criteria[""all""] = True
				specialcases[""maponly""] = True
			else:
				criteria[""all""] = False
				specialcases[""maponly""] = False
			
			# go for it !	
			all_opcodes = findSEH(modulecriteria,criteria)
			#report findings to log
			logfile = MnLog(""seh.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog,specialcases)
			
			
		# ----- MODULES ------ #
		def procShowMODULES(args):
			modulecriteria={}
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			modulestosearch = getModulesToQuery(modulecriteria)
			showModuleTable("""",modulestosearch)

		# ----- ROP ----- #
		def procFindROPFUNC(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			#modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			ropfuncs = {}
			ropfuncoffsets ={}
			ropfuncs,ropfuncoffsets = findROPFUNC(modulecriteria,criteria)
			#report findings to log
			dbg.log(""[+] Processing pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc.txt"")
			thislog = logfile.reset()
			processResults(ropfuncs,logfile,thislog)
			global silent
			silent = True
			dbg.log(""[+] Processing offsets to pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc_offset.txt"")
			thislog = logfile.reset()
			processResults(ropfuncoffsets,logfile,thislog)			
			
		def procStackPivots(args):
			procROP(args,""stackpivot"")
			
		def procROP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			maxoffset = 40
			thedistance = 8
			split = False
			fast = False
			sortedprint = False
			endingstr = """"
			endings = []
			technique = """"            
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass
			
			if ""offset"" in args:
				if type(args[""offset""]).__name__.lower() != ""bool"":
					try:
						maxoffset = int(args[""offset""])
					except:
						pass
			
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() != ""bool"":
					try:
						thedistance = args[""distance""]
					except:
						pass
			
			if ""split"" in args:
				if type(args[""split""]).__name__.lower() == ""bool"":
					split = args[""split""]

			if ""s"" in args:
				if type(args[""s""]).__name__.lower() != ""bool"":
					technique = args[""s""].replace(""'"","""").replace('""',"""").strip().lower()                   
					
			if ""fast"" in args:
				if type(args[""fast""]).__name__.lower() == ""bool"":
					fast = args[""fast""]
			
			if ""end"" in args:
				if type(args[""end""]).__name__.lower() == ""str"":
					endingstr = args[""end""].replace(""'"","""").replace('""',"""").strip()
					endings = endingstr.split(""#"")
					
			if ""f"" in args:
				if args[""f""] != """":
					criteria[""f""] = args[""f""]
			
			if ""sort"" in args:
				sortedprint = True
			
			if ""rva"" in args:
				criteria[""rva""] = True
			
			if mode == ""stackpivot"":
				fast = False
				endings = """"
				split = False
			else:
				mode = ""all""
			
			findROPGADGETS(modulecriteria,criteria,endings,maxoffset,depth,split,thedistance,fast,mode,sortedprint,technique)
			

		def procJseh(args):
			results = []
			showred=0
			showall=False
			if ""all"" in args:
				showall = True
			nrfound = 0
			dbg.log(""-----------------------------------------------------------------------"")
			dbg.log(""Search for jmp/call dword[ebp/esp+nn] (and other) combinations started "")
			dbg.log(""-----------------------------------------------------------------------"")
			opcodej=[""\xff\x54\x24\x08"", #call dword ptr [esp+08]
					""\xff\x64\x24\x08"", #jmp dword ptr [esp+08]
					""\xff\x54\x24\x14"", #call dword ptr [esp+14]
					""\xff\x54\x24\x14"", #jmp dword ptr [esp+14]
					""\xff\x54\x24\x1c"", #call dword ptr [esp+1c]
					""\xff\x54\x24\x1c"", #jmp dword ptr [esp+1c]
					""\xff\x54\x24\x2c"", #call dword ptr [esp+2c]
					""\xff\x54\x24\x2c"", #jmp dword ptr [esp+2c]
					""\xff\x54\x24\x44"", #call dword ptr [esp+44]
					""\xff\x54\x24\x44"", #jmp dword ptr [esp+44]
					""\xff\x54\x24\x50"", #call dword ptr [esp+50]
					""\xff\x54\x24\x50"", #jmp dword ptr [esp+50]
					""\xff\x55\x0c"",     #call dword ptr [ebp+0c]
					""\xff\x65\x0c"",     #jmp dword ptr [ebp+0c]
					""\xff\x55\x24"",     #call dword ptr [ebp+24]
					""\xff\x65\x24"",     #jmp dword ptr [ebp+24]
					""\xff\x55\x30"",     #call dword ptr [ebp+30]
					""\xff\x65\x30"",     #jmp dword ptr [ebp+30]
					""\xff\x55\xfc"",     #call dword ptr [ebp-04]
					""\xff\x65\xfc"",     #jmp dword ptr [ebp-04]
					""\xff\x55\xf4"",     #call dword ptr [ebp-0c]
					""\xff\x65\xf4"",     #jmp dword ptr [ebp-0c]
					""\xff\x55\xe8"",     #call dword ptr [ebp-18]
					""\xff\x65\xe8"",     #jmp dword ptr [ebp-18]
					""\x83\xc4\x08\xc3"", #add esp,8 + ret
					""\x83\xc4\x08\xc2""] #add esp,8 + ret X
			fakeptrcriteria = {}
			fakeptrcriteria[""accesslevel""] = ""*""
			for opjc in opcodej:
				addys = []
				addys = searchInRange( [[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
				results += addys
				for ptrtypes in addys:
					for ad1 in addys[ptrtypes]:
						ptr = MnPointer(ad1)
						module = ptr.belongsTo()
						if not module:
							module=""""
							page   = dbg.getMemoryPageByAddress( ad1 )
							access = page.getAccess( human = True )
							op = dbg.disasm( ad1 )
							opstring=op.getDisasm()
							dbg.log(""Found %s at 0x%08x - Access: (%s) - Outside of a loaded module"" % (opstring, ad1, access), address = ad1,highlight=1)
							nrfound+=1
						else:
							if showall:
								page   = dbg.getMemoryPageByAddress( ad1 )
								access = page.getAccess( human = True )
								op = dbg.disasm( ad1 )
								opstring=op.getDisasm()
								thismod = MnModule(module)
								if not thismod.isSafeSEH:
								#if ismodulenosafeseh(module[0])==1:
									extratext=""=== Safeseh : NO ===""
									showred=1
								else:
									extratext=""Safeseh protected""
									showred=0
								dbg.log(""Found %s at 0x%08x (%s) - Access: (%s) - %s"" % (opstring, ad1, module,access,extratext), address = ad1,highlight=showred)
								nrfound+=1
			dbg.log(""Search complete"")
			if results:
				dbg.log(""Found %d address(es)"" % nrfound)
				return ""Found %d address(es) (Check the log Windows for details)"" % nrfound
			else:
				dbg.log(""No addresses found"")
				return ""Sorry, no addresses found""

			
		def procJOP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass			
			findJOPGADGETS(modulecriteria,criteria,depth)			
			
			
		def procCreatePATTERN(args):
			size = 0
			pattern = """"
			if ""?"" in args and args[""?""] != """":
				try:
					if ""0x"" in args[""?""].lower():
						try:
							size = int(args[""?""],16)
						except:
							size = 0
					else:
						size = int(args[""?""])
				except:
					size = 0
			if size == 0:
				dbg.log(""Please enter a valid size"",highlight=1)
			else:
				pattern = createPattern(size,args)
				dbg.log(""Creating cyclic pattern of %d bytes"" % size)				
				dbg.log(pattern)
				global ignoremodules
				ignoremodules = True
				objpatternfile = MnLog(""pattern.txt"")
				patternfile = objpatternfile.reset()
				# ASCII
				objpatternfile.write(""\nPattern of "" + str(size) + "" bytes :\n"",patternfile)
				objpatternfile.write(""-"" * (19 + len(str(size))),patternfile)
				objpatternfile.write(""\nASCII:"",patternfile)
				objpatternfile.write(""\n"" + pattern,patternfile)
				# Hex
				patternhex = """"
				for patternchar in pattern:
					patternhex += str(hex(ord(patternchar))).replace(""0x"",""\\x"")
				objpatternfile.write(""\n\nHEX:\n"",patternfile)
				objpatternfile.write(patternhex,patternfile)
				# Javascript
				patternjs = str2js(pattern)
				objpatternfile.write(""\n\nJAVASCRIPT (unescape() friendly):\n"",patternfile)
				objpatternfile.write(patternjs,patternfile)
				if not silent:
					dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"",highlight=1)
					dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile,highlight=1)
				
				ignoremodules = False
			return


		def procOffsetPATTERN(args):
			egg = """"
			if ""?"" in args and args[""?""] != """":
				try:
					egg = args[""?""]
				except:
					egg = """"
			if egg == """":
				dbg.log(""Please enter a valid target"",highlight=1)
			else:
				findOffsetInPattern(egg,-1,args)
			return
		
		# ----- Comparing file output ----- #
		def procFileCOMPARE(args):
			modulecriteria={}
			criteria={}
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			allfiles=[]
			tomatch=""""
			checkstrict=True
			rangeval = 0
			fast = False
			if ""ptronly"" in args or ""ptrsonly"" in args:
				fast = True
			if ""f"" in args:
				if args[""f""] != """":
					rawfilenames=args[""f""].replace('""',"""")
					allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
					dbg.log(""[+] Number of files to be examined : %d "" % len(allfiles))
			if ""range"" in args:
				if not type(args[""range""]).__name__.lower() == ""bool"":
					strrange = args[""range""].lower()
					if strrange.startswith(""0x"") and len(strrange) > 2 :
						rangeval = int(strrange,16)
					else:
						try:
							rangeval = int(args[""range""])
						except:
							rangeval = 0
					if rangeval > 0:
						dbg.log(""[+] Find overlap using pointer +/- range, value %d"" % rangeval)
						dbg.log(""    Note : this will significantly slow down the comparison process !"")
				else:
					dbg.log(""Please provide a numeric value ^(> 0) with option -range"",highlight=1)
					return
			else:
				if ""contains"" in args:
					if type(args[""contains""]).__name__.lower() == ""str"":
						tomatch = args[""contains""].replace(""'"","""").replace('""',"""")
				if ""nostrict"" in args:
					if type(args[""nostrict""]).__name__.lower() == ""bool"":
						checkstrict = not args[""nostrict""]
						dbg.log(""[+] Instructions must match in all files ? %s"" % checkstrict)
			# maybe one of the arguments is a folder
			callfiles = allfiles
			allfiles = []
			for tfile in callfiles:
				if os.path.isdir(tfile):
					# folder, get all files from this folder
					for root,dirs,files in os.walk(tfile):
						for dfile in files:
							allfiles.append(os.path.join(root,dfile))
				else:
					allfiles.append(tfile)
			if len(allfiles) > 1:
				findFILECOMPARISON(modulecriteria,criteria,allfiles,tomatch,checkstrict,rangeval,fast)
			else:
				dbg.log(""Please specify at least 2 filenames to compare"",highlight=1)

		# ----- Find bytes in memory ----- #
		def procFind(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			base = 0
			offset = 0
			top  = TOP_USERLAND
			consecutive = False
			ftype = """"
			
			level = 0
			offsetlevel = 0			
			
			if not ""a"" in args:
				args[""a""] = ""*""

			ptronly = False

			if ""ptronly"" in args or ""ptrsonly"" in args:
				ptronly = True	
			
			#search for all pointers by default
			if not ""x"" in args:
				args[""x""] = ""*""
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			if criteria[""accesslevel""] == """":
				return
			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			if ""unicode"" in args:
				criteria[""unic""] = True

			if ""b"" in args:
				try:
					base = int(args[""b""],16)
				except:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return
			if ""t"" in args:
				try:
					top = int(args[""t""],16)
				except:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
			if ""offset"" in args:
				if not args[""offset""].__class__.__name__ == ""bool"":
					if ""0x"" in args[""offset""].lower():
						try:
							offset = 0 - int(args[""offset""],16)
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return
					else:	
						try:
							offset = 0 - int(args[""offset""])
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return	
				else:
					dbg.log(""invalid offset value"",highlight=1)
					return
					
			if ""level"" in args:
				try:
					level = int(args[""level""])
				except:
					dbg.log(""invalid level value"",highlight=1)
					return

			if ""offsetlevel"" in args:
				try:
					offsetlevel = int(args[""offsetlevel""])
				except:
					dbg.log(""invalid offsetlevel value"",highlight=1)
					return						
					
			if ""c"" in args:
				dbg.log(""    - Skipping consecutive pointers, showing size instead"")			
				consecutive = True
				
			if ""type"" in args:
				if not args[""type""] in [""bin"",""asc"",""ptr"",""instr"",""file""]:
					dbg.log(""Invalid search type : %s"" % args[""type""], highlight=1)
					return
				ftype = args[""type""] 
				if ftype == ""file"":
					filename = args[""s""].replace('""',"""").replace(""'"","""")
					#see if we can read the file
					if not os.path.isfile(filename):
						dbg.log(""Unable to find/read file %s"" % filename,highlight=1)
						return
			rangep2p = 0

			
			if ""p2p"" in args or level > 0:
				dbg.log(""    - Looking for pointers to pointers"")
				criteria[""p2p""] = True
				if ""r"" in args:	
					try:
						rangep2p = int(args[""r""])
					except:
						pass
					if rangep2p > 0:
						dbg.log(""    - Will search for close pointers (%d bytes backwards)"" % rangep2p)
				if ""p2p"" in args:
					level = 1
			
			
			if level > 0:
				dbg.log(""    - Recursive levels : %d"" % level)
			

			allpointers = findPattern(modulecriteria,criteria,pattern,ftype,base,top,consecutive,rangep2p,level,offset,offsetlevel)
				
			logfile = MnLog(""find.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog,{},ptronly)
			return
			
			
		# ---- Find instructions, wildcard search ----- #
		def procFindWild(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			patterntype = """"
			base = 0
			top  = TOP_USERLAND
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			patterntypes = [""bin"",""str""]
			if ""type"" in args:
				if type(args[""type""]).__name__.lower() != ""bool"":
					if args[""type""] in patterntypes:
						patterntype = args[""type""]
					else:
						dbg.log(""-type argument only takes one of these values: %s"" % patterntypes,highlight=1)
						return
				else:
					dbg.log(""Please specify a valid value for -type. Valid values are %s"" % patterntypes,highlight=1)
					return


			if patterntype == """":
				if ""\\x"" in pattern:
					patterntype = ""bin""
				else:
					patterntype = ""str""
			
			if ""b"" in args:
				base,addyok = getAddyArg(args[""b""])
				if not addyok:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return

			if ""t"" in args:
				top,addyok = getAddyArg(args[""t""])
				if not addyok:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
					
			if ""depth"" in args:
				try:
					criteria[""depth""] = int(args[""depth""])
				except:
					dbg.log(""invalid depth value"",highlight=1)
					return	

			if ""all"" in args:
				criteria[""all""] = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					dbg.log(""invalid distance value(s)"",highlight=1)
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0	
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0	
			
				if maxdistance < mindistance:
					tmp = maxdistance
					maxdistance = mindistance
					mindistance = tmp
				
				criteria[""mindistance""] = mindistance
				criteria[""maxdistance""] = maxdistance
						
			allpointers = findPatternWild(modulecriteria,criteria,pattern,base,top,patterntype)
				
			logfile = MnLog(""findwild.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog)		
			return
	
			
		# ----- assemble: assemble instructions to opcodes ----- #
		def procAssemble(args):
			opcodes = """"
			encoder = """"
			
			if not 's' in args:
				dbg.log(""Mandatory argument -s <opcodes> missing"", highlight=1)
				return
			opcodes = args['s']
			
			if 'e' in args:
				# TODO: implement encoder support
				dbg.log(""Encoder support not yet implemented"", highlight=1)
				return
				encoder = args['e'].lowercase()
				if encoder not in [""ascii""]:
					dbg.log(""Invalid encoder : %s"" % encoder, highlight=1)
					return
			
			assemble(opcodes,encoder)
			
		# ----- info: show information about an address ----- #
		def procInfo(args):
			if not ""a"" in args:
				dbg.log(""Missing mandatory argument -a"", highlight=1)
				return
			
			address,addyok = getAddyArg(args[""a""])
			if not addyok:
				dbg.log(""%s is an invalid address"" % args[""a""], highlight=1)
				return
			
			ptr = MnPointer(address)
			modname = ptr.belongsTo()
			modinfo = None
			if modname != """":
				modinfo = MnModule(modname)
			rebase = """"
			rva=0
			if modinfo :
				rva = address - modinfo.moduleBase
			procFlags(args)
			dbg.log("""")			
			dbg.log(""[+] Information about address 0x%s"" % toHex(address))
			dbg.log(""    %s"" % ptr.__str__())
			thepage = dbg.getMemoryPageByAddress(address)
			dbg.log(""    Address is part of page 0x%08x - 0x%08x"" % (thepage.getBaseAddress(),thepage.getBaseAddress()+thepage.getSize()))
			section = """"
			try:
				section = thepage.getSection()
			except:
				section = """"
			if section != """":
				dbg.log(""    Section : %s"" % section)
			
			if ptr.isOnStack():
				stacks = getStacks()
				stackref = """"
				for tid in stacks:
					currstack = stacks[tid]
					if currstack[0] <= address and address <= currstack[1]:
						stackref = "" (Thread 0x%08x, Stack Base : 0x%08x, Stack Top : 0x%08x)"" % (tid,currstack[0],currstack[1])
						break
				dbg.log(""    This address is in a stack segment %s"" % stackref)
			if modinfo:
				dbg.log(""    Address is part of a module:"")
				dbg.log(""    %s"" % modinfo.__str__())
				if rva != 0:
					dbg.log(""    Offset from module base: 0x%x"" % rva)
					if modinfo:
						eatlist = modinfo.getEAT()
						if address in eatlist:
							dbg.log(""    Address is start of function '%s' in %s"" % (eatlist[address],modname))
						else:
							iatlist = modinfo.getIAT()
							if address in iatlist:
								iatentry = iatlist[address]
								dbg.log(""    Address is part of IAT, and contains pointer to '%s'"" % iatentry)				
			else:
				output = """"
				if ptr.isInHeap():
					dbg.log(""    This address resides in the heap"")
					dbg.log("""")
					ptr.showHeapBlockInfo()
				else:
					dbg.log(""    Module: None"")					
			try:
				dbg.log("""")
				dbg.log(""[+] Disassembly:"")
				op = dbg.disasm(address)
				opstring=getDisasmInstruction(op)
				dbg.log(""    Instruction at %s : %s"" % (toHex(address),opstring))
			except:
				pass
			if __DEBUGGERAPP__ == ""WinDBG"":
				dbg.log("""")
				dbg.log(""Output of !address 0x%08x:"" % address)
				output = dbg.nativeCommand(""!address 0x%08","for new in registers_to_fill[::2]:
    n = new[0]
    registers['e%sx' % n] = calculateNewXregister(registers['e%sx' % n], new_values_dict['%sh' % n], new_values_dict['%sl' % n])","for new in registers_to_fill[::2]:
    (new_0, *new_rnewmaining) = new
    n = new[0]
    registers['e%sx' % n] = calculateNewXregister(registers['e%sx' % n], new_values_dict['%sh' % n], new_values_dict['%sl' % n])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
	dbg.createLogWindow()
	global currentArgs
	currentArgs = copy.copy(args)
	try:
		starttime = datetime.datetime.now()
		ptr_counter = 0
		
		# initialize list of commands
		commands = {}
		
		# ----- HELP ----- #
		def getBanner():
			banners = {}
			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                         __               __                      |\n""
			bannertext += ""    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n""
			bannertext += ""    |  / ___/ __ \/ ___/ _ \/ / __ `/ __ \   / __/ _ \/ __ `/ __ `__ \ |\n""
			bannertext += ""    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n""
			bannertext += ""    | \___/\____/_/   \___/_/\__,_/_/ /_/   \__/\___/\__,_/_/ /_/ /_/  |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |     https://www.corelan.be | https://www.corelan-training.com    |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[0] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""			
			bannertext += ""    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n""
			bannertext += ""    |       | '_ ` _ \  / _ \ | '_ \  / _` |   | '_ \ | | | |          |\n""
			bannertext += ""    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n""
			bannertext += ""    |       |_| |_| |_| \___/ |_| |_| \__,_|(_)| .__/  \__, |          |\n""
			bannertext += ""    |                                          |_|     |___/           |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""	
			banners[1] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |    _____ ___  ____  ____  ____ _                                 |\n""
			bannertext += ""    |    / __ `__ \/ __ \/ __ \/ __ `/  https://www.corelan.be         |\n""
			bannertext += ""    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n""
			bannertext += ""    |  /_/ /_/ /_/\____/_/ /_/\__,_/  #corelan (Freenode IRC)          |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[2] = bannertext

			bannertext = """"
			bannertext += ""\n    .##.....##..#######..##....##....###........########..##....##\n""
			bannertext += ""    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n""
			bannertext += ""    .####.####.##.....##.####..##..##...##......##.....##...####..\n""
			bannertext += ""    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n""
			bannertext += ""    .##.....##.##.....##.##..####.#########.....##...........##...\n""
			bannertext += ""    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n""
			bannertext += ""    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n""
			banners[3] = bannertext


			# pick random banner
			bannerlist = []
			for i in range (0, len(banners)):
				bannerlist.append(i)

			random.shuffle(bannerlist)
			return banners[bannerlist[0]]

		
		def procHelp(args):
			dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__,str(arch)))
			dbg.log(""     Plugin version : %s r%s"" % (__VERSION__,__REV__))
			dbg.log(""     Python version : %s"" % (getPythonVersion()))
			if __DEBUGGERAPP__ == ""WinDBG"":
				pykdversion = dbg.getPyKDVersionNr()
				dbg.log(""     PyKD version %s"" % pykdversion)
			dbg.log(""     Written by Corelan - https://www.corelan.be"")
			dbg.log(""     Project page : https://github.com/corelan/mona"")
			dbg.logLines(getBanner(),highlight=1)
			dbg.log(""Global options :"")
			dbg.log(""----------------"")
			dbg.log(""You can use one or more of the following global options on any command that will perform"")
			dbg.log(""a search in one or more modules, returning a list of pointers :"")
			dbg.log("" -n                     : Skip modules that start with a null byte. If this is too broad, use"")
			dbg.log(""                          option -cp nonull instead"")
			dbg.log("" -o                     : Ignore OS modules"")
			dbg.log("" -p <nr>                : Stop search after <nr> pointers."")
			dbg.log("" -m <module,module,...> : only query the given modules. Be sure what you are doing !"")
			dbg.log(""                          You can specify multiple modules (comma separated)"")
			dbg.log(""                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored"")
			dbg.log(""                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,"")
			dbg.log(""                          blah or *blah* = contains blah"")
			dbg.log("" -cm <crit,crit,...>    : Apply some additional criteria to the modules to query."")
			dbg.log(""                          You can use one or more of the following criteria :"")
			dbg.log(""                          aslr,safeseh,rebase,nx,os"")
			dbg.log(""                          You can enable or disable a certain criterium by setting it to true or false"")
			dbg.log(""                          Example :  -cm aslr=true,safeseh=false"")
			dbg.log(""                          Suppose you want to search for p/p/r in aslr enabled modules, you could call"")
			dbg.log(""                          !mona seh -cm aslr"")
			dbg.log("" -cp <crit,crit,...>    : Apply some criteria to the pointers to return"")
			dbg.log(""                          Available options are :"")
			dbg.log(""                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev"")
			dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
			dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
			dbg.log(""                          You can use .. to indicate a range of bytes (in between 2 bad chars)"")
			dbg.log("" -x <access>            : Specify desired access level of the returning pointers. If not specified,"")
			dbg.log(""                          only executable pointers will be returned."")
			dbg.log(""                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *"")
			
			if not args:
				args = []
			if len(args) > 1:
				thiscmd = args[1].lower().strip()
				if thiscmd in commands:
					dbg.log("""")
					dbg.log(""Usage of command '%s' :"" % thiscmd)
					dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
					dbg.logLines(commands[thiscmd].usage)
					dbg.log("""")
				else:
					aliasfound = False
					for cmd in commands:
						if commands[cmd].alias == thiscmd:
							dbg.log("""")
							dbg.log(""Usage of command '%s' :"" % thiscmd)
							dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
							dbg.logLines(commands[cmd].usage)
							dbg.log("""")
							aliasfound = True
					if not aliasfound:
						dbg.logLines(""\nCommand %s does not exist. Run !mona to get a list of available commands\n"" % thiscmd,highlight=1)
			else:
				dbg.logLines(""\nUsage :"")
				dbg.logLines(""-------\n"")
				dbg.log("" !mona <command> <parameter>"")
				dbg.logLines(""\nAvailable commands and parameters :\n"")

				items = commands.items()
				items.sort(key = itemgetter(0))
				for item in items:
					if commands[item[0]].usage != """":
						aliastxt = """"
						if commands[item[0]].alias != """":
							aliastxt = "" / "" + commands[item[0]].alias
						dbg.logLines(""%s | %s"" % (item[0] + aliastxt + ("" "" * (20 - len(item[0]+aliastxt))), commands[item[0]].description))
				dbg.log("""")
				dbg.log(""Want more info about a given command ?  Run !mona help <command>"",highlight=1)
				dbg.log("""")
		
		commands[""help""] = MnCommand(""help"", ""show help"", ""!mona help [command]"",procHelp)
		
		# ----- Config file management ----- #
		
		def procConfig(args):
			#did we specify -get, -set or -add?
			showerror = False
			if not ""set"" in args and not ""get"" in args and not ""add"" in args:
				showerror = True
				
			if ""set"" in args:
				if type(args[""set""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""set""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""add"" in args:
				if type(args[""add""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""add""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""get"" in args:
				if type(args[""get""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""get""].split("" "")
					if len(params) < 1:
						showerror = True
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(configUsage,highlight=1)
				return
			else:
				if ""get"" in args:
					dbg.log(""Reading value from configuration file"")
					monaConfig = MnConfig()
					thevalue = monaConfig.get(args[""get""])
					dbg.log(""Parameter %s = %s"" % (args[""get""],thevalue))
				
				if ""set"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""set""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = args[""set""][0+len(configparam):len(args[""set""])]
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
				if ""add"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""add""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = monaConfig.get(configparam).strip() + "","" + args[""add""][0+len(configparam):len(args[""add""])].strip()
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
		# ----- Jump to register ----- #
	
		def procFindJ(args):
			return procFindJMP(args)
		
		def procFindJMP(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			
			if (inspect.stack()[1][3] == ""procFindJ""):
				dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."",highlight=1)

			criteria={}
			all_opcodes={}
			
			global ptr_to_get
			ptr_to_get = -1
			
			distancestr = """"
			mindistance = 0
			maxdistance = 0
			
			#did user specify -r <reg> ?
			showerror = False
			if ""r"" in args:
				if type(args[""r""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#valid register ?
					thisreg = args[""r""].upper().strip()
					validregs = dbglib.Registers32BitsOrder
					if not thisreg in validregs:
						showerror = True
			else:
				showerror = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0		
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0						
			
			if maxdistance < mindistance:
				tmp = maxdistance
				maxdistance = mindistance
				mindistance = tmp
			
			criteria[""mindistance""] = mindistance
			criteria[""maxdistance""] = maxdistance
			
			
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(jmpUsage,highlight=1)
				return				
			else:
				modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
				# go for it !	
				all_opcodes=findJMP(modulecriteria,criteria,args[""r""].lower().strip())
			
			# write to log
			logfile = MnLog(""jmp.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog)
		
		# ----- Exception Handler Overwrites ----- #
		
					
		def procFindSEH(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""safeseh""] = False
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False

			criteria = {}
			specialcases = {}
			all_opcodes = {}
			
			global ptr_to_get
			ptr_to_get = -1
			
			#what is the caller function (backwards compatibility with pvefindaddr)
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if ""rop"" in args:
				criteria[""rop""] = True
			
			if ""all"" in args:
				criteria[""all""] = True
				specialcases[""maponly""] = True
			else:
				criteria[""all""] = False
				specialcases[""maponly""] = False
			
			# go for it !	
			all_opcodes = findSEH(modulecriteria,criteria)
			#report findings to log
			logfile = MnLog(""seh.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog,specialcases)
			
			
		# ----- MODULES ------ #
		def procShowMODULES(args):
			modulecriteria={}
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			modulestosearch = getModulesToQuery(modulecriteria)
			showModuleTable("""",modulestosearch)

		# ----- ROP ----- #
		def procFindROPFUNC(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			#modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			ropfuncs = {}
			ropfuncoffsets ={}
			ropfuncs,ropfuncoffsets = findROPFUNC(modulecriteria,criteria)
			#report findings to log
			dbg.log(""[+] Processing pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc.txt"")
			thislog = logfile.reset()
			processResults(ropfuncs,logfile,thislog)
			global silent
			silent = True
			dbg.log(""[+] Processing offsets to pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc_offset.txt"")
			thislog = logfile.reset()
			processResults(ropfuncoffsets,logfile,thislog)			
			
		def procStackPivots(args):
			procROP(args,""stackpivot"")
			
		def procROP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			maxoffset = 40
			thedistance = 8
			split = False
			fast = False
			sortedprint = False
			endingstr = """"
			endings = []
			technique = """"            
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass
			
			if ""offset"" in args:
				if type(args[""offset""]).__name__.lower() != ""bool"":
					try:
						maxoffset = int(args[""offset""])
					except:
						pass
			
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() != ""bool"":
					try:
						thedistance = args[""distance""]
					except:
						pass
			
			if ""split"" in args:
				if type(args[""split""]).__name__.lower() == ""bool"":
					split = args[""split""]

			if ""s"" in args:
				if type(args[""s""]).__name__.lower() != ""bool"":
					technique = args[""s""].replace(""'"","""").replace('""',"""").strip().lower()                   
					
			if ""fast"" in args:
				if type(args[""fast""]).__name__.lower() == ""bool"":
					fast = args[""fast""]
			
			if ""end"" in args:
				if type(args[""end""]).__name__.lower() == ""str"":
					endingstr = args[""end""].replace(""'"","""").replace('""',"""").strip()
					endings = endingstr.split(""#"")
					
			if ""f"" in args:
				if args[""f""] != """":
					criteria[""f""] = args[""f""]
			
			if ""sort"" in args:
				sortedprint = True
			
			if ""rva"" in args:
				criteria[""rva""] = True
			
			if mode == ""stackpivot"":
				fast = False
				endings = """"
				split = False
			else:
				mode = ""all""
			
			findROPGADGETS(modulecriteria,criteria,endings,maxoffset,depth,split,thedistance,fast,mode,sortedprint,technique)
			

		def procJseh(args):
			results = []
			showred=0
			showall=False
			if ""all"" in args:
				showall = True
			nrfound = 0
			dbg.log(""-----------------------------------------------------------------------"")
			dbg.log(""Search for jmp/call dword[ebp/esp+nn] (and other) combinations started "")
			dbg.log(""-----------------------------------------------------------------------"")
			opcodej=[""\xff\x54\x24\x08"", #call dword ptr [esp+08]
					""\xff\x64\x24\x08"", #jmp dword ptr [esp+08]
					""\xff\x54\x24\x14"", #call dword ptr [esp+14]
					""\xff\x54\x24\x14"", #jmp dword ptr [esp+14]
					""\xff\x54\x24\x1c"", #call dword ptr [esp+1c]
					""\xff\x54\x24\x1c"", #jmp dword ptr [esp+1c]
					""\xff\x54\x24\x2c"", #call dword ptr [esp+2c]
					""\xff\x54\x24\x2c"", #jmp dword ptr [esp+2c]
					""\xff\x54\x24\x44"", #call dword ptr [esp+44]
					""\xff\x54\x24\x44"", #jmp dword ptr [esp+44]
					""\xff\x54\x24\x50"", #call dword ptr [esp+50]
					""\xff\x54\x24\x50"", #jmp dword ptr [esp+50]
					""\xff\x55\x0c"",     #call dword ptr [ebp+0c]
					""\xff\x65\x0c"",     #jmp dword ptr [ebp+0c]
					""\xff\x55\x24"",     #call dword ptr [ebp+24]
					""\xff\x65\x24"",     #jmp dword ptr [ebp+24]
					""\xff\x55\x30"",     #call dword ptr [ebp+30]
					""\xff\x65\x30"",     #jmp dword ptr [ebp+30]
					""\xff\x55\xfc"",     #call dword ptr [ebp-04]
					""\xff\x65\xfc"",     #jmp dword ptr [ebp-04]
					""\xff\x55\xf4"",     #call dword ptr [ebp-0c]
					""\xff\x65\xf4"",     #jmp dword ptr [ebp-0c]
					""\xff\x55\xe8"",     #call dword ptr [ebp-18]
					""\xff\x65\xe8"",     #jmp dword ptr [ebp-18]
					""\x83\xc4\x08\xc3"", #add esp,8 + ret
					""\x83\xc4\x08\xc2""] #add esp,8 + ret X
			fakeptrcriteria = {}
			fakeptrcriteria[""accesslevel""] = ""*""
			for opjc in opcodej:
				addys = []
				addys = searchInRange( [[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
				results += addys
				for ptrtypes in addys:
					for ad1 in addys[ptrtypes]:
						ptr = MnPointer(ad1)
						module = ptr.belongsTo()
						if not module:
							module=""""
							page   = dbg.getMemoryPageByAddress( ad1 )
							access = page.getAccess( human = True )
							op = dbg.disasm( ad1 )
							opstring=op.getDisasm()
							dbg.log(""Found %s at 0x%08x - Access: (%s) - Outside of a loaded module"" % (opstring, ad1, access), address = ad1,highlight=1)
							nrfound+=1
						else:
							if showall:
								page   = dbg.getMemoryPageByAddress( ad1 )
								access = page.getAccess( human = True )
								op = dbg.disasm( ad1 )
								opstring=op.getDisasm()
								thismod = MnModule(module)
								if not thismod.isSafeSEH:
								#if ismodulenosafeseh(module[0])==1:
									extratext=""=== Safeseh : NO ===""
									showred=1
								else:
									extratext=""Safeseh protected""
									showred=0
								dbg.log(""Found %s at 0x%08x (%s) - Access: (%s) - %s"" % (opstring, ad1, module,access,extratext), address = ad1,highlight=showred)
								nrfound+=1
			dbg.log(""Search complete"")
			if results:
				dbg.log(""Found %d address(es)"" % nrfound)
				return ""Found %d address(es) (Check the log Windows for details)"" % nrfound
			else:
				dbg.log(""No addresses found"")
				return ""Sorry, no addresses found""

			
		def procJOP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass			
			findJOPGADGETS(modulecriteria,criteria,depth)			
			
			
		def procCreatePATTERN(args):
			size = 0
			pattern = """"
			if ""?"" in args and args[""?""] != """":
				try:
					if ""0x"" in args[""?""].lower():
						try:
							size = int(args[""?""],16)
						except:
							size = 0
					else:
						size = int(args[""?""])
				except:
					size = 0
			if size == 0:
				dbg.log(""Please enter a valid size"",highlight=1)
			else:
				pattern = createPattern(size,args)
				dbg.log(""Creating cyclic pattern of %d bytes"" % size)				
				dbg.log(pattern)
				global ignoremodules
				ignoremodules = True
				objpatternfile = MnLog(""pattern.txt"")
				patternfile = objpatternfile.reset()
				# ASCII
				objpatternfile.write(""\nPattern of "" + str(size) + "" bytes :\n"",patternfile)
				objpatternfile.write(""-"" * (19 + len(str(size))),patternfile)
				objpatternfile.write(""\nASCII:"",patternfile)
				objpatternfile.write(""\n"" + pattern,patternfile)
				# Hex
				patternhex = """"
				for patternchar in pattern:
					patternhex += str(hex(ord(patternchar))).replace(""0x"",""\\x"")
				objpatternfile.write(""\n\nHEX:\n"",patternfile)
				objpatternfile.write(patternhex,patternfile)
				# Javascript
				patternjs = str2js(pattern)
				objpatternfile.write(""\n\nJAVASCRIPT (unescape() friendly):\n"",patternfile)
				objpatternfile.write(patternjs,patternfile)
				if not silent:
					dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"",highlight=1)
					dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile,highlight=1)
				
				ignoremodules = False
			return


		def procOffsetPATTERN(args):
			egg = """"
			if ""?"" in args and args[""?""] != """":
				try:
					egg = args[""?""]
				except:
					egg = """"
			if egg == """":
				dbg.log(""Please enter a valid target"",highlight=1)
			else:
				findOffsetInPattern(egg,-1,args)
			return
		
		# ----- Comparing file output ----- #
		def procFileCOMPARE(args):
			modulecriteria={}
			criteria={}
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			allfiles=[]
			tomatch=""""
			checkstrict=True
			rangeval = 0
			fast = False
			if ""ptronly"" in args or ""ptrsonly"" in args:
				fast = True
			if ""f"" in args:
				if args[""f""] != """":
					rawfilenames=args[""f""].replace('""',"""")
					allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
					dbg.log(""[+] Number of files to be examined : %d "" % len(allfiles))
			if ""range"" in args:
				if not type(args[""range""]).__name__.lower() == ""bool"":
					strrange = args[""range""].lower()
					if strrange.startswith(""0x"") and len(strrange) > 2 :
						rangeval = int(strrange,16)
					else:
						try:
							rangeval = int(args[""range""])
						except:
							rangeval = 0
					if rangeval > 0:
						dbg.log(""[+] Find overlap using pointer +/- range, value %d"" % rangeval)
						dbg.log(""    Note : this will significantly slow down the comparison process !"")
				else:
					dbg.log(""Please provide a numeric value ^(> 0) with option -range"",highlight=1)
					return
			else:
				if ""contains"" in args:
					if type(args[""contains""]).__name__.lower() == ""str"":
						tomatch = args[""contains""].replace(""'"","""").replace('""',"""")
				if ""nostrict"" in args:
					if type(args[""nostrict""]).__name__.lower() == ""bool"":
						checkstrict = not args[""nostrict""]
						dbg.log(""[+] Instructions must match in all files ? %s"" % checkstrict)
			# maybe one of the arguments is a folder
			callfiles = allfiles
			allfiles = []
			for tfile in callfiles:
				if os.path.isdir(tfile):
					# folder, get all files from this folder
					for root,dirs,files in os.walk(tfile):
						for dfile in files:
							allfiles.append(os.path.join(root,dfile))
				else:
					allfiles.append(tfile)
			if len(allfiles) > 1:
				findFILECOMPARISON(modulecriteria,criteria,allfiles,tomatch,checkstrict,rangeval,fast)
			else:
				dbg.log(""Please specify at least 2 filenames to compare"",highlight=1)

		# ----- Find bytes in memory ----- #
		def procFind(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			base = 0
			offset = 0
			top  = TOP_USERLAND
			consecutive = False
			ftype = """"
			
			level = 0
			offsetlevel = 0			
			
			if not ""a"" in args:
				args[""a""] = ""*""

			ptronly = False

			if ""ptronly"" in args or ""ptrsonly"" in args:
				ptronly = True	
			
			#search for all pointers by default
			if not ""x"" in args:
				args[""x""] = ""*""
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			if criteria[""accesslevel""] == """":
				return
			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			if ""unicode"" in args:
				criteria[""unic""] = True

			if ""b"" in args:
				try:
					base = int(args[""b""],16)
				except:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return
			if ""t"" in args:
				try:
					top = int(args[""t""],16)
				except:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
			if ""offset"" in args:
				if not args[""offset""].__class__.__name__ == ""bool"":
					if ""0x"" in args[""offset""].lower():
						try:
							offset = 0 - int(args[""offset""],16)
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return
					else:	
						try:
							offset = 0 - int(args[""offset""])
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return	
				else:
					dbg.log(""invalid offset value"",highlight=1)
					return
					
			if ""level"" in args:
				try:
					level = int(args[""level""])
				except:
					dbg.log(""invalid level value"",highlight=1)
					return

			if ""offsetlevel"" in args:
				try:
					offsetlevel = int(args[""offsetlevel""])
				except:
					dbg.log(""invalid offsetlevel value"",highlight=1)
					return						
					
			if ""c"" in args:
				dbg.log(""    - Skipping consecutive pointers, showing size instead"")			
				consecutive = True
				
			if ""type"" in args:
				if not args[""type""] in [""bin"",""asc"",""ptr"",""instr"",""file""]:
					dbg.log(""Invalid search type : %s"" % args[""type""], highlight=1)
					return
				ftype = args[""type""] 
				if ftype == ""file"":
					filename = args[""s""].replace('""',"""").replace(""'"","""")
					#see if we can read the file
					if not os.path.isfile(filename):
						dbg.log(""Unable to find/read file %s"" % filename,highlight=1)
						return
			rangep2p = 0

			
			if ""p2p"" in args or level > 0:
				dbg.log(""    - Looking for pointers to pointers"")
				criteria[""p2p""] = True
				if ""r"" in args:	
					try:
						rangep2p = int(args[""r""])
					except:
						pass
					if rangep2p > 0:
						dbg.log(""    - Will search for close pointers (%d bytes backwards)"" % rangep2p)
				if ""p2p"" in args:
					level = 1
			
			
			if level > 0:
				dbg.log(""    - Recursive levels : %d"" % level)
			

			allpointers = findPattern(modulecriteria,criteria,pattern,ftype,base,top,consecutive,rangep2p,level,offset,offsetlevel)
				
			logfile = MnLog(""find.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog,{},ptronly)
			return
			
			
		# ---- Find instructions, wildcard search ----- #
		def procFindWild(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			patterntype = """"
			base = 0
			top  = TOP_USERLAND
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			patterntypes = [""bin"",""str""]
			if ""type"" in args:
				if type(args[""type""]).__name__.lower() != ""bool"":
					if args[""type""] in patterntypes:
						patterntype = args[""type""]
					else:
						dbg.log(""-type argument only takes one of these values: %s"" % patterntypes,highlight=1)
						return
				else:
					dbg.log(""Please specify a valid value for -type. Valid values are %s"" % patterntypes,highlight=1)
					return


			if patterntype == """":
				if ""\\x"" in pattern:
					patterntype = ""bin""
				else:
					patterntype = ""str""
			
			if ""b"" in args:
				base,addyok = getAddyArg(args[""b""])
				if not addyok:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return

			if ""t"" in args:
				top,addyok = getAddyArg(args[""t""])
				if not addyok:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
					
			if ""depth"" in args:
				try:
					criteria[""depth""] = int(args[""depth""])
				except:
					dbg.log(""invalid depth value"",highlight=1)
					return	

			if ""all"" in args:
				criteria[""all""] = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					dbg.log(""invalid distance value(s)"",highlight=1)
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0	
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0	
			
				if maxdistance < mindistance:
					tmp = maxdistance
					maxdistance = mindistance
					mindistance = tmp
				
				criteria[""mindistance""] = mindistance
				criteria[""maxdistance""] = maxdistance
						
			allpointers = findPatternWild(modulecriteria,criteria,pattern,base,top,patterntype)
				
			logfile = MnLog(""findwild.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog)		
			return
	
			
		# ----- assemble: assemble instructions to opcodes ----- #
		def procAssemble(args):
			opcodes = """"
			encoder = """"
			
			if not 's' in args:
				dbg.log(""Mandatory argument -s <opcodes> missing"", highlight=1)
				return
			opcodes = args['s']
			
			if 'e' in args:
				# TODO: implement encoder support
				dbg.log(""Encoder support not yet implemented"", highlight=1)
				return
				encoder = args['e'].lowercase()
				if encoder not in [""ascii""]:
					dbg.log(""Invalid encoder : %s"" % encoder, highlight=1)
					return
			
			assemble(opcodes,encoder)
			
		# ----- info: show information about an address ----- #
		def procInfo(args):
			if not ""a"" in args:
				dbg.log(""Missing mandatory argument -a"", highlight=1)
				return
			
			address,addyok = getAddyArg(args[""a""])
			if not addyok:
				dbg.log(""%s is an invalid address"" % args[""a""], highlight=1)
				return
			
			ptr = MnPointer(address)
			modname = ptr.belongsTo()
			modinfo = None
			if modname != """":
				modinfo = MnModule(modname)
			rebase = """"
			rva=0
			if modinfo :
				rva = address - modinfo.moduleBase
			procFlags(args)
			dbg.log("""")			
			dbg.log(""[+] Information about address 0x%s"" % toHex(address))
			dbg.log(""    %s"" % ptr.__str__())
			thepage = dbg.getMemoryPageByAddress(address)
			dbg.log(""    Address is part of page 0x%08x - 0x%08x"" % (thepage.getBaseAddress(),thepage.getBaseAddress()+thepage.getSize()))
			section = """"
			try:
				section = thepage.getSection()
			except:
				section = """"
			if section != """":
				dbg.log(""    Section : %s"" % section)
			
			if ptr.isOnStack():
				stacks = getStacks()
				stackref = """"
				for tid in stacks:
					currstack = stacks[tid]
					if currstack[0] <= address and address <= currstack[1]:
						stackref = "" (Thread 0x%08x, Stack Base : 0x%08x, Stack Top : 0x%08x)"" % (tid,currstack[0],currstack[1])
						break
				dbg.log(""    This address is in a stack segment %s"" % stackref)
			if modinfo:
				dbg.log(""    Address is part of a module:"")
				dbg.log(""    %s"" % modinfo.__str__())
				if rva != 0:
					dbg.log(""    Offset from module base: 0x%x"" % rva)
					if modinfo:
						eatlist = modinfo.getEAT()
						if address in eatlist:
							dbg.log(""    Address is start of function '%s' in %s"" % (eatlist[address],modname))
						else:
							iatlist = modinfo.getIAT()
							if address in iatlist:
								iatentry = iatlist[address]
								dbg.log(""    Address is part of IAT, and contains pointer to '%s'"" % iatentry)				
			else:
				output = """"
				if ptr.isInHeap():
					dbg.log(""    This address resides in the heap"")
					dbg.log("""")
					ptr.showHeapBlockInfo()
				else:
					dbg.log(""    Module: None"")					
			try:
				dbg.log("""")
				dbg.log(""[+] Disassembly:"")
				op = dbg.disasm(address)
				opstring=getDisasmInstruction(op)
				dbg.log(""    Instruction at %s : %s"" % (toHex(address),opstring))
			except:
				pass
			if __DEBUGGERAPP__ == ""WinDBG"":
				dbg.log("""")
				dbg.log(""Output of !address 0x%08x:"" % address)
				output = dbg.nativeCommand(""!address 0x%08","for item in items:
    if commands[item[0]].usage != '':
        aliastxt = ''
        if commands[item[0]].alias != '':
            aliastxt = ' / ' + commands[item[0]].alias
        dbg.logLines('%s | %s' % (item[0] + aliastxt + ' ' * (20 - len(item[0] + aliastxt)), commands[item[0]].description))","for item in items:
    (item_0, *item_ritemmaining) = item
    if commands[item[0]].usage != '':
        aliastxt = ''
        if commands[item[0]].alias != '':
            aliastxt = ' / ' + commands[item[0]].alias
        dbg.logLines('%s | %s' % (item[0] + aliastxt + ' ' * (20 - len(item[0] + aliastxt)), commands[item[0]].description))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
	dbg.createLogWindow()
	global currentArgs
	currentArgs = copy.copy(args)
	try:
		starttime = datetime.datetime.now()
		ptr_counter = 0
		
		# initialize list of commands
		commands = {}
		
		# ----- HELP ----- #
		def getBanner():
			banners = {}
			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                         __               __                      |\n""
			bannertext += ""    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n""
			bannertext += ""    |  / ___/ __ \/ ___/ _ \/ / __ `/ __ \   / __/ _ \/ __ `/ __ `__ \ |\n""
			bannertext += ""    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n""
			bannertext += ""    | \___/\____/_/   \___/_/\__,_/_/ /_/   \__/\___/\__,_/_/ /_/ /_/  |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |     https://www.corelan.be | https://www.corelan-training.com    |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[0] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""			
			bannertext += ""    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n""
			bannertext += ""    |       | '_ ` _ \  / _ \ | '_ \  / _` |   | '_ \ | | | |          |\n""
			bannertext += ""    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n""
			bannertext += ""    |       |_| |_| |_| \___/ |_| |_| \__,_|(_)| .__/  \__, |          |\n""
			bannertext += ""    |                                          |_|     |___/           |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""	
			banners[1] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |    _____ ___  ____  ____  ____ _                                 |\n""
			bannertext += ""    |    / __ `__ \/ __ \/ __ \/ __ `/  https://www.corelan.be         |\n""
			bannertext += ""    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n""
			bannertext += ""    |  /_/ /_/ /_/\____/_/ /_/\__,_/  #corelan (Freenode IRC)          |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[2] = bannertext

			bannertext = """"
			bannertext += ""\n    .##.....##..#######..##....##....###........########..##....##\n""
			bannertext += ""    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n""
			bannertext += ""    .####.####.##.....##.####..##..##...##......##.....##...####..\n""
			bannertext += ""    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n""
			bannertext += ""    .##.....##.##.....##.##..####.#########.....##...........##...\n""
			bannertext += ""    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n""
			bannertext += ""    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n""
			banners[3] = bannertext


			# pick random banner
			bannerlist = []
			for i in range (0, len(banners)):
				bannerlist.append(i)

			random.shuffle(bannerlist)
			return banners[bannerlist[0]]

		
		def procHelp(args):
			dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__,str(arch)))
			dbg.log(""     Plugin version : %s r%s"" % (__VERSION__,__REV__))
			dbg.log(""     Python version : %s"" % (getPythonVersion()))
			if __DEBUGGERAPP__ == ""WinDBG"":
				pykdversion = dbg.getPyKDVersionNr()
				dbg.log(""     PyKD version %s"" % pykdversion)
			dbg.log(""     Written by Corelan - https://www.corelan.be"")
			dbg.log(""     Project page : https://github.com/corelan/mona"")
			dbg.logLines(getBanner(),highlight=1)
			dbg.log(""Global options :"")
			dbg.log(""----------------"")
			dbg.log(""You can use one or more of the following global options on any command that will perform"")
			dbg.log(""a search in one or more modules, returning a list of pointers :"")
			dbg.log("" -n                     : Skip modules that start with a null byte. If this is too broad, use"")
			dbg.log(""                          option -cp nonull instead"")
			dbg.log("" -o                     : Ignore OS modules"")
			dbg.log("" -p <nr>                : Stop search after <nr> pointers."")
			dbg.log("" -m <module,module,...> : only query the given modules. Be sure what you are doing !"")
			dbg.log(""                          You can specify multiple modules (comma separated)"")
			dbg.log(""                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored"")
			dbg.log(""                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,"")
			dbg.log(""                          blah or *blah* = contains blah"")
			dbg.log("" -cm <crit,crit,...>    : Apply some additional criteria to the modules to query."")
			dbg.log(""                          You can use one or more of the following criteria :"")
			dbg.log(""                          aslr,safeseh,rebase,nx,os"")
			dbg.log(""                          You can enable or disable a certain criterium by setting it to true or false"")
			dbg.log(""                          Example :  -cm aslr=true,safeseh=false"")
			dbg.log(""                          Suppose you want to search for p/p/r in aslr enabled modules, you could call"")
			dbg.log(""                          !mona seh -cm aslr"")
			dbg.log("" -cp <crit,crit,...>    : Apply some criteria to the pointers to return"")
			dbg.log(""                          Available options are :"")
			dbg.log(""                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev"")
			dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
			dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
			dbg.log(""                          You can use .. to indicate a range of bytes (in between 2 bad chars)"")
			dbg.log("" -x <access>            : Specify desired access level of the returning pointers. If not specified,"")
			dbg.log(""                          only executable pointers will be returned."")
			dbg.log(""                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *"")
			
			if not args:
				args = []
			if len(args) > 1:
				thiscmd = args[1].lower().strip()
				if thiscmd in commands:
					dbg.log("""")
					dbg.log(""Usage of command '%s' :"" % thiscmd)
					dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
					dbg.logLines(commands[thiscmd].usage)
					dbg.log("""")
				else:
					aliasfound = False
					for cmd in commands:
						if commands[cmd].alias == thiscmd:
							dbg.log("""")
							dbg.log(""Usage of command '%s' :"" % thiscmd)
							dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
							dbg.logLines(commands[cmd].usage)
							dbg.log("""")
							aliasfound = True
					if not aliasfound:
						dbg.logLines(""\nCommand %s does not exist. Run !mona to get a list of available commands\n"" % thiscmd,highlight=1)
			else:
				dbg.logLines(""\nUsage :"")
				dbg.logLines(""-------\n"")
				dbg.log("" !mona <command> <parameter>"")
				dbg.logLines(""\nAvailable commands and parameters :\n"")

				items = commands.items()
				items.sort(key = itemgetter(0))
				for item in items:
					if commands[item[0]].usage != """":
						aliastxt = """"
						if commands[item[0]].alias != """":
							aliastxt = "" / "" + commands[item[0]].alias
						dbg.logLines(""%s | %s"" % (item[0] + aliastxt + ("" "" * (20 - len(item[0]+aliastxt))), commands[item[0]].description))
				dbg.log("""")
				dbg.log(""Want more info about a given command ?  Run !mona help <command>"",highlight=1)
				dbg.log("""")
		
		commands[""help""] = MnCommand(""help"", ""show help"", ""!mona help [command]"",procHelp)
		
		# ----- Config file management ----- #
		
		def procConfig(args):
			#did we specify -get, -set or -add?
			showerror = False
			if not ""set"" in args and not ""get"" in args and not ""add"" in args:
				showerror = True
				
			if ""set"" in args:
				if type(args[""set""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""set""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""add"" in args:
				if type(args[""add""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""add""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""get"" in args:
				if type(args[""get""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""get""].split("" "")
					if len(params) < 1:
						showerror = True
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(configUsage,highlight=1)
				return
			else:
				if ""get"" in args:
					dbg.log(""Reading value from configuration file"")
					monaConfig = MnConfig()
					thevalue = monaConfig.get(args[""get""])
					dbg.log(""Parameter %s = %s"" % (args[""get""],thevalue))
				
				if ""set"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""set""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = args[""set""][0+len(configparam):len(args[""set""])]
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
				if ""add"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""add""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = monaConfig.get(configparam).strip() + "","" + args[""add""][0+len(configparam):len(args[""add""])].strip()
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
		# ----- Jump to register ----- #
	
		def procFindJ(args):
			return procFindJMP(args)
		
		def procFindJMP(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			
			if (inspect.stack()[1][3] == ""procFindJ""):
				dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."",highlight=1)

			criteria={}
			all_opcodes={}
			
			global ptr_to_get
			ptr_to_get = -1
			
			distancestr = """"
			mindistance = 0
			maxdistance = 0
			
			#did user specify -r <reg> ?
			showerror = False
			if ""r"" in args:
				if type(args[""r""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#valid register ?
					thisreg = args[""r""].upper().strip()
					validregs = dbglib.Registers32BitsOrder
					if not thisreg in validregs:
						showerror = True
			else:
				showerror = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0		
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0						
			
			if maxdistance < mindistance:
				tmp = maxdistance
				maxdistance = mindistance
				mindistance = tmp
			
			criteria[""mindistance""] = mindistance
			criteria[""maxdistance""] = maxdistance
			
			
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(jmpUsage,highlight=1)
				return				
			else:
				modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
				# go for it !	
				all_opcodes=findJMP(modulecriteria,criteria,args[""r""].lower().strip())
			
			# write to log
			logfile = MnLog(""jmp.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog)
		
		# ----- Exception Handler Overwrites ----- #
		
					
		def procFindSEH(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""safeseh""] = False
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False

			criteria = {}
			specialcases = {}
			all_opcodes = {}
			
			global ptr_to_get
			ptr_to_get = -1
			
			#what is the caller function (backwards compatibility with pvefindaddr)
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if ""rop"" in args:
				criteria[""rop""] = True
			
			if ""all"" in args:
				criteria[""all""] = True
				specialcases[""maponly""] = True
			else:
				criteria[""all""] = False
				specialcases[""maponly""] = False
			
			# go for it !	
			all_opcodes = findSEH(modulecriteria,criteria)
			#report findings to log
			logfile = MnLog(""seh.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog,specialcases)
			
			
		# ----- MODULES ------ #
		def procShowMODULES(args):
			modulecriteria={}
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			modulestosearch = getModulesToQuery(modulecriteria)
			showModuleTable("""",modulestosearch)

		# ----- ROP ----- #
		def procFindROPFUNC(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			#modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			ropfuncs = {}
			ropfuncoffsets ={}
			ropfuncs,ropfuncoffsets = findROPFUNC(modulecriteria,criteria)
			#report findings to log
			dbg.log(""[+] Processing pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc.txt"")
			thislog = logfile.reset()
			processResults(ropfuncs,logfile,thislog)
			global silent
			silent = True
			dbg.log(""[+] Processing offsets to pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc_offset.txt"")
			thislog = logfile.reset()
			processResults(ropfuncoffsets,logfile,thislog)			
			
		def procStackPivots(args):
			procROP(args,""stackpivot"")
			
		def procROP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			maxoffset = 40
			thedistance = 8
			split = False
			fast = False
			sortedprint = False
			endingstr = """"
			endings = []
			technique = """"            
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass
			
			if ""offset"" in args:
				if type(args[""offset""]).__name__.lower() != ""bool"":
					try:
						maxoffset = int(args[""offset""])
					except:
						pass
			
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() != ""bool"":
					try:
						thedistance = args[""distance""]
					except:
						pass
			
			if ""split"" in args:
				if type(args[""split""]).__name__.lower() == ""bool"":
					split = args[""split""]

			if ""s"" in args:
				if type(args[""s""]).__name__.lower() != ""bool"":
					technique = args[""s""].replace(""'"","""").replace('""',"""").strip().lower()                   
					
			if ""fast"" in args:
				if type(args[""fast""]).__name__.lower() == ""bool"":
					fast = args[""fast""]
			
			if ""end"" in args:
				if type(args[""end""]).__name__.lower() == ""str"":
					endingstr = args[""end""].replace(""'"","""").replace('""',"""").strip()
					endings = endingstr.split(""#"")
					
			if ""f"" in args:
				if args[""f""] != """":
					criteria[""f""] = args[""f""]
			
			if ""sort"" in args:
				sortedprint = True
			
			if ""rva"" in args:
				criteria[""rva""] = True
			
			if mode == ""stackpivot"":
				fast = False
				endings = """"
				split = False
			else:
				mode = ""all""
			
			findROPGADGETS(modulecriteria,criteria,endings,maxoffset,depth,split,thedistance,fast,mode,sortedprint,technique)
			

		def procJseh(args):
			results = []
			showred=0
			showall=False
			if ""all"" in args:
				showall = True
			nrfound = 0
			dbg.log(""-----------------------------------------------------------------------"")
			dbg.log(""Search for jmp/call dword[ebp/esp+nn] (and other) combinations started "")
			dbg.log(""-----------------------------------------------------------------------"")
			opcodej=[""\xff\x54\x24\x08"", #call dword ptr [esp+08]
					""\xff\x64\x24\x08"", #jmp dword ptr [esp+08]
					""\xff\x54\x24\x14"", #call dword ptr [esp+14]
					""\xff\x54\x24\x14"", #jmp dword ptr [esp+14]
					""\xff\x54\x24\x1c"", #call dword ptr [esp+1c]
					""\xff\x54\x24\x1c"", #jmp dword ptr [esp+1c]
					""\xff\x54\x24\x2c"", #call dword ptr [esp+2c]
					""\xff\x54\x24\x2c"", #jmp dword ptr [esp+2c]
					""\xff\x54\x24\x44"", #call dword ptr [esp+44]
					""\xff\x54\x24\x44"", #jmp dword ptr [esp+44]
					""\xff\x54\x24\x50"", #call dword ptr [esp+50]
					""\xff\x54\x24\x50"", #jmp dword ptr [esp+50]
					""\xff\x55\x0c"",     #call dword ptr [ebp+0c]
					""\xff\x65\x0c"",     #jmp dword ptr [ebp+0c]
					""\xff\x55\x24"",     #call dword ptr [ebp+24]
					""\xff\x65\x24"",     #jmp dword ptr [ebp+24]
					""\xff\x55\x30"",     #call dword ptr [ebp+30]
					""\xff\x65\x30"",     #jmp dword ptr [ebp+30]
					""\xff\x55\xfc"",     #call dword ptr [ebp-04]
					""\xff\x65\xfc"",     #jmp dword ptr [ebp-04]
					""\xff\x55\xf4"",     #call dword ptr [ebp-0c]
					""\xff\x65\xf4"",     #jmp dword ptr [ebp-0c]
					""\xff\x55\xe8"",     #call dword ptr [ebp-18]
					""\xff\x65\xe8"",     #jmp dword ptr [ebp-18]
					""\x83\xc4\x08\xc3"", #add esp,8 + ret
					""\x83\xc4\x08\xc2""] #add esp,8 + ret X
			fakeptrcriteria = {}
			fakeptrcriteria[""accesslevel""] = ""*""
			for opjc in opcodej:
				addys = []
				addys = searchInRange( [[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
				results += addys
				for ptrtypes in addys:
					for ad1 in addys[ptrtypes]:
						ptr = MnPointer(ad1)
						module = ptr.belongsTo()
						if not module:
							module=""""
							page   = dbg.getMemoryPageByAddress( ad1 )
							access = page.getAccess( human = True )
							op = dbg.disasm( ad1 )
							opstring=op.getDisasm()
							dbg.log(""Found %s at 0x%08x - Access: (%s) - Outside of a loaded module"" % (opstring, ad1, access), address = ad1,highlight=1)
							nrfound+=1
						else:
							if showall:
								page   = dbg.getMemoryPageByAddress( ad1 )
								access = page.getAccess( human = True )
								op = dbg.disasm( ad1 )
								opstring=op.getDisasm()
								thismod = MnModule(module)
								if not thismod.isSafeSEH:
								#if ismodulenosafeseh(module[0])==1:
									extratext=""=== Safeseh : NO ===""
									showred=1
								else:
									extratext=""Safeseh protected""
									showred=0
								dbg.log(""Found %s at 0x%08x (%s) - Access: (%s) - %s"" % (opstring, ad1, module,access,extratext), address = ad1,highlight=showred)
								nrfound+=1
			dbg.log(""Search complete"")
			if results:
				dbg.log(""Found %d address(es)"" % nrfound)
				return ""Found %d address(es) (Check the log Windows for details)"" % nrfound
			else:
				dbg.log(""No addresses found"")
				return ""Sorry, no addresses found""

			
		def procJOP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass			
			findJOPGADGETS(modulecriteria,criteria,depth)			
			
			
		def procCreatePATTERN(args):
			size = 0
			pattern = """"
			if ""?"" in args and args[""?""] != """":
				try:
					if ""0x"" in args[""?""].lower():
						try:
							size = int(args[""?""],16)
						except:
							size = 0
					else:
						size = int(args[""?""])
				except:
					size = 0
			if size == 0:
				dbg.log(""Please enter a valid size"",highlight=1)
			else:
				pattern = createPattern(size,args)
				dbg.log(""Creating cyclic pattern of %d bytes"" % size)				
				dbg.log(pattern)
				global ignoremodules
				ignoremodules = True
				objpatternfile = MnLog(""pattern.txt"")
				patternfile = objpatternfile.reset()
				# ASCII
				objpatternfile.write(""\nPattern of "" + str(size) + "" bytes :\n"",patternfile)
				objpatternfile.write(""-"" * (19 + len(str(size))),patternfile)
				objpatternfile.write(""\nASCII:"",patternfile)
				objpatternfile.write(""\n"" + pattern,patternfile)
				# Hex
				patternhex = """"
				for patternchar in pattern:
					patternhex += str(hex(ord(patternchar))).replace(""0x"",""\\x"")
				objpatternfile.write(""\n\nHEX:\n"",patternfile)
				objpatternfile.write(patternhex,patternfile)
				# Javascript
				patternjs = str2js(pattern)
				objpatternfile.write(""\n\nJAVASCRIPT (unescape() friendly):\n"",patternfile)
				objpatternfile.write(patternjs,patternfile)
				if not silent:
					dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"",highlight=1)
					dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile,highlight=1)
				
				ignoremodules = False
			return


		def procOffsetPATTERN(args):
			egg = """"
			if ""?"" in args and args[""?""] != """":
				try:
					egg = args[""?""]
				except:
					egg = """"
			if egg == """":
				dbg.log(""Please enter a valid target"",highlight=1)
			else:
				findOffsetInPattern(egg,-1,args)
			return
		
		# ----- Comparing file output ----- #
		def procFileCOMPARE(args):
			modulecriteria={}
			criteria={}
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			allfiles=[]
			tomatch=""""
			checkstrict=True
			rangeval = 0
			fast = False
			if ""ptronly"" in args or ""ptrsonly"" in args:
				fast = True
			if ""f"" in args:
				if args[""f""] != """":
					rawfilenames=args[""f""].replace('""',"""")
					allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
					dbg.log(""[+] Number of files to be examined : %d "" % len(allfiles))
			if ""range"" in args:
				if not type(args[""range""]).__name__.lower() == ""bool"":
					strrange = args[""range""].lower()
					if strrange.startswith(""0x"") and len(strrange) > 2 :
						rangeval = int(strrange,16)
					else:
						try:
							rangeval = int(args[""range""])
						except:
							rangeval = 0
					if rangeval > 0:
						dbg.log(""[+] Find overlap using pointer +/- range, value %d"" % rangeval)
						dbg.log(""    Note : this will significantly slow down the comparison process !"")
				else:
					dbg.log(""Please provide a numeric value ^(> 0) with option -range"",highlight=1)
					return
			else:
				if ""contains"" in args:
					if type(args[""contains""]).__name__.lower() == ""str"":
						tomatch = args[""contains""].replace(""'"","""").replace('""',"""")
				if ""nostrict"" in args:
					if type(args[""nostrict""]).__name__.lower() == ""bool"":
						checkstrict = not args[""nostrict""]
						dbg.log(""[+] Instructions must match in all files ? %s"" % checkstrict)
			# maybe one of the arguments is a folder
			callfiles = allfiles
			allfiles = []
			for tfile in callfiles:
				if os.path.isdir(tfile):
					# folder, get all files from this folder
					for root,dirs,files in os.walk(tfile):
						for dfile in files:
							allfiles.append(os.path.join(root,dfile))
				else:
					allfiles.append(tfile)
			if len(allfiles) > 1:
				findFILECOMPARISON(modulecriteria,criteria,allfiles,tomatch,checkstrict,rangeval,fast)
			else:
				dbg.log(""Please specify at least 2 filenames to compare"",highlight=1)

		# ----- Find bytes in memory ----- #
		def procFind(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			base = 0
			offset = 0
			top  = TOP_USERLAND
			consecutive = False
			ftype = """"
			
			level = 0
			offsetlevel = 0			
			
			if not ""a"" in args:
				args[""a""] = ""*""

			ptronly = False

			if ""ptronly"" in args or ""ptrsonly"" in args:
				ptronly = True	
			
			#search for all pointers by default
			if not ""x"" in args:
				args[""x""] = ""*""
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			if criteria[""accesslevel""] == """":
				return
			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			if ""unicode"" in args:
				criteria[""unic""] = True

			if ""b"" in args:
				try:
					base = int(args[""b""],16)
				except:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return
			if ""t"" in args:
				try:
					top = int(args[""t""],16)
				except:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
			if ""offset"" in args:
				if not args[""offset""].__class__.__name__ == ""bool"":
					if ""0x"" in args[""offset""].lower():
						try:
							offset = 0 - int(args[""offset""],16)
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return
					else:	
						try:
							offset = 0 - int(args[""offset""])
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return	
				else:
					dbg.log(""invalid offset value"",highlight=1)
					return
					
			if ""level"" in args:
				try:
					level = int(args[""level""])
				except:
					dbg.log(""invalid level value"",highlight=1)
					return

			if ""offsetlevel"" in args:
				try:
					offsetlevel = int(args[""offsetlevel""])
				except:
					dbg.log(""invalid offsetlevel value"",highlight=1)
					return						
					
			if ""c"" in args:
				dbg.log(""    - Skipping consecutive pointers, showing size instead"")			
				consecutive = True
				
			if ""type"" in args:
				if not args[""type""] in [""bin"",""asc"",""ptr"",""instr"",""file""]:
					dbg.log(""Invalid search type : %s"" % args[""type""], highlight=1)
					return
				ftype = args[""type""] 
				if ftype == ""file"":
					filename = args[""s""].replace('""',"""").replace(""'"","""")
					#see if we can read the file
					if not os.path.isfile(filename):
						dbg.log(""Unable to find/read file %s"" % filename,highlight=1)
						return
			rangep2p = 0

			
			if ""p2p"" in args or level > 0:
				dbg.log(""    - Looking for pointers to pointers"")
				criteria[""p2p""] = True
				if ""r"" in args:	
					try:
						rangep2p = int(args[""r""])
					except:
						pass
					if rangep2p > 0:
						dbg.log(""    - Will search for close pointers (%d bytes backwards)"" % rangep2p)
				if ""p2p"" in args:
					level = 1
			
			
			if level > 0:
				dbg.log(""    - Recursive levels : %d"" % level)
			

			allpointers = findPattern(modulecriteria,criteria,pattern,ftype,base,top,consecutive,rangep2p,level,offset,offsetlevel)
				
			logfile = MnLog(""find.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog,{},ptronly)
			return
			
			
		# ---- Find instructions, wildcard search ----- #
		def procFindWild(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			patterntype = """"
			base = 0
			top  = TOP_USERLAND
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			patterntypes = [""bin"",""str""]
			if ""type"" in args:
				if type(args[""type""]).__name__.lower() != ""bool"":
					if args[""type""] in patterntypes:
						patterntype = args[""type""]
					else:
						dbg.log(""-type argument only takes one of these values: %s"" % patterntypes,highlight=1)
						return
				else:
					dbg.log(""Please specify a valid value for -type. Valid values are %s"" % patterntypes,highlight=1)
					return


			if patterntype == """":
				if ""\\x"" in pattern:
					patterntype = ""bin""
				else:
					patterntype = ""str""
			
			if ""b"" in args:
				base,addyok = getAddyArg(args[""b""])
				if not addyok:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return

			if ""t"" in args:
				top,addyok = getAddyArg(args[""t""])
				if not addyok:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
					
			if ""depth"" in args:
				try:
					criteria[""depth""] = int(args[""depth""])
				except:
					dbg.log(""invalid depth value"",highlight=1)
					return	

			if ""all"" in args:
				criteria[""all""] = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					dbg.log(""invalid distance value(s)"",highlight=1)
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0	
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0	
			
				if maxdistance < mindistance:
					tmp = maxdistance
					maxdistance = mindistance
					mindistance = tmp
				
				criteria[""mindistance""] = mindistance
				criteria[""maxdistance""] = maxdistance
						
			allpointers = findPatternWild(modulecriteria,criteria,pattern,base,top,patterntype)
				
			logfile = MnLog(""findwild.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog)		
			return
	
			
		# ----- assemble: assemble instructions to opcodes ----- #
		def procAssemble(args):
			opcodes = """"
			encoder = """"
			
			if not 's' in args:
				dbg.log(""Mandatory argument -s <opcodes> missing"", highlight=1)
				return
			opcodes = args['s']
			
			if 'e' in args:
				# TODO: implement encoder support
				dbg.log(""Encoder support not yet implemented"", highlight=1)
				return
				encoder = args['e'].lowercase()
				if encoder not in [""ascii""]:
					dbg.log(""Invalid encoder : %s"" % encoder, highlight=1)
					return
			
			assemble(opcodes,encoder)
			
		# ----- info: show information about an address ----- #
		def procInfo(args):
			if not ""a"" in args:
				dbg.log(""Missing mandatory argument -a"", highlight=1)
				return
			
			address,addyok = getAddyArg(args[""a""])
			if not addyok:
				dbg.log(""%s is an invalid address"" % args[""a""], highlight=1)
				return
			
			ptr = MnPointer(address)
			modname = ptr.belongsTo()
			modinfo = None
			if modname != """":
				modinfo = MnModule(modname)
			rebase = """"
			rva=0
			if modinfo :
				rva = address - modinfo.moduleBase
			procFlags(args)
			dbg.log("""")			
			dbg.log(""[+] Information about address 0x%s"" % toHex(address))
			dbg.log(""    %s"" % ptr.__str__())
			thepage = dbg.getMemoryPageByAddress(address)
			dbg.log(""    Address is part of page 0x%08x - 0x%08x"" % (thepage.getBaseAddress(),thepage.getBaseAddress()+thepage.getSize()))
			section = """"
			try:
				section = thepage.getSection()
			except:
				section = """"
			if section != """":
				dbg.log(""    Section : %s"" % section)
			
			if ptr.isOnStack():
				stacks = getStacks()
				stackref = """"
				for tid in stacks:
					currstack = stacks[tid]
					if currstack[0] <= address and address <= currstack[1]:
						stackref = "" (Thread 0x%08x, Stack Base : 0x%08x, Stack Top : 0x%08x)"" % (tid,currstack[0],currstack[1])
						break
				dbg.log(""    This address is in a stack segment %s"" % stackref)
			if modinfo:
				dbg.log(""    Address is part of a module:"")
				dbg.log(""    %s"" % modinfo.__str__())
				if rva != 0:
					dbg.log(""    Offset from module base: 0x%x"" % rva)
					if modinfo:
						eatlist = modinfo.getEAT()
						if address in eatlist:
							dbg.log(""    Address is start of function '%s' in %s"" % (eatlist[address],modname))
						else:
							iatlist = modinfo.getIAT()
							if address in iatlist:
								iatentry = iatlist[address]
								dbg.log(""    Address is part of IAT, and contains pointer to '%s'"" % iatentry)				
			else:
				output = """"
				if ptr.isInHeap():
					dbg.log(""    This address resides in the heap"")
					dbg.log("""")
					ptr.showHeapBlockInfo()
				else:
					dbg.log(""    Module: None"")					
			try:
				dbg.log("""")
				dbg.log(""[+] Disassembly:"")
				op = dbg.disasm(address)
				opstring=getDisasmInstruction(op)
				dbg.log(""    Instruction at %s : %s"" % (toHex(address),opstring))
			except:
				pass
			if __DEBUGGERAPP__ == ""WinDBG"":
				dbg.log("""")
				dbg.log(""Output of !address 0x%08x:"" % address)
				output = dbg.nativeCommand(""!address 0x%08","for sehrecord in sehchain:
    address = sehrecord[0]
    sehandler = sehrecord[1]
    nseh = ''
    try:
        nsehvalue = struct.unpack('<L', dbg.readMemory(address, 4))[0]
        nseh = '0x%08x' % nsehvalue
    except:
        nseh = '0x????????'
    bpsuccess = True
    try:
        if __DEBUGGERAPP__ == 'WinDBG':
            bpsuccess = dbg.setBreakpoint(sehandler)
        else:
            dbg.setBreakpoint(sehandler)
            bpsuccess = True
    except:
        bpsuccess = False
    bptext = ''
    if not bpsuccess:
        bptext = 'BP failed'
    else:
        bptext = 'BP set'
    ptr = MnPointer(sehandler)
    funcinfo = ptr.getPtrFunction()
    dbg.log('0x%08x  %s  0x%08x %s <- %s' % (address, nseh, sehandler, funcinfo, bptext))","for sehrecord in sehchain:
    (sehrecord_0, sehrecord_1, *_) = sehrecord
    address = sehrecord[0]
    sehandler = sehrecord[1]
    nseh = ''
    try:
        nsehvalue = struct.unpack('<L', dbg.readMemory(address, 4))[0]
        nseh = '0x%08x' % nsehvalue
    except:
        nseh = '0x????????'
    bpsuccess = True
    try:
        if __DEBUGGERAPP__ == 'WinDBG':
            bpsuccess = dbg.setBreakpoint(sehandler)
        else:
            dbg.setBreakpoint(sehandler)
            bpsuccess = True
    except:
        bpsuccess = False
    bptext = ''
    if not bpsuccess:
        bptext = 'BP failed'
    else:
        bptext = 'BP set'
    ptr = MnPointer(sehandler)
    funcinfo = ptr.getPtrFunction()
    dbg.log('0x%08x  %s  0x%08x %s <- %s' % (address, nseh, sehandler, funcinfo, bptext))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
	dbg.createLogWindow()
	global currentArgs
	currentArgs = copy.copy(args)
	try:
		starttime = datetime.datetime.now()
		ptr_counter = 0
		
		# initialize list of commands
		commands = {}
		
		# ----- HELP ----- #
		def getBanner():
			banners = {}
			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                         __               __                      |\n""
			bannertext += ""    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n""
			bannertext += ""    |  / ___/ __ \/ ___/ _ \/ / __ `/ __ \   / __/ _ \/ __ `/ __ `__ \ |\n""
			bannertext += ""    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n""
			bannertext += ""    | \___/\____/_/   \___/_/\__,_/_/ /_/   \__/\___/\__,_/_/ /_/ /_/  |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |     https://www.corelan.be | https://www.corelan-training.com    |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[0] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""			
			bannertext += ""    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n""
			bannertext += ""    |       | '_ ` _ \  / _ \ | '_ \  / _` |   | '_ \ | | | |          |\n""
			bannertext += ""    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n""
			bannertext += ""    |       |_| |_| |_| \___/ |_| |_| \__,_|(_)| .__/  \__, |          |\n""
			bannertext += ""    |                                          |_|     |___/           |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""	
			banners[1] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |    _____ ___  ____  ____  ____ _                                 |\n""
			bannertext += ""    |    / __ `__ \/ __ \/ __ \/ __ `/  https://www.corelan.be         |\n""
			bannertext += ""    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n""
			bannertext += ""    |  /_/ /_/ /_/\____/_/ /_/\__,_/  #corelan (Freenode IRC)          |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[2] = bannertext

			bannertext = """"
			bannertext += ""\n    .##.....##..#######..##....##....###........########..##....##\n""
			bannertext += ""    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n""
			bannertext += ""    .####.####.##.....##.####..##..##...##......##.....##...####..\n""
			bannertext += ""    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n""
			bannertext += ""    .##.....##.##.....##.##..####.#########.....##...........##...\n""
			bannertext += ""    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n""
			bannertext += ""    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n""
			banners[3] = bannertext


			# pick random banner
			bannerlist = []
			for i in range (0, len(banners)):
				bannerlist.append(i)

			random.shuffle(bannerlist)
			return banners[bannerlist[0]]

		
		def procHelp(args):
			dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__,str(arch)))
			dbg.log(""     Plugin version : %s r%s"" % (__VERSION__,__REV__))
			dbg.log(""     Python version : %s"" % (getPythonVersion()))
			if __DEBUGGERAPP__ == ""WinDBG"":
				pykdversion = dbg.getPyKDVersionNr()
				dbg.log(""     PyKD version %s"" % pykdversion)
			dbg.log(""     Written by Corelan - https://www.corelan.be"")
			dbg.log(""     Project page : https://github.com/corelan/mona"")
			dbg.logLines(getBanner(),highlight=1)
			dbg.log(""Global options :"")
			dbg.log(""----------------"")
			dbg.log(""You can use one or more of the following global options on any command that will perform"")
			dbg.log(""a search in one or more modules, returning a list of pointers :"")
			dbg.log("" -n                     : Skip modules that start with a null byte. If this is too broad, use"")
			dbg.log(""                          option -cp nonull instead"")
			dbg.log("" -o                     : Ignore OS modules"")
			dbg.log("" -p <nr>                : Stop search after <nr> pointers."")
			dbg.log("" -m <module,module,...> : only query the given modules. Be sure what you are doing !"")
			dbg.log(""                          You can specify multiple modules (comma separated)"")
			dbg.log(""                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored"")
			dbg.log(""                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,"")
			dbg.log(""                          blah or *blah* = contains blah"")
			dbg.log("" -cm <crit,crit,...>    : Apply some additional criteria to the modules to query."")
			dbg.log(""                          You can use one or more of the following criteria :"")
			dbg.log(""                          aslr,safeseh,rebase,nx,os"")
			dbg.log(""                          You can enable or disable a certain criterium by setting it to true or false"")
			dbg.log(""                          Example :  -cm aslr=true,safeseh=false"")
			dbg.log(""                          Suppose you want to search for p/p/r in aslr enabled modules, you could call"")
			dbg.log(""                          !mona seh -cm aslr"")
			dbg.log("" -cp <crit,crit,...>    : Apply some criteria to the pointers to return"")
			dbg.log(""                          Available options are :"")
			dbg.log(""                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev"")
			dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
			dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
			dbg.log(""                          You can use .. to indicate a range of bytes (in between 2 bad chars)"")
			dbg.log("" -x <access>            : Specify desired access level of the returning pointers. If not specified,"")
			dbg.log(""                          only executable pointers will be returned."")
			dbg.log(""                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *"")
			
			if not args:
				args = []
			if len(args) > 1:
				thiscmd = args[1].lower().strip()
				if thiscmd in commands:
					dbg.log("""")
					dbg.log(""Usage of command '%s' :"" % thiscmd)
					dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
					dbg.logLines(commands[thiscmd].usage)
					dbg.log("""")
				else:
					aliasfound = False
					for cmd in commands:
						if commands[cmd].alias == thiscmd:
							dbg.log("""")
							dbg.log(""Usage of command '%s' :"" % thiscmd)
							dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
							dbg.logLines(commands[cmd].usage)
							dbg.log("""")
							aliasfound = True
					if not aliasfound:
						dbg.logLines(""\nCommand %s does not exist. Run !mona to get a list of available commands\n"" % thiscmd,highlight=1)
			else:
				dbg.logLines(""\nUsage :"")
				dbg.logLines(""-------\n"")
				dbg.log("" !mona <command> <parameter>"")
				dbg.logLines(""\nAvailable commands and parameters :\n"")

				items = commands.items()
				items.sort(key = itemgetter(0))
				for item in items:
					if commands[item[0]].usage != """":
						aliastxt = """"
						if commands[item[0]].alias != """":
							aliastxt = "" / "" + commands[item[0]].alias
						dbg.logLines(""%s | %s"" % (item[0] + aliastxt + ("" "" * (20 - len(item[0]+aliastxt))), commands[item[0]].description))
				dbg.log("""")
				dbg.log(""Want more info about a given command ?  Run !mona help <command>"",highlight=1)
				dbg.log("""")
		
		commands[""help""] = MnCommand(""help"", ""show help"", ""!mona help [command]"",procHelp)
		
		# ----- Config file management ----- #
		
		def procConfig(args):
			#did we specify -get, -set or -add?
			showerror = False
			if not ""set"" in args and not ""get"" in args and not ""add"" in args:
				showerror = True
				
			if ""set"" in args:
				if type(args[""set""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""set""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""add"" in args:
				if type(args[""add""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""add""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""get"" in args:
				if type(args[""get""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""get""].split("" "")
					if len(params) < 1:
						showerror = True
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(configUsage,highlight=1)
				return
			else:
				if ""get"" in args:
					dbg.log(""Reading value from configuration file"")
					monaConfig = MnConfig()
					thevalue = monaConfig.get(args[""get""])
					dbg.log(""Parameter %s = %s"" % (args[""get""],thevalue))
				
				if ""set"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""set""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = args[""set""][0+len(configparam):len(args[""set""])]
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
				if ""add"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""add""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = monaConfig.get(configparam).strip() + "","" + args[""add""][0+len(configparam):len(args[""add""])].strip()
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
		# ----- Jump to register ----- #
	
		def procFindJ(args):
			return procFindJMP(args)
		
		def procFindJMP(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			
			if (inspect.stack()[1][3] == ""procFindJ""):
				dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."",highlight=1)

			criteria={}
			all_opcodes={}
			
			global ptr_to_get
			ptr_to_get = -1
			
			distancestr = """"
			mindistance = 0
			maxdistance = 0
			
			#did user specify -r <reg> ?
			showerror = False
			if ""r"" in args:
				if type(args[""r""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#valid register ?
					thisreg = args[""r""].upper().strip()
					validregs = dbglib.Registers32BitsOrder
					if not thisreg in validregs:
						showerror = True
			else:
				showerror = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0		
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0						
			
			if maxdistance < mindistance:
				tmp = maxdistance
				maxdistance = mindistance
				mindistance = tmp
			
			criteria[""mindistance""] = mindistance
			criteria[""maxdistance""] = maxdistance
			
			
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(jmpUsage,highlight=1)
				return				
			else:
				modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
				# go for it !	
				all_opcodes=findJMP(modulecriteria,criteria,args[""r""].lower().strip())
			
			# write to log
			logfile = MnLog(""jmp.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog)
		
		# ----- Exception Handler Overwrites ----- #
		
					
		def procFindSEH(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""safeseh""] = False
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False

			criteria = {}
			specialcases = {}
			all_opcodes = {}
			
			global ptr_to_get
			ptr_to_get = -1
			
			#what is the caller function (backwards compatibility with pvefindaddr)
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if ""rop"" in args:
				criteria[""rop""] = True
			
			if ""all"" in args:
				criteria[""all""] = True
				specialcases[""maponly""] = True
			else:
				criteria[""all""] = False
				specialcases[""maponly""] = False
			
			# go for it !	
			all_opcodes = findSEH(modulecriteria,criteria)
			#report findings to log
			logfile = MnLog(""seh.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog,specialcases)
			
			
		# ----- MODULES ------ #
		def procShowMODULES(args):
			modulecriteria={}
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			modulestosearch = getModulesToQuery(modulecriteria)
			showModuleTable("""",modulestosearch)

		# ----- ROP ----- #
		def procFindROPFUNC(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			#modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			ropfuncs = {}
			ropfuncoffsets ={}
			ropfuncs,ropfuncoffsets = findROPFUNC(modulecriteria,criteria)
			#report findings to log
			dbg.log(""[+] Processing pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc.txt"")
			thislog = logfile.reset()
			processResults(ropfuncs,logfile,thislog)
			global silent
			silent = True
			dbg.log(""[+] Processing offsets to pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc_offset.txt"")
			thislog = logfile.reset()
			processResults(ropfuncoffsets,logfile,thislog)			
			
		def procStackPivots(args):
			procROP(args,""stackpivot"")
			
		def procROP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			maxoffset = 40
			thedistance = 8
			split = False
			fast = False
			sortedprint = False
			endingstr = """"
			endings = []
			technique = """"            
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass
			
			if ""offset"" in args:
				if type(args[""offset""]).__name__.lower() != ""bool"":
					try:
						maxoffset = int(args[""offset""])
					except:
						pass
			
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() != ""bool"":
					try:
						thedistance = args[""distance""]
					except:
						pass
			
			if ""split"" in args:
				if type(args[""split""]).__name__.lower() == ""bool"":
					split = args[""split""]

			if ""s"" in args:
				if type(args[""s""]).__name__.lower() != ""bool"":
					technique = args[""s""].replace(""'"","""").replace('""',"""").strip().lower()                   
					
			if ""fast"" in args:
				if type(args[""fast""]).__name__.lower() == ""bool"":
					fast = args[""fast""]
			
			if ""end"" in args:
				if type(args[""end""]).__name__.lower() == ""str"":
					endingstr = args[""end""].replace(""'"","""").replace('""',"""").strip()
					endings = endingstr.split(""#"")
					
			if ""f"" in args:
				if args[""f""] != """":
					criteria[""f""] = args[""f""]
			
			if ""sort"" in args:
				sortedprint = True
			
			if ""rva"" in args:
				criteria[""rva""] = True
			
			if mode == ""stackpivot"":
				fast = False
				endings = """"
				split = False
			else:
				mode = ""all""
			
			findROPGADGETS(modulecriteria,criteria,endings,maxoffset,depth,split,thedistance,fast,mode,sortedprint,technique)
			

		def procJseh(args):
			results = []
			showred=0
			showall=False
			if ""all"" in args:
				showall = True
			nrfound = 0
			dbg.log(""-----------------------------------------------------------------------"")
			dbg.log(""Search for jmp/call dword[ebp/esp+nn] (and other) combinations started "")
			dbg.log(""-----------------------------------------------------------------------"")
			opcodej=[""\xff\x54\x24\x08"", #call dword ptr [esp+08]
					""\xff\x64\x24\x08"", #jmp dword ptr [esp+08]
					""\xff\x54\x24\x14"", #call dword ptr [esp+14]
					""\xff\x54\x24\x14"", #jmp dword ptr [esp+14]
					""\xff\x54\x24\x1c"", #call dword ptr [esp+1c]
					""\xff\x54\x24\x1c"", #jmp dword ptr [esp+1c]
					""\xff\x54\x24\x2c"", #call dword ptr [esp+2c]
					""\xff\x54\x24\x2c"", #jmp dword ptr [esp+2c]
					""\xff\x54\x24\x44"", #call dword ptr [esp+44]
					""\xff\x54\x24\x44"", #jmp dword ptr [esp+44]
					""\xff\x54\x24\x50"", #call dword ptr [esp+50]
					""\xff\x54\x24\x50"", #jmp dword ptr [esp+50]
					""\xff\x55\x0c"",     #call dword ptr [ebp+0c]
					""\xff\x65\x0c"",     #jmp dword ptr [ebp+0c]
					""\xff\x55\x24"",     #call dword ptr [ebp+24]
					""\xff\x65\x24"",     #jmp dword ptr [ebp+24]
					""\xff\x55\x30"",     #call dword ptr [ebp+30]
					""\xff\x65\x30"",     #jmp dword ptr [ebp+30]
					""\xff\x55\xfc"",     #call dword ptr [ebp-04]
					""\xff\x65\xfc"",     #jmp dword ptr [ebp-04]
					""\xff\x55\xf4"",     #call dword ptr [ebp-0c]
					""\xff\x65\xf4"",     #jmp dword ptr [ebp-0c]
					""\xff\x55\xe8"",     #call dword ptr [ebp-18]
					""\xff\x65\xe8"",     #jmp dword ptr [ebp-18]
					""\x83\xc4\x08\xc3"", #add esp,8 + ret
					""\x83\xc4\x08\xc2""] #add esp,8 + ret X
			fakeptrcriteria = {}
			fakeptrcriteria[""accesslevel""] = ""*""
			for opjc in opcodej:
				addys = []
				addys = searchInRange( [[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
				results += addys
				for ptrtypes in addys:
					for ad1 in addys[ptrtypes]:
						ptr = MnPointer(ad1)
						module = ptr.belongsTo()
						if not module:
							module=""""
							page   = dbg.getMemoryPageByAddress( ad1 )
							access = page.getAccess( human = True )
							op = dbg.disasm( ad1 )
							opstring=op.getDisasm()
							dbg.log(""Found %s at 0x%08x - Access: (%s) - Outside of a loaded module"" % (opstring, ad1, access), address = ad1,highlight=1)
							nrfound+=1
						else:
							if showall:
								page   = dbg.getMemoryPageByAddress( ad1 )
								access = page.getAccess( human = True )
								op = dbg.disasm( ad1 )
								opstring=op.getDisasm()
								thismod = MnModule(module)
								if not thismod.isSafeSEH:
								#if ismodulenosafeseh(module[0])==1:
									extratext=""=== Safeseh : NO ===""
									showred=1
								else:
									extratext=""Safeseh protected""
									showred=0
								dbg.log(""Found %s at 0x%08x (%s) - Access: (%s) - %s"" % (opstring, ad1, module,access,extratext), address = ad1,highlight=showred)
								nrfound+=1
			dbg.log(""Search complete"")
			if results:
				dbg.log(""Found %d address(es)"" % nrfound)
				return ""Found %d address(es) (Check the log Windows for details)"" % nrfound
			else:
				dbg.log(""No addresses found"")
				return ""Sorry, no addresses found""

			
		def procJOP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass			
			findJOPGADGETS(modulecriteria,criteria,depth)			
			
			
		def procCreatePATTERN(args):
			size = 0
			pattern = """"
			if ""?"" in args and args[""?""] != """":
				try:
					if ""0x"" in args[""?""].lower():
						try:
							size = int(args[""?""],16)
						except:
							size = 0
					else:
						size = int(args[""?""])
				except:
					size = 0
			if size == 0:
				dbg.log(""Please enter a valid size"",highlight=1)
			else:
				pattern = createPattern(size,args)
				dbg.log(""Creating cyclic pattern of %d bytes"" % size)				
				dbg.log(pattern)
				global ignoremodules
				ignoremodules = True
				objpatternfile = MnLog(""pattern.txt"")
				patternfile = objpatternfile.reset()
				# ASCII
				objpatternfile.write(""\nPattern of "" + str(size) + "" bytes :\n"",patternfile)
				objpatternfile.write(""-"" * (19 + len(str(size))),patternfile)
				objpatternfile.write(""\nASCII:"",patternfile)
				objpatternfile.write(""\n"" + pattern,patternfile)
				# Hex
				patternhex = """"
				for patternchar in pattern:
					patternhex += str(hex(ord(patternchar))).replace(""0x"",""\\x"")
				objpatternfile.write(""\n\nHEX:\n"",patternfile)
				objpatternfile.write(patternhex,patternfile)
				# Javascript
				patternjs = str2js(pattern)
				objpatternfile.write(""\n\nJAVASCRIPT (unescape() friendly):\n"",patternfile)
				objpatternfile.write(patternjs,patternfile)
				if not silent:
					dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"",highlight=1)
					dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile,highlight=1)
				
				ignoremodules = False
			return


		def procOffsetPATTERN(args):
			egg = """"
			if ""?"" in args and args[""?""] != """":
				try:
					egg = args[""?""]
				except:
					egg = """"
			if egg == """":
				dbg.log(""Please enter a valid target"",highlight=1)
			else:
				findOffsetInPattern(egg,-1,args)
			return
		
		# ----- Comparing file output ----- #
		def procFileCOMPARE(args):
			modulecriteria={}
			criteria={}
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			allfiles=[]
			tomatch=""""
			checkstrict=True
			rangeval = 0
			fast = False
			if ""ptronly"" in args or ""ptrsonly"" in args:
				fast = True
			if ""f"" in args:
				if args[""f""] != """":
					rawfilenames=args[""f""].replace('""',"""")
					allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
					dbg.log(""[+] Number of files to be examined : %d "" % len(allfiles))
			if ""range"" in args:
				if not type(args[""range""]).__name__.lower() == ""bool"":
					strrange = args[""range""].lower()
					if strrange.startswith(""0x"") and len(strrange) > 2 :
						rangeval = int(strrange,16)
					else:
						try:
							rangeval = int(args[""range""])
						except:
							rangeval = 0
					if rangeval > 0:
						dbg.log(""[+] Find overlap using pointer +/- range, value %d"" % rangeval)
						dbg.log(""    Note : this will significantly slow down the comparison process !"")
				else:
					dbg.log(""Please provide a numeric value ^(> 0) with option -range"",highlight=1)
					return
			else:
				if ""contains"" in args:
					if type(args[""contains""]).__name__.lower() == ""str"":
						tomatch = args[""contains""].replace(""'"","""").replace('""',"""")
				if ""nostrict"" in args:
					if type(args[""nostrict""]).__name__.lower() == ""bool"":
						checkstrict = not args[""nostrict""]
						dbg.log(""[+] Instructions must match in all files ? %s"" % checkstrict)
			# maybe one of the arguments is a folder
			callfiles = allfiles
			allfiles = []
			for tfile in callfiles:
				if os.path.isdir(tfile):
					# folder, get all files from this folder
					for root,dirs,files in os.walk(tfile):
						for dfile in files:
							allfiles.append(os.path.join(root,dfile))
				else:
					allfiles.append(tfile)
			if len(allfiles) > 1:
				findFILECOMPARISON(modulecriteria,criteria,allfiles,tomatch,checkstrict,rangeval,fast)
			else:
				dbg.log(""Please specify at least 2 filenames to compare"",highlight=1)

		# ----- Find bytes in memory ----- #
		def procFind(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			base = 0
			offset = 0
			top  = TOP_USERLAND
			consecutive = False
			ftype = """"
			
			level = 0
			offsetlevel = 0			
			
			if not ""a"" in args:
				args[""a""] = ""*""

			ptronly = False

			if ""ptronly"" in args or ""ptrsonly"" in args:
				ptronly = True	
			
			#search for all pointers by default
			if not ""x"" in args:
				args[""x""] = ""*""
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			if criteria[""accesslevel""] == """":
				return
			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			if ""unicode"" in args:
				criteria[""unic""] = True

			if ""b"" in args:
				try:
					base = int(args[""b""],16)
				except:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return
			if ""t"" in args:
				try:
					top = int(args[""t""],16)
				except:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
			if ""offset"" in args:
				if not args[""offset""].__class__.__name__ == ""bool"":
					if ""0x"" in args[""offset""].lower():
						try:
							offset = 0 - int(args[""offset""],16)
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return
					else:	
						try:
							offset = 0 - int(args[""offset""])
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return	
				else:
					dbg.log(""invalid offset value"",highlight=1)
					return
					
			if ""level"" in args:
				try:
					level = int(args[""level""])
				except:
					dbg.log(""invalid level value"",highlight=1)
					return

			if ""offsetlevel"" in args:
				try:
					offsetlevel = int(args[""offsetlevel""])
				except:
					dbg.log(""invalid offsetlevel value"",highlight=1)
					return						
					
			if ""c"" in args:
				dbg.log(""    - Skipping consecutive pointers, showing size instead"")			
				consecutive = True
				
			if ""type"" in args:
				if not args[""type""] in [""bin"",""asc"",""ptr"",""instr"",""file""]:
					dbg.log(""Invalid search type : %s"" % args[""type""], highlight=1)
					return
				ftype = args[""type""] 
				if ftype == ""file"":
					filename = args[""s""].replace('""',"""").replace(""'"","""")
					#see if we can read the file
					if not os.path.isfile(filename):
						dbg.log(""Unable to find/read file %s"" % filename,highlight=1)
						return
			rangep2p = 0

			
			if ""p2p"" in args or level > 0:
				dbg.log(""    - Looking for pointers to pointers"")
				criteria[""p2p""] = True
				if ""r"" in args:	
					try:
						rangep2p = int(args[""r""])
					except:
						pass
					if rangep2p > 0:
						dbg.log(""    - Will search for close pointers (%d bytes backwards)"" % rangep2p)
				if ""p2p"" in args:
					level = 1
			
			
			if level > 0:
				dbg.log(""    - Recursive levels : %d"" % level)
			

			allpointers = findPattern(modulecriteria,criteria,pattern,ftype,base,top,consecutive,rangep2p,level,offset,offsetlevel)
				
			logfile = MnLog(""find.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog,{},ptronly)
			return
			
			
		# ---- Find instructions, wildcard search ----- #
		def procFindWild(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			patterntype = """"
			base = 0
			top  = TOP_USERLAND
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			patterntypes = [""bin"",""str""]
			if ""type"" in args:
				if type(args[""type""]).__name__.lower() != ""bool"":
					if args[""type""] in patterntypes:
						patterntype = args[""type""]
					else:
						dbg.log(""-type argument only takes one of these values: %s"" % patterntypes,highlight=1)
						return
				else:
					dbg.log(""Please specify a valid value for -type. Valid values are %s"" % patterntypes,highlight=1)
					return


			if patterntype == """":
				if ""\\x"" in pattern:
					patterntype = ""bin""
				else:
					patterntype = ""str""
			
			if ""b"" in args:
				base,addyok = getAddyArg(args[""b""])
				if not addyok:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return

			if ""t"" in args:
				top,addyok = getAddyArg(args[""t""])
				if not addyok:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
					
			if ""depth"" in args:
				try:
					criteria[""depth""] = int(args[""depth""])
				except:
					dbg.log(""invalid depth value"",highlight=1)
					return	

			if ""all"" in args:
				criteria[""all""] = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					dbg.log(""invalid distance value(s)"",highlight=1)
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0	
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0	
			
				if maxdistance < mindistance:
					tmp = maxdistance
					maxdistance = mindistance
					mindistance = tmp
				
				criteria[""mindistance""] = mindistance
				criteria[""maxdistance""] = maxdistance
						
			allpointers = findPatternWild(modulecriteria,criteria,pattern,base,top,patterntype)
				
			logfile = MnLog(""findwild.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog)		
			return
	
			
		# ----- assemble: assemble instructions to opcodes ----- #
		def procAssemble(args):
			opcodes = """"
			encoder = """"
			
			if not 's' in args:
				dbg.log(""Mandatory argument -s <opcodes> missing"", highlight=1)
				return
			opcodes = args['s']
			
			if 'e' in args:
				# TODO: implement encoder support
				dbg.log(""Encoder support not yet implemented"", highlight=1)
				return
				encoder = args['e'].lowercase()
				if encoder not in [""ascii""]:
					dbg.log(""Invalid encoder : %s"" % encoder, highlight=1)
					return
			
			assemble(opcodes,encoder)
			
		# ----- info: show information about an address ----- #
		def procInfo(args):
			if not ""a"" in args:
				dbg.log(""Missing mandatory argument -a"", highlight=1)
				return
			
			address,addyok = getAddyArg(args[""a""])
			if not addyok:
				dbg.log(""%s is an invalid address"" % args[""a""], highlight=1)
				return
			
			ptr = MnPointer(address)
			modname = ptr.belongsTo()
			modinfo = None
			if modname != """":
				modinfo = MnModule(modname)
			rebase = """"
			rva=0
			if modinfo :
				rva = address - modinfo.moduleBase
			procFlags(args)
			dbg.log("""")			
			dbg.log(""[+] Information about address 0x%s"" % toHex(address))
			dbg.log(""    %s"" % ptr.__str__())
			thepage = dbg.getMemoryPageByAddress(address)
			dbg.log(""    Address is part of page 0x%08x - 0x%08x"" % (thepage.getBaseAddress(),thepage.getBaseAddress()+thepage.getSize()))
			section = """"
			try:
				section = thepage.getSection()
			except:
				section = """"
			if section != """":
				dbg.log(""    Section : %s"" % section)
			
			if ptr.isOnStack():
				stacks = getStacks()
				stackref = """"
				for tid in stacks:
					currstack = stacks[tid]
					if currstack[0] <= address and address <= currstack[1]:
						stackref = "" (Thread 0x%08x, Stack Base : 0x%08x, Stack Top : 0x%08x)"" % (tid,currstack[0],currstack[1])
						break
				dbg.log(""    This address is in a stack segment %s"" % stackref)
			if modinfo:
				dbg.log(""    Address is part of a module:"")
				dbg.log(""    %s"" % modinfo.__str__())
				if rva != 0:
					dbg.log(""    Offset from module base: 0x%x"" % rva)
					if modinfo:
						eatlist = modinfo.getEAT()
						if address in eatlist:
							dbg.log(""    Address is start of function '%s' in %s"" % (eatlist[address],modname))
						else:
							iatlist = modinfo.getIAT()
							if address in iatlist:
								iatentry = iatlist[address]
								dbg.log(""    Address is part of IAT, and contains pointer to '%s'"" % iatentry)				
			else:
				output = """"
				if ptr.isInHeap():
					dbg.log(""    This address resides in the heap"")
					dbg.log("""")
					ptr.showHeapBlockInfo()
				else:
					dbg.log(""    Module: None"")					
			try:
				dbg.log("""")
				dbg.log(""[+] Disassembly:"")
				op = dbg.disasm(address)
				opstring=getDisasmInstruction(op)
				dbg.log(""    Instruction at %s : %s"" % (toHex(address),opstring))
			except:
				pass
			if __DEBUGGERAPP__ == ""WinDBG"":
				dbg.log("""")
				dbg.log(""Output of !address 0x%08x:"" % address)
				output = dbg.nativeCommand(""!address 0x%08","for sehrecord in sehchain:
    recaddress = sehrecord[0]
    sehandler = sehrecord[1]
    nseh = ''
    try:
        nsehvalue = struct.unpack('<L', dbg.readMemory(recaddress, 4))[0]
        nseh = '0x%08x' % nsehvalue
    except:
        nseh = 0
        sehandler = 0
    overwritedata = checkSEHOverwrite(recaddress, nseh, sehandler)
    overwritemark = ''
    funcinfo = ''
    if sehandler > 0:
        ptr = MnPointer(sehandler)
        funcinfo = ptr.getPtrFunction()
    else:
        funcinfo = ' (corrupted record)'
        if str(nseh).startswith('0x'):
            nseh = '0x%08x' % int(nseh, 16)
        else:
            nseh = '0x%08x' % int(nseh)
    if len(overwritedata) > 0:
        handlersoverwritten[recaddress] = overwritedata
        smashoffset = int(overwritedata[1])
        typeinfo = ''
        if overwritedata[0] == 'unicode':
            smashoffset += 2
            typeinfo = ' [unicode]'
        overwritemark = ' (record smashed at offset %d%s)' % (smashoffset, typeinfo)
    dbg.log('0x%08x  %s  0x%08x %s%s' % (recaddress, nseh, sehandler, funcinfo, overwritemark), recaddress)","for sehrecord in sehchain:
    (sehrecord_0, sehrecord_1, *_) = sehrecord
    recaddress = sehrecord[0]
    sehandler = sehrecord[1]
    nseh = ''
    try:
        nsehvalue = struct.unpack('<L', dbg.readMemory(recaddress, 4))[0]
        nseh = '0x%08x' % nsehvalue
    except:
        nseh = 0
        sehandler = 0
    overwritedata = checkSEHOverwrite(recaddress, nseh, sehandler)
    overwritemark = ''
    funcinfo = ''
    if sehandler > 0:
        ptr = MnPointer(sehandler)
        funcinfo = ptr.getPtrFunction()
    else:
        funcinfo = ' (corrupted record)'
        if str(nseh).startswith('0x'):
            nseh = '0x%08x' % int(nseh, 16)
        else:
            nseh = '0x%08x' % int(nseh)
    if len(overwritedata) > 0:
        handlersoverwritten[recaddress] = overwritedata
        smashoffset = int(overwritedata[1])
        typeinfo = ''
        if overwritedata[0] == 'unicode':
            smashoffset += 2
            typeinfo = ' [unicode]'
        overwritemark = ' (record smashed at offset %d%s)' % (smashoffset, typeinfo)
    dbg.log('0x%08x  %s  0x%08x %s%s' % (recaddress, nseh, sehandler, funcinfo, overwritemark), recaddress)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
	dbg.createLogWindow()
	global currentArgs
	currentArgs = copy.copy(args)
	try:
		starttime = datetime.datetime.now()
		ptr_counter = 0
		
		# initialize list of commands
		commands = {}
		
		# ----- HELP ----- #
		def getBanner():
			banners = {}
			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                         __               __                      |\n""
			bannertext += ""    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n""
			bannertext += ""    |  / ___/ __ \/ ___/ _ \/ / __ `/ __ \   / __/ _ \/ __ `/ __ `__ \ |\n""
			bannertext += ""    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n""
			bannertext += ""    | \___/\____/_/   \___/_/\__,_/_/ /_/   \__/\___/\__,_/_/ /_/ /_/  |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |     https://www.corelan.be | https://www.corelan-training.com    |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[0] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""			
			bannertext += ""    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n""
			bannertext += ""    |       | '_ ` _ \  / _ \ | '_ \  / _` |   | '_ \ | | | |          |\n""
			bannertext += ""    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n""
			bannertext += ""    |       |_| |_| |_| \___/ |_| |_| \__,_|(_)| .__/  \__, |          |\n""
			bannertext += ""    |                                          |_|     |___/           |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""	
			banners[1] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |    _____ ___  ____  ____  ____ _                                 |\n""
			bannertext += ""    |    / __ `__ \/ __ \/ __ \/ __ `/  https://www.corelan.be         |\n""
			bannertext += ""    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n""
			bannertext += ""    |  /_/ /_/ /_/\____/_/ /_/\__,_/  #corelan (Freenode IRC)          |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[2] = bannertext

			bannertext = """"
			bannertext += ""\n    .##.....##..#######..##....##....###........########..##....##\n""
			bannertext += ""    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n""
			bannertext += ""    .####.####.##.....##.####..##..##...##......##.....##...####..\n""
			bannertext += ""    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n""
			bannertext += ""    .##.....##.##.....##.##..####.#########.....##...........##...\n""
			bannertext += ""    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n""
			bannertext += ""    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n""
			banners[3] = bannertext


			# pick random banner
			bannerlist = []
			for i in range (0, len(banners)):
				bannerlist.append(i)

			random.shuffle(bannerlist)
			return banners[bannerlist[0]]

		
		def procHelp(args):
			dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__,str(arch)))
			dbg.log(""     Plugin version : %s r%s"" % (__VERSION__,__REV__))
			dbg.log(""     Python version : %s"" % (getPythonVersion()))
			if __DEBUGGERAPP__ == ""WinDBG"":
				pykdversion = dbg.getPyKDVersionNr()
				dbg.log(""     PyKD version %s"" % pykdversion)
			dbg.log(""     Written by Corelan - https://www.corelan.be"")
			dbg.log(""     Project page : https://github.com/corelan/mona"")
			dbg.logLines(getBanner(),highlight=1)
			dbg.log(""Global options :"")
			dbg.log(""----------------"")
			dbg.log(""You can use one or more of the following global options on any command that will perform"")
			dbg.log(""a search in one or more modules, returning a list of pointers :"")
			dbg.log("" -n                     : Skip modules that start with a null byte. If this is too broad, use"")
			dbg.log(""                          option -cp nonull instead"")
			dbg.log("" -o                     : Ignore OS modules"")
			dbg.log("" -p <nr>                : Stop search after <nr> pointers."")
			dbg.log("" -m <module,module,...> : only query the given modules. Be sure what you are doing !"")
			dbg.log(""                          You can specify multiple modules (comma separated)"")
			dbg.log(""                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored"")
			dbg.log(""                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,"")
			dbg.log(""                          blah or *blah* = contains blah"")
			dbg.log("" -cm <crit,crit,...>    : Apply some additional criteria to the modules to query."")
			dbg.log(""                          You can use one or more of the following criteria :"")
			dbg.log(""                          aslr,safeseh,rebase,nx,os"")
			dbg.log(""                          You can enable or disable a certain criterium by setting it to true or false"")
			dbg.log(""                          Example :  -cm aslr=true,safeseh=false"")
			dbg.log(""                          Suppose you want to search for p/p/r in aslr enabled modules, you could call"")
			dbg.log(""                          !mona seh -cm aslr"")
			dbg.log("" -cp <crit,crit,...>    : Apply some criteria to the pointers to return"")
			dbg.log(""                          Available options are :"")
			dbg.log(""                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev"")
			dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
			dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
			dbg.log(""                          You can use .. to indicate a range of bytes (in between 2 bad chars)"")
			dbg.log("" -x <access>            : Specify desired access level of the returning pointers. If not specified,"")
			dbg.log(""                          only executable pointers will be returned."")
			dbg.log(""                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *"")
			
			if not args:
				args = []
			if len(args) > 1:
				thiscmd = args[1].lower().strip()
				if thiscmd in commands:
					dbg.log("""")
					dbg.log(""Usage of command '%s' :"" % thiscmd)
					dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
					dbg.logLines(commands[thiscmd].usage)
					dbg.log("""")
				else:
					aliasfound = False
					for cmd in commands:
						if commands[cmd].alias == thiscmd:
							dbg.log("""")
							dbg.log(""Usage of command '%s' :"" % thiscmd)
							dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
							dbg.logLines(commands[cmd].usage)
							dbg.log("""")
							aliasfound = True
					if not aliasfound:
						dbg.logLines(""\nCommand %s does not exist. Run !mona to get a list of available commands\n"" % thiscmd,highlight=1)
			else:
				dbg.logLines(""\nUsage :"")
				dbg.logLines(""-------\n"")
				dbg.log("" !mona <command> <parameter>"")
				dbg.logLines(""\nAvailable commands and parameters :\n"")

				items = commands.items()
				items.sort(key = itemgetter(0))
				for item in items:
					if commands[item[0]].usage != """":
						aliastxt = """"
						if commands[item[0]].alias != """":
							aliastxt = "" / "" + commands[item[0]].alias
						dbg.logLines(""%s | %s"" % (item[0] + aliastxt + ("" "" * (20 - len(item[0]+aliastxt))), commands[item[0]].description))
				dbg.log("""")
				dbg.log(""Want more info about a given command ?  Run !mona help <command>"",highlight=1)
				dbg.log("""")
		
		commands[""help""] = MnCommand(""help"", ""show help"", ""!mona help [command]"",procHelp)
		
		# ----- Config file management ----- #
		
		def procConfig(args):
			#did we specify -get, -set or -add?
			showerror = False
			if not ""set"" in args and not ""get"" in args and not ""add"" in args:
				showerror = True
				
			if ""set"" in args:
				if type(args[""set""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""set""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""add"" in args:
				if type(args[""add""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""add""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""get"" in args:
				if type(args[""get""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""get""].split("" "")
					if len(params) < 1:
						showerror = True
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(configUsage,highlight=1)
				return
			else:
				if ""get"" in args:
					dbg.log(""Reading value from configuration file"")
					monaConfig = MnConfig()
					thevalue = monaConfig.get(args[""get""])
					dbg.log(""Parameter %s = %s"" % (args[""get""],thevalue))
				
				if ""set"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""set""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = args[""set""][0+len(configparam):len(args[""set""])]
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
				if ""add"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""add""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = monaConfig.get(configparam).strip() + "","" + args[""add""][0+len(configparam):len(args[""add""])].strip()
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
		# ----- Jump to register ----- #
	
		def procFindJ(args):
			return procFindJMP(args)
		
		def procFindJMP(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			
			if (inspect.stack()[1][3] == ""procFindJ""):
				dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."",highlight=1)

			criteria={}
			all_opcodes={}
			
			global ptr_to_get
			ptr_to_get = -1
			
			distancestr = """"
			mindistance = 0
			maxdistance = 0
			
			#did user specify -r <reg> ?
			showerror = False
			if ""r"" in args:
				if type(args[""r""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#valid register ?
					thisreg = args[""r""].upper().strip()
					validregs = dbglib.Registers32BitsOrder
					if not thisreg in validregs:
						showerror = True
			else:
				showerror = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0		
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0						
			
			if maxdistance < mindistance:
				tmp = maxdistance
				maxdistance = mindistance
				mindistance = tmp
			
			criteria[""mindistance""] = mindistance
			criteria[""maxdistance""] = maxdistance
			
			
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(jmpUsage,highlight=1)
				return				
			else:
				modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
				# go for it !	
				all_opcodes=findJMP(modulecriteria,criteria,args[""r""].lower().strip())
			
			# write to log
			logfile = MnLog(""jmp.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog)
		
		# ----- Exception Handler Overwrites ----- #
		
					
		def procFindSEH(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""safeseh""] = False
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False

			criteria = {}
			specialcases = {}
			all_opcodes = {}
			
			global ptr_to_get
			ptr_to_get = -1
			
			#what is the caller function (backwards compatibility with pvefindaddr)
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if ""rop"" in args:
				criteria[""rop""] = True
			
			if ""all"" in args:
				criteria[""all""] = True
				specialcases[""maponly""] = True
			else:
				criteria[""all""] = False
				specialcases[""maponly""] = False
			
			# go for it !	
			all_opcodes = findSEH(modulecriteria,criteria)
			#report findings to log
			logfile = MnLog(""seh.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog,specialcases)
			
			
		# ----- MODULES ------ #
		def procShowMODULES(args):
			modulecriteria={}
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			modulestosearch = getModulesToQuery(modulecriteria)
			showModuleTable("""",modulestosearch)

		# ----- ROP ----- #
		def procFindROPFUNC(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			#modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			ropfuncs = {}
			ropfuncoffsets ={}
			ropfuncs,ropfuncoffsets = findROPFUNC(modulecriteria,criteria)
			#report findings to log
			dbg.log(""[+] Processing pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc.txt"")
			thislog = logfile.reset()
			processResults(ropfuncs,logfile,thislog)
			global silent
			silent = True
			dbg.log(""[+] Processing offsets to pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc_offset.txt"")
			thislog = logfile.reset()
			processResults(ropfuncoffsets,logfile,thislog)			
			
		def procStackPivots(args):
			procROP(args,""stackpivot"")
			
		def procROP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			maxoffset = 40
			thedistance = 8
			split = False
			fast = False
			sortedprint = False
			endingstr = """"
			endings = []
			technique = """"            
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass
			
			if ""offset"" in args:
				if type(args[""offset""]).__name__.lower() != ""bool"":
					try:
						maxoffset = int(args[""offset""])
					except:
						pass
			
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() != ""bool"":
					try:
						thedistance = args[""distance""]
					except:
						pass
			
			if ""split"" in args:
				if type(args[""split""]).__name__.lower() == ""bool"":
					split = args[""split""]

			if ""s"" in args:
				if type(args[""s""]).__name__.lower() != ""bool"":
					technique = args[""s""].replace(""'"","""").replace('""',"""").strip().lower()                   
					
			if ""fast"" in args:
				if type(args[""fast""]).__name__.lower() == ""bool"":
					fast = args[""fast""]
			
			if ""end"" in args:
				if type(args[""end""]).__name__.lower() == ""str"":
					endingstr = args[""end""].replace(""'"","""").replace('""',"""").strip()
					endings = endingstr.split(""#"")
					
			if ""f"" in args:
				if args[""f""] != """":
					criteria[""f""] = args[""f""]
			
			if ""sort"" in args:
				sortedprint = True
			
			if ""rva"" in args:
				criteria[""rva""] = True
			
			if mode == ""stackpivot"":
				fast = False
				endings = """"
				split = False
			else:
				mode = ""all""
			
			findROPGADGETS(modulecriteria,criteria,endings,maxoffset,depth,split,thedistance,fast,mode,sortedprint,technique)
			

		def procJseh(args):
			results = []
			showred=0
			showall=False
			if ""all"" in args:
				showall = True
			nrfound = 0
			dbg.log(""-----------------------------------------------------------------------"")
			dbg.log(""Search for jmp/call dword[ebp/esp+nn] (and other) combinations started "")
			dbg.log(""-----------------------------------------------------------------------"")
			opcodej=[""\xff\x54\x24\x08"", #call dword ptr [esp+08]
					""\xff\x64\x24\x08"", #jmp dword ptr [esp+08]
					""\xff\x54\x24\x14"", #call dword ptr [esp+14]
					""\xff\x54\x24\x14"", #jmp dword ptr [esp+14]
					""\xff\x54\x24\x1c"", #call dword ptr [esp+1c]
					""\xff\x54\x24\x1c"", #jmp dword ptr [esp+1c]
					""\xff\x54\x24\x2c"", #call dword ptr [esp+2c]
					""\xff\x54\x24\x2c"", #jmp dword ptr [esp+2c]
					""\xff\x54\x24\x44"", #call dword ptr [esp+44]
					""\xff\x54\x24\x44"", #jmp dword ptr [esp+44]
					""\xff\x54\x24\x50"", #call dword ptr [esp+50]
					""\xff\x54\x24\x50"", #jmp dword ptr [esp+50]
					""\xff\x55\x0c"",     #call dword ptr [ebp+0c]
					""\xff\x65\x0c"",     #jmp dword ptr [ebp+0c]
					""\xff\x55\x24"",     #call dword ptr [ebp+24]
					""\xff\x65\x24"",     #jmp dword ptr [ebp+24]
					""\xff\x55\x30"",     #call dword ptr [ebp+30]
					""\xff\x65\x30"",     #jmp dword ptr [ebp+30]
					""\xff\x55\xfc"",     #call dword ptr [ebp-04]
					""\xff\x65\xfc"",     #jmp dword ptr [ebp-04]
					""\xff\x55\xf4"",     #call dword ptr [ebp-0c]
					""\xff\x65\xf4"",     #jmp dword ptr [ebp-0c]
					""\xff\x55\xe8"",     #call dword ptr [ebp-18]
					""\xff\x65\xe8"",     #jmp dword ptr [ebp-18]
					""\x83\xc4\x08\xc3"", #add esp,8 + ret
					""\x83\xc4\x08\xc2""] #add esp,8 + ret X
			fakeptrcriteria = {}
			fakeptrcriteria[""accesslevel""] = ""*""
			for opjc in opcodej:
				addys = []
				addys = searchInRange( [[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
				results += addys
				for ptrtypes in addys:
					for ad1 in addys[ptrtypes]:
						ptr = MnPointer(ad1)
						module = ptr.belongsTo()
						if not module:
							module=""""
							page   = dbg.getMemoryPageByAddress( ad1 )
							access = page.getAccess( human = True )
							op = dbg.disasm( ad1 )
							opstring=op.getDisasm()
							dbg.log(""Found %s at 0x%08x - Access: (%s) - Outside of a loaded module"" % (opstring, ad1, access), address = ad1,highlight=1)
							nrfound+=1
						else:
							if showall:
								page   = dbg.getMemoryPageByAddress( ad1 )
								access = page.getAccess( human = True )
								op = dbg.disasm( ad1 )
								opstring=op.getDisasm()
								thismod = MnModule(module)
								if not thismod.isSafeSEH:
								#if ismodulenosafeseh(module[0])==1:
									extratext=""=== Safeseh : NO ===""
									showred=1
								else:
									extratext=""Safeseh protected""
									showred=0
								dbg.log(""Found %s at 0x%08x (%s) - Access: (%s) - %s"" % (opstring, ad1, module,access,extratext), address = ad1,highlight=showred)
								nrfound+=1
			dbg.log(""Search complete"")
			if results:
				dbg.log(""Found %d address(es)"" % nrfound)
				return ""Found %d address(es) (Check the log Windows for details)"" % nrfound
			else:
				dbg.log(""No addresses found"")
				return ""Sorry, no addresses found""

			
		def procJOP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass			
			findJOPGADGETS(modulecriteria,criteria,depth)			
			
			
		def procCreatePATTERN(args):
			size = 0
			pattern = """"
			if ""?"" in args and args[""?""] != """":
				try:
					if ""0x"" in args[""?""].lower():
						try:
							size = int(args[""?""],16)
						except:
							size = 0
					else:
						size = int(args[""?""])
				except:
					size = 0
			if size == 0:
				dbg.log(""Please enter a valid size"",highlight=1)
			else:
				pattern = createPattern(size,args)
				dbg.log(""Creating cyclic pattern of %d bytes"" % size)				
				dbg.log(pattern)
				global ignoremodules
				ignoremodules = True
				objpatternfile = MnLog(""pattern.txt"")
				patternfile = objpatternfile.reset()
				# ASCII
				objpatternfile.write(""\nPattern of "" + str(size) + "" bytes :\n"",patternfile)
				objpatternfile.write(""-"" * (19 + len(str(size))),patternfile)
				objpatternfile.write(""\nASCII:"",patternfile)
				objpatternfile.write(""\n"" + pattern,patternfile)
				# Hex
				patternhex = """"
				for patternchar in pattern:
					patternhex += str(hex(ord(patternchar))).replace(""0x"",""\\x"")
				objpatternfile.write(""\n\nHEX:\n"",patternfile)
				objpatternfile.write(patternhex,patternfile)
				# Javascript
				patternjs = str2js(pattern)
				objpatternfile.write(""\n\nJAVASCRIPT (unescape() friendly):\n"",patternfile)
				objpatternfile.write(patternjs,patternfile)
				if not silent:
					dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"",highlight=1)
					dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile,highlight=1)
				
				ignoremodules = False
			return


		def procOffsetPATTERN(args):
			egg = """"
			if ""?"" in args and args[""?""] != """":
				try:
					egg = args[""?""]
				except:
					egg = """"
			if egg == """":
				dbg.log(""Please enter a valid target"",highlight=1)
			else:
				findOffsetInPattern(egg,-1,args)
			return
		
		# ----- Comparing file output ----- #
		def procFileCOMPARE(args):
			modulecriteria={}
			criteria={}
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			allfiles=[]
			tomatch=""""
			checkstrict=True
			rangeval = 0
			fast = False
			if ""ptronly"" in args or ""ptrsonly"" in args:
				fast = True
			if ""f"" in args:
				if args[""f""] != """":
					rawfilenames=args[""f""].replace('""',"""")
					allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
					dbg.log(""[+] Number of files to be examined : %d "" % len(allfiles))
			if ""range"" in args:
				if not type(args[""range""]).__name__.lower() == ""bool"":
					strrange = args[""range""].lower()
					if strrange.startswith(""0x"") and len(strrange) > 2 :
						rangeval = int(strrange,16)
					else:
						try:
							rangeval = int(args[""range""])
						except:
							rangeval = 0
					if rangeval > 0:
						dbg.log(""[+] Find overlap using pointer +/- range, value %d"" % rangeval)
						dbg.log(""    Note : this will significantly slow down the comparison process !"")
				else:
					dbg.log(""Please provide a numeric value ^(> 0) with option -range"",highlight=1)
					return
			else:
				if ""contains"" in args:
					if type(args[""contains""]).__name__.lower() == ""str"":
						tomatch = args[""contains""].replace(""'"","""").replace('""',"""")
				if ""nostrict"" in args:
					if type(args[""nostrict""]).__name__.lower() == ""bool"":
						checkstrict = not args[""nostrict""]
						dbg.log(""[+] Instructions must match in all files ? %s"" % checkstrict)
			# maybe one of the arguments is a folder
			callfiles = allfiles
			allfiles = []
			for tfile in callfiles:
				if os.path.isdir(tfile):
					# folder, get all files from this folder
					for root,dirs,files in os.walk(tfile):
						for dfile in files:
							allfiles.append(os.path.join(root,dfile))
				else:
					allfiles.append(tfile)
			if len(allfiles) > 1:
				findFILECOMPARISON(modulecriteria,criteria,allfiles,tomatch,checkstrict,rangeval,fast)
			else:
				dbg.log(""Please specify at least 2 filenames to compare"",highlight=1)

		# ----- Find bytes in memory ----- #
		def procFind(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			base = 0
			offset = 0
			top  = TOP_USERLAND
			consecutive = False
			ftype = """"
			
			level = 0
			offsetlevel = 0			
			
			if not ""a"" in args:
				args[""a""] = ""*""

			ptronly = False

			if ""ptronly"" in args or ""ptrsonly"" in args:
				ptronly = True	
			
			#search for all pointers by default
			if not ""x"" in args:
				args[""x""] = ""*""
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			if criteria[""accesslevel""] == """":
				return
			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			if ""unicode"" in args:
				criteria[""unic""] = True

			if ""b"" in args:
				try:
					base = int(args[""b""],16)
				except:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return
			if ""t"" in args:
				try:
					top = int(args[""t""],16)
				except:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
			if ""offset"" in args:
				if not args[""offset""].__class__.__name__ == ""bool"":
					if ""0x"" in args[""offset""].lower():
						try:
							offset = 0 - int(args[""offset""],16)
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return
					else:	
						try:
							offset = 0 - int(args[""offset""])
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return	
				else:
					dbg.log(""invalid offset value"",highlight=1)
					return
					
			if ""level"" in args:
				try:
					level = int(args[""level""])
				except:
					dbg.log(""invalid level value"",highlight=1)
					return

			if ""offsetlevel"" in args:
				try:
					offsetlevel = int(args[""offsetlevel""])
				except:
					dbg.log(""invalid offsetlevel value"",highlight=1)
					return						
					
			if ""c"" in args:
				dbg.log(""    - Skipping consecutive pointers, showing size instead"")			
				consecutive = True
				
			if ""type"" in args:
				if not args[""type""] in [""bin"",""asc"",""ptr"",""instr"",""file""]:
					dbg.log(""Invalid search type : %s"" % args[""type""], highlight=1)
					return
				ftype = args[""type""] 
				if ftype == ""file"":
					filename = args[""s""].replace('""',"""").replace(""'"","""")
					#see if we can read the file
					if not os.path.isfile(filename):
						dbg.log(""Unable to find/read file %s"" % filename,highlight=1)
						return
			rangep2p = 0

			
			if ""p2p"" in args or level > 0:
				dbg.log(""    - Looking for pointers to pointers"")
				criteria[""p2p""] = True
				if ""r"" in args:	
					try:
						rangep2p = int(args[""r""])
					except:
						pass
					if rangep2p > 0:
						dbg.log(""    - Will search for close pointers (%d bytes backwards)"" % rangep2p)
				if ""p2p"" in args:
					level = 1
			
			
			if level > 0:
				dbg.log(""    - Recursive levels : %d"" % level)
			

			allpointers = findPattern(modulecriteria,criteria,pattern,ftype,base,top,consecutive,rangep2p,level,offset,offsetlevel)
				
			logfile = MnLog(""find.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog,{},ptronly)
			return
			
			
		# ---- Find instructions, wildcard search ----- #
		def procFindWild(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			patterntype = """"
			base = 0
			top  = TOP_USERLAND
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			patterntypes = [""bin"",""str""]
			if ""type"" in args:
				if type(args[""type""]).__name__.lower() != ""bool"":
					if args[""type""] in patterntypes:
						patterntype = args[""type""]
					else:
						dbg.log(""-type argument only takes one of these values: %s"" % patterntypes,highlight=1)
						return
				else:
					dbg.log(""Please specify a valid value for -type. Valid values are %s"" % patterntypes,highlight=1)
					return


			if patterntype == """":
				if ""\\x"" in pattern:
					patterntype = ""bin""
				else:
					patterntype = ""str""
			
			if ""b"" in args:
				base,addyok = getAddyArg(args[""b""])
				if not addyok:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return

			if ""t"" in args:
				top,addyok = getAddyArg(args[""t""])
				if not addyok:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
					
			if ""depth"" in args:
				try:
					criteria[""depth""] = int(args[""depth""])
				except:
					dbg.log(""invalid depth value"",highlight=1)
					return	

			if ""all"" in args:
				criteria[""all""] = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					dbg.log(""invalid distance value(s)"",highlight=1)
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0	
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0	
			
				if maxdistance < mindistance:
					tmp = maxdistance
					maxdistance = mindistance
					mindistance = tmp
				
				criteria[""mindistance""] = mindistance
				criteria[""maxdistance""] = maxdistance
						
			allpointers = findPatternWild(modulecriteria,criteria,pattern,base,top,patterntype)
				
			logfile = MnLog(""findwild.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog)		
			return
	
			
		# ----- assemble: assemble instructions to opcodes ----- #
		def procAssemble(args):
			opcodes = """"
			encoder = """"
			
			if not 's' in args:
				dbg.log(""Mandatory argument -s <opcodes> missing"", highlight=1)
				return
			opcodes = args['s']
			
			if 'e' in args:
				# TODO: implement encoder support
				dbg.log(""Encoder support not yet implemented"", highlight=1)
				return
				encoder = args['e'].lowercase()
				if encoder not in [""ascii""]:
					dbg.log(""Invalid encoder : %s"" % encoder, highlight=1)
					return
			
			assemble(opcodes,encoder)
			
		# ----- info: show information about an address ----- #
		def procInfo(args):
			if not ""a"" in args:
				dbg.log(""Missing mandatory argument -a"", highlight=1)
				return
			
			address,addyok = getAddyArg(args[""a""])
			if not addyok:
				dbg.log(""%s is an invalid address"" % args[""a""], highlight=1)
				return
			
			ptr = MnPointer(address)
			modname = ptr.belongsTo()
			modinfo = None
			if modname != """":
				modinfo = MnModule(modname)
			rebase = """"
			rva=0
			if modinfo :
				rva = address - modinfo.moduleBase
			procFlags(args)
			dbg.log("""")			
			dbg.log(""[+] Information about address 0x%s"" % toHex(address))
			dbg.log(""    %s"" % ptr.__str__())
			thepage = dbg.getMemoryPageByAddress(address)
			dbg.log(""    Address is part of page 0x%08x - 0x%08x"" % (thepage.getBaseAddress(),thepage.getBaseAddress()+thepage.getSize()))
			section = """"
			try:
				section = thepage.getSection()
			except:
				section = """"
			if section != """":
				dbg.log(""    Section : %s"" % section)
			
			if ptr.isOnStack():
				stacks = getStacks()
				stackref = """"
				for tid in stacks:
					currstack = stacks[tid]
					if currstack[0] <= address and address <= currstack[1]:
						stackref = "" (Thread 0x%08x, Stack Base : 0x%08x, Stack Top : 0x%08x)"" % (tid,currstack[0],currstack[1])
						break
				dbg.log(""    This address is in a stack segment %s"" % stackref)
			if modinfo:
				dbg.log(""    Address is part of a module:"")
				dbg.log(""    %s"" % modinfo.__str__())
				if rva != 0:
					dbg.log(""    Offset from module base: 0x%x"" % rva)
					if modinfo:
						eatlist = modinfo.getEAT()
						if address in eatlist:
							dbg.log(""    Address is start of function '%s' in %s"" % (eatlist[address],modname))
						else:
							iatlist = modinfo.getIAT()
							if address in iatlist:
								iatentry = iatlist[address]
								dbg.log(""    Address is part of IAT, and contains pointer to '%s'"" % iatentry)				
			else:
				output = """"
				if ptr.isInHeap():
					dbg.log(""    This address resides in the heap"")
					dbg.log("""")
					ptr.showHeapBlockInfo()
				else:
					dbg.log(""    Module: None"")					
			try:
				dbg.log("""")
				dbg.log(""[+] Disassembly:"")
				op = dbg.disasm(address)
				opstring=getDisasmInstruction(op)
				dbg.log(""    Instruction at %s : %s"" % (toHex(address),opstring))
			except:
				pass
			if __DEBUGGERAPP__ == ""WinDBG"":
				dbg.log("""")
				dbg.log(""Output of !address 0x%08x:"" % address)
				output = dbg.nativeCommand(""!address 0x%08","for (example_instr, example_op) in [('mov eax,0x41004300', '\\xb8\\x00\\x43\\x00\\x41'), ('mov ebx,0x4100af00', '\\xbb\\x00\\xaf\\x00\\x41'), ('mov ecx,0x41004300', '\\xb9\\x00\\x43\\x00\\x41'), ('mov edx,0x41004300', '\\xba\\x00\\x43\\x00\\x41')]:
    for i in range(0, 256):
        padding = ''
        if i < 16:
            padding = '0'
        new_instr = example_instr[:14] + padding + hex(i)[2:] + example_instr[16:]
        new_op = example_op[:10] + padding + hex(i)[2:] + example_op[12:]
        ass_operation[new_instr] = new_op","for (example_instr, example_op) in [('mov eax,0x41004300', '\\xb8\\x00\\x43\\x00\\x41'), ('mov ebx,0x4100af00', '\\xbb\\x00\\xaf\\x00\\x41'), ('mov ecx,0x41004300', '\\xb9\\x00\\x43\\x00\\x41'), ('mov edx,0x41004300', '\\xba\\x00\\x43\\x00\\x41')]:
    (example_op_12, *example_op_10, __) = example_op
    (example_instr_0, example_instr_1, example_instr_2, example_instr_3, example_instr_4, example_instr_5, example_instr_6, example_instr_7, example_instr_8, example_instr_9, example_instr_10, example_instr_11, example_instr_12, example_instr_13, example_instr_14, *example_instr_16_rexample_instrmaining) = example_instr
    for i in range(0, 256):
        padding = ''
        if i < 16:
            padding = '0'
        new_instr = example_instr[:14] + padding + hex(i)[2:] + example_instr[16:]
        new_op = example_op[:10] + padding + hex(i)[2:] + example_op[12:]
        ass_operation[new_instr] = new_op",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4, e_5, e_6, e_7, e_8, e_9, e_10, e_11, e_12, e_13, e_14, *e_16_remaining = e
variable mapping:
e_16[:]: e[16:]
e_14[:]: e[:14]***************
Answer: Yes
Iterable Unpacking: e_12, *e_10, _ = e
variable mapping:
e_12: e[12:]
e_10: e[:10]",,,,,,,
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/cisco.py,https://github.com/google/capirca/tree/master/capirca/lib/cisco.py,TermStandard,__str__$106,"def __str__(self):
    # Verify platform specific terms. Skip whole term if platform does not
    # match.
    if self.term.platform:
      if self.platform not in self.term.platform:
        return ''
    if self.term.platform_exclude:
      if self.platform in self.term.platform_exclude:
        return ''

    ret_str = []

    # Term verbatim output - this will skip over normal term creation
    # code by returning early.  Warnings provided in policy.py.
    if self.term.verbatim:
      for next_verbatim in self.term.verbatim:
        if next_verbatim[0] == self.platform:
          ret_str.append(str(next_verbatim[1]))
        return '\n'.join(ret_str)

    v4_addresses = [x for x in self.term.address if
                    not isinstance(x, nacaddr.IPv6)]
    if self.filter_name.isdigit():
      if self.verbose:
        ret_str.append('access-list %s remark %s' % (self.filter_name,
                                                     self.term.name))
        comments = aclgenerator.WrapWords(self.term.comment,
                                          _COMMENT_MAX_WIDTH)
        for comment in comments:
          ret_str.append('access-list %s remark %s' % (self.filter_name,
                                                       comment))

      action = _ACTION_TABLE.get(str(self.term.action[0]))
      if v4_addresses:
        for addr in v4_addresses:
          if addr.prefixlen == 32:
            ret_str.append('access-list %s %s %s%s%s' % (self.filter_name,
                                                         action,
                                                         addr.network_address,
                                                         self.logstring,
                                                         self.dscpstring))
          else:
            ret_str.append('access-list %s %s %s %s%s%s' % (
                self.filter_name,
                action,
                addr.network_address,
                addr.hostmask,
                self.logstring,
                self.dscpstring))
      else:
        ret_str.append('access-list %s %s %s%s%s' % (self.filter_name,
                                                     action,
                                                     'any',
                                                     self.logstring,
                                                     self.dscpstring))
    else:
      if self.verbose:
        ret_str.append(' remark ' + self.term.name)
        comments = aclgenerator.WrapWords(self.term.comment,
                                          _COMMENT_MAX_WIDTH)
        if comments and comments[0]:
          for comment in comments:
            ret_str.append(' remark ' + str(comment))

      action = _ACTION_TABLE.get(str(self.term.action[0]))
      if v4_addresses:
        for addr in v4_addresses:
          if addr.prefixlen == 32:
            ret_str.append(' %s host %s%s%s' % (action,
                                                addr.network_address,
                                                self.logstring,
                                                self.dscpstring))
          elif self.platform == 'arista':
            ret_str.append(' %s %s/%s%s%s' % (action,
                                              addr.network_address,
                                              addr.prefixlen,
                                              self.logstring,
                                              self.dscpstring))
          else:
            ret_str.append(' %s %s %s%s%s' % (action,
                                              addr.network_address,
                                              addr.hostmask,
                                              self.logstring,
                                              self.dscpstring))
      else:
        ret_str.append(' %s %s%s%s' % (action,
                                       'any',
                                       self.logstring,
                                       self.dscpstring))

    return '\n'.join(ret_str)","for next_verbatim in self.term.verbatim:
    if next_verbatim[0] == self.platform:
        ret_str.append(str(next_verbatim[1]))
    return '\n'.join(ret_str)","for next_verbatim in self.term.verbatim:
    (next_verbatim_0, next_verbatim_1, *_) = next_verbatim
    if next_verbatim[0] == self.platform:
        ret_str.append(str(next_verbatim[1]))
    return '\n'.join(ret_str)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
VideoSuperResolution,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VideoSuperResolution/VSR/Backend/TF/Framework/Trainer.py,https://github.com/LoSealL/VideoSuperResolution/tree/master/VSR/Backend/TF/Framework/Trainer.py,,_ensemble_reduce_mean$47,"def _ensemble_reduce_mean(outputs):
  results = []
  for i in outputs:
    outputs_ensemble = [
      i[0],
      np.rot90(i[1], 3, axes=[-3, -2]),
      np.rot90(i[2], 2, axes=[-3, -2]),
      np.rot90(i[3], 1, axes=[-3, -2]),
      np.flip(i[4], axis=-2),
      np.flip(np.rot90(i[5], 3, axes=[-3, -2]), axis=-2),
      np.flip(np.rot90(i[6], 2, axes=[-3, -2]), axis=-2),
      np.flip(np.rot90(i[7], 1, axes=[-3, -2]), axis=-2),
    ]
    results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))
  return results","for i in outputs:
    outputs_ensemble = [i[0], np.rot90(i[1], 3, axes=[-3, -2]), np.rot90(i[2], 2, axes=[-3, -2]), np.rot90(i[3], 1, axes=[-3, -2]), np.flip(i[4], axis=-2), np.flip(np.rot90(i[5], 3, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[6], 2, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[7], 1, axes=[-3, -2]), axis=-2)]
    results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))","for i in outputs:
    (i_0, i_1, i_2, i_3, i_4, i_5, i_6, i_7, *_) = i
    outputs_ensemble = [i[0], np.rot90(i[1], 3, axes=[-3, -2]), np.rot90(i[2], 2, axes=[-3, -2]), np.rot90(i[3], 1, axes=[-3, -2]), np.flip(i[4], axis=-2), np.flip(np.rot90(i[5], 3, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[6], 2, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[7], 1, axes=[-3, -2]), axis=-2)]
    results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4, e_5, e_6, e_7 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]
e_5: e[5]
e_6: e[6]
e_7: e[7]",,,,,,,
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers/va/csv_bills.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers/va/csv_bills.py,VaCSVBillScraper,load_amendments$88,"def load_amendments(self):
        resp = self.get(self._url_base + ""Amendments.csv"").text
        reader = csv.reader(resp.splitlines(), delimiter="","")

        # ['BILL_NUMBER', 'TXT_DOCID']
        for row in reader:
            self._amendments[row[0].strip()].append(
                {""bill_number"": row[0].strip(), ""txt_docid"": row[1].strip()}
            )
        self.warning(""Total Amendments Loaded: "" + str(len(self._amendments)))","for row in reader:
    self._amendments[row[0].strip()].append({'bill_number': row[0].strip(), 'txt_docid': row[1].strip()})","for row in reader:
    (row_0, row_1, *_) = row
    self._amendments[row[0].strip()].append({'bill_number': row[0].strip(), 'txt_docid': row[1].strip()})","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
meld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meld/meld/matchers/merge.py,https://github.com/GNOME/meld/tree/master/meld/matchers/merge.py,AutoMergeDiffer,_auto_merge$32,"def _auto_merge(self, using, texts):
        for out0, out1 in super()._auto_merge(using, texts):
            if self.auto_merge and out0[0] == 'conflict':
                # we will try to resolve more complex conflicts automatically
                # here... if possible
                l0, h0, l1, h1, l2, h2 = (
                    out0[3], out0[4], out0[1], out0[2], out1[3], out1[4])
                len0 = h0 - l0
                len1 = h1 - l1
                len2 = h2 - l2
                if (len0 > 0 and len2 > 0) and (
                        len0 == len1 or len2 == len1 or len1 == 0):
                    matcher = self._matcher(
                        None, texts[0][l0:h0], texts[2][l2:h2])
                    for chunk in matcher.get_opcodes():
                        s1 = l1
                        e1 = l1
                        if len0 == len1:
                            s1 += chunk[1]
                            e1 += chunk[2]
                        elif len2 == len1:
                            s1 += chunk[3]
                            e1 += chunk[4]
                        out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                        out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                        if chunk[0] == 'equal':
                            out0 = ('replace',) + out0_bounds
                            out1 = ('replace',) + out1_bounds
                            yield out0, out1
                        else:
                            out0 = ('conflict',) + out0_bounds
                            out1 = ('conflict',) + out1_bounds
                            yield out0, out1
                    return
                # elif len0 > 0 and len2 > 0:
                #     # this logic will resolve more conflicts automatically,
                #     # but unresolved conflicts may sometimes look confusing
                #     # as the line numbers in ancestor file will be
                #     # interpolated and may not reflect the actual changes
                #     matcher = self._matcher(
                #         None, texts[0][l0:h0], texts[2][l2:h2])
                #     if len0 > len2:
                #         maxindex = 1
                #         maxlen = len0
                #     else:
                #         maxindex = 3
                #         maxlen = len2
                #     for chunk in matcher.get_opcodes():
                #         new_start = l1 + len1 * chunk[maxindex] / maxlen
                #         new_end = l1 + len1 * chunk[maxindex + 1] / maxlen
                #         out0_bounds = (
                #             new_start, new_end, l0 + chunk[1], l0 + chunk[2])
                #         out1_bounds = (
                #             new_start, new_end, l2 + chunk[3], l2 + chunk[4])
                #         if chunk[0] == 'equal':
                #             out0 = ('replace',) + out0_bounds
                #             out1 = ('replace',) + out1_bounds
                #             yield out0, out1
                #         else:
                #             out0 = ('conflict',) + out0_bounds
                #             out1 = ('conflict',) + out1_bounds
                #             yield out0, out1
                #     return
                else:
                    # some tricks to resolve even more conflicts automatically
                    # unfortunately the resulting chunks cannot be used to
                    # highlight changes but hey, they are good enough to merge
                    # the resulting file :)
                    chunktype = using[0][0][0]
                    for chunkarr in using:
                        for chunk in chunkarr:
                            if chunk[0] != chunktype:
                                chunktype = None
                                break
                        if not chunktype:
                            break
                    if chunktype == 'delete':
                        # delete + delete -> split into delete/conflict
                        seq0 = seq1 = None
                        while 1:
                            if seq0 is None:
                                try:
                                    seq0 = using[0].pop(0)
                                    i0 = seq0[1]
                                    end0 = seq0[4]
                                except IndexError:
                                    break
                            if seq1 is None:
                                try:
                                    seq1 = using[1].pop(0)
                                    i1 = seq1[1]
                                    end1 = seq1[4]
                                except IndexError:
                                    break
                            highstart = max(i0, i1)
                            if i0 != i1:
                                out0 = (
                                    'conflict', i0 - highstart + i1, highstart,
                                    seq0[3] - highstart + i1, seq0[3]
                                )
                                out1 = (
                                    'conflict', i1 - highstart + i0, highstart,
                                    seq1[3] - highstart + i0, seq1[3]
                                )
                                yield out0, out1
                            lowend = min(seq0[2], seq1[2])
                            if highstart != lowend:
                                out0 = (
                                    'delete', highstart, lowend,
                                    seq0[3], seq0[4]
                                )
                                out1 = (
                                    'delete', highstart, lowend,
                                    seq1[3], seq1[4]
                                )
                                yield out0, out1
                            i0 = i1 = lowend
                            if lowend == seq0[2]:
                                seq0 = None
                            if lowend == seq1[2]:
                                seq1 = None

                        if seq0:
                            out0 = (
                                'conflict', i0, seq0[2],
                                seq0[3], seq0[4]
                            )
                            out1 = (
                                'conflict', i0, seq0[2],
                                end1, end1 + seq0[2] - i0
                            )
                            yield out0, out1
                        elif seq1:
                            out0 = (
                                'conflict', i1, seq1[2],
                                end0, end0 + seq1[2] - i1
                            )
                            out1 = (
                                'conflict', i1,
                                seq1[2], seq1[3], seq1[4]
                            )
                            yield out0, out1
                        return
            yield out0, out1","for (out0, out1) in super()._auto_merge(using, texts):
    if self.auto_merge and out0[0] == 'conflict':
        (l0, h0, l1, h1, l2, h2) = (out0[3], out0[4], out0[1], out0[2], out1[3], out1[4])
        len0 = h0 - l0
        len1 = h1 - l1
        len2 = h2 - l2
        if (len0 > 0 and len2 > 0) and (len0 == len1 or len2 == len1 or len1 == 0):
            matcher = self._matcher(None, texts[0][l0:h0], texts[2][l2:h2])
            for chunk in matcher.get_opcodes():
                s1 = l1
                e1 = l1
                if len0 == len1:
                    s1 += chunk[1]
                    e1 += chunk[2]
                elif len2 == len1:
                    s1 += chunk[3]
                    e1 += chunk[4]
                out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                if chunk[0] == 'equal':
                    out0 = ('replace',) + out0_bounds
                    out1 = ('replace',) + out1_bounds
                    yield (out0, out1)
                else:
                    out0 = ('conflict',) + out0_bounds
                    out1 = ('conflict',) + out1_bounds
                    yield (out0, out1)
            return
        else:
            chunktype = using[0][0][0]
            for chunkarr in using:
                for chunk in chunkarr:
                    if chunk[0] != chunktype:
                        chunktype = None
                        break
                if not chunktype:
                    break
            if chunktype == 'delete':
                seq0 = seq1 = None
                while 1:
                    if seq0 is None:
                        try:
                            seq0 = using[0].pop(0)
                            i0 = seq0[1]
                            end0 = seq0[4]
                        except IndexError:
                            break
                    if seq1 is None:
                        try:
                            seq1 = using[1].pop(0)
                            i1 = seq1[1]
                            end1 = seq1[4]
                        except IndexError:
                            break
                    highstart = max(i0, i1)
                    if i0 != i1:
                        out0 = ('conflict', i0 - highstart + i1, highstart, seq0[3] - highstart + i1, seq0[3])
                        out1 = ('conflict', i1 - highstart + i0, highstart, seq1[3] - highstart + i0, seq1[3])
                        yield (out0, out1)
                    lowend = min(seq0[2], seq1[2])
                    if highstart != lowend:
                        out0 = ('delete', highstart, lowend, seq0[3], seq0[4])
                        out1 = ('delete', highstart, lowend, seq1[3], seq1[4])
                        yield (out0, out1)
                    i0 = i1 = lowend
                    if lowend == seq0[2]:
                        seq0 = None
                    if lowend == seq1[2]:
                        seq1 = None
                if seq0:
                    out0 = ('conflict', i0, seq0[2], seq0[3], seq0[4])
                    out1 = ('conflict', i0, seq0[2], end1, end1 + seq0[2] - i0)
                    yield (out0, out1)
                elif seq1:
                    out0 = ('conflict', i1, seq1[2], end0, end0 + seq1[2] - i1)
                    out1 = ('conflict', i1, seq1[2], seq1[3], seq1[4])
                    yield (out0, out1)
                return
    yield (out0, out1)","for (out0, out1) in super()._auto_merge(using, texts):
    (_, _, _, out1_3, out1_4, *_) = out1
    (out0_0, out0_1, out0_2, out0_3, out0_4, *_) = out0
    if self.auto_merge and out0[0] == 'conflict':
        (l0, h0, l1, h1, l2, h2) = (out0[3], out0[4], out0[1], out0[2], out1[3], out1[4])
        len0 = h0 - l0
        len1 = h1 - l1
        len2 = h2 - l2
        if (len0 > 0 and len2 > 0) and (len0 == len1 or len2 == len1 or len1 == 0):
            matcher = self._matcher(None, texts[0][l0:h0], texts[2][l2:h2])
            for chunk in matcher.get_opcodes():
                s1 = l1
                e1 = l1
                if len0 == len1:
                    s1 += chunk[1]
                    e1 += chunk[2]
                elif len2 == len1:
                    s1 += chunk[3]
                    e1 += chunk[4]
                out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                if chunk[0] == 'equal':
                    out0 = ('replace',) + out0_bounds
                    out1 = ('replace',) + out1_bounds
                    yield (out0, out1)
                else:
                    out0 = ('conflict',) + out0_bounds
                    out1 = ('conflict',) + out1_bounds
                    yield (out0, out1)
            return
        else:
            chunktype = using[0][0][0]
            for chunkarr in using:
                for chunk in chunkarr:
                    if chunk[0] != chunktype:
                        chunktype = None
                        break
                if not chunktype:
                    break
            if chunktype == 'delete':
                seq0 = seq1 = None
                while 1:
                    if seq0 is None:
                        try:
                            seq0 = using[0].pop(0)
                            i0 = seq0[1]
                            end0 = seq0[4]
                        except IndexError:
                            break
                    if seq1 is None:
                        try:
                            seq1 = using[1].pop(0)
                            i1 = seq1[1]
                            end1 = seq1[4]
                        except IndexError:
                            break
                    highstart = max(i0, i1)
                    if i0 != i1:
                        out0 = ('conflict', i0 - highstart + i1, highstart, seq0[3] - highstart + i1, seq0[3])
                        out1 = ('conflict', i1 - highstart + i0, highstart, seq1[3] - highstart + i0, seq1[3])
                        yield (out0, out1)
                    lowend = min(seq0[2], seq1[2])
                    if highstart != lowend:
                        out0 = ('delete', highstart, lowend, seq0[3], seq0[4])
                        out1 = ('delete', highstart, lowend, seq1[3], seq1[4])
                        yield (out0, out1)
                    i0 = i1 = lowend
                    if lowend == seq0[2]:
                        seq0 = None
                    if lowend == seq1[2]:
                        seq1 = None
                if seq0:
                    out0 = ('conflict', i0, seq0[2], seq0[3], seq0[4])
                    out1 = ('conflict', i0, seq0[2], end1, end1 + seq0[2] - i0)
                    yield (out0, out1)
                elif seq1:
                    out0 = ('conflict', i1, seq1[2], end0, end0 + seq1[2] - i1)
                    out1 = ('conflict', i1, seq1[2], seq1[3], seq1[4])
                    yield (out0, out1)
                return
    yield (out0, out1)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]***************
Answer: Yes
Iterable Unpacking: _, _, _, e_3, e_4 = e
variable mapping:
e_3: e[3]
e_4: e[4]",,,,,,,
meld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meld/meld/matchers/merge.py,https://github.com/GNOME/meld/tree/master/meld/matchers/merge.py,AutoMergeDiffer,_auto_merge$32,"def _auto_merge(self, using, texts):
        for out0, out1 in super()._auto_merge(using, texts):
            if self.auto_merge and out0[0] == 'conflict':
                # we will try to resolve more complex conflicts automatically
                # here... if possible
                l0, h0, l1, h1, l2, h2 = (
                    out0[3], out0[4], out0[1], out0[2], out1[3], out1[4])
                len0 = h0 - l0
                len1 = h1 - l1
                len2 = h2 - l2
                if (len0 > 0 and len2 > 0) and (
                        len0 == len1 or len2 == len1 or len1 == 0):
                    matcher = self._matcher(
                        None, texts[0][l0:h0], texts[2][l2:h2])
                    for chunk in matcher.get_opcodes():
                        s1 = l1
                        e1 = l1
                        if len0 == len1:
                            s1 += chunk[1]
                            e1 += chunk[2]
                        elif len2 == len1:
                            s1 += chunk[3]
                            e1 += chunk[4]
                        out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                        out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                        if chunk[0] == 'equal':
                            out0 = ('replace',) + out0_bounds
                            out1 = ('replace',) + out1_bounds
                            yield out0, out1
                        else:
                            out0 = ('conflict',) + out0_bounds
                            out1 = ('conflict',) + out1_bounds
                            yield out0, out1
                    return
                # elif len0 > 0 and len2 > 0:
                #     # this logic will resolve more conflicts automatically,
                #     # but unresolved conflicts may sometimes look confusing
                #     # as the line numbers in ancestor file will be
                #     # interpolated and may not reflect the actual changes
                #     matcher = self._matcher(
                #         None, texts[0][l0:h0], texts[2][l2:h2])
                #     if len0 > len2:
                #         maxindex = 1
                #         maxlen = len0
                #     else:
                #         maxindex = 3
                #         maxlen = len2
                #     for chunk in matcher.get_opcodes():
                #         new_start = l1 + len1 * chunk[maxindex] / maxlen
                #         new_end = l1 + len1 * chunk[maxindex + 1] / maxlen
                #         out0_bounds = (
                #             new_start, new_end, l0 + chunk[1], l0 + chunk[2])
                #         out1_bounds = (
                #             new_start, new_end, l2 + chunk[3], l2 + chunk[4])
                #         if chunk[0] == 'equal':
                #             out0 = ('replace',) + out0_bounds
                #             out1 = ('replace',) + out1_bounds
                #             yield out0, out1
                #         else:
                #             out0 = ('conflict',) + out0_bounds
                #             out1 = ('conflict',) + out1_bounds
                #             yield out0, out1
                #     return
                else:
                    # some tricks to resolve even more conflicts automatically
                    # unfortunately the resulting chunks cannot be used to
                    # highlight changes but hey, they are good enough to merge
                    # the resulting file :)
                    chunktype = using[0][0][0]
                    for chunkarr in using:
                        for chunk in chunkarr:
                            if chunk[0] != chunktype:
                                chunktype = None
                                break
                        if not chunktype:
                            break
                    if chunktype == 'delete':
                        # delete + delete -> split into delete/conflict
                        seq0 = seq1 = None
                        while 1:
                            if seq0 is None:
                                try:
                                    seq0 = using[0].pop(0)
                                    i0 = seq0[1]
                                    end0 = seq0[4]
                                except IndexError:
                                    break
                            if seq1 is None:
                                try:
                                    seq1 = using[1].pop(0)
                                    i1 = seq1[1]
                                    end1 = seq1[4]
                                except IndexError:
                                    break
                            highstart = max(i0, i1)
                            if i0 != i1:
                                out0 = (
                                    'conflict', i0 - highstart + i1, highstart,
                                    seq0[3] - highstart + i1, seq0[3]
                                )
                                out1 = (
                                    'conflict', i1 - highstart + i0, highstart,
                                    seq1[3] - highstart + i0, seq1[3]
                                )
                                yield out0, out1
                            lowend = min(seq0[2], seq1[2])
                            if highstart != lowend:
                                out0 = (
                                    'delete', highstart, lowend,
                                    seq0[3], seq0[4]
                                )
                                out1 = (
                                    'delete', highstart, lowend,
                                    seq1[3], seq1[4]
                                )
                                yield out0, out1
                            i0 = i1 = lowend
                            if lowend == seq0[2]:
                                seq0 = None
                            if lowend == seq1[2]:
                                seq1 = None

                        if seq0:
                            out0 = (
                                'conflict', i0, seq0[2],
                                seq0[3], seq0[4]
                            )
                            out1 = (
                                'conflict', i0, seq0[2],
                                end1, end1 + seq0[2] - i0
                            )
                            yield out0, out1
                        elif seq1:
                            out0 = (
                                'conflict', i1, seq1[2],
                                end0, end0 + seq1[2] - i1
                            )
                            out1 = (
                                'conflict', i1,
                                seq1[2], seq1[3], seq1[4]
                            )
                            yield out0, out1
                        return
            yield out0, out1","for chunk in matcher.get_opcodes():
    s1 = l1
    e1 = l1
    if len0 == len1:
        s1 += chunk[1]
        e1 += chunk[2]
    elif len2 == len1:
        s1 += chunk[3]
        e1 += chunk[4]
    out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
    out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
    if chunk[0] == 'equal':
        out0 = ('replace',) + out0_bounds
        out1 = ('replace',) + out1_bounds
        yield (out0, out1)
    else:
        out0 = ('conflict',) + out0_bounds
        out1 = ('conflict',) + out1_bounds
        yield (out0, out1)","for chunk in matcher.get_opcodes():
    (chunk_0, chunk_1, chunk_2, chunk_3, chunk_4, *_) = chunk
    s1 = l1
    e1 = l1
    if len0 == len1:
        s1 += chunk[1]
        e1 += chunk[2]
    elif len2 == len1:
        s1 += chunk[3]
        e1 += chunk[4]
    out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
    out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
    if chunk[0] == 'equal':
        out0 = ('replace',) + out0_bounds
        out1 = ('replace',) + out1_bounds
        yield (out0, out1)
    else:
        out0 = ('conflict',) + out0_bounds
        out1 = ('conflict',) + out1_bounds
        yield (out0, out1)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,
meld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meld/meld/matchers/merge.py,https://github.com/GNOME/meld/tree/master/meld/matchers/merge.py,AutoMergeDiffer,_auto_merge$32,"def _auto_merge(self, using, texts):
        for out0, out1 in super()._auto_merge(using, texts):
            if self.auto_merge and out0[0] == 'conflict':
                # we will try to resolve more complex conflicts automatically
                # here... if possible
                l0, h0, l1, h1, l2, h2 = (
                    out0[3], out0[4], out0[1], out0[2], out1[3], out1[4])
                len0 = h0 - l0
                len1 = h1 - l1
                len2 = h2 - l2
                if (len0 > 0 and len2 > 0) and (
                        len0 == len1 or len2 == len1 or len1 == 0):
                    matcher = self._matcher(
                        None, texts[0][l0:h0], texts[2][l2:h2])
                    for chunk in matcher.get_opcodes():
                        s1 = l1
                        e1 = l1
                        if len0 == len1:
                            s1 += chunk[1]
                            e1 += chunk[2]
                        elif len2 == len1:
                            s1 += chunk[3]
                            e1 += chunk[4]
                        out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                        out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                        if chunk[0] == 'equal':
                            out0 = ('replace',) + out0_bounds
                            out1 = ('replace',) + out1_bounds
                            yield out0, out1
                        else:
                            out0 = ('conflict',) + out0_bounds
                            out1 = ('conflict',) + out1_bounds
                            yield out0, out1
                    return
                # elif len0 > 0 and len2 > 0:
                #     # this logic will resolve more conflicts automatically,
                #     # but unresolved conflicts may sometimes look confusing
                #     # as the line numbers in ancestor file will be
                #     # interpolated and may not reflect the actual changes
                #     matcher = self._matcher(
                #         None, texts[0][l0:h0], texts[2][l2:h2])
                #     if len0 > len2:
                #         maxindex = 1
                #         maxlen = len0
                #     else:
                #         maxindex = 3
                #         maxlen = len2
                #     for chunk in matcher.get_opcodes():
                #         new_start = l1 + len1 * chunk[maxindex] / maxlen
                #         new_end = l1 + len1 * chunk[maxindex + 1] / maxlen
                #         out0_bounds = (
                #             new_start, new_end, l0 + chunk[1], l0 + chunk[2])
                #         out1_bounds = (
                #             new_start, new_end, l2 + chunk[3], l2 + chunk[4])
                #         if chunk[0] == 'equal':
                #             out0 = ('replace',) + out0_bounds
                #             out1 = ('replace',) + out1_bounds
                #             yield out0, out1
                #         else:
                #             out0 = ('conflict',) + out0_bounds
                #             out1 = ('conflict',) + out1_bounds
                #             yield out0, out1
                #     return
                else:
                    # some tricks to resolve even more conflicts automatically
                    # unfortunately the resulting chunks cannot be used to
                    # highlight changes but hey, they are good enough to merge
                    # the resulting file :)
                    chunktype = using[0][0][0]
                    for chunkarr in using:
                        for chunk in chunkarr:
                            if chunk[0] != chunktype:
                                chunktype = None
                                break
                        if not chunktype:
                            break
                    if chunktype == 'delete':
                        # delete + delete -> split into delete/conflict
                        seq0 = seq1 = None
                        while 1:
                            if seq0 is None:
                                try:
                                    seq0 = using[0].pop(0)
                                    i0 = seq0[1]
                                    end0 = seq0[4]
                                except IndexError:
                                    break
                            if seq1 is None:
                                try:
                                    seq1 = using[1].pop(0)
                                    i1 = seq1[1]
                                    end1 = seq1[4]
                                except IndexError:
                                    break
                            highstart = max(i0, i1)
                            if i0 != i1:
                                out0 = (
                                    'conflict', i0 - highstart + i1, highstart,
                                    seq0[3] - highstart + i1, seq0[3]
                                )
                                out1 = (
                                    'conflict', i1 - highstart + i0, highstart,
                                    seq1[3] - highstart + i0, seq1[3]
                                )
                                yield out0, out1
                            lowend = min(seq0[2], seq1[2])
                            if highstart != lowend:
                                out0 = (
                                    'delete', highstart, lowend,
                                    seq0[3], seq0[4]
                                )
                                out1 = (
                                    'delete', highstart, lowend,
                                    seq1[3], seq1[4]
                                )
                                yield out0, out1
                            i0 = i1 = lowend
                            if lowend == seq0[2]:
                                seq0 = None
                            if lowend == seq1[2]:
                                seq1 = None

                        if seq0:
                            out0 = (
                                'conflict', i0, seq0[2],
                                seq0[3], seq0[4]
                            )
                            out1 = (
                                'conflict', i0, seq0[2],
                                end1, end1 + seq0[2] - i0
                            )
                            yield out0, out1
                        elif seq1:
                            out0 = (
                                'conflict', i1, seq1[2],
                                end0, end0 + seq1[2] - i1
                            )
                            out1 = (
                                'conflict', i1,
                                seq1[2], seq1[3], seq1[4]
                            )
                            yield out0, out1
                        return
            yield out0, out1","for chunk in chunkarr:
    if chunk[0] != chunktype:
        chunktype = None
        break","for chunk in chunkarr:
    (chunk_0, *chunk_rchunkmaining) = chunk
    if chunk[0] != chunktype:
        chunktype = None
        break","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/incubate/fleet/tests/ctr_dataset_reader.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/incubate/fleet/tests/ctr_dataset_reader.py,,load_lr_input_record$54,"def load_lr_input_record(sent):
    res = []
    for _ in [x.split(':') for x in sent.split()]:
        res.append(int(_[0]))
    return res","for _ in [x.split(':') for x in sent.split()]:
    res.append(int(_[0]))","for _ in [x.split(':') for x in sent.split()]:
    (__0, *__r_maining) = _
    res.append(int(_[0]))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
spot-sdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spot-sdk/docs/python/fetch_tutorial/files/network_compute_server.py,https://github.com/boston-dynamics/spot-sdk/tree/master/docs/python/fetch_tutorial/files/network_compute_server.py,,main$256,"def main(argv):
    default_port = '50051'

    parser = argparse.ArgumentParser()
    parser.add_argument('-m', '--model', help='[MODEL_DIR] [LABELS_FILE.pbtxt]: Path to a model\'s directory and path to its labels .pbtxt file', action='append', nargs=2, required=True)
    parser.add_argument('-p', '--port', help='Server\'s port number, default: ' + default_port,
                        default=default_port)
    parser.add_argument('-d', '--no-debug', help='Disable writing debug images.', action='store_true')
    parser.add_argument('-n', '--name', help='Service name', default='fetch-server')
    bosdyn.client.util.add_base_arguments(parser)

    options = parser.parse_args(argv)

    print(options.model)

    for model in options.model:
        if not os.path.isdir(model[0]):
            print('Error: model directory (' + model[0] + ') not found or is not a directory.')
            sys.exit(1)

    # Perform registration.
    register_with_robot(options)

    # Thread-safe queues for communication between the GRPC endpoint and the ML thread.
    request_queue = queue.Queue()
    response_queue = queue.Queue()

    # Start server thread
    thread = threading.Thread(target=process_thread, args=([options, request_queue, response_queue]))
    thread.start()

    # Set up GRPC endpoint
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    network_compute_bridge_service_pb2_grpc.add_NetworkComputeBridgeWorkerServicer_to_server(
        NetworkComputeBridgeWorkerServicer(request_queue, response_queue), server)
    server.add_insecure_port('[::]:' + options.port)
    server.start()

    print('Running...')
    thread.join()

    return True","for model in options.model:
    if not os.path.isdir(model[0]):
        print('Error: model directory (' + model[0] + ') not found or is not a directory.')
        sys.exit(1)","for model in options.model:
    (model_0, *model_rmodelmaining) = model
    if not os.path.isdir(model[0]):
        print('Error: model directory (' + model[0] + ') not found or is not a directory.')
        sys.exit(1)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_es.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_es.py,Num2WordsESTest,test_currency_omr$2686,"def test_currency_omr(self):
        for test in TEST_CASES_TO_CURRENCY_OMR:
            self.assertEqual(
                num2words(test[0], lang='es', to='currency', currency='OMR'),
                test[1]
            )","for test in TEST_CASES_TO_CURRENCY_OMR:
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='OMR'), test[1])","for test in TEST_CASES_TO_CURRENCY_OMR:
    (test_0, test_1, *_) = test
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='OMR'), test[1])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pyGAT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyGAT/visualize_graph.py,https://github.com/Diego999/pyGAT/tree/master//visualize_graph.py,,add_nodes$32,"def add_nodes(var):
        if var not in seen:
            if torch.is_tensor(var):
                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')
            elif hasattr(var, 'variable'):
                u = var.variable
                node_name = '%s\n %s' % (param_map.get(id(u)), size_to_str(u.size()))
                dot.node(str(id(var)), node_name, fillcolor='lightblue')
            else:
                dot.node(str(id(var)), str(type(var).__name__))
            seen.add(var)
            if hasattr(var, 'next_functions'):
                for u in var.next_functions:
                    if u[0] is not None:
                        dot.edge(str(id(u[0])), str(id(var)))
                        add_nodes(u[0])
            if hasattr(var, 'saved_tensors'):
                for t in var.saved_tensors:
                    dot.edge(str(id(t)), str(id(var)))
                    add_nodes(t)","for u in var.next_functions:
    if u[0] is not None:
        dot.edge(str(id(u[0])), str(id(var)))
        add_nodes(u[0])","for u in var.next_functions:
    (u_0, *u_rumaining) = u
    if u[0] is not None:
        dot.edge(str(id(u[0])), str(id(var)))
        add_nodes(u[0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
jackdaw,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jackdaw/jackdaw/credentials/credentials.py,https://github.com/skelsec/jackdaw/tree/master/jackdaw/credentials/credentials.py,JackDawCredentials,get_uncracked_hashes$129,"def get_uncracked_hashes(self, hash_type, history):
		self.get_dbsession()
		try:
			if hash_type == 'NT':
				qry = self.dbsession.query(Credential.nt_hash).outerjoin(HashEntry, Credential.nt_hash == HashEntry.nt_hash).filter(Credential.nt_hash != None).distinct(Credential.nt_hash)
			else:
				qry = self.dbsession.query(Credential.lm_hash).outerjoin(HashEntry, Credential.lm_hash == HashEntry.lm_hash).filter(Credential.lm_hash != None).distinct(Credential.lm_hash)
			
			if history == False:
				qry = qry.filter(Credential.history_no == 0)
				
			for some_hash in qry.all():
				yield some_hash[0]
		except Exception as e:
			print(e)
		finally:
			self.dbsession.close()","for some_hash in qry.all():
    yield some_hash[0]","for some_hash in qry.all():
    (some_hash_0, *some_hash_rsome_hashmaining) = some_hash
    yield some_hash[0]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
clusterfuzz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/clusterfuzz/_internal/tests/appengine/libs/query/datastore_query_test.py,https://github.com/google/clusterfuzz/tree/master/src/clusterfuzz/_internal/tests/appengine/libs/query/datastore_query_test.py,QueryMockTest,test_third_page$253,"def test_third_page(self):
    """"""Test getting the third page with more total count.""""""
    query = datastore_query.Query(TestDatastoreModel)
    query.filter_in('tokens', ['a', 'b'])
    query.filter('boolean_value', True)
    query.order('datetime_value', is_desc=True)

    query.fetch_page(page=1, page_size=2, projection=['tokens'], more_limit=4)

    self.assertIsInstance(self.queries[0][-1].filters, ndb.AND)
    six.assertCountEqual(self, [
        ('tokens', '=', 'a'),
        ('boolean_value', '=', True),
    ], [f.__getnewargs__() for f in self.queries[0][-1].filters])

    self.assertIsInstance(self.queries[1][-1].filters, ndb.AND)
    six.assertCountEqual(self, [
        ('tokens', '=', 'b'),
        ('boolean_value', '=', True),
    ], [f.__getnewargs__() for f in self.queries[1][-1].filters])

    self.assertIsInstance(self.queries[2][-1].filters, ndb.OR)

    expected = []
    for item in [f.__getnewargs__() for f in self.queries[2][-1].filters]:
      expected.append((item[0], item[1], repr(item[2])))

    six.assertCountEqual(self, [
        ('__key__', '=',
         '<Key(\'TestDatastoreModel\', 0), project=test-clusterfuzz>'),
        ('__key__', '=',
         '<Key(\'TestDatastoreModel\', 1), project=test-clusterfuzz>'),
    ], expected)","for item in [f.__getnewargs__() for f in self.queries[2][-1].filters]:
    expected.append((item[0], item[1], repr(item[2])))","for item in [f.__getnewargs__() for f in self.queries[2][-1].filters]:
    (item_0, item_1, item_2, *_) = item
    expected.append((item[0], item[1], repr(item[2])))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
joinmarket-clientserver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joinmarket-clientserver/jmbitcoin/jmbitcoin/secp256k1_transaction.py,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/jmbitcoin/jmbitcoin/secp256k1_transaction.py,,mktx$315,"def mktx(ins, outs, version=1, locktime=0):
    """""" Given a list of input tuples (txid(bytes), n(int)),
    and a list of outputs which are dicts with
    keys ""address"" (value should be *str* not CCoinAddress) (
    or alternately ""script"" (for nonstandard outputs, value
    should be CScript)),
    ""value"" (value should be integer satoshis), outputs a
    CMutableTransaction object.
    Tx version and locktime are optionally set, for non-default
    locktimes, inputs are given nSequence as per below comment.
    """"""
    vin = []
    vout = []
    # This does NOT trigger rbf and mimics Core's standard behaviour as of
    # Jan 2019.
    # Tx creators wishing to use rbf will need to set it explicitly outside
    # of this function.
    if locktime != 0:
        sequence = 0xffffffff - 1
    else:
        sequence = 0xffffffff
    for i in ins:
        outpoint = CMutableOutPoint((i[0][::-1]), i[1])
        inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)
        vin.append(inp)
    for o in outs:
        if ""script"" in o:
            sPK = o[""script""]
        else:
            # note the to_scriptPubKey method is only available for standard
            # address types
            sPK = CCoinAddress(o[""address""]).to_scriptPubKey()
        out = CMutableTxOut(o[""value""], sPK)
        vout.append(out)
    return CMutableTransaction(vin, vout, nLockTime=locktime, nVersion=version)","for i in ins:
    outpoint = CMutableOutPoint(i[0][::-1], i[1])
    inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)
    vin.append(inp)","for i in ins:
    ((i_0_rivirsi, *i_0_rimaining), i_1, *i_rimaining) = i
    outpoint = CMutableOutPoint(i[0][::-1], i[1])
    inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)
    vin.append(inp)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",0,,,"Answer: Yes
Iterable Unpacking: (e_0_reverse, *e_0_remaining), e_1, *e_remaining = e
variable mapping:
e_0_reverse: e[0][::-1]
e_1: e[1]",,,,,,,
joinmarket-clientserver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joinmarket-clientserver/jmbitcoin/jmbitcoin/secp256k1_transaction.py,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/jmbitcoin/jmbitcoin/secp256k1_transaction.py,,mktx$315,"def mktx(ins, outs, version=1, locktime=0):
    """""" Given a list of input tuples (txid(bytes), n(int)),
    and a list of outputs which are dicts with
    keys ""address"" (value should be *str* not CCoinAddress) (
    or alternately ""script"" (for nonstandard outputs, value
    should be CScript)),
    ""value"" (value should be integer satoshis), outputs a
    CMutableTransaction object.
    Tx version and locktime are optionally set, for non-default
    locktimes, inputs are given nSequence as per below comment.
    """"""
    vin = []
    vout = []
    # This does NOT trigger rbf and mimics Core's standard behaviour as of
    # Jan 2019.
    # Tx creators wishing to use rbf will need to set it explicitly outside
    # of this function.
    if locktime != 0:
        sequence = 0xffffffff - 1
    else:
        sequence = 0xffffffff
    for i in ins:
        outpoint = CMutableOutPoint((i[0][::-1]), i[1])
        inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)
        vin.append(inp)
    for o in outs:
        if ""script"" in o:
            sPK = o[""script""]
        else:
            # note the to_scriptPubKey method is only available for standard
            # address types
            sPK = CCoinAddress(o[""address""]).to_scriptPubKey()
        out = CMutableTxOut(o[""value""], sPK)
        vout.append(out)
    return CMutableTransaction(vin, vout, nLockTime=locktime, nVersion=version)","for o in outs:
    if 'script' in o:
        sPK = o['script']
    else:
        sPK = CCoinAddress(o['address']).to_scriptPubKey()
    out = CMutableTxOut(o['value'], sPK)
    vout.append(out)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_address, e_script, e_value = e['address'], e['script'], e['value']
variable mapping:
e_address: e['address']
e_script: e['script']
e_value: e['value']",,,,,,,
sphinx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sphinx/sphinx/writers/latex.py,https://github.com/sphinx-doc/sphinx/tree/master/sphinx/writers/latex.py,LaTeXTranslator,generate_indices$474,"def generate_indices(self) -> str:
        def generate(content: List[Tuple[str, List[IndexEntry]]], collapsed: bool) -> None:
            ret.append(r'\begin{sphinxtheindex}' + CR)
            ret.append(r'\let\bigletter\sphinxstyleindexlettergroup' + CR)
            for i, (letter, entries) in enumerate(content):
                if i > 0:
                    ret.append(r'\indexspace' + CR)
                ret.append(r'\bigletter{%s}' % self.escape(letter) + CR)
                for entry in entries:
                    if not entry[3]:
                        continue
                    ret.append(r'\item\relax\sphinxstyleindexentry{%s}' %
                               self.encode(entry[0]))
                    if entry[4]:
                        # add ""extra"" info
                        ret.append(r'\sphinxstyleindexextra{%s}' % self.encode(entry[4]))
                    ret.append(r'\sphinxstyleindexpageref{%s:%s}' %
                               (entry[2], self.idescape(entry[3])) + CR)
            ret.append(r'\end{sphinxtheindex}' + CR)

        ret = []
        # latex_domain_indices can be False/True or a list of index names
        indices_config = self.config.latex_domain_indices
        if indices_config:
            for domain in self.builder.env.domains.values():
                for indexcls in domain.indices:
                    indexname = '%s-%s' % (domain.name, indexcls.name)
                    if isinstance(indices_config, list):
                        if indexname not in indices_config:
                            continue
                    content, collapsed = indexcls(domain).generate(
                        self.builder.docnames)
                    if not content:
                        continue
                    ret.append(r'\renewcommand{\indexname}{%s}' % indexcls.localname + CR)
                    generate(content, collapsed)

        return ''.join(ret)","for entry in entries:
    if not entry[3]:
        continue
    ret.append('\\item\\relax\\sphinxstyleindexentry{%s}' % self.encode(entry[0]))
    if entry[4]:
        ret.append('\\sphinxstyleindexextra{%s}' % self.encode(entry[4]))
    ret.append('\\sphinxstyleindexpageref{%s:%s}' % (entry[2], self.idescape(entry[3])) + CR)","for entry in entries:
    (entry_0, _, entry_2, entry_3, entry_4, *_) = entry
    if not entry[3]:
        continue
    ret.append('\\item\\relax\\sphinxstyleindexentry{%s}' % self.encode(entry[0]))
    if entry[4]:
        ret.append('\\sphinxstyleindexextra{%s}' % self.encode(entry[4]))
    ret.append('\\sphinxstyleindexpageref{%s:%s}' % (entry[2], self.idescape(entry[3])) + CR)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, _, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,
PersonRelationKnowledgeGraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PersonRelationKnowledgeGraph/collect_person_rel.py,https://github.com/liuhuanyong/PersonRelationKnowledgeGraph/tree/master//collect_person_rel.py,PersonSpider,modify_data$131,"def modify_data(self):
        f_rel = open('rel_data.txt', 'w+')
        f_reltype = open('rel_type.txt', 'w+')
        f_person = open('person2id.txt', 'w+')
        person_dict = {}
        rel_dict = {}
        rel_list = set()
        rel_types = []

        for item in self.conn['person_rel']['data2'].find():
            nodes = item['nodes']
            for node in nodes:
                id = node['id']
                name = node['name']
                person_dict[id] = name

        for item in self.conn['person_rel']['data2'].find():
            links = item['links']
            for link in links:
                from_person = person_dict.get(link['from'], '')
                to_person = person_dict.get(link['to'], '')
                if not from_person or not to_person:
                    continue
                rel_name = link['name']
                rel_type = link['type']

                rel_dict[rel_name] = rel_type
                data = [from_person, to_person, rel_name, str(rel_type)]
                rel_list.add('###'.join(data))

        rels_num = len(rel_list)
        persons_num = len(person_dict.keys())

        for rel in rel_list:
            if len(rel.split('###')) != 4:
                continue
            rel_name = rel.split('###')[2]
            rel_types.append(rel_name)


        for id, name in person_dict.items():
            f_person.write(str(id) + '\t' + name + '\n')

        reltype_dict = Counter(rel_types).most_common()

        sum = 0.0
        for i in reltype_dict:
            rel_name = i[0]
            rel_freq = i[1]
            rel_percent = rel_freq/rels_num
            sum += rel_percent

            f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')

        f_rel.write('\n'.join(list(rel_list)))
        f_person.close()
        f_rel.close()
        f_reltype.close()

        print('rels_num', rels_num)
        print('persons_num', persons_num)
        return","for item in self.conn['person_rel']['data2'].find():
    nodes = item['nodes']
    for node in nodes:
        id = node['id']
        name = node['name']
        person_dict[id] = name",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['nodes'] is a dictionary value that can be accessed directly using the key 'nodes'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
PersonRelationKnowledgeGraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PersonRelationKnowledgeGraph/collect_person_rel.py,https://github.com/liuhuanyong/PersonRelationKnowledgeGraph/tree/master//collect_person_rel.py,PersonSpider,modify_data$131,"def modify_data(self):
        f_rel = open('rel_data.txt', 'w+')
        f_reltype = open('rel_type.txt', 'w+')
        f_person = open('person2id.txt', 'w+')
        person_dict = {}
        rel_dict = {}
        rel_list = set()
        rel_types = []

        for item in self.conn['person_rel']['data2'].find():
            nodes = item['nodes']
            for node in nodes:
                id = node['id']
                name = node['name']
                person_dict[id] = name

        for item in self.conn['person_rel']['data2'].find():
            links = item['links']
            for link in links:
                from_person = person_dict.get(link['from'], '')
                to_person = person_dict.get(link['to'], '')
                if not from_person or not to_person:
                    continue
                rel_name = link['name']
                rel_type = link['type']

                rel_dict[rel_name] = rel_type
                data = [from_person, to_person, rel_name, str(rel_type)]
                rel_list.add('###'.join(data))

        rels_num = len(rel_list)
        persons_num = len(person_dict.keys())

        for rel in rel_list:
            if len(rel.split('###')) != 4:
                continue
            rel_name = rel.split('###')[2]
            rel_types.append(rel_name)


        for id, name in person_dict.items():
            f_person.write(str(id) + '\t' + name + '\n')

        reltype_dict = Counter(rel_types).most_common()

        sum = 0.0
        for i in reltype_dict:
            rel_name = i[0]
            rel_freq = i[1]
            rel_percent = rel_freq/rels_num
            sum += rel_percent

            f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')

        f_rel.write('\n'.join(list(rel_list)))
        f_person.close()
        f_rel.close()
        f_reltype.close()

        print('rels_num', rels_num)
        print('persons_num', persons_num)
        return","for item in self.conn['person_rel']['data2'].find():
    links = item['links']
    for link in links:
        from_person = person_dict.get(link['from'], '')
        to_person = person_dict.get(link['to'], '')
        if not from_person or not to_person:
            continue
        rel_name = link['name']
        rel_type = link['type']
        rel_dict[rel_name] = rel_type
        data = [from_person, to_person, rel_name, str(rel_type)]
        rel_list.add('###'.join(data))",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['links'] is a dictionary value that can be accessed directly using the key 'links'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
PersonRelationKnowledgeGraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PersonRelationKnowledgeGraph/collect_person_rel.py,https://github.com/liuhuanyong/PersonRelationKnowledgeGraph/tree/master//collect_person_rel.py,PersonSpider,modify_data$131,"def modify_data(self):
        f_rel = open('rel_data.txt', 'w+')
        f_reltype = open('rel_type.txt', 'w+')
        f_person = open('person2id.txt', 'w+')
        person_dict = {}
        rel_dict = {}
        rel_list = set()
        rel_types = []

        for item in self.conn['person_rel']['data2'].find():
            nodes = item['nodes']
            for node in nodes:
                id = node['id']
                name = node['name']
                person_dict[id] = name

        for item in self.conn['person_rel']['data2'].find():
            links = item['links']
            for link in links:
                from_person = person_dict.get(link['from'], '')
                to_person = person_dict.get(link['to'], '')
                if not from_person or not to_person:
                    continue
                rel_name = link['name']
                rel_type = link['type']

                rel_dict[rel_name] = rel_type
                data = [from_person, to_person, rel_name, str(rel_type)]
                rel_list.add('###'.join(data))

        rels_num = len(rel_list)
        persons_num = len(person_dict.keys())

        for rel in rel_list:
            if len(rel.split('###')) != 4:
                continue
            rel_name = rel.split('###')[2]
            rel_types.append(rel_name)


        for id, name in person_dict.items():
            f_person.write(str(id) + '\t' + name + '\n')

        reltype_dict = Counter(rel_types).most_common()

        sum = 0.0
        for i in reltype_dict:
            rel_name = i[0]
            rel_freq = i[1]
            rel_percent = rel_freq/rels_num
            sum += rel_percent

            f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')

        f_rel.write('\n'.join(list(rel_list)))
        f_person.close()
        f_rel.close()
        f_reltype.close()

        print('rels_num', rels_num)
        print('persons_num', persons_num)
        return","for i in reltype_dict:
    rel_name = i[0]
    rel_freq = i[1]
    rel_percent = rel_freq / rels_num
    sum += rel_percent
    f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')","for i in reltype_dict:
    (i_0, i_1, *_) = i
    rel_name = i[0]
    rel_freq = i[1]
    rel_percent = rel_freq / rels_num
    sum += rel_percent
    f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
PersonRelationKnowledgeGraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PersonRelationKnowledgeGraph/collect_person_rel.py,https://github.com/liuhuanyong/PersonRelationKnowledgeGraph/tree/master//collect_person_rel.py,PersonSpider,modify_data$131,"def modify_data(self):
        f_rel = open('rel_data.txt', 'w+')
        f_reltype = open('rel_type.txt', 'w+')
        f_person = open('person2id.txt', 'w+')
        person_dict = {}
        rel_dict = {}
        rel_list = set()
        rel_types = []

        for item in self.conn['person_rel']['data2'].find():
            nodes = item['nodes']
            for node in nodes:
                id = node['id']
                name = node['name']
                person_dict[id] = name

        for item in self.conn['person_rel']['data2'].find():
            links = item['links']
            for link in links:
                from_person = person_dict.get(link['from'], '')
                to_person = person_dict.get(link['to'], '')
                if not from_person or not to_person:
                    continue
                rel_name = link['name']
                rel_type = link['type']

                rel_dict[rel_name] = rel_type
                data = [from_person, to_person, rel_name, str(rel_type)]
                rel_list.add('###'.join(data))

        rels_num = len(rel_list)
        persons_num = len(person_dict.keys())

        for rel in rel_list:
            if len(rel.split('###')) != 4:
                continue
            rel_name = rel.split('###')[2]
            rel_types.append(rel_name)


        for id, name in person_dict.items():
            f_person.write(str(id) + '\t' + name + '\n')

        reltype_dict = Counter(rel_types).most_common()

        sum = 0.0
        for i in reltype_dict:
            rel_name = i[0]
            rel_freq = i[1]
            rel_percent = rel_freq/rels_num
            sum += rel_percent

            f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')

        f_rel.write('\n'.join(list(rel_list)))
        f_person.close()
        f_rel.close()
        f_reltype.close()

        print('rels_num', rels_num)
        print('persons_num', persons_num)
        return","for node in nodes:
    id = node['id']
    name = node['name']
    person_dict[id] = name",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked elements e['id'] and e['name'] are dictionary values that can be accessed directly using the keys 'id' and 'name'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
PersonRelationKnowledgeGraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PersonRelationKnowledgeGraph/collect_person_rel.py,https://github.com/liuhuanyong/PersonRelationKnowledgeGraph/tree/master//collect_person_rel.py,PersonSpider,modify_data$131,"def modify_data(self):
        f_rel = open('rel_data.txt', 'w+')
        f_reltype = open('rel_type.txt', 'w+')
        f_person = open('person2id.txt', 'w+')
        person_dict = {}
        rel_dict = {}
        rel_list = set()
        rel_types = []

        for item in self.conn['person_rel']['data2'].find():
            nodes = item['nodes']
            for node in nodes:
                id = node['id']
                name = node['name']
                person_dict[id] = name

        for item in self.conn['person_rel']['data2'].find():
            links = item['links']
            for link in links:
                from_person = person_dict.get(link['from'], '')
                to_person = person_dict.get(link['to'], '')
                if not from_person or not to_person:
                    continue
                rel_name = link['name']
                rel_type = link['type']

                rel_dict[rel_name] = rel_type
                data = [from_person, to_person, rel_name, str(rel_type)]
                rel_list.add('###'.join(data))

        rels_num = len(rel_list)
        persons_num = len(person_dict.keys())

        for rel in rel_list:
            if len(rel.split('###')) != 4:
                continue
            rel_name = rel.split('###')[2]
            rel_types.append(rel_name)


        for id, name in person_dict.items():
            f_person.write(str(id) + '\t' + name + '\n')

        reltype_dict = Counter(rel_types).most_common()

        sum = 0.0
        for i in reltype_dict:
            rel_name = i[0]
            rel_freq = i[1]
            rel_percent = rel_freq/rels_num
            sum += rel_percent

            f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')

        f_rel.write('\n'.join(list(rel_list)))
        f_person.close()
        f_rel.close()
        f_reltype.close()

        print('rels_num', rels_num)
        print('persons_num', persons_num)
        return","for link in links:
    from_person = person_dict.get(link['from'], '')
    to_person = person_dict.get(link['to'], '')
    if not from_person or not to_person:
        continue
    rel_name = link['name']
    rel_type = link['type']
    rel_dict[rel_name] = rel_type
    data = [from_person, to_person, rel_name, str(rel_type)]
    rel_list.add('###'.join(data))",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_from, e_name, e_to, e_type = e['from'], e['name'], e['to'], e['type']
variable mapping:
e_from: e['from']
e_name: e['name']
e_to: e['to']
e_type: e['type']",,,,,,,
trezor-firmware,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/trezor-firmware/core/tests/test_trezor.strings.py,https://github.com/trezor/trezor-firmware/tree/master/core/tests/test_trezor.strings.py,TestStrings,test_format_plural$25,"def test_format_plural(self):
        VECTORS = [
            (""We need {count} more {plural}"", 1, ""share"", ""We need 1 more share""),
            (""We need {count} more {plural}"", 3, ""share"", ""We need 3 more shares""),
            (""We need {count} more {plural}"", 1, ""candy"", ""We need 1 more candy""),
            (""We need {count} more {plural}"", 7, ""candy"", ""We need 7 more candies""),
            (""We need {count} more {plural}"", 1, ""key"", ""We need 1 more key""),
            (""We need {count} more {plural}"", 5, ""key"", ""We need 5 more keys""),
            (""We need {count} more {plural}"", 1, ""hash"", ""We need 1 more hash""),
            (""We need {count} more {plural}"", 2, ""hash"", ""We need 2 more hashes""),
            (""We need {count} more {plural}"", 1, ""fuzz"", ""We need 1 more fuzz""),
            (""We need {count} more {plural}"", 2, ""fuzz"", ""We need 2 more fuzzes""),
        ]
        for v in VECTORS:
            self.assertEqual(strings.format_plural(v[0], v[1], v[2]), v[3])

        with self.assertRaises(ValueError):
            strings.format_plural(""Hello"", 1, ""share"")","for v in VECTORS:
    self.assertEqual(strings.format_plural(v[0], v[1], v[2]), v[3])","for v in VECTORS:
    (v_0, v_1, v_2, v_3, *_) = v
    self.assertEqual(strings.format_plural(v[0], v[1], v[2]), v[3])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
pyglossary,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglossary/pyglossary/text_utils.py,https://github.com/ilius/pyglossary/tree/master/pyglossary/text_utils.py,,replace$56,"def replace(st: str) -> str:
		for rpl in rplList:
			st = st.replace(rpl[0], rpl[1])
		return st","for rpl in rplList:
    st = st.replace(rpl[0], rpl[1])","for rpl in rplList:
    (rpl_0, rpl_1, *_) = rpl
    st = st.replace(rpl[0], rpl[1])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/offlinearize.py,https://github.com/nlplab/brat/tree/master/tools/offlinearize.py,,convert_coll$47,"def convert_coll(coll):
    if coll == '':
        ajax_coll = '/'
    else:
        ajax_coll = '/%s/' % coll

    coll_query_url = urljoin(
        base_url,
        'ajax.cgi?action=getCollectionInformation&collection=%s' %
        ajax_coll)
    coll_dir = joinpath(datadir, coll)
    try:
        makedirs(coll_dir)
    except BaseException:
        pass  # hopefully because it exists; TODO: check the error value?

    print(ajax_coll)
    conn = urlopen(coll_query_url)
    jsonp = conn.read()
    conn.close
    with open(joinpath(coll_dir, 'collection.js'), 'w') as f:
        f.write(""jsonp="")
        f.write(jsonp)

    coll_data = loads(jsonp)
    for item in coll_data['items']:
        if item[0] == 'd':
            doc = item[2]
            print(""  %s"" % doc)
            doc_query_url = urljoin(
                base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' %
                (ajax_coll, doc))

            conn = urlopen(doc_query_url)
            jsonp = conn.read()
            conn.close
            with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:
                f.write(""jsonp="")
                f.write(jsonp)
        elif item[0] == 'c' and item[2] != '..':
            convert_coll(item[2])","for item in coll_data['items']:
    if item[0] == 'd':
        doc = item[2]
        print('  %s' % doc)
        doc_query_url = urljoin(base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' % (ajax_coll, doc))
        conn = urlopen(doc_query_url)
        jsonp = conn.read()
        conn.close
        with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:
            f.write('jsonp=')
            f.write(jsonp)
    elif item[0] == 'c' and item[2] != '..':
        convert_coll(item[2])","for item in coll_data['items']:
    (item_0, _, item_2, *item_ritemmaining) = item
    if item[0] == 'd':
        doc = item[2]
        print('  %s' % doc)
        doc_query_url = urljoin(base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' % (ajax_coll, doc))
        conn = urlopen(doc_query_url)
        jsonp = conn.read()
        conn.close
        with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:
            f.write('jsonp=')
            f.write(jsonp)
    elif item[0] == 'c' and item[2] != '..':
        convert_coll(item[2])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, _, e_2, *e_remaining = e
variable mapping:
e_0: e[0]
e_2: e[2]",,,,,,,
pyradio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyradio/pyradio/radio.py,https://github.com/coderholic/pyradio/tree/master/pyradio/radio.py,PyRadio,is_search_mode$3875,"def is_search_mode(self, a_mode):
        for it in self._search_modes.items():
            if it[1] == a_mode:
                return True
        return False","for it in self._search_modes.items():
    if it[1] == a_mode:
        return True","for it in self._search_modes.items():
    (_, it_1, *it_ritmaining) = it
    if it[1] == a_mode:
        return True","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
freeipa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/topology.py,https://github.com/freeipa/freeipa/tree/master/ipaserver/topology.py,,_format_topology_errors$116,"def _format_topology_errors(topo_errors):
    msg_lines = []
    for error in topo_errors:
        msg_lines.append(
            _(""Topology does not allow server %(server)s to replicate with ""
              ""servers:"")
            % {'server': error[0]}
        )
        for srv in error[2]:
            msg_lines.append(""    %s"" % srv)

    return ""\n"".join(msg_lines)","for error in topo_errors:
    msg_lines.append(_('Topology does not allow server %(server)s to replicate with servers:') % {'server': error[0]})
    for srv in error[2]:
        msg_lines.append('    %s' % srv)","for error in topo_errors:
    (error_0, _, error_2, *error_rerrormaining) = error
    msg_lines.append(_('Topology does not allow server %(server)s to replicate with servers:') % {'server': error[0]})
    for srv in error[2]:
        msg_lines.append('    %s' % srv)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, _, e_2, *e_remaining = e
variable mapping:
e_0: e[0]
e_2: e[2]",,,,,,,
unilm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unilm/xtune/src/pequod/training/xtrainer.py,https://github.com/microsoft/unilm/tree/master/xtune/src/pequod/training/xtrainer.py,BaseTrainer,base_step$90,"def base_step(self, batches, is_qa=False, **kwargs):
    tot_loss = 0.0
    for step_batches in batches:
      batch = step_batches[0]
      batch_dict = self._parse_batch(batch)
      loss = self.model(**batch_dict)[0]
      self.backward_step(loss)
      tot_loss += loss.item()
    self.optim_step()
    return tot_loss / len(batches)","for step_batches in batches:
    batch = step_batches[0]
    batch_dict = self._parse_batch(batch)
    loss = self.model(**batch_dict)[0]
    self.backward_step(loss)
    tot_loss += loss.item()","for step_batches in batches:
    (step_batches_0, *step_batches_rstep_batchesmaining) = step_batches
    batch = step_batches[0]
    batch_dict = self._parse_batch(batch)
    loss = self.model(**batch_dict)[0]
    self.backward_step(loss)
    tot_loss += loss.item()","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
spacy-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spacy-transformers/spacy_transformers/tests/test_pipeline_component.py,https://github.com/explosion/spacy-transformers/tree/master/spacy_transformers/tests/test_pipeline_component.py,,test_transformer_pipeline_tagger_senter_listener$181,"def test_transformer_pipeline_tagger_senter_listener():
    """"""Test that a pipeline with just a transformer+tagger+senter runs and
    trains properly""""""
    orig_config = Config().from_str(cfg_string)
    nlp = util.load_model_from_config(orig_config, auto_fill=True, validate=True)
    assert nlp.pipe_names == [""transformer"", ""tagger"", ""senter""]
    tagger = nlp.get_pipe(""tagger"")
    transformer = nlp.get_pipe(""transformer"")
    tagger_trf = tagger.model.get_ref(""tok2vec"").layers[0]
    assert isinstance(transformer, Transformer)
    assert isinstance(tagger_trf, TransformerListener)
    assert tagger_trf.upstream_name == ""custom_upstream""
    train_examples = []
    for t in TRAIN_DATA:
        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
        for tag in t[1][""tags""]:
            tagger.add_label(tag)

    # Check that the Transformer component finds it listeners
    assert transformer.listeners == []
    optimizer = nlp.initialize(lambda: train_examples)
    assert tagger_trf in transformer.listeners

    for i in range(2):
        losses = {}
        nlp.update(train_examples, sgd=optimizer, losses=losses)

    text = ""We're interested at underwater basket weaving.""
    doc = nlp(text)
    doc_tensor = tagger_trf.predict([doc])
    _assert_equal_tensors(doc._.trf_data.tensors, doc_tensor[0].tensors)

    # ensure IO goes OK
    with make_tempdir() as d:
        file_path = d / ""trained_nlp""
        nlp.to_disk(file_path)
        nlp2 = util.load_model_from_path(file_path)
        doc2 = nlp2(text)
        tagger2 = nlp2.get_pipe(""tagger"")
        tagger_trf2 = tagger2.model.get_ref(""tok2vec"").layers[0]
        doc_tensor2 = tagger_trf2.predict([doc2])
        _assert_equal_tensors(doc_tensor2[0].tensors, doc_tensor[0].tensors)

        # make sure that this can be saved to directory once more
        file_path_2 = d / ""trained_nlp_2""
        nlp2.to_disk(file_path_2)

    # ensure to_bytes / from_bytes works
    nlp_bytes = nlp.to_bytes()
    nlp3 = util.load_model_from_config(orig_config, auto_fill=True, validate=True)
    nlp3.from_bytes(nlp_bytes)
    doc3 = nlp3(text)
    tagger3 = nlp3.get_pipe(""tagger"")
    tagger_trf3 = tagger3.model.get_ref(""tok2vec"").layers[0]
    doc_tensor3 = tagger_trf3.predict([doc3])
    _assert_equal_tensors(doc_tensor3[0].tensors, doc_tensor[0].tensors)","for t in TRAIN_DATA:
    train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
    for tag in t[1]['tags']:
        tagger.add_label(tag)","for t in TRAIN_DATA:
    (t_0, t_1, _, _, _, t_1_tags, *t_rtmaining) = t
    train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
    for tag in t[1]['tags']:
        tagger.add_label(tag)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",0,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, _, _, _, e_1_eags, *e_remaining = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_1_eags: e[1]['eags']",,,,,,,
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers/md/bills.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers/md/bills.py,MDBillScraper,scrape_bill_subjects$341,"def scrape_bill_subjects(self, bill, page):
        # TODO: this xpath gets both 'subjects' and 'file codes'
        # they both link to subjects on the site and seem to fit the definition of subject
        for row in page.xpath(
            '//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()'
        ):
            bill.add_subject(row[0])","for row in page.xpath('//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()'):
    bill.add_subject(row[0])","for row in page.xpath('//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()'):
    (row_0, *row_rrowmaining) = row
    bill.add_subject(row[0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
PyGaze,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyGaze/pygaze/_eyetracker/libtobiilegacy.py,https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libtobiilegacy.py,TobiiController,flushData$1922,"def flushData(self):
        
        """"""
        
        arguments
        None
        
        keyword arguments
        None
        
        returns
        None
        """"""
        
        # return if there is no datafile or no data
        if self.datafile == None:
            print(""WARNING! libtobii.TobiiController.flushData: data file is not set."")
            return
        
        if len(self.gazeData)==0:
            print(""WARNING! libtobii.TobiiController.flushData: no data to write to file."")
            return
        
        # write header
        self.datafile.write('\t'.join(['TimeStamp',
                                       'GazePointXLeft',
                                       'GazePointYLeft',
                                       'ValidityLeft',
                                       'GazePointXRight',
                                       'GazePointYRight',
                                       'ValidityRight',
                                       'GazePointX',
                                       'GazePointY',
                                       'Event'])+'\n')
        
        # time of the first event
        timeStampStart = self.gazeData[0].Timestamp
        
        # loop through all data points
        for g in self.gazeData:
            
            # write timestamp and gaze position for both eyes to the datafile
            self.datafile.write('{}\t{}\t{}\t{}\t{}\t{}\t{}'.format(
                round((g.Timestamp-timeStampStart)/1000.0, ndigits=1),
                round(g.LeftGazePoint2D.x*self.disp.dispsize[0] if g.LeftValidity!=4 else -1.0, ndigits=4),
                round(g.LeftGazePoint2D.y*self.disp.dispsize[1] if g.LeftValidity!=4 else -1.0, ndigits=4),
                g.LeftValidity,
                round(g.RightGazePoint2D.x*self.disp.dispsize[0] if g.RightValidity!=4 else -1.0, ndigits=4),
                round(g.RightGazePoint2D.y*self.disp.dispsize[1] if g.RightValidity!=4 else -1.0, ndigits=4),
                g.RightValidity))
            
            # if no correct sample is available, data is missing
            if g.LeftValidity == 4 and g.RightValidity == 4: #not detected
                ave = (-1.0,-1.0)
            # if the right sample is unavailable, use left sample
            elif g.LeftValidity == 4:
                ave = (g.RightGazePoint2D.x,g.RightGazePoint2D.y)
            # if the left sample is unavailable, use right sample
            elif g.RightValidity == 4:
                ave = (g.LeftGazePoint2D.x,g.LeftGazePoint2D.y)
            # if we have both samples, use both samples
            else:
                ave = ((g.LeftGazePoint2D.x + g.RightGazePoint2D.x) / 2.0,
                       (g.LeftGazePoint2D.y + g.RightGazePoint2D.y) / 2.0)
            
            # write gaze position to the datafile, based on the selected sample(s)
            self.datafile.write('\t{}\t{}\t'.format(\
                round(ave[0], ndigits=4), round(ave[1], ndigits=4)))
            self.datafile.write('\n')
        
        # general format of an event string
        formatstr = '{}'+'\t'*9+'{}\n'
        
        # write all events to the datafile, using the formatstring
        for e in self.eventData:
            self.datafile.write(formatstr.format( \
                round((e[0]-timeStampStart)/1000.0, ndigits=4), e[1]))
        
        # write data to disk
        self.datafile.flush() # internal buffer to RAM
        os.fsync(self.datafile.fileno())","for e in self.eventData:
    self.datafile.write(formatstr.format(round((e[0] - timeStampStart) / 1000.0, ndigits=4), e[1]))","for e in self.eventData:
    (e_0, e_1, *_) = e
    self.datafile.write(formatstr.format(round((e_0 - timeStampStart) / 1000.0, ndigits=4), e_1))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
cats-blender-plugin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cats-blender-plugin/extern_tools/google_trans_new/google_trans_new.py,https://github.com/absolute-quantum/cats-blender-plugin/tree/master/extern_tools/google_trans_new/google_trans_new.py,google_translator,translate$111,"def translate(self, text, lang_tgt='auto', lang_src='auto', pronounce=False):
        print(""\n\nDEBUG: Translating"", text)
        try:
            lang = LANGUAGES[lang_src]
        except:
            lang_src = 'auto'
        try:
            lang = LANGUAGES[lang_tgt]
        except:
            lang_src = 'auto'
        text = str(text)
        if len(text) >= 5000:
            return ""Warning: Can only detect less than 5000 characters""
        if len(text) == 0:
            return """"
        headers = {
            ""Referer"": ""http://translate.google.{}/"".format(self.url_suffix),
            ""User-Agent"":
                ""Mozilla/5.0 (Windows NT 10.0; WOW64) ""
                ""AppleWebKit/537.36 (KHTML, like Gecko) ""
                ""Chrome/47.0.2526.106 Safari/537.36"",
            ""Content-Type"": ""application/x-www-form-urlencoded;charset=utf-8""
        }
        freq = self._package_rpc(text, lang_src, lang_tgt)
        response = requests.Request(method='POST',
                                    url=self.url,
                                    data=freq,
                                    headers=headers,
                                    )
        try:
            if self.proxies == None or type(self.proxies) != dict:
                self.proxies = {}
            with requests.Session() as s:
                s.proxies = self.proxies
                r = s.send(request=response.prepare(),
                           verify=False,
                           timeout=self.timeout)
            for line in r.iter_lines(chunk_size=1024):
                decoded_line = line.decode('utf-8')
                if ""MkEWBc"" in decoded_line:
                    try:
                        response = (decoded_line + ']')
                        response = json.loads(response)
                        print(""DEBUG1"", response)
                        response = list(response)
                        response = json.loads(response[0][2])
                        response_ = list(response)
                        response = response_[1][0]
                        print(""DEBUG2"", response)
                        if len(response) == 1:
                            if len(response[0]) > 5:
                                sentences = response[0][5]
                                print(""DEBUG3"", sentences)
                            else:  # only url
                                sentences = response[0][0]
                                print(""DEBUG4"", sentences)
                                if pronounce == False:
                                    return sentences
                                elif pronounce == True:
                                    return [sentences,None,None]
                            translate_text = """"
                            # for sentence in sentences:
                            #     sentence = sentence[0]
                            #     translate_text += sentence.strip() + ' '

                            translations = sentences[0]
                            if not translations:
                                return text

                            translate_text = translations[0]

                            if pronounce == False:
                                return translate_text
                            elif pronounce == True:
                                pronounce_src = (response_[0][0])
                                pronounce_tgt = (response_[1][0][0][1])
                                return [translate_text, pronounce_src, pronounce_tgt]
                        elif len(response) == 2:
                            sentences = []
                            for i in response:
                                sentences.append(i[0])
                            if pronounce == False:
                                return sentences
                            elif pronounce == True:
                                pronounce_src = (response_[0][0])
                                pronounce_tgt = (response_[1][0][0][1])
                                return [sentences, pronounce_src, pronounce_tgt]
                    except Exception as e:
                        raise e
            r.raise_for_status()
        except requests.exceptions.ConnectTimeout as e:
            raise e
        except requests.exceptions.HTTPError as e:
            # Request successful, bad response
            raise google_new_transError(tts=self, response=r)
        except requests.exceptions.RequestException as e:
            # Request failed
            raise google_new_transError(tts=self)","for i in response:
    sentences.append(i[0])","for i in response:
    (i_0, *i_rimaining) = i
    sentences.append(i[0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Mycodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/utils/utils_general.py,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_general.py,,generate_form_input_list$1635,"def generate_form_input_list(dict_inputs):
    # Sort dictionary entries by input_manufacturer, then input_name
    # Results in list of sorted dictionary keys
    list_tuples_sorted = sorted(dict_inputs.items(), key=lambda x: (x[1]['input_manufacturer'], x[1]['input_name']))
    list_inputs_sorted = []
    for each_input in list_tuples_sorted:
        list_inputs_sorted.append(each_input[0])
    return list_inputs_sorted","for each_input in list_tuples_sorted:
    list_inputs_sorted.append(each_input[0])","for each_input in list_tuples_sorted:
    (each_input_0, *each_input_reach_inputmaining) = each_input
    list_inputs_sorted.append(each_input[0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
magic-wormhole,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/magic-wormhole/src/wormhole/test/test_machines.py,https://github.com/magic-wormhole/magic-wormhole/tree/master/src/wormhole/test/test_machines.py,Rendezvous,sent_messages$1601,"def sent_messages(ws):
            for c in ws.mock_calls:
                self.assertEqual(c[0], ""sendMessage"", ws.mock_calls)
                self.assertEqual(c[1][1], False, ws.mock_calls)
                yield bytes_to_dict(c[1][0])","for c in ws.mock_calls:
    self.assertEqual(c[0], 'sendMessage', ws.mock_calls)
    self.assertEqual(c[1][1], False, ws.mock_calls)
    yield bytes_to_dict(c[1][0])","for c in ws.mock_calls:
    (c_0, (c_1_0, c_1_1, *c_1_rcmaining), *c_rcmaining) = c
    self.assertEqual(c[0], 'sendMessage', ws.mock_calls)
    self.assertEqual(c[1][1], False, ws.mock_calls)
    yield bytes_to_dict(c[1][0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, (e_1_0, e_1_1, *e_1_remaining), *e_remaining = e
variable mapping:
e_0: e[0]
e_1_0: e[1][0]
e_1_1: e[1][1]",,,,,,,
DrQA,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DrQA/scripts/distant/generate.py,https://github.com/facebookresearch/DrQA/tree/master/scripts/distant/generate.py,,find_answer$80,"def find_answer(paragraph, q_tokens, answer, opts):
    """"""Return the best matching answer offsets from a paragraph.

    The paragraph is skipped if:
    * It is too long or short.
    * It doesn't contain the answer at all.
    * It doesn't contain named entities found in the question.
    * The answer context match score is too low.
      - This is the unigram + bigram overlap within +/- window_sz.
    """"""
    # Length check
    if len(paragraph) > opts['char_max'] or len(paragraph) < opts['char_min']:
        return

    # Answer check
    if opts['regex']:
        # Add group around the whole answer
        answer = '(%s)' % answer[0]
        ans_regex = re.compile(answer, flags=re.IGNORECASE + re.UNICODE)
        answers = ans_regex.findall(paragraph)
        answers = {a[0] if isinstance(a, tuple) else a for a in answers}
        answers = {a.strip() for a in answers if len(a.strip()) > 0}
    else:
        answers = {a for a in answer if a in paragraph}
    if len(answers) == 0:
        return

    # Entity check. Default tokenizer + NLTK to minimize falling through cracks
    q_tokens, q_nltk_ner = q_tokens
    for ne in q_tokens.entity_groups():
        if ne[0] not in paragraph:
            return
    for ne in q_nltk_ner:
        if ne not in paragraph:
            return

    # Search...
    p_tokens = tokenize_text(paragraph)
    p_words = p_tokens.words(uncased=True)
    q_grams = Counter(q_tokens.ngrams(
        n=2, uncased=True, filter_fn=utils.filter_ngram
    ))

    best_score = 0
    best_ex = None
    for ans in answers:
        try:
            a_words = tokenize_text(ans).words(uncased=True)
        except RuntimeError:
            logger.warn('Failed to tokenize answer: %s' % ans)
            continue
        for idx in range(len(p_words)):
            if p_words[idx:idx + len(a_words)] == a_words:
                # Overlap check
                w_s = max(idx - opts['window_sz'], 0)
                w_e = min(idx + opts['window_sz'] + len(a_words), len(p_words))
                w_tokens = p_tokens.slice(w_s, w_e)
                w_grams = Counter(w_tokens.ngrams(
                    n=2, uncased=True, filter_fn=utils.filter_ngram
                ))
                score = sum((w_grams & q_grams).values())
                if score > best_score:
                    # Success! Set new score + formatted example
                    best_score = score
                    best_ex = {
                        'id': uuid.uuid4().hex,
                        'question': q_tokens.words(),
                        'document': p_tokens.words(),
                        'offsets': p_tokens.offsets(),
                        'answers': [(idx, idx + len(a_words) - 1)],
                        'qlemma': q_tokens.lemmas(),
                        'lemma': p_tokens.lemmas(),
                        'pos': p_tokens.pos(),
                        'ner': p_tokens.entities(),
                    }
    if best_score >= opts['match_threshold']:
        return best_score, best_ex","for ne in q_tokens.entity_groups():
    if ne[0] not in paragraph:
        return","for ne in q_tokens.entity_groups():
    (ne_0, *ne_rnemaining) = ne
    if ne[0] not in paragraph:
        return","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
subDomainsBrute,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/subDomainsBrute/lib/scanner_py2.py,https://github.com/lijiejie/subDomainsBrute/tree/master/lib/scanner_py2.py,SubNameBrute,check_https_alt_names$90,"def check_https_alt_names(self, domain):
        try:
            x509 = reqs.OpenSSL.crypto.load_certificate(
                reqs.OpenSSL.crypto.FILETYPE_PEM,
                reqs.ssl.get_server_certificate((domain, 443))
            )
            for item in reqs.get_subj_alt_name(x509):
                if item[0].upper() == 'DNS':
                    name = item[1].lower()
                    if name.endswith(self.domain):
                        sub = name[:len(name) - len(self.domain) - 1]    # new sub
                        sub = sub.replace('*', '')
                        sub = sub.strip('.')
                        if sub and sub not in self.found_subs and \
                                sub not in self.normal_names_set and sub not in self.cert_subs:
                            self.cert_subs.add(sub)
                            self.queue.put((0, sub))
        except Exception as e:
            pass","for item in reqs.get_subj_alt_name(x509):
    if item[0].upper() == 'DNS':
        name = item[1].lower()
        if name.endswith(self.domain):
            sub = name[:len(name) - len(self.domain) - 1]
            sub = sub.replace('*', '')
            sub = sub.strip('.')
            if sub and sub not in self.found_subs and (sub not in self.normal_names_set) and (sub not in self.cert_subs):
                self.cert_subs.add(sub)
                self.queue.put((0, sub))","for item in reqs.get_subj_alt_name(x509):
    (item_0, item_1, *_) = item
    if item[0].upper() == 'DNS':
        name = item[1].lower()
        if name.endswith(self.domain):
            sub = name[:len(name) - len(self.domain) - 1]
            sub = sub.replace('*', '')
            sub = sub.strip('.')
            if sub and sub not in self.found_subs and (sub not in self.normal_names_set) and (sub not in self.cert_subs):
                self.cert_subs.add(sub)
                self.queue.put((0, sub))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/dygraph_to_static/test_for_enumerate.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/dygraph_to_static/test_for_enumerate.py,,for_tuple_as_enumerate_iter$258,"def for_tuple_as_enumerate_iter(x_array):
    x = paddle.to_tensor(x_array)
    x_list = [x, x, x]

    a_result = paddle.zeros([5])

    for t in enumerate(x_list):
        a_result += t[1]

    return a_result","for t in enumerate(x_list):
    a_result += t[1]","for t in enumerate(x_list):
    (t_0, t_1, *t_rtmaining) = t
    a_result += t[1]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
pycoin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycoin/pycoin/key/Keychain.py,https://github.com/richardkiss/pycoin/tree/master/pycoin/key/Keychain.py,Keychain,interested_hashes$128,"def interested_hashes(self):
        SQL = ""select hash160 from HASH160""
        c = self._exec_sql(SQL)
        for r in c:
            yield r[0]
        SQL = ""select hash160, hash256 from P2S""
        c = self._exec_sql(SQL)
        for r in c:
            yield r[0]
            yield r[1]","for r in c:
    yield r[0]","for r in c:
    (r_0, *r_rrmaining) = r
    yield r[0]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
pycoin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycoin/pycoin/key/Keychain.py,https://github.com/richardkiss/pycoin/tree/master/pycoin/key/Keychain.py,Keychain,interested_hashes$128,"def interested_hashes(self):
        SQL = ""select hash160 from HASH160""
        c = self._exec_sql(SQL)
        for r in c:
            yield r[0]
        SQL = ""select hash160, hash256 from P2S""
        c = self._exec_sql(SQL)
        for r in c:
            yield r[0]
            yield r[1]","for r in c:
    yield r[0]
    yield r[1]","for r in c:
    (r_0, r_1, *_) = r
    yield r[0]
    yield r[1]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
spiderfoot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spiderfoot/sfwebui.py,https://github.com/smicallef/spiderfoot/tree/master//sfwebui.py,SpiderFootWebUi,scanexportlogs$328,"def scanexportlogs(self: 'SpiderFootWebUi', id: str, dialect: str = ""excel"") -> bytes:
        """"""Get scan log

        Args:
            id (str): scan ID
            dialect (str): CSV dialect (default: excel)

        Returns:
            bytes: scan logs in CSV format
        """"""
        dbh = SpiderFootDb(self.config)

        try:
            data = dbh.scanLogs(id, None, None, True)
        except Exception:
            return self.error(""Scan ID not found."")

        if not data:
            return self.error(""Scan ID not found."")

        fileobj = StringIO()
        parser = csv.writer(fileobj, dialect=dialect)
        parser.writerow([""Date"", ""Component"", ""Type"", ""Event"", ""Event ID""])
        for row in data:
            parser.writerow([
                time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime(row[0] / 1000)),
                str(row[1]),
                str(row[2]),
                str(row[3]),
                row[4]
            ])

        cherrypy.response.headers['Content-Disposition'] = f""attachment; filename=SpiderFoot-{id}.log.csv""
        cherrypy.response.headers['Content-Type'] = ""application/csv""
        cherrypy.response.headers['Pragma'] = ""no-cache""
        return fileobj.getvalue().encode('utf-8')","for row in data:
    parser.writerow([time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(row[0] / 1000)), str(row[1]), str(row[2]), str(row[3]), row[4]])","for row in data:
    (row_0, row_1, row_2, row_3, row_4, *_) = row
    parser.writerow([time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(row[0] / 1000)), str(row[1]), str(row[2]), str(row[3]), row[4]])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,
python-ternary,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-ternary/examples/scatter_colorbar.py,https://github.com/marcharper/python-ternary/tree/master/examples/scatter_colorbar.py,,_energy_to_enthalpy$37,"def _energy_to_enthalpy(energy):
    """"""Converts energy to enthalpy.
    
    This function take the energies stored in the energy array and
    converts them to formation enthalpy.

    Parameters
    ---------
    energy : list of lists of floats
    
    Returns
    -------
    enthalpy : list of lists containing the enthalpies.
    """"""
    
    pureA = [energy[0][0], energy[0][1]]
    pureB = [energy[1][0], energy[1][1]]
    pureC = [energy[2][0], energy[2][1]]

    enthalpy = []
    for en in energy:
        c = en[2]
        conc = [float(i) / sum(c) for i in c]

        CE = _en_to_enth(en[0], conc, pureA[0], pureB[0], pureC[0])
        VASP = _en_to_enth(en[1], conc, pureA[1], pureB[1], pureC[1])

        enthalpy.append([CE, VASP, c])

    return enthalpy","for en in energy:
    c = en[2]
    conc = [float(i) / sum(c) for i in c]
    CE = _en_to_enth(en[0], conc, pureA[0], pureB[0], pureC[0])
    VASP = _en_to_enth(en[1], conc, pureA[1], pureB[1], pureC[1])
    enthalpy.append([CE, VASP, c])","for en in energy:
    (en_0, en_1, en_2, *_) = en
    c = en[2]
    conc = [float(i) / sum(c) for i in c]
    CE = _en_to_enth(en[0], conc, pureA[0], pureB[0], pureC[0])
    VASP = _en_to_enth(en[1], conc, pureA[1], pureB[1], pureC[1])
    enthalpy.append([CE, VASP, c])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
StyleGAN-nada,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/StyleGAN-nada/convert_weight.py,https://github.com/rinongal/StyleGAN-nada/tree/master//convert_weight.py,,if_main_my$199,"if __name__ == ""__main__"":
    device = ""cuda""

    parser = argparse.ArgumentParser(
        description=""Tensorflow to pytorch model checkpoint converter""
    )
    parser.add_argument(
        ""--repo"",
        type=str,
        required=True,
        help=""path to the offical StyleGAN2 repository with dnnlib/ folder"",
    )
    parser.add_argument(
        ""--gen"", action=""store_true"", help=""convert the generator weights""
    )
    parser.add_argument(
        ""--disc"", action=""store_true"", help=""convert the discriminator weights""
    )
    parser.add_argument(
        ""--channel_multiplier"",
        type=int,
        default=2,
        help=""channel multiplier factor. config-f = 2, else = 1"",
    )
    parser.add_argument(""path"", metavar=""PATH"", help=""path to the tensorflow weights"")

    args = parser.parse_args()

    sys.path.append(args.repo)

    import dnnlib
    from dnnlib import tflib

    tflib.init_tf()

    with open(args.path, ""rb"") as f:
        generator, discriminator, g_ema = pickle.load(f)

    size = g_ema.output_shape[2]

    n_mlp = 0
    mapping_layers_names = g_ema.__getstate__()['components']['mapping'].list_layers()
    for layer in mapping_layers_names:
        if layer[0].startswith('Dense'):
            n_mlp += 1

    g = Generator(size, 512, n_mlp, channel_multiplier=args.channel_multiplier)
    state_dict = g.state_dict()
    state_dict = fill_statedict(state_dict, g_ema.vars, size, n_mlp)

    g.load_state_dict(state_dict)

    latent_avg = torch.from_numpy(g_ema.vars[""dlatent_avg""].value().eval())

    ckpt = {""g_ema"": state_dict, ""latent_avg"": latent_avg}

    if args.gen:
        g_train = Generator(size, 512, n_mlp, channel_multiplier=args.channel_multiplier)
        g_train_state = g_train.state_dict()
        g_train_state = fill_statedict(g_train_state, generator.vars, size, n_mlp)
        ckpt[""g""] = g_train_state

    if args.disc:
        disc = Discriminator(size, channel_multiplier=args.channel_multiplier)
        d_state = disc.state_dict()
        d_state = discriminator_fill_statedict(d_state, discriminator.vars, size)
        ckpt[""d""] = d_state

    name = os.path.splitext(args.path)[0]
    torch.save(ckpt, name + "".pt"")

    batch_size = {256: 16, 512: 9, 1024: 4}
    n_sample = batch_size.get(size, 25)

    g = g.to(device)

    z = np.random.RandomState(0).randn(n_sample, 512).astype(""float32"")

    with torch.no_grad():
        img_pt, _ = g(
            [torch.from_numpy(z).to(device)],
            truncation=0.5,
            truncation_latent=latent_avg.to(device),
            randomize_noise=False,
        )

    Gs_kwargs = dnnlib.EasyDict()
    Gs_kwargs.randomize_noise = False
    img_tf = g_ema.run(z, None, **Gs_kwargs)
    img_tf = torch.from_numpy(img_tf).to(device)

    img_diff = ((img_pt + 1) / 2).clamp(0.0, 1.0) - ((img_tf.to(device) + 1) / 2).clamp(
        0.0, 1.0
    )

    img_concat = torch.cat((img_tf, img_pt, img_diff), dim=0)

    print(img_diff.abs().max())

    utils.save_image(
        img_concat, name + "".png"", nrow=n_sample, normalize=True, range=(-1, 1)
    )","for layer in mapping_layers_names:
    if layer[0].startswith('Dense'):
        n_mlp += 1","for layer in mapping_layers_names:
    (layer_0, *layer_rlayermaining) = layer
    if layer[0].startswith('Dense'):
        n_mlp += 1","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
quodlibet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quodlibet/quodlibet/ext/songsmenu/duplicates.py,https://github.com/quodlibet/quodlibet/tree/master/quodlibet/ext/songsmenu/duplicates.py,DuplicateSongsView,get_selected_songs$36,"def get_selected_songs(self):
        selection = self.get_selection()
        if selection is None:
            return []
        model, rows = selection.get_selected_rows()
        if not rows:
            return []
        selected = []
        for row in rows:
            row = model[row]
            if row.parent is None:
                for child in row.iterchildren():
                    selected.append(child[0])
            else:
                selected.append(row[0])
        return selected","for row in rows:
    row = model[row]
    if row.parent is None:
        for child in row.iterchildren():
            selected.append(child[0])
    else:
        selected.append(row[0])","for row in rows:
    (row_0, *row_rrowmaining) = row
    row = model[row]
    if row.parent is None:
        for child in row.iterchildren():
            selected.append(child[0])
    else:
        selected.append(row[0])",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
quodlibet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quodlibet/quodlibet/ext/songsmenu/duplicates.py,https://github.com/quodlibet/quodlibet/tree/master/quodlibet/ext/songsmenu/duplicates.py,DuplicateSongsView,get_selected_songs$36,"def get_selected_songs(self):
        selection = self.get_selection()
        if selection is None:
            return []
        model, rows = selection.get_selected_rows()
        if not rows:
            return []
        selected = []
        for row in rows:
            row = model[row]
            if row.parent is None:
                for child in row.iterchildren():
                    selected.append(child[0])
            else:
                selected.append(row[0])
        return selected","for child in row.iterchildren():
    selected.append(child[0])","for child in row.iterchildren():
    (child_0, *child_rchildmaining) = child
    selected.append(child[0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,,downgrade$83,"def downgrade(migrate_engine):
    metadata.bind = migrate_engine
    metadata.reflect()

    Table(""form_definition"", metadata, autoload=True)
    Table(""form_values"", metadata, autoload=True)
    # remove the name attribute in the content column JSON dict in the form_values table
    # and restore it to a list of values
    cmd = ""SELECT form_values.id, form_values.content, form_definition.fields"" \
          "" FROM form_values, form_definition"" \
          "" WHERE form_values.form_definition_id=form_definition.id"" \
          "" ORDER BY form_values.id ASC""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_values_id = int(row[0])
        if not str(row[1]).strip():
            continue
        values_dict = loads(str(row[1]))
        if not str(row[2]).strip():
            continue
        fields_list = loads(str(row[2]))
        if fields_list:
            values_list = []
            for field in fields_list:
                field_name = field['name']
                field_value = values_dict[field_name]
                values_list.append(field_value)
            cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
            migrate_engine.execute(cmd)
    # remove name attribute from the field column of the form_definition table
    cmd = ""SELECT f.id, f.fields FROM form_definition AS f""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_definition_id = row[0]
        fields = str(row[1])
        if not fields.strip():
            continue
        fields_list = loads(_sniffnfix_pg9_hex(fields))
        if len(fields_list):
            for field in fields_list:
                if 'name' in field:
                    del field['name']
            if migrate_engine.name == 'mysql':
                cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
            else:
                cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
        migrate_engine.execute(cmd)","for row in result:
    form_values_id = int(row[0])
    if not str(row[1]).strip():
        continue
    values_dict = loads(str(row[1]))
    if not str(row[2]).strip():
        continue
    fields_list = loads(str(row[2]))
    if fields_list:
        values_list = []
        for field in fields_list:
            field_name = field['name']
            field_value = values_dict[field_name]
            values_list.append(field_value)
        cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
        migrate_engine.execute(cmd)","for row in result:
    (row_0, row_1, row_2, *_) = row
    form_values_id = int(row[0])
    if not str(row[1]).strip():
        continue
    values_dict = loads(str(row[1]))
    if not str(row[2]).strip():
        continue
    fields_list = loads(str(row[2]))
    if fields_list:
        values_list = []
        for field in fields_list:
            field_name = field['name']
            field_value = values_dict[field_name]
            values_list.append(field_value)
        cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
        migrate_engine.execute(cmd)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,,downgrade$83,"def downgrade(migrate_engine):
    metadata.bind = migrate_engine
    metadata.reflect()

    Table(""form_definition"", metadata, autoload=True)
    Table(""form_values"", metadata, autoload=True)
    # remove the name attribute in the content column JSON dict in the form_values table
    # and restore it to a list of values
    cmd = ""SELECT form_values.id, form_values.content, form_definition.fields"" \
          "" FROM form_values, form_definition"" \
          "" WHERE form_values.form_definition_id=form_definition.id"" \
          "" ORDER BY form_values.id ASC""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_values_id = int(row[0])
        if not str(row[1]).strip():
            continue
        values_dict = loads(str(row[1]))
        if not str(row[2]).strip():
            continue
        fields_list = loads(str(row[2]))
        if fields_list:
            values_list = []
            for field in fields_list:
                field_name = field['name']
                field_value = values_dict[field_name]
                values_list.append(field_value)
            cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
            migrate_engine.execute(cmd)
    # remove name attribute from the field column of the form_definition table
    cmd = ""SELECT f.id, f.fields FROM form_definition AS f""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_definition_id = row[0]
        fields = str(row[1])
        if not fields.strip():
            continue
        fields_list = loads(_sniffnfix_pg9_hex(fields))
        if len(fields_list):
            for field in fields_list:
                if 'name' in field:
                    del field['name']
            if migrate_engine.name == 'mysql':
                cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
            else:
                cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
        migrate_engine.execute(cmd)","for row in result:
    form_definition_id = row[0]
    fields = str(row[1])
    if not fields.strip():
        continue
    fields_list = loads(_sniffnfix_pg9_hex(fields))
    if len(fields_list):
        for field in fields_list:
            if 'name' in field:
                del field['name']
        if migrate_engine.name == 'mysql':
            cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
        else:
            cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
    migrate_engine.execute(cmd)","for row in result:
    (row_0, row_1, *_) = row
    form_definition_id = row[0]
    fields = str(row[1])
    if not fields.strip():
        continue
    fields_list = loads(_sniffnfix_pg9_hex(fields))
    if len(fields_list):
        for field in fields_list:
            if 'name' in field:
                del field['name']
        if migrate_engine.name == 'mysql':
            cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
        else:
            cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
    migrate_engine.execute(cmd)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,,downgrade$83,"def downgrade(migrate_engine):
    metadata.bind = migrate_engine
    metadata.reflect()

    Table(""form_definition"", metadata, autoload=True)
    Table(""form_values"", metadata, autoload=True)
    # remove the name attribute in the content column JSON dict in the form_values table
    # and restore it to a list of values
    cmd = ""SELECT form_values.id, form_values.content, form_definition.fields"" \
          "" FROM form_values, form_definition"" \
          "" WHERE form_values.form_definition_id=form_definition.id"" \
          "" ORDER BY form_values.id ASC""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_values_id = int(row[0])
        if not str(row[1]).strip():
            continue
        values_dict = loads(str(row[1]))
        if not str(row[2]).strip():
            continue
        fields_list = loads(str(row[2]))
        if fields_list:
            values_list = []
            for field in fields_list:
                field_name = field['name']
                field_value = values_dict[field_name]
                values_list.append(field_value)
            cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
            migrate_engine.execute(cmd)
    # remove name attribute from the field column of the form_definition table
    cmd = ""SELECT f.id, f.fields FROM form_definition AS f""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_definition_id = row[0]
        fields = str(row[1])
        if not fields.strip():
            continue
        fields_list = loads(_sniffnfix_pg9_hex(fields))
        if len(fields_list):
            for field in fields_list:
                if 'name' in field:
                    del field['name']
            if migrate_engine.name == 'mysql':
                cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
            else:
                cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
        migrate_engine.execute(cmd)","for field in fields_list:
    field_name = field['name']
    field_value = values_dict[field_name]
    values_list.append(field_value)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['name'] is a dictionary value that can be accessed directly using the key 'name'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,,downgrade$83,"def downgrade(migrate_engine):
    metadata.bind = migrate_engine
    metadata.reflect()

    Table(""form_definition"", metadata, autoload=True)
    Table(""form_values"", metadata, autoload=True)
    # remove the name attribute in the content column JSON dict in the form_values table
    # and restore it to a list of values
    cmd = ""SELECT form_values.id, form_values.content, form_definition.fields"" \
          "" FROM form_values, form_definition"" \
          "" WHERE form_values.form_definition_id=form_definition.id"" \
          "" ORDER BY form_values.id ASC""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_values_id = int(row[0])
        if not str(row[1]).strip():
            continue
        values_dict = loads(str(row[1]))
        if not str(row[2]).strip():
            continue
        fields_list = loads(str(row[2]))
        if fields_list:
            values_list = []
            for field in fields_list:
                field_name = field['name']
                field_value = values_dict[field_name]
                values_list.append(field_value)
            cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
            migrate_engine.execute(cmd)
    # remove name attribute from the field column of the form_definition table
    cmd = ""SELECT f.id, f.fields FROM form_definition AS f""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_definition_id = row[0]
        fields = str(row[1])
        if not fields.strip():
            continue
        fields_list = loads(_sniffnfix_pg9_hex(fields))
        if len(fields_list):
            for field in fields_list:
                if 'name' in field:
                    del field['name']
            if migrate_engine.name == 'mysql':
                cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
            else:
                cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
        migrate_engine.execute(cmd)","for field in fields_list:
    if 'name' in field:
        del field['name']",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['name'] is a dictionary value that can be accessed directly using the key 'name'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/fix_annotations.py,https://github.com/nlplab/brat/tree/master/tools/fix_annotations.py,,correct_annotations$28,"def correct_annotations(orig_fn, ann_fn, change_fn):
    with annotation.TextAnnotations(ann_fn) as anns:
        orig_text = anns.get_document_text()
        with annotation.open_textfile(change_fn, 'r') as f:
            changed_text = f.read()
        diffs = diff_match_patch().diff_main(orig_text, changed_text)
        orig_offset = 0
        offsets = []
        for diff in diffs:
            kind = diff[0]
            text = diff[1]
            size = len(text)
            delta = size * kind
            offsets.append((orig_offset, delta))
            if kind != 1:
                orig_offset += size
        offsets = offsets[::-1]
        tbs = list(anns.get_textbounds())
        indices = []
        for tbi, tb in enumerate(tbs):
            for spani, span in enumerate(tb.spans):
                indices.append((span[0], tbi, spani, 0))
                indices.append((span[1], tbi, spani, 1))
        indices.sort(reverse=True)
        for orig_offset, delta in offsets:
            for index in indices:
                if index[0] < orig_offset:
                    break
                frag = list(tbs[index[1]].spans[index[2]])
                frag[index[3]] += delta
                tbs[index[1]].spans[index[2]] = tuple(frag)
        for tb in tbs:
            if isinstance(tb, annotation.TextBoundAnnotationWithText):
                tb.text = annotation.DISCONT_SEP.join(
                    (changed_text[start:end] for start, end in tb.spans))
    copy(change_fn, orig_fn)","for diff in diffs:
    kind = diff[0]
    text = diff[1]
    size = len(text)
    delta = size * kind
    offsets.append((orig_offset, delta))
    if kind != 1:
        orig_offset += size","for diff in diffs:
    (diff_0, diff_1, *_) = diff
    kind = diff[0]
    text = diff[1]
    size = len(text)
    delta = size * kind
    offsets.append((orig_offset, delta))
    if kind != 1:
        orig_offset += size","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/fix_annotations.py,https://github.com/nlplab/brat/tree/master/tools/fix_annotations.py,,correct_annotations$28,"def correct_annotations(orig_fn, ann_fn, change_fn):
    with annotation.TextAnnotations(ann_fn) as anns:
        orig_text = anns.get_document_text()
        with annotation.open_textfile(change_fn, 'r') as f:
            changed_text = f.read()
        diffs = diff_match_patch().diff_main(orig_text, changed_text)
        orig_offset = 0
        offsets = []
        for diff in diffs:
            kind = diff[0]
            text = diff[1]
            size = len(text)
            delta = size * kind
            offsets.append((orig_offset, delta))
            if kind != 1:
                orig_offset += size
        offsets = offsets[::-1]
        tbs = list(anns.get_textbounds())
        indices = []
        for tbi, tb in enumerate(tbs):
            for spani, span in enumerate(tb.spans):
                indices.append((span[0], tbi, spani, 0))
                indices.append((span[1], tbi, spani, 1))
        indices.sort(reverse=True)
        for orig_offset, delta in offsets:
            for index in indices:
                if index[0] < orig_offset:
                    break
                frag = list(tbs[index[1]].spans[index[2]])
                frag[index[3]] += delta
                tbs[index[1]].spans[index[2]] = tuple(frag)
        for tb in tbs:
            if isinstance(tb, annotation.TextBoundAnnotationWithText):
                tb.text = annotation.DISCONT_SEP.join(
                    (changed_text[start:end] for start, end in tb.spans))
    copy(change_fn, orig_fn)","for (spani, span) in enumerate(tb.spans):
    indices.append((span[0], tbi, spani, 0))
    indices.append((span[1], tbi, spani, 1))","for (spani, span) in enumerate(tb.spans):
    (span_0, span_1, *span_rspanmaining) = span
    indices.append((span[0], tbi, spani, 0))
    indices.append((span[1], tbi, spani, 1))",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/fix_annotations.py,https://github.com/nlplab/brat/tree/master/tools/fix_annotations.py,,correct_annotations$28,"def correct_annotations(orig_fn, ann_fn, change_fn):
    with annotation.TextAnnotations(ann_fn) as anns:
        orig_text = anns.get_document_text()
        with annotation.open_textfile(change_fn, 'r') as f:
            changed_text = f.read()
        diffs = diff_match_patch().diff_main(orig_text, changed_text)
        orig_offset = 0
        offsets = []
        for diff in diffs:
            kind = diff[0]
            text = diff[1]
            size = len(text)
            delta = size * kind
            offsets.append((orig_offset, delta))
            if kind != 1:
                orig_offset += size
        offsets = offsets[::-1]
        tbs = list(anns.get_textbounds())
        indices = []
        for tbi, tb in enumerate(tbs):
            for spani, span in enumerate(tb.spans):
                indices.append((span[0], tbi, spani, 0))
                indices.append((span[1], tbi, spani, 1))
        indices.sort(reverse=True)
        for orig_offset, delta in offsets:
            for index in indices:
                if index[0] < orig_offset:
                    break
                frag = list(tbs[index[1]].spans[index[2]])
                frag[index[3]] += delta
                tbs[index[1]].spans[index[2]] = tuple(frag)
        for tb in tbs:
            if isinstance(tb, annotation.TextBoundAnnotationWithText):
                tb.text = annotation.DISCONT_SEP.join(
                    (changed_text[start:end] for start, end in tb.spans))
    copy(change_fn, orig_fn)","for index in indices:
    if index[0] < orig_offset:
        break
    frag = list(tbs[index[1]].spans[index[2]])
    frag[index[3]] += delta
    tbs[index[1]].spans[index[2]] = tuple(frag)","for index in indices:
    (index_0, *index_rindexmaining) = index
    if index[0] < orig_offset:
        break
    frag = list(tbs[index[1]].spans[index[2]])
    frag[index[3]] += delta
    tbs[index[1]].spans[index[2]] = tuple(frag)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/extra/tunings.py,https://github.com/bspaans/python-mingus/tree/master/mingus/extra/tunings.py,StringTuning,find_chord_fingering$149,"def find_chord_fingering(
        self,
        notes,
        max_distance=4,
        maxfret=18,
        max_fingers=4,
        return_best_as_NoteContainer=False,
    ):
        """"""Return a list of fret lists that are considered possible fingerings.

        This function only looks at and matches on the note _names_ so it
        does more than find_fingering.

        Example:
        >>> t = get_tuning('guitar', 'standard', 6, 1)
        >>> t.find_chord_fingering(NoteContainer().from_chord('Am'))
        [[0, 0, 2, 2, 1, 0], [0, 3, 2, 2, 1, 0], ......]
        """"""

        def follow(string, next, name, prev=-1):
            """"""Follow the fret 'next' on 'string'; build result on the way.""""""
            if string >= len(self.tuning) - 1:
                return [[(next, name)]]
            result = []
            cur = res[string][next]
            if cur != []:
                for y in cur[1]:
                    for sub in follow(string + 1, y[0], y[1]):
                        if prev < 0:
                            result.append([(next, name)] + sub)
                        else:
                            if sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
                                result.append([(next, name)] + sub)
            for s in follow(string + 1, maxfret + 1, None, next):
                result.append([(next, name)] + s)
            return [[(next, name)]] if result == [] else result

        def make_lookup_table():
            """"""Prepare the lookup table.

            table[string][fret] = (name, dest_frets)
            """"""
            res = [[[] for x in range(maxfret + 2)] for x in range(len(self.tuning) - 1)]
            for x in range(0, len(self.tuning) - 1):
                addedNone = -1
                next = fretdict[x + 1]
                for (fret, name) in fretdict[x]:
                    for (f2, n2) in next:
                        if n2 != name and (f2 == 0 or abs(fret - f2) < max_distance):
                            if res[x][fret] != []:
                                res[x][fret][1].append((f2, n2))
                            else:
                                res[x][fret] = (name, [(f2, n2)])
                        if addedNone < x:
                            if res[x][maxfret + 1] != []:
                                res[x][maxfret + 1][1].append((f2, n2))
                            else:
                                res[x][maxfret + 1] = (None, [(f2, n2)])
                    addedNone = x
            return res

        # Convert to NoteContainer if necessary
        n = notes
        if notes != [] and isinstance(notes, list) and isinstance(notes[0], six.string_types):
            n = NoteContainer(notes)

        # Check number of note names.
        notenames = [x.name for x in n]
        if len(notenames) == 0 or len(notenames) > len(self.tuning):
            return []

        # Make string-fret dictionary
        fretdict = []
        for x in range(0, len(self.tuning)):
            fretdict.append(self.find_note_names(notes, x, maxfret))

        # Build table
        res = make_lookup_table()

        # Build result using table
        result = []

        # For each fret on the first string
        for (i, y) in enumerate(res[0]):
            if y != []:
                (yname, next) = (y[0], y[1])

                # For each destination fret in y
                for (fret, name) in next:

                    # For each followed result
                    for s in follow(1, fret, name):
                        subresult = [(i, yname)] + s

                        # Get boundaries
                        (mi, ma, names) = (1000, -1000, [])
                        for (f, n) in subresult:
                            if n is not None:
                                if f != 0 and f <= mi:
                                    mi = f
                                if f != 0 and f >= ma:
                                    ma = f
                                names.append(n)

                        # Enforce boundaries
                        if abs(ma - mi) < max_distance:
                            # Check if all note
                            # names are present
                            covered = True
                            for n in notenames:
                                if n not in names:
                                    covered = False

                            # Add to result
                            if covered and names != []:
                                result.append(
                                    [y[0] if y[1] is not None else y[1] for y in subresult]
                                )

        # Return semi-sorted list
        s = sorted(
            result,
            key=lambda x: sum([t if t is not None else 1000 for (i, t) in enumerate(x)]),
        )
        s = [a for a in s if fingers_needed(a) <= max_fingers]
        if not return_best_as_NoteContainer:
            return s
        else:
            rnotes = self.frets_to_NoteContainer(s[0])
            for (i, x) in enumerate(rnotes):
                if x.string < len(self.tuning) - 1:
                    if res[x.string][x.fret] != []:
                        rnotes[i].name = res[x.string][x.fret][0]
            return rnotes","for (i, y) in enumerate(res[0]):
    if y != []:
        (yname, next) = (y[0], y[1])
        for (fret, name) in next:
            for s in follow(1, fret, name):
                subresult = [(i, yname)] + s
                (mi, ma, names) = (1000, -1000, [])
                for (f, n) in subresult:
                    if n is not None:
                        if f != 0 and f <= mi:
                            mi = f
                        if f != 0 and f >= ma:
                            ma = f
                        names.append(n)
                if abs(ma - mi) < max_distance:
                    covered = True
                    for n in notenames:
                        if n not in names:
                            covered = False
                    if covered and names != []:
                        result.append([y[0] if y[1] is not None else y[1] for y in subresult])","for (i, y) in enumerate(res[0]):
    (y_0, y_1, *_) = y
    if y != []:
        (yname, next) = (y[0], y[1])
        for (fret, name) in next:
            for s in follow(1, fret, name):
                subresult = [(i, yname)] + s
                (mi, ma, names) = (1000, -1000, [])
                for (f, n) in subresult:
                    if n is not None:
                        if f != 0 and f <= mi:
                            mi = f
                        if f != 0 and f >= ma:
                            ma = f
                        names.append(n)
                if abs(ma - mi) < max_distance:
                    covered = True
                    for n in notenames:
                        if n not in names:
                            covered = False
                    if covered and names != []:
                        result.append([y[0] if y[1] is not None else y[1] for y in subresult])",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/extra/tunings.py,https://github.com/bspaans/python-mingus/tree/master/mingus/extra/tunings.py,StringTuning,find_chord_fingering$149,"def find_chord_fingering(
        self,
        notes,
        max_distance=4,
        maxfret=18,
        max_fingers=4,
        return_best_as_NoteContainer=False,
    ):
        """"""Return a list of fret lists that are considered possible fingerings.

        This function only looks at and matches on the note _names_ so it
        does more than find_fingering.

        Example:
        >>> t = get_tuning('guitar', 'standard', 6, 1)
        >>> t.find_chord_fingering(NoteContainer().from_chord('Am'))
        [[0, 0, 2, 2, 1, 0], [0, 3, 2, 2, 1, 0], ......]
        """"""

        def follow(string, next, name, prev=-1):
            """"""Follow the fret 'next' on 'string'; build result on the way.""""""
            if string >= len(self.tuning) - 1:
                return [[(next, name)]]
            result = []
            cur = res[string][next]
            if cur != []:
                for y in cur[1]:
                    for sub in follow(string + 1, y[0], y[1]):
                        if prev < 0:
                            result.append([(next, name)] + sub)
                        else:
                            if sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
                                result.append([(next, name)] + sub)
            for s in follow(string + 1, maxfret + 1, None, next):
                result.append([(next, name)] + s)
            return [[(next, name)]] if result == [] else result

        def make_lookup_table():
            """"""Prepare the lookup table.

            table[string][fret] = (name, dest_frets)
            """"""
            res = [[[] for x in range(maxfret + 2)] for x in range(len(self.tuning) - 1)]
            for x in range(0, len(self.tuning) - 1):
                addedNone = -1
                next = fretdict[x + 1]
                for (fret, name) in fretdict[x]:
                    for (f2, n2) in next:
                        if n2 != name and (f2 == 0 or abs(fret - f2) < max_distance):
                            if res[x][fret] != []:
                                res[x][fret][1].append((f2, n2))
                            else:
                                res[x][fret] = (name, [(f2, n2)])
                        if addedNone < x:
                            if res[x][maxfret + 1] != []:
                                res[x][maxfret + 1][1].append((f2, n2))
                            else:
                                res[x][maxfret + 1] = (None, [(f2, n2)])
                    addedNone = x
            return res

        # Convert to NoteContainer if necessary
        n = notes
        if notes != [] and isinstance(notes, list) and isinstance(notes[0], six.string_types):
            n = NoteContainer(notes)

        # Check number of note names.
        notenames = [x.name for x in n]
        if len(notenames) == 0 or len(notenames) > len(self.tuning):
            return []

        # Make string-fret dictionary
        fretdict = []
        for x in range(0, len(self.tuning)):
            fretdict.append(self.find_note_names(notes, x, maxfret))

        # Build table
        res = make_lookup_table()

        # Build result using table
        result = []

        # For each fret on the first string
        for (i, y) in enumerate(res[0]):
            if y != []:
                (yname, next) = (y[0], y[1])

                # For each destination fret in y
                for (fret, name) in next:

                    # For each followed result
                    for s in follow(1, fret, name):
                        subresult = [(i, yname)] + s

                        # Get boundaries
                        (mi, ma, names) = (1000, -1000, [])
                        for (f, n) in subresult:
                            if n is not None:
                                if f != 0 and f <= mi:
                                    mi = f
                                if f != 0 and f >= ma:
                                    ma = f
                                names.append(n)

                        # Enforce boundaries
                        if abs(ma - mi) < max_distance:
                            # Check if all note
                            # names are present
                            covered = True
                            for n in notenames:
                                if n not in names:
                                    covered = False

                            # Add to result
                            if covered and names != []:
                                result.append(
                                    [y[0] if y[1] is not None else y[1] for y in subresult]
                                )

        # Return semi-sorted list
        s = sorted(
            result,
            key=lambda x: sum([t if t is not None else 1000 for (i, t) in enumerate(x)]),
        )
        s = [a for a in s if fingers_needed(a) <= max_fingers]
        if not return_best_as_NoteContainer:
            return s
        else:
            rnotes = self.frets_to_NoteContainer(s[0])
            for (i, x) in enumerate(rnotes):
                if x.string < len(self.tuning) - 1:
                    if res[x.string][x.fret] != []:
                        rnotes[i].name = res[x.string][x.fret][0]
            return rnotes","for y in cur[1]:
    for sub in follow(string + 1, y[0], y[1]):
        if prev < 0:
            result.append([(next, name)] + sub)
        elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
            result.append([(next, name)] + sub)","for y in cur[1]:
    (y_0, y_1, *_) = y
    for sub in follow(string + 1, y[0], y[1]):
        if prev < 0:
            result.append([(next, name)] + sub)
        elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
            result.append([(next, name)] + sub)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/extra/tunings.py,https://github.com/bspaans/python-mingus/tree/master/mingus/extra/tunings.py,StringTuning,find_chord_fingering$149,"def find_chord_fingering(
        self,
        notes,
        max_distance=4,
        maxfret=18,
        max_fingers=4,
        return_best_as_NoteContainer=False,
    ):
        """"""Return a list of fret lists that are considered possible fingerings.

        This function only looks at and matches on the note _names_ so it
        does more than find_fingering.

        Example:
        >>> t = get_tuning('guitar', 'standard', 6, 1)
        >>> t.find_chord_fingering(NoteContainer().from_chord('Am'))
        [[0, 0, 2, 2, 1, 0], [0, 3, 2, 2, 1, 0], ......]
        """"""

        def follow(string, next, name, prev=-1):
            """"""Follow the fret 'next' on 'string'; build result on the way.""""""
            if string >= len(self.tuning) - 1:
                return [[(next, name)]]
            result = []
            cur = res[string][next]
            if cur != []:
                for y in cur[1]:
                    for sub in follow(string + 1, y[0], y[1]):
                        if prev < 0:
                            result.append([(next, name)] + sub)
                        else:
                            if sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
                                result.append([(next, name)] + sub)
            for s in follow(string + 1, maxfret + 1, None, next):
                result.append([(next, name)] + s)
            return [[(next, name)]] if result == [] else result

        def make_lookup_table():
            """"""Prepare the lookup table.

            table[string][fret] = (name, dest_frets)
            """"""
            res = [[[] for x in range(maxfret + 2)] for x in range(len(self.tuning) - 1)]
            for x in range(0, len(self.tuning) - 1):
                addedNone = -1
                next = fretdict[x + 1]
                for (fret, name) in fretdict[x]:
                    for (f2, n2) in next:
                        if n2 != name and (f2 == 0 or abs(fret - f2) < max_distance):
                            if res[x][fret] != []:
                                res[x][fret][1].append((f2, n2))
                            else:
                                res[x][fret] = (name, [(f2, n2)])
                        if addedNone < x:
                            if res[x][maxfret + 1] != []:
                                res[x][maxfret + 1][1].append((f2, n2))
                            else:
                                res[x][maxfret + 1] = (None, [(f2, n2)])
                    addedNone = x
            return res

        # Convert to NoteContainer if necessary
        n = notes
        if notes != [] and isinstance(notes, list) and isinstance(notes[0], six.string_types):
            n = NoteContainer(notes)

        # Check number of note names.
        notenames = [x.name for x in n]
        if len(notenames) == 0 or len(notenames) > len(self.tuning):
            return []

        # Make string-fret dictionary
        fretdict = []
        for x in range(0, len(self.tuning)):
            fretdict.append(self.find_note_names(notes, x, maxfret))

        # Build table
        res = make_lookup_table()

        # Build result using table
        result = []

        # For each fret on the first string
        for (i, y) in enumerate(res[0]):
            if y != []:
                (yname, next) = (y[0], y[1])

                # For each destination fret in y
                for (fret, name) in next:

                    # For each followed result
                    for s in follow(1, fret, name):
                        subresult = [(i, yname)] + s

                        # Get boundaries
                        (mi, ma, names) = (1000, -1000, [])
                        for (f, n) in subresult:
                            if n is not None:
                                if f != 0 and f <= mi:
                                    mi = f
                                if f != 0 and f >= ma:
                                    ma = f
                                names.append(n)

                        # Enforce boundaries
                        if abs(ma - mi) < max_distance:
                            # Check if all note
                            # names are present
                            covered = True
                            for n in notenames:
                                if n not in names:
                                    covered = False

                            # Add to result
                            if covered and names != []:
                                result.append(
                                    [y[0] if y[1] is not None else y[1] for y in subresult]
                                )

        # Return semi-sorted list
        s = sorted(
            result,
            key=lambda x: sum([t if t is not None else 1000 for (i, t) in enumerate(x)]),
        )
        s = [a for a in s if fingers_needed(a) <= max_fingers]
        if not return_best_as_NoteContainer:
            return s
        else:
            rnotes = self.frets_to_NoteContainer(s[0])
            for (i, x) in enumerate(rnotes):
                if x.string < len(self.tuning) - 1:
                    if res[x.string][x.fret] != []:
                        rnotes[i].name = res[x.string][x.fret][0]
            return rnotes","for sub in follow(string + 1, y[0], y[1]):
    if prev < 0:
        result.append([(next, name)] + sub)
    elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
        result.append([(next, name)] + sub)","for sub in follow(string + 1, y[0], y[1]):
    ((sub_0_0, _, *sub_0_rsubmaining), *sub_rsubmaining) = sub
    if prev < 0:
        result.append([(next, name)] + sub)
    elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
        result.append([(next, name)] + sub)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: (e_0_0, _, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]",,,,,,,
PathPlanning,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPlanning/Search_based_Planning/Search_2D/D_star.py,https://github.com/zhm-real/PathPlanning/tree/master/Search_based_Planning/Search_2D/D_star.py,DStar,plot_visited$284,"def plot_visited(self, visited):
        color = ['gainsboro', 'lightgray', 'silver', 'darkgray',
                 'bisque', 'navajowhite', 'moccasin', 'wheat',
                 'powderblue', 'skyblue', 'lightskyblue', 'cornflowerblue']

        if self.count >= len(color) - 1:
            self.count = 0

        for x in visited:
            plt.plot(x[0], x[1], marker='s', color=color[self.count])","for x in visited:
    plt.plot(x[0], x[1], marker='s', color=color[self.count])","for x in visited:
    (x_0, x_1, *_) = x
    plt.plot(x[0], x[1], marker='s', color=color[self.count])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
OpsManage,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpsManage/service/mysql/mysql_base.py,https://github.com/welliamcao/OpsManage/tree/master/service/mysql/mysql_base.py,MySQLBase,get_global_status$87,"def get_global_status(self):
        dataList = []
        logs = self.execute_for_query(sql='show global variables;')
        for ds in logs[1]:
            data = {}
            data['value'] = ds[1]
            data['name'] = ds[0].capitalize()
            dataList.append(data)        
        return dataList","for ds in logs[1]:
    data = {}
    data['value'] = ds[1]
    data['name'] = ds[0].capitalize()
    dataList.append(data)","for ds in logs[1]:
    (ds_0, ds_1, *_) = ds
    data = {}
    data['value'] = ds[1]
    data['name'] = ds[0].capitalize()
    dataList.append(data)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
mimicry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mimicry/tests/modules/test_resblocks.py,https://github.com/kwotsin/mimicry/tree/master/tests/modules/test_resblocks.py,TestResBlocks,test_GBlock$12,"def test_GBlock(self):
        # Arguments
        num_classes_list = [0, 10]
        spectral_norm_list = [True, False]
        in_channels = 3
        out_channels = 8
        args_comb = product(num_classes_list, spectral_norm_list)

        for args in args_comb:
            num_classes = args[0]
            spectral_norm = args[1]

            if num_classes > 0:
                y = torch.ones((4, ), dtype=torch.int64)
            else:
                y = None

            gen_block_up = resblocks.GBlock(in_channels=in_channels,
                                            out_channels=out_channels,
                                            upsample=True,
                                            num_classes=num_classes,
                                            spectral_norm=spectral_norm)

            gen_block = resblocks.GBlock(in_channels=in_channels,
                                         out_channels=out_channels,
                                         upsample=False,
                                         num_classes=num_classes,
                                         spectral_norm=spectral_norm)

            gen_block_no_sc = resblocks.GBlock(in_channels=in_channels,
                                               out_channels=in_channels,
                                               upsample=False,
                                               num_classes=num_classes,
                                               spectral_norm=spectral_norm)

            assert gen_block_up(self.images, y).shape == (4, 8, 32, 32)
            assert gen_block(self.images, y).shape == (4, 8, 16, 16)
            assert gen_block_no_sc(self.images, y).shape == (4, 3, 16, 16)","for args in args_comb:
    num_classes = args[0]
    spectral_norm = args[1]
    if num_classes > 0:
        y = torch.ones((4,), dtype=torch.int64)
    else:
        y = None
    gen_block_up = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=True, num_classes=num_classes, spectral_norm=spectral_norm)
    gen_block = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)
    gen_block_no_sc = resblocks.GBlock(in_channels=in_channels, out_channels=in_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)
    assert gen_block_up(self.images, y).shape == (4, 8, 32, 32)
    assert gen_block(self.images, y).shape == (4, 8, 16, 16)
    assert gen_block_no_sc(self.images, y).shape == (4, 3, 16, 16)","for args in args_comb:
    (args_0, args_1, *_) = args
    num_classes = args[0]
    spectral_norm = args[1]
    if num_classes > 0:
        y = torch.ones((4,), dtype=torch.int64)
    else:
        y = None
    gen_block_up = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=True, num_classes=num_classes, spectral_norm=spectral_norm)
    gen_block = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)
    gen_block_no_sc = resblocks.GBlock(in_channels=in_channels, out_channels=in_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)
    assert gen_block_up(self.images, y).shape == (4, 8, 32, 32)
    assert gen_block(self.images, y).shape == (4, 8, 16, 16)
    assert gen_block_no_sc(self.images, y).shape == (4, 3, 16, 16)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
BracketHighlighter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BracketHighlighter/tests/validate_json_format.py,https://github.com/facelessuser/BracketHighlighter/tree/master/tests/validate_json_format.py,CheckJsonFormat,get_line$106,"def get_line(self, pt):
        """"""Get the line from char index.""""""

        line = None
        for r in self.line_range:
            if pt >= r[0] and pt <= r[1]:
                line = r[2]
                break
        return line","for r in self.line_range:
    if pt >= r[0] and pt <= r[1]:
        line = r[2]
        break","for r in self.line_range:
    (r_0, r_1, r_2, *_) = r
    if pt >= r[0] and pt <= r[1]:
        line = r[2]
        break","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
fonttools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/cffLib/specializer.py,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/cffLib/specializer.py,_GeneralizerDecombinerCommandsMap,hhcurveto$200,"def hhcurveto(args):
		if len(args) < 4 or len(args) % 4 > 1: raise ValueError(args)
		if len(args) % 2 == 1:
			yield ('rrcurveto', [args[1], args[0], args[2], args[3], args[4], 0])
			args = args[5:]
		for args in _everyN(args, 4):
			yield ('rrcurveto', [args[0], 0, args[1], args[2], args[3], 0])","for args in _everyN(args, 4):
    yield ('rrcurveto', [args[0], 0, args[1], args[2], args[3], 0])","for args in _everyN(args, 4):
    (args_0, args_1, args_2, args_3, *_) = args
    yield ('rrcurveto', [args[0], 0, args[1], args[2], args[3], 0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
joinmarket-clientserver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joinmarket-clientserver/scripts/snicker/snicker-recovery.py,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/scripts/snicker/snicker-recovery.py,,main$74,"def main():
    parser = OptionParser(
        usage=
        'usage: %prog [options] walletname',
        description=description
    )
    parser.add_option('-m', '--mixdepth', action='store', type='int',
                  dest='mixdepth', default=0,
                  help=""mixdepth to source coins from"")
    parser.add_option('-a',
                      '--amtmixdepths',
                      action='store',
                      type='int',
                      dest='amtmixdepths',
                      help='number of mixdepths in wallet, default 5',
                      default=5)
    parser.add_option('-g',
                      '--gap-limit',
                      type=""int"",
                      action='store',
                      dest='gaplimit',
                      help='gap limit for wallet, default=6',
                      default=6)
    add_base_options(parser)
    (options, args) = parser.parse_args()
    load_program_config(config_path=options.datadir)
    check_regtest()
    if len(args) != 1:
        log.error(""Invalid arguments, see --help"")
        sys.exit(EXIT_ARGERROR)
    wallet_name = args[0]
    wallet_path = get_wallet_path(wallet_name, None)
    max_mix_depth = max([options.mixdepth, options.amtmixdepths - 1])
    wallet = open_test_wallet_maybe(
        wallet_path, wallet_name, max_mix_depth,
        wallet_password_stdin=options.wallet_password_stdin,
        gap_limit=options.gaplimit)
    wallet_service = WalletService(wallet)

    # step 1: do a full recovery style sync. this will pick up
    # all addresses that we expect to match transactions against,
    # from a blank slate Core wallet that originally had no imports.
    if not options.recoversync:
        jmprint(""Recovery sync was not set, but using it anyway."")
    while not wallet_service.synced:
        wallet_service.sync_wallet(fast=False)
    # Note that the user may be interrupted above by the rescan
    # request; this is as for normal scripts; after the rescan is done
    # (usually, only once, but, this *IS* needed here, unlike a normal
    # wallet generation event), we just try again.

    # Now all address from HD are imported, we need to grab
    # all the transactions for those addresses; this includes txs
    # that *spend* as well as receive our coins, so will include
    # ""first-out"" SNICKER txs as well as ordinary spends and JM coinjoins.
    seed_transactions = wallet_service.get_all_transactions()

    # Search for SNICKER txs and add them if they match.
    # We proceed recursively; we find all one-out matches, then
    # all 2-out matches, until we find no new ones and stop.

    if len(seed_transactions) == 0:
        jmprint(""No transactions were found for this wallet. Did you rescan?"")
        return False
    
    new_txs = []
    current_block_heights = set()
    for tx in seed_transactions:
        if btc.is_snicker_tx(tx):
            jmprint(""Found a snicker tx: {}"".format(bintohex(tx.GetTxid()[::-1])))
            equal_outs = btc.get_equal_outs(tx)
            if not equal_outs:
                continue
            if all([wallet_service.is_known_script(
                x.scriptPubKey) == False for x in [a[1] for a in equal_outs]]):
                # it is now *very* likely that one of the two equal
                # outputs is our SNICKER custom output
                # script; notice that in this case, the transaction *must*
                # have spent our inputs, since it didn't recognize ownership
                # of either coinjoin output (and if it did recognize the change,
                # it would have recognized the cj output also).
                # We try to regenerate one of the outputs, but warn if
                # we can't.
                my_indices = get_pubs_and_indices_of_inputs(tx, wallet_service, ours=True)
                for mypub, mi in my_indices:
                    for eo in equal_outs:
                        for (other_pub, i) in get_pubs_and_indices_of_inputs(tx, wallet_service, ours=False):
                            for (our_pub, j) in get_pubs_and_indices_of_ancestor_inputs(tx.vin[mi], wallet_service, ours=True):
                                our_spk = wallet_service.pubkey_to_script(our_pub)
                                our_priv = wallet_service.get_key_from_addr(
                                    wallet_service.script_to_addr(our_spk))
                                tweak_bytes = btc.ecdh(our_priv[:-1], other_pub)
                                tweaked_pub = btc.snicker_pubkey_tweak(our_pub, tweak_bytes)
                                tweaked_spk = wallet_service.pubkey_to_script(tweaked_pub)
                                if tweaked_spk == eo[1].scriptPubKey:
                                    # TODO wallet.script_to_addr has a dubious assertion, that's why
                                    # we use btc method directly:
                                    address_found = str(btc.CCoinAddress.from_scriptPubKey(btc.CScript(tweaked_spk)))
                                    #address_found = wallet_service.script_to_addr(tweaked_spk)
                                    jmprint(""Found a new SNICKER output belonging to us."")
                                    jmprint(""Output address {} in the following transaction:"".format(
                                        address_found))
                                    jmprint(btc.human_readable_transaction(tx))
                                    jmprint(""Importing the address into the joinmarket wallet..."")
                                    # NB for a recovery we accept putting any imported keys all into
                                    # the same mixdepth (0); TODO investigate correcting this, it will
                                    # be a little complicated.
                                    success, msg = wallet_service.check_tweak_matches_and_import(wallet_service.script_to_addr(our_spk),
                                                tweak_bytes, tweaked_pub, wallet_service.mixdepth)
                                    if not success:
                                        jmprint(""Failed to import SNICKER key: {}"".format(msg), ""error"")
                                        return False
                                    else:
                                        jmprint(""... success."")
                                    # we want the blockheight to track where the next-round rescan
                                    # must start from
                                    current_block_heights.add(wallet_service.get_transaction_block_height(tx))
                                    # add this transaction to the next round.
                                    new_txs.append(tx)
    if len(new_txs) == 0:
        return True
    seed_transactions.extend(new_txs)
    earliest_new_blockheight = min(current_block_heights)
    jmprint(""New SNICKER addresses were imported to the Core wallet; ""
            ""do rescanblockchain again, starting from block {}, before ""
            ""restarting this script."".format(earliest_new_blockheight))
    return False","for eo in equal_outs:
    for (other_pub, i) in get_pubs_and_indices_of_inputs(tx, wallet_service, ours=False):
        for (our_pub, j) in get_pubs_and_indices_of_ancestor_inputs(tx.vin[mi], wallet_service, ours=True):
            our_spk = wallet_service.pubkey_to_script(our_pub)
            our_priv = wallet_service.get_key_from_addr(wallet_service.script_to_addr(our_spk))
            tweak_bytes = btc.ecdh(our_priv[:-1], other_pub)
            tweaked_pub = btc.snicker_pubkey_tweak(our_pub, tweak_bytes)
            tweaked_spk = wallet_service.pubkey_to_script(tweaked_pub)
            if tweaked_spk == eo[1].scriptPubKey:
                address_found = str(btc.CCoinAddress.from_scriptPubKey(btc.CScript(tweaked_spk)))
                jmprint('Found a new SNICKER output belonging to us.')
                jmprint('Output address {} in the following transaction:'.format(address_found))
                jmprint(btc.human_readable_transaction(tx))
                jmprint('Importing the address into the joinmarket wallet...')
                (success, msg) = wallet_service.check_tweak_matches_and_import(wallet_service.script_to_addr(our_spk), tweak_bytes, tweaked_pub, wallet_service.mixdepth)
                if not success:
                    jmprint('Failed to import SNICKER key: {}'.format(msg), 'error')
                    return False
                else:
                    jmprint('... success.')
                current_block_heights.add(wallet_service.get_transaction_block_height(tx))
                new_txs.append(tx)","for eo in equal_outs:
    (_, eo_1, *eo_reomaining) = eo
    for (other_pub, i) in get_pubs_and_indices_of_inputs(tx, wallet_service, ours=False):
        for (our_pub, j) in get_pubs_and_indices_of_ancestor_inputs(tx.vin[mi], wallet_service, ours=True):
            our_spk = wallet_service.pubkey_to_script(our_pub)
            our_priv = wallet_service.get_key_from_addr(wallet_service.script_to_addr(our_spk))
            tweak_bytes = btc.ecdh(our_priv[:-1], other_pub)
            tweaked_pub = btc.snicker_pubkey_tweak(our_pub, tweak_bytes)
            tweaked_spk = wallet_service.pubkey_to_script(tweaked_pub)
            if tweaked_spk == eo[1].scriptPubKey:
                address_found = str(btc.CCoinAddress.from_scriptPubKey(btc.CScript(tweaked_spk)))
                jmprint('Found a new SNICKER output belonging to us.')
                jmprint('Output address {} in the following transaction:'.format(address_found))
                jmprint(btc.human_readable_transaction(tx))
                jmprint('Importing the address into the joinmarket wallet...')
                (success, msg) = wallet_service.check_tweak_matches_and_import(wallet_service.script_to_addr(our_spk), tweak_bytes, tweaked_pub, wallet_service.mixdepth)
                if not success:
                    jmprint('Failed to import SNICKER key: {}'.format(msg), 'error')
                    return False
                else:
                    jmprint('... success.')
                current_block_heights.add(wallet_service.get_transaction_block_height(tx))
                new_txs.append(tx)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
pywikibot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pywikibot/pywikibot/site/_apisite.py,https://github.com/wikimedia/pywikibot/tree/master/pywikibot/site/_apisite.py,APISite,movepage$1923,"def movepage(self, page, newtitle: str, summary, movetalk=True,
                 noredirect=False):
        """"""Move a Page to a new title.

        :see: https://www.mediawiki.org/wiki/API:Move

        :param page: the Page to be moved (must exist)
        :param newtitle: the new title for the Page
        :param summary: edit summary (required!)
        :param movetalk: if True (default), also move the talk page if possible
        :param noredirect: if True, suppress creation of a redirect from the
            old title to the new one
        :return: Page object with the new title
        :rtype: pywikibot.Page
        """"""
        oldtitle = page.title(with_section=False)
        newlink = pywikibot.Link(newtitle, self)
        newpage = pywikibot.Page(newlink)
        if newlink.namespace:
            newtitle = self.namespace(newlink.namespace) + ':' + newlink.title
        else:
            newtitle = newlink.title
        if oldtitle == newtitle:
            raise Error('Cannot move page {} to its own title.'
                        .format(oldtitle))
        if not page.exists():
            raise NoPageError(page,
                              'Cannot move page {page} because it '
                              'does not exist on {site}.')
        token = self.tokens['move']
        self.lock_page(page)
        req = self._simple_request(action='move',
                                   noredirect=noredirect,
                                   reason=summary,
                                   movetalk=movetalk,
                                   token=token,
                                   to=newtitle)
        req['from'] = oldtitle  # ""from"" is a python keyword
        try:
            result = req.submit()
            pywikibot.debug('movepage response: {}'.format(result),
                            _logger)
        except APIError as err:
            if err.code.endswith('anon') and self.logged_in():
                pywikibot.debug(
                    ""movepage: received '{}' even though bot is logged in""
                    .format(err.code),
                    _logger)
            if err.code in self._mv_errors:
                on_error = self._mv_errors[err.code]
                if hasattr(on_error, 'exception'):
                    # LockedPageError can be raised both if ""from"" or ""to"" page
                    # are locked for the user.
                    # Both pages locked is not considered
                    # (a double failure has low probability)
                    if issubclass(on_error.exception, LockedPageError):
                        # we assume ""from"" is locked unless proven otherwise
                        failed_page = page
                        if newpage.exists():
                            for prot in self.page_restrictions(
                                    newpage).values():
                                if not self.has_group(prot[0]):
                                    failed_page = newpage
                                    break
                    else:
                        failed_page = newpage if on_error.on_new_page else page
                    raise on_error.exception(failed_page) from None

                errdata = {
                    'site': self,
                    'oldtitle': oldtitle,
                    'oldnamespace': self.namespace(page.namespace()),
                    'newtitle': newtitle,
                    'newnamespace': self.namespace(newlink.namespace),
                    'user': self.user(),
                }

                raise Error(on_error.format_map(errdata)) from None

            pywikibot.debug(""movepage: Unexpected error code '{}' received.""
                            .format(err.code),
                            _logger)
            raise
        finally:
            self.unlock_page(page)
        if 'move' not in result:
            pywikibot.error('movepage: {}'.format(result))
            raise Error('movepage: unexpected response')
        # TODO: Check for talkmove-error messages
        if 'talkmove-error-code' in result['move']:
            pywikibot.warning(
                'movepage: Talk page {} not moved'
                .format(page.toggleTalkPage().title(as_link=True)))
        return pywikibot.Page(page, newtitle)","for prot in self.page_restrictions(newpage).values():
    if not self.has_group(prot[0]):
        failed_page = newpage
        break","for prot in self.page_restrictions(newpage).values():
    (prot_0, *prot_rprotmaining) = prot
    if not self.has_group(prot[0]):
        failed_page = newpage
        break","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
s3prl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3prl/s3prl/downstream/voxceleb2_amsoftmax_segment_eval/dataset.py,https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/voxceleb2_amsoftmax_segment_eval/dataset.py,SpeakerVerifi_dev,segment_processing$162,"def segment_processing(self):
        wav_list = self.pair_dict['wav_table']
        utterance_id = 0
        segment_list = []
        print(""processing test set to segments"")
        for wav_info in tqdm.tqdm(wav_list):
            label_info = wav_info[0]
            pair_info = wav_info[1]

            wav, _ = apply_effects_file(wav_info[2], EFFECTS)
            wav = wav.squeeze(0)

            index_end = len(wav) -self.segment_config[""window""]
            segment_num = index_end // self.segment_config['stride']

            if index_end < 0:
                segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), wav_info[2]])
            else:
                for index in range(0, index_end, self.segment_config['stride']):
                    segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index+self.segment_config['window'], wav_info[2]])

            utterance_id += 1
            
        return segment_list","for wav_info in tqdm.tqdm(wav_list):
    label_info = wav_info[0]
    pair_info = wav_info[1]
    (wav, _) = apply_effects_file(wav_info[2], EFFECTS)
    wav = wav.squeeze(0)
    index_end = len(wav) - self.segment_config['window']
    segment_num = index_end // self.segment_config['stride']
    if index_end < 0:
        segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), wav_info[2]])
    else:
        for index in range(0, index_end, self.segment_config['stride']):
            segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index + self.segment_config['window'], wav_info[2]])
    utterance_id += 1","for wav_info in tqdm.tqdm(wav_list):
    (wav_info_0, wav_info_1, wav_info_2, *_) = wav_info
    label_info = wav_info[0]
    pair_info = wav_info[1]
    (wav, _) = apply_effects_file(wav_info[2], EFFECTS)
    wav = wav.squeeze(0)
    index_end = len(wav) - self.segment_config['window']
    segment_num = index_end // self.segment_config['stride']
    if index_end < 0:
        segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), wav_info[2]])
    else:
        for index in range(0, index_end, self.segment_config['stride']):
            segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index + self.segment_config['window'], wav_info[2]])
    utterance_id += 1","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/printing/pretty/pretty.py,https://github.com/sympy/sympy/tree/master/sympy/printing/pretty/pretty.py,PrettyPrinter,_print_Integral$426,"def _print_Integral(self, integral):
        f = integral.function

        # Add parentheses if arg involves addition of terms and
        # create a pretty form for the argument
        prettyF = self._print(f)
        # XXX generalize parens
        if f.is_Add:
            prettyF = prettyForm(*prettyF.parens())

        # dx dy dz ...
        arg = prettyF
        for x in integral.limits:
            prettyArg = self._print(x[0])
            # XXX qparens (parens if needs-parens)
            if prettyArg.width() > 1:
                prettyArg = prettyForm(*prettyArg.parens())

            arg = prettyForm(*arg.right(' d', prettyArg))

        # \int \int \int ...
        firstterm = True
        s = None
        for lim in integral.limits:
            # Create bar based on the height of the argument
            h = arg.height()
            H = h + 2

            # XXX hack!
            ascii_mode = not self._use_unicode
            if ascii_mode:
                H += 2

            vint = vobj('int', H)

            # Construct the pretty form with the integral sign and the argument
            pform = prettyForm(vint)
            pform.baseline = arg.baseline + (
                H - h)//2    # covering the whole argument

            if len(lim) > 1:
                # Create pretty forms for endpoints, if definite integral.
                # Do not print empty endpoints.
                if len(lim) == 2:
                    prettyA = prettyForm("""")
                    prettyB = self._print(lim[1])
                if len(lim) == 3:
                    prettyA = self._print(lim[1])
                    prettyB = self._print(lim[2])

                if ascii_mode:  # XXX hack
                    # Add spacing so that endpoint can more easily be
                    # identified with the correct integral sign
                    spc = max(1, 3 - prettyB.width())
                    prettyB = prettyForm(*prettyB.left(' ' * spc))

                    spc = max(1, 4 - prettyA.width())
                    prettyA = prettyForm(*prettyA.right(' ' * spc))

                pform = prettyForm(*pform.above(prettyB))
                pform = prettyForm(*pform.below(prettyA))

            if not ascii_mode:  # XXX hack
                pform = prettyForm(*pform.right(' '))

            if firstterm:
                s = pform   # first term
                firstterm = False
            else:
                s = prettyForm(*s.left(pform))

        pform = prettyForm(*arg.left(s))
        pform.binding = prettyForm.MUL
        return pform","for x in integral.limits:
    prettyArg = self._print(x[0])
    if prettyArg.width() > 1:
        prettyArg = prettyForm(*prettyArg.parens())
    arg = prettyForm(*arg.right(' d', prettyArg))","for x in integral.limits:
    (x_0, *x_rxmaining) = x
    prettyArg = self._print(x[0])
    if prettyArg.width() > 1:
        prettyArg = prettyForm(*prettyArg.parens())
    arg = prettyForm(*arg.right(' d', prettyArg))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/printing/pretty/pretty.py,https://github.com/sympy/sympy/tree/master/sympy/printing/pretty/pretty.py,PrettyPrinter,_print_Integral$426,"def _print_Integral(self, integral):
        f = integral.function

        # Add parentheses if arg involves addition of terms and
        # create a pretty form for the argument
        prettyF = self._print(f)
        # XXX generalize parens
        if f.is_Add:
            prettyF = prettyForm(*prettyF.parens())

        # dx dy dz ...
        arg = prettyF
        for x in integral.limits:
            prettyArg = self._print(x[0])
            # XXX qparens (parens if needs-parens)
            if prettyArg.width() > 1:
                prettyArg = prettyForm(*prettyArg.parens())

            arg = prettyForm(*arg.right(' d', prettyArg))

        # \int \int \int ...
        firstterm = True
        s = None
        for lim in integral.limits:
            # Create bar based on the height of the argument
            h = arg.height()
            H = h + 2

            # XXX hack!
            ascii_mode = not self._use_unicode
            if ascii_mode:
                H += 2

            vint = vobj('int', H)

            # Construct the pretty form with the integral sign and the argument
            pform = prettyForm(vint)
            pform.baseline = arg.baseline + (
                H - h)//2    # covering the whole argument

            if len(lim) > 1:
                # Create pretty forms for endpoints, if definite integral.
                # Do not print empty endpoints.
                if len(lim) == 2:
                    prettyA = prettyForm("""")
                    prettyB = self._print(lim[1])
                if len(lim) == 3:
                    prettyA = self._print(lim[1])
                    prettyB = self._print(lim[2])

                if ascii_mode:  # XXX hack
                    # Add spacing so that endpoint can more easily be
                    # identified with the correct integral sign
                    spc = max(1, 3 - prettyB.width())
                    prettyB = prettyForm(*prettyB.left(' ' * spc))

                    spc = max(1, 4 - prettyA.width())
                    prettyA = prettyForm(*prettyA.right(' ' * spc))

                pform = prettyForm(*pform.above(prettyB))
                pform = prettyForm(*pform.below(prettyA))

            if not ascii_mode:  # XXX hack
                pform = prettyForm(*pform.right(' '))

            if firstterm:
                s = pform   # first term
                firstterm = False
            else:
                s = prettyForm(*s.left(pform))

        pform = prettyForm(*arg.left(s))
        pform.binding = prettyForm.MUL
        return pform","for lim in integral.limits:
    h = arg.height()
    H = h + 2
    ascii_mode = not self._use_unicode
    if ascii_mode:
        H += 2
    vint = vobj('int', H)
    pform = prettyForm(vint)
    pform.baseline = arg.baseline + (H - h) // 2
    if len(lim) > 1:
        if len(lim) == 2:
            prettyA = prettyForm('')
            prettyB = self._print(lim[1])
        if len(lim) == 3:
            prettyA = self._print(lim[1])
            prettyB = self._print(lim[2])
        if ascii_mode:
            spc = max(1, 3 - prettyB.width())
            prettyB = prettyForm(*prettyB.left(' ' * spc))
            spc = max(1, 4 - prettyA.width())
            prettyA = prettyForm(*prettyA.right(' ' * spc))
        pform = prettyForm(*pform.above(prettyB))
        pform = prettyForm(*pform.below(prettyA))
    if not ascii_mode:
        pform = prettyForm(*pform.right(' '))
    if firstterm:
        s = pform
        firstterm = False
    else:
        s = prettyForm(*s.left(pform))","for lim in integral.limits:
    (_, lim_1, lim_2, *lim_rlimmaining) = lim
    h = arg.height()
    H = h + 2
    ascii_mode = not self._use_unicode
    if ascii_mode:
        H += 2
    vint = vobj('int', H)
    pform = prettyForm(vint)
    pform.baseline = arg.baseline + (H - h) // 2
    if len(lim) > 1:
        if len(lim) == 2:
            prettyA = prettyForm('')
            prettyB = self._print(lim[1])
        if len(lim) == 3:
            prettyA = self._print(lim[1])
            prettyB = self._print(lim[2])
        if ascii_mode:
            spc = max(1, 3 - prettyB.width())
            prettyB = prettyForm(*prettyB.left(' ' * spc))
            spc = max(1, 4 - prettyA.width())
            prettyA = prettyForm(*prettyA.right(' ' * spc))
        pform = prettyForm(*pform.above(prettyB))
        pform = prettyForm(*pform.below(prettyA))
    if not ascii_mode:
        pform = prettyForm(*pform.right(' '))
    if firstterm:
        s = pform
        firstterm = False
    else:
        s = prettyForm(*s.left(pform))",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for vuln_info in vuln_infos:
    if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif vuln_info[3] == 'INFO':
        sql = ""select * from js_file where id='%s'"" % vuln_info[2]
        cursor.execute(sql)
        js_infos = cursor.fetchall()
        for js_info in js_infos:
            js_name = js_info[1]
            js_path = js_info[2]
            para2 = para1.insert_paragraph_before('')
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
            run2.font.name = 'Arial'
            run2.font.size = Pt(10)
            run2.font.bold = True
            run3 = para2.add_run(js_path)
            run3.font.name = 'Arial'
            run3.font.size = Pt(10)
            run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
            run4.font.name = 'Arial'
            run4.font.size = Pt(10)
            run4.font.bold = True
            run5 = para2.add_run(vuln_info[8])
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
            run6.font.name = 'Arial'
            run6.font.size = Pt(10)
            run6.font.bold = True
            self.creat_num = self.creat_num + 1
            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
    elif vuln_info[3] == 'CORS':
        sql = ""select vaule from info where name='%s'"" % 'host'
        cursor.execute(sql)
        infos = cursor.fetchall()
        for info in infos:
            api_path = info[0]
        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
        para3 = para2.insert_paragraph_before('')
        run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
        run5.font.name = 'Arial'
        run5.font.size = Pt(16)
        run5.font.bold = True
        run6 = para3.add_run('网址:')
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        run7 = para3.add_run(api_path)
        run7.font.name = 'Arial'
        run7.font.size = Pt(10)
        run8 = para3.add_run('\n' + '{response_head}')
        run8.font.name = 'Arial'
        run8.font.size = Pt(10)
        run8.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
        run9 = para2.add_run('\n' + '{request_head}')
        run9.font.name = 'Arial'
        run9.font.size = Pt(10)
        run9.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","for vuln_info in vuln_infos:
    (vuln_info_1, vuln_info_2, vuln_info_3, vuln_info_4, vuln_info_5, vuln_info_6, vuln_info_7, vuln_info_8, *_) = vuln_info
    if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif vuln_info[3] == 'INFO':
        sql = ""select * from js_file where id='%s'"" % vuln_info[2]
        cursor.execute(sql)
        js_infos = cursor.fetchall()
        for js_info in js_infos:
            js_name = js_info[1]
            js_path = js_info[2]
            para2 = para1.insert_paragraph_before('')
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
            run2.font.name = 'Arial'
            run2.font.size = Pt(10)
            run2.font.bold = True
            run3 = para2.add_run(js_path)
            run3.font.name = 'Arial'
            run3.font.size = Pt(10)
            run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
            run4.font.name = 'Arial'
            run4.font.size = Pt(10)
            run4.font.bold = True
            run5 = para2.add_run(vuln_info[8])
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
            run6.font.name = 'Arial'
            run6.font.size = Pt(10)
            run6.font.bold = True
            self.creat_num = self.creat_num + 1
            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
    elif vuln_info[3] == 'CORS':
        sql = ""select vaule from info where name='%s'"" % 'host'
        cursor.execute(sql)
        infos = cursor.fetchall()
        for info in infos:
            api_path = info[0]
        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
        para3 = para2.insert_paragraph_before('')
        run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
        run5.font.name = 'Arial'
        run5.font.size = Pt(16)
        run5.font.bold = True
        run6 = para3.add_run('网址:')
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        run7 = para3.add_run(api_path)
        run7.font.name = 'Arial'
        run7.font.size = Pt(10)
        run8 = para3.add_run('\n' + '{response_head}')
        run8.font.name = 'Arial'
        run8.font.size = Pt(10)
        run8.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
        run9 = para2.add_run('\n' + '{request_head}')
        run9.font.name = 'Arial'
        run9.font.size = Pt(10)
        run9.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",0,,,"Answer: Yes
Iterable Unpacking: e_1, e_2, e_3, e_4, e_5, e_6, e_7, e_8 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]
e_5: e[5]
e_6: e[6]
e_7: e[7]
e_8: e[8]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for vuln_info in vuln_infos:
    if vuln_info[3] == 'passWord':
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
            para3 = para2.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para3.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para3.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num1 = self.creat_num1 + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
    elif vuln_info[3] == 'BAC':
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
            para3 = para2.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para3.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para3.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num1 = self.creat_num1 + 1
                info1 = '请求内容1: ' + vuln_info[5].split('§§§')[0] + '\n\n' + '请求内容2: ' + vuln_info[5].split('§§§')[1]
                info2 = '响应内容1: ' + vuln_info[6].split('§§§')[0] + '\n\n' + '响应内容2: ' + vuln_info[6].split('§§§')[1]
                Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
    elif vuln_info[3] == 'upLoad':
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
            para3 = para2.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para3.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para3.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num1 = self.creat_num1 + 1
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
    elif vuln_info[3] == 'SQL':
        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
        para3 = para2.insert_paragraph_before('')
        UserLogin = api_info[2]
        api_path = api_info[1]
        run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
        run.font.name = 'Arial'
        run.font.size = Pt(16)
        run.font.bold = True
        sql = ""select path from js_file where id='%s'"" % vuln_info[2]
        cursor.execute(sql)
        js_paths = cursor.fetchall()
        for js_path in js_paths:
            run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
            run2.font.name = 'Arial'
            run2.font.size = Pt(10)
            run2.font.bold = True
            run3 = para3.add_run(api_path)
            run3.font.name = 'Arial'
            run3.font.size = Pt(10)
            run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
            run4.font.name = 'Arial'
            run4.font.size = Pt(10)
            run4.font.bold = True
            run5 = para3.add_run(js_path[0])
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run5.font.bold = True
            self.creat_num1 = self.creat_num1 + 1
            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
            run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
            run6.font.name = 'Arial'
            run6.font.size = Pt(10)
            run6.font.bold = True
            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","for vuln_info in vuln_infos:
    (_, vuln_info_1, vuln_info_2, vuln_info_3, _, vuln_info_5, vuln_info_6, *_) = vuln_info
    if vuln_info[3] == 'passWord':
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
            para3 = para2.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para3.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para3.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num1 = self.creat_num1 + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
    elif vuln_info[3] == 'BAC':
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
            para3 = para2.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para3.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para3.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num1 = self.creat_num1 + 1
                info1 = '请求内容1: ' + vuln_info[5].split('§§§')[0] + '\n\n' + '请求内容2: ' + vuln_info[5].split('§§§')[1]
                info2 = '响应内容1: ' + vuln_info[6].split('§§§')[0] + '\n\n' + '响应内容2: ' + vuln_info[6].split('§§§')[1]
                Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
    elif vuln_info[3] == 'upLoad':
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
            para3 = para2.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para3.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para3.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num1 = self.creat_num1 + 1
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
    elif vuln_info[3] == 'SQL':
        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
        para3 = para2.insert_paragraph_before('')
        UserLogin = api_info[2]
        api_path = api_info[1]
        run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
        run.font.name = 'Arial'
        run.font.size = Pt(16)
        run.font.bold = True
        sql = ""select path from js_file where id='%s'"" % vuln_info[2]
        cursor.execute(sql)
        js_paths = cursor.fetchall()
        for js_path in js_paths:
            run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
            run2.font.name = 'Arial'
            run2.font.size = Pt(10)
            run2.font.bold = True
            run3 = para3.add_run(api_path)
            run3.font.name = 'Arial'
            run3.font.size = Pt(10)
            run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
            run4.font.name = 'Arial'
            run4.font.size = Pt(10)
            run4.font.bold = True
            run5 = para3.add_run(js_path[0])
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run5.font.bold = True
            self.creat_num1 = self.creat_num1 + 1
            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
            run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
            run6.font.name = 'Arial'
            run6.font.size = Pt(10)
            run6.font.bold = True
            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, e_3, _, e_5, e_6 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_5: e[5]
e_6: e[6]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for api_info in api_infos:
    para2 = para1.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para2.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para2.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num = self.creat_num + 1
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","for api_info in api_infos:
    (_, api_info_1, api_info_2, *api_info_rapi_infomaining) = api_info
    para2 = para1.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para2.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para2.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num = self.creat_num + 1
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for api_info in api_infos:
    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
    para3 = para2.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para3.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para3.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num1 = self.creat_num1 + 1
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)","for api_info in api_infos:
    (_, api_info_1, api_info_2, *api_info_rapi_infomaining) = api_info
    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
    para3 = para2.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para3.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para3.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num1 = self.creat_num1 + 1
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for js_path in js_paths:
    run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para2.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para2.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num = self.creat_num + 1
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","for js_path in js_paths:
    (js_path_0, *js_path_rjs_pathmaining) = js_path
    run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para2.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para2.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num = self.creat_num + 1
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for api_info in api_infos:
    para2 = para1.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para2.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para2.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num = self.creat_num + 1
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","for api_info in api_infos:
    (_, api_info_1, api_info_2, *api_info_rapi_infomaining) = api_info
    para2 = para1.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para2.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para2.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num = self.creat_num + 1
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for js_path in js_paths:
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    run6 = para2.add_run(Utils().getMyWord('{request_info}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)","for js_path in js_paths:
    (js_path_0, *js_path_rjs_pathmaining) = js_path
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    run6 = para2.add_run(Utils().getMyWord('{request_info}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for api_info in api_infos:
    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
    para3 = para2.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para3.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para3.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num1 = self.creat_num1 + 1
        info1 = '请求内容1: ' + vuln_info[5].split('§§§')[0] + '\n\n' + '请求内容2: ' + vuln_info[5].split('§§§')[1]
        info2 = '响应内容1: ' + vuln_info[6].split('§§§')[0] + '\n\n' + '响应内容2: ' + vuln_info[6].split('§§§')[1]
        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)","for api_info in api_infos:
    (_, api_info_1, api_info_2, *api_info_rapi_infomaining) = api_info
    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
    para3 = para2.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para3.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para3.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num1 = self.creat_num1 + 1
        info1 = '请求内容1: ' + vuln_info[5].split('§§§')[0] + '\n\n' + '请求内容2: ' + vuln_info[5].split('§§§')[1]
        info2 = '响应内容1: ' + vuln_info[6].split('§§§')[0] + '\n\n' + '响应内容2: ' + vuln_info[6].split('§§§')[1]
        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for js_path in js_paths:
    run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para2.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para2.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num = self.creat_num + 1
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","for js_path in js_paths:
    (js_path_0, *js_path_rjs_pathmaining) = js_path
    run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para2.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para2.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num = self.creat_num + 1
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for js_info in js_infos:
    js_name = js_info[1]
    js_path = js_info[2]
    para2 = para1.insert_paragraph_before('')
    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para2.add_run(js_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para2.add_run(vuln_info[8])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    self.creat_num = self.creat_num + 1
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)","for js_info in js_infos:
    (_, js_info_1, js_info_2, *js_info_rjs_infomaining) = js_info
    js_name = js_info[1]
    js_path = js_info[2]
    para2 = para1.insert_paragraph_before('')
    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para2.add_run(js_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para2.add_run(vuln_info[8])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    self.creat_num = self.creat_num + 1
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for js_path in js_paths:
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    info1 = '请求内容1: ' + vuln_info[5].split('§§§')[0] + '\n\n' + '请求内容2: ' + vuln_info[5].split('§§§')[1]
    info2 = '响应内容1: ' + vuln_info[6].split('§§§')[0] + '\n\n' + '响应内容2: ' + vuln_info[6].split('§§§')[1]
    Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)","for js_path in js_paths:
    (js_path_0, *js_path_rjs_pathmaining) = js_path
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    info1 = '请求内容1: ' + vuln_info[5].split('§§§')[0] + '\n\n' + '请求内容2: ' + vuln_info[5].split('§§§')[1]
    info2 = '响应内容1: ' + vuln_info[6].split('§§§')[0] + '\n\n' + '响应内容2: ' + vuln_info[6].split('§§§')[1]
    Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for api_info in api_infos:
    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
    para3 = para2.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para3.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para3.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num1 = self.creat_num1 + 1
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","for api_info in api_infos:
    (_, api_info_1, api_info_2, *api_info_rapi_infomaining) = api_info
    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
    para3 = para2.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para3.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para3.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num1 = self.creat_num1 + 1
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for info in infos:
    api_path = info[0]","for info in infos:
    (info_0, *info_rinfomaining) = info
    api_path = info[0]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for js_path in js_paths:
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","for js_path in js_paths:
    (js_path_0, *js_path_rjs_pathmaining) = js_path
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""网址:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""网址:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""请求头:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""响应头:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""请求内容1: "" + vuln_info[5].split(""§§§"")[0] + ""\n\n"" + ""请求内容2: "" + vuln_info[5].split(""§§§"")[1]
                            info2 = ""响应内容1: "" + vuln_info[6].split(""§§§"")[0] + ""\n\n"" + ""响应内容2: "" + vuln_info[6].split(""§§§"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail模块正常"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for js_path in js_paths:
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","for js_path in js_paths:
    (js_path_0, *js_path_rjs_pathmaining) = js_path
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/slicemesh.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/slicemesh.py,Tool,slice$211,"def slice(self, verts, faces, z, zout=None, axis=""z""):
        tags = ""[slice]""
        if axis == ""z"":
            tags = f""[slice,minz:{float(z):f}]""
        block = Block(f""slice {axis}{float(z):f} {tags}"")

        # FIXME: slice along different axes
        if axis == ""x"":
            plane_orig = (z, 0, 0)
            plane_norm = (1, 0, 0)
        elif axis == ""y"":
            plane_orig = (0, z, 0)
            plane_norm = (0, 1, 0)
        else:
            plane_orig = (0, 0, z)  # z height to slice
            plane_norm = (0, 0, 1)

        # Crosscut
        contours = meshcut.cross_section(verts, faces, plane_orig, plane_norm)

        # Flatten contours
        if zout is not None:
            for contour in contours:
                for segment in contour:
                    segment[2] = zout

        # Contours to G-code
        for contour in contours:
            gtype = 0
            for segment in contour:
                block.append(
                    f""g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}""
                )
                gtype = 1
            block.append(
                f""g1 x{contour[0][0]:f} y{contour[0][1]:f} z{contour[0][2]:f}""
            )  # Close shape
            block.append(""( ---------- cut-here ---------- )"")
        if block:
            del block[-1]

        if not block:
            block = None
        return block","for contour in contours:
    gtype = 0
    for segment in contour:
        block.append(f'g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}')
        gtype = 1
    block.append(f'g1 x{contour[0][0]:f} y{contour[0][1]:f} z{contour[0][2]:f}')
    block.append('( ---------- cut-here ---------- )')","for contour in contours:
    ((contour_0_0, contour_0_1, contour_0_2, *contour_0_rcontourmaining), *contour_rcontourmaining) = contour
    gtype = 0
    for segment in contour:
        block.append(f'g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}')
        gtype = 1
    block.append(f'g1 x{contour[0][0]:f} y{contour[0][1]:f} z{contour[0][2]:f}')
    block.append('( ---------- cut-here ---------- )')",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: (e_0_0, e_0_1, e_0_2, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]
e_0_1: e[0][1]
e_0_2: e[0][2]",,,,,,,
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/slicemesh.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/slicemesh.py,Tool,slice$211,"def slice(self, verts, faces, z, zout=None, axis=""z""):
        tags = ""[slice]""
        if axis == ""z"":
            tags = f""[slice,minz:{float(z):f}]""
        block = Block(f""slice {axis}{float(z):f} {tags}"")

        # FIXME: slice along different axes
        if axis == ""x"":
            plane_orig = (z, 0, 0)
            plane_norm = (1, 0, 0)
        elif axis == ""y"":
            plane_orig = (0, z, 0)
            plane_norm = (0, 1, 0)
        else:
            plane_orig = (0, 0, z)  # z height to slice
            plane_norm = (0, 0, 1)

        # Crosscut
        contours = meshcut.cross_section(verts, faces, plane_orig, plane_norm)

        # Flatten contours
        if zout is not None:
            for contour in contours:
                for segment in contour:
                    segment[2] = zout

        # Contours to G-code
        for contour in contours:
            gtype = 0
            for segment in contour:
                block.append(
                    f""g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}""
                )
                gtype = 1
            block.append(
                f""g1 x{contour[0][0]:f} y{contour[0][1]:f} z{contour[0][2]:f}""
            )  # Close shape
            block.append(""( ---------- cut-here ---------- )"")
        if block:
            del block[-1]

        if not block:
            block = None
        return block","for segment in contour:
    block.append(f'g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}')
    gtype = 1","for segment in contour:
    (segment_0, segment_1, segment_2, *_) = segment
    block.append(f'g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}')
    gtype = 1","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/slicemesh.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/slicemesh.py,Tool,slice$211,"def slice(self, verts, faces, z, zout=None, axis=""z""):
        tags = ""[slice]""
        if axis == ""z"":
            tags = f""[slice,minz:{float(z):f}]""
        block = Block(f""slice {axis}{float(z):f} {tags}"")

        # FIXME: slice along different axes
        if axis == ""x"":
            plane_orig = (z, 0, 0)
            plane_norm = (1, 0, 0)
        elif axis == ""y"":
            plane_orig = (0, z, 0)
            plane_norm = (0, 1, 0)
        else:
            plane_orig = (0, 0, z)  # z height to slice
            plane_norm = (0, 0, 1)

        # Crosscut
        contours = meshcut.cross_section(verts, faces, plane_orig, plane_norm)

        # Flatten contours
        if zout is not None:
            for contour in contours:
                for segment in contour:
                    segment[2] = zout

        # Contours to G-code
        for contour in contours:
            gtype = 0
            for segment in contour:
                block.append(
                    f""g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}""
                )
                gtype = 1
            block.append(
                f""g1 x{contour[0][0]:f} y{contour[0][1]:f} z{contour[0][2]:f}""
            )  # Close shape
            block.append(""( ---------- cut-here ---------- )"")
        if block:
            del block[-1]

        if not block:
            block = None
        return block","for segment in contour:
    segment[2] = zout","for segment in contour:
    (_, _, segment_2, *segment_rsegmentmaining) = segment
    segment[2] = zout",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, _, e_2, *e_remaining = e
variable mapping:
e_2: e[2]",,,,,,,
mindmeld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mindmeld/mindmeld/models/taggers/taggers.py,https://github.com/cisco/mindmeld/tree/master/mindmeld/models/taggers/taggers.py,,_contains_O$461,"def _contains_O(entity):
    """"""Returns true if there is an O tag in the list of tokens we are considering
    as an entity""""""
    for token in entity:
        if token[0] == O_TAG:
            return True
    return False","for token in entity:
    if token[0] == O_TAG:
        return True","for token in entity:
    (token_0, *token_rtokenmaining) = token
    if token[0] == O_TAG:
        return True","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
ROMP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ROMP/romp/lib/loss_funcs/maps_loss.py,https://github.com/Arthur151/ROMP/tree/master/romp/lib/loss_funcs/maps_loss.py,AELoss,singleTagLoss$78,"def singleTagLoss(self, pred_tag, joints):
        """"""
        associative embedding loss for one image
        """"""
        tags = []
        pull = 0
        for joints_per_person in joints:
            tmp = []
            for joint in joints_per_person:
                if joint[1] > 0:
                    tmp.append(pred_tag[joint[0]])
            if len(tmp) == 0:
                continue
            tmp = torch.stack(tmp)
            tags.append(torch.mean(tmp, dim=0))
            pull = pull + torch.mean((tmp - tags[-1].expand_as(tmp))**2)

        num_tags = len(tags)
        if num_tags == 0:
            return make_input(torch.zeros(1).float()), \
                make_input(torch.zeros(1).float())
        elif num_tags == 1:
            return make_input(torch.zeros(1).float()), \
                pull/(num_tags)

        tags = torch.stack(tags)

        size = (num_tags, num_tags)
        A = tags.expand(*size)
        B = A.permute(1, 0)

        diff = A - B

        if self.loss_type == 'exp':
            diff = torch.pow(diff, 2)
            push = torch.exp(-diff)
            push = torch.sum(push) - num_tags
        elif self.loss_type == 'max':
            diff = 1 - torch.abs(diff)
            push = torch.clamp(diff, min=0).sum() - num_tags
        else:
            raise ValueError('Unkown ae loss type')

        return push/((num_tags - 1) * num_tags) * 0.5, \
            pull/(num_tags)","for joint in joints_per_person:
    if joint[1] > 0:
        tmp.append(pred_tag[joint[0]])","for joint in joints_per_person:
    (_, joint_1, *joint_rjointmaining) = joint
    if joint[1] > 0:
        tmp.append(pred_tag[joint[0]])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
neutron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/db/migration/alembic_migrations/versions/ussuri/expand/Ibac91d24da2_port_forwarding_description.py,https://github.com/openstack/neutron/tree/master/neutron/db/migration/alembic_migrations/versions/ussuri/expand/Ibac91d24da2_port_forwarding_description.py,,update_existing_records$45,"def update_existing_records():
    session = sa.orm.Session(bind=op.get_bind())
    with session.begin(subtransactions=True):
        for row in session.query(TABLE_MODEL):
            res = session.execute(
                STDATTRS_TABLE.insert().values(resource_type=TABLE_NAME)
            )
            session.execute(
                TABLE_MODEL.update().values(
                    standard_attr_id=res.inserted_primary_key[0]).where(
                    TABLE_MODEL.c.id == row[0])
            )
    session.commit()","for row in session.query(TABLE_MODEL):
    res = session.execute(STDATTRS_TABLE.insert().values(resource_type=TABLE_NAME))
    session.execute(TABLE_MODEL.update().values(standard_attr_id=res.inserted_primary_key[0]).where(TABLE_MODEL.c.id == row[0]))","for row in session.query(TABLE_MODEL):
    (row_0, *row_rrowmaining) = row
    res = session.execute(STDATTRS_TABLE.insert().values(resource_type=TABLE_NAME))
    session.execute(TABLE_MODEL.update().values(standard_attr_id=res.inserted_primary_key[0]).where(TABLE_MODEL.c.id == row[0]))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
pytorch-deeplab-xception,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch-deeplab-xception/modeling/deeplab.py,https://github.com/jfzhang95/pytorch-deeplab-xception/tree/master/modeling/deeplab.py,DeepLab,get_10x_lr_params$58,"def get_10x_lr_params(self):
        modules = [self.aspp, self.decoder]
        for i in range(len(modules)):
            for m in modules[i].named_modules():
                if self.freeze_bn:
                    if isinstance(m[1], nn.Conv2d):
                        for p in m[1].parameters():
                            if p.requires_grad:
                                yield p
                else:
                    if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \
                            or isinstance(m[1], nn.BatchNorm2d):
                        for p in m[1].parameters():
                            if p.requires_grad:
                                yield p","for m in modules[i].named_modules():
    if self.freeze_bn:
        if isinstance(m[1], nn.Conv2d):
            for p in m[1].parameters():
                if p.requires_grad:
                    yield p
    elif isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) or isinstance(m[1], nn.BatchNorm2d):
        for p in m[1].parameters():
            if p.requires_grad:
                yield p","for m in modules[i].named_modules():
    (_, m_1, *m_rmmaining) = m
    if self.freeze_bn:
        if isinstance(m[1], nn.Conv2d):
            for p in m[1].parameters():
                if p.requires_grad:
                    yield p
    elif isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) or isinstance(m[1], nn.BatchNorm2d):
        for p in m[1].parameters():
            if p.requires_grad:
                yield p","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
django-extensions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-extensions/django_extensions/management/commands/show_urls.py,https://github.com/django-extensions/django-extensions/tree/master/django_extensions/management/commands/show_urls.py,Command,extract_views_from_urlpatterns$190,"def extract_views_from_urlpatterns(self, urlpatterns, base='', namespace=None):
        """"""
        Return a list of views from a list of urlpatterns.

        Each object in the returned list is a three-tuple: (view_func, regex, name)
        """"""
        views = []
        for p in urlpatterns:
            if isinstance(p, (URLPattern, RegexURLPattern)):
                try:
                    if not p.name:
                        name = p.name
                    elif namespace:
                        name = '{0}:{1}'.format(namespace, p.name)
                    else:
                        name = p.name
                    pattern = describe_pattern(p)
                    views.append((p.callback, base + pattern, name))
                except ViewDoesNotExist:
                    continue
            elif isinstance(p, (URLResolver, RegexURLResolver)):
                try:
                    patterns = p.url_patterns
                except ImportError:
                    continue
                if namespace and p.namespace:
                    _namespace = '{0}:{1}'.format(namespace, p.namespace)
                else:
                    _namespace = (p.namespace or namespace)
                pattern = describe_pattern(p)
                if isinstance(p, LocaleRegexURLResolver):
                    for language in self.LANGUAGES:
                        with translation.override(language[0]):
                            views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))
                else:
                    views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))
            elif hasattr(p, '_get_callback'):
                try:
                    views.append((p._get_callback(), base + describe_pattern(p), p.name))
                except ViewDoesNotExist:
                    continue
            elif hasattr(p, 'url_patterns') or hasattr(p, '_get_url_patterns'):
                try:
                    patterns = p.url_patterns
                except ImportError:
                    continue
                views.extend(self.extract_views_from_urlpatterns(patterns, base + describe_pattern(p), namespace=namespace))
            else:
                raise TypeError(""%s does not appear to be a urlpattern object"" % p)
        return views","for language in self.LANGUAGES:
    with translation.override(language[0]):
        views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))","for language in self.LANGUAGES:
    (language_0, *language_rlanguagemaining) = language
    with translation.override(language[0]):
        views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
gluon-nlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/scripts/processing/learn_subword.py,https://github.com/dmlc/gluon-nlp/tree/master/scripts/processing/learn_subword.py,,main$103,"def main(args):
    corpus_path_list = args.corpus
    if args.save_dir is None:
        args.save_dir = args.model
    for corpus_path in corpus_path_list:
        if not os.path.exists(corpus_path):
            raise ValueError('The path=""{}"" provided by --corpus does not exist!'
                             .format(corpus_path))
    print('Learn the ""{}""s subword model based on {}.'.format(args.model, args.corpus))
    os.makedirs(args.save_dir, exist_ok=True)
    model_prefix = os.path.join(args.save_dir, args.model)
    print('Save the subword model to {}.model'.format(model_prefix))
    print('Save the vocabulary to {}.vocab'.format(model_prefix))
    print()
    print('------- Start Training -------------')
    special_tokens_kv = OrderedDict()
    if not args.disable_unk:
        special_tokens_kv['unk_token'] = Vocab.UNK_TOKEN
    if not args.disable_bos:
        special_tokens_kv['bos_token'] = Vocab.BOS_TOKEN
    if not args.disable_eos:
        special_tokens_kv['eos_token'] = Vocab.EOS_TOKEN
    if not args.disable_pad:
        special_tokens_kv['pad_token'] = Vocab.PAD_TOKEN
    # split custom special tokens
    if args.model in ['yttm'] and len(args.custom_special_tokens) > 0:
        raise ValueError('model {} do not support custom_special_tokens'.format(args.model))
    additional_custom_special_token = OrderedDict()
    for custom_special_token in args.custom_special_tokens:
        kv = custom_special_token.split('=')
        if not len(kv) == 2:
            raise ValueError('parameter {} has wrong format'.format(custom_special_token))
        k, v = kv[0], kv[1]
        if k in special_tokens_kv:
            warnings.warn(f'There are overlaps between the custom special tokens and the'
                          f' unk, bos, eos, pad tokens. Currently, we will overwrite the '
                          f'default tokens. We will overwrite ""{k}"" to ""{v}""')
        special_tokens_kv[k] = v
        additional_custom_special_token[k] = v
    if args.model == 'hf_wordpiece':
        tokenizers = try_import_huggingface_tokenizers()
        if 'unk_token' not in special_tokens_kv or special_tokens_kv['unk_token'] != '[UNK]':
            # TODO, HF Tokenizer must have the unk token.
            special_tokens_kv['unk_token'] = '[UNK]'
        if parse_version(tokenizers.__version__) < parse_version('0.8'):
            # The older version of Tokenizers
            # hf_wordpiece must contain mask, cls and sep tokens
            # the custom defined mask,cls,sep can overwrite the default settings
            if 'mask_token' not in special_tokens_kv:
                special_tokens_kv['mask_token'] = Vocab.MASK_TOKEN
            if 'cls_token' not in special_tokens_kv:
                special_tokens_kv['cls_token'] = Vocab.CLS_TOKEN
            if 'sep_token' not in special_tokens_kv:
                special_tokens_kv['sep_token'] = Vocab.SEP_TOKEN
    special_tokens = list(special_tokens_kv.values())
    print('special tokens: ' + ', '.join(special_tokens))
    vocab = []
    if args.model == 'spm':
        try_import_sentencepiece()
        import sentencepiece as spm
        corpus_path = ','.join(corpus_path_list)
        script = '--input={} --model_prefix={} --vocab_size={} --character_coverage={} --input_sentence_size={}' \
                 .format(corpus_path, model_prefix, args.vocab_size, args.coverage, args.input_sentence_size)
        script += (' --unk_id=' + str(list(special_tokens_kv.keys()).index('unk_token')))
        script += (' --bos_id=' + ('-1' if args.disable_bos else str(list(special_tokens_kv.keys()).index('bos_token'))))
        script += (' --eos_id=' + ('-1' if args.disable_eos else str(list(special_tokens_kv.keys()).index('eos_token'))))
        script += (' --pad_id=' + ('-1' if args.disable_pad else str(list(special_tokens_kv.keys()).index('pad_token'))))
        if len(additional_custom_special_token) > 0:
            script += (' --control_symbols=' + ','.join(list(additional_custom_special_token.values())))
        print(script)
        spm.SentencePieceTrainer.Train(script)
        if 'bos_token' in special_tokens_kv:
            special_tokens_kv['bos_token'] = '<s>'
        if 'eos_token' in special_tokens_kv:
            special_tokens_kv['eos_token'] = '</s>'
        # build spm vocab
        spm_model = spm.SentencePieceProcessor()
        spm_model.load(model_prefix + '.model')
        vocab = [spm_model.id_to_piece(i) for i in range(len(spm_model))]
        os.remove(model_prefix + '.vocab')
    elif args.model == 'subword_nmt':
        try_import_subword_nmt()
        from subword_nmt import learn_bpe
        corpus_path = cat_corpus(corpus_path_list)\
            if len(corpus_path_list) > 1 else corpus_path_list[0]
        # build model
        with open(corpus_path, 'r', encoding='utf-8') as fc,\
             open(model_prefix + '.model', 'w', encoding='utf-8') as fm:
            learn_bpe.learn_bpe(fc, fm, args.vocab_size - len(special_tokens), total_symbols=True)
        # build vocab
        with open(corpus_path, 'r', encoding='utf-8') as fc, \
             open(model_prefix + '.model', 'r', encoding='utf-8') as fm:
            vocab.extend(special_tokens)
            uniq_chars_internal = set()
            uniq_chars_final = set()
            uniq_words = set()
            for line in fc:
                for word in line.strip('\r\n ').split(' '):
                    if word:
                        uniq_words.add(word)
            # this code piece is same as 
            # https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/learn_bpe.py shows
            uniq_words = [tuple(x[:-1]) + (x[-1]+'</w>',) for x in uniq_words]
            for word in uniq_words:
                for char in word[:-1]:
                    uniq_chars_internal.add(char)
                uniq_chars_final.add(word[-1])
            # sort to ensure the same settings produce the same vocab
            vocab.extend(sorted(list(uniq_chars_internal)))
            vocab.extend(sorted(list(uniq_chars_final)))
            fm.readline()
            pair = fm.readline()
            while pair:
                vocab.append(pair.replace(' ', '', 1).strip())
                pair = fm.readline()
        if len(corpus_path_list) > 1:
            os.remove(corpus_path)
    elif args.model == 'yttm':
        try_import_yttm()
        import youtokentome as yttm
        corpus_path = cat_corpus(corpus_path_list)\
            if len(corpus_path_list) > 1 else corpus_path_list[0]
        tokenizer = yttm.BPE.train(
            data=corpus_path, 
            model=model_prefix + '.model',
            vocab_size=args.vocab_size, 
            coverage=args.coverage, 
            n_threads=args.n_threads,
            unk_id=special_tokens.index(Vocab.UNK_TOKEN),
            bos_id=-1 if args.disable_bos else special_tokens.index(Vocab.BOS_TOKEN),
            eos_id=-1 if args.disable_eos else special_tokens.index(Vocab.EOS_TOKEN),
            pad_id=-1 if args.disable_pad else special_tokens.index(Vocab.PAD_TOKEN))
        vocab = tokenizer.vocab()
        if 'unk_token' in special_tokens_kv:
            special_tokens_kv['unk_token'] = '<UNK>'
        if 'bos_token' in special_tokens_kv:
            special_tokens_kv['bos_token'] = '<BOS>'
        if 'eos_token' in special_tokens_kv:
            special_tokens_kv['eos_token'] = '<EOS>'        
        if 'pad_token' in special_tokens_kv:
            special_tokens_kv['pad_token'] = '<PAD>'
        if len(corpus_path_list) > 1:
            os.remove(corpus_path)
    elif args.model in ['hf_bpe', 'hf_bytebpe', 'hf_wordpiece']:
        tokenizers = try_import_huggingface_tokenizers()
        if args.model == 'hf_bpe':
            split_on_whitespace_only = not args.split_punctuation
            tokenizer = tokenizers.CharBPETokenizer(
                lowercase=args.lowercase,
                bert_normalizer=args.bert_normalizer,
                split_on_whitespace_only=split_on_whitespace_only)
        elif args.model == 'hf_bytebpe':
            tokenizer = tokenizers.ByteLevelBPETokenizer(lowercase=args.lowercase)
        elif args.model == 'hf_wordpiece':
            unk_token = special_tokens_kv.get('unk_token', None)
            sep_token = special_tokens_kv.get('sep_token', None)
            cls_token = special_tokens_kv.get('cls_token', None)
            pad_token = special_tokens_kv.get('pad_token', None)
            mask_token = special_tokens_kv.get('mask_token', None)
            if args.bert_normalizer:
                strip_accents = None
                clean_text = True
                handle_chinese_chars = True
            else:
                strip_accents = False
                clean_text = False
                handle_chinese_chars = False
            tokenizer = tokenizers.BertWordPieceTokenizer(
                unk_token=unk_token,
                sep_token=sep_token,
                cls_token=cls_token,
                pad_token=pad_token,
                mask_token=mask_token,
                lowercase=args.lowercase,
                strip_accents=strip_accents,
                handle_chinese_chars=handle_chinese_chars,
                clean_text=clean_text
            )
        else:
            raise NotImplementedError
        tokenizer.train(
            corpus_path_list,
            vocab_size=args.vocab_size,
            show_progress=True,
            special_tokens=special_tokens)
        # Deal with the API change of tokenizers >= 0.8
        if version.parse(tokenizers.__version__) >= version.parse('0.8'):
            save_model_path = model_prefix + '.model'
            tokenizer.save(save_model_path)
            model_info = json.load(open(save_model_path, encoding='utf-8'))
            special_tokens_in_tokenizer = model_info['added_tokens']
            assert len(special_tokens_in_tokenizer) == len(special_tokens)
            hf_vocab = model_info['model']['vocab']
            hf_vocab_sorted = sorted(list(hf_vocab.items()), key=lambda x: x[1])
            hf_vocab_ids = [ele[1] for ele in hf_vocab_sorted]
            assert min(hf_vocab_ids) == 0 and max(hf_vocab_ids) == len(hf_vocab_ids) - 1
            vocab = [ele[0] for ele in hf_vocab_sorted]
        else:
            tokenizer.save(args.save_dir, args.model)
            # we replace the huggingface vocab file with our Vocab implementation
            if args.model == 'hf_wordpiece':
                hf_vocab_file = model_prefix + '-vocab.txt'
                with open(hf_vocab_file, 'r', encoding='utf-8') as fv:
                    for line in fv:
                        vocab.append(line.strip())
            else:
                # Move the hf_${model}-merges.txt to hf_${model}.models
                os.rename(os.path.join(args.save_dir, '{}-merges.txt'.format(args.model)),
                          os.path.join(args.save_dir, '{}.model'.format(args.model)))
                hf_vocab_file = model_prefix + '-vocab.json'
                with open(hf_vocab_file, 'r', encoding='utf-8') as fv:
                    vocab_kv = json.load(fv)
                    vocab_kv = sorted(list(vocab_kv.items()), key=lambda x: x[1])
                    for kv in vocab_kv:
                        vocab.append(kv[0])
            os.remove(hf_vocab_file)
    else:
        raise NotImplementedError
    vocab_obj = Vocab(vocab, **special_tokens_kv)
    vocab_obj.save(model_prefix + '.vocab')
    print('-------- Done Training -------------')","for word in uniq_words:
    for char in word[:-1]:
        uniq_chars_internal.add(char)
    uniq_chars_final.add(word[-1])","for word in uniq_words:
    (*word_rwordmaining, word_nwordg_1) = word
    for char in word[:-1]:
        uniq_chars_internal.add(char)
    uniq_chars_final.add(word[-1])",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg_1 = e
variable mapping:
e_neg_1: e[-1]
e[:-1]: e_remaining",,,,,,,
gluon-nlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/scripts/processing/learn_subword.py,https://github.com/dmlc/gluon-nlp/tree/master/scripts/processing/learn_subword.py,,main$103,"def main(args):
    corpus_path_list = args.corpus
    if args.save_dir is None:
        args.save_dir = args.model
    for corpus_path in corpus_path_list:
        if not os.path.exists(corpus_path):
            raise ValueError('The path=""{}"" provided by --corpus does not exist!'
                             .format(corpus_path))
    print('Learn the ""{}""s subword model based on {}.'.format(args.model, args.corpus))
    os.makedirs(args.save_dir, exist_ok=True)
    model_prefix = os.path.join(args.save_dir, args.model)
    print('Save the subword model to {}.model'.format(model_prefix))
    print('Save the vocabulary to {}.vocab'.format(model_prefix))
    print()
    print('------- Start Training -------------')
    special_tokens_kv = OrderedDict()
    if not args.disable_unk:
        special_tokens_kv['unk_token'] = Vocab.UNK_TOKEN
    if not args.disable_bos:
        special_tokens_kv['bos_token'] = Vocab.BOS_TOKEN
    if not args.disable_eos:
        special_tokens_kv['eos_token'] = Vocab.EOS_TOKEN
    if not args.disable_pad:
        special_tokens_kv['pad_token'] = Vocab.PAD_TOKEN
    # split custom special tokens
    if args.model in ['yttm'] and len(args.custom_special_tokens) > 0:
        raise ValueError('model {} do not support custom_special_tokens'.format(args.model))
    additional_custom_special_token = OrderedDict()
    for custom_special_token in args.custom_special_tokens:
        kv = custom_special_token.split('=')
        if not len(kv) == 2:
            raise ValueError('parameter {} has wrong format'.format(custom_special_token))
        k, v = kv[0], kv[1]
        if k in special_tokens_kv:
            warnings.warn(f'There are overlaps between the custom special tokens and the'
                          f' unk, bos, eos, pad tokens. Currently, we will overwrite the '
                          f'default tokens. We will overwrite ""{k}"" to ""{v}""')
        special_tokens_kv[k] = v
        additional_custom_special_token[k] = v
    if args.model == 'hf_wordpiece':
        tokenizers = try_import_huggingface_tokenizers()
        if 'unk_token' not in special_tokens_kv or special_tokens_kv['unk_token'] != '[UNK]':
            # TODO, HF Tokenizer must have the unk token.
            special_tokens_kv['unk_token'] = '[UNK]'
        if parse_version(tokenizers.__version__) < parse_version('0.8'):
            # The older version of Tokenizers
            # hf_wordpiece must contain mask, cls and sep tokens
            # the custom defined mask,cls,sep can overwrite the default settings
            if 'mask_token' not in special_tokens_kv:
                special_tokens_kv['mask_token'] = Vocab.MASK_TOKEN
            if 'cls_token' not in special_tokens_kv:
                special_tokens_kv['cls_token'] = Vocab.CLS_TOKEN
            if 'sep_token' not in special_tokens_kv:
                special_tokens_kv['sep_token'] = Vocab.SEP_TOKEN
    special_tokens = list(special_tokens_kv.values())
    print('special tokens: ' + ', '.join(special_tokens))
    vocab = []
    if args.model == 'spm':
        try_import_sentencepiece()
        import sentencepiece as spm
        corpus_path = ','.join(corpus_path_list)
        script = '--input={} --model_prefix={} --vocab_size={} --character_coverage={} --input_sentence_size={}' \
                 .format(corpus_path, model_prefix, args.vocab_size, args.coverage, args.input_sentence_size)
        script += (' --unk_id=' + str(list(special_tokens_kv.keys()).index('unk_token')))
        script += (' --bos_id=' + ('-1' if args.disable_bos else str(list(special_tokens_kv.keys()).index('bos_token'))))
        script += (' --eos_id=' + ('-1' if args.disable_eos else str(list(special_tokens_kv.keys()).index('eos_token'))))
        script += (' --pad_id=' + ('-1' if args.disable_pad else str(list(special_tokens_kv.keys()).index('pad_token'))))
        if len(additional_custom_special_token) > 0:
            script += (' --control_symbols=' + ','.join(list(additional_custom_special_token.values())))
        print(script)
        spm.SentencePieceTrainer.Train(script)
        if 'bos_token' in special_tokens_kv:
            special_tokens_kv['bos_token'] = '<s>'
        if 'eos_token' in special_tokens_kv:
            special_tokens_kv['eos_token'] = '</s>'
        # build spm vocab
        spm_model = spm.SentencePieceProcessor()
        spm_model.load(model_prefix + '.model')
        vocab = [spm_model.id_to_piece(i) for i in range(len(spm_model))]
        os.remove(model_prefix + '.vocab')
    elif args.model == 'subword_nmt':
        try_import_subword_nmt()
        from subword_nmt import learn_bpe
        corpus_path = cat_corpus(corpus_path_list)\
            if len(corpus_path_list) > 1 else corpus_path_list[0]
        # build model
        with open(corpus_path, 'r', encoding='utf-8') as fc,\
             open(model_prefix + '.model', 'w', encoding='utf-8') as fm:
            learn_bpe.learn_bpe(fc, fm, args.vocab_size - len(special_tokens), total_symbols=True)
        # build vocab
        with open(corpus_path, 'r', encoding='utf-8') as fc, \
             open(model_prefix + '.model', 'r', encoding='utf-8') as fm:
            vocab.extend(special_tokens)
            uniq_chars_internal = set()
            uniq_chars_final = set()
            uniq_words = set()
            for line in fc:
                for word in line.strip('\r\n ').split(' '):
                    if word:
                        uniq_words.add(word)
            # this code piece is same as 
            # https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/learn_bpe.py shows
            uniq_words = [tuple(x[:-1]) + (x[-1]+'</w>',) for x in uniq_words]
            for word in uniq_words:
                for char in word[:-1]:
                    uniq_chars_internal.add(char)
                uniq_chars_final.add(word[-1])
            # sort to ensure the same settings produce the same vocab
            vocab.extend(sorted(list(uniq_chars_internal)))
            vocab.extend(sorted(list(uniq_chars_final)))
            fm.readline()
            pair = fm.readline()
            while pair:
                vocab.append(pair.replace(' ', '', 1).strip())
                pair = fm.readline()
        if len(corpus_path_list) > 1:
            os.remove(corpus_path)
    elif args.model == 'yttm':
        try_import_yttm()
        import youtokentome as yttm
        corpus_path = cat_corpus(corpus_path_list)\
            if len(corpus_path_list) > 1 else corpus_path_list[0]
        tokenizer = yttm.BPE.train(
            data=corpus_path, 
            model=model_prefix + '.model',
            vocab_size=args.vocab_size, 
            coverage=args.coverage, 
            n_threads=args.n_threads,
            unk_id=special_tokens.index(Vocab.UNK_TOKEN),
            bos_id=-1 if args.disable_bos else special_tokens.index(Vocab.BOS_TOKEN),
            eos_id=-1 if args.disable_eos else special_tokens.index(Vocab.EOS_TOKEN),
            pad_id=-1 if args.disable_pad else special_tokens.index(Vocab.PAD_TOKEN))
        vocab = tokenizer.vocab()
        if 'unk_token' in special_tokens_kv:
            special_tokens_kv['unk_token'] = '<UNK>'
        if 'bos_token' in special_tokens_kv:
            special_tokens_kv['bos_token'] = '<BOS>'
        if 'eos_token' in special_tokens_kv:
            special_tokens_kv['eos_token'] = '<EOS>'        
        if 'pad_token' in special_tokens_kv:
            special_tokens_kv['pad_token'] = '<PAD>'
        if len(corpus_path_list) > 1:
            os.remove(corpus_path)
    elif args.model in ['hf_bpe', 'hf_bytebpe', 'hf_wordpiece']:
        tokenizers = try_import_huggingface_tokenizers()
        if args.model == 'hf_bpe':
            split_on_whitespace_only = not args.split_punctuation
            tokenizer = tokenizers.CharBPETokenizer(
                lowercase=args.lowercase,
                bert_normalizer=args.bert_normalizer,
                split_on_whitespace_only=split_on_whitespace_only)
        elif args.model == 'hf_bytebpe':
            tokenizer = tokenizers.ByteLevelBPETokenizer(lowercase=args.lowercase)
        elif args.model == 'hf_wordpiece':
            unk_token = special_tokens_kv.get('unk_token', None)
            sep_token = special_tokens_kv.get('sep_token', None)
            cls_token = special_tokens_kv.get('cls_token', None)
            pad_token = special_tokens_kv.get('pad_token', None)
            mask_token = special_tokens_kv.get('mask_token', None)
            if args.bert_normalizer:
                strip_accents = None
                clean_text = True
                handle_chinese_chars = True
            else:
                strip_accents = False
                clean_text = False
                handle_chinese_chars = False
            tokenizer = tokenizers.BertWordPieceTokenizer(
                unk_token=unk_token,
                sep_token=sep_token,
                cls_token=cls_token,
                pad_token=pad_token,
                mask_token=mask_token,
                lowercase=args.lowercase,
                strip_accents=strip_accents,
                handle_chinese_chars=handle_chinese_chars,
                clean_text=clean_text
            )
        else:
            raise NotImplementedError
        tokenizer.train(
            corpus_path_list,
            vocab_size=args.vocab_size,
            show_progress=True,
            special_tokens=special_tokens)
        # Deal with the API change of tokenizers >= 0.8
        if version.parse(tokenizers.__version__) >= version.parse('0.8'):
            save_model_path = model_prefix + '.model'
            tokenizer.save(save_model_path)
            model_info = json.load(open(save_model_path, encoding='utf-8'))
            special_tokens_in_tokenizer = model_info['added_tokens']
            assert len(special_tokens_in_tokenizer) == len(special_tokens)
            hf_vocab = model_info['model']['vocab']
            hf_vocab_sorted = sorted(list(hf_vocab.items()), key=lambda x: x[1])
            hf_vocab_ids = [ele[1] for ele in hf_vocab_sorted]
            assert min(hf_vocab_ids) == 0 and max(hf_vocab_ids) == len(hf_vocab_ids) - 1
            vocab = [ele[0] for ele in hf_vocab_sorted]
        else:
            tokenizer.save(args.save_dir, args.model)
            # we replace the huggingface vocab file with our Vocab implementation
            if args.model == 'hf_wordpiece':
                hf_vocab_file = model_prefix + '-vocab.txt'
                with open(hf_vocab_file, 'r', encoding='utf-8') as fv:
                    for line in fv:
                        vocab.append(line.strip())
            else:
                # Move the hf_${model}-merges.txt to hf_${model}.models
                os.rename(os.path.join(args.save_dir, '{}-merges.txt'.format(args.model)),
                          os.path.join(args.save_dir, '{}.model'.format(args.model)))
                hf_vocab_file = model_prefix + '-vocab.json'
                with open(hf_vocab_file, 'r', encoding='utf-8') as fv:
                    vocab_kv = json.load(fv)
                    vocab_kv = sorted(list(vocab_kv.items()), key=lambda x: x[1])
                    for kv in vocab_kv:
                        vocab.append(kv[0])
            os.remove(hf_vocab_file)
    else:
        raise NotImplementedError
    vocab_obj = Vocab(vocab, **special_tokens_kv)
    vocab_obj.save(model_prefix + '.vocab')
    print('-------- Done Training -------------')","for kv in vocab_kv:
    vocab.append(kv[0])","for kv in vocab_kv:
    (kv_0, *kv_rkvmaining) = kv
    vocab.append(kv[0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/circuit/library/test_evolution_gate.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/circuit/library/test_evolution_gate.py,TestEvolutionGate,test_qdrift_manual$135,"def test_qdrift_manual(self, op, time, reps, sampled_ops):
        """"""Test the evolution circuit of Suzuki Trotter against a manually constructed circuit.""""""
        qdrift = QDrift(reps=reps)
        evo_gate = PauliEvolutionGate(op, time, synthesis=qdrift)
        evo_gate.definition.decompose()

        # manually construct expected evolution
        expected = QuantumCircuit(1)
        for pauli in sampled_ops:
            if pauli[0].to_label() == ""X"":
                expected.rx(2 * pauli[1], 0)
            elif pauli[0].to_label() == ""Y"":
                expected.ry(2 * pauli[1], 0)

        self.assertTrue(Operator(evo_gate.definition).equiv(expected))","for pauli in sampled_ops:
    if pauli[0].to_label() == 'X':
        expected.rx(2 * pauli[1], 0)
    elif pauli[0].to_label() == 'Y':
        expected.ry(2 * pauli[1], 0)","for pauli in sampled_ops:
    (pauli_0, pauli_1, *_) = pauli
    if pauli[0].to_label() == 'X':
        expected.rx(2 * pauli[1], 0)
    elif pauli[0].to_label() == 'Y':
        expected.ry(2 * pauli[1], 0)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/datadog_checks_dev/tests/tooling/config_validator/test_validator.py,https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/tests/tooling/config_validator/test_validator.py,,test_validate_type$69,"def test_validate_type():
    test_cases = [
        (create(""name"", ""boolean""), True),
        (create(""name"", ""string""), True),
        (create(""name"", ""integer""), True),
        (create(""name"", ""double""), True),
        (create(""name"", ""float""), True),
        (create(""name"", ""object""), True),
        (create(""name"", ""list of anything_really""), True),
        (create(""name"", ""list of something""), True),
        (create(""name"", ""dictionary""), True),
        (create(""name"", ""custom object""), False),
    ]

    for c in test_cases:
        errors = []
        c[0]._validate_type(errors)
        if c[1]:
            assert len(errors) == 0
        else:
            type_name = c[0].param_prop.type_name
            assert len(errors) == 1
            assert errors[0].error_str == ""Type {} is not accepted"".format(type_name)","for c in test_cases:
    errors = []
    c[0]._validate_type(errors)
    if c[1]:
        assert len(errors) == 0
    else:
        type_name = c[0].param_prop.type_name
        assert len(errors) == 1
        assert errors[0].error_str == 'Type {} is not accepted'.format(type_name)","for c in test_cases:
    (c_0, c_1, *_) = c
    errors = []
    c[0]._validate_type(errors)
    if c[1]:
        assert len(errors) == 0
    else:
        type_name = c[0].param_prop.type_name
        assert len(errors) == 1
        assert errors[0].error_str == 'Type {} is not accepted'.format(type_name)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
edx-platform,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/common/djangoapps/util/testing.py,https://github.com/edx/edx-platform/tree/master/common/djangoapps/util/testing.py,EventTestMixin,assert_event_emission_count$99,"def assert_event_emission_count(self, event_name, expected_count):
        """"""
        Verify that the event with the given name was emitted
        a specific number of times.
        """"""
        actual_count = 0
        for call_args in self.mock_tracker.emit.call_args_list:
            if call_args[0][0] == event_name:
                actual_count += 1
        assert actual_count == expected_count","for call_args in self.mock_tracker.emit.call_args_list:
    if call_args[0][0] == event_name:
        actual_count += 1","for call_args in self.mock_tracker.emit.call_args_list:
    ((call_args_0_0, _, *call_args_0_rcall_argsmaining), *call_args_rcall_argsmaining) = call_args
    if call_args[0][0] == event_name:
        actual_count += 1","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",0,,,"Answer: Yes
Iterable Unpacking: (e_0_0, _, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]",,,,,,,
stellargraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stellargraph/tests/test_ensemble.py,https://github.com/stellargraph/stellargraph/tree/master/tests/test_ensemble.py,,test_evaluate_link_prediction$620,"def test_evaluate_link_prediction():
    tf.keras.backend.clear_session()
    edge_ids_test = np.array([[1, 2], [2, 3], [1, 3]])
    edge_labels_test = np.array([1, 1, 0])

    graph = example_graph_1(feature_size=4)

    # base_model, keras_model, generator, train_gen
    gnn_models = [
        create_graphSAGE_model(graph, link_prediction=True),
        create_HinSAGE_model(graph, link_prediction=True),
    ]

    for gnn_model in gnn_models:
        keras_model = gnn_model[1]
        generator = gnn_model[2]

        ens = Ensemble(keras_model, n_estimators=2, n_predictions=1)

        ens.compile(
            optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=[""acc""]
        )

        # Check that passing invalid parameters is handled correctly. We will not check error handling for those
        # parameters that Keras will be responsible for.
        with pytest.raises(ValueError):
            ens.evaluate(
                generator=generator,
                test_data=edge_ids_test,
                test_targets=edge_labels_test,
            )

        with pytest.raises(ValueError):
            ens.evaluate(
                generator=generator,
                test_data=edge_labels_test,
                test_targets=None,  # must give test_targets
            )

        with pytest.raises(ValueError):
            ens.evaluate(
                generator=generator.flow(edge_ids_test, edge_labels_test),
                test_data=edge_ids_test,
                test_targets=edge_labels_test,
            )

        # We won't train the model instead use the initial random weights to test
        # the evaluate method.
        test_metrics_mean, test_metrics_std = ens.evaluate(
            generator.flow(edge_ids_test, edge_labels_test)
        )

        assert len(test_metrics_mean) == len(test_metrics_std)
        assert len(test_metrics_mean.shape) == 1
        assert len(test_metrics_std.shape) == 1

        #
        # Repeat for BaggingEnsemble

        ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)

        ens.compile(
            optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=[""acc""]
        )

        # Check that passing invalid parameters is handled correctly. We will not check error handling for those
        # parameters that Keras will be responsible for.
        with pytest.raises(ValueError):
            ens.evaluate(
                generator=generator,
                test_data=edge_ids_test,
                test_targets=edge_labels_test,
            )

        with pytest.raises(ValueError):
            ens.evaluate(
                generator=generator,
                test_data=edge_labels_test,
                test_targets=None,  # must give test_targets
            )

        with pytest.raises(ValueError):
            ens.evaluate(
                generator=generator.flow(edge_ids_test, edge_labels_test),
                test_data=edge_ids_test,
                test_targets=edge_labels_test,
            )

        # We won't train the model instead use the initial random weights to test
        # the evaluate method.
        test_metrics_mean, test_metrics_std = ens.evaluate(
            generator.flow(edge_ids_test, edge_labels_test)
        )

        assert len(test_metrics_mean) == len(test_metrics_std)
        assert len(test_metrics_mean.shape) == 1
        assert len(test_metrics_std.shape) == 1","for gnn_model in gnn_models:
    keras_model = gnn_model[1]
    generator = gnn_model[2]
    ens = Ensemble(keras_model, n_estimators=2, n_predictions=1)
    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
    assert len(test_metrics_mean) == len(test_metrics_std)
    assert len(test_metrics_mean.shape) == 1
    assert len(test_metrics_std.shape) == 1
    ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)
    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
    assert len(test_metrics_mean) == len(test_metrics_std)
    assert len(test_metrics_mean.shape) == 1
    assert len(test_metrics_std.shape) == 1","for gnn_model in gnn_models:
    (_, gnn_model_1, gnn_model_2, *gnn_model_rgnn_modelmaining) = gnn_model
    keras_model = gnn_model[1]
    generator = gnn_model[2]
    ens = Ensemble(keras_model, n_estimators=2, n_predictions=1)
    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
    assert len(test_metrics_mean) == len(test_metrics_std)
    assert len(test_metrics_mean.shape) == 1
    assert len(test_metrics_std.shape) == 1
    ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)
    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
    assert len(test_metrics_mean) == len(test_metrics_std)
    assert len(test_metrics_mean.shape) == 1
    assert len(test_metrics_std.shape) == 1","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
osroom,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/osroom/apps/core/flask/reqparse.py,https://github.com/osroom/osroom/tree/master/apps/core/flask/reqparse.py,ArgVerify,regex_rule$84,"def regex_rule(self, **kwargs):

        vr = kwargs.get(""vr"")
        if vr[""is_match""]:
            for reqarg in kwargs.get(""reqargs""):
                if not re.search(vr[""rule""], reqarg[1]):
                    return False, {
                        'msg': gettext('The value of parameter ""{}"" is illegal').format(
                            reqarg[0]), 'msg_type': ""w"", ""custom_status"": 422}

        else:
            for reqarg in kwargs.get(""reqargs""):
                if re.search(vr[""rule""], reqarg[1]):
                    return False, {
                        'msg': gettext('The value of parameter ""{}"" is illegal').format(
                            reqarg[0]), 'msg_type': ""w"", ""custom_status"": 422}

        return True, None","for reqarg in kwargs.get('reqargs'):
    if not re.search(vr['rule'], reqarg[1]):
        return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})","for reqarg in kwargs.get('reqargs'):
    (reqarg_0, reqarg_1, *_) = reqarg
    if not re.search(vr['rule'], reqarg[1]):
        return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
osroom,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/osroom/apps/core/flask/reqparse.py,https://github.com/osroom/osroom/tree/master/apps/core/flask/reqparse.py,ArgVerify,regex_rule$84,"def regex_rule(self, **kwargs):

        vr = kwargs.get(""vr"")
        if vr[""is_match""]:
            for reqarg in kwargs.get(""reqargs""):
                if not re.search(vr[""rule""], reqarg[1]):
                    return False, {
                        'msg': gettext('The value of parameter ""{}"" is illegal').format(
                            reqarg[0]), 'msg_type': ""w"", ""custom_status"": 422}

        else:
            for reqarg in kwargs.get(""reqargs""):
                if re.search(vr[""rule""], reqarg[1]):
                    return False, {
                        'msg': gettext('The value of parameter ""{}"" is illegal').format(
                            reqarg[0]), 'msg_type': ""w"", ""custom_status"": 422}

        return True, None","for reqarg in kwargs.get('reqargs'):
    if re.search(vr['rule'], reqarg[1]):
        return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})","for reqarg in kwargs.get('reqargs'):
    (reqarg_0, reqarg_1, *_) = reqarg
    if re.search(vr['rule'], reqarg[1]):
        return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
toil,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/toil/src/toil/fileStores/cachingFileStore.py,https://github.com/DataBiosphere/toil/tree/master/src/toil/fileStores/cachingFileStore.py,CachingFileStore,getFileReaderCount$541,"def getFileReaderCount(self, fileID):
        """"""
        Return the number of current outstanding reads of the given file.

        Counts mutable references too.
        """"""

        for row in self.cur.execute('SELECT COUNT(*) FROM refs WHERE file_id = ?', (fileID,)):
            return row[0]
        return 0","for row in self.cur.execute('SELECT COUNT(*) FROM refs WHERE file_id = ?', (fileID,)):
    return row[0]","for row in self.cur.execute('SELECT COUNT(*) FROM refs WHERE file_id = ?', (fileID,)):
    (row_0, *row_rrowmaining) = row
    return row[0]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
congress-legislators,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/congress-legislators/scripts/icpsr_ids.py,https://github.com/unitedstates/congress-legislators/tree/master/scripts/icpsr_ids.py,,run$15,"def run():

    # default to caching
    cache = utils.flags().get('cache', True)
    force = not cache


    only_bioguide = utils.flags().get('bioguide', None)
    congress = utils.flags().get('congress',None)


    data_files = []

    print(""Loading %s..."" % ""legislators-current.yaml"")
    legislators = load_data(""legislators-current.yaml"")
    data_files.append((legislators,""legislators-current.yaml""))
    print(""Loading %s..."" % ""legislators-historical.yaml"")
    legislators = load_data(""legislators-historical.yaml"")
    data_files.append((legislators,""legislators-historical.yaml""))

    # load member data from vote view
    if congress == None:
        raise Exception(""the --congress flag is required"")
    elif int(congress) < 10 and int(congress) > 0:
        url_senate = ""https://voteview.com/static/data/out/members/S00%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H00%s_members.csv"" % congress
    elif int(congress) < 100 and int(congress) >= 10:
        url_senate = ""https://voteview.com/static/data/out/members/S0%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H0%s_members.csv"" % congress
    elif int(congress) >= 100:
        url_senate = ""https://voteview.com/static/data/out/members/S%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H%s_members.csv"" % congress
    else:
        raise Exception(""no data for congress "" + congress)

    senate_destination = ""icpsr/source/senate_rollcall%s.txt"" % congress
    senate_data = utils.download(url_senate, senate_destination, force)

    house_destination = ""icpsr/source/house_rollcall%s.txt"" % congress
    house_data = utils.download(url_house, house_destination, force)

    error_log = csv.writer(open(""cache/errors/mismatch/mismatch_%s.csv"" % congress, ""w""))
    error_log.writerow([""error_type"",""matches"",""icpsr_name"",""icpsr_state"",""is_territory"",""old_id"",""new_id""])



    read_files = [(""sen"",senate_data),(""rep"",house_data)]
    print(""Running for congress "" + congress)
    for read_file_chamber,read_file_content in read_files:
        for data_file in data_files:
            for legislator in data_file[0]:
                num_matches = 0
                write_id = """"
                # this can't run unless we've already collected a bioguide for this person
                bioguide = legislator[""id""].get(""bioguide"", None)
                # if we've limited this to just one bioguide, skip over everyone else
                if only_bioguide and (bioguide != only_bioguide):
                    continue
                #if not in currently read chamber, skip
                chamber = legislator['terms'][len(legislator['terms'])-1]['type']
                if chamber != read_file_chamber:
                    continue

                #only run for selected congress
                latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms'])-1]['start'])))
                if chamber == ""sen"":
                    congresses = [latest_congress,latest_congress+1,latest_congress+2]
                else:
                    congresses =[latest_congress]

                if int(congress) not in congresses:
                    continue

                # pull data to match from yaml

                last_name = legislator['name']['last'].upper()
                state = utils.states[legislator['terms'][len(legislator['terms'])-1]['state']].upper()[:7].strip()

                # convert read_file_content str to file object, then parse as csv file
                content_as_file = StringIO(read_file_content)
                content_parsed = csv.reader(content_as_file, delimiter=',')

                # loop through congress members in read file, see if one matches the current legislator
                for icpsr_member in content_parsed:
                    # ensure unique match bassed of bioguide id
                    if bioguide == icpsr_member[10]:
                        num_matches += 1
                        write_id = int(icpsr_member[2])

                # skip if icpsr id is currently in data
                if ""icpsr"" in legislator[""id""]:
                    if write_id == legislator[""id""][""icpsr""] or write_id == """":
                        continue
                    elif write_id != legislator[""id""][""icpsr""] and write_id != """":
                        error_log.writerow([""Incorrect_ID"",""NA"",last_name[:8],state,""NA"",legislator[""id""][""icpsr""],write_id])
                        print(""ID updated for %s"" % last_name)

                if num_matches == 1:
                    legislator['id']['icpsr'] = int(write_id)
                else:
                    if state == 'GUAM' or state == 'PUERTO' or state == ""VIRGIN"" or state == ""DISTRIC"" or state == ""AMERICA"" or state == ""NORTHER"" or state == ""PHILIPP"":
                        print('error: non 1 match')
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name[:8],state,""Y"",""NA"",""NA""])
                    else:
                        print(str(num_matches) + "" matches found for ""+ last_name[:8] + "", "" + state + "" in congress "" + str(congress))
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name,state,""N"",""NA"",""NA""])

            save_data(data_file[0], data_file[1])","for data_file in data_files:
    for legislator in data_file[0]:
        num_matches = 0
        write_id = ''
        bioguide = legislator['id'].get('bioguide', None)
        if only_bioguide and bioguide != only_bioguide:
            continue
        chamber = legislator['terms'][len(legislator['terms']) - 1]['type']
        if chamber != read_file_chamber:
            continue
        latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms']) - 1]['start'])))
        if chamber == 'sen':
            congresses = [latest_congress, latest_congress + 1, latest_congress + 2]
        else:
            congresses = [latest_congress]
        if int(congress) not in congresses:
            continue
        last_name = legislator['name']['last'].upper()
        state = utils.states[legislator['terms'][len(legislator['terms']) - 1]['state']].upper()[:7].strip()
        content_as_file = StringIO(read_file_content)
        content_parsed = csv.reader(content_as_file, delimiter=',')
        for icpsr_member in content_parsed:
            if bioguide == icpsr_member[10]:
                num_matches += 1
                write_id = int(icpsr_member[2])
        if 'icpsr' in legislator['id']:
            if write_id == legislator['id']['icpsr'] or write_id == '':
                continue
            elif write_id != legislator['id']['icpsr'] and write_id != '':
                error_log.writerow(['Incorrect_ID', 'NA', last_name[:8], state, 'NA', legislator['id']['icpsr'], write_id])
                print('ID updated for %s' % last_name)
        if num_matches == 1:
            legislator['id']['icpsr'] = int(write_id)
        elif state == 'GUAM' or state == 'PUERTO' or state == 'VIRGIN' or (state == 'DISTRIC') or (state == 'AMERICA') or (state == 'NORTHER') or (state == 'PHILIPP'):
            print('error: non 1 match')
            error_log.writerow(['Non_1_match_number', str(num_matches), last_name[:8], state, 'Y', 'NA', 'NA'])
        else:
            print(str(num_matches) + ' matches found for ' + last_name[:8] + ', ' + state + ' in congress ' + str(congress))
            error_log.writerow(['Non_1_match_number', str(num_matches), last_name, state, 'N', 'NA', 'NA'])
    save_data(data_file[0], data_file[1])","for data_file in data_files:
    (data_file_0, data_file_1, *_) = data_file
    for legislator in data_file[0]:
        num_matches = 0
        write_id = ''
        bioguide = legislator['id'].get('bioguide', None)
        if only_bioguide and bioguide != only_bioguide:
            continue
        chamber = legislator['terms'][len(legislator['terms']) - 1]['type']
        if chamber != read_file_chamber:
            continue
        latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms']) - 1]['start'])))
        if chamber == 'sen':
            congresses = [latest_congress, latest_congress + 1, latest_congress + 2]
        else:
            congresses = [latest_congress]
        if int(congress) not in congresses:
            continue
        last_name = legislator['name']['last'].upper()
        state = utils.states[legislator['terms'][len(legislator['terms']) - 1]['state']].upper()[:7].strip()
        content_as_file = StringIO(read_file_content)
        content_parsed = csv.reader(content_as_file, delimiter=',')
        for icpsr_member in content_parsed:
            if bioguide == icpsr_member[10]:
                num_matches += 1
                write_id = int(icpsr_member[2])
        if 'icpsr' in legislator['id']:
            if write_id == legislator['id']['icpsr'] or write_id == '':
                continue
            elif write_id != legislator['id']['icpsr'] and write_id != '':
                error_log.writerow(['Incorrect_ID', 'NA', last_name[:8], state, 'NA', legislator['id']['icpsr'], write_id])
                print('ID updated for %s' % last_name)
        if num_matches == 1:
            legislator['id']['icpsr'] = int(write_id)
        elif state == 'GUAM' or state == 'PUERTO' or state == 'VIRGIN' or (state == 'DISTRIC') or (state == 'AMERICA') or (state == 'NORTHER') or (state == 'PHILIPP'):
            print('error: non 1 match')
            error_log.writerow(['Non_1_match_number', str(num_matches), last_name[:8], state, 'Y', 'NA', 'NA'])
        else:
            print(str(num_matches) + ' matches found for ' + last_name[:8] + ', ' + state + ' in congress ' + str(congress))
            error_log.writerow(['Non_1_match_number', str(num_matches), last_name, state, 'N', 'NA', 'NA'])
    save_data(data_file[0], data_file[1])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
congress-legislators,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/congress-legislators/scripts/icpsr_ids.py,https://github.com/unitedstates/congress-legislators/tree/master/scripts/icpsr_ids.py,,run$15,"def run():

    # default to caching
    cache = utils.flags().get('cache', True)
    force = not cache


    only_bioguide = utils.flags().get('bioguide', None)
    congress = utils.flags().get('congress',None)


    data_files = []

    print(""Loading %s..."" % ""legislators-current.yaml"")
    legislators = load_data(""legislators-current.yaml"")
    data_files.append((legislators,""legislators-current.yaml""))
    print(""Loading %s..."" % ""legislators-historical.yaml"")
    legislators = load_data(""legislators-historical.yaml"")
    data_files.append((legislators,""legislators-historical.yaml""))

    # load member data from vote view
    if congress == None:
        raise Exception(""the --congress flag is required"")
    elif int(congress) < 10 and int(congress) > 0:
        url_senate = ""https://voteview.com/static/data/out/members/S00%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H00%s_members.csv"" % congress
    elif int(congress) < 100 and int(congress) >= 10:
        url_senate = ""https://voteview.com/static/data/out/members/S0%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H0%s_members.csv"" % congress
    elif int(congress) >= 100:
        url_senate = ""https://voteview.com/static/data/out/members/S%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H%s_members.csv"" % congress
    else:
        raise Exception(""no data for congress "" + congress)

    senate_destination = ""icpsr/source/senate_rollcall%s.txt"" % congress
    senate_data = utils.download(url_senate, senate_destination, force)

    house_destination = ""icpsr/source/house_rollcall%s.txt"" % congress
    house_data = utils.download(url_house, house_destination, force)

    error_log = csv.writer(open(""cache/errors/mismatch/mismatch_%s.csv"" % congress, ""w""))
    error_log.writerow([""error_type"",""matches"",""icpsr_name"",""icpsr_state"",""is_territory"",""old_id"",""new_id""])



    read_files = [(""sen"",senate_data),(""rep"",house_data)]
    print(""Running for congress "" + congress)
    for read_file_chamber,read_file_content in read_files:
        for data_file in data_files:
            for legislator in data_file[0]:
                num_matches = 0
                write_id = """"
                # this can't run unless we've already collected a bioguide for this person
                bioguide = legislator[""id""].get(""bioguide"", None)
                # if we've limited this to just one bioguide, skip over everyone else
                if only_bioguide and (bioguide != only_bioguide):
                    continue
                #if not in currently read chamber, skip
                chamber = legislator['terms'][len(legislator['terms'])-1]['type']
                if chamber != read_file_chamber:
                    continue

                #only run for selected congress
                latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms'])-1]['start'])))
                if chamber == ""sen"":
                    congresses = [latest_congress,latest_congress+1,latest_congress+2]
                else:
                    congresses =[latest_congress]

                if int(congress) not in congresses:
                    continue

                # pull data to match from yaml

                last_name = legislator['name']['last'].upper()
                state = utils.states[legislator['terms'][len(legislator['terms'])-1]['state']].upper()[:7].strip()

                # convert read_file_content str to file object, then parse as csv file
                content_as_file = StringIO(read_file_content)
                content_parsed = csv.reader(content_as_file, delimiter=',')

                # loop through congress members in read file, see if one matches the current legislator
                for icpsr_member in content_parsed:
                    # ensure unique match bassed of bioguide id
                    if bioguide == icpsr_member[10]:
                        num_matches += 1
                        write_id = int(icpsr_member[2])

                # skip if icpsr id is currently in data
                if ""icpsr"" in legislator[""id""]:
                    if write_id == legislator[""id""][""icpsr""] or write_id == """":
                        continue
                    elif write_id != legislator[""id""][""icpsr""] and write_id != """":
                        error_log.writerow([""Incorrect_ID"",""NA"",last_name[:8],state,""NA"",legislator[""id""][""icpsr""],write_id])
                        print(""ID updated for %s"" % last_name)

                if num_matches == 1:
                    legislator['id']['icpsr'] = int(write_id)
                else:
                    if state == 'GUAM' or state == 'PUERTO' or state == ""VIRGIN"" or state == ""DISTRIC"" or state == ""AMERICA"" or state == ""NORTHER"" or state == ""PHILIPP"":
                        print('error: non 1 match')
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name[:8],state,""Y"",""NA"",""NA""])
                    else:
                        print(str(num_matches) + "" matches found for ""+ last_name[:8] + "", "" + state + "" in congress "" + str(congress))
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name,state,""N"",""NA"",""NA""])

            save_data(data_file[0], data_file[1])","for legislator in data_file[0]:
    num_matches = 0
    write_id = ''
    bioguide = legislator['id'].get('bioguide', None)
    if only_bioguide and bioguide != only_bioguide:
        continue
    chamber = legislator['terms'][len(legislator['terms']) - 1]['type']
    if chamber != read_file_chamber:
        continue
    latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms']) - 1]['start'])))
    if chamber == 'sen':
        congresses = [latest_congress, latest_congress + 1, latest_congress + 2]
    else:
        congresses = [latest_congress]
    if int(congress) not in congresses:
        continue
    last_name = legislator['name']['last'].upper()
    state = utils.states[legislator['terms'][len(legislator['terms']) - 1]['state']].upper()[:7].strip()
    content_as_file = StringIO(read_file_content)
    content_parsed = csv.reader(content_as_file, delimiter=',')
    for icpsr_member in content_parsed:
        if bioguide == icpsr_member[10]:
            num_matches += 1
            write_id = int(icpsr_member[2])
    if 'icpsr' in legislator['id']:
        if write_id == legislator['id']['icpsr'] or write_id == '':
            continue
        elif write_id != legislator['id']['icpsr'] and write_id != '':
            error_log.writerow(['Incorrect_ID', 'NA', last_name[:8], state, 'NA', legislator['id']['icpsr'], write_id])
            print('ID updated for %s' % last_name)
    if num_matches == 1:
        legislator['id']['icpsr'] = int(write_id)
    elif state == 'GUAM' or state == 'PUERTO' or state == 'VIRGIN' or (state == 'DISTRIC') or (state == 'AMERICA') or (state == 'NORTHER') or (state == 'PHILIPP'):
        print('error: non 1 match')
        error_log.writerow(['Non_1_match_number', str(num_matches), last_name[:8], state, 'Y', 'NA', 'NA'])
    else:
        print(str(num_matches) + ' matches found for ' + last_name[:8] + ', ' + state + ' in congress ' + str(congress))
        error_log.writerow(['Non_1_match_number', str(num_matches), last_name, state, 'N', 'NA', 'NA'])",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: 
```
e_id = e['id']
e_icpsr = e['id']['icpsr']
e_last_name = e['name']['last']
e_start_date = e['terms'][len(e['terms']) - 1]['start']
e_term_type = e['terms'][len(e['terms']) - 1]['type']
```
variable mapping:
e_id: e['id']
e_icpsr: e['id']['icpsr']
e_last_name: e['name']['last']
e_start_date: e['terms'][len(e['terms']) - 1]['start']
e_term_type: e['terms'][len(e['terms']) - 1]['type']",,,,,,,
congress-legislators,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/congress-legislators/scripts/icpsr_ids.py,https://github.com/unitedstates/congress-legislators/tree/master/scripts/icpsr_ids.py,,run$15,"def run():

    # default to caching
    cache = utils.flags().get('cache', True)
    force = not cache


    only_bioguide = utils.flags().get('bioguide', None)
    congress = utils.flags().get('congress',None)


    data_files = []

    print(""Loading %s..."" % ""legislators-current.yaml"")
    legislators = load_data(""legislators-current.yaml"")
    data_files.append((legislators,""legislators-current.yaml""))
    print(""Loading %s..."" % ""legislators-historical.yaml"")
    legislators = load_data(""legislators-historical.yaml"")
    data_files.append((legislators,""legislators-historical.yaml""))

    # load member data from vote view
    if congress == None:
        raise Exception(""the --congress flag is required"")
    elif int(congress) < 10 and int(congress) > 0:
        url_senate = ""https://voteview.com/static/data/out/members/S00%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H00%s_members.csv"" % congress
    elif int(congress) < 100 and int(congress) >= 10:
        url_senate = ""https://voteview.com/static/data/out/members/S0%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H0%s_members.csv"" % congress
    elif int(congress) >= 100:
        url_senate = ""https://voteview.com/static/data/out/members/S%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H%s_members.csv"" % congress
    else:
        raise Exception(""no data for congress "" + congress)

    senate_destination = ""icpsr/source/senate_rollcall%s.txt"" % congress
    senate_data = utils.download(url_senate, senate_destination, force)

    house_destination = ""icpsr/source/house_rollcall%s.txt"" % congress
    house_data = utils.download(url_house, house_destination, force)

    error_log = csv.writer(open(""cache/errors/mismatch/mismatch_%s.csv"" % congress, ""w""))
    error_log.writerow([""error_type"",""matches"",""icpsr_name"",""icpsr_state"",""is_territory"",""old_id"",""new_id""])



    read_files = [(""sen"",senate_data),(""rep"",house_data)]
    print(""Running for congress "" + congress)
    for read_file_chamber,read_file_content in read_files:
        for data_file in data_files:
            for legislator in data_file[0]:
                num_matches = 0
                write_id = """"
                # this can't run unless we've already collected a bioguide for this person
                bioguide = legislator[""id""].get(""bioguide"", None)
                # if we've limited this to just one bioguide, skip over everyone else
                if only_bioguide and (bioguide != only_bioguide):
                    continue
                #if not in currently read chamber, skip
                chamber = legislator['terms'][len(legislator['terms'])-1]['type']
                if chamber != read_file_chamber:
                    continue

                #only run for selected congress
                latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms'])-1]['start'])))
                if chamber == ""sen"":
                    congresses = [latest_congress,latest_congress+1,latest_congress+2]
                else:
                    congresses =[latest_congress]

                if int(congress) not in congresses:
                    continue

                # pull data to match from yaml

                last_name = legislator['name']['last'].upper()
                state = utils.states[legislator['terms'][len(legislator['terms'])-1]['state']].upper()[:7].strip()

                # convert read_file_content str to file object, then parse as csv file
                content_as_file = StringIO(read_file_content)
                content_parsed = csv.reader(content_as_file, delimiter=',')

                # loop through congress members in read file, see if one matches the current legislator
                for icpsr_member in content_parsed:
                    # ensure unique match bassed of bioguide id
                    if bioguide == icpsr_member[10]:
                        num_matches += 1
                        write_id = int(icpsr_member[2])

                # skip if icpsr id is currently in data
                if ""icpsr"" in legislator[""id""]:
                    if write_id == legislator[""id""][""icpsr""] or write_id == """":
                        continue
                    elif write_id != legislator[""id""][""icpsr""] and write_id != """":
                        error_log.writerow([""Incorrect_ID"",""NA"",last_name[:8],state,""NA"",legislator[""id""][""icpsr""],write_id])
                        print(""ID updated for %s"" % last_name)

                if num_matches == 1:
                    legislator['id']['icpsr'] = int(write_id)
                else:
                    if state == 'GUAM' or state == 'PUERTO' or state == ""VIRGIN"" or state == ""DISTRIC"" or state == ""AMERICA"" or state == ""NORTHER"" or state == ""PHILIPP"":
                        print('error: non 1 match')
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name[:8],state,""Y"",""NA"",""NA""])
                    else:
                        print(str(num_matches) + "" matches found for ""+ last_name[:8] + "", "" + state + "" in congress "" + str(congress))
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name,state,""N"",""NA"",""NA""])

            save_data(data_file[0], data_file[1])","for icpsr_member in content_parsed:
    if bioguide == icpsr_member[10]:
        num_matches += 1
        write_id = int(icpsr_member[2])","for icpsr_member in content_parsed:
    (*icpsr_member_ricpsr_membermaining, icpsr_member_10, _, icpsr_member_2, *icpsr_member_ricpsr_membermaining_2) = icpsr_member
    if bioguide == icpsr_member[10]:
        num_matches += 1
        write_id = int(icpsr_member[2])",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_10, _, e_2, *e_remaining_2 = e
variable mapping:
e_10: e[10]
e_2: e[2]",,,,,,,
coa_tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/operators/edit_mesh.py,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/operators/edit_mesh.py,,average_edge_cuts$91,"def average_edge_cuts(bm,obj,cuts=1):
    ### collapse short edges
    edges_len_average, shortest_edge = get_average_edge_length(bm,obj)
    
    subdivide_edges = []
    for edge in bm.edges:
        cut_count = int(edge.calc_length()/shortest_edge)*cuts
        if cut_count < 0:
            cut_count = 0
        if not edge.is_boundary:
            subdivide_edges.append([edge,cut_count])
    for edge in subdivide_edges:
        bmesh.ops.subdivide_edges(bm,edges=[edge[0]],cuts=edge[1])
        bmesh.update_edit_mesh(obj.data)","for edge in subdivide_edges:
    bmesh.ops.subdivide_edges(bm, edges=[edge[0]], cuts=edge[1])
    bmesh.update_edit_mesh(obj.data)","for edge in subdivide_edges:
    (edge_0, edge_1, *_) = edge
    bmesh.ops.subdivide_edges(bm, edges=[edge[0]], cuts=edge[1])
    bmesh.update_edit_mesh(obj.data)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
bert,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bert/modeling.py,https://github.com/google-research/bert/tree/master//modeling.py,,get_assignment_map_from_checkpoint$317,"def get_assignment_map_from_checkpoint(tvars, init_checkpoint):
  """"""Compute the union of the current variables and checkpoint variables.""""""
  assignment_map = {}
  initialized_variable_names = {}

  name_to_variable = collections.OrderedDict()
  for var in tvars:
    name = var.name
    m = re.match(""^(.*):\\d+$"", name)
    if m is not None:
      name = m.group(1)
    name_to_variable[name] = var

  init_vars = tf.train.list_variables(init_checkpoint)

  assignment_map = collections.OrderedDict()
  for x in init_vars:
    (name, var) = (x[0], x[1])
    if name not in name_to_variable:
      continue
    assignment_map[name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[name + "":0""] = 1

  return (assignment_map, initialized_variable_names)","for x in init_vars:
    (name, var) = (x[0], x[1])
    if name not in name_to_variable:
        continue
    assignment_map[name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[name + ':0'] = 1","for x in init_vars:
    (x_0, x_1, *_) = x
    (name, var) = (x[0], x[1])
    if name not in name_to_variable:
        continue
    assignment_map[name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[name + ':0'] = 1","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Home-Assistant-custom-components-Xiaomi-Cloud-Map-Extractor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Home-Assistant-custom-components-Xiaomi-Cloud-Map-Extractor/custom_components/xiaomi_cloud_map_extractor/roidmi/map_data_parser.py,https://github.com/PiotrMachowski/Home-Assistant-custom-components-Xiaomi-Cloud-Map-Extractor/tree/master/custom_components/xiaomi_cloud_map_extractor/roidmi/map_data_parser.py,MapDataParserRoidmi,parse_path$85,"def parse_path(map_info: dict) -> Path:
        path_points = []
        if ""posArray"" in map_info:
            raw_points = json.loads(map_info[""posArray""])
            for raw_point in raw_points:
                point = Point(raw_point[0], raw_point[1])
                path_points.append(point)
        return Path(None, None, None, [path_points])","for raw_point in raw_points:
    point = Point(raw_point[0], raw_point[1])
    path_points.append(point)","for raw_point in raw_points:
    (raw_point_0, raw_point_1, *_) = raw_point
    point = Point(raw_point[0], raw_point[1])
    path_points.append(point)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
adminset,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adminset/cmdb/asset.py,https://github.com/guohongze/adminset/tree/master/cmdb/asset.py,,asset_import$160,"def asset_import(request):
    if request.method == ""POST"":
        uf = request.FILES.get('asset_import')
        with open(""/var/opt/adminset/data/asset.csv"", ""wb+"") as f:
            for chunk in uf.chunks(chunk_size=1024):
                f.write(chunk)
        try:
            filename = ""/var/opt/adminset/data/asset.csv""
            with open(filename, ""rb"") as f:
                title = next(csv.reader(f))
                for data in csv.reader(f):
                    data0 = str2gb2utf8(data[0])
                    if data0 == u""主机名"":
                        continue
                    try:
                        host = Host.objects.get(hostname=data0)
                    except Exception as msg:
                        host = Host()
                        host.hostname = data0
                    host.ip = data[1]
                    host.other_ip = str2gb2utf8(data[2])
                    if data[3]:
                        try:
                            idc_name = str2gb2utf8(data[3])
                            print(""idc name is : {}"".format(idc_name))
                            print(""idc name type: {}"".format(type(idc_name)))
                            item = Idc.objects.get(name=idc_name)
                            host.idc_id = item.id
                        except Exception as e:
                            print(e)
                            print(""idc info import error"")
                    host.asset_no = str2gb2utf8(data[4])
                    if data[5]:
                        asset_type = str2gb2utf8(data[5])
                        for x, v in ASSET_TYPE:
                            if v == asset_type:
                                ret = x
                        host.asset_type = ret
                    if data[6]:
                        status = str2gb2utf8(data[6])
                        for x, v in ASSET_STATUS:
                            if v == status:
                                ret = x
                        host.status = ret
                    host.os = str2gb2utf8(data[7])
                    host.vendor = str2gb2utf8(data[8])
                    host.cpu_model = str2gb2utf8(data[9])
                    host.cpu_num = str2gb2utf8(data[10])
                    host.memory = str2gb2utf8(data[11])
                    host.disk = (data[12])
                    host.sn = str2gb2utf8(data[13])
                    host.position = str2gb2utf8(data[14])
                    host.memo = str2gb2utf8(data[15])
                    host.save()
            os.remove(filename)
            status = 1
        except Exception as e:
            print(e)
            print(""import asset csv file error!"")
            status = 2

    return render(request, 'cmdb/import.html', locals())","for data in csv.reader(f):
    data0 = str2gb2utf8(data[0])
    if data0 == u'主机名':
        continue
    try:
        host = Host.objects.get(hostname=data0)
    except Exception as msg:
        host = Host()
        host.hostname = data0
    host.ip = data[1]
    host.other_ip = str2gb2utf8(data[2])
    if data[3]:
        try:
            idc_name = str2gb2utf8(data[3])
            print('idc name is : {}'.format(idc_name))
            print('idc name type: {}'.format(type(idc_name)))
            item = Idc.objects.get(name=idc_name)
            host.idc_id = item.id
        except Exception as e:
            print(e)
            print('idc info import error')
    host.asset_no = str2gb2utf8(data[4])
    if data[5]:
        asset_type = str2gb2utf8(data[5])
        for (x, v) in ASSET_TYPE:
            if v == asset_type:
                ret = x
        host.asset_type = ret
    if data[6]:
        status = str2gb2utf8(data[6])
        for (x, v) in ASSET_STATUS:
            if v == status:
                ret = x
        host.status = ret
    host.os = str2gb2utf8(data[7])
    host.vendor = str2gb2utf8(data[8])
    host.cpu_model = str2gb2utf8(data[9])
    host.cpu_num = str2gb2utf8(data[10])
    host.memory = str2gb2utf8(data[11])
    host.disk = data[12]
    host.sn = str2gb2utf8(data[13])
    host.position = str2gb2utf8(data[14])
    host.memo = str2gb2utf8(data[15])
    host.save()","for data in csv.reader(f):
    (data_0, data_10, data_11, data_12, data_13, data_14, data_15, data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, *_) = data
    data0 = str2gb2utf8(data[0])
    if data0 == u'主机名':
        continue
    try:
        host = Host.objects.get(hostname=data0)
    except Exception as msg:
        host = Host()
        host.hostname = data0
    host.ip = data[1]
    host.other_ip = str2gb2utf8(data[2])
    if data[3]:
        try:
            idc_name = str2gb2utf8(data[3])
            print('idc name is : {}'.format(idc_name))
            print('idc name type: {}'.format(type(idc_name)))
            item = Idc.objects.get(name=idc_name)
            host.idc_id = item.id
        except Exception as e:
            print(e)
            print('idc info import error')
    host.asset_no = str2gb2utf8(data[4])
    if data[5]:
        asset_type = str2gb2utf8(data[5])
        for (x, v) in ASSET_TYPE:
            if v == asset_type:
                ret = x
        host.asset_type = ret
    if data[6]:
        status = str2gb2utf8(data[6])
        for (x, v) in ASSET_STATUS:
            if v == status:
                ret = x
        host.status = ret
    host.os = str2gb2utf8(data[7])
    host.vendor = str2gb2utf8(data[8])
    host.cpu_model = str2gb2utf8(data[9])
    host.cpu_num = str2gb2utf8(data[10])
    host.memory = str2gb2utf8(data[11])
    host.disk = data[12]
    host.sn = str2gb2utf8(data[13])
    host.position = str2gb2utf8(data[14])
    host.memo = str2gb2utf8(data[15])
    host.save()","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_10, e_11, e_12, e_13, e_14, e_15, e_1, e_2, e_3, e_4, e_5, e_6, e_7, e_8, e_9 = e
variable mapping:
e_0: e[0]
e_10: e[10]
e_11: e[11]
e_12: e[12]
e_13: e[13]
e_14: e[14]
e_15: e[15]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]
e_5: e[5]
e_6: e[6]
e_7: e[7]
e_8: e[8]
e_9: e[9]",,,,,,,
SSDTTime,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SSDTTime/SSDTTime.py,https://github.com/corpnewt/SSDTTime/tree/master//SSDTTime.py,SSDT,fake_ec$134,"def fake_ec(self, laptop = False):
        rename = False
        if not self.ensure_dsdt():
            return
        self.u.head(""Fake EC"")
        print("""")
        print(""Locating PNP0C09 (EC) devices..."")
        ec_list = self.d.get_device_paths_with_hid(""PNP0C09"")
        ec_to_patch  = []
        patches = []
        lpc_name = None
        if len(ec_list):
            lpc_name = ""."".join(ec_list[0][0].split(""."")[:-1])
            print("" - Got {}"".format(len(ec_list)))
            print("" - Validating..."")
            for x in ec_list:
                device = x[0]
                print("" --> {}"".format(device))
                if device.split(""."")[-1] == ""EC"":
                    if laptop:
                        print("" ----> Named EC device located - no fake needed."")
                        print("""")
                        self.u.grab(""Press [enter] to return to main menu..."")
                        return
                    print("" ----> EC called EC. Renaming"")
                    device = ""."".join(device.split(""."")[:-1]+[""EC0""])
                    rename = True
                scope = ""\n"".join(self.d.get_scope(x[1],strip_comments=True))
                # We need to check for _HID, _CRS, and _GPE
                if all((y in scope for y in [""_HID"",""_CRS"",""_GPE""])):
                    print("" ----> Valid EC Device"")
                    sta = self.d.get_method_paths(device+""._STA"")
                    if len(sta):
                        print("" ----> Contains _STA method. Skipping"")
                        continue
                    if not laptop:
                        ec_to_patch.append(device)
                else:
                    print("" ----> NOT Valid EC Device"")
        else:
            print("" - None found - only needs a Fake EC device"")
        print(""Locating LPC(B)/SBRG..."")
        if lpc_name == None:
            for x in (""LPCB"", ""LPC0"", ""LPC"", ""SBRG"", ""PX40""):
                try:
                    lpc_name = self.d.get_device_paths(x)[0][0]
                    break
                except: pass
        if not lpc_name:
            print("" - Could not locate LPC(B)! Aborting!"")
            print("""")
            self.u.grab(""Press [enter] to return to main menu..."")
            return
        print("" - Found {}"".format(lpc_name))
        comment = ""SSDT-EC""
        if rename == True:
            patches.append({""Comment"":""EC to EC0"",""Find"":""45435f5f"",""Replace"":""4543305f""})
            comment += "" (Needs EC to EC0 rename)""
        oc = {""Comment"":comment,""Enabled"":True,""Path"":""SSDT-EC.aml""}
        self.make_plist(oc, ""SSDT-EC.aml"", patches)
        print(""Creating SSDT-EC..."")
        ssdt = """"""
DefinitionBlock ("""", ""SSDT"", 2, ""CORP "", ""SsdtEC"", 0x00001000)
{
    External ([[LPCName]], DeviceObj)
"""""".replace(""[[LPCName]]"",lpc_name)
        for x in ec_to_patch:
            ssdt += ""    External ({}, DeviceObj)\n"".format(x)
        # Walk them again and add the _STAs
        for x in ec_to_patch:
            ssdt += """"""
    Scope ([[ECName]])
    {
        Method (_STA, 0, NotSerialized)  // _STA: Status
        {
            If (_OSI (""Darwin""))
            {
                Return (0)
            }
            Else
            {
                Return (0x0F)
            }
        }
    }
"""""".replace(""[[LPCName]]"",lpc_name).replace(""[[ECName]]"",x)
        # Create the faked EC
        ssdt += """"""
    Scope ([[LPCName]])
    {
        Device (EC)
        {
            Name (_HID, ""ACID0001"")  // _HID: Hardware ID
            Method (_STA, 0, NotSerialized)  // _STA: Status
            {
                If (_OSI (""Darwin""))
                {
                    Return (0x0F)
                }
                Else
                {
                    Return (Zero)
                }
            }
        }
    }
}"""""".replace(""[[LPCName]]"",lpc_name)
        self.write_ssdt(""SSDT-EC"",ssdt)
        print("""")
        print(""Done."")
        print("""")
        self.u.grab(""Press [enter] to return..."")","for x in ec_list:
    device = x[0]
    print(' --> {}'.format(device))
    if device.split('.')[-1] == 'EC':
        if laptop:
            print(' ----> Named EC device located - no fake needed.')
            print('')
            self.u.grab('Press [enter] to return to main menu...')
            return
        print(' ----> EC called EC. Renaming')
        device = '.'.join(device.split('.')[:-1] + ['EC0'])
        rename = True
    scope = '\n'.join(self.d.get_scope(x[1], strip_comments=True))
    if all((y in scope for y in ['_HID', '_CRS', '_GPE'])):
        print(' ----> Valid EC Device')
        sta = self.d.get_method_paths(device + '._STA')
        if len(sta):
            print(' ----> Contains _STA method. Skipping')
            continue
        if not laptop:
            ec_to_patch.append(device)
    else:
        print(' ----> NOT Valid EC Device')","for x in ec_list:
    (x_0, x_1, *_) = x
    device = x[0]
    print(' --> {}'.format(device))
    if device.split('.')[-1] == 'EC':
        if laptop:
            print(' ----> Named EC device located - no fake needed.')
            print('')
            self.u.grab('Press [enter] to return to main menu...')
            return
        print(' ----> EC called EC. Renaming')
        device = '.'.join(device.split('.')[:-1] + ['EC0'])
        rename = True
    scope = '\n'.join(self.d.get_scope(x[1], strip_comments=True))
    if all((y in scope for y in ['_HID', '_CRS', '_GPE'])):
        print(' ----> Valid EC Device')
        sta = self.d.get_method_paths(device + '._STA')
        if len(sta):
            print(' ----> Contains _STA method. Skipping')
            continue
        if not laptop:
            ec_to_patch.append(device)
    else:
        print(' ----> NOT Valid EC Device')","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
AlgorithmsByPython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython/Target Offer/multiSparse.py,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master/Target Offer/multiSparse.py,,tripleToSparse$25,"def tripleToSparse(triple, m, n):
    outMatrix = zeros([m, n])
    for pointTuple in triple:
        mLocation = pointTuple[0]
        nLocation = pointTuple[1]
        value = pointTuple[2]
        outMatrix[mLocation][nLocation] = value
    return outMatrix","for pointTuple in triple:
    mLocation = pointTuple[0]
    nLocation = pointTuple[1]
    value = pointTuple[2]
    outMatrix[mLocation][nLocation] = value","for pointTuple in triple:
    (pointTuple_0, pointTuple_1, pointTuple_2, *_) = pointTuple
    mLocation = pointTuple[0]
    nLocation = pointTuple[1]
    value = pointTuple[2]
    outMatrix[mLocation][nLocation] = value","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
pinax-blog,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pinax-blog/pinax/blog/migrations/0003_auto_20150529_0405.py,https://github.com/pinax/pinax-blog/tree/master/pinax/blog/migrations/0003_auto_20150529_0405.py,,seed_sections$8,"def seed_sections(apps, schema_editor):
    Section = apps.get_model(""blog"", ""Section"")
    db_alias = schema_editor.connection.alias
    for section in settings.PINAX_BLOG_SECTIONS:
        Section.objects.using(db_alias).create(slug=section[0], name=section[1])","for section in settings.PINAX_BLOG_SECTIONS:
    Section.objects.using(db_alias).create(slug=section[0], name=section[1])","for section in settings.PINAX_BLOG_SECTIONS:
    (section_0, section_1, *_) = section
    Section.objects.using(db_alias).create(slug=section[0], name=section[1])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
kivy-designer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy-designer/designer/components/playground.py,https://github.com/kivy/kivy-designer/tree/master/designer/components/playground.py,Playground,get_playground_drag_element$847,"def get_playground_drag_element(self, instance, widget_name, touch,
                                    default_args, extra_args, *args):
        '''This function will return the desired playground element
           for widget_name.
           :param extra_args: extra args used to display the dragging widget
           :param default_args: default widget args
           :param touch: instance of the current touch
           :param instance: if from toolbox, ToolboxButton instance.
                    None otherwise
           :param widget_name: name of the widget that will be dragged
        '''

        # create default widget that will be added and the custom to display
        widget = self.get_widget(widget_name, **default_args)
        widget._KD_KV_STR = self.generate_kv_from_args(widget_name,
                                                       default_args)
        values = default_args.copy()
        values.update(extra_args)
        child = self.get_widget(widget_name, **values)
        custom = False
        for op in widgets_common:
            if op[0] == widget_name:
                if op[1] == 'custom':
                    custom = True
                break
        container = PlaygroundDragElement(
                playground=self, child=child, widget=widget)
        if not custom:
            container.fit_child()
        touch.grab(container)
        touch_pos = [touch.x, touch.y]
        if instance:
            touch_pos = instance.to_window(*touch.pos)
        container.center_x = touch_pos[0]
        container.y = touch_pos[1] + 20
        return container","for op in widgets_common:
    if op[0] == widget_name:
        if op[1] == 'custom':
            custom = True
        break","for op in widgets_common:
    (op_0, op_1, *_) = op
    if op[0] == widget_name:
        if op[1] == 'custom':
            custom = True
        break","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
anime-downloader,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anime-downloader/anime_downloader/sites/init.py,https://github.com/anime-dl/anime-downloader/tree/master/anime_downloader/sites/init.py,,get_anime_class$53,"def get_anime_class(url):
    """"""
    Get anime class corresposing to url or name.
    See :py:data:`anime_downloader.sites.ALL_ANIME_SITES` to get the possible anime sites.

    Parameters
    ----------
    url: string
        URL of the anime.

    Returns
    -------
    :py:class:`anime_downloader.sites.anime.Anime`
        Concrete implementation of :py:class:`anime_downloader.sites.anime.Anime`
    """"""
    for site in ALL_ANIME_SITES:
        if site[1] in url:
            try:
                module = import_module(
                    'anime_downloader.sites.{}'.format(site[0])
                )
            except ImportError:
                raise
            return getattr(module, site[2])","for site in ALL_ANIME_SITES:
    if site[1] in url:
        try:
            module = import_module('anime_downloader.sites.{}'.format(site[0]))
        except ImportError:
            raise
        return getattr(module, site[2])","for site in ALL_ANIME_SITES:
    (site_0, site_1, site_2, *_) = site
    if site[1] in url:
        try:
            module = import_module('anime_downloader.sites.{}'.format(site[0]))
        except ImportError:
            raise
        return getattr(module, site[2])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/core/chords.py,https://github.com/bspaans/python-mingus/tree/master/mingus/core/chords.py,,inversion_exhauster$1183,"def inversion_exhauster(
        chord,
        shorthand,
        tries,
        result,
        polychords,
    ):
        """"""Recursive helper function""""""

        # Determine polychords
        if tries == 1 and not no_polychords:
            polychords += determine_polychords(chord, shorthand)

        def add_result(short):
            result.append((short, tries, chord[0]))

        ch = determine_extended_chord5(chord[:5], True, True, True)
        intval5 = intervals.determine(chord[0], chord[5])
        for c in ch:
            c = c[len(chord[0]) :]
            if c == ""9"":
                if intval5 == ""perfect fourth"":
                    add_result(""11"")
                elif intval5 == ""augmented fourth"":
                    add_result(""7#11"")
                elif intval5 == ""major sixth"":
                    add_result(""13"")
            elif c == ""m9"":
                if intval5 == ""perfect fourth"":
                    add_result(""m11"")
                elif intval5 == ""major sixth"":
                    add_result(""m13"")
            elif c == ""M9"":
                if intval5 == ""perfect fourth"":
                    add_result(""M11"")
                elif intval5 == ""major sixth"":
                    add_result(""M13"")
        if tries != 6 and not no_inversions:
            return inversion_exhauster(
                [chord[-1]] + chord[:-1], shorthand, tries + 1, result, polychords
            )
        else:
            res = []
            for r in result:
                if shorthand:
                    res.append(r[2] + r[0])
                else:
                    res.append(r[2] + chord_shorthand_meaning[r[0]] + int_desc(r[1]))
            return res + polychords","for c in ch:
    c = c[len(chord[0]):]
    if c == '9':
        if intval5 == 'perfect fourth':
            add_result('11')
        elif intval5 == 'augmented fourth':
            add_result('7#11')
        elif intval5 == 'major sixth':
            add_result('13')
    elif c == 'm9':
        if intval5 == 'perfect fourth':
            add_result('m11')
        elif intval5 == 'major sixth':
            add_result('m13')
    elif c == 'M9':
        if intval5 == 'perfect fourth':
            add_result('M11')
        elif intval5 == 'major sixth':
            add_result('M13')",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, = e[len(ehord[0]):]
variable mapping:
e[len(ehord[0]):]: e[len(ehord[0]):]",,,,,,,
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/core/chords.py,https://github.com/bspaans/python-mingus/tree/master/mingus/core/chords.py,,inversion_exhauster$1183,"def inversion_exhauster(
        chord,
        shorthand,
        tries,
        result,
        polychords,
    ):
        """"""Recursive helper function""""""

        # Determine polychords
        if tries == 1 and not no_polychords:
            polychords += determine_polychords(chord, shorthand)

        def add_result(short):
            result.append((short, tries, chord[0]))

        ch = determine_extended_chord5(chord[:5], True, True, True)
        intval5 = intervals.determine(chord[0], chord[5])
        for c in ch:
            c = c[len(chord[0]) :]
            if c == ""9"":
                if intval5 == ""perfect fourth"":
                    add_result(""11"")
                elif intval5 == ""augmented fourth"":
                    add_result(""7#11"")
                elif intval5 == ""major sixth"":
                    add_result(""13"")
            elif c == ""m9"":
                if intval5 == ""perfect fourth"":
                    add_result(""m11"")
                elif intval5 == ""major sixth"":
                    add_result(""m13"")
            elif c == ""M9"":
                if intval5 == ""perfect fourth"":
                    add_result(""M11"")
                elif intval5 == ""major sixth"":
                    add_result(""M13"")
        if tries != 6 and not no_inversions:
            return inversion_exhauster(
                [chord[-1]] + chord[:-1], shorthand, tries + 1, result, polychords
            )
        else:
            res = []
            for r in result:
                if shorthand:
                    res.append(r[2] + r[0])
                else:
                    res.append(r[2] + chord_shorthand_meaning[r[0]] + int_desc(r[1]))
            return res + polychords","for r in result:
    if shorthand:
        res.append(r[2] + r[0])
    else:
        res.append(r[2] + chord_shorthand_meaning[r[0]] + int_desc(r[1]))","for r in result:
    (r_0, r_1, r_2, *_) = r
    if shorthand:
        res.append(r[2] + r[0])
    else:
        res.append(r[2] + chord_shorthand_meaning[r[0]] + int_desc(r[1]))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
rope,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rope/rope/refactor/change_signature.py,https://github.com/python-rope/rope/tree/master/rope/refactor/change_signature.py,ArgumentAdder,change_definition_info$255,"def change_definition_info(self, definition_info):
        for pair in definition_info.args_with_defaults:
            if pair[0] == self.name:
                raise rope.base.exceptions.RefactoringError(
                    ""Adding duplicate parameter: <%s>."" % self.name
                )
        definition_info.args_with_defaults.insert(self.index, (self.name, self.default))","for pair in definition_info.args_with_defaults:
    if pair[0] == self.name:
        raise rope.base.exceptions.RefactoringError('Adding duplicate parameter: <%s>.' % self.name)","for pair in definition_info.args_with_defaults:
    (pair_0, *pair_rpairmaining) = pair
    if pair[0] == self.name:
        raise rope.base.exceptions.RefactoringError('Adding duplicate parameter: <%s>.' % self.name)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/special_agents/agent_vsphere.py,https://github.com/tribe29/checkmk/tree/master/cmk/special_agents/agent_vsphere.py,,eval_snapshot_list$1689,"def eval_snapshot_list(info, _datastores):
    response = []
    snapshot_info = get_pattern(
        ""<name>(.*?)</name>.*?<id>(.*?)</id><createTime>(.*?)</createTime><state>(.*?)</state>"",
        info,
    )
    for entry in snapshot_info:
        try:
            # 2013-11-06T15:39:39.347543Z
            creation_time = int(time.mktime(time.strptime(entry[2][:19], ""%Y-%m-%dT%H:%M:%S"")))
        except ValueError:
            creation_time = 0
        response.append(
            ""%s %s %s %s"" % (entry[1], creation_time, entry[3], entry[0].replace(""|"", "" ""))
        )
    return ""|"".join(response)","for entry in snapshot_info:
    try:
        creation_time = int(time.mktime(time.strptime(entry[2][:19], '%Y-%m-%dT%H:%M:%S')))
    except ValueError:
        creation_time = 0
    response.append('%s %s %s %s' % (entry[1], creation_time, entry[3], entry[0].replace('|', ' ')))","for entry in snapshot_info:
    (entry_0, entry_1, (entry_2_0, *entry_2_rentrymaining), entry_3, *entry_rentrymaining) = entry
    try:
        creation_time = int(time.mktime(time.strptime(entry[2][:19], '%Y-%m-%dT%H:%M:%S')))
    except ValueError:
        creation_time = 0
    response.append('%s %s %s %s' % (entry[1], creation_time, entry[3], entry[0].replace('|', ' ')))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",0,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, (e_2_0, *e_2_remaining), e_3, *e_remaining = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2[:19]: e[2][:19]
e_3: e[3]",,,,,,,
frigate,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frigate/frigate/edgetpu.py,https://github.com/blakeblackshear/frigate/tree/master/frigate/edgetpu.py,RemoteObjectDetector,detect$260,"def detect(self, tensor_input, threshold=0.4):
        detections = []

        # copy input to shared memory
        self.np_shm[:] = tensor_input[:]
        self.event.clear()
        self.detection_queue.put(self.name)
        result = self.event.wait(timeout=10.0)

        # if it timed out
        if result is None:
            return detections

        for d in self.out_np_shm:
            if d[1] < threshold:
                break
            detections.append(
                (self.labels[int(d[0])], float(d[1]), (d[2], d[3], d[4], d[5]))
            )
        self.fps.update()
        return detections","for d in self.out_np_shm:
    if d[1] < threshold:
        break
    detections.append((self.labels[int(d[0])], float(d[1]), (d[2], d[3], d[4], d[5])))","for d in self.out_np_shm:
    (d_1, d_2, d_3, d_4, d_5, *_) = d
    if d[1] < threshold:
        break
    detections.append((self.labels[int(d[0])], float(d[1]), (d[2], d[3], d[4], d[5])))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",0,,,"Answer: Yes
Iterable Unpacking: e_1, e_2, e_3, e_4, e_5 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]
e_5: e[5]",,,,,,,
sktime,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sktime/sktime/benchmarking/evaluation.py,https://github.com/alan-turing-institute/sktime/tree/master/sktime/benchmarking/evaluation.py,Evaluator,wilcoxon_test$322,"def wilcoxon_test(self, metric_name=None):
        """"""Wilcoxon signed-rank test.

        http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
        `Wilcoxon signed-rank test
        <https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test>`_.
        Tests whether two  related paired samples come from the same
        distribution. In particular, it tests whether the distribution of the
        differences x-y is symmetric about zero
        """"""
        self._check_is_evaluated()
        metric_name = self._validate_metric_name(metric_name)
        metrics_per_estimator_dataset = self._get_metrics_per_estimator_dataset(
            metric_name
        )

        wilcoxon_df = pd.DataFrame()
        prod = itertools.combinations(metrics_per_estimator_dataset.keys(), 2)
        for p in prod:
            estim_1 = p[0]
            estim_2 = p[1]
            w, p_val = stats.wilcoxon(
                metrics_per_estimator_dataset[p[0]], metrics_per_estimator_dataset[p[1]]
            )

            w_test = {
                ""estimator_1"": estim_1,
                ""estimator_2"": estim_2,
                ""statistic"": w,
                ""p_val"": p_val,
            }

            wilcoxon_df = wilcoxon_df.append(w_test, ignore_index=True)

        return wilcoxon_df","for p in prod:
    estim_1 = p[0]
    estim_2 = p[1]
    (w, p_val) = stats.wilcoxon(metrics_per_estimator_dataset[p[0]], metrics_per_estimator_dataset[p[1]])
    w_test = {'estimator_1': estim_1, 'estimator_2': estim_2, 'statistic': w, 'p_val': p_val}
    wilcoxon_df = wilcoxon_df.append(w_test, ignore_index=True)","for p in prod:
    (p_0, p_1, *_) = p
    estim_1 = p[0]
    estim_2 = p[1]
    (w, p_val) = stats.wilcoxon(metrics_per_estimator_dataset[p[0]], metrics_per_estimator_dataset[p[1]])
    w_test = {'estimator_1': estim_1, 'estimator_2': estim_2, 'statistic': w, 'p_val': p_val}
    wilcoxon_df = wilcoxon_df.append(w_test, ignore_index=True)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Transformer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer/Beam.py,https://github.com/SamLynnEvans/Transformer/tree/master//Beam.py,,beam_search$55,"def beam_search(src, model, SRC, TRG, opt):
    

    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, opt)
    eos_tok = TRG.vocab.stoi['<eos>']
    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)
    ind = None
    for i in range(2, opt.max_len):
    
        trg_mask = nopeak_mask(i, opt)

        out = model.out(model.decoder(outputs[:,:i],
        e_outputs, src_mask, trg_mask))

        out = F.softmax(out, dim=-1)
    
        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, opt.k)
        
        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.
        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()
        for vec in ones:
            i = vec[0]
            if sentence_lengths[i]==0: # First end symbol has not been found yet
                sentence_lengths[i] = vec[1] # Position of first end symbol

        num_finished_sentences = len([s for s in sentence_lengths if s > 0])

        if num_finished_sentences == opt.k:
            alpha = 0.7
            div = 1/(sentence_lengths.type_as(log_scores)**alpha)
            _, ind = torch.max(log_scores * div, 1)
            ind = ind.data[0]
            break
    
    if ind is None:
        length = (outputs[0]==eos_tok).nonzero()[0]
        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])
    
    else:
        length = (outputs[ind]==eos_tok).nonzero()[0]
        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])","for vec in ones:
    i = vec[0]
    if sentence_lengths[i] == 0:
        sentence_lengths[i] = vec[1]","for vec in ones:
    (vec_0, vec_1, *_) = vec
    i = vec[0]
    if sentence_lengths[i] == 0:
        sentence_lengths[i] = vec[1]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pytext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytext/pytext/torchscript/seq2seq/export_model.py,https://github.com/facebookresearch/pytext/tree/master/pytext/torchscript/seq2seq/export_model.py,Seq2SeqJIT,forward$130,"def forward(
        self,
        src_tokens: List[str],
        dict_feat: Optional[Tuple[List[str], List[float], List[int]]] = None,
        contextual_token_embedding: Optional[List[float]] = None,
    ) -> List[Tuple[List[str], float, List[float]]]:

        word_ids = self.source_vocab.lookup_indices_1d(src_tokens)

        # find, if there exists, the unk token in the source utterance.
        # If multiple we select the first unk token.
        single_unk_token: Optional[str] = get_single_unk_token(
            src_tokens, word_ids, self.copy_unk_token, self.unk_idx
        )

        (
            words,
            dict_tensors,
            contextual_embedding_tensor,
            src_lengths,
        ) = self.prepare_generator_inputs(
            word_ids, dict_feat, contextual_token_embedding
        )
        hypos_etc = self.sequence_generator(
            words, dict_tensors, contextual_embedding_tensor, src_lengths
        )
        hypos_list: List[Tuple[List[str], float, List[float]]] = []

        filter_token_list: List[int] = []
        if self.filter_eos_bos:
            filter_token_list = [self.target_vocab.bos_idx, self.target_vocab.eos_idx]

        for seq in hypos_etc:
            hyopthesis = seq[0]
            stringified = self.target_vocab.lookup_words_1d(
                hyopthesis,
                filter_token_list=filter_token_list,
                possible_unk_token=single_unk_token,
            )
            hypos_list.append((stringified, seq[1], seq[2]))
        return hypos_list","for seq in hypos_etc:
    hyopthesis = seq[0]
    stringified = self.target_vocab.lookup_words_1d(hyopthesis, filter_token_list=filter_token_list, possible_unk_token=single_unk_token)
    hypos_list.append((stringified, seq[1], seq[2]))","for seq in hypos_etc:
    (seq_0, seq_1, seq_2, *_) = seq
    hyopthesis = seq[0]
    stringified = self.target_vocab.lookup_words_1d(hyopthesis, filter_token_list=filter_token_list, possible_unk_token=single_unk_token)
    hypos_list.append((stringified, seq[1], seq[2]))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
jesse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jesse/jesse/store/state_candles.py,https://github.com/jesse-ai/jesse/tree/master/jesse/store/state_candles.py,CandlesState,time_loop_per_second$29,"def time_loop_per_second():
            # make sure all candles are already initiated
            if not self.are_all_initiated:
                return

            # only at first second on each minute
            if jh.now() % 60_000 != 1000:
                return

            for c in config['app']['considering_candles']:
                exchange, symbol = c[0], c[1]
                current_candle = self.get_current_candle(exchange, symbol, '1m')

                # fix for for a bug
                if current_candle[0] <= 60_000:
                    continue

                if jh.now() > current_candle[0] + 60_000:
                    new_candle = self._generate_empty_candle_from_previous_candle(current_candle)
                    self.add_candle(new_candle, exchange, symbol, '1m')","for c in config['app']['considering_candles']:
    (exchange, symbol) = (c[0], c[1])
    current_candle = self.get_current_candle(exchange, symbol, '1m')
    if current_candle[0] <= 60000:
        continue
    if jh.now() > current_candle[0] + 60000:
        new_candle = self._generate_empty_candle_from_previous_candle(current_candle)
        self.add_candle(new_candle, exchange, symbol, '1m')","for c in config['app']['considering_candles']:
    (c_0, c_1, *_) = c
    (exchange, symbol) = (c[0], c[1])
    current_candle = self.get_current_candle(exchange, symbol, '1m')
    if current_candle[0] <= 60000:
        continue
    if jh.now() > current_candle[0] + 60000:
        new_candle = self._generate_empty_candle_from_previous_candle(current_candle)
        self.add_candle(new_candle, exchange, symbol, '1m')","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/common.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/common.py,,__get_loop_sequence_internal$824,"def __get_loop_sequence_internal(uv_layer, pairs, island_info, closed):
    loop_sequences = []
    for pair in pairs:
        seqs = [pair]
        p = pair
        isl_grp = __get_island_group_include_pair(pair, island_info)
        if isl_grp == -1:
            return None, ""Can not find the island or invalid island""

        while True:
            nlp = __get_next_loop_pair(p)
            if not nlp:
                break       # no more loop pair
            nlp_isl_grp = __get_island_group_include_pair(nlp, island_info)
            if nlp_isl_grp != isl_grp:
                break       # another island
            for nlpl in nlp:
                if nlpl[uv_layer].select:
                    return None, ""Do not select UV which does not belong to "" \
                                 ""the end edge""

            seqs.append(nlp)

            # when face is triangle, it indicates CLOSED
            if (len(nlp) == 1) and closed:
                break

            nplp = __get_next_poly_loop_pair(nlp)
            if not nplp:
                break       # no more loop pair
            nplp_isl_grp = __get_island_group_include_pair(nplp, island_info)
            if nplp_isl_grp != isl_grp:
                break       # another island

            # check if the UVs are already parsed.
            # this check is needed for the mesh which has the circular
            # sequence of the vertices
            matched = False
            for p1 in seqs:
                p2 = nplp
                if ((p1[0] == p2[0]) and (p1[1] == p2[1])) or \
                   ((p1[0] == p2[1]) and (p1[1] == p2[0])):
                    matched = True
            if matched:
                debug_print(""This is a circular sequence"")
                break

            for nlpl in nplp:
                if nlpl[uv_layer].select:
                    return None, ""Do not select UV which does not belong to "" \
                                 ""the end edge""

            seqs.append(nplp)

            p = nplp

        loop_sequences.append(seqs)
    return loop_sequences, """"","for nlpl in nlp:
    if nlpl[uv_layer].select:
        return (None, 'Do not select UV which does not belong to the end edge')","for nlpl in nlp:
    (nlpl_uv_laynlplr, *nlpl_rnlplmaining) = nlpl
    if nlpl[uv_layer].select:
        return (None, 'Do not select UV which does not belong to the end edge')",Cannot refactor,Cannot refactor,2,,,"Answer: It depends on the value of ""uv_layer"" and the type of iterable object ""e"".
Iterable Unpacking: If the iterable object ""e"" is a sequence type like a list or tuple and ""uv_layer"" is an integer type constant, then the answer is Yes and the iterable unpacking code is:

e_uv_layer, *e_remaining = e
variable mapping:
e_uv_layer: e[uv_layer]

If the iterable object ""e"" is a dictionary type and ""uv_layer"" is a string type constant, then the answer is Yes and the iterable unpacking code is:

e_uv_layer = e.get(uv_layer)
variable mapping:
e_uv_layer: e[uv_layer]

Otherwise, if the iterable object ""e"" is not a sequence or dictionary type or ""uv_layer"" is not an integer or string type constant, then the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/common.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/common.py,,__get_loop_sequence_internal$824,"def __get_loop_sequence_internal(uv_layer, pairs, island_info, closed):
    loop_sequences = []
    for pair in pairs:
        seqs = [pair]
        p = pair
        isl_grp = __get_island_group_include_pair(pair, island_info)
        if isl_grp == -1:
            return None, ""Can not find the island or invalid island""

        while True:
            nlp = __get_next_loop_pair(p)
            if not nlp:
                break       # no more loop pair
            nlp_isl_grp = __get_island_group_include_pair(nlp, island_info)
            if nlp_isl_grp != isl_grp:
                break       # another island
            for nlpl in nlp:
                if nlpl[uv_layer].select:
                    return None, ""Do not select UV which does not belong to "" \
                                 ""the end edge""

            seqs.append(nlp)

            # when face is triangle, it indicates CLOSED
            if (len(nlp) == 1) and closed:
                break

            nplp = __get_next_poly_loop_pair(nlp)
            if not nplp:
                break       # no more loop pair
            nplp_isl_grp = __get_island_group_include_pair(nplp, island_info)
            if nplp_isl_grp != isl_grp:
                break       # another island

            # check if the UVs are already parsed.
            # this check is needed for the mesh which has the circular
            # sequence of the vertices
            matched = False
            for p1 in seqs:
                p2 = nplp
                if ((p1[0] == p2[0]) and (p1[1] == p2[1])) or \
                   ((p1[0] == p2[1]) and (p1[1] == p2[0])):
                    matched = True
            if matched:
                debug_print(""This is a circular sequence"")
                break

            for nlpl in nplp:
                if nlpl[uv_layer].select:
                    return None, ""Do not select UV which does not belong to "" \
                                 ""the end edge""

            seqs.append(nplp)

            p = nplp

        loop_sequences.append(seqs)
    return loop_sequences, """"","for p1 in seqs:
    p2 = nplp
    if p1[0] == p2[0] and p1[1] == p2[1] or (p1[0] == p2[1] and p1[1] == p2[0]):
        matched = True","for p1 in seqs:
    (p1_0, p1_1, *_) = p1
    p2 = nplp
    if p1[0] == p2[0] and p1[1] == p2[1] or (p1[0] == p2[1] and p1[1] == p2[0]):
        matched = True","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/common.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/common.py,,__get_loop_sequence_internal$824,"def __get_loop_sequence_internal(uv_layer, pairs, island_info, closed):
    loop_sequences = []
    for pair in pairs:
        seqs = [pair]
        p = pair
        isl_grp = __get_island_group_include_pair(pair, island_info)
        if isl_grp == -1:
            return None, ""Can not find the island or invalid island""

        while True:
            nlp = __get_next_loop_pair(p)
            if not nlp:
                break       # no more loop pair
            nlp_isl_grp = __get_island_group_include_pair(nlp, island_info)
            if nlp_isl_grp != isl_grp:
                break       # another island
            for nlpl in nlp:
                if nlpl[uv_layer].select:
                    return None, ""Do not select UV which does not belong to "" \
                                 ""the end edge""

            seqs.append(nlp)

            # when face is triangle, it indicates CLOSED
            if (len(nlp) == 1) and closed:
                break

            nplp = __get_next_poly_loop_pair(nlp)
            if not nplp:
                break       # no more loop pair
            nplp_isl_grp = __get_island_group_include_pair(nplp, island_info)
            if nplp_isl_grp != isl_grp:
                break       # another island

            # check if the UVs are already parsed.
            # this check is needed for the mesh which has the circular
            # sequence of the vertices
            matched = False
            for p1 in seqs:
                p2 = nplp
                if ((p1[0] == p2[0]) and (p1[1] == p2[1])) or \
                   ((p1[0] == p2[1]) and (p1[1] == p2[0])):
                    matched = True
            if matched:
                debug_print(""This is a circular sequence"")
                break

            for nlpl in nplp:
                if nlpl[uv_layer].select:
                    return None, ""Do not select UV which does not belong to "" \
                                 ""the end edge""

            seqs.append(nplp)

            p = nplp

        loop_sequences.append(seqs)
    return loop_sequences, """"","for nlpl in nplp:
    if nlpl[uv_layer].select:
        return (None, 'Do not select UV which does not belong to the end edge')","for nlpl in nplp:
    (nlpl_uv_laynlplr, *nlpl_rnlplmaining) = nlpl
    if nlpl[uv_layer].select:
        return (None, 'Do not select UV which does not belong to the end edge')",Cannot refactor,Cannot refactor,2,,,"Answer: It depends on the value of ""uv_layer"" and the type of iterable object ""e"".
Iterable Unpacking: If the iterable object ""e"" is a sequence type like a list or tuple and ""uv_layer"" is an integer type constant, then the answer is Yes and the iterable unpacking code is:

e_uv_layer, *e_remaining = e
variable mapping:
e_uv_layer: e[uv_layer]

If the iterable object ""e"" is a dictionary and ""uv_layer"" is a string type constant, then the answer is Yes and the iterable unpacking code is:

e_uv_layer = e.get(uv_layer)
variable mapping:
e_uv_layer: e[uv_layer]

Otherwise, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
BlenderProc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BlenderProc/docs/source/ext/moduleoverview.py,https://github.com/DLR-RM/BlenderProc/tree/master/docs/source/ext/moduleoverview.py,,generate_tutorials_sidebar$63,"def generate_tutorials_sidebar(app, fromdocname, container):
    tutorials_dir = Path(__file__).absolute().parent.parent / ""docs"" / ""tutorials""

    tutorials = [
        (""Loading and manipulating objects"", ""loader""),
        (""Configuring the camera"", ""camera""),
        (""Rendering the scene"", ""renderer""),
        (""Writing the results to file"", ""writer""),
        (""How key frames work"", ""key_frames""),
        (""Positioning objects via the physics simulator"", ""physics""),
    ]

    container += nodes.caption(""Tutorials"", '', *[nodes.Text(""Tutorials"")])
    for tutorial in tutorials:
        toc = nodes.bullet_list()

        ref = nodes.reference('', '')
        ref['refuri'] = app.builder.get_relative_uri(fromdocname, ""docs/tutorials/"" + tutorial[1])
        ref.append(nodes.Text(tutorial[0]))
        module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=[""toctree-l1""])
        if fromdocname.startswith(""docs/tutorials/"" + tutorial[1]):
            module_item[""classes""].append('current')
        toc += module_item
        container += toc","for tutorial in tutorials:
    toc = nodes.bullet_list()
    ref = nodes.reference('', '')
    ref['refuri'] = app.builder.get_relative_uri(fromdocname, 'docs/tutorials/' + tutorial[1])
    ref.append(nodes.Text(tutorial[0]))
    module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=['toctree-l1'])
    if fromdocname.startswith('docs/tutorials/' + tutorial[1]):
        module_item['classes'].append('current')
    toc += module_item
    container += toc","for tutorial in tutorials:
    (tutorial_0, tutorial_1, *_) = tutorial
    toc = nodes.bullet_list()
    ref = nodes.reference('', '')
    ref['refuri'] = app.builder.get_relative_uri(fromdocname, 'docs/tutorials/' + tutorial[1])
    ref.append(nodes.Text(tutorial[0]))
    module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=['toctree-l1'])
    if fromdocname.startswith('docs/tutorials/' + tutorial[1]):
        module_item['classes'].append('current')
    toc += module_item
    container += toc","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
parliament,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/parliament/parliament/statement.py,https://github.com/duo-labs/parliament/tree/master/parliament/statement.py,Statement,_check_condition$496,"def _check_condition(self, operator, condition_block, expanded_actions):
        """"""
        operator is something like ""StringLike""
        condition_block is something like {""s3:prefix"":[""home/${aws:username}/*""]}
        """"""

        operator_type_requirement = None
        for documented_operator in OPERATORS:
            op = documented_operator.lower()
            if operator.lower() in [
                op,
                op + ""ifexists"",
                ""forallvalues:"" + op,
                ""foranyvalue:"" + op,
                ""forallvalues:"" + op + ""ifexists"",
                ""foranyvalue:"" + op + ""ifexists"",
            ]:
                operator_type_requirement = OPERATORS[documented_operator]
                break

        if operator_type_requirement is None:
            self.add_finding(
                ""UNKNOWN_OPERATOR"",
                detail=operator,
                location=condition_block,
            )

        if operator_type_requirement == ""Bool"":
            # Get the value that is being compared against
            for c in condition_block:
                value = str(c[1].value).lower()
                if value != ""true"" and value != ""false"":
                    self.add_finding(
                        ""MISMATCHED_TYPE_OPERATION_TO_NULL"", location=condition_block
                    )
                    return False

        for block in condition_block:
            key = block[0]
            values = []
            for v in make_list(block[1]):
                values.append(v.value)

            # Check for known bad pattern
            if operator.lower() == ""bool"":
                if (
                    key.lower() == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.',
                        location=condition_block,
                    )
            elif operator.lower() == ""null"":
                if (
                    key.lower == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.',
                        location=condition_block,
                    )

            if operator.lower() in [""null""]:
                # The following condition is valid:
                # ""Condition"": { ""Null"": { ""aws:MultiFactorAuthAge"": true }
                # If we check further we'll get a MISMATCHED_TYPE finding due to
                # aws:MultiFactorAuthAge being checked against a bool value instead of a date
                continue

            # The key here from the example is ""s3:prefix""
            condition_type = get_global_key_type(key)
            if condition_type:
                # This is a global key, like aws:CurrentTime
                # Check if the values match the type (ex. must all be Date values)
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding(
                        ""MISMATCHED_TYPE"",
                        detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                            key, condition_type, values
                        ),
                        location=condition_block,
                    )
            else:
                # See if this is a service specific key
                for action_struct in expanded_actions:
                    privilege_info = get_privilege_info(
                        action_struct[""service""], action_struct[""action""]
                    )

                    # Ensure the condition_key exists
                    match = None
                    for resource_type in privilege_info[""resource_types""]:
                        for condition_key in resource_type[""condition_keys""]:
                            if is_condition_key_match(condition_key, key):
                                match = condition_key

                    if match is None:
                        self.add_finding(
                            ""UNKNOWN_CONDITION_FOR_ACTION"",
                            detail=""Unknown condition {} for action {}:{}"".format(
                                key, action_struct[""service""], action_struct[""action""]
                            ),
                            location=condition_block,
                        )
                        continue

                    condition_type = None
                    for condition in privilege_info[""service_conditions""]:
                        if condition[""condition""] == match:
                            condition_type = condition[""type""]

                    if condition_type is None:
                        raise Exception(
                            ""Action condition not found in service definition for {}"".format(
                                match
                            )
                        )

                    if not is_value_in_correct_format_for_type(condition_type, values):
                        self.add_finding(
                            ""MISMATCHED_TYPE"",
                            detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                key, condition_type, values
                            ),
                            location=condition_block,
                        )

                if condition_type is not None:
                    # if operator_type_requirement.lower() == 'string' and condition_type.lower() = 'arn':
                    #     # Ignore these.
                    #     pass
                    documenation_condition_type = translate_documentation_types(
                        condition_type
                    )
                    if operator_type_requirement != documenation_condition_type:
                        if (
                            operator_type_requirement == ""String""
                            and documenation_condition_type == ""Arn""
                        ):
                            self.add_finding(
                                ""MISMATCHED_TYPE_BUT_USABLE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )
                        else:
                            self.add_finding(
                                ""MISMATCHED_TYPE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )

        return","for block in condition_block:
    key = block[0]
    values = []
    for v in make_list(block[1]):
        values.append(v.value)
    if operator.lower() == 'bool':
        if key.lower() == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
            self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.', location=condition_block)
    elif operator.lower() == 'null':
        if key.lower == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
            self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.', location=condition_block)
    if operator.lower() in ['null']:
        continue
    condition_type = get_global_key_type(key)
    if condition_type:
        if not is_value_in_correct_format_for_type(condition_type, values):
            self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
    else:
        for action_struct in expanded_actions:
            privilege_info = get_privilege_info(action_struct['service'], action_struct['action'])
            match = None
            for resource_type in privilege_info['resource_types']:
                for condition_key in resource_type['condition_keys']:
                    if is_condition_key_match(condition_key, key):
                        match = condition_key
            if match is None:
                self.add_finding('UNKNOWN_CONDITION_FOR_ACTION', detail='Unknown condition {} for action {}:{}'.format(key, action_struct['service'], action_struct['action']), location=condition_block)
                continue
            condition_type = None
            for condition in privilege_info['service_conditions']:
                if condition['condition'] == match:
                    condition_type = condition['type']
            if condition_type is None:
                raise Exception('Action condition not found in service definition for {}'.format(match))
            if not is_value_in_correct_format_for_type(condition_type, values):
                self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
        if condition_type is not None:
            documenation_condition_type = translate_documentation_types(condition_type)
            if operator_type_requirement != documenation_condition_type:
                if operator_type_requirement == 'String' and documenation_condition_type == 'Arn':
                    self.add_finding('MISMATCHED_TYPE_BUT_USABLE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)
                else:
                    self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)","for block in condition_block:
    (block_0, block_1, *_) = block
    key = block[0]
    values = []
    for v in make_list(block[1]):
        values.append(v.value)
    if operator.lower() == 'bool':
        if key.lower() == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
            self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.', location=condition_block)
    elif operator.lower() == 'null':
        if key.lower == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
            self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.', location=condition_block)
    if operator.lower() in ['null']:
        continue
    condition_type = get_global_key_type(key)
    if condition_type:
        if not is_value_in_correct_format_for_type(condition_type, values):
            self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
    else:
        for action_struct in expanded_actions:
            privilege_info = get_privilege_info(action_struct['service'], action_struct['action'])
            match = None
            for resource_type in privilege_info['resource_types']:
                for condition_key in resource_type['condition_keys']:
                    if is_condition_key_match(condition_key, key):
                        match = condition_key
            if match is None:
                self.add_finding('UNKNOWN_CONDITION_FOR_ACTION', detail='Unknown condition {} for action {}:{}'.format(key, action_struct['service'], action_struct['action']), location=condition_block)
                continue
            condition_type = None
            for condition in privilege_info['service_conditions']:
                if condition['condition'] == match:
                    condition_type = condition['type']
            if condition_type is None:
                raise Exception('Action condition not found in service definition for {}'.format(match))
            if not is_value_in_correct_format_for_type(condition_type, values):
                self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
        if condition_type is not None:
            documenation_condition_type = translate_documentation_types(condition_type)
            if operator_type_requirement != documenation_condition_type:
                if operator_type_requirement == 'String' and documenation_condition_type == 'Arn':
                    self.add_finding('MISMATCHED_TYPE_BUT_USABLE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)
                else:
                    self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
parliament,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/parliament/parliament/statement.py,https://github.com/duo-labs/parliament/tree/master/parliament/statement.py,Statement,_check_condition$496,"def _check_condition(self, operator, condition_block, expanded_actions):
        """"""
        operator is something like ""StringLike""
        condition_block is something like {""s3:prefix"":[""home/${aws:username}/*""]}
        """"""

        operator_type_requirement = None
        for documented_operator in OPERATORS:
            op = documented_operator.lower()
            if operator.lower() in [
                op,
                op + ""ifexists"",
                ""forallvalues:"" + op,
                ""foranyvalue:"" + op,
                ""forallvalues:"" + op + ""ifexists"",
                ""foranyvalue:"" + op + ""ifexists"",
            ]:
                operator_type_requirement = OPERATORS[documented_operator]
                break

        if operator_type_requirement is None:
            self.add_finding(
                ""UNKNOWN_OPERATOR"",
                detail=operator,
                location=condition_block,
            )

        if operator_type_requirement == ""Bool"":
            # Get the value that is being compared against
            for c in condition_block:
                value = str(c[1].value).lower()
                if value != ""true"" and value != ""false"":
                    self.add_finding(
                        ""MISMATCHED_TYPE_OPERATION_TO_NULL"", location=condition_block
                    )
                    return False

        for block in condition_block:
            key = block[0]
            values = []
            for v in make_list(block[1]):
                values.append(v.value)

            # Check for known bad pattern
            if operator.lower() == ""bool"":
                if (
                    key.lower() == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.',
                        location=condition_block,
                    )
            elif operator.lower() == ""null"":
                if (
                    key.lower == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.',
                        location=condition_block,
                    )

            if operator.lower() in [""null""]:
                # The following condition is valid:
                # ""Condition"": { ""Null"": { ""aws:MultiFactorAuthAge"": true }
                # If we check further we'll get a MISMATCHED_TYPE finding due to
                # aws:MultiFactorAuthAge being checked against a bool value instead of a date
                continue

            # The key here from the example is ""s3:prefix""
            condition_type = get_global_key_type(key)
            if condition_type:
                # This is a global key, like aws:CurrentTime
                # Check if the values match the type (ex. must all be Date values)
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding(
                        ""MISMATCHED_TYPE"",
                        detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                            key, condition_type, values
                        ),
                        location=condition_block,
                    )
            else:
                # See if this is a service specific key
                for action_struct in expanded_actions:
                    privilege_info = get_privilege_info(
                        action_struct[""service""], action_struct[""action""]
                    )

                    # Ensure the condition_key exists
                    match = None
                    for resource_type in privilege_info[""resource_types""]:
                        for condition_key in resource_type[""condition_keys""]:
                            if is_condition_key_match(condition_key, key):
                                match = condition_key

                    if match is None:
                        self.add_finding(
                            ""UNKNOWN_CONDITION_FOR_ACTION"",
                            detail=""Unknown condition {} for action {}:{}"".format(
                                key, action_struct[""service""], action_struct[""action""]
                            ),
                            location=condition_block,
                        )
                        continue

                    condition_type = None
                    for condition in privilege_info[""service_conditions""]:
                        if condition[""condition""] == match:
                            condition_type = condition[""type""]

                    if condition_type is None:
                        raise Exception(
                            ""Action condition not found in service definition for {}"".format(
                                match
                            )
                        )

                    if not is_value_in_correct_format_for_type(condition_type, values):
                        self.add_finding(
                            ""MISMATCHED_TYPE"",
                            detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                key, condition_type, values
                            ),
                            location=condition_block,
                        )

                if condition_type is not None:
                    # if operator_type_requirement.lower() == 'string' and condition_type.lower() = 'arn':
                    #     # Ignore these.
                    #     pass
                    documenation_condition_type = translate_documentation_types(
                        condition_type
                    )
                    if operator_type_requirement != documenation_condition_type:
                        if (
                            operator_type_requirement == ""String""
                            and documenation_condition_type == ""Arn""
                        ):
                            self.add_finding(
                                ""MISMATCHED_TYPE_BUT_USABLE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )
                        else:
                            self.add_finding(
                                ""MISMATCHED_TYPE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )

        return","for c in condition_block:
    value = str(c[1].value).lower()
    if value != 'true' and value != 'false':
        self.add_finding('MISMATCHED_TYPE_OPERATION_TO_NULL', location=condition_block)
        return False","for c in condition_block:
    (_, c_1, *c_rcmaining) = c
    value = str(c[1].value).lower()
    if value != 'true' and value != 'false':
        self.add_finding('MISMATCHED_TYPE_OPERATION_TO_NULL', location=condition_block)
        return False","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
parliament,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/parliament/parliament/statement.py,https://github.com/duo-labs/parliament/tree/master/parliament/statement.py,Statement,_check_condition$496,"def _check_condition(self, operator, condition_block, expanded_actions):
        """"""
        operator is something like ""StringLike""
        condition_block is something like {""s3:prefix"":[""home/${aws:username}/*""]}
        """"""

        operator_type_requirement = None
        for documented_operator in OPERATORS:
            op = documented_operator.lower()
            if operator.lower() in [
                op,
                op + ""ifexists"",
                ""forallvalues:"" + op,
                ""foranyvalue:"" + op,
                ""forallvalues:"" + op + ""ifexists"",
                ""foranyvalue:"" + op + ""ifexists"",
            ]:
                operator_type_requirement = OPERATORS[documented_operator]
                break

        if operator_type_requirement is None:
            self.add_finding(
                ""UNKNOWN_OPERATOR"",
                detail=operator,
                location=condition_block,
            )

        if operator_type_requirement == ""Bool"":
            # Get the value that is being compared against
            for c in condition_block:
                value = str(c[1].value).lower()
                if value != ""true"" and value != ""false"":
                    self.add_finding(
                        ""MISMATCHED_TYPE_OPERATION_TO_NULL"", location=condition_block
                    )
                    return False

        for block in condition_block:
            key = block[0]
            values = []
            for v in make_list(block[1]):
                values.append(v.value)

            # Check for known bad pattern
            if operator.lower() == ""bool"":
                if (
                    key.lower() == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.',
                        location=condition_block,
                    )
            elif operator.lower() == ""null"":
                if (
                    key.lower == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.',
                        location=condition_block,
                    )

            if operator.lower() in [""null""]:
                # The following condition is valid:
                # ""Condition"": { ""Null"": { ""aws:MultiFactorAuthAge"": true }
                # If we check further we'll get a MISMATCHED_TYPE finding due to
                # aws:MultiFactorAuthAge being checked against a bool value instead of a date
                continue

            # The key here from the example is ""s3:prefix""
            condition_type = get_global_key_type(key)
            if condition_type:
                # This is a global key, like aws:CurrentTime
                # Check if the values match the type (ex. must all be Date values)
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding(
                        ""MISMATCHED_TYPE"",
                        detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                            key, condition_type, values
                        ),
                        location=condition_block,
                    )
            else:
                # See if this is a service specific key
                for action_struct in expanded_actions:
                    privilege_info = get_privilege_info(
                        action_struct[""service""], action_struct[""action""]
                    )

                    # Ensure the condition_key exists
                    match = None
                    for resource_type in privilege_info[""resource_types""]:
                        for condition_key in resource_type[""condition_keys""]:
                            if is_condition_key_match(condition_key, key):
                                match = condition_key

                    if match is None:
                        self.add_finding(
                            ""UNKNOWN_CONDITION_FOR_ACTION"",
                            detail=""Unknown condition {} for action {}:{}"".format(
                                key, action_struct[""service""], action_struct[""action""]
                            ),
                            location=condition_block,
                        )
                        continue

                    condition_type = None
                    for condition in privilege_info[""service_conditions""]:
                        if condition[""condition""] == match:
                            condition_type = condition[""type""]

                    if condition_type is None:
                        raise Exception(
                            ""Action condition not found in service definition for {}"".format(
                                match
                            )
                        )

                    if not is_value_in_correct_format_for_type(condition_type, values):
                        self.add_finding(
                            ""MISMATCHED_TYPE"",
                            detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                key, condition_type, values
                            ),
                            location=condition_block,
                        )

                if condition_type is not None:
                    # if operator_type_requirement.lower() == 'string' and condition_type.lower() = 'arn':
                    #     # Ignore these.
                    #     pass
                    documenation_condition_type = translate_documentation_types(
                        condition_type
                    )
                    if operator_type_requirement != documenation_condition_type:
                        if (
                            operator_type_requirement == ""String""
                            and documenation_condition_type == ""Arn""
                        ):
                            self.add_finding(
                                ""MISMATCHED_TYPE_BUT_USABLE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )
                        else:
                            self.add_finding(
                                ""MISMATCHED_TYPE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )

        return","for action_struct in expanded_actions:
    privilege_info = get_privilege_info(action_struct['service'], action_struct['action'])
    match = None
    for resource_type in privilege_info['resource_types']:
        for condition_key in resource_type['condition_keys']:
            if is_condition_key_match(condition_key, key):
                match = condition_key
    if match is None:
        self.add_finding('UNKNOWN_CONDITION_FOR_ACTION', detail='Unknown condition {} for action {}:{}'.format(key, action_struct['service'], action_struct['action']), location=condition_block)
        continue
    condition_type = None
    for condition in privilege_info['service_conditions']:
        if condition['condition'] == match:
            condition_type = condition['type']
    if condition_type is None:
        raise Exception('Action condition not found in service definition for {}'.format(match))
    if not is_value_in_correct_format_for_type(condition_type, values):
        self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_action, e_service = e['action'], e['service']
variable mapping:
e_action: e['action']
e_service: e['service']",,,,,,,
parliament,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/parliament/parliament/statement.py,https://github.com/duo-labs/parliament/tree/master/parliament/statement.py,Statement,_check_condition$496,"def _check_condition(self, operator, condition_block, expanded_actions):
        """"""
        operator is something like ""StringLike""
        condition_block is something like {""s3:prefix"":[""home/${aws:username}/*""]}
        """"""

        operator_type_requirement = None
        for documented_operator in OPERATORS:
            op = documented_operator.lower()
            if operator.lower() in [
                op,
                op + ""ifexists"",
                ""forallvalues:"" + op,
                ""foranyvalue:"" + op,
                ""forallvalues:"" + op + ""ifexists"",
                ""foranyvalue:"" + op + ""ifexists"",
            ]:
                operator_type_requirement = OPERATORS[documented_operator]
                break

        if operator_type_requirement is None:
            self.add_finding(
                ""UNKNOWN_OPERATOR"",
                detail=operator,
                location=condition_block,
            )

        if operator_type_requirement == ""Bool"":
            # Get the value that is being compared against
            for c in condition_block:
                value = str(c[1].value).lower()
                if value != ""true"" and value != ""false"":
                    self.add_finding(
                        ""MISMATCHED_TYPE_OPERATION_TO_NULL"", location=condition_block
                    )
                    return False

        for block in condition_block:
            key = block[0]
            values = []
            for v in make_list(block[1]):
                values.append(v.value)

            # Check for known bad pattern
            if operator.lower() == ""bool"":
                if (
                    key.lower() == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.',
                        location=condition_block,
                    )
            elif operator.lower() == ""null"":
                if (
                    key.lower == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.',
                        location=condition_block,
                    )

            if operator.lower() in [""null""]:
                # The following condition is valid:
                # ""Condition"": { ""Null"": { ""aws:MultiFactorAuthAge"": true }
                # If we check further we'll get a MISMATCHED_TYPE finding due to
                # aws:MultiFactorAuthAge being checked against a bool value instead of a date
                continue

            # The key here from the example is ""s3:prefix""
            condition_type = get_global_key_type(key)
            if condition_type:
                # This is a global key, like aws:CurrentTime
                # Check if the values match the type (ex. must all be Date values)
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding(
                        ""MISMATCHED_TYPE"",
                        detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                            key, condition_type, values
                        ),
                        location=condition_block,
                    )
            else:
                # See if this is a service specific key
                for action_struct in expanded_actions:
                    privilege_info = get_privilege_info(
                        action_struct[""service""], action_struct[""action""]
                    )

                    # Ensure the condition_key exists
                    match = None
                    for resource_type in privilege_info[""resource_types""]:
                        for condition_key in resource_type[""condition_keys""]:
                            if is_condition_key_match(condition_key, key):
                                match = condition_key

                    if match is None:
                        self.add_finding(
                            ""UNKNOWN_CONDITION_FOR_ACTION"",
                            detail=""Unknown condition {} for action {}:{}"".format(
                                key, action_struct[""service""], action_struct[""action""]
                            ),
                            location=condition_block,
                        )
                        continue

                    condition_type = None
                    for condition in privilege_info[""service_conditions""]:
                        if condition[""condition""] == match:
                            condition_type = condition[""type""]

                    if condition_type is None:
                        raise Exception(
                            ""Action condition not found in service definition for {}"".format(
                                match
                            )
                        )

                    if not is_value_in_correct_format_for_type(condition_type, values):
                        self.add_finding(
                            ""MISMATCHED_TYPE"",
                            detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                key, condition_type, values
                            ),
                            location=condition_block,
                        )

                if condition_type is not None:
                    # if operator_type_requirement.lower() == 'string' and condition_type.lower() = 'arn':
                    #     # Ignore these.
                    #     pass
                    documenation_condition_type = translate_documentation_types(
                        condition_type
                    )
                    if operator_type_requirement != documenation_condition_type:
                        if (
                            operator_type_requirement == ""String""
                            and documenation_condition_type == ""Arn""
                        ):
                            self.add_finding(
                                ""MISMATCHED_TYPE_BUT_USABLE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )
                        else:
                            self.add_finding(
                                ""MISMATCHED_TYPE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )

        return","for resource_type in privilege_info['resource_types']:
    for condition_key in resource_type['condition_keys']:
        if is_condition_key_match(condition_key, key):
            match = condition_key",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_condition_keys, *_ = e.values()
variable mapping:
e_condition_keys: e['condition_keys']",,,,,,,
parliament,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/parliament/parliament/statement.py,https://github.com/duo-labs/parliament/tree/master/parliament/statement.py,Statement,_check_condition$496,"def _check_condition(self, operator, condition_block, expanded_actions):
        """"""
        operator is something like ""StringLike""
        condition_block is something like {""s3:prefix"":[""home/${aws:username}/*""]}
        """"""

        operator_type_requirement = None
        for documented_operator in OPERATORS:
            op = documented_operator.lower()
            if operator.lower() in [
                op,
                op + ""ifexists"",
                ""forallvalues:"" + op,
                ""foranyvalue:"" + op,
                ""forallvalues:"" + op + ""ifexists"",
                ""foranyvalue:"" + op + ""ifexists"",
            ]:
                operator_type_requirement = OPERATORS[documented_operator]
                break

        if operator_type_requirement is None:
            self.add_finding(
                ""UNKNOWN_OPERATOR"",
                detail=operator,
                location=condition_block,
            )

        if operator_type_requirement == ""Bool"":
            # Get the value that is being compared against
            for c in condition_block:
                value = str(c[1].value).lower()
                if value != ""true"" and value != ""false"":
                    self.add_finding(
                        ""MISMATCHED_TYPE_OPERATION_TO_NULL"", location=condition_block
                    )
                    return False

        for block in condition_block:
            key = block[0]
            values = []
            for v in make_list(block[1]):
                values.append(v.value)

            # Check for known bad pattern
            if operator.lower() == ""bool"":
                if (
                    key.lower() == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.',
                        location=condition_block,
                    )
            elif operator.lower() == ""null"":
                if (
                    key.lower == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.',
                        location=condition_block,
                    )

            if operator.lower() in [""null""]:
                # The following condition is valid:
                # ""Condition"": { ""Null"": { ""aws:MultiFactorAuthAge"": true }
                # If we check further we'll get a MISMATCHED_TYPE finding due to
                # aws:MultiFactorAuthAge being checked against a bool value instead of a date
                continue

            # The key here from the example is ""s3:prefix""
            condition_type = get_global_key_type(key)
            if condition_type:
                # This is a global key, like aws:CurrentTime
                # Check if the values match the type (ex. must all be Date values)
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding(
                        ""MISMATCHED_TYPE"",
                        detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                            key, condition_type, values
                        ),
                        location=condition_block,
                    )
            else:
                # See if this is a service specific key
                for action_struct in expanded_actions:
                    privilege_info = get_privilege_info(
                        action_struct[""service""], action_struct[""action""]
                    )

                    # Ensure the condition_key exists
                    match = None
                    for resource_type in privilege_info[""resource_types""]:
                        for condition_key in resource_type[""condition_keys""]:
                            if is_condition_key_match(condition_key, key):
                                match = condition_key

                    if match is None:
                        self.add_finding(
                            ""UNKNOWN_CONDITION_FOR_ACTION"",
                            detail=""Unknown condition {} for action {}:{}"".format(
                                key, action_struct[""service""], action_struct[""action""]
                            ),
                            location=condition_block,
                        )
                        continue

                    condition_type = None
                    for condition in privilege_info[""service_conditions""]:
                        if condition[""condition""] == match:
                            condition_type = condition[""type""]

                    if condition_type is None:
                        raise Exception(
                            ""Action condition not found in service definition for {}"".format(
                                match
                            )
                        )

                    if not is_value_in_correct_format_for_type(condition_type, values):
                        self.add_finding(
                            ""MISMATCHED_TYPE"",
                            detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                key, condition_type, values
                            ),
                            location=condition_block,
                        )

                if condition_type is not None:
                    # if operator_type_requirement.lower() == 'string' and condition_type.lower() = 'arn':
                    #     # Ignore these.
                    #     pass
                    documenation_condition_type = translate_documentation_types(
                        condition_type
                    )
                    if operator_type_requirement != documenation_condition_type:
                        if (
                            operator_type_requirement == ""String""
                            and documenation_condition_type == ""Arn""
                        ):
                            self.add_finding(
                                ""MISMATCHED_TYPE_BUT_USABLE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )
                        else:
                            self.add_finding(
                                ""MISMATCHED_TYPE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )

        return","for condition in privilege_info['service_conditions']:
    if condition['condition'] == match:
        condition_type = condition['type']",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_e, e_type = e['e'], e['type']
variable mapping:
e_e: e['e']
e_type: e['type']",,,,,,,
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/scripts/detection/ssd/train_ssd.py,https://github.com/dmlc/gluon-cv/tree/master/scripts/detection/ssd/train_ssd.py,,validate$223,"def validate(net, val_data, ctx, eval_metric):
    """"""Test on validation dataset.""""""
    eval_metric.reset()
    # set nms threshold and topk constraint
    net.set_nms(nms_thresh=0.45, nms_topk=400)
    net.hybridize(static_alloc=True, static_shape=True)
    for batch in val_data:
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)
        det_bboxes = []
        det_ids = []
        det_scores = []
        gt_bboxes = []
        gt_ids = []
        gt_difficults = []
        for x, y in zip(data, label):
            # get prediction results
            ids, scores, bboxes = net(x)
            det_ids.append(ids)
            det_scores.append(scores)
            # clip to image size
            det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
            # split ground truths
            gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
            gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
            gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)

        # update metric
        eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)
    return eval_metric.get()","for batch in val_data:
    data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)
    label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)
    det_bboxes = []
    det_ids = []
    det_scores = []
    gt_bboxes = []
    gt_ids = []
    gt_difficults = []
    for (x, y) in zip(data, label):
        (ids, scores, bboxes) = net(x)
        det_ids.append(ids)
        det_scores.append(scores)
        det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)","for batch in val_data:
    (batch_0, batch_1, *_) = batch
    data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)
    label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)
    det_bboxes = []
    det_ids = []
    det_scores = []
    gt_bboxes = []
    gt_ids = []
    gt_difficults = []
    for (x, y) in zip(data, label):
        (ids, scores, bboxes) = net(x)
        det_ids.append(ids)
        det_scores.append(scores)
        det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
PaddleX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex_restful/restful/utils.py,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex_restful/restful/utils.py,PrintableStructure,__str__$715,"def __str__(self):
            result = []
            for x in self._fields_:
                key = x[0]
                value = getattr(self, key)
                fmt = ""%s""
                if key in self._fmt_:
                    fmt = self._fmt_[key]
                elif ""<default>"" in self._fmt_:
                    fmt = self._fmt_[""<default>""]
                result.append((""%s: "" + fmt) % (key, value))
            return self.__class__.__name__ + ""("" + string.join(result,
                                                               "", "") + "")""","for x in self._fields_:
    key = x[0]
    value = getattr(self, key)
    fmt = '%s'
    if key in self._fmt_:
        fmt = self._fmt_[key]
    elif '<default>' in self._fmt_:
        fmt = self._fmt_['<default>']
    result.append(('%s: ' + fmt) % (key, value))","for x in self._fields_:
    (x_0, *x_rxmaining) = x
    key = x[0]
    value = getattr(self, key)
    fmt = '%s'
    if key in self._fmt_:
        fmt = self._fmt_[key]
    elif '<default>' in self._fmt_:
        fmt = self._fmt_['<default>']
    result.append(('%s: ' + fmt) % (key, value))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_fr.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_fr.py,Num2WordsENTest,test_ordinal$172,"def test_ordinal(self):
        for test in TEST_CASES_ORDINAL:
            self.assertEqual(
                num2words(test[0], lang='fr', ordinal=True),
                test[1]
            )","for test in TEST_CASES_ORDINAL:
    self.assertEqual(num2words(test[0], lang='fr', ordinal=True), test[1])","for test in TEST_CASES_ORDINAL:
    (test_0, test_1, *_) = test
    self.assertEqual(num2words(test[0], lang='fr', ordinal=True), test[1])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
dex,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dex/dex/dex.py,https://github.com/mongolab/dex/tree/master/dex/dex.py,Dex,_get_requested_databases$431,"def _get_requested_databases(self):
        """"""Returns a list of databases requested, not including ignored dbs""""""
        requested_databases = []
        if ((self._requested_namespaces is not None) and
                (self._requested_namespaces != [])):
            for requested_namespace in self._requested_namespaces:
                if requested_namespace[0] is '*':
                    return []
                elif requested_namespace[0] not in IGNORE_DBS:
                    requested_databases.append(requested_namespace[0])
        return requested_databases","for requested_namespace in self._requested_namespaces:
    if requested_namespace[0] is '*':
        return []
    elif requested_namespace[0] not in IGNORE_DBS:
        requested_databases.append(requested_namespace[0])","for requested_namespace in self._requested_namespaces:
    (requested_namespace_0, *requested_namespace_rrequested_namespacemaining) = requested_namespace
    if requested_namespace[0] is '*':
        return []
    elif requested_namespace[0] not in IGNORE_DBS:
        requested_databases.append(requested_namespace[0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
grokking-the-object-oriented-design-interview,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/grokking-the-object-oriented-design-interview/example-codes/chess/king.py,https://github.com/tssovi/grokking-the-object-oriented-design-interview/tree/master/example-codes/chess/king.py,King,get_threatened_positions$20,"def get_threatened_positions(self, board):
        positions = []
        for increment in King.SPOT_INCREMENTS:
            positions.append(board.spot_search_threat(self._position, self._color, increment[0], increment[1]))
        positions = [x for x in positions if x is not None]
        return positions","for increment in King.SPOT_INCREMENTS:
    positions.append(board.spot_search_threat(self._position, self._color, increment[0], increment[1]))","for increment in King.SPOT_INCREMENTS:
    (increment_0, increment_1, *_) = increment
    positions.append(board.spot_search_threat(self._position, self._color, increment[0], increment[1]))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/qiskit/dagcircuit/dagdependency.py,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/dagcircuit/dagdependency.py,DAGDependency,copy$567,"def copy(self):
        """"""
        Function to copy a DAGDependency object.
        Returns:
            DAGDependency: a copy of a DAGDependency object.
        """"""

        dag = DAGDependency()
        dag.name = self.name
        dag.cregs = self.cregs.copy()
        dag.qregs = self.qregs.copy()

        for node in self.get_nodes():
            dag._multi_graph.add_node(node.copy())
        for edges in self.get_all_edges():
            dag._multi_graph.add_edge(edges[0], edges[1], edges[2])
        return dag","for edges in self.get_all_edges():
    dag._multi_graph.add_edge(edges[0], edges[1], edges[2])","for edges in self.get_all_edges():
    (edges_0, edges_1, edges_2, *_) = edges
    dag._multi_graph.add_edge(edges[0], edges[1], edges[2])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/occ/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/occ/geometry.py,Geometry,boolean_difference$197,"def boolean_difference(
        self, d0, d1, delete_first: bool = True, delete_other: bool = True
    ):
        """"""Boolean difference, see
        https://gmsh.info/doc/texinfo/gmsh.html#Boolean-operations input_entity
        and tool_entity are called object and tool in gmsh documentation.
        """"""
        d0 = d0 if isinstance(d0, list) else [d0]
        d1 = d1 if isinstance(d1, list) else [d1]
        dim_tags, _ = gmsh.model.occ.cut(
            [d.dim_tag for d in d0],
            [d.dim_tag for d in d1],
            removeObject=delete_first,
            removeTool=delete_other,
        )

        # remove entities from SIZE_QUEUE if necessary
        all_entities = []
        if delete_first:
            all_entities += d0
        if delete_other:
            all_entities += d1
        for s in self._SIZE_QUEUE:
            if s[0] in all_entities:
                warnings.warn(
                    f""Specified mesh size for {s[0]} ""
                    ""discarded in Boolean difference operation.""
                )
        self._SIZE_QUEUE = [s for s in self._SIZE_QUEUE if s[0] not in all_entities]

        return [Dummy(*dim_tag) for dim_tag in dim_tags]","for s in self._SIZE_QUEUE:
    if s[0] in all_entities:
        warnings.warn(f'Specified mesh size for {s[0]} discarded in Boolean difference operation.')","for s in self._SIZE_QUEUE:
    (s_0, *s_rsmaining) = s
    if s[0] in all_entities:
        warnings.warn(f'Specified mesh size for {s[0]} discarded in Boolean difference operation.')","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/check_op_desc.py,https://github.com/PaddlePaddle/Paddle/tree/master/tools/check_op_desc.py,,print_version_error_message$427,"def print_version_error_message(error_message):
    print(
        ""\n======================= \n""
        ""Operator registration error for the changes of Inputs/Outputs/Attrs of OPs:\n""
    )
    for op_name in error_message:
        print(""For OP '{}':"".format(op_name))

        # 1. print inputs error message
        inputs_error = error_message.get(op_name, {}).get(INPUTS, {})
        error_list = inputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print(
                    "" * The added input '{}' is not yet registered."".format(
                        tup[1]
                    )
                )

        # 2. print outputs error message
        outputs_error = error_message.get(op_name, {}).get(OUTPUTS, {})
        error_list = outputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print(
                    "" * The added output '{}' is not yet registered."".format(
                        tup[1]
                    )
                )

        # 3. print attrs error message
        attrs_error = error_message.get(op_name, {}).get(ATTRS, {})
        error_list = attrs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print(
                    "" * The added attribute '{}' is not yet registered."".format(
                        tup[1]
                    )
                )
        error_dic = (
            error_message.get(op_name, {}).get(ATTRS, {}).get(CHANGE, {})
        )
        for key, val in error_dic.items():
            print(
                "" * The change of attribute '{}' is not yet registered."".format(
                    key
                )
            )","for tup in error_list:
    print("" * The added input '{}' is not yet registered."".format(tup[1]))","for tup in error_list:
    (tup_0, tup_1, *tup_rtupmaining) = tup
    print("" * The added input '{}' is not yet registered."".format(tup[1]))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/check_op_desc.py,https://github.com/PaddlePaddle/Paddle/tree/master/tools/check_op_desc.py,,print_version_error_message$427,"def print_version_error_message(error_message):
    print(
        ""\n======================= \n""
        ""Operator registration error for the changes of Inputs/Outputs/Attrs of OPs:\n""
    )
    for op_name in error_message:
        print(""For OP '{}':"".format(op_name))

        # 1. print inputs error message
        inputs_error = error_message.get(op_name, {}).get(INPUTS, {})
        error_list = inputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print(
                    "" * The added input '{}' is not yet registered."".format(
                        tup[1]
                    )
                )

        # 2. print outputs error message
        outputs_error = error_message.get(op_name, {}).get(OUTPUTS, {})
        error_list = outputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print(
                    "" * The added output '{}' is not yet registered."".format(
                        tup[1]
                    )
                )

        # 3. print attrs error message
        attrs_error = error_message.get(op_name, {}).get(ATTRS, {})
        error_list = attrs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print(
                    "" * The added attribute '{}' is not yet registered."".format(
                        tup[1]
                    )
                )
        error_dic = (
            error_message.get(op_name, {}).get(ATTRS, {}).get(CHANGE, {})
        )
        for key, val in error_dic.items():
            print(
                "" * The change of attribute '{}' is not yet registered."".format(
                    key
                )
            )","for tup in error_list:
    print("" * The added output '{}' is not yet registered."".format(tup[1]))","for tup in error_list:
    (_, tup_1, *tup_rtupmaining) = tup
    print("" * The added output '{}' is not yet registered."".format(tup[1]))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/check_op_desc.py,https://github.com/PaddlePaddle/Paddle/tree/master/tools/check_op_desc.py,,print_version_error_message$427,"def print_version_error_message(error_message):
    print(
        ""\n======================= \n""
        ""Operator registration error for the changes of Inputs/Outputs/Attrs of OPs:\n""
    )
    for op_name in error_message:
        print(""For OP '{}':"".format(op_name))

        # 1. print inputs error message
        inputs_error = error_message.get(op_name, {}).get(INPUTS, {})
        error_list = inputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print(
                    "" * The added input '{}' is not yet registered."".format(
                        tup[1]
                    )
                )

        # 2. print outputs error message
        outputs_error = error_message.get(op_name, {}).get(OUTPUTS, {})
        error_list = outputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print(
                    "" * The added output '{}' is not yet registered."".format(
                        tup[1]
                    )
                )

        # 3. print attrs error message
        attrs_error = error_message.get(op_name, {}).get(ATTRS, {})
        error_list = attrs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print(
                    "" * The added attribute '{}' is not yet registered."".format(
                        tup[1]
                    )
                )
        error_dic = (
            error_message.get(op_name, {}).get(ATTRS, {}).get(CHANGE, {})
        )
        for key, val in error_dic.items():
            print(
                "" * The change of attribute '{}' is not yet registered."".format(
                    key
                )
            )","for tup in error_list:
    print("" * The added attribute '{}' is not yet registered."".format(tup[1]))","for tup in error_list:
    (_, tup_1, *tup_rtupmaining) = tup
    print("" * The added attribute '{}' is not yet registered."".format(tup[1]))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/thread_modules.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/thread_modules.py,FindPosterThread,run$393,"def run(self):
        name = self.name
        url = self.url
        direct_url = self.direct_url
        #print(name, url, direct_url, '--poster--thread--')
        fanart = os.path.join(TMPDIR, name+'-fanart.jpg')
        thumb = os.path.join(TMPDIR, name+'.jpg')
        fan_text = os.path.join(TMPDIR, name+'-fanart.txt')
        post_text = os.path.join(TMPDIR, name+'-poster.txt')
        logger.info(fanart)
        logger.info(thumb)
        final_link = """"
        m = []

        if site == 'Music':
            final = ''
            if (self.copy_fanart and self.copy_poster and self.copy_summary):
                if not direct_url and not url:
                    nam = ui.metaengine.name_adjust(name)
                    url = ""http://www.last.fm/search?q=""+nam
                    logger.info(url)
                wiki = """"
                content = ccurl(url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('div', {'class':'row clearfix'})
                name3 = """"
                for i in link:
                    j = i.findAll('a')
                    for k in j:
                        try:
                            url = k['href']
                            if '?q=' not in url:
                                logger.info(url)
                                break
                        except Exception as err:
                            print(err, '--108--')
                logger.info(url)
                if url.startswith('http'):
                    url = url
                else:
                    url = ""http://www.last.fm"" + url
                logger.info(url)
                img_url = url+'/+images'
                wiki_url = url + '/+wiki'
                logger.info(wiki_url)
                content = ccurl(wiki_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.find('div', {'class':'wiki-content'})
                if link:
                    wiki = link.text
                    self.summary_signal.emit(name, wiki, 'summary')
                content = ccurl(img_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('ul', {'class':'image-list'})
                img = []
                for i in link:
                    j = i.findAll('img')
                    for k in j:
                        l = k['src']
                        u1 = l.rsplit('/', 2)[0]
                        u2 = l.split('/')[-1]
                        u = u1 + '/770x0/'+u2
                        img.append(u)
                img = list(set(img))
                logger.info(len(img))
                thumb = os.path.join(TMPDIR, name+'.jpg')
                if img:
                    url = img[0]
                    try:
                        ccurl(url, curl_opt='-o', out_file=thumb)
                    except Exception as err:
                        print(err, '--151--')
            elif (self.copy_poster or self.copy_fanart) and url and direct_url:
                if 'last.fm' in url:
                    logger.info('--artist-link---{0}'.format(url))
                    content = ccurl(url)
                    soup = BeautifulSoup(content, 'lxml')
                    link = soup.findAll('img')
                    url1Code = url.split('/')[-1]
                    found = None
                    for i in link:
                        if 'src' in str(i):
                            j = i['src']
                            k = j.split('/')[-1]
                            if url1Code == k:
                                found = j
                                break
                    logger.info(str(found))
                    if found:
                        u1 = found.rsplit('/', 2)[0]
                        u2 = found.split('/')[-1]
                        final = u1 + '/770x0/'+u2
                        logger.info(final)
                elif ("".jpg"" in url or "".png"" in url) and url.startswith('http'):
                    final = url
                else:
                    final = ''
                try:
                    if final.startswith('http'):
                        ccurl(final, curl_opt='-o', out_file=thumb)
                except Exception as e:
                    print(e)
        else:
            nam = ui.metaengine.name_adjust(name)
            src_site = 'tvdb'
            epn_arr = []
            post_val = ''
            fan_val = ''
            logger.debug('\nvideo_dir={0}\n'.format(self.video_dir))
            if site.lower() == 'video' and self.video_dir:
                 video_db = os.path.join(ui.home_folder, 'VideoDB', 'Video.db')
                 if os.path.exists(video_db):
                    epn_arr_tmp = ui.media_data.get_video_db(video_db, ""Directory"", self.video_dir)
                    for i in epn_arr_tmp:
                        epn_name = i[0]+'	'+i[1]
                        logger.debug(epn_name)
                        epn_arr.append(epn_name)
            elif self.video_dir:
                new_name_with_info = self.video_dir.strip()
                extra_info = ''
                if '	' in new_name_with_info:
                    name_title = new_name_with_info.split('	')[0]
                    extra_info = new_name_with_info.split('	')[1]
                else:
                    name_title = new_name_with_info
                
                if site.lower() == 'subbedanime' or site.lower() == 'dubbedanime' and siteName:
                    hist_site = os.path.join(ui.home_folder, 'History', site, siteName, name_title)
                else:
                    hist_site = os.path.join(ui.home_folder, 'History', site, name_title)
                    
                hist_epn = os.path.join(hist_site, 'Ep.txt')
                logger.info(hist_epn)
                if os.path.exists(hist_epn):
                    lines = open_files(hist_epn, True)
                    for i in lines:
                        i = i.strip()
                        j = i.split('	')
                        if len(j) == 1:
                            epn_arr.append(i+'	'+i+'	'+name)
                        elif len(j) >= 2:
                            epn_arr.append(i+'	'+name)
                            
            if ui.series_info_dict.get(name) and not epn_arr:
                logger.debug('getting values from cache')
                dict_val = ui.series_info_dict.get(name)
                post_arr = dict_val.get('poster')
                fan_arr = dict_val.get('fanart')
                fan_index = dict_val.get('f')
                post_index = dict_val.get('p')
                if fan_index < len(fan_arr):
                    fan_val = fan_arr[fan_index]
                    fan_index = (fan_index + 1) % len(fan_arr)
                    dict_val.update({'f':fan_index})
                if post_index < len(post_arr):
                    post_val = post_arr[post_index]
                    post_index = (post_index + 1) % len(post_arr)
                    dict_val.update({'p':post_index})
                ui.series_info_dict.update({name:dict_val})
                if isinstance(self.use_search, bool):
                    src_site = 'tvdb'
                else:
                    src_site = self.use_search
            else:
                m, final_link, src_site = self.init_search(
                    nam, url, direct_url, thumb, fanart, src_site
                    )
            if (m and src_site in ['tvdb', 'tvdb+g', 'tvdb+ddg']) or post_val or fan_val:
                if post_val or fan_val:
                    if post_val:
                        url = ""http://thetvdb.com/"" + post_val
                        ccurl(url+'#'+'-o'+'#'+thumb)
                    if fan_val:
                        url = ""http://thetvdb.com/"" + fan_val
                        ccurl(url+'#'+'-o'+'#'+fanart)
                else:
                    if not final_link:
                        n = re.sub('amp;', '', m[0])
                        elist = re.sub('tab=series', 'tab=seasonall', n)
                        url = ""http://thetvdb.com"" + n
                        logger.info(url)
                        elist_url = ""http://thetvdb.com"" + elist
                    else:
                        url = final_link
                    post_arr, fan_arr = self.parse_tvdb(name, url)
                    if post_arr:
                        url = ""http://thetvdb.com/"" + post_arr[0]
                        ccurl(url+'#'+'-o'+'#'+thumb)
                        logger.info(post_arr)
                    if fan_arr:
                        #if ui.player_theme != 'default':
                        fan_arr = [i for i in fan_arr if 'vignette' not in i]
                        if fan_arr:
                            url = ""http://thetvdb.com/"" + fan_arr[0]
                            ccurl(url+'#'+'-o'+'#'+fanart)
                        logger.debug(fan_arr)
                    fan_arr.sort()
                    post_arr.sort()
                    ui.series_info_dict.update(
                            {
                            name:{
                                'fanart':fan_arr.copy(), 'poster':post_arr.copy(),
                                'f':0, 'p':0
                                }
                            }
                        )
                    elist_url = re.sub('tab=series', 'tab=seasonall', final_link)
                    if epn_arr:
                        ui.metaengine.getTvdbEpnInfo(
                            elist_url, epn_arr=epn_arr.copy(), site=site,
                            name=name, thread=self, video_dir=self.video_dir
                            )
                        image_dict = self.image_dict_list.copy()
                        dest_dir = self.dest_dir
                        self.imagesignal.emit(image_dict, dest_dir, site)
            elif m and src_site in ['tmdb', 'tmdb+g', 'tmdb+ddg']:
                self.parse_tmdb(name, final_link, thumb, fanart)","for k in j:
    try:
        url = k['href']
        if '?q=' not in url:
            logger.info(url)
            break
    except Exception as err:
        print(err, '--108--')",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['href'] is a dictionary value that can be accessed directly using the key 'href'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/thread_modules.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/thread_modules.py,FindPosterThread,run$393,"def run(self):
        name = self.name
        url = self.url
        direct_url = self.direct_url
        #print(name, url, direct_url, '--poster--thread--')
        fanart = os.path.join(TMPDIR, name+'-fanart.jpg')
        thumb = os.path.join(TMPDIR, name+'.jpg')
        fan_text = os.path.join(TMPDIR, name+'-fanart.txt')
        post_text = os.path.join(TMPDIR, name+'-poster.txt')
        logger.info(fanart)
        logger.info(thumb)
        final_link = """"
        m = []

        if site == 'Music':
            final = ''
            if (self.copy_fanart and self.copy_poster and self.copy_summary):
                if not direct_url and not url:
                    nam = ui.metaengine.name_adjust(name)
                    url = ""http://www.last.fm/search?q=""+nam
                    logger.info(url)
                wiki = """"
                content = ccurl(url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('div', {'class':'row clearfix'})
                name3 = """"
                for i in link:
                    j = i.findAll('a')
                    for k in j:
                        try:
                            url = k['href']
                            if '?q=' not in url:
                                logger.info(url)
                                break
                        except Exception as err:
                            print(err, '--108--')
                logger.info(url)
                if url.startswith('http'):
                    url = url
                else:
                    url = ""http://www.last.fm"" + url
                logger.info(url)
                img_url = url+'/+images'
                wiki_url = url + '/+wiki'
                logger.info(wiki_url)
                content = ccurl(wiki_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.find('div', {'class':'wiki-content'})
                if link:
                    wiki = link.text
                    self.summary_signal.emit(name, wiki, 'summary')
                content = ccurl(img_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('ul', {'class':'image-list'})
                img = []
                for i in link:
                    j = i.findAll('img')
                    for k in j:
                        l = k['src']
                        u1 = l.rsplit('/', 2)[0]
                        u2 = l.split('/')[-1]
                        u = u1 + '/770x0/'+u2
                        img.append(u)
                img = list(set(img))
                logger.info(len(img))
                thumb = os.path.join(TMPDIR, name+'.jpg')
                if img:
                    url = img[0]
                    try:
                        ccurl(url, curl_opt='-o', out_file=thumb)
                    except Exception as err:
                        print(err, '--151--')
            elif (self.copy_poster or self.copy_fanart) and url and direct_url:
                if 'last.fm' in url:
                    logger.info('--artist-link---{0}'.format(url))
                    content = ccurl(url)
                    soup = BeautifulSoup(content, 'lxml')
                    link = soup.findAll('img')
                    url1Code = url.split('/')[-1]
                    found = None
                    for i in link:
                        if 'src' in str(i):
                            j = i['src']
                            k = j.split('/')[-1]
                            if url1Code == k:
                                found = j
                                break
                    logger.info(str(found))
                    if found:
                        u1 = found.rsplit('/', 2)[0]
                        u2 = found.split('/')[-1]
                        final = u1 + '/770x0/'+u2
                        logger.info(final)
                elif ("".jpg"" in url or "".png"" in url) and url.startswith('http'):
                    final = url
                else:
                    final = ''
                try:
                    if final.startswith('http'):
                        ccurl(final, curl_opt='-o', out_file=thumb)
                except Exception as e:
                    print(e)
        else:
            nam = ui.metaengine.name_adjust(name)
            src_site = 'tvdb'
            epn_arr = []
            post_val = ''
            fan_val = ''
            logger.debug('\nvideo_dir={0}\n'.format(self.video_dir))
            if site.lower() == 'video' and self.video_dir:
                 video_db = os.path.join(ui.home_folder, 'VideoDB', 'Video.db')
                 if os.path.exists(video_db):
                    epn_arr_tmp = ui.media_data.get_video_db(video_db, ""Directory"", self.video_dir)
                    for i in epn_arr_tmp:
                        epn_name = i[0]+'	'+i[1]
                        logger.debug(epn_name)
                        epn_arr.append(epn_name)
            elif self.video_dir:
                new_name_with_info = self.video_dir.strip()
                extra_info = ''
                if '	' in new_name_with_info:
                    name_title = new_name_with_info.split('	')[0]
                    extra_info = new_name_with_info.split('	')[1]
                else:
                    name_title = new_name_with_info
                
                if site.lower() == 'subbedanime' or site.lower() == 'dubbedanime' and siteName:
                    hist_site = os.path.join(ui.home_folder, 'History', site, siteName, name_title)
                else:
                    hist_site = os.path.join(ui.home_folder, 'History', site, name_title)
                    
                hist_epn = os.path.join(hist_site, 'Ep.txt')
                logger.info(hist_epn)
                if os.path.exists(hist_epn):
                    lines = open_files(hist_epn, True)
                    for i in lines:
                        i = i.strip()
                        j = i.split('	')
                        if len(j) == 1:
                            epn_arr.append(i+'	'+i+'	'+name)
                        elif len(j) >= 2:
                            epn_arr.append(i+'	'+name)
                            
            if ui.series_info_dict.get(name) and not epn_arr:
                logger.debug('getting values from cache')
                dict_val = ui.series_info_dict.get(name)
                post_arr = dict_val.get('poster')
                fan_arr = dict_val.get('fanart')
                fan_index = dict_val.get('f')
                post_index = dict_val.get('p')
                if fan_index < len(fan_arr):
                    fan_val = fan_arr[fan_index]
                    fan_index = (fan_index + 1) % len(fan_arr)
                    dict_val.update({'f':fan_index})
                if post_index < len(post_arr):
                    post_val = post_arr[post_index]
                    post_index = (post_index + 1) % len(post_arr)
                    dict_val.update({'p':post_index})
                ui.series_info_dict.update({name:dict_val})
                if isinstance(self.use_search, bool):
                    src_site = 'tvdb'
                else:
                    src_site = self.use_search
            else:
                m, final_link, src_site = self.init_search(
                    nam, url, direct_url, thumb, fanart, src_site
                    )
            if (m and src_site in ['tvdb', 'tvdb+g', 'tvdb+ddg']) or post_val or fan_val:
                if post_val or fan_val:
                    if post_val:
                        url = ""http://thetvdb.com/"" + post_val
                        ccurl(url+'#'+'-o'+'#'+thumb)
                    if fan_val:
                        url = ""http://thetvdb.com/"" + fan_val
                        ccurl(url+'#'+'-o'+'#'+fanart)
                else:
                    if not final_link:
                        n = re.sub('amp;', '', m[0])
                        elist = re.sub('tab=series', 'tab=seasonall', n)
                        url = ""http://thetvdb.com"" + n
                        logger.info(url)
                        elist_url = ""http://thetvdb.com"" + elist
                    else:
                        url = final_link
                    post_arr, fan_arr = self.parse_tvdb(name, url)
                    if post_arr:
                        url = ""http://thetvdb.com/"" + post_arr[0]
                        ccurl(url+'#'+'-o'+'#'+thumb)
                        logger.info(post_arr)
                    if fan_arr:
                        #if ui.player_theme != 'default':
                        fan_arr = [i for i in fan_arr if 'vignette' not in i]
                        if fan_arr:
                            url = ""http://thetvdb.com/"" + fan_arr[0]
                            ccurl(url+'#'+'-o'+'#'+fanart)
                        logger.debug(fan_arr)
                    fan_arr.sort()
                    post_arr.sort()
                    ui.series_info_dict.update(
                            {
                            name:{
                                'fanart':fan_arr.copy(), 'poster':post_arr.copy(),
                                'f':0, 'p':0
                                }
                            }
                        )
                    elist_url = re.sub('tab=series', 'tab=seasonall', final_link)
                    if epn_arr:
                        ui.metaengine.getTvdbEpnInfo(
                            elist_url, epn_arr=epn_arr.copy(), site=site,
                            name=name, thread=self, video_dir=self.video_dir
                            )
                        image_dict = self.image_dict_list.copy()
                        dest_dir = self.dest_dir
                        self.imagesignal.emit(image_dict, dest_dir, site)
            elif m and src_site in ['tmdb', 'tmdb+g', 'tmdb+ddg']:
                self.parse_tmdb(name, final_link, thumb, fanart)","for k in j:
    l = k['src']
    u1 = l.rsplit('/', 2)[0]
    u2 = l.split('/')[-1]
    u = u1 + '/770x0/' + u2
    img.append(u)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: The given unpacked element e['src'] is a dictionary value that can be accessed directly using the key 'src'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/thread_modules.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/thread_modules.py,FindPosterThread,run$393,"def run(self):
        name = self.name
        url = self.url
        direct_url = self.direct_url
        #print(name, url, direct_url, '--poster--thread--')
        fanart = os.path.join(TMPDIR, name+'-fanart.jpg')
        thumb = os.path.join(TMPDIR, name+'.jpg')
        fan_text = os.path.join(TMPDIR, name+'-fanart.txt')
        post_text = os.path.join(TMPDIR, name+'-poster.txt')
        logger.info(fanart)
        logger.info(thumb)
        final_link = """"
        m = []

        if site == 'Music':
            final = ''
            if (self.copy_fanart and self.copy_poster and self.copy_summary):
                if not direct_url and not url:
                    nam = ui.metaengine.name_adjust(name)
                    url = ""http://www.last.fm/search?q=""+nam
                    logger.info(url)
                wiki = """"
                content = ccurl(url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('div', {'class':'row clearfix'})
                name3 = """"
                for i in link:
                    j = i.findAll('a')
                    for k in j:
                        try:
                            url = k['href']
                            if '?q=' not in url:
                                logger.info(url)
                                break
                        except Exception as err:
                            print(err, '--108--')
                logger.info(url)
                if url.startswith('http'):
                    url = url
                else:
                    url = ""http://www.last.fm"" + url
                logger.info(url)
                img_url = url+'/+images'
                wiki_url = url + '/+wiki'
                logger.info(wiki_url)
                content = ccurl(wiki_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.find('div', {'class':'wiki-content'})
                if link:
                    wiki = link.text
                    self.summary_signal.emit(name, wiki, 'summary')
                content = ccurl(img_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('ul', {'class':'image-list'})
                img = []
                for i in link:
                    j = i.findAll('img')
                    for k in j:
                        l = k['src']
                        u1 = l.rsplit('/', 2)[0]
                        u2 = l.split('/')[-1]
                        u = u1 + '/770x0/'+u2
                        img.append(u)
                img = list(set(img))
                logger.info(len(img))
                thumb = os.path.join(TMPDIR, name+'.jpg')
                if img:
                    url = img[0]
                    try:
                        ccurl(url, curl_opt='-o', out_file=thumb)
                    except Exception as err:
                        print(err, '--151--')
            elif (self.copy_poster or self.copy_fanart) and url and direct_url:
                if 'last.fm' in url:
                    logger.info('--artist-link---{0}'.format(url))
                    content = ccurl(url)
                    soup = BeautifulSoup(content, 'lxml')
                    link = soup.findAll('img')
                    url1Code = url.split('/')[-1]
                    found = None
                    for i in link:
                        if 'src' in str(i):
                            j = i['src']
                            k = j.split('/')[-1]
                            if url1Code == k:
                                found = j
                                break
                    logger.info(str(found))
                    if found:
                        u1 = found.rsplit('/', 2)[0]
                        u2 = found.split('/')[-1]
                        final = u1 + '/770x0/'+u2
                        logger.info(final)
                elif ("".jpg"" in url or "".png"" in url) and url.startswith('http'):
                    final = url
                else:
                    final = ''
                try:
                    if final.startswith('http'):
                        ccurl(final, curl_opt='-o', out_file=thumb)
                except Exception as e:
                    print(e)
        else:
            nam = ui.metaengine.name_adjust(name)
            src_site = 'tvdb'
            epn_arr = []
            post_val = ''
            fan_val = ''
            logger.debug('\nvideo_dir={0}\n'.format(self.video_dir))
            if site.lower() == 'video' and self.video_dir:
                 video_db = os.path.join(ui.home_folder, 'VideoDB', 'Video.db')
                 if os.path.exists(video_db):
                    epn_arr_tmp = ui.media_data.get_video_db(video_db, ""Directory"", self.video_dir)
                    for i in epn_arr_tmp:
                        epn_name = i[0]+'	'+i[1]
                        logger.debug(epn_name)
                        epn_arr.append(epn_name)
            elif self.video_dir:
                new_name_with_info = self.video_dir.strip()
                extra_info = ''
                if '	' in new_name_with_info:
                    name_title = new_name_with_info.split('	')[0]
                    extra_info = new_name_with_info.split('	')[1]
                else:
                    name_title = new_name_with_info
                
                if site.lower() == 'subbedanime' or site.lower() == 'dubbedanime' and siteName:
                    hist_site = os.path.join(ui.home_folder, 'History', site, siteName, name_title)
                else:
                    hist_site = os.path.join(ui.home_folder, 'History', site, name_title)
                    
                hist_epn = os.path.join(hist_site, 'Ep.txt')
                logger.info(hist_epn)
                if os.path.exists(hist_epn):
                    lines = open_files(hist_epn, True)
                    for i in lines:
                        i = i.strip()
                        j = i.split('	')
                        if len(j) == 1:
                            epn_arr.append(i+'	'+i+'	'+name)
                        elif len(j) >= 2:
                            epn_arr.append(i+'	'+name)
                            
            if ui.series_info_dict.get(name) and not epn_arr:
                logger.debug('getting values from cache')
                dict_val = ui.series_info_dict.get(name)
                post_arr = dict_val.get('poster')
                fan_arr = dict_val.get('fanart')
                fan_index = dict_val.get('f')
                post_index = dict_val.get('p')
                if fan_index < len(fan_arr):
                    fan_val = fan_arr[fan_index]
                    fan_index = (fan_index + 1) % len(fan_arr)
                    dict_val.update({'f':fan_index})
                if post_index < len(post_arr):
                    post_val = post_arr[post_index]
                    post_index = (post_index + 1) % len(post_arr)
                    dict_val.update({'p':post_index})
                ui.series_info_dict.update({name:dict_val})
                if isinstance(self.use_search, bool):
                    src_site = 'tvdb'
                else:
                    src_site = self.use_search
            else:
                m, final_link, src_site = self.init_search(
                    nam, url, direct_url, thumb, fanart, src_site
                    )
            if (m and src_site in ['tvdb', 'tvdb+g', 'tvdb+ddg']) or post_val or fan_val:
                if post_val or fan_val:
                    if post_val:
                        url = ""http://thetvdb.com/"" + post_val
                        ccurl(url+'#'+'-o'+'#'+thumb)
                    if fan_val:
                        url = ""http://thetvdb.com/"" + fan_val
                        ccurl(url+'#'+'-o'+'#'+fanart)
                else:
                    if not final_link:
                        n = re.sub('amp;', '', m[0])
                        elist = re.sub('tab=series', 'tab=seasonall', n)
                        url = ""http://thetvdb.com"" + n
                        logger.info(url)
                        elist_url = ""http://thetvdb.com"" + elist
                    else:
                        url = final_link
                    post_arr, fan_arr = self.parse_tvdb(name, url)
                    if post_arr:
                        url = ""http://thetvdb.com/"" + post_arr[0]
                        ccurl(url+'#'+'-o'+'#'+thumb)
                        logger.info(post_arr)
                    if fan_arr:
                        #if ui.player_theme != 'default':
                        fan_arr = [i for i in fan_arr if 'vignette' not in i]
                        if fan_arr:
                            url = ""http://thetvdb.com/"" + fan_arr[0]
                            ccurl(url+'#'+'-o'+'#'+fanart)
                        logger.debug(fan_arr)
                    fan_arr.sort()
                    post_arr.sort()
                    ui.series_info_dict.update(
                            {
                            name:{
                                'fanart':fan_arr.copy(), 'poster':post_arr.copy(),
                                'f':0, 'p':0
                                }
                            }
                        )
                    elist_url = re.sub('tab=series', 'tab=seasonall', final_link)
                    if epn_arr:
                        ui.metaengine.getTvdbEpnInfo(
                            elist_url, epn_arr=epn_arr.copy(), site=site,
                            name=name, thread=self, video_dir=self.video_dir
                            )
                        image_dict = self.image_dict_list.copy()
                        dest_dir = self.dest_dir
                        self.imagesignal.emit(image_dict, dest_dir, site)
            elif m and src_site in ['tmdb', 'tmdb+g', 'tmdb+ddg']:
                self.parse_tmdb(name, final_link, thumb, fanart)","for i in epn_arr_tmp:
    epn_name = i[0] + '\t' + i[1]
    logger.debug(epn_name)
    epn_arr.append(epn_name)","for i in epn_arr_tmp:
    (i_0, i_1, *_) = i
    epn_name = i[0] + '\t' + i[1]
    logger.debug(epn_name)
    epn_arr.append(epn_name)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/thread_modules.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/thread_modules.py,FindPosterThread,run$393,"def run(self):
        name = self.name
        url = self.url
        direct_url = self.direct_url
        #print(name, url, direct_url, '--poster--thread--')
        fanart = os.path.join(TMPDIR, name+'-fanart.jpg')
        thumb = os.path.join(TMPDIR, name+'.jpg')
        fan_text = os.path.join(TMPDIR, name+'-fanart.txt')
        post_text = os.path.join(TMPDIR, name+'-poster.txt')
        logger.info(fanart)
        logger.info(thumb)
        final_link = """"
        m = []

        if site == 'Music':
            final = ''
            if (self.copy_fanart and self.copy_poster and self.copy_summary):
                if not direct_url and not url:
                    nam = ui.metaengine.name_adjust(name)
                    url = ""http://www.last.fm/search?q=""+nam
                    logger.info(url)
                wiki = """"
                content = ccurl(url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('div', {'class':'row clearfix'})
                name3 = """"
                for i in link:
                    j = i.findAll('a')
                    for k in j:
                        try:
                            url = k['href']
                            if '?q=' not in url:
                                logger.info(url)
                                break
                        except Exception as err:
                            print(err, '--108--')
                logger.info(url)
                if url.startswith('http'):
                    url = url
                else:
                    url = ""http://www.last.fm"" + url
                logger.info(url)
                img_url = url+'/+images'
                wiki_url = url + '/+wiki'
                logger.info(wiki_url)
                content = ccurl(wiki_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.find('div', {'class':'wiki-content'})
                if link:
                    wiki = link.text
                    self.summary_signal.emit(name, wiki, 'summary')
                content = ccurl(img_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('ul', {'class':'image-list'})
                img = []
                for i in link:
                    j = i.findAll('img')
                    for k in j:
                        l = k['src']
                        u1 = l.rsplit('/', 2)[0]
                        u2 = l.split('/')[-1]
                        u = u1 + '/770x0/'+u2
                        img.append(u)
                img = list(set(img))
                logger.info(len(img))
                thumb = os.path.join(TMPDIR, name+'.jpg')
                if img:
                    url = img[0]
                    try:
                        ccurl(url, curl_opt='-o', out_file=thumb)
                    except Exception as err:
                        print(err, '--151--')
            elif (self.copy_poster or self.copy_fanart) and url and direct_url:
                if 'last.fm' in url:
                    logger.info('--artist-link---{0}'.format(url))
                    content = ccurl(url)
                    soup = BeautifulSoup(content, 'lxml')
                    link = soup.findAll('img')
                    url1Code = url.split('/')[-1]
                    found = None
                    for i in link:
                        if 'src' in str(i):
                            j = i['src']
                            k = j.split('/')[-1]
                            if url1Code == k:
                                found = j
                                break
                    logger.info(str(found))
                    if found:
                        u1 = found.rsplit('/', 2)[0]
                        u2 = found.split('/')[-1]
                        final = u1 + '/770x0/'+u2
                        logger.info(final)
                elif ("".jpg"" in url or "".png"" in url) and url.startswith('http'):
                    final = url
                else:
                    final = ''
                try:
                    if final.startswith('http'):
                        ccurl(final, curl_opt='-o', out_file=thumb)
                except Exception as e:
                    print(e)
        else:
            nam = ui.metaengine.name_adjust(name)
            src_site = 'tvdb'
            epn_arr = []
            post_val = ''
            fan_val = ''
            logger.debug('\nvideo_dir={0}\n'.format(self.video_dir))
            if site.lower() == 'video' and self.video_dir:
                 video_db = os.path.join(ui.home_folder, 'VideoDB', 'Video.db')
                 if os.path.exists(video_db):
                    epn_arr_tmp = ui.media_data.get_video_db(video_db, ""Directory"", self.video_dir)
                    for i in epn_arr_tmp:
                        epn_name = i[0]+'	'+i[1]
                        logger.debug(epn_name)
                        epn_arr.append(epn_name)
            elif self.video_dir:
                new_name_with_info = self.video_dir.strip()
                extra_info = ''
                if '	' in new_name_with_info:
                    name_title = new_name_with_info.split('	')[0]
                    extra_info = new_name_with_info.split('	')[1]
                else:
                    name_title = new_name_with_info
                
                if site.lower() == 'subbedanime' or site.lower() == 'dubbedanime' and siteName:
                    hist_site = os.path.join(ui.home_folder, 'History', site, siteName, name_title)
                else:
                    hist_site = os.path.join(ui.home_folder, 'History', site, name_title)
                    
                hist_epn = os.path.join(hist_site, 'Ep.txt')
                logger.info(hist_epn)
                if os.path.exists(hist_epn):
                    lines = open_files(hist_epn, True)
                    for i in lines:
                        i = i.strip()
                        j = i.split('	')
                        if len(j) == 1:
                            epn_arr.append(i+'	'+i+'	'+name)
                        elif len(j) >= 2:
                            epn_arr.append(i+'	'+name)
                            
            if ui.series_info_dict.get(name) and not epn_arr:
                logger.debug('getting values from cache')
                dict_val = ui.series_info_dict.get(name)
                post_arr = dict_val.get('poster')
                fan_arr = dict_val.get('fanart')
                fan_index = dict_val.get('f')
                post_index = dict_val.get('p')
                if fan_index < len(fan_arr):
                    fan_val = fan_arr[fan_index]
                    fan_index = (fan_index + 1) % len(fan_arr)
                    dict_val.update({'f':fan_index})
                if post_index < len(post_arr):
                    post_val = post_arr[post_index]
                    post_index = (post_index + 1) % len(post_arr)
                    dict_val.update({'p':post_index})
                ui.series_info_dict.update({name:dict_val})
                if isinstance(self.use_search, bool):
                    src_site = 'tvdb'
                else:
                    src_site = self.use_search
            else:
                m, final_link, src_site = self.init_search(
                    nam, url, direct_url, thumb, fanart, src_site
                    )
            if (m and src_site in ['tvdb', 'tvdb+g', 'tvdb+ddg']) or post_val or fan_val:
                if post_val or fan_val:
                    if post_val:
                        url = ""http://thetvdb.com/"" + post_val
                        ccurl(url+'#'+'-o'+'#'+thumb)
                    if fan_val:
                        url = ""http://thetvdb.com/"" + fan_val
                        ccurl(url+'#'+'-o'+'#'+fanart)
                else:
                    if not final_link:
                        n = re.sub('amp;', '', m[0])
                        elist = re.sub('tab=series', 'tab=seasonall', n)
                        url = ""http://thetvdb.com"" + n
                        logger.info(url)
                        elist_url = ""http://thetvdb.com"" + elist
                    else:
                        url = final_link
                    post_arr, fan_arr = self.parse_tvdb(name, url)
                    if post_arr:
                        url = ""http://thetvdb.com/"" + post_arr[0]
                        ccurl(url+'#'+'-o'+'#'+thumb)
                        logger.info(post_arr)
                    if fan_arr:
                        #if ui.player_theme != 'default':
                        fan_arr = [i for i in fan_arr if 'vignette' not in i]
                        if fan_arr:
                            url = ""http://thetvdb.com/"" + fan_arr[0]
                            ccurl(url+'#'+'-o'+'#'+fanart)
                        logger.debug(fan_arr)
                    fan_arr.sort()
                    post_arr.sort()
                    ui.series_info_dict.update(
                            {
                            name:{
                                'fanart':fan_arr.copy(), 'poster':post_arr.copy(),
                                'f':0, 'p':0
                                }
                            }
                        )
                    elist_url = re.sub('tab=series', 'tab=seasonall', final_link)
                    if epn_arr:
                        ui.metaengine.getTvdbEpnInfo(
                            elist_url, epn_arr=epn_arr.copy(), site=site,
                            name=name, thread=self, video_dir=self.video_dir
                            )
                        image_dict = self.image_dict_list.copy()
                        dest_dir = self.dest_dir
                        self.imagesignal.emit(image_dict, dest_dir, site)
            elif m and src_site in ['tmdb', 'tmdb+g', 'tmdb+ddg']:
                self.parse_tmdb(name, final_link, thumb, fanart)","for i in link:
    if 'src' in str(i):
        j = i['src']
        k = j.split('/')[-1]
        if url1Code == k:
            found = j
            break",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['src'] is a dictionary value that can be accessed directly using the key 'src'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
WireViz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WireViz/src/wireviz/Harness.py,https://github.com/formatc1702/WireViz/tree/master/src/wireviz/Harness.py,Harness,create_graph$94,"def create_graph(self) -> Graph:
        dot = Graph()
        dot.body.append(f'// Graph generated by {APP_NAME} {__version__}\n')
        dot.body.append(f'// {APP_URL}\n')
        dot.attr('graph', rankdir='LR',
                 ranksep='2',
                 bgcolor=wv_colors.translate_color(self.options.bgcolor, ""HEX""),
                 nodesep='0.33',
                 fontname=self.options.fontname)
        dot.attr('node',
                 shape='none',
                 width='0', height='0', margin='0',  # Actual size of the node is entirely determined by the label.
                 style='filled',
                 fillcolor=wv_colors.translate_color(self.options.bgcolor_node, ""HEX""),
                 fontname=self.options.fontname)
        dot.attr('edge', style='bold',
                 fontname=self.options.fontname)

        # prepare ports on connectors depending on which side they will connect
        for _, cable in self.cables.items():
            for connection_color in cable.connections:
                if connection_color.from_port is not None:  # connect to left
                    self.connectors[connection_color.from_name].ports_right = True
                if connection_color.to_port is not None:  # connect to right
                    self.connectors[connection_color.to_name].ports_left = True

        for connector in self.connectors.values():

            # If no wires connected (except maybe loop wires)?
            if not (connector.ports_left or connector.ports_right):
                connector.ports_left = True  # Use left side pins.

            html = []

            rows = [[f'{html_bgcolor(connector.bgcolor_title)}{remove_links(connector.name)}'
                        if connector.show_name else None],
                    [pn_info_string(HEADER_PN, None, remove_links(connector.pn)),
                     html_line_breaks(pn_info_string(HEADER_MPN, connector.manufacturer, connector.mpn)),
                     html_line_breaks(pn_info_string(HEADER_SPN, connector.supplier, connector.spn))],
                    [html_line_breaks(connector.type),
                     html_line_breaks(connector.subtype),
                     f'{connector.pincount}-pin' if connector.show_pincount else None,
                     translate_color(connector.color, self.options.color_mode) if connector.color else None,
                     html_colorbar(connector.color)],
                    '<!-- connector table -->' if connector.style != 'simple' else None,
                    [html_image(connector.image)],
                    [html_caption(connector.image)]]
            rows.extend(get_additional_component_table(self, connector))
            rows.append([html_line_breaks(connector.notes)])
            html.extend(nested_html_table(rows, html_bgcolor_attr(connector.bgcolor)))

            if connector.style != 'simple':
                pinhtml = []
                pinhtml.append('<table border=""0"" cellspacing=""0"" cellpadding=""3"" cellborder=""1"">')

                for pinindex, (pinname, pinlabel, pincolor) in enumerate(zip_longest(connector.pins, connector.pinlabels, connector.pincolors)):
                    if connector.hide_disconnected_pins and not connector.visible_pins.get(pinname, False):
                        continue
                    pinhtml.append('   <tr>')
                    if connector.ports_left:
                        pinhtml.append(f'    <td port=""p{pinindex+1}l"">{pinname}</td>')
                    if pinlabel:
                        pinhtml.append(f'    <td>{pinlabel}</td>')
                    if connector.pincolors:
                        if pincolor in wv_colors._color_hex.keys():
                            pinhtml.append(f'    <td sides=""tbl"">{translate_color(pincolor, self.options.color_mode)}</td>')
                            pinhtml.append( '    <td sides=""tbr"">')
                            pinhtml.append( '     <table border=""0"" cellborder=""1""><tr>')
                            pinhtml.append(f'      <td bgcolor=""{wv_colors.translate_color(pincolor, ""HEX"")}"" width=""8"" height=""8"" fixedsize=""true""></td>')
                            pinhtml.append( '     </tr></table>')
                            pinhtml.append( '    </td>')
                        else:
                            pinhtml.append( '    <td colspan=""2""></td>')

                    if connector.ports_right:
                        pinhtml.append(f'    <td port=""p{pinindex+1}r"">{pinname}</td>')
                    pinhtml.append('   </tr>')

                pinhtml.append('  </table>')

                html = [row.replace('<!-- connector table -->', '\n'.join(pinhtml)) for row in html]

            html = '\n'.join(html)
            dot.node(connector.name, label=f'<\n{html}\n>', shape='box', style='filled',
                     fillcolor=translate_color(self.options.bgcolor_connector, ""HEX""))

            if len(connector.loops) > 0:
                dot.attr('edge', color='#000000:#ffffff:#000000')
                if connector.ports_left:
                    loop_side = 'l'
                    loop_dir = 'w'
                elif connector.ports_right:
                    loop_side = 'r'
                    loop_dir = 'e'
                else:
                    raise Exception('No side for loops')
                for loop in connector.loops:
                    dot.edge(f'{connector.name}:p{loop[0]}{loop_side}:{loop_dir}',
                             f'{connector.name}:p{loop[1]}{loop_side}:{loop_dir}')


        # determine if there are double- or triple-colored wires in the harness;
        # if so, pad single-color wires to make all wires of equal thickness
        pad = any(len(colorstr) > 2 for cable in self.cables.values() for colorstr in cable.colors)

        for cable in self.cables.values():

            html = []

            awg_fmt = ''
            if cable.show_equiv:
                # Only convert units we actually know about, i.e. currently
                # mm2 and awg --- other units _are_ technically allowed,
                # and passed through as-is.
                if cable.gauge_unit =='mm\u00B2':
                    awg_fmt = f' ({awg_equiv(cable.gauge)} AWG)'
                elif cable.gauge_unit.upper() == 'AWG':
                    awg_fmt = f' ({mm2_equiv(cable.gauge)} mm\u00B2)'

            rows = [[f'{html_bgcolor(cable.bgcolor_title)}{remove_links(cable.name)}'
                        if cable.show_name else None],
                    [pn_info_string(HEADER_PN, None,
                        remove_links(cable.pn)) if not isinstance(cable.pn, list) else None,
                     html_line_breaks(pn_info_string(HEADER_MPN,
                        cable.manufacturer if not isinstance(cable.manufacturer, list) else None,
                        cable.mpn if not isinstance(cable.mpn, list) else None)),
                     html_line_breaks(pn_info_string(HEADER_SPN,
                        cable.supplier if not isinstance(cable.supplier, list) else None,
                        cable.spn if not isinstance(cable.spn, list) else None))],
                    [html_line_breaks(cable.type),
                     f'{cable.wirecount}x' if cable.show_wirecount else None,
                     f'{cable.gauge} {cable.gauge_unit}{awg_fmt}' if cable.gauge else None,
                     '+ S' if cable.shield else None,
                     f'{cable.length} {cable.length_unit}' if cable.length > 0 else None,
                     translate_color(cable.color, self.options.color_mode) if cable.color else None,
                     html_colorbar(cable.color)],
                    '<!-- wire table -->',
                    [html_image(cable.image)],
                    [html_caption(cable.image)]]

            rows.extend(get_additional_component_table(self, cable))
            rows.append([html_line_breaks(cable.notes)])
            html.extend(nested_html_table(rows, html_bgcolor_attr(cable.bgcolor)))

            wirehtml = []
            wirehtml.append('<table border=""0"" cellspacing=""0"" cellborder=""0"">')  # conductor table
            wirehtml.append('   <tr><td>&nbsp;</td></tr>')

            for i, (connection_color, wirelabel) in enumerate(zip_longest(cable.colors, cable.wirelabels), 1):
                wirehtml.append('   <tr>')
                wirehtml.append(f'    <td><!-- {i}_in --></td>')
                wirehtml.append(f'    <td>')

                wireinfo = []
                if cable.show_wirenumbers:
                    wireinfo.append(str(i))
                colorstr = wv_colors.translate_color(connection_color, self.options.color_mode)
                if colorstr:
                    wireinfo.append(colorstr)
                if cable.wirelabels:
                    wireinfo.append(wirelabel if wirelabel is not None else '')
                wirehtml.append(f'     {"":"".join(wireinfo)}')

                wirehtml.append(f'    </td>')
                wirehtml.append(f'    <td><!-- {i}_out --></td>')
                wirehtml.append('   </tr>')

                bgcolors = ['#000000'] + get_color_hex(connection_color, pad=pad) + ['#000000']
                wirehtml.append(f'   <tr>')
                wirehtml.append(f'    <td colspan=""3"" border=""0"" cellspacing=""0"" cellpadding=""0"" port=""w{i}"" height=""{(2 * len(bgcolors))}"">')
                wirehtml.append('     <table cellspacing=""0"" cellborder=""0"" border=""0"">')
                for j, bgcolor in enumerate(bgcolors[::-1]):  # Reverse to match the curved wires when more than 2 colors
                    wirehtml.append(f'      <tr><td colspan=""3"" cellpadding=""0"" height=""2"" bgcolor=""{bgcolor if bgcolor != """" else wv_colors.default_color}"" border=""0""></td></tr>')
                wirehtml.append('     </table>')
                wirehtml.append('    </td>')
                wirehtml.append('   </tr>')
                if cable.category == 'bundle':  # for bundles individual wires can have part information
                    # create a list of wire parameters
                    wireidentification = []
                    if isinstance(cable.pn, list):
                        wireidentification.append(pn_info_string(HEADER_PN, None, remove_links(cable.pn[i - 1])))
                    manufacturer_info = pn_info_string(HEADER_MPN,
                        cable.manufacturer[i - 1] if isinstance(cable.manufacturer, list) else None,
                        cable.mpn[i - 1] if isinstance(cable.mpn, list) else None)
                    supplier_info = pn_info_string(HEADER_SPN,
                        cable.supplier[i - 1] if isinstance(cable.supplier, list) else None,
                        cable.spn[i - 1] if isinstance(cable.spn, list) else None)
                    if manufacturer_info:
                        wireidentification.append(html_line_breaks(manufacturer_info))
                    if supplier_info:
                        wireidentification.append(html_line_breaks(supplier_info))
                    # print parameters into a table row under the wire
                    if len(wireidentification) > 0 :
                        wirehtml.append('   <tr><td colspan=""3"">')
                        wirehtml.append('    <table border=""0"" cellspacing=""0"" cellborder=""0""><tr>')
                        for attrib in wireidentification:
                            wirehtml.append(f'     <td>{attrib}</td>')
                        wirehtml.append('    </tr></table>')
                        wirehtml.append('   </td></tr>')

            if cable.shield:
                wirehtml.append('   <tr><td>&nbsp;</td></tr>')  # spacer
                wirehtml.append('   <tr>')
                wirehtml.append('    <td><!-- s_in --></td>')
                wirehtml.append('    <td>Shield</td>')
                wirehtml.append('    <td><!-- s_out --></td>')
                wirehtml.append('   </tr>')
                if isinstance(cable.shield, str):
                    # shield is shown with specified color and black borders
                    shield_color_hex = wv_colors.get_color_hex(cable.shield)[0]
                    attributes = f'height=""6"" bgcolor=""{shield_color_hex}"" border=""2"" sides=""tb""'
                else:
                    # shield is shown as a thin black wire
                    attributes = f'height=""2"" bgcolor=""#000000"" border=""0""'
                wirehtml.append(f'   <tr><td colspan=""3"" cellpadding=""0"" {attributes} port=""ws""></td></tr>')

            wirehtml.append('   <tr><td>&nbsp;</td></tr>')
            wirehtml.append('  </table>')

            html = [row.replace('<!-- wire table -->', '\n'.join(wirehtml)) for row in html]

            # connections
            for connection in cable.connections:
                if isinstance(connection.via_port, int):  # check if it's an actual wire and not a shield
                    dot.attr('edge', color=':'.join(['#000000'] + wv_colors.get_color_hex(cable.colors[connection.via_port - 1], pad=pad) + ['#000000']))
                else:  # it's a shield connection
                    # shield is shown with specified color and black borders, or as a thin black wire otherwise
                    dot.attr('edge', color=':'.join(['#000000', shield_color_hex, '#000000']) if isinstance(cable.shield, str) else '#000000')
                if connection.from_port is not None:  # connect to left
                    from_connector = self.connectors[connection.from_name]
                    from_port = f':p{connection.from_port+1}r' if from_connector.style != 'simple' else ''
                    code_left_1 = f'{connection.from_name}{from_port}:e'
                    code_left_2 = f'{cable.name}:w{connection.via_port}:w'
                    dot.edge(code_left_1, code_left_2)
                    if from_connector.show_name:
                        from_info = [str(connection.from_name), str(self.connectors[connection.from_name].pins[connection.from_port])]
                        if from_connector.pinlabels:
                            pinlabel = from_connector.pinlabels[connection.from_port]
                            if pinlabel != '':
                                from_info.append(pinlabel)
                        from_string = ':'.join(from_info)
                    else:
                        from_string = ''
                    html = [row.replace(f'<!-- {connection.via_port}_in -->', from_string) for row in html]
                if connection.to_port is not None:  # connect to right
                    to_connector = self.connectors[connection.to_name]
                    code_right_1 = f'{cable.name}:w{connection.via_port}:e'
                    to_port = f':p{connection.to_port+1}l' if self.connectors[connection.to_name].style != 'simple' else ''
                    code_right_2 = f'{connection.to_name}{to_port}:w'
                    dot.edge(code_right_1, code_right_2)
                    if to_connector.show_name:
                        to_info = [str(connection.to_name), str(self.connectors[connection.to_name].pins[connection.to_port])]
                        if to_connector.pinlabels:
                            pinlabel = to_connector.pinlabels[connection.to_port]
                            if pinlabel != '':
                                to_info.append(pinlabel)
                        to_string = ':'.join(to_info)
                    else:
                        to_string = ''
                    html = [row.replace(f'<!-- {connection.via_port}_out -->', to_string) for row in html]

            style, bgcolor = ('filled,dashed', self.options.bgcolor_bundle) if cable.category == 'bundle' else \
                             ('filled',        self.options.bgcolor_cable)
            html = '\n'.join(html)
            dot.node(cable.name, label=f'<\n{html}\n>', shape='box',
                     style=style, fillcolor=translate_color(bgcolor, ""HEX""))

        def typecheck(name: str, value: Any, expect: type) -> None:
            if not isinstance(value, expect):
                raise Exception(f'Unexpected value type of {name}: Expected {expect}, got {type(value)}\n{value}')

        # TODO?: Differ between override attributes and HTML?
        if self.tweak.override is not None:
            typecheck('tweak.override', self.tweak.override, dict)
            for k, d in self.tweak.override.items():
                typecheck(f'tweak.override.{k} key', k, str)
                typecheck(f'tweak.override.{k} value', d, dict)
                for a, v in d.items():
                    typecheck(f'tweak.override.{k}.{a} key', a, str)
                    typecheck(f'tweak.override.{k}.{a} value', v, (str, type(None)))

            # Override generated attributes of selected entries matching tweak.override.
            for i, entry in enumerate(dot.body):
                if isinstance(entry, str):
                    # Find a possibly quoted keyword after leading TAB(s) and followed by [ ].
                    match = re.match(r'^\t*("")?((?(1)[^""]|[^ ""])+)(?(1)"") \[.*\]$', entry, re.S)
                    keyword = match and match[2]
                    if keyword in self.tweak.override.keys():
                        for attr, value in self.tweak.override[keyword].items():
                            if value is None:
                                entry, n_subs = re.subn(f'( +)?{attr}=(""[^""]*""|[^] ]*)(?(1)| *)', '', entry)
                                if n_subs < 1:
                                    print(f'Harness.create_graph() warning: {attr} not found in {keyword}!')
                                elif n_subs > 1:
                                    print(f'Harness.create_graph() warning: {attr} removed {n_subs} times in {keyword}!')
                                continue

                            if len(value) == 0 or ' ' in value:
                                value = value.replace('""', r'\""')
                                value = f'""{value}""'
                            entry, n_subs = re.subn(f'{attr}=(""[^""]*""|[^] ]*)', f'{attr}={value}', entry)
                            if n_subs < 1:
                                # If attr not found, then append it
                                entry = re.sub(r'\]$', f' {attr}={value}]', entry)
                            elif n_subs > 1:
                                print(f'Harness.create_graph() warning: {attr} overridden {n_subs} times in {keyword}!')

                        dot.body[i] = entry

        if self.tweak.append is not None:
            if isinstance(self.tweak.append, list):
                for i, element in enumerate(self.tweak.append, 1):
                    typecheck(f'tweak.append[{i}]', element, str)
                dot.body.extend(self.tweak.append)
            else:
                typecheck('tweak.append', self.tweak.append, str)
                dot.body.append(self.tweak.append)

        return dot","for loop in connector.loops:
    dot.edge(f'{connector.name}:p{loop[0]}{loop_side}:{loop_dir}', f'{connector.name}:p{loop[1]}{loop_side}:{loop_dir}')","for loop in connector.loops:
    (loop_0, loop_1, *_) = loop
    dot.edge(f'{connector.name}:p{loop[0]}{loop_side}:{loop_dir}', f'{connector.name}:p{loop[1]}{loop_side}:{loop_dir}')","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
gif-for-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gif-for-cli/tests/test_display.py,https://github.com/google/gif-for-cli/tree/master/tests/test_display.py,TestDisplayTxtFrames,test_0_loops$58,"def test_0_loops(self):
        stdout = io.StringIO()

        txt_frames = self.txt_frames
        num_loops = 0
        error_after_num_loops = 5
        error_after_num_sleep_calls = error_after_num_loops * len(txt_frames)

        with patch('time.sleep') as mock_sleep:
            num_sleep_calls = 0

            def sleep_side_effect(s):
                nonlocal num_sleep_calls
                num_sleep_calls += 1
                if num_sleep_calls >= error_after_num_sleep_calls:
                    raise KeyboardInterrupt()
                return
            mock_sleep.side_effect = sleep_side_effect

            display_txt_frames(txt_frames, stdout, num_loops, self.seconds_per_frame)

        self.assertEqual(mock_sleep.call_count, error_after_num_loops * len(txt_frames))
        for call in mock_sleep.call_args_list:
            self.assertEqual(call[0][0], self.seconds_per_frame)

        output_ending = '\n' + ANSI_RESET + '\n'
        output = stdout.getvalue()

        self.assertEqual(output[-len(output_ending):], output_ending)

        output = output[:-len(output_ending)]
        output = output.split('\n' + (ANSI_CURSOR_UP * self.height))

        self.assertEqual(output, self.txt_frames * error_after_num_loops)","for call in mock_sleep.call_args_list:
    self.assertEqual(call[0][0], self.seconds_per_frame)","for call in mock_sleep.call_args_list:
    ((call_0_0, _, *call_0_rcallmaining), *call_rcallmaining) = call
    self.assertEqual(call[0][0], self.seconds_per_frame)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",0,,,"Answer: Yes
Iterable Unpacking: (e_0_0, _, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]",,,,,,,
FACT_core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FACT_core/src/statistic/update.py,https://github.com/fkie-cad/FACT_core/tree/master/src/statistic/update.py,StatisticUpdater,_calculate_total_files$401,"def _calculate_total_files(list_of_stat_tuples):
        total_amount_of_files = 0
        for item in list_of_stat_tuples:
            with suppress(IndexError):
                total_amount_of_files += item[0][1]
        return total_amount_of_files","for item in list_of_stat_tuples:
    with suppress(IndexError):
        total_amount_of_files += item[0][1]","for item in list_of_stat_tuples:
    ((item_0_0, item_0_1, *item_0_ritemmaining), *item_ritemmaining) = item
    with suppress(IndexError):
        total_amount_of_files += item[0][1]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: (e_0_0, e_0_1, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_1: e[0][1]",,,,,,,
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/arista_tp.py,https://github.com/google/capirca/tree/master/capirca/lib/arista_tp.py,Term,__str__$160,"def __str__(self):
    # verify platform specific terms. skip the whole term if the platform
    # does not match.
    if (self.term.platform and self._PLATFORM not in self.term.platform):
      return """"
    if (self.term.platform_exclude and
        self._PLATFORM in self.term.platform_exclude):
      return """"

    config = Config()

    # a LoL which will be appended to the config at the end of this method
    # elements will be of the form [indentation, string, verbatim] by
    # default verbatim = False
    term_block = []

    # don't render icmpv6 protocol terms under inet, or icmp under inet6
    if (self.term_type == ""inet6"" and
        ""icmp"" in self.term.protocol) or (self.term_type == ""inet"" and
                                          ""icmpv6"" in self.term.protocol):
      logging.debug(
          self.NO_AF_LOG_PROTO.substitute(
              term=self.term.name,
              proto="", "".join(self.term.protocol),
              af=self.term_type,
          ))
      return """"

    # term verbatim output - this will skip over normal term creation
    # code.  warning generated from policy.py if appropriate.
    if self.term.verbatim:
      for line in self.term.verbatim:
        if line[0] == self._PLATFORM:
          # pass MATCH_INDENT, but this should be ignored in the
          # rendering
          term_block.append([MATCH_INDENT, str(line[1]), True])
      # we return immediately, there's no action to be formed
      for i, s, v in term_block:
        config.Append(i, s, verbatim=v)

      return str(config)

    # option processing
    flags = []
    misc_options = []
    if self.term.option:
      flags, misc_options = self._processTermOptions(self.term,
                                                     self.term.option)

    # helper for per-address-family keywords.
    family_keywords = self._TERM_TYPE.get(self.term_type)

    term_block.append([
        TERM_INDENT,
        ""match %s %s"" % (self.term.name, family_keywords[""addr_fam""]), False
    ])

    term_af = self.AF_MAP.get(self.term_type)
    if self.term.owner and not self.noverbose:
      self.term.comment.append(""owner: %s"" % self.term.owner)
    if self.term.comment and not self.noverbose:
      reflowed_comments = self._reflowComments(self.term.comment,
                                               MAX_COMMENT_LENGTH)
      for line in reflowed_comments:
        term_block.append([MATCH_INDENT, ""!! "" + line, False])

    has_match_criteria = (
        self.term.destination_address or
        self.term.destination_address_exclude or self.term.destination_port or
        self.term.destination_prefix or self.term.fragment_offset or
        self.term.hop_limit or self.term.port or self.term.protocol or
        self.term.protocol_except or self.term.source_address or
        self.term.source_address_exclude or self.term.source_port or
        self.term.source_prefix or self.term.ttl)

    # if the term name is default-* we will render this into the
    # appropriate default term name to be used in this filter.
    is_default_term = re.match(r""^ipv(4|6)\-default\-.*"", self.term.name,
                               re.IGNORECASE)

    if (not has_match_criteria and not is_default_term):
      # this term doesn't match on anything and isn't a default-term
      logging.warning(
          ""WARNING: term %s has no valid match criteria and ""
          ""will not be rendered."",
          self.term.name,
      )
      return """"

    else:
      # source address
      src_addr = self.term.GetAddressOfVersion(""source_address"", term_af)
      src_addr_ex = self.term.GetAddressOfVersion(""source_address_exclude"",
                                                  term_af)

      if src_addr:
        src_str = ""source prefix""
        if src_addr_ex:
          # this should correspond to the generated field set
          src_str += "" field-set src-%s"" % self.term.name
        else:
          for addr in src_addr:
            src_str += "" %s"" % addr

        term_block.append([MATCH_INDENT, src_str, False])
      elif self.term.source_address:
        logging.debug(
            self.NO_AF_LOG_ADDR.substitute(
                term=self.term.name, direction=""source"", af=self.term_type))
        return """"

      # destination address
      dst_addr = self.term.GetAddressOfVersion(""destination_address"", term_af)
      dst_addr_ex = self.term.GetAddressOfVersion(""destination_address_exclude"",
                                                  term_af)

      if dst_addr:
        dst_str = ""destination prefix""
        if dst_addr_ex:
          # this should correspond to the generated field set
          dst_str += "" field-set dst-%s"" % self.term.name
        else:
          for addr in dst_addr:
            dst_str += "" %s"" % addr

        term_block.append([MATCH_INDENT, dst_str, False])

      elif self.term.destination_address:
        logging.debug(
            self.NO_AF_LOG_ADDR.substitute(
                term=self.term.name, direction=""destination"",
                af=self.term_type))
        return """"

      if self.term.source_prefix:
        src_pfx_str = ""source prefix field-set""
        for pfx in self.term.source_prefix:
          src_pfx_str += "" %s"" % pfx

        term_block.append([MATCH_INDENT, "" %s"" % src_pfx_str, False])

      if self.term.destination_prefix:
        dst_pfx_str = ""destination prefix field-set""
        for pfx in self.term.destination_prefix:
          dst_pfx_str += "" %s"" % pfx

        term_block.append([MATCH_INDENT, "" %s"" % dst_pfx_str, False])

      # PROTOCOL MATCHES
      protocol_str = """"
      if self.term.protocol:
        protocol_str = self._processProtocol(self.term_type, self.term, flags)

      # protocol-except handling
      if self.term.protocol_except:
        protocol_str = self._processProtocolExcept(self.term_type, self.term,
                                                   flags)

      # tcp/udp port generation
      port_str = self._processPorts(self.term)
      if port_str:
        protocol_str += port_str

      # icmp[v6] handling
      icmp_type_str = """"
      icmp_code_str = """"
      if self.term.protocol == [""icmp""] or \
         self.term.protocol == [""icmpv6""]:
        icmp_type_str, icmp_code_str = self._processICMP(self.term)

      if self.term.icmp_type:
        protocol_str += icmp_type_str
      if self.term.icmp_code:
        protocol_str += icmp_code_str

      # don't render empty protocol strings.
      if protocol_str:
        term_block.append([MATCH_INDENT, protocol_str, False])

      # ADDITIONAL SUPPORTED MATCH OPTIONS ------------------------------
      # packet length
      if self.term.packet_length:
        term_block.append(
            [MATCH_INDENT,
             ""ip length %s"" % self.term.packet_length, False])

      # fragment offset
      if self.term.fragment_offset:
        term_block.append([
            MATCH_INDENT,
            ""fragment offset %s"" % self.term.fragment_offset, False
        ])

      if self.term.hop_limit:
        term_block.append([MATCH_INDENT, ""ttl %s"" % self.term.hop_limit, False])

      if self.term.ttl:
        term_block.append([MATCH_INDENT, ""ttl %s"" % self.term.ttl, False])

      if misc_options:
        for mopt in misc_options:
          term_block.append([MATCH_INDENT, mopt, False])

    # ACTION HANDLING
    # if there's no action, then this is an implicit permit
    current_action = self._ACTIONS.get(self.term.action[0])
    # non-permit/drop actions should be added here
    has_extra_actions = (
        self.term.logging or self.term.counter or self.term.dscp_set)

    # if !accept - generate an action statement
    # if accept and there are extra actions generate an actions statement
    # if accept and no extra actions don't generate an actions statement
    if self.term.action != [""accept""]:
      term_block.append([MATCH_INDENT, ""actions"", False])
      term_block.append([ACTION_INDENT, ""%s"" % current_action, False])
    elif self.term.action == [""accept""] and has_extra_actions:
      term_block.append([MATCH_INDENT, ""actions"", False])

    if has_extra_actions:
      # logging - only supported on deny actions
      if self.term.logging and self.term.action != [""accept""]:
        term_block.append([ACTION_INDENT, ""log"", False])
      elif self.term.logging and self.term.action == [""accept""]:
        logging.warning(
            ""WARNING: term %s uses logging option but is not a deny ""
            ""action. logging will not be added."",
            self.term.name,
        )

        # counters
      if self.term.counter:
        term_block.append(
            [ACTION_INDENT,
             ""count %s"" % self.term.counter, False])

      term_block.append([MATCH_INDENT, ""!"", False])  # end of actions
    term_block.append([TERM_INDENT, ""!"", False])  # end of match entry

    for tindent, tstr, tverb in term_block:
      config.Append(tindent, tstr, verbatim=tverb)

    return str(config)","for line in self.term.verbatim:
    if line[0] == self._PLATFORM:
        term_block.append([MATCH_INDENT, str(line[1]), True])","for line in self.term.verbatim:
    (line_0, line_1, *_) = line
    if line[0] == self._PLATFORM:
        term_block.append([MATCH_INDENT, str(line[1]), True])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
nlp_xiaojiang,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp_xiaojiang/AugmentText/augment_eda/enhance_word2vec.py,https://github.com/yongzhuo/nlp_xiaojiang/tree/master/AugmentText/augment_eda/enhance_word2vec.py,,get_synonyms_from_word2vec$112,"def get_synonyms_from_word2vec(word2vec_model, word, topn=20, score_top=0.75):
    word_syn = []
    try:
        topn_words = word2vec_model.most_similar(word, topn=topn)
        for topn_word_num in topn_words:
            if topn_word_num[1] >= score_top:
                word_syn.append(topn_word_num[0])
    except Exception as e:
        logger.info(str(e))
    return word_syn","for topn_word_num in topn_words:
    if topn_word_num[1] >= score_top:
        word_syn.append(topn_word_num[0])","for topn_word_num in topn_words:
    (topn_word_num_0, topn_word_num_1, *_) = topn_word_num
    if topn_word_num[1] >= score_top:
        word_syn.append(topn_word_num[0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
PaddleX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex/interpret/core/normlime_base.py,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex/interpret/core/normlime_base.py,,compute_normlime_weights$204,"def compute_normlime_weights(a_list_lime_fnames, save_dir, lime_num_samples):
    normlime_weights_all_labels = {}

    for f in a_list_lime_fnames:
        try:
            lime_weights_and_cluster = np.load(f, allow_pickle=True).item()
            lime_weights = lime_weights_and_cluster['lime_weights']
            cluster = lime_weights_and_cluster['cluster']
        except:
            logging.info('When loading precomputed LIME result, skipping' +
                         str(f))
            continue
        logging.info('Loading precomputed LIME result,' + str(f))
        pred_labels = lime_weights.keys()
        for y in pred_labels:
            normlime_weights = normlime_weights_all_labels.get(y, {})
            w_f_y = [abs(w[1]) for w in lime_weights[y]]
            w_f_y_l1norm = sum(w_f_y)

            for w in lime_weights[y]:
                seg_label = w[0]
                weight = w[1] * w[1] / w_f_y_l1norm
                a = normlime_weights.get(cluster[seg_label], [])
                a.append(weight)
                normlime_weights[cluster[seg_label]] = a

            normlime_weights_all_labels[y] = normlime_weights

    # compute normlime
    for y in normlime_weights_all_labels:
        normlime_weights = normlime_weights_all_labels.get(y, {})
        for k in normlime_weights:
            normlime_weights[k] = sum(normlime_weights[k]) / len(
                normlime_weights[k])

    # check normlime
    if len(normlime_weights_all_labels.keys()) < max(
            normlime_weights_all_labels.keys()) + 1:
        logging.info(
            ""\n"" + \
            ""Warning: !!! \n"" + \
            ""There are at least {} classes, "".format(max(normlime_weights_all_labels.keys()) + 1) + \
            ""but the NormLIME has results of only {} classes. \n"".format(len(normlime_weights_all_labels.keys())) + \
            ""It may have cause unstable results in the later computation"" + \
            "" but can be improved by computing more test samples."" + \
            ""\n""
        )

    n = 0
    f_out = 'normlime_weights_s{}_samples_{}-{}.npy'.format(
        lime_num_samples, len(a_list_lime_fnames), n)
    while os.path.exists(os.path.join(save_dir, f_out)):
        n += 1
        f_out = 'normlime_weights_s{}_samples_{}-{}.npy'.format(
            lime_num_samples, len(a_list_lime_fnames), n)
        continue

    np.save(os.path.join(save_dir, f_out), normlime_weights_all_labels)
    return os.path.join(save_dir, f_out)","for w in lime_weights[y]:
    seg_label = w[0]
    weight = w[1] * w[1] / w_f_y_l1norm
    a = normlime_weights.get(cluster[seg_label], [])
    a.append(weight)
    normlime_weights[cluster[seg_label]] = a","for w in lime_weights[y]:
    (w_0, w_1, *_) = w
    seg_label = w[0]
    weight = w[1] * w[1] / w_f_y_l1norm
    a = normlime_weights.get(cluster[seg_label], [])
    a.append(weight)
    normlime_weights[cluster[seg_label]] = a","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
PythonStdioGames,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonStdioGames/barebones/blackjack-barebones.py,https://github.com/asweigart/PythonStdioGames/tree/master/barebones/blackjack-barebones.py,,getHandValue$83,"def getHandValue(allCards):
    value = 0
    numberOfAces = 0

    # Add the value for the non-ace cards:
    for card in allCards:
        rank = card[0]  # card is a list like [rank, suit]
        if rank == 'A':
            numberOfAces += 1  # Aces are worth at least 1.
        elif rank in ['K', 'Q', 'J']:  # Face cards are worth 10.
            value += 10
        elif rank in ['2', '3', '4', '5', '6', '7', '8', '9', '10']:
            value += int(rank)  # Numbered cards are worth their number.

    # Add the value for the aces:
    value += numberOfAces  # Add 1 per ace.
    for i in range(numberOfAces):
        # If another 10 can be added without busting, do so:
        if value + 10 <= 21:
            value += 10

    return value","for card in allCards:
    rank = card[0]
    if rank == 'A':
        numberOfAces += 1
    elif rank in ['K', 'Q', 'J']:
        value += 10
    elif rank in ['2', '3', '4', '5', '6', '7', '8', '9', '10']:
        value += int(rank)","for card in allCards:
    (card_0, *card_rcardmaining) = card
    rank = card[0]
    if rank == 'A':
        numberOfAces += 1
    elif rank in ['K', 'Q', 'J']:
        value += 10
    elif rank in ['2', '3', '4', '5', '6', '7', '8', '9', '10']:
        value += int(rank)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
nltk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/sem/evaluate.py,https://github.com/nltk/nltk/tree/master/nltk/sem/evaluate.py,Assignment,_addvariant$355,"def _addvariant(self):
        """"""
        Create a more pretty-printable version of the assignment.
        """"""
        list_ = []
        for item in self.items():
            pair = (item[1], item[0])
            list_.append(pair)
        self.variant = list_
        return None","for item in self.items():
    pair = (item[1], item[0])
    list_.append(pair)","for item in self.items():
    (item_0, item_1, *_) = item
    pair = (item[1], item[0])
    list_.append(pair)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
nefarious,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nefarious/src/nefarious/tests/test_import_movie.py,https://github.com/lardbit/nefarious/tree/master/src/nefarious/tests/test_import_movie.py,MovieImportTest,test_movie$25,"def test_movie(self):
        # populate some required data
        nefarious_settings = NefariousSettings()
        # required tmdb config data for saving models
        nefarious_settings.tmdb_configuration = {
            'images': {
                'secure_base_url': 'https://image.tmdb.org/t/p/',
            },
        }
        tmdb_client = get_tmdb_client(nefarious_settings)
        # use the first super user account to assign media
        user = User.objects.create_superuser('test', 'test@test.com', 'test')

        # import
        importer = MovieImporter(
            nefarious_settings=nefarious_settings,
            root_path='/test-download',
            tmdb_client=tmdb_client,
            user=user,
        )
        for test_result in self.movie_tests:
            # prepend '/movie' to the test path
            test_path = os.path.join('/movie', test_result[0])
            import_result = importer.ingest_path(test_path)
            if test_result[1] is False or import_result is False:
                self.assertEqual(test_result[1], import_result, '{} != {}'.format(test_result[1], import_result))
            else:
                watch_movie = WatchMovie(name=test_result[1])
                self.assertTrue(
                    watch_movie.name == import_result.name,
                    '{} != {}'.format(watch_movie.name, import_result.name))","for test_result in self.movie_tests:
    test_path = os.path.join('/movie', test_result[0])
    import_result = importer.ingest_path(test_path)
    if test_result[1] is False or import_result is False:
        self.assertEqual(test_result[1], import_result, '{} != {}'.format(test_result[1], import_result))
    else:
        watch_movie = WatchMovie(name=test_result[1])
        self.assertTrue(watch_movie.name == import_result.name, '{} != {}'.format(watch_movie.name, import_result.name))","for test_result in self.movie_tests:
    (test_result_0, test_result_1, *_) = test_result
    test_path = os.path.join('/movie', test_result[0])
    import_result = importer.ingest_path(test_path)
    if test_result[1] is False or import_result is False:
        self.assertEqual(test_result[1], import_result, '{} != {}'.format(test_result[1], import_result))
    else:
        watch_movie = WatchMovie(name=test_result[1])
        self.assertTrue(watch_movie.name == import_result.name, '{} != {}'.format(watch_movie.name, import_result.name))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
CudaText,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CudaText/app/py/cuda_emmet/proc_snip_insert.py,https://github.com/Alexey-T/CudaText/tree/master/app/py/cuda_emmet/proc_snip_insert.py,,insert_snip_into_editor$8,"def insert_snip_into_editor(ed, snip_lines):
    items = list(snip_lines) #copy list value
    if not items: return

    carets = ed.get_carets()
    if len(carets)!=1: return
    x0, y0, x1, y1 = carets[0]

    tab_spaces = ed.get_prop(ct.PROP_TAB_SPACES)
    tab_size = ed.get_prop(ct.PROP_TAB_SIZE)

    #apply indent to lines from second
    x_col, y_col = ed.convert(ct.CONVERT_CHAR_TO_COL, x0, y0)
    indent = ' '*x_col

    if not tab_spaces:
        indent = indent.replace(' '*tab_size, '\t')

    for i in range(1, len(items)):
        items[i] = indent+items[i]

    #replace tab-chars
    if tab_spaces:
        indent = ' '*tab_size
        items = [item.replace('\t', indent) for item in items]

    #parse tabstops ${0}, ${0:text}
    stops = []
    for index in range(len(items)):
        s = items[index]
        while True:
            digit = 0
            deftext = ''

            n = s.find('${')
            if n<0: break
            n_close = s.find('}', n)
            if n_close<0: break
            n_colon = s.find(':', n)

            digit_end = n_close
            if n_colon>=0:
                digit_end = min(n_close, n_colon)

            try:
                digit = int(s[n+2:digit_end])
            except ValueError:
                break

            #text in tabstop
            if n_colon>=0:
                deftext = s[n_colon+1:n_close]
                s = s[:n]+deftext+s[n_close+1:]
            else:
                s = s[:n]+s[n_close+1:]

            stops += [(digit, deftext, index, n)]
            items[index] = s
    #print('tabstops', stops)

    #insert
    ed.insert(x0, y0, '\n'.join(items))

    #place markers
    mark_placed = False
    ed.markers(ct.MARKERS_DELETE_ALL)

    for digit in MARKS_INDEXES:
        for stop in reversed(stops):
            if stop[0]==digit:
                pos_x = stop[3]
                pos_y = stop[2]
                if pos_y==0:
                    pos_x += x0
                pos_y += y0
                deftext = stop[1]
                ed.markers(ct.MARKERS_ADD, pos_x, pos_y, digit, len(deftext))
                mark_placed = True

    if mark_placed:
        ed.set_prop(ct.PROP_TAB_COLLECT_MARKERS, '1')
        ed.cmd(cudatext_cmd.cmd_Markers_GotoLastAndDelete)","for stop in reversed(stops):
    if stop[0] == digit:
        pos_x = stop[3]
        pos_y = stop[2]
        if pos_y == 0:
            pos_x += x0
        pos_y += y0
        deftext = stop[1]
        ed.markers(ct.MARKERS_ADD, pos_x, pos_y, digit, len(deftext))
        mark_placed = True","for stop in reversed(stops):
    (stop_0, stop_1, stop_2, stop_3, *_) = stop
    if stop[0] == digit:
        pos_x = stop[3]
        pos_y = stop[2]
        if pos_y == 0:
            pos_x += x0
        pos_y += y0
        deftext = stop[1]
        ed.markers(ct.MARKERS_ADD, pos_x, pos_y, digit, len(deftext))
        mark_placed = True","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
camelot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/camelot/camelot/plotting.py,https://github.com/camelot-dev/camelot/tree/master/camelot/plotting.py,PlotMethods,text$50,"def text(self, table):
        """"""Generates a plot for all text elements present
        on the PDF page.

        Parameters
        ----------
        table : camelot.core.Table

        Returns
        -------
        fig : matplotlib.fig.Figure

        """"""
        fig = plt.figure()
        ax = fig.add_subplot(111, aspect=""equal"")
        xs, ys = [], []
        for t in table._text:
            xs.extend([t[0], t[2]])
            ys.extend([t[1], t[3]])
            ax.add_patch(patches.Rectangle((t[0], t[1]), t[2] - t[0], t[3] - t[1]))
        ax.set_xlim(min(xs) - 10, max(xs) + 10)
        ax.set_ylim(min(ys) - 10, max(ys) + 10)
        return fig","for t in table._text:
    xs.extend([t[0], t[2]])
    ys.extend([t[1], t[3]])
    ax.add_patch(patches.Rectangle((t[0], t[1]), t[2] - t[0], t[3] - t[1]))","for t in table._text:
    (t_0, t_1, t_2, t_3, *_) = t
    xs.extend([t[0], t[2]])
    ys.extend([t[1], t[3]])
    ax.add_patch(patches.Rectangle((t[0], t[1]), t[2] - t[0], t[3] - t[1]))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
inception,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/inception/inception/interfaces/slotscreamer.py,https://github.com/carmaa/inception/tree/master/inception/interfaces/slotscreamer.py,SlotScreamer,readv$109,"def readv(self, req):
        # sort requests so sequential reads are cached
        # req.sort()
        for r in req:
            yield(r[0], self.read(r[0], r[1]))","for r in req:
    yield (r[0], self.read(r[0], r[1]))","for r in req:
    (r_0, r_1, *_) = r
    yield (r[0], self.read(r[0], r[1]))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/dataset/tests/flowers_test.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/dataset/tests/flowers_test.py,TestFlowers,check_reader$23,"def check_reader(self, reader):
        sum = 0
        label = 0
        size = 224 * 224 * 3
        for l in reader():
            self.assertEqual(l[0].size, size)
            if l[1] > label:
                label = l[1]
            sum += 1
        return sum, label","for l in reader():
    self.assertEqual(l[0].size, size)
    if l[1] > label:
        label = l[1]
    sum += 1","for l in reader():
    (l_0, l_1, *_) = l
    self.assertEqual(l[0].size, size)
    if l[1] > label:
        label = l[1]
    sum += 1","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
nlp_xiaojiang,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp_xiaojiang/ClassificationText/bert/keras_bert_classify_bi_lstm.py,https://github.com/yongzhuo/nlp_xiaojiang/tree/master/ClassificationText/bert/keras_bert_classify_bi_lstm.py,BertBiLstmModel,process_pair$99,"def process_pair(self, textss):
        # 文本预处理，传入一个list，返回的是ids\mask\type-ids
        input_ids = []
        input_masks = []
        input_type_ids = []
        for texts in textss:
            tokens_text = self.tokenizer.tokenize(texts[0])
            logger.info('Tokens1:', tokens_text)
            tokens_text2 = self.tokenizer.tokenize(texts[1])
            logger.info('Tokens2:', tokens_text2)
            input_id, input_type_id = self.tokenizer.encode(first=texts[0], second=texts[1], max_len=self.max_seq_len)
            input_mask = [0 if ids == 0 else 1 for ids in input_id]
            input_ids.append(input_id)
            input_type_ids.append(input_type_id)
            input_masks.append(input_mask)
        # numpy处理list
        input_ids = np.array(input_ids)
        input_masks = np.array(input_masks)
        input_type_ids = np.array(input_type_ids)
        logger.info(""process ok!"")
        return input_ids, input_masks, input_type_ids","for texts in textss:
    tokens_text = self.tokenizer.tokenize(texts[0])
    logger.info('Tokens1:', tokens_text)
    tokens_text2 = self.tokenizer.tokenize(texts[1])
    logger.info('Tokens2:', tokens_text2)
    (input_id, input_type_id) = self.tokenizer.encode(first=texts[0], second=texts[1], max_len=self.max_seq_len)
    input_mask = [0 if ids == 0 else 1 for ids in input_id]
    input_ids.append(input_id)
    input_type_ids.append(input_type_id)
    input_masks.append(input_mask)","for texts in textss:
    (texts_0, texts_1, *_) = texts
    tokens_text = self.tokenizer.tokenize(texts[0])
    logger.info('Tokens1:', tokens_text)
    tokens_text2 = self.tokenizer.tokenize(texts[1])
    logger.info('Tokens2:', tokens_text2)
    (input_id, input_type_id) = self.tokenizer.encode(first=texts[0], second=texts[1], max_len=self.max_seq_len)
    input_mask = [0 if ids == 0 else 1 for ids in input_id]
    input_ids.append(input_id)
    input_type_ids.append(input_type_id)
    input_masks.append(input_mask)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
redis-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/redis-py/tests/test_search.py,https://github.com/redis/redis-py/tree/master/tests/test_search.py,,test_drop_index$308,"def test_drop_index(client):
    """"""
    Ensure the index gets dropped by data remains by default
    """"""
    for x in range(20):
        for keep_docs in [[True, {}], [False, {""name"": ""haveit""}]]:
            idx = ""HaveIt""
            index = getClient(client)
            index.hset(""index:haveit"", mapping={""name"": ""haveit""})
            idef = IndexDefinition(prefix=[""index:""])
            index.ft(idx).create_index((TextField(""name""),), definition=idef)
            waitForIndex(index, idx)
            index.ft(idx).dropindex(delete_documents=keep_docs[0])
            i = index.hgetall(""index:haveit"")
            assert i == keep_docs[1]","for keep_docs in [[True, {}], [False, {'name': 'haveit'}]]:
    idx = 'HaveIt'
    index = getClient(client)
    index.hset('index:haveit', mapping={'name': 'haveit'})
    idef = IndexDefinition(prefix=['index:'])
    index.ft(idx).create_index((TextField('name'),), definition=idef)
    waitForIndex(index, idx)
    index.ft(idx).dropindex(delete_documents=keep_docs[0])
    i = index.hgetall('index:haveit')
    assert i == keep_docs[1]","for keep_docs in [[True, {}], [False, {'name': 'haveit'}]]:
    (keep_docs_0, keep_docs_1, *_) = keep_docs
    idx = 'HaveIt'
    index = getClient(client)
    index.hset('index:haveit', mapping={'name': 'haveit'})
    idef = IndexDefinition(prefix=['index:'])
    index.ft(idx).create_index((TextField('name'),), definition=idef)
    waitForIndex(index, idx)
    index.ft(idx).dropindex(delete_documents=keep_docs[0])
    i = index.hgetall('index:haveit')
    assert i == keep_docs[1]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/tool_shed/grids/util.py,https://github.com/ansible/galaxy/tree/master/lib/tool_shed/grids/util.py,,build_approved_select_field$9,"def build_approved_select_field(trans, name, selected_value=None, for_component=True):
    options = [('No', trans.model.ComponentReview.approved_states.NO),
               ('Yes', trans.model.ComponentReview.approved_states.YES)]
    if for_component:
        options.append(('Not applicable', trans.model.ComponentReview.approved_states.NA))
        if selected_value is None:
            selected_value = trans.model.ComponentReview.approved_states.NA
    select_field = SelectField(name=name)
    for option_tup in options:
        selected = selected_value and option_tup[1] == selected_value
        select_field.add_option(option_tup[0], option_tup[1], selected=selected)
    return select_field","for option_tup in options:
    selected = selected_value and option_tup[1] == selected_value
    select_field.add_option(option_tup[0], option_tup[1], selected=selected)","for option_tup in options:
    (option_tup_0, option_tup_1, *_) = option_tup
    selected = selected_value and option_tup[1] == selected_value
    select_field.add_option(option_tup[0], option_tup[1], selected=selected)","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Grokking-the-Coding-Interview-Patterns-for-Coding-Questions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Grokking-the-Coding-Interview-Patterns-for-Coding-Questions/16. Pattern Topological Sort (Graph)/Problem Challenge 2 - Minimum Height Trees (hard).py,https://github.com/cl2333/Grokking-the-Coding-Interview-Patterns-for-Coding-Questions/tree/master/16. Pattern Topological Sort (Graph)/Problem Challenge 2 - Minimum Height Trees (hard).py,,find_trees$67,"def find_trees(nodes, edges):
  if nodes <= 0:
    return []

  # with only one node, since its in-degrees will be 0, therefore, we need to handle it separately
  if nodes == 1:
    return [0]

  # a. Initialize the graph
  inDegree = {i: 0 for i in range(nodes)}  # count of incoming edges
  graph = {i: [] for i in range(nodes)}  # adjacency list graph

  # b. Build the graph
  for edge in edges:
    n1, n2 = edge[0], edge[1]
    # since this is an undirected graph, therefore, add a link for both the nodes
    graph[n1].append(n2)
    graph[n2].append(n1)
    # increment the in-degrees of both the nodes
    inDegree[n1] += 1
    inDegree[n2] += 1

  # c. Find all leaves i.e., all nodes with 0 in-degrees
  leaves = deque()
  for key in inDegree:
    if inDegree[key] == 1:
      leaves.append(key)

  # d. Remove leaves level by level and subtract each leave's children's in-degrees.
  # Repeat this until we are left with 1 or 2 nodes, which will be our answer.
  # Any node that has already been a leaf cannot be the root of a minimum height tree, because
  # its adjacent non-leaf node will always be a better candidate.
  totalNodes = nodes
  while totalNodes > 2:
    leavesSize = len(leaves)
    totalNodes -= leavesSize
    for i in range(0, leavesSize):
      vertex = leaves.popleft()
      # get the node's children to decrement their in-degrees
      for child in graph[vertex]:
        inDegree[child] -= 1
        if inDegree[child] == 1:
          leaves.append(child)

  return list(leaves)","for edge in edges:
    (n1, n2) = (edge[0], edge[1])
    graph[n1].append(n2)
    graph[n2].append(n1)
    inDegree[n1] += 1
    inDegree[n2] += 1","for edge in edges:
    (edge_0, edge_1, *_) = edge
    (n1, n2) = (edge[0], edge[1])
    graph[n1].append(n2)
    graph[n2].append(n1)
    inDegree[n1] += 1
    inDegree[n2] += 1","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
kickthemout,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kickthemout/kickthemout.py,https://github.com/k4m4/kickthemout/tree/master//kickthemout.py,,kickalloff$552,"def kickalloff():
    os.system(""clear||cls"")

    print(""\n{}kickALLOff{} selected...{}\n"".format(RED, GREEN, END))
    global stopAnimation
    stopAnimation = False
    t = threading.Thread(target=scanningAnimation, args=('Hang on...',))
    t.daemon = True
    t.start()

    # commence scanning process
    try:
        scanNetwork()
    except KeyboardInterrupt:
        shutdown()
    stopAnimation = True

    print(""Target(s): "")
    for i in range(len(onlineIPs)):
        mac = """"
        for host in hostsList:
            if host[0] == onlineIPs[i]:
                mac = host[1]
        try:
            hostname = utils.socket.gethostbyaddr(onlineIPs[i])[0]
        except:
            hostname = ""N/A""
        vendor = resolveMac(mac)
        print(""  [{}{}{}] {}{}{}\t{}{}\t{} ({}{}{}){}"".format(YELLOW, str(i), WHITE, RED, str(onlineIPs[i]), BLUE, mac, GREEN, vendor, YELLOW, hostname, GREEN, END))
    
    if options.packets is not None:
        print(""\n{}Spoofing started... {}( {} pkts/min )"".format(GREEN, END, str(options.packets)))
    else:
        print(""\n{}Spoofing started... {}"".format(GREEN, END))
    try:
        # broadcast malicious ARP packets
        reScan = 0
        while True:
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    # dodge gateway (avoid crashing network itself)
                    spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, host[0], host[1])
            reScan += 1
            if reScan == 4:
                reScan = 0
                scanNetwork()
            if options.packets is not None:
                time.sleep(60/float(options.packets))
            else:
                time.sleep(10)
    except KeyboardInterrupt:
        print(""\n{}Re-arping{} targets...{}"".format(RED, GREEN, END))
        reArp = 1
        while reArp != 10:
            # broadcast ARP packets with legitimate info to restore connection
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    try:
                        # dodge gateway
                        spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, host[0], host[1])
                    except KeyboardInterrupt:
                        pass
                    except:
                        runDebug()
            reArp += 1
            time.sleep(0.2)
        print(""{}Re-arped{} targets successfully.{}"".format(RED, GREEN, END))","for host in hostsList:
    if host[0] == onlineIPs[i]:
        mac = host[1]","for host in hostsList:
    (host_0, host_1, *_) = host
    if host[0] == onlineIPs[i]:
        mac = host[1]","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
kickthemout,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kickthemout/kickthemout.py,https://github.com/k4m4/kickthemout/tree/master//kickthemout.py,,kickalloff$552,"def kickalloff():
    os.system(""clear||cls"")

    print(""\n{}kickALLOff{} selected...{}\n"".format(RED, GREEN, END))
    global stopAnimation
    stopAnimation = False
    t = threading.Thread(target=scanningAnimation, args=('Hang on...',))
    t.daemon = True
    t.start()

    # commence scanning process
    try:
        scanNetwork()
    except KeyboardInterrupt:
        shutdown()
    stopAnimation = True

    print(""Target(s): "")
    for i in range(len(onlineIPs)):
        mac = """"
        for host in hostsList:
            if host[0] == onlineIPs[i]:
                mac = host[1]
        try:
            hostname = utils.socket.gethostbyaddr(onlineIPs[i])[0]
        except:
            hostname = ""N/A""
        vendor = resolveMac(mac)
        print(""  [{}{}{}] {}{}{}\t{}{}\t{} ({}{}{}){}"".format(YELLOW, str(i), WHITE, RED, str(onlineIPs[i]), BLUE, mac, GREEN, vendor, YELLOW, hostname, GREEN, END))
    
    if options.packets is not None:
        print(""\n{}Spoofing started... {}( {} pkts/min )"".format(GREEN, END, str(options.packets)))
    else:
        print(""\n{}Spoofing started... {}"".format(GREEN, END))
    try:
        # broadcast malicious ARP packets
        reScan = 0
        while True:
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    # dodge gateway (avoid crashing network itself)
                    spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, host[0], host[1])
            reScan += 1
            if reScan == 4:
                reScan = 0
                scanNetwork()
            if options.packets is not None:
                time.sleep(60/float(options.packets))
            else:
                time.sleep(10)
    except KeyboardInterrupt:
        print(""\n{}Re-arping{} targets...{}"".format(RED, GREEN, END))
        reArp = 1
        while reArp != 10:
            # broadcast ARP packets with legitimate info to restore connection
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    try:
                        # dodge gateway
                        spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, host[0], host[1])
                    except KeyboardInterrupt:
                        pass
                    except:
                        runDebug()
            reArp += 1
            time.sleep(0.2)
        print(""{}Re-arped{} targets successfully.{}"".format(RED, GREEN, END))","for host in hostsList:
    if host[0] != defaultGatewayIP:
        spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, host[0], host[1])","for host in hostsList:
    (host_0, host_1, *_) = host
    if host[0] != defaultGatewayIP:
        spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, host[0], host[1])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
kickthemout,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kickthemout/kickthemout.py,https://github.com/k4m4/kickthemout/tree/master//kickthemout.py,,kickalloff$552,"def kickalloff():
    os.system(""clear||cls"")

    print(""\n{}kickALLOff{} selected...{}\n"".format(RED, GREEN, END))
    global stopAnimation
    stopAnimation = False
    t = threading.Thread(target=scanningAnimation, args=('Hang on...',))
    t.daemon = True
    t.start()

    # commence scanning process
    try:
        scanNetwork()
    except KeyboardInterrupt:
        shutdown()
    stopAnimation = True

    print(""Target(s): "")
    for i in range(len(onlineIPs)):
        mac = """"
        for host in hostsList:
            if host[0] == onlineIPs[i]:
                mac = host[1]
        try:
            hostname = utils.socket.gethostbyaddr(onlineIPs[i])[0]
        except:
            hostname = ""N/A""
        vendor = resolveMac(mac)
        print(""  [{}{}{}] {}{}{}\t{}{}\t{} ({}{}{}){}"".format(YELLOW, str(i), WHITE, RED, str(onlineIPs[i]), BLUE, mac, GREEN, vendor, YELLOW, hostname, GREEN, END))
    
    if options.packets is not None:
        print(""\n{}Spoofing started... {}( {} pkts/min )"".format(GREEN, END, str(options.packets)))
    else:
        print(""\n{}Spoofing started... {}"".format(GREEN, END))
    try:
        # broadcast malicious ARP packets
        reScan = 0
        while True:
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    # dodge gateway (avoid crashing network itself)
                    spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, host[0], host[1])
            reScan += 1
            if reScan == 4:
                reScan = 0
                scanNetwork()
            if options.packets is not None:
                time.sleep(60/float(options.packets))
            else:
                time.sleep(10)
    except KeyboardInterrupt:
        print(""\n{}Re-arping{} targets...{}"".format(RED, GREEN, END))
        reArp = 1
        while reArp != 10:
            # broadcast ARP packets with legitimate info to restore connection
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    try:
                        # dodge gateway
                        spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, host[0], host[1])
                    except KeyboardInterrupt:
                        pass
                    except:
                        runDebug()
            reArp += 1
            time.sleep(0.2)
        print(""{}Re-arped{} targets successfully.{}"".format(RED, GREEN, END))","for host in hostsList:
    if host[0] != defaultGatewayIP:
        try:
            spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, host[0], host[1])
        except KeyboardInterrupt:
            pass
        except:
            runDebug()","for host in hostsList:
    (host_0, host_1, *_) = host
    if host[0] != defaultGatewayIP:
        try:
            spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, host[0], host[1])
        except KeyboardInterrupt:
            pass
        except:
            runDebug()","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
two-stream-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/two-stream-pytorch/datasets/build_file_list.py,https://github.com/bryanyzhu/two-stream-pytorch/tree/master/datasets/build_file_list.py,,build_set_list$42,"def build_set_list(set_list):
        rgb_list, flow_list = list(), list()
        for item in set_list:
            rgb_cnt = frame_info[0][item[0]]
            flow_cnt = frame_info[1][item[0]]
            rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
            flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return rgb_list, flow_list","for item in set_list:
    rgb_cnt = frame_info[0][item[0]]
    flow_cnt = frame_info[1][item[0]]
    rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
    flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))","for item in set_list:
    (item_0, item_1, *_) = item
    rgb_cnt = frame_info[0][item[0]]
    flow_cnt = frame_info[1][item[0]]
    rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
    flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
ParlAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/core/torch_generator_agent.py,https://github.com/facebookresearch/ParlAI/tree/master/parlai/core/torch_generator_agent.py,TorchGeneratorAgent,eval_step$894,"def eval_step(self, batch):
        """"""
        Evaluate a single batch of examples.
        """"""
        if batch.text_vec is None and batch.image is None:
            return
        if batch.text_vec is not None:
            bsz = batch.text_vec.size(0)
        else:
            bsz = len(batch.image)
        self.model.eval()
        cand_scores = None
        token_losses = None
        text_token_info = None

        if batch.label_vec is not None:
            # calculate loss on targets with teacher forcing
            loss, model_output = self.compute_loss(batch, return_output=True)
            if self.show_token_details:
                token_losses = self._construct_label_token_losses(
                    batch.label_vec, model_output
                )

        beam_preds_scores = None
        preds = None
        if self.skip_generation:
            warn_once(""--skip-generation true produces limited metrics"")
        else:
            maxlen = self.label_truncate or 256
            prefix_tokens = self.get_prefix_tokens(batch)
            beam_preds_scores, beams = self._generate(
                batch, self.beam_size, maxlen, prefix_tokens=prefix_tokens
            )
            preds, _, _ = zip(*beam_preds_scores)
            self._add_generation_metrics(batch, preds)

            # bsz x beamsize
            beam_texts: List[List[Tuple[str, float]]] = []
            beam_texts_token_info: List[List[List[Tuple]]] = []
            for beam in beams:
                beam_texts.append([])
                if self.show_token_details:
                    beam_texts_token_info.append([])

                for tokens, score, token_metadata in beam.get_rescored_finished():
                    try:
                        if self.show_token_details:
                            beam_texts_token_info[-1].append(
                                self._construct_generated_token_details(
                                    tokens, token_metadata
                                )
                            )
                        beam_texts[-1].append((self._v2t(tokens), score.item()))
                    except KeyError:
                        logging.error(""Decoding error: %s"", tokens)
                        continue

        cand_choices = None
        cand_scores = None
        if self.rank_candidates:
            cand_choices, cand_scores = self.rank_eval_label_candidates(batch, bsz)

        text = (
            [self._v2t(pred_data[0]) for pred_data in beam_preds_scores]
            if beam_preds_scores is not None
            else None
        )

        if self.show_token_details and beam_preds_scores is not None:
            text_token_info = []
            for beam_text_token_info in beam_texts_token_info:
                text_token_info.append(beam_text_token_info[0])

        if text and self.compute_tokenized_bleu:
            # compute additional bleu scores
            self._compute_fairseq_bleu(batch, preds)
        retval = Output(
            text, cand_choices, token_losses=token_losses, cand_scores=cand_scores
        )

        if not self.skip_generation:
            retval.beam_texts = beam_texts
            retval.beam_texts_token_info = beam_texts_token_info
            retval.text_token_info = text_token_info
        return retval","for beam_text_token_info in beam_texts_token_info:
    text_token_info.append(beam_text_token_info[0])","for beam_text_token_info in beam_texts_token_info:
    (beam_text_token_info_0, *beam_text_token_info_rbeam_text_token_infomaining) = beam_text_token_info
    text_token_info.append(beam_text_token_info[0])","(data, data, *data)","for (i_0, i_1, *i_len) in html_str_list:
    s = s.replace(
    i_0, 
    i_1)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Synonyms,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Synonyms/synonyms/synonyms.py,https://github.com/chatopera/Synonyms/tree/master/synonyms/synonyms.py,,nearby$318,"def nearby(word, size = 10):
    '''
    Nearby word
    '''
    w = any2unicode(word)
    wk = w + '-' + str(size)
    # read from cache
    if wk in _cache_nearby: return _cache_nearby[wk]

    words, scores = [], []
    try:
        for x in _vectors.neighbours(w, size):
            words.append(x[0])
            scores.append(x[1])
    except: pass # ignore key error, OOV
    # put into cache
    _cache_nearby[wk] = (words, scores)
    return words, scores","for x in _vectors.neighbours(w, size):
    words.append(x[0])
    scores.append(x[1])","for x in _vectors.neighbours(w, size):
    (x_0, x_1, *_) = x
    words.append(x[0])
    scores.append(x[1])","(data, data, *data)","for (inputs_0, inputs_1, *inputs_len) in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(
        inputs_0)
    assert t.scale == 
    inputs_1
    t2 = Time(
    inputs_0[:
    inputs_0.index('(')], format='isot', scale=
    inputs_1)
    assert t == t2",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
PaddleOCR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleOCR/PPOCRLabel/PPOCRLabel.py,https://github.com/PaddlePaddle/PaddleOCR/tree/master/PPOCRLabel/PPOCRLabel.py,MainWindow,editBox$904,"def editBox(self):  # ADD
        if not self.canvas.editing():
            return
        item = self.currentBox()
        if not item:
            return
        text = self.labelDialog.popUp(item.text())

        imageSize = str(self.image.size())
        width, height = self.image.width(), self.image.height()
        if text:
            try:
                text_list = eval(text)
            except:
                msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Please enter the correct format')
                msg_box.exec_()
                return
            if len(text_list) < 4:
                msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Please enter the coordinates of 4 points')
                msg_box.exec_()
                return
            for box in text_list:
                if box[0] > width or box[0] < 0 or box[1] > height or box[1] < 0:
                    msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')
                    msg_box.exec_()
                    return

            item.setText(text)
            # item.setBackground(generateColorByText(text))
            self.setDirty()
            self.updateComboBox()","for box in text_list:
    if box[0] > width or box[0] < 0 or box[1] > height or (box[1] < 0):
        msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')
        msg_box.exec_()
        return","for box in text_list:
    (box_0, box_1, *_) = box
    if box[0] > width or box[0] < 0 or box[1] > height or (box[1] < 0):
        msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')
        msg_box.exec_()
        return","(data, data, *data)","for (inputs_0, inputs_1, *inputs_len) in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(
        inputs_0)
    assert t.scale == 
    inputs_1
    t2 = Time(
    inputs_0[:
    inputs_0.index('(')], format='isot', scale=
    inputs_1)
    assert t == t2",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
NeoVintageous,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NeoVintageous/tests/functional/test__plugin_surround_ds.py,https://github.com/NeoVintageous/NeoVintageous/tree/master/tests/functional/test__plugin_surround_ds.py,TestSurround_ds,test_t_target_should_delete_tag$215,"def test_t_target_should_delete_tag(self):
        for t in tag_targets_data:
            self.eq('x {}a|b{} y'.format(t[0], t[1]), 'dst', 'x |ab y')","for t in tag_targets_data:
    self.eq('x {}a|b{} y'.format(t[0], t[1]), 'dst', 'x |ab y')","for t in tag_targets_data:
    (t_0, t_1, *_) = t
    self.eq('x {}a|b{} y'.format(t[0], t[1]), 'dst', 'x |ab y')","(data, data, *data)","for (inputs_0, inputs_1, *inputs_len) in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(
        inputs_0)
    assert t.scale == 
    inputs_1
    t2 = Time(
    inputs_0[:
    inputs_0.index('(')], format='isot', scale=
    inputs_1)
    assert t == t2",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
TSD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TSD/mmdet/models/anchor_heads/fovea_head.py,https://github.com/Sense-X/TSD/tree/master/mmdet/models/anchor_heads/fovea_head.py,FoveaHead,get_points$181,"def get_points(self, featmap_sizes, dtype, device, flatten=False):
        points = []
        for featmap_size in featmap_sizes:
            x_range = torch.arange(featmap_size[1], dtype=dtype, device=device) + 0.5
            y_range = torch.arange(featmap_size[0], dtype=dtype, device=device) + 0.5
            y, x = torch.meshgrid(y_range, x_range)
            if flatten:
                points.append((y.flatten(), x.flatten()))
            else:
                points.append((y, x))
        return points","for featmap_size in featmap_sizes:
    x_range = torch.arange(featmap_size[1], dtype=dtype, device=device) + 0.5
    y_range = torch.arange(featmap_size[0], dtype=dtype, device=device) + 0.5
    (y, x) = torch.meshgrid(y_range, x_range)
    if flatten:
        points.append((y.flatten(), x.flatten()))
    else:
        points.append((y, x))","for featmap_size in featmap_sizes:
    (featmap_size_0, featmap_size_1, *_) = featmap_size
    x_range = torch.arange(featmap_size[1], dtype=dtype, device=device) + 0.5
    y_range = torch.arange(featmap_size[0], dtype=dtype, device=device) + 0.5
    (y, x) = torch.meshgrid(y_range, x_range)
    if flatten:
        points.append((y.flatten(), x.flatten()))
    else:
        points.append((y, x))","(data, data, *data)","for (inputs_0, inputs_1, *inputs_len) in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(
        inputs_0)
    assert t.scale == 
    inputs_1
    t2 = Time(
    inputs_0[:
    inputs_0.index('(')], format='isot', scale=
    inputs_1)
    assert t == t2",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/lib/tokeniterator.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/tokeniterator.py,TokenIterator,_get_tokens_in_realm$359,"def _get_tokens_in_realm(self, valid_realms):
        # get all matching realms
        realm_id_tuples = (
            db.session.query(Realm.id)
            .filter(Realm.name.in_(valid_realms))
            .all()
        )
        realm_ids = set()
        for realm_tuple in realm_id_tuples:
            realm_ids.add(realm_tuple[0])
        # get all tokenrealm ids
        token_id_tuples = (
            db.session.query(TokenRealm.token_id)
            .filter(TokenRealm.realm_id.in_(realm_ids))
            .all()
        )
        token_ids = set()
        for token_tuple in token_id_tuples:
            token_ids.add(token_tuple[0])

        return token_ids","for realm_tuple in realm_id_tuples:
    realm_ids.add(realm_tuple[0])","for realm_tuple in realm_id_tuples:
    (realm_tuple_0, *realm_tuple_rrealm_tuplemaining) = realm_tuple
    realm_ids.add(realm_tuple[0])","(data, data, *data)","for (inputs_0, inputs_1, *inputs_len) in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(
        inputs_0)
    assert t.scale == 
    inputs_1
    t2 = Time(
    inputs_0[:
    inputs_0.index('(')], format='isot', scale=
    inputs_1)
    assert t == t2",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/lib/tokeniterator.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/tokeniterator.py,TokenIterator,_get_tokens_in_realm$359,"def _get_tokens_in_realm(self, valid_realms):
        # get all matching realms
        realm_id_tuples = (
            db.session.query(Realm.id)
            .filter(Realm.name.in_(valid_realms))
            .all()
        )
        realm_ids = set()
        for realm_tuple in realm_id_tuples:
            realm_ids.add(realm_tuple[0])
        # get all tokenrealm ids
        token_id_tuples = (
            db.session.query(TokenRealm.token_id)
            .filter(TokenRealm.realm_id.in_(realm_ids))
            .all()
        )
        token_ids = set()
        for token_tuple in token_id_tuples:
            token_ids.add(token_tuple[0])

        return token_ids","for token_tuple in token_id_tuples:
    token_ids.add(token_tuple[0])","for token_tuple in token_id_tuples:
    (token_tuple_0, *token_tuple_rtoken_tuplemaining) = token_tuple
    token_ids.add(token_tuple[0])","(data, data, *data)","for (inputs_0, inputs_1, *inputs_len) in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(
        inputs_0)
    assert t.scale == 
    inputs_1
    t2 = Time(
    inputs_0[:
    inputs_0.index('(')], format='isot', scale=
    inputs_1)
    assert t == t2",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
raven-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/raven-python/tests/handlers/logging/tests.py,https://github.com/getsentry/raven-python/tree/master/tests/handlers/logging/tests.py,LoggingIntegrationTest,test_can_record$70,"def test_can_record(self):
        tests = [
            (""raven"", False),
            (""raven.foo"", False),
            (""sentry.errors"", False),
            (""sentry.errors.foo"", False),
            (""raven_utils"", True),
        ]

        for test in tests:
            record = self.make_record(""Test"", name=test[0])
            self.assertEqual(self.handler.can_record(record), test[1])","for test in tests:
    record = self.make_record('Test', name=test[0])
    self.assertEqual(self.handler.can_record(record), test[1])","for test in tests:
    (test_0, test_1, *_) = test
    record = self.make_record('Test', name=test[0])
    self.assertEqual(self.handler.can_record(record), test[1])","(data, data, *data)","for (inputs_0, inputs_1, *inputs_len) in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(
        inputs_0)
    assert t.scale == 
    inputs_1
    t2 = Time(
    inputs_0[:
    inputs_0.index('(')], format='isot', scale=
    inputs_1)
    assert t == t2",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
hfnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hfnet/colmap-helpers/features_from_npz.py,https://github.com/ethz-asl/hfnet/tree/master/colmap-helpers/features_from_npz.py,,export_features_from_npz$17,"def export_features_from_npz(filename, in_path, out_path):
    path_file = os.path.join(in_path, filename)
    frame1 = np.load(path_file)

    filename = os.path.splitext(os.path.basename(path_file))[0]

    out_path_and_name = os.path.join(out_path, filename) + '.jpg.txt'
    outfile = open(out_path_and_name, 'w+')

    SIFT_SIZE = 128
    kp1 = frame1['keypoints']
    outfile.write(str(kp1.shape[0]) + ' ' + str(SIFT_SIZE) + '\n')

    for keypoint in kp1:
        # Generate some dummy SIFT values as we will anyway use external
        # from a matches.txt file.
        s = str(keypoint[0]) + ' ' + str(keypoint[1])
        s += ' 1 1 ' + '1 '*SIFT_SIZE + '\n'
        outfile.write(s)

    outfile.close()","for keypoint in kp1:
    s = str(keypoint[0]) + ' ' + str(keypoint[1])
    s += ' 1 1 ' + '1 ' * SIFT_SIZE + '\n'
    outfile.write(s)","for keypoint in kp1:
    (keypoint_0, keypoint_1, *_) = keypoint
    s = str(keypoint[0]) + ' ' + str(keypoint[1])
    s += ' 1 1 ' + '1 ' * SIFT_SIZE + '\n'
    outfile.write(s)","(data, data, *data)","for (inputs_0, inputs_1, *inputs_len) in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(
        inputs_0)
    assert t.scale == 
    inputs_1
    t2 = Time(
    inputs_0[:
    inputs_0.index('(')], format='isot', scale=
    inputs_1)
    assert t == t2",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
rotki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rotki/rotkehlchen/db/dbhandler.py,https://github.com/rotki/rotki/tree/master/rotkehlchen/db/dbhandler.py,DBHandler,get_manually_tracked_balances$1356,"def get_manually_tracked_balances(
            self,
            cursor: 'DBCursor',
            balance_type: Optional[BalanceType] = BalanceType.ASSET,
    ) -> List[ManuallyTrackedBalance]:
        """"""Returns the manually tracked balances from the DB""""""
        query_balance_type = ''
        if balance_type is not None:
            query_balance_type = f'WHERE A.category=""{balance_type.serialize_for_db()}""'
        query = cursor.execute(
            f'SELECT A.asset, A.label, A.amount, A.location, group_concat(B.tag_name,"",""), '
            f'A.category, A.id FROM manually_tracked_balances as A '
            f'LEFT OUTER JOIN tag_mappings as B on B.object_reference = A.id '
            f'{query_balance_type} GROUP BY label;',
        )

        data = []
        for entry in query:
            tags = deserialize_tags_from_db(entry[4])
            try:
                balance_type = BalanceType.deserialize_from_db(entry[5])
                data.append(ManuallyTrackedBalance(
                    id=entry[6],
                    asset=Asset(entry[0]).check_existence(),
                    label=entry[1],
                    amount=FVal(entry[2]),
                    location=Location.deserialize_from_db(entry[3]),
                    tags=tags,
                    balance_type=balance_type,
                ))
            except (DeserializationError, UnknownAsset, UnsupportedAsset, ValueError) as e:
                # ValueError would be due to FVal failing
                self.msg_aggregator.add_warning(
                    f'Unexpected data in a ManuallyTrackedBalance entry in the DB: {str(e)}',
                )

        return data","for entry in query:
    tags = deserialize_tags_from_db(entry[4])
    try:
        balance_type = BalanceType.deserialize_from_db(entry[5])
        data.append(ManuallyTrackedBalance(id=entry[6], asset=Asset(entry[0]).check_existence(), label=entry[1], amount=FVal(entry[2]), location=Location.deserialize_from_db(entry[3]), tags=tags, balance_type=balance_type))
    except (DeserializationError, UnknownAsset, UnsupportedAsset, ValueError) as e:
        self.msg_aggregator.add_warning(f'Unexpected data in a ManuallyTrackedBalance entry in the DB: {str(e)}')","for entry in query:
    (entry_0, entry_1, entry_2, entry_3, entry_4, entry_5, entry_6, *_) = entry
    tags = deserialize_tags_from_db(entry[4])
    try:
        balance_type = BalanceType.deserialize_from_db(entry[5])
        data.append(ManuallyTrackedBalance(id=entry[6], asset=Asset(entry[0]).check_existence(), label=entry[1], amount=FVal(entry[2]), location=Location.deserialize_from_db(entry[3]), tags=tags, balance_type=balance_type))
    except (DeserializationError, UnknownAsset, UnsupportedAsset, ValueError) as e:
        self.msg_aggregator.add_warning(f'Unexpected data in a ManuallyTrackedBalance entry in the DB: {str(e)}')","(data, data, *data)","for (inputs_0, inputs_1, *inputs_len) in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(
        inputs_0)
    assert t.scale == 
    inputs_1
    t2 = Time(
    inputs_0[:
    inputs_0.index('(')], format='isot', scale=
    inputs_1)
    assert t == t2",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4, e_5, e_6 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]
e_5: e[5]
e_6: e[6]",,,,,,,
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/obj/test_diskfile.py,https://github.com/openstack/swift/tree/master/test/unit/obj/test_diskfile.py,DiskFileMixin,_run_test$5727,"def _run_test():
            # Set up mock of `splice`
            splice_called = [False]  # State hack

            def fake_splice(fd_in, off_in, fd_out, off_out, len_, flags):
                if fd_out == devnull.fileno() and not splice_called[0]:
                    splice_called[0] = True
                    err = errno.EWOULDBLOCK
                    raise IOError(err, os.strerror(err))

                return splice(fd_in, off_in, fd_out, off_out,
                              len_, flags)

            mock_splice.side_effect = fake_splice

            # Set up mock of `trampoline`
            # There are 2 reasons to mock this:
            #
            # - We want to ensure it's called with the expected arguments at
            #   least once
            # - When called with our write FD (which points to `/dev/null`), we
            #   can't actually call `trampoline`, because adding such FD to an
            #   `epoll` handle results in `EPERM`
            def fake_trampoline(fd, read=None, write=None, timeout=None,
                                timeout_exc=timeout.Timeout,
                                mark_as_closed=None):
                if write and fd == devnull.fileno():
                    return
                else:
                    hubs.trampoline(fd, read=read, write=write,
                                    timeout=timeout, timeout_exc=timeout_exc,
                                    mark_as_closed=mark_as_closed)

            mock_trampoline.side_effect = fake_trampoline

            reader.zero_copy_send(devnull.fileno())

            # Assert the end of `zero_copy_send` was reached
            self.assertTrue(mock_close.called)
            # Assert there was at least one call to `trampoline` waiting for
            # `write` access to the output FD
            mock_trampoline.assert_any_call(devnull.fileno(), write=True)
            # Assert at least one call to `splice` with the output FD we expect
            for call in mock_splice.call_args_list:
                args = call[0]
                if args[2] == devnull.fileno():
                    break
            else:
                self.fail('`splice` not called with expected arguments')","for call in mock_splice.call_args_list:
    args = call[0]
    if args[2] == devnull.fileno():
        break
else:
    self.fail('`splice` not called with expected arguments')","for call in mock_splice.call_args_list:
    (call_0, *call_rcallmaining) = call
    args = call[0]
    if args[2] == devnull.fileno():
        break
else:
    self.fail('`splice` not called with expected arguments')","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_fr.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_fr.py,Num2WordsENTest,test_number$168,"def test_number(self):
        for test in TEST_CASES_CARDINAL:
            self.assertEqual(num2words(test[0], lang='fr'), test[1])","for test in TEST_CASES_CARDINAL:
    self.assertEqual(num2words(test[0], lang='fr'), test[1])","for test in TEST_CASES_CARDINAL:
    (test_0, test_1, *_) = test
    self.assertEqual(num2words(test[0], lang='fr'), test[1])","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pydicom,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pydicom/pydicom/tests/test_util.py,https://github.com/pydicom/pydicom/tree/master/pydicom/tests/test_util.py,TestLeanRead,test_implicit_little$388,"def test_implicit_little(self):
        p = get_testdata_file(""MR_small_implicit.dcm"")
        ds = dcmread(p)
        assert ds.file_meta.TransferSyntaxUID == ImplicitVRLittleEndian
        with dicomfile(p) as ds:
            assert ds.preamble is not None
            for elem in ds:
                if elem[0] == (0x7fe0, 0x0010):
                    assert elem[2] == 8192","for elem in ds:
    if elem[0] == (32736, 16):
        assert elem[2] == 8192","for elem in ds:
    (elem_0, _, elem_2, *elem_relemmaining) = elem
    if elem[0] == (32736, 16):
        assert elem[2] == 8192","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, _, e_2, *e_remaining = e
variable mapping:
e_0: e[0]
e_2: e[2]",,,,,,,
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/base/plugins/agent_based/jenkins_jobs.py,https://github.com/tribe29/checkmk/tree/master/cmk/base/plugins/agent_based/jenkins_jobs.py,,parse_jenkins_jobs$59,"def parse_jenkins_jobs(string_table) -> Section:
    parsed: Dict[str, JenkinsJobInfo] = {}

    for line in string_table:
        jenkins_data = json.loads(line[0])

        parsed.update(_handle_job_type(jenkins_data, {}, """"))

    return parsed","for line in string_table:
    jenkins_data = json.loads(line[0])
    parsed.update(_handle_job_type(jenkins_data, {}, ''))","for line in string_table:
    (line_0, *line_rlinemaining) = line
    jenkins_data = json.loads(line[0])
    parsed.update(_handle_job_type(jenkins_data, {}, ''))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2017/nn/part2.py,https://github.com/3b1b/videos/tree/master/_2017/nn/part2.py,EmphasizeComplexityOfCostFunction,show_network_as_a_function$1233,"def show_network_as_a_function(self):
        title = TexText(""Neural network function"")
        title.shift(FRAME_X_RADIUS*RIGHT/2)
        title.to_edge(UP)
        underline = Line(LEFT, RIGHT)
        underline.stretch_to_fit_width(title.get_width())
        underline.next_to(title, DOWN, SMALL_BUFF)
        self.add(title, underline)

        words = self.get_function_description_words(
            ""784 numbers (pixels)"",
            ""10 numbers"",
            ""13{,}002 weights/biases"",
        )
        input_words, output_words, parameter_words = words
        for word in words:
            self.add(word[0])

        in_vect = get_organized_images()[7][8]
        activations = self.network.get_activation_of_all_layers(in_vect)
        image = MNistMobject(in_vect)
        image.set_height(1.5)
        image_label = TexText(""Input"")
        image_label.set_color(input_words[0].get_color())
        image_label.next_to(image, UP, SMALL_BUFF)

        arrow = Arrow(LEFT, RIGHT, color = WHITE)
        arrow.next_to(image, RIGHT)
        output = self.num_vect_to_column_vector(activations[-1], 2)
        output.next_to(arrow, RIGHT)

        group = Group(image, image_label, arrow, output)
        group.next_to(self.network_mob, UP, 0, RIGHT)

        dot = Dot()
        dot.move_to(input_words.get_right())
        dot.set_fill(opacity = 0.5)

        self.play(FadeIn(input_words[1], lag_ratio = 0.5))
        self.play(
            dot.move_to, image,
            dot.set_fill, None, 0,
            FadeIn(image),
            FadeIn(image_label),
        )
        self.activate_network(in_vect, 
            GrowArrow(arrow),
            FadeIn(output),
            FadeIn(output_words[1])
        )
        self.wait()
        self.play(
            FadeIn(parameter_words[1]), 
            self.get_edge_animation()
        )
        self.wait(2)

        self.to_fade = group
        self.curr_words = words
        self.title = title
        self.underline = underline","for word in words:
    self.add(word[0])","for word in words:
    (word_0, *word_rwordmaining) = word
    self.add(word[0])","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
few-shot-vid2vid,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/few-shot-vid2vid/data/image_folder.py,https://github.com/NVlabs/few-shot-vid2vid/tree/master/data/image_folder.py,,make_grouped_dataset$63,"def make_grouped_dataset(dir):
    images = []
    assert os.path.isdir(dir), '%s is not a valid directory' % dir
    fnames = sorted(os.walk(dir))
    for fname in sorted(fnames):
        paths = []
        root = fname[0]
        for f in sorted(fname[2]):
            if is_image_file(f):
                paths.append(os.path.join(root, f))
        if len(paths) > 0:
            images.append(paths)
    return images","for fname in sorted(fnames):
    paths = []
    root = fname[0]
    for f in sorted(fname[2]):
        if is_image_file(f):
            paths.append(os.path.join(root, f))
    if len(paths) > 0:
        images.append(paths)","for fname in sorted(fnames):
    (fname_0, _, fname_2, *fname_rfnamemaining) = fname
    paths = []
    root = fname[0]
    for f in sorted(fname[2]):
        if is_image_file(f):
            paths.append(os.path.join(root, f))
    if len(paths) > 0:
        images.append(paths)","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, _, e_2, *e_remaining = e
variable mapping:
e_0: e[0]
e_2: e[2]",,,,,,,
wand,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wand/wand/image.py,https://github.com/emcconville/wand/tree/master/wand/image.py,BaseImage,features$4970,"def features(self, distance):
        """"""Calculate directional image features for each color channel.
        Feature metrics including:

        - angular second moment
        - contrast
        - correlation
        - variance sum of squares
        - inverse difference moment
        - sum average
        - sum variance
        - sum entropy
        - entropy
        - difference variance
        - difference entropy
        - information measures of correlation 1
        - information measures of correlation 2
        - maximum correlation coefficient

        With each metric containing horizontal, vertical, left & right
        diagonal values.

        .. code::

            from wand.image import Image

            with Image(filename='rose:') as img:
                channel_features = img.features(distance=32)
                for channels, features in channel_features.items():
                    print(channels)
                    for feature, directions in features.items():
                        print('  ', feature)
                        for name, value in directions.items():
                            print('    ', name, value)

        :param distance: Define the distance if pixels to calculate.
        :type distance: :class:`numbers.Integral`
        :returns: a dict mapping each color channel with a dict of each
                  feature.
        :rtype: :class:`dict`

        .. versionadded:: 0.5.5
        """"""
        def build_channel(address, channel):
            feature = ChannelFeature()
            size = ctypes.sizeof(feature)
            ctypes.memmove(ctypes.addressof(feature),
                           feature_ptr + (CHANNELS[channel] * size),
                           size)
            keys = ('horizontal', 'vertical',
                    'left_diagonal', 'right_diagonal')
            feature_dict = {}
            for k in feature._fields_:
                a = k[0]
                feature_dict[a] = dict(zip(keys, getattr(feature, a)))
            return feature_dict
        if MAGICK_VERSION_NUMBER < 0x700:
            method = library.MagickGetImageChannelFeatures
        else:  # pragma: no cover
            method = library.MagickGetImageFeatures
        assertions.assert_unsigned_integer(distance=distance)
        feature_ptr = method(self.wand, distance)
        response = {}
        if feature_ptr:
            colorspace = self.colorspace
            if self.alpha_channel:
                response['alpha'] = build_channel(feature_ptr, 'alpha')
            if colorspace == 'gray':
                response['gray'] = build_channel(feature_ptr, 'gray')
            elif colorspace == 'cmyk':
                response['cyan'] = build_channel(feature_ptr, 'cyan')
                response['magenta'] = build_channel(feature_ptr, 'magenta')
                response['yellow'] = build_channel(feature_ptr, 'yellow')
                response['black'] = build_channel(feature_ptr, 'black')
            else:
                response['red'] = build_channel(feature_ptr, 'red')
                response['green'] = build_channel(feature_ptr, 'green')
                response['blue'] = build_channel(feature_ptr, 'blue')
            feature_ptr = library.MagickRelinquishMemory(feature_ptr)
        return response","for k in feature._fields_:
    a = k[0]
    feature_dict[a] = dict(zip(keys, getattr(feature, a)))","for k in feature._fields_:
    (k_0, *k_rkmaining) = k
    a = k[0]
    feature_dict[a] = dict(zip(keys, getattr(feature, a)))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
ReAgent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/ope/test/yandex_web_search.py,https://github.com/facebookresearch/ReAgent/tree/master/reagent/ope/test/yandex_web_search.py,LoggedQuery,click$82,"def click(self, url_id: int, dwell_time: int):
        self._position_relevances = None
        self._url_relevances = None
        i = 0
        for r in self.list:
            if url_id == r[0]:
                self.clicks.append((i, dwell_time))
                break
            i += 1","for r in self.list:
    if url_id == r[0]:
        self.clicks.append((i, dwell_time))
        break
    i += 1","for r in self.list:
    (r_0, *r_rrmaining) = r
    if url_id == r[0]:
        self.clicks.append((i, dwell_time))
        break
    i += 1","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
anki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anki/pylib/anki/stats.py,https://github.com/ankitects/anki/tree/master/pylib/anki/stats.py,CollectionStats,hourGraph$772,"def hourGraph(self) -> str:
        data = self._hourRet()
        if not data:
            return """"
        shifted = []
        counts = []
        mcount = 0
        trend: list[tuple[int, int]] = []
        peak = 0
        for d in data:
            hour = (d[0] - 4) % 24
            pct = d[1]
            if pct > peak:
                peak = pct
            shifted.append((hour, pct))
            counts.append((hour, d[2]))
            if d[2] > mcount:
                mcount = d[2]
        shifted.sort()
        counts.sort()
        if len(counts) < 4:
            return """"
        for d in shifted:
            hour = d[0]
            pct = d[1]
            if not trend:
                trend.append((hour, pct))
            else:
                prev = trend[-1][1]
                diff = pct - prev
                diff /= 3.0
                diff = round(diff, 1)
                trend.append((hour, prev + diff))
        txt = self._title(
            ""Hourly Breakdown"", ""Review success rate for each hour of the day.""
        )
        txt += self._graph(
            id=""hour"",
            data=[
                dict(data=shifted, color=colCum, label=""% Correct""),
                dict(
                    data=counts,
                    color=colHour,
                    label=""Answers"",
                    yaxis=2,
                    bars=dict(barWidth=0.2),
                    stack=False,
                ),
            ],
            conf=dict(
                xaxis=dict(
                    ticks=[
                        [0, ""4AM""],
                        [6, ""10AM""],
                        [12, ""4PM""],
                        [18, ""10PM""],
                        [23, ""3AM""],
                    ]
                ),
                yaxes=[dict(max=peak), dict(position=""right"", max=mcount)],
            ),
            ylabel=""% Correct"",
            ylabel2=""Reviews"",
        )
        txt += ""Hours with less than 30 reviews are not shown.""
        return txt","for d in data:
    hour = (d[0] - 4) % 24
    pct = d[1]
    if pct > peak:
        peak = pct
    shifted.append((hour, pct))
    counts.append((hour, d[2]))
    if d[2] > mcount:
        mcount = d[2]","for d in data:
    (d_0, d_1, d_2, *_) = d
    hour = (d[0] - 4) % 24
    pct = d[1]
    if pct > peak:
        peak = pct
    shifted.append((hour, pct))
    counts.append((hour, d[2]))
    if d[2] > mcount:
        mcount = d[2]","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
anki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anki/pylib/anki/stats.py,https://github.com/ankitects/anki/tree/master/pylib/anki/stats.py,CollectionStats,hourGraph$772,"def hourGraph(self) -> str:
        data = self._hourRet()
        if not data:
            return """"
        shifted = []
        counts = []
        mcount = 0
        trend: list[tuple[int, int]] = []
        peak = 0
        for d in data:
            hour = (d[0] - 4) % 24
            pct = d[1]
            if pct > peak:
                peak = pct
            shifted.append((hour, pct))
            counts.append((hour, d[2]))
            if d[2] > mcount:
                mcount = d[2]
        shifted.sort()
        counts.sort()
        if len(counts) < 4:
            return """"
        for d in shifted:
            hour = d[0]
            pct = d[1]
            if not trend:
                trend.append((hour, pct))
            else:
                prev = trend[-1][1]
                diff = pct - prev
                diff /= 3.0
                diff = round(diff, 1)
                trend.append((hour, prev + diff))
        txt = self._title(
            ""Hourly Breakdown"", ""Review success rate for each hour of the day.""
        )
        txt += self._graph(
            id=""hour"",
            data=[
                dict(data=shifted, color=colCum, label=""% Correct""),
                dict(
                    data=counts,
                    color=colHour,
                    label=""Answers"",
                    yaxis=2,
                    bars=dict(barWidth=0.2),
                    stack=False,
                ),
            ],
            conf=dict(
                xaxis=dict(
                    ticks=[
                        [0, ""4AM""],
                        [6, ""10AM""],
                        [12, ""4PM""],
                        [18, ""10PM""],
                        [23, ""3AM""],
                    ]
                ),
                yaxes=[dict(max=peak), dict(position=""right"", max=mcount)],
            ),
            ylabel=""% Correct"",
            ylabel2=""Reviews"",
        )
        txt += ""Hours with less than 30 reviews are not shown.""
        return txt","for d in shifted:
    hour = d[0]
    pct = d[1]
    if not trend:
        trend.append((hour, pct))
    else:
        prev = trend[-1][1]
        diff = pct - prev
        diff /= 3.0
        diff = round(diff, 1)
        trend.append((hour, prev + diff))","for d in shifted:
    (d_0, d_1, *_) = d
    hour = d[0]
    pct = d[1]
    if not trend:
        trend.append((hour, pct))
    else:
        prev = trend[-1][1]
        diff = pct - prev
        diff /= 3.0
        diff = round(diff, 1)
        trend.append((hour, prev + diff))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
FeatureLearningRotNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FeatureLearningRotNet/dataloader.py,https://github.com/gidariss/FeatureLearningRotNet/tree/master//dataloader.py,Places205,__init__$37,"def __init__(self, root, split, transform=None, target_transform=None):
        self.root = os.path.expanduser(root)
        self.data_folder  = os.path.join(self.root, 'data', 'vision', 'torralba', 'deeplearning', 'images256')
        self.split_folder = os.path.join(self.root, 'trainvalsplit_places205')
        assert(split=='train' or split=='val')
        split_csv_file = os.path.join(self.split_folder, split+'_places205.csv')

        self.transform = transform
        self.target_transform = target_transform
        with open(split_csv_file, 'rb') as f:
            reader = csv.reader(f, delimiter=' ')
            self.img_files = []
            self.labels = []
            for row in reader:
                self.img_files.append(row[0])
                self.labels.append(long(row[1]))","for row in reader:
    self.img_files.append(row[0])
    self.labels.append(long(row[1]))","for row in reader:
    (row_0, row_1, *_) = row
    self.img_files.append(row[0])
    self.labels.append(long(row[1]))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
ottertune,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ottertune/server/analysis/simulation.py,https://github.com/cmu-db/ottertune/tree/master/server/analysis/simulation.py,,gpr_new$280,"def gpr_new(env, config, n_loops=100):
    model_name = 'BasicGP'
    model_opt_frequency = 0
    model_kwargs = {}
    model_kwargs['model_learning_rate'] = 0.001
    model_kwargs['model_maxiter'] = 5000
    opt_kwargs = {}
    opt_kwargs['learning_rate'] = 0.01
    opt_kwargs['maxiter'] = 500

    results = []
    x_axis = []
    memory = ReplayMemory()
    num_samples = config['num_samples']
    num_collections = config['num_collections']
    X_min = np.zeros(env.knob_dim)
    X_max = np.ones(env.knob_dim)
    X_bounds = [X_min, X_max]
    opt_kwargs['bounds'] = X_bounds

    for _ in range(num_collections):
        action = np.random.rand(env.knob_dim)
        reward, _ = env.simulate(action)
        memory.push(action, reward)

    for i in range(n_loops):
        X_samples = np.random.rand(num_samples, env.knob_dim)
        if i >= 5:
            actions, rewards = memory.get_all()
            tuples = tuple(zip(actions, rewards))
            top10 = heapq.nlargest(10, tuples, key=lambda e: e[1])
            for entry in top10:
                # Tensorflow get broken if we use the training data points as
                # starting points for GPRGD.
                X_samples = np.vstack((X_samples, np.array(entry[0]) * 0.97 + 0.01))

        actions, rewards = memory.get_all()

        ucb_beta = config['beta']
        opt_kwargs['ucb_beta'] = ucb.get_ucb_beta(ucb_beta, scale=config['scale'],
                                                  t=i + 1., ndim=env.knob_dim)
        if model_opt_frequency > 0:
            optimize_hyperparams = i % model_opt_frequency == 0
            if not optimize_hyperparams:
                model_kwargs['hyperparameters'] = hyperparameters
        else:
            optimize_hyperparams = False
            model_kwargs['hyperparameters'] = None
        model_kwargs['optimize_hyperparameters'] = optimize_hyperparams
        X_new, ypred, _, hyperparameters = run_optimize(np.array(actions),
                                                        -np.array(rewards),
                                                        X_samples,
                                                        model_name,
                                                        opt_kwargs,
                                                        model_kwargs)

        sort_index = np.argsort(ypred.squeeze())
        X_new = X_new[sort_index]
        ypred = ypred[sort_index].squeeze()
        action = X_new[0]
        reward, _ = env.simulate(action)
        memory.push(action, reward)
        LOG.info('loop: %d reward: %f', i, reward[0])
        results.append(reward)
        x_axis.append(i+1)

    return np.array(results), np.array(x_axis)","for entry in top10:
    X_samples = np.vstack((X_samples, np.array(entry[0]) * 0.97 + 0.01))","for entry in top10:
    (entry_0, *entry_rentrymaining) = entry
    X_samples = np.vstack((X_samples, np.array(entry[0]) * 0.97 + 0.01))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
sparrow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparrow/views/report.py,https://github.com/wylok/sparrow/tree/master/views/report.py,,alarm_report$222,"def alarm_report():
    try:
        INFOS = []
        total_key = 'op_totals_alarms_tmp'
        def counts_alarm(key,total_key = None):
            vals = []
            for i in range(7):
                count_key = 'op_counts_alarms_tmp'
                data_now = datetime.datetime.now() - datetime.timedelta(days=i)
                dd = data_now.strftime('%Y-%m-%d')
                alarm_count_key = '%s_%s' %(key,dd)
                if RC_CLUSTER.exists(alarm_count_key):
                    vals = RC_CLUSTER.hgetall(alarm_count_key)
                    vals = sorted(vals.items(), key=lambda item: int(item[1]))
                    for val in vals:
                        RC_CLUSTER.hincrby(count_key,val[0],val[1])
                        if total_key:
                            RC_CLUSTER.hincrby(total_key,dd, val[1])
            if RC_CLUSTER.exists(count_key):
                vals = RC_CLUSTER.hgetall(count_key)
                RC_CLUSTER.delete(count_key)
                vals = sorted(vals.items(), key=lambda item: int(item[1]),reverse=True)
            if len(vals) >10:
                return vals[:10]
            else:
                return vals
        alarm_count = counts_alarm('op_business_alarm_count',total_key=total_key)
        vals = counts_alarm('op_business_alarm_perf')
        if vals:
            pie_perf = Pie(""近7天报警接口性能统计TOP10"", width='100%', height='100%', title_pos='center', title_text_size=14)
            attrs = [val[0] for val in vals]
            vals = [int(val[1]) for val in vals]
            pie_perf.add("""", attrs, vals, is_label_show=True, is_toolbox_show=False,legend_orient='vertical',legend_pos='left', xaxis_interval=0, is_random=True,rosetype = 'area')
            INFOS.append(pie_perf)
        vals = counts_alarm('op_business_alarm_busi')
        if vals:
            pie_busi = Pie(""近7天报警业务归属统计TOP10"", width='100%', height='100%', title_pos='center', title_text_size=14)
            attrs = [val[0] for val in vals]
            vals = [int(val[1]) for val in vals]
            pie_busi.add("""", attrs, vals, is_label_show=True, is_toolbox_show=False,legend_orient='vertical',legend_pos='left',xaxis_interval=0, is_random=True,rosetype = 'radius',radius=[35, 75])
            INFOS.append(pie_busi)
        if RC_CLUSTER.exists(total_key):
            vals = RC_CLUSTER.hgetall(total_key)
            vals = sorted(vals.items(), key=lambda item: item[0],reverse=True)
            RC_CLUSTER.delete(total_key)
            line = Line(""近7天业务接口每日报警统计"", width='100%', height='100%', title_pos='center', title_text_size=14)
            attrs = [val[0] for val in vals]
            vals = [int(val[1]) for val in vals]
            line.add("""", attrs, vals, is_label_show=True, is_toolbox_show=False,is_legend_show  = False, xaxis_interval=0, is_random=True)
            INFOS.append(line)
    except Exception as e:
        logging.error(e)
        return redirect(url_for('error'))
    return render_template('alarm_report.html',INFOS=INFOS,alarm_count=alarm_count)","for val in vals:
    RC_CLUSTER.hincrby(count_key, val[0], val[1])
    if total_key:
        RC_CLUSTER.hincrby(total_key, dd, val[1])","for val in vals:
    (val_0, val_1, *_) = val
    RC_CLUSTER.hincrby(count_key, val[0], val[1])
    if total_key:
        RC_CLUSTER.hincrby(total_key, dd, val[1])","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
GitGutter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GitGutter/tests/test_json.py,https://github.com/jisaacks/GitGutter/tree/master/tests/test_json.py,CheckJsonFormat,get_line$111,"def get_line(self, pt):
        """"""Get the line from char index.""""""
        line = None
        for r in self.line_range:
            if pt >= r[0] and pt <= r[1]:
                line = r[2]
                break
        return line","for r in self.line_range:
    if pt >= r[0] and pt <= r[1]:
        line = r[2]
        break","for r in self.line_range:
    (r_0, r_1, r_2, *_) = r
    if pt >= r[0] and pt <= r[1]:
        line = r[2]
        break","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):

        ''' Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        '''
        obj = {'Id': item_id}

        try:
            obj['KodiId'] = e_item[0]
            obj['FileId'] = e_item[1]
            obj['ParentId'] = e_item[3]
            obj['Media'] = e_item[4]
        except TypeError:
            return

        if obj['Media'] == 'episode':

            temp_obj = dict(obj)
            self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
            season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            try:
                temp_obj['Id'] = season[0]
                temp_obj['ParentId'] = season[1]
            except TypeError:
                return

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):

                self.remove_season(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

            temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))

            if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):

                for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                    self.remove_season(season[1], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))

                self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

        elif obj['Media'] == 'tvshow':
            obj['ParentId'] = obj['KodiId']

            for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):

                temp_obj = dict(obj)
                temp_obj['ParentId'] = season[1]

                for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                    self.remove_episode(episode[1], episode[2], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            self.remove_tvshow(obj['KodiId'], obj['Id'])

        elif obj['Media'] == 'season':

            for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))

            self.remove_season(obj['KodiId'], obj['Id'])

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):

                self.remove_tvshow(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))

        # Remove any series pooling episodes
        for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
            self.remove_episode(episode[2], episode[3], obj['Id'])
        else:
            self.jellyfin_db.remove_media_by_parent_id(obj['Id'])

        self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
    self.remove_episode(episode[2], episode[3], obj['Id'])
else:
    self.jellyfin_db.remove_media_by_parent_id(obj['Id'])","for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
    (_, _, episode_2, episode_3, *_) = episode
    self.remove_episode(episode[2], episode[3], obj['Id'])
else:
    self.jellyfin_db.remove_media_by_parent_id(obj['Id'])",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, _, e_2, e_3 = e
variable mapping:
e_2: e[2]
e_3: e[3]",,,,,,,
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):

        ''' Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        '''
        obj = {'Id': item_id}

        try:
            obj['KodiId'] = e_item[0]
            obj['FileId'] = e_item[1]
            obj['ParentId'] = e_item[3]
            obj['Media'] = e_item[4]
        except TypeError:
            return

        if obj['Media'] == 'episode':

            temp_obj = dict(obj)
            self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
            season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            try:
                temp_obj['Id'] = season[0]
                temp_obj['ParentId'] = season[1]
            except TypeError:
                return

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):

                self.remove_season(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

            temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))

            if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):

                for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                    self.remove_season(season[1], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))

                self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

        elif obj['Media'] == 'tvshow':
            obj['ParentId'] = obj['KodiId']

            for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):

                temp_obj = dict(obj)
                temp_obj['ParentId'] = season[1]

                for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                    self.remove_episode(episode[1], episode[2], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            self.remove_tvshow(obj['KodiId'], obj['Id'])

        elif obj['Media'] == 'season':

            for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))

            self.remove_season(obj['KodiId'], obj['Id'])

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):

                self.remove_tvshow(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))

        # Remove any series pooling episodes
        for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
            self.remove_episode(episode[2], episode[3], obj['Id'])
        else:
            self.jellyfin_db.remove_media_by_parent_id(obj['Id'])

        self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
    self.remove_season(season[1], obj['Id'])
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))","for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
    (season_0, season_1, *season_rseasonmaining) = season
    self.remove_season(season[1], obj['Id'])
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):

        ''' Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        '''
        obj = {'Id': item_id}

        try:
            obj['KodiId'] = e_item[0]
            obj['FileId'] = e_item[1]
            obj['ParentId'] = e_item[3]
            obj['Media'] = e_item[4]
        except TypeError:
            return

        if obj['Media'] == 'episode':

            temp_obj = dict(obj)
            self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
            season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            try:
                temp_obj['Id'] = season[0]
                temp_obj['ParentId'] = season[1]
            except TypeError:
                return

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):

                self.remove_season(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

            temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))

            if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):

                for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                    self.remove_season(season[1], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))

                self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

        elif obj['Media'] == 'tvshow':
            obj['ParentId'] = obj['KodiId']

            for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):

                temp_obj = dict(obj)
                temp_obj['ParentId'] = season[1]

                for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                    self.remove_episode(episode[1], episode[2], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            self.remove_tvshow(obj['KodiId'], obj['Id'])

        elif obj['Media'] == 'season':

            for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))

            self.remove_season(obj['KodiId'], obj['Id'])

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):

                self.remove_tvshow(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))

        # Remove any series pooling episodes
        for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
            self.remove_episode(episode[2], episode[3], obj['Id'])
        else:
            self.jellyfin_db.remove_media_by_parent_id(obj['Id'])

        self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):
    temp_obj = dict(obj)
    temp_obj['ParentId'] = season[1]
    for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
        self.remove_episode(episode[1], episode[2], obj['Id'])
    else:
        self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))","for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):
    (season_0, season_1, *season_rseasonmaining) = season
    temp_obj = dict(obj)
    temp_obj['ParentId'] = season[1]
    for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
        self.remove_episode(episode[1], episode[2], obj['Id'])
    else:
        self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):

        ''' Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        '''
        obj = {'Id': item_id}

        try:
            obj['KodiId'] = e_item[0]
            obj['FileId'] = e_item[1]
            obj['ParentId'] = e_item[3]
            obj['Media'] = e_item[4]
        except TypeError:
            return

        if obj['Media'] == 'episode':

            temp_obj = dict(obj)
            self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
            season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            try:
                temp_obj['Id'] = season[0]
                temp_obj['ParentId'] = season[1]
            except TypeError:
                return

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):

                self.remove_season(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

            temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))

            if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):

                for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                    self.remove_season(season[1], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))

                self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

        elif obj['Media'] == 'tvshow':
            obj['ParentId'] = obj['KodiId']

            for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):

                temp_obj = dict(obj)
                temp_obj['ParentId'] = season[1]

                for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                    self.remove_episode(episode[1], episode[2], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            self.remove_tvshow(obj['KodiId'], obj['Id'])

        elif obj['Media'] == 'season':

            for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))

            self.remove_season(obj['KodiId'], obj['Id'])

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):

                self.remove_tvshow(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))

        # Remove any series pooling episodes
        for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
            self.remove_episode(episode[2], episode[3], obj['Id'])
        else:
            self.jellyfin_db.remove_media_by_parent_id(obj['Id'])

        self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
    self.remove_episode(episode[1], episode[2], obj['Id'])
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))","for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
    (_, episode_1, episode_2, *episode_repisodemaining) = episode
    self.remove_episode(episode[1], episode[2], obj['Id'])
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):

        ''' Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        '''
        obj = {'Id': item_id}

        try:
            obj['KodiId'] = e_item[0]
            obj['FileId'] = e_item[1]
            obj['ParentId'] = e_item[3]
            obj['Media'] = e_item[4]
        except TypeError:
            return

        if obj['Media'] == 'episode':

            temp_obj = dict(obj)
            self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
            season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            try:
                temp_obj['Id'] = season[0]
                temp_obj['ParentId'] = season[1]
            except TypeError:
                return

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):

                self.remove_season(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

            temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))

            if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):

                for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                    self.remove_season(season[1], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))

                self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

        elif obj['Media'] == 'tvshow':
            obj['ParentId'] = obj['KodiId']

            for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):

                temp_obj = dict(obj)
                temp_obj['ParentId'] = season[1]

                for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                    self.remove_episode(episode[1], episode[2], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            self.remove_tvshow(obj['KodiId'], obj['Id'])

        elif obj['Media'] == 'season':

            for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))

            self.remove_season(obj['KodiId'], obj['Id'])

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):

                self.remove_tvshow(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))

        # Remove any series pooling episodes
        for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
            self.remove_episode(episode[2], episode[3], obj['Id'])
        else:
            self.jellyfin_db.remove_media_by_parent_id(obj['Id'])

        self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
    self.remove_episode(episode[1], episode[2], obj['Id'])
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))","for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
    (_, episode_1, episode_2, *episode_repisodemaining) = episode
    self.remove_episode(episode[1], episode[2], obj['Id'])
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
geany-themes,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/geany-themes/scripts/autobump.py,https://github.com/codebrainz/geany-themes/tree/master/scripts/autobump.py,,write_log$26,"def write_log(log_file, entries):
  new_lines = []
  for ent in entries:
    new_lines.append('\t'.join((ent[1], ent[0])))
  open(log_file, 'w').write('\n'.join(new_lines) + '\n')","for ent in entries:
    new_lines.append('\t'.join((ent[1], ent[0])))","for ent in entries:
    (ent_0, ent_1, *_) = ent
    new_lines.append('\t'.join((ent[1], ent[0])))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
buku,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/buku/bukuserver/server.py,https://github.com/jarun/buku/tree/master/bukuserver/server.py,ApiBookmarkSearchView,get$532,"def get(self):
        arg_obj = request.args
        keywords = arg_obj.getlist('keywords')
        all_keywords = arg_obj.get('all_keywords')
        deep = arg_obj.get('deep')
        regex = arg_obj.get('regex')
        # api request is more strict
        all_keywords = False if all_keywords is None else all_keywords
        deep = False if deep is None else deep
        regex = False if regex is None else regex
        all_keywords = (
            all_keywords if isinstance(all_keywords, bool) else
            all_keywords.lower() == 'true'
        )
        deep = deep if isinstance(deep, bool) else deep.lower() == 'true'
        regex = regex if isinstance(regex, bool) else regex.lower() == 'true'

        result = {'bookmarks': []}
        bukudb = getattr(flask.g, 'bukudb', get_bukudb())
        found_bookmarks = bukudb.searchdb(keywords, all_keywords, deep, regex)
        found_bookmarks = [] if found_bookmarks is None else found_bookmarks
        res = None
        if found_bookmarks is not None:
            for bookmark in found_bookmarks:
                result_bookmark = {
                    'id': bookmark[0],
                    'url': bookmark[1],
                    'title': bookmark[2],
                    'tags': list(filter(lambda x: x, bookmark[3].split(','))),
                    'description': bookmark[4]
                }
                result['bookmarks'].append(result_bookmark)
        current_app.logger.debug('total bookmarks:{}'.format(len(result['bookmarks'])))
        res = jsonify(result)
        return res","for bookmark in found_bookmarks:
    result_bookmark = {'id': bookmark[0], 'url': bookmark[1], 'title': bookmark[2], 'tags': list(filter(lambda x: x, bookmark[3].split(','))), 'description': bookmark[4]}
    result['bookmarks'].append(result_bookmark)","for bookmark in found_bookmarks:
    (bookmark_0, bookmark_1, bookmark_2, bookmark_3, bookmark_4, *_) = bookmark
    result_bookmark = {'id': bookmark[0], 'url': bookmark[1], 'title': bookmark[2], 'tags': list(filter(lambda x: x, bookmark[3].split(','))), 'description': bookmark[4]}
    result['bookmarks'].append(result_bookmark)","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,
GWSL-Source,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GWSL-Source/manager.py,https://github.com/Opticos/GWSL-Source/tree/master//manager.py,,get_version$605,"def get_version(machine):
    try:
        machines = os.popen(""wsl.exe -l -v"").read()  # lines()
        machines = re.sub(r'[^a-z A-Z0-9./\n-]', r'', machines).splitlines()
        #machines = machines.splitlines()
        machines2 = []
        wsl_1 = True
        for i in machines:
            b = ''.join(i).split()
            if 'VERSION' in b:
                wsl_1 = False
            if 'NAME' not in b and b != [] and b != None:
                machines2.append(b)
        if wsl_1 == True:
            print(""assuming wsl 1"")
            return 1
        
        for i in machines2:
            if i[0] == machine:
                return int(i[2])
        return 1
        
    except:
        return 1","for i in machines2:
    if i[0] == machine:
        return int(i[2])","for i in machines2:
    (i_0, _, i_2, *i_rimaining) = i
    if i[0] == machine:
        return int(i[2])","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, _, e_2, *e_remaining = e
variable mapping:
e_0: e[0]
e_2: e[2]",,,,,,,
PornHub-downloader-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PornHub-downloader-python/functions.py,https://github.com/mariosemes/PornHub-downloader-python/tree/master//functions.py,,dl_all_new_items$173,"def dl_all_new_items(conn):
    c = conn.cursor()
    try:
        c.execute(""SELECT * FROM ph_items WHERE new='1'"")
    except Error as e:
        print(e)
        sys.exit()

    rows = c.fetchall()

    for row in rows:

        if str(row[1]) == ""model"":
            url_after = ""/videos/upload""
        # elif str(row[1]) == ""pornstar"":
        #     url_after = ""/videos""
        elif str(row[1]) == ""users"":
            url_after = ""/videos/public""
        elif str(row[1]) == ""channels"":
            url_after = ""/videos""
        else:
            url_after = """"

        print(""-----------------------------"")
        print(row[1])
        print(row[2])
        print(""https://www.pornhub.com/"" + str(row[1]) + ""/"" + str(row[2]) + url_after)
        print(""-----------------------------"")

        # Find more available options here: https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/YoutubeDL.py#L129-L279
        outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
        ydl_opts = {
            'format': 'best',
            'outtmpl': outtmpl,
            'nooverwrites': True,
            'no_warnings': False,
            'ignoreerrors': True,
        }

        url = ""https://www.pornhub.com/"" + str(row[1]) + ""/"" + str(row[2]) + url_after
        with youtube_dl.YoutubeDL(ydl_opts) as ydl:
            ydl.download([url])

        try:
            c.execute(""UPDATE ph_items SET new='0', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row[2],))
            conn.commit()
        except Error as e:
            print(e)
            sys.exit()","for row in rows:
    if str(row[1]) == 'model':
        url_after = '/videos/upload'
    elif str(row[1]) == 'users':
        url_after = '/videos/public'
    elif str(row[1]) == 'channels':
        url_after = '/videos'
    else:
        url_after = ''
    print('-----------------------------')
    print(row[1])
    print(row[2])
    print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
    print('-----------------------------')
    outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
    ydl_opts = {'format': 'best', 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
    url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after
    with youtube_dl.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])
    try:
        c.execute(""UPDATE ph_items SET new='0', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row[2],))
        conn.commit()
    except Error as e:
        print(e)
        sys.exit()","for row in rows:
    (_, row_1, row_2, row_3, *_) = row
    if str(row[1]) == 'model':
        url_after = '/videos/upload'
    elif str(row[1]) == 'users':
        url_after = '/videos/public'
    elif str(row[1]) == 'channels':
        url_after = '/videos'
    else:
        url_after = ''
    print('-----------------------------')
    print(row[1])
    print(row[2])
    print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
    print('-----------------------------')
    outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
    ydl_opts = {'format': 'best', 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
    url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after
    with youtube_dl.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])
    try:
        c.execute(""UPDATE ph_items SET new='0', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row[2],))
        conn.commit()
    except Error as e:
        print(e)
        sys.exit()","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, e_3 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
quay,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quay/test/test_ldap.py,https://github.com/quay/quay/tree/master/test/test_ldap.py,TestLDAP,test_ldap_superuser_and_restricted_user_invalid_filter$810,"def test_ldap_superuser_and_restricted_user_invalid_filter(self):
        valid_user_filter = ""(filterField=somevalue)""
        invalid_superuser_filter = ""(filterField=notsuperuser)""
        invalid_restricted_user_filter = ""(filterField=notrestricted)""

        with mock_ldap(user_filter=valid_user_filter) as ldap:
            # Verify we can login.
            (response, _) = ldap.verify_and_link_user(""someuser"", ""somepass"")
            self.assertEqual(response.username, ""someuser"")

        with mock_ldap(
            user_filter=valid_user_filter,
            superuser_filter=invalid_superuser_filter,
            restricted_user_filter=invalid_restricted_user_filter,
        ) as ldap:
            (it, err) = ldap.iterate_group_members(
                {""group_dn"": ""cn=AwesomeFolk""}, disable_pagination=True
            )
            self.assertIsNone(err)

            results = list(it)
            self.assertEqual(4, len(results))

            for u in results:
                user = u[0]

                is_superuser = ldap.is_superuser(user.username)
                is_restricted_user = ldap.is_restricted_user(user.username)
                self.assertFalse(is_superuser)
                self.assertFalse(is_restricted_user)

            self.assertFalse(ldap.has_superusers())
            self.assertFalse(ldap.has_restricted_users())","for u in results:
    user = u[0]
    is_superuser = ldap.is_superuser(user.username)
    is_restricted_user = ldap.is_restricted_user(user.username)
    self.assertFalse(is_superuser)
    self.assertFalse(is_restricted_user)","for u in results:
    (u_0, *u_rumaining) = u
    user = u[0]
    is_superuser = ldap.is_superuser(user.username)
    is_restricted_user = ldap.is_restricted_user(user.username)
    self.assertFalse(is_superuser)
    self.assertFalse(is_restricted_user)","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
dionaea,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dionaea/modules/python/dionaea/sip/extras.py,https://github.com/DinoTools/dionaea/tree/master/modules/python/dionaea/sip/extras.py,PCAP,open$329,"def open(self, msg_stack, **params):
        path = self.path.format(**params)
        today = datetime.datetime.now()
        path = today.strftime(path)
        #'%H:%M:%S_{remote_host}_{remote_port}_in.rtp'
        filename = today.strftime(self.filename)
        filename = filename.format(**params)
        # ToDo: error check
        try:
            if not os.path.exists(path):
                os.makedirs(path)
        except:
            logger.info(""Can't create RTP-Dump dir: %s"", path)

        try:
            self._fp = open(os.path.join(path, filename), ""wb"")
        except:
            logger.warning(""Can't create RTP-Dump file: %s"", os.path.join(path, filename))

        if self._fp is None:
            return False

        # write pcap global header
        self._fp.write(b""\xd4\xc3\xb2\xa1"")
        # version 2.4
        self._fp.write(b""\x02\x00\x04\x00"")
        # GMT to local correction
        self._fp.write(b""\x00\x00\x00\x00"")
        # accuracy of timestamps
        self._fp.write(b""\x00\x00\x00\x00"")
        # max length of captured packets, in octets
        self._fp.write(b""\xff\xff\x00\x00"")
        # data link type (1 = Ethernet) http://www.tcpdump.org/linktypes.html
        self._fp.write(b""\x01\x00\x00\x00"")

        for msg in msg_stack:
            t = msg[1].time
            ts = int(t)
            tm = int((t - ts) * 1000000)

            src_port = 5060
            dst_port = 5060
            if msg[0] == ""in"":
                src_ether = b""\x00\x00\x00\x00\x00\x02""
                src_host = b""\x0A\x00\x00\x02"" # 10.0.0.2
                dst_ether = b""\x00\x00\x00\x00\x00\x01""
                dst_host = b""\x0A\x00\x00\x01"" # 10.0.0.1
            else:
                src_ether = b""\x00\x00\x00\x00\x00\x01""
                src_host = b""\x0A\x00\x00\x01"" # 10.0.0.1
                dst_ether = b""\x00\x00\x00\x00\x00\x02""
                dst_host = b""\x0A\x00\x00\x02"" # 10.0.0.2

            self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=msg[1].dumps())","for msg in msg_stack:
    t = msg[1].time
    ts = int(t)
    tm = int((t - ts) * 1000000)
    src_port = 5060
    dst_port = 5060
    if msg[0] == 'in':
        src_ether = b'\x00\x00\x00\x00\x00\x02'
        src_host = b'\n\x00\x00\x02'
        dst_ether = b'\x00\x00\x00\x00\x00\x01'
        dst_host = b'\n\x00\x00\x01'
    else:
        src_ether = b'\x00\x00\x00\x00\x00\x01'
        src_host = b'\n\x00\x00\x01'
        dst_ether = b'\x00\x00\x00\x00\x00\x02'
        dst_host = b'\n\x00\x00\x02'
    self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=msg[1].dumps())","for msg in msg_stack:
    (msg_0, msg_1, *_) = msg
    t = msg[1].time
    ts = int(t)
    tm = int((t - ts) * 1000000)
    src_port = 5060
    dst_port = 5060
    if msg[0] == 'in':
        src_ether = b'\x00\x00\x00\x00\x00\x02'
        src_host = b'\n\x00\x00\x02'
        dst_ether = b'\x00\x00\x00\x00\x00\x01'
        dst_host = b'\n\x00\x00\x01'
    else:
        src_ether = b'\x00\x00\x00\x00\x00\x01'
        src_host = b'\n\x00\x00\x01'
        dst_ether = b'\x00\x00\x00\x00\x00\x02'
        dst_host = b'\n\x00\x00\x02'
    self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=msg[1].dumps())","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
virt-manager,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/virt-manager/virtinst/install/installer.py,https://github.com/virt-manager/virt-manager/tree/master/virtinst/install/installer.py,Installer,_prepare_cloudinit$371,"def _prepare_cloudinit(self, guest, meter):
        scratchdir = InstallerTreeMedia.make_scratchdir(guest)
        filepairs = cloudinit.create_files(scratchdir, self._cloudinit_data)
        for filepair in filepairs:
            self._tmpfiles.append(filepair[0])

        iso = perform_cdrom_injections(filepairs, scratchdir, cloudinit=True)
        self._tmpfiles.append(iso)
        iso = self._upload_media(guest, meter, [iso])[0]
        self._add_unattended_install_cdrom_device(guest, iso)","for filepair in filepairs:
    self._tmpfiles.append(filepair[0])","for filepair in filepairs:
    (filepair_0, *filepair_rfilepairmaining) = filepair
    self._tmpfiles.append(filepair[0])","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
takeover,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/takeover/takeover.py,https://github.com/m4ll0k/takeover/tree/master//takeover.py,,find$158,"def find(status, content, ok):
    for service in services:
        for values in services[service].items():
            if re.findall(str(values[1]), str(content), re.I) and int(status) in range(201 if ok is False else 200, 599):
                return str(service), str(values[1])","for values in services[service].items():
    if re.findall(str(values[1]), str(content), re.I) and int(status) in range(201 if ok is False else 200, 599):
        return (str(service), str(values[1]))","for values in services[service].items():
    (_, values_1, *values_rvaluesmaining) = values
    if re.findall(str(values[1]), str(content), re.I) and int(status) in range(201 if ok is False else 200, 599):
        return (str(service), str(values[1]))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
mmcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmcv/tests/test_ops/test_focal_loss.py,https://github.com/open-mmlab/mmcv/tree/master/tests/test_ops/test_focal_loss.py,Testfocalloss,_test_grad_softmax$80,"def _test_grad_softmax(self, dtype=torch.float):
        if not torch.cuda.is_available():
            return
        from mmcv.ops import SoftmaxFocalLoss
        alpha = 0.25
        gamma = 2.0
        for case in inputs:
            np_x = np.array(case[0])
            np_y = np.array(case[1])

            x = torch.from_numpy(np_x).cuda().type(dtype)
            x.requires_grad_()
            y = torch.from_numpy(np_y).cuda().long()

            floss = SoftmaxFocalLoss(gamma, alpha)
            if _USING_PARROTS:
                # gradcheck(floss, (x, y),
                #           no_grads=[y])
                pass
            else:
                gradcheck(floss, (x, y), eps=1e-2, atol=1e-2)","for case in inputs:
    np_x = np.array(case[0])
    np_y = np.array(case[1])
    x = torch.from_numpy(np_x).cuda().type(dtype)
    x.requires_grad_()
    y = torch.from_numpy(np_y).cuda().long()
    floss = SoftmaxFocalLoss(gamma, alpha)
    if _USING_PARROTS:
        pass
    else:
        gradcheck(floss, (x, y), eps=0.01, atol=0.01)","for case in inputs:
    (case_0, case_1, *_) = case
    np_x = np.array(case[0])
    np_y = np.array(case[1])
    x = torch.from_numpy(np_x).cuda().type(dtype)
    x.requires_grad_()
    y = torch.from_numpy(np_y).cuda().long()
    floss = SoftmaxFocalLoss(gamma, alpha)
    if _USING_PARROTS:
        pass
    else:
        gradcheck(floss, (x, y), eps=0.01, atol=0.01)","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/tests/providers/aws/test_default.py,https://github.com/cloudtools/stacker/tree/master/stacker/tests/providers/aws/test_default.py,TestProviderDefaultMode,test_select_update_method$512,"def test_select_update_method(self):
        for i in [[{'force_interactive': True,
                    'force_change_set': False},
                   self.provider.interactive_update_stack],
                  [{'force_interactive': False,
                    'force_change_set': False},
                   self.provider.default_update_stack],
                  [{'force_interactive': False,
                    'force_change_set': True},
                   self.provider.noninteractive_changeset_update],
                  [{'force_interactive': True,
                    'force_change_set': True},
                   self.provider.interactive_update_stack]]:
            self.assertEquals(
                self.provider.select_update_method(**i[0]),
                i[1]
            )","for i in [[{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': False}, self.provider.default_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.noninteractive_changeset_update], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
    self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","for i in [[{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': False}, self.provider.default_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.noninteractive_changeset_update], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
    (i_0, i_1, *_) = i
    self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
tartube,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tartube/tartube/config.py,https://github.com/axcore/tartube/tree/master/tartube/config.py,SystemPrefWin,setup_downloader_paths_tab$25199,"def setup_downloader_paths_tab(self, inner_notebook):

        """"""Called by self.setup_downloader_tab().

        Sets up the 'File Paths' inner notebook tab.

        Args:

            inner_notebook (Gtk.Notebook): The container for this tab

        """"""

        tab, grid = self.add_inner_notebook_tab(
            _('File _paths'),
            inner_notebook,
        )
        grid_width = 3

        # Downloader file paths
        self.add_label(grid,
            '<u>' + _('Downloader file paths') + '</u>',
            0, 0, grid_width, 1,
        )

        # youtube-dl file paths
        self.add_label(grid,
            _('Path to the executable'),
            0, 1, 1, 1,
        )

        combo_list = [
            [
                _('Use default path') + ' (' + self.app_obj.ytdl_path_default \
                + ')',
                self.app_obj.ytdl_path_default,
            ],
        ]

        if os.name != 'nt':

            combo_list.append(
                [
                    _('Use local path') + ' (' + self.app_obj.ytdl_bin + ')',
                    self.app_obj.ytdl_bin,
                ],
            )

        if os.name == 'nt':
            msg = _('Use custom path (not recommended on MS Windows)')
        else:
            msg = _('Use custom path')
        combo_list.append(
            [
                msg,
                None,       # Set by the callback
            ],
        )

        if os.name != 'nt':

            combo_list.append(
                [
                    _('Use PyPI path') + ' (' + self.app_obj.ytdl_path_pypi \
                    + ')',
                    self.app_obj.ytdl_path_pypi,
                ],
            )

        self.path_liststore = Gtk.ListStore(str, str)
        for mini_list in combo_list:
            self.path_liststore.append( [ mini_list[0], mini_list[1] ] )

        self.filepaths_combo = Gtk.ComboBox.new_with_model(self.path_liststore)
        grid.attach(self.filepaths_combo, 1, 1, (grid_width - 1), 1)
        renderer_text = Gtk.CellRendererText()
        self.filepaths_combo.pack_start(renderer_text, True)
        self.filepaths_combo.add_attribute(renderer_text, 'text', 0)
        self.filepaths_combo.set_entry_text_column(0)
        # (Signal connect appears below)

        entry = self.add_entry(grid,
            None,
            False,
            1, 2, 1, 1,
        )

        button = Gtk.Button(_('Set'))
        grid.attach(button, 2, 2, 1, 1)
        # (Signal connect appears below)

        # Set up those widgets
        if os.name == 'nt':

            if self.app_obj.ytdl_path_custom_flag:
                self.filepaths_combo.set_active(1)
            else:
                self.filepaths_combo.set_active(0)

        else:

            if self.app_obj.ytdl_path_custom_flag:
                self.filepaths_combo.set_active(2)
            elif self.app_obj.ytdl_path == self.app_obj.ytdl_path_default:
                self.filepaths_combo.set_active(0)
            elif self.app_obj.ytdl_path == self.app_obj.ytdl_path_pypi:
                self.filepaths_combo.set_active(3)
            else:
                self.filepaths_combo.set_active(1)

        if self.app_obj.ytdl_path_custom_flag:

            # (If this window is loaded due to
            #   mainapp.TartubeApp.debug_open_pref_win_flag, this value will be
            #   None)
            if self.app_obj.ytdl_path:
                entry.set_text(self.app_obj.ytdl_path)

        else:
            button.set_sensitive(False)

        # Now set up the next combo
        self.add_label(grid,
            _('Command for update operations'),
            0, 3, 1, 1,
        )

        self.cmd_liststore = Gtk.ListStore(str, str)
        for item in self.app_obj.ytdl_update_list:
            self.cmd_liststore.append( [item, formats.YTDL_UPDATE_DICT[item]] )

        combo2 = Gtk.ComboBox.new_with_model(self.cmd_liststore)
        grid.attach(combo2, 1, 3, (grid_width - 1), 1)

        renderer_text = Gtk.CellRendererText()
        combo2.pack_start(renderer_text, True)
        combo2.add_attribute(renderer_text, 'text', 1)
        combo2.set_entry_text_column(1)

        combo2.set_active(
            self.app_obj.ytdl_update_list.index(
                self.app_obj.ytdl_update_current,
            ),
        )
        if __main__.__pkg_strict_install_flag__:
            combo2.set_sensitive(False)
        # (Signal connect appears below)

        # Update the combos, so that the youtube-dl fork, rather than
        #   youtube-dl itself, is visible (if applicable)
        self.update_ytdl_combos()

        # (Signal connects from above)
        self.filepaths_combo.connect(
            'changed',
            self.on_ytdl_path_combo_changed,
            entry,
            button,
        )
        button.connect('clicked', self.on_ytdl_path_button_clicked, entry)
        combo2.connect('changed', self.on_update_combo_changed)","for mini_list in combo_list:
    self.path_liststore.append([mini_list[0], mini_list[1]])","for mini_list in combo_list:
    (mini_list_0, mini_list_1, *_) = mini_list
    self.path_liststore.append([mini_list[0], mini_list[1]])","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
coveragepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coveragepy/coverage/sqldata.py,https://github.com/nedbat/coveragepy/tree/master/coverage/sqldata.py,CoverageData,_read_db$269,"def _read_db(self):
        """"""Read the metadata from a database so that we are ready to use it.""""""
        with self._dbs[threading.get_ident()] as db:
            try:
                schema_version, = db.execute_one(""select version from coverage_schema"")
            except Exception as exc:
                if ""no such table: coverage_schema"" in str(exc):
                    self._init_db(db)
                else:
                    raise DataError(
                        ""Data file {!r} doesn't seem to be a coverage data file: {}"".format(
                            self._filename, exc
                        )
                    ) from exc
            else:
                if schema_version != SCHEMA_VERSION:
                    raise DataError(
                        ""Couldn't use data file {!r}: wrong schema: {} instead of {}"".format(
                            self._filename, schema_version, SCHEMA_VERSION
                        )
                    )

            with db.execute(""select value from meta where key = 'has_arcs'"") as cur:
                for row in cur:
                    self._has_arcs = bool(int(row[0]))
                    self._has_lines = not self._has_arcs

            with db.execute(""select id, path from file"") as cur:
                for file_id, path in cur:
                    self._file_map[path] = file_id","for row in cur:
    self._has_arcs = bool(int(row[0]))
    self._has_lines = not self._has_arcs","for row in cur:
    (row_0, *row_rrowmaining) = row
    self._has_arcs = bool(int(row[0]))
    self._has_lines = not self._has_arcs","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/tests/python/magic_uv_test/common.py,https://github.com/nutti/Magic-UV/tree/master/tests/python/magic_uv_test/common.py,TestBase,tearDownClass$264,"def tearDownClass(cls):
        try:
            check_addon_disabled(cls.package_name)
            for op in cls.idname:
                if op[0] == 'OPERATOR':
                    assert not operator_exists(op[1]), ""Operator {} exists"".format(op[1])
                elif op[0] == 'MENU':
                    assert not menu_exists(op[1]), ""Menu %s exists"".format(op[1])
        except AssertionError as e:
            print(e)
            sys.exit(1)","for op in cls.idname:
    if op[0] == 'OPERATOR':
        assert not operator_exists(op[1]), 'Operator {} exists'.format(op[1])
    elif op[0] == 'MENU':
        assert not menu_exists(op[1]), 'Menu %s exists'.format(op[1])","for op in cls.idname:
    (op_0, op_1, *_) = op
    if op[0] == 'OPERATOR':
        assert not operator_exists(op[1]), 'Operator {} exists'.format(op[1])
    elif op[0] == 'MENU':
        assert not menu_exists(op[1]), 'Menu %s exists'.format(op[1])","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
playwright-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/playwright-python/playwright/_impl/_wait_helper.py,https://github.com/microsoft/playwright-python/tree/master/playwright/_impl/_wait_helper.py,WaitHelper,_cleanup$89,"def _cleanup(self) -> None:
        for task in self._pending_tasks:
            if not task.done():
                task.cancel()
        for listener in self._registered_listeners:
            listener[0].remove_listener(listener[1], listener[2])","for listener in self._registered_listeners:
    listener[0].remove_listener(listener[1], listener[2])","for listener in self._registered_listeners:
    (listener_0, listener_1, listener_2, *_) = listener
    listener[0].remove_listener(listener[1], listener[2])","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
peregrine,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/peregrine/peregrinearb/tests/test_bellmannx.py,https://github.com/wardbradt/peregrine/tree/master/peregrinearb/tests/test_bellmannx.py,,build_graph_from_edge_list$51,"def build_graph_from_edge_list(edges, fee):
    graph = nx.DiGraph()
    for edge in edges:
        sell = edge[4] == 'SELL'
        graph.add_edge(
            edge[0], edge[1], weight=-math.log(edge[2] * (1 - fee)), depth=-math.log(edge[3]), trade_type=edge[4],
            fee=fee, no_fee_rate=edge[2] if sell else 1 / edge[2],
            market_name='{}/{}'.format(edge[0], edge[1]) if sell else '{}/{}'.format(edge[1], edge[0])
        )

    return graph","for edge in edges:
    sell = edge[4] == 'SELL'
    graph.add_edge(edge[0], edge[1], weight=-math.log(edge[2] * (1 - fee)), depth=-math.log(edge[3]), trade_type=edge[4], fee=fee, no_fee_rate=edge[2] if sell else 1 / edge[2], market_name='{}/{}'.format(edge[0], edge[1]) if sell else '{}/{}'.format(edge[1], edge[0]))","for edge in edges:
    (edge_0, edge_1, edge_2, edge_3, edge_4, *_) = edge
    sell = edge[4] == 'SELL'
    graph.add_edge(edge[0], edge[1], weight=-math.log(edge[2] * (1 - fee)), depth=-math.log(edge[3]), trade_type=edge[4], fee=fee, no_fee_rate=edge[2] if sell else 1 / edge[2], market_name='{}/{}'.format(edge[0], edge[1]) if sell else '{}/{}'.format(edge[1], edge[0]))","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,
airflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/api_connexion/endpoints/role_and_permission_endpoint.py,https://github.com/apache/airflow/tree/master/airflow/api_connexion/endpoints/role_and_permission_endpoint.py,,_check_action_and_resource$37,"def _check_action_and_resource(sm, perms):
    """"""
    Checks if the action or resource exists and raise 400 if not

    This function is intended for use in the REST API because it raise 400
    """"""
    for item in perms:
        if not sm.get_action(item[0]):
            raise BadRequest(detail=f""The specified action: '{item[0]}' was not found"")
        if not sm.get_resource(item[1]):
            raise BadRequest(detail=f""The specified resource: '{item[1]}' was not found"")","for item in perms:
    if not sm.get_action(item[0]):
        raise BadRequest(detail=f""The specified action: '{item[0]}' was not found"")
    if not sm.get_resource(item[1]):
        raise BadRequest(detail=f""The specified resource: '{item[1]}' was not found"")","for item in perms:
    (item_0, item_1, *_) = item
    if not sm.get_action(item[0]):
        raise BadRequest(detail=f""The specified action: '{item[0]}' was not found"")
    if not sm.get_resource(item[1]):
        raise BadRequest(detail=f""The specified resource: '{item[1]}' was not found"")","(data, data, data, data, data, *data)","for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:
    virtualpath = '\\'.join([folder, 
    file_data_1])
    size = 
    file_data_2
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, 
    file_data_4)
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
sparrow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparrow/views/report.py,https://github.com/wylok/sparrow/tree/master/views/report.py,,work_order_report$287,"def work_order_report(start_time=None,end_time=None,source='all_order'):
    INFOS = []
    db_sso = db_op.user_sso
    db_work_order = db_op.work_order
    dm_key = 'op_work_order_report_dm'
    stat_key = 'op_work_order_report_status'
    dep_key = 'op_work_order_report_department'
    if not start_time or not end_time:
        tm = datetime.datetime.now() - datetime.timedelta(days=7)
        start_time = tm.strftime('%Y-%m-%d')
        end_time = time.strftime('%Y-%m-%d',time.localtime())
    try:
        infos = db_sso.query.with_entities(db_sso.dingunionid, db_sso.department,db_sso.realName).all()
        departments = {info[0]: info[1] for info in infos}
        users = {info[0]: info[-1] for info in infos}
        #统计运维工单状态
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.status,func.count(db_work_order.status)).filter(and_(
                db_work_order.source == source,db_work_order.date >=start_time,db_work_order.date<=end_time)).group_by(db_work_order.status).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.status,func.count(db_work_order.status)).filter(and_(
                    db_work_order.date >=start_time,db_work_order.date<=end_time)).group_by(db_work_order.status).all()
            pie = Pie(""运维工单状态统计"", width='100%', height='100%', title_pos='center', title_text_size=14)
            pie_vals = [val[0] for val in vals]
            pie_counts = [int(val[1]) for val in vals]
            pie.add("""", pie_vals, pie_counts, is_label_show=True, is_toolbox_show=False, legend_orient='vertical',
                           legend_pos='right',radius=[1, 65], is_random=True)
            INFOS.append(pie)
        except Exception as e:
            logging.error(e)
        #统计月度工单数量及受理率
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.date,db_work_order.status).filter(and_(
                db_work_order.source == source,db_work_order.date >=start_time,db_work_order.date<=end_time)).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.date, db_work_order.status).filter(and_(
                    db_work_order.date >= start_time,db_work_order.date <= end_time)).all()
            if vals:
                for val in vals:
                    dm,status = val
                    dm = dm.split('-')[1]
                    RC.hincrby(dm_key,dm)
                    if status not in ('未受理', '未审核'):
                        RC.hincrby(stat_key,dm)
            line = Line(""月度工单数量及受理率统计"", width='100%', height='100%', title_pos='center',title_text_size=14)
            total_vals = RC.hgetall(dm_key)
            vals = sorted(total_vals.items(), key=lambda item: int(item[0]))
            dm_vals = [val[0] for val in vals]
            dm_counts = [int(val[1]) for val in vals]
            line.add('工单数量', dm_vals, dm_counts, is_label_show=True, is_toolbox_show=False,
                         legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            stat_vals = RC.hgetall(stat_key)
            stat_counts = [round((float(stat_vals[val])/float(total_vals[val]))*100,1) for val in stat_vals]
            line.add('受理率', dm_vals, stat_counts, is_label_show=True, is_toolbox_show=False,
                         legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            RC.delete(stat_key)
            RC.delete(dm_key)
            INFOS.append(line)
        except Exception as e:
            logging.error(e)
        #工单申请数量部门排名
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.applicant).filter(and_(
                db_work_order.source == source,db_work_order.date >=start_time,db_work_order.date<=end_time)).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.applicant).filter(and_(
                db_work_order.date >=start_time,db_work_order.date<=end_time)).all()
            if vals:
                for val in vals:
                    RC.hincrby(dep_key,departments[val[0]])
            bar = Bar(""部门提交工单统计"", width='100%', height='100%', title_pos='center', title_text_size=14)
            vals = RC.hgetall(dep_key)
            dep_vals = [val for val in vals]
            dep_counts = [int(vals[val]) for val in vals]
            bar.add('', dep_vals, dep_counts, is_label_show=True, is_toolbox_show=False,
                         legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            RC.delete(dep_key)
            INFOS.append(bar)
        except Exception as e:
            logging.error(e)
        #工单申请数量个人排名
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.applicant,func.count(db_work_order.applicant)).filter(and_(
                db_work_order.source == source,db_work_order.date >=start_time,db_work_order.date<=end_time)).group_by(
                db_work_order.applicant).order_by(desc(func.count(db_work_order.applicant))).limit(15).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.applicant,
                                                         func.count(db_work_order.applicant)).filter(and_(
                    db_work_order.date >= start_time,db_work_order.date <= end_time)).group_by(
                    db_work_order.applicant).order_by(desc(func.count(db_work_order.applicant))).limit(15).all()
            vals = [list(val) for val in vals]
            for val in vals:
                val[0] = users[val[0]]
            bar = Bar(""个人提交工单统计"", width='100%', height='100%', title_pos='center', title_text_size=14)
            dep_vals = [val[0] for val in vals]
            dep_counts = [int(val[1]) for val in vals]
            bar.add('', dep_vals, dep_counts, is_label_show=True, is_toolbox_show=False,
                    legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            INFOS.append(bar)
        except Exception as e:
            logging.error(e)
    except Exception as e:
        logging.error(e)
    return render_template('work_order_report.html',INFOS=INFOS,tt=(start_time,end_time))","for val in vals:
    val[0] = users[val[0]]","for val in vals:
    (val_0, *val_rvalmaining) = val
    val[0] = users[val[0]]",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
OpenSfM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenSfM/opensfm/large/tools.py,https://github.com/mapillary/OpenSfM/tree/master/opensfm/large/tools.py,,add_cluster_neighbors$43,"def add_cluster_neighbors(
    positions, labels, centers, max_distance
) -> List[List[np.ndarray]]:
    reflla = np.mean(positions, 0)
    reference = geo.TopocentricConverter(reflla[0], reflla[1], 0)

    topocentrics = []
    for position in positions:
        x, y, z = reference.to_topocentric(position[0], position[1], 0)
        topocentrics.append([x, y])

    topocentrics = np.array(topocentrics)
    topo_tree = spatial.cKDTree(topocentrics)

    clusters = []
    for label in np.arange(centers.shape[0]):
        cluster_indices = np.where(labels == label)[0]

        neighbors = []
        for i in cluster_indices:
            neighbors.extend(topo_tree.query_ball_point(topocentrics[i], max_distance))

        cluster = list(np.union1d(cluster_indices, neighbors))
        clusters.append(cluster)

    return clusters","for position in positions:
    (x, y, z) = reference.to_topocentric(position[0], position[1], 0)
    topocentrics.append([x, y])","for position in positions:
    (position_0, position_1, *_) = position
    (x, y, z) = reference.to_topocentric(position[0], position[1], 0)
    topocentrics.append([x, y])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
mochi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mochi/mochi/core/translation.py,https://github.com/i2y/mochi/tree/master/mochi/core/translation.py,Translator,translate_with_old$1029,"def translate_with_old(self, exp):
        keyword_with, items, *body = exp
        pre = []
        first_with_py = None
        with_py = None
        for item in items:
            item_pre, item_value = self.translate(item[0], False)
            pre.extend(item_pre)
            var = item[1]
            if with_py is None:
                with_py = ast.With(context_expr=item_value,
                                   optional_vars=ast.Name(id=var.name,
                                                          ctx=ast.Store(),
                                                          lineno=var.lineno,
                                                          col_offset=0),
                                   lineno=var.lineno,
                                   col_offset=0)
                first_with_py = with_py
            else:
                inner_with_py = ast.With(context_expr=item_value,
                                         optional_vars=ast.Name(id=var.name,
                                                                ctx=ast.Store(),
                                                                lineno=var.lineno,
                                                                col_offset=0),
                                         lineno=var.lineno,
                                         col_offset=0)
                with_py.body = [inner_with_py]
                with_py = inner_with_py

        with_py.body = self._translate_sequence(body, True)
        pre.append(first_with_py)
        return pre, self.translate(NONE_SYM, False)[1]","for item in items:
    (item_pre, item_value) = self.translate(item[0], False)
    pre.extend(item_pre)
    var = item[1]
    if with_py is None:
        with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
        first_with_py = with_py
    else:
        inner_with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
        with_py.body = [inner_with_py]
        with_py = inner_with_py","for item in items:
    (item_0, item_1, *_) = item
    (item_pre, item_value) = self.translate(item[0], False)
    pre.extend(item_pre)
    var = item[1]
    if with_py is None:
        with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
        first_with_py = with_py
    else:
        inner_with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
        with_py.body = [inner_with_py]
        with_py = inner_with_py","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
kazoo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kazoo/kazoo/tests/test_utils.py,https://github.com/python-zk/kazoo/tree/master/kazoo/tests/test_utils.py,TestCreateTCPConnection,test_timeout_arg_eventlet$32,"def test_timeout_arg_eventlet(self):
        if not EVENTLET_HANDLER_AVAILABLE:
            pytest.skip('eventlet handler not available.')

        from kazoo.handlers import utils
        from kazoo.handlers.utils import create_tcp_connection, time

        with patch.object(socket, 'create_connection') as create_connection:
            with patch.object(utils, '_set_default_tcpsock_options'):
                # Ensure a gap between calls to time.time() does not result in
                # create_connection being called with a negative timeout
                # argument.
                with patch.object(time, 'time', side_effect=range(10)):
                    create_tcp_connection(socket, ('127.0.0.1', 2181),
                                          timeout=1.5)

                for call_args in create_connection.call_args_list:
                    timeout = call_args[0][1]
                    assert timeout >= 0, 'socket timeout must be nonnegative'","for call_args in create_connection.call_args_list:
    timeout = call_args[0][1]
    assert timeout >= 0, 'socket timeout must be nonnegative'","for call_args in create_connection.call_args_list:
    ((call_args_0_0, call_args_0_1, *call_args_0_rcall_argsmaining), *call_args_rcall_argsmaining) = call_args
    timeout = call_args[0][1]
    assert timeout >= 0, 'socket timeout must be nonnegative'","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: (e_0_0, e_0_1, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_1: e[0][1]",,,,,,,
MultiQC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MultiQC/multiqc/modules/bbmap/plot_bhist.py,https://github.com/ewels/MultiQC/tree/master/multiqc/modules/bbmap/plot_bhist.py,,plot_bhist$6,"def plot_bhist(samples, file_type, **plot_args):
    """"""Create line graph plot of histogram data for BBMap 'bhist' output.

    The 'samples' parameter could be from the bbmap mod_data dictionary:
    samples = bbmap.MultiqcModule.mod_data[file_type]
    """"""

    all_x = set()
    for item in sorted(chain(*[samples[sample][""data""].items() for sample in samples])):
        all_x.add(item[0])

    columns_to_plot = {
        ""GC"": {
            1: ""C"",
            2: ""G"",
        },
        ""AT"": {
            0: ""A"",
            3: ""T"",
        },
        ""N"": {4: ""N""},
    }
    nucleotide_data = []
    for column_type in columns_to_plot:
        nucleotide_data.append(
            {
                sample
                + "".""
                + column_name: {
                    x: samples[sample][""data""][x][column] * 100 if x in samples[sample][""data""] else 0 for x in all_x
                }
                for sample in samples
                for column, column_name in columns_to_plot[column_type].items()
            }
        )

    plot_params = {
        ""id"": ""bbmap-"" + file_type + ""_plot"",
        ""title"": ""BBTools: "" + plot_args[""plot_title""],
        ""xlab"": ""Read position"",
        ""ylab"": ""Percentage of G+C bases"",
        ""ymin"": 0,
        ""ymax"": 100,
        ""data_labels"": [
            {""name"": ""Percentage of G+C bases""},
            {""name"": ""Percentage of A+T bases""},
            {""name"": ""Percentage of N bases""},
        ],
    }
    plot_params.update(plot_args[""plot_params""])
    plot = linegraph.plot(nucleotide_data, plot_params)

    return plot","for item in sorted(chain(*[samples[sample]['data'].items() for sample in samples])):
    all_x.add(item[0])","for item in sorted(chain(*[samples[sample]['data'].items() for sample in samples])):
    (item_0, *item_ritemmaining) = item
    all_x.add(item[0])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
mssql-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mssql-cli/mssqlcli/mssqlcompleter.py,https://github.com/dbcli/mssql-cli/tree/master/mssqlcli/mssqlcompleter.py,MssqlCompleter,list_dict$632,"def list_dict(pairs):  # Turns [(a, b), (a, c)] into {a: [b, c]}
            d = defaultdict(list)
            for pair in pairs:
                d[pair[0]].append(pair[1])
            return d","for pair in pairs:
    d[pair[0]].append(pair[1])","for pair in pairs:
    (_, pair_1, *pair_rpairmaining) = pair
    d[pair[0]].append(pair[1])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_imperative_se_resnext.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_imperative_se_resnext.py,TestImperativeResneXt,reader_decorator$326,"def reader_decorator(self, reader):
        def _reader_imple():
            for item in reader():
                doc = np.array(item[0]).reshape(3, 224, 224)
                label = np.array(item[1]).astype('int64').reshape(1)
                yield doc, label

        return _reader_imple","for item in reader():
    doc = np.array(item[0]).reshape(3, 224, 224)
    label = np.array(item[1]).astype('int64').reshape(1)
    yield (doc, label)","for item in reader():
    (item_0, item_1, *_) = item
    doc = np.array(item[0]).reshape(3, 224, 224)
    label = np.array(item[1]).astype('int64').reshape(1)
    yield (doc, label)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for file in files:
    if file[-4:] == '.wav':
        sound_files[ExtractAudioID(file)] = os.path.join(root, file)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg_4, e_neg_3, e_neg_2, e_neg_1 = e[-4:]
variable mapping:
e[-4:]: e[-4:]",,,,,,,
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for file in files:
    if file[-4:] == '.trs':
        XMLRefine(os.path.join(root, file), os.path.join(target_dir, 'temp', file))
        annotation_files[file] = os.path.join(target_dir, 'temp', file)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg_4, e_neg_3, e_neg_2, e_neg_1 = e[-4:]
variable mapping:
e[-4:]: e[-4:]",,,,,,,
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for segment in segment_info:
    if segment[0] == 'None':
        spk = spk_id
        spk_id += 1
    else:
        spk = temp_speaker_id[segment[0]]
    segment_id = '%s_%s_%s' % (PackZero(spk), audio_name, PackZero(segment_number))
    skip = False
    for seg in segment:
        if len(seg) < 1:
            print('warning segment %s in %s' % (segment_id, audio_name))
            skip = True
    if skip:
        continue
    print('%s %s %s %s' % (segment_id, audio_name, segment[2], segment[3]), file=segments)
    print('%s %s' % (segment_id, PackZero(spk)), file=utt2spk)
    print('%s %s' % (segment_id, segment[1]), file=text)
    spk2utt_prep[spk] = spk2utt_prep.get(spk, '') + ' %s' % segment_id
    segment_number += 1","for segment in segment_info:
    (segment_0, segment_1, segment_2, segment_3, *_) = segment
    if segment[0] == 'None':
        spk = spk_id
        spk_id += 1
    else:
        spk = temp_speaker_id[segment[0]]
    segment_id = '%s_%s_%s' % (PackZero(spk), audio_name, PackZero(segment_number))
    skip = False
    for seg in segment:
        if len(seg) < 1:
            print('warning segment %s in %s' % (segment_id, audio_name))
            skip = True
    if skip:
        continue
    print('%s %s %s %s' % (segment_id, audio_name, segment[2], segment[3]), file=segments)
    print('%s %s' % (segment_id, PackZero(spk)), file=utt2spk)
    print('%s %s' % (segment_id, segment[1]), file=text)
    spk2utt_prep[spk] = spk2utt_prep.get(spk, '') + ' %s' % segment_id
    segment_number += 1",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for file in files:
    if file[-4:] == '.wav':
        sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)","for file in files:
    (*file_rfilemaining, file_nfileg_4, file_nfileg_3, file_nfileg_2, file_nfileg_1) = file
    if file[-4:] == '.wav':
        sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg_4, e_neg_3, e_neg_2, e_neg_1 = e
variable mapping:
e[-4:]: e[-4:] (This unpacked element can be directly assigned to a variable using slicing)
",,,,,,,
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for file in files:
    if file[-4:] == '.eaf':
        annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg_4, e_neg_3, e_neg_2, e_neg_1 = e[-4:]
variable mapping:
e[-4:]: e[-4:]",,,,,,,
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for segment in left_channel_segments:
    segment_id = '%s_%s-L_%s' % (spk_info[0], afile, PackZero(segment_number))
    if float(segment[1]) > max_length:
        continue
    print('%s %s-L %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
    print('%s %s' % (segment_id, spk_info[0]), file=utt2spk)
    print('%s %s' % (segment_id, segment[2]), file=text)
    spk2utt_prep[spk_info[0]] = spk2utt_prep.get(spk_info[0], '') + ' %s' % segment_id
    segment_number += 1","for segment in left_channel_segments:
    (segment_0, segment_1, segment_2, *_) = segment
    segment_id = '%s_%s-L_%s' % (spk_info[0], afile, PackZero(segment_number))
    if float(segment[1]) > max_length:
        continue
    print('%s %s-L %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
    print('%s %s' % (segment_id, spk_info[0]), file=utt2spk)
    print('%s %s' % (segment_id, segment[2]), file=text)
    spk2utt_prep[spk_info[0]] = spk2utt_prep.get(spk_info[0], '') + ' %s' % segment_id
    segment_number += 1","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for segment in right_channel_segments:
    segment_id = '%s_%s-R_%s' % (spk_info[1], afile, PackZero(segment_number))
    if float(segment[1]) > max_length:
        continue
    print('%s %s-R %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
    print('%s %s' % (segment_id, spk_info[1]), file=utt2spk)
    print('%s %s' % (segment_id, segment[2]), file=text)
    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(spk_info[1], '') + ' %s' % segment_id
    segment_number += 1","for segment in right_channel_segments:
    (segment_0, segment_1, segment_2, *_) = segment
    segment_id = '%s_%s-R_%s' % (spk_info[1], afile, PackZero(segment_number))
    if float(segment[1]) > max_length:
        continue
    print('%s %s-R %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
    print('%s %s' % (segment_id, spk_info[1]), file=utt2spk)
    print('%s %s' % (segment_id, segment[2]), file=text)
    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(spk_info[1], '') + ' %s' % segment_id
    segment_number += 1","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
SMARTS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/zoo/evaluation/metrics/evaluation_report.py,https://github.com/huawei-noah/SMARTS/tree/master/zoo/evaluation/metrics/evaluation_report.py,EvaluationReport,__init__$12,"def __init__(self, scenarios_list, agents_list, csv_file_result_path):
        self.scenarios_list = scenarios_list
        self.csv_file_result_path = csv_file_result_path
        self.group_agents_list = []
        for value in agents_list.items():
            for agent in value[1]:
                self.group_agents_list.append(value[0] + "":"" + agent)
        self.agents_list = [agent.split("":"")[-1] for agent in self.group_agents_list]
        self.group_list = [agent.split("":"")[0] for agent in self.group_agents_list]","for value in agents_list.items():
    for agent in value[1]:
        self.group_agents_list.append(value[0] + ':' + agent)","for value in agents_list.items():
    (value_0, value_1, *_) = value
    for agent in value[1]:
        self.group_agents_list.append(value[0] + ':' + agent)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,,_reduce_terms$138,"def _reduce_terms(terms, stabilizer_list, manual_input, fixed_positions):
    """"""
    Perform the term reduction using stabilizer conditions.

    Auxiliary function to reduce_number_of_terms.

    Args:
        terms (QubitOperator): Operator the number of terms is to be reduced.
        stabilizer_list (list): List of the stabilizers as QubitOperators.
        manual_input (Boolean): Option to pass the list of fixed qubits
                                positions manually. Set to False by default.
        fixed_positions (list): (optional) List of fixed qubit positions.
                                Passing a list is only effective if
                                manual_input is True.
    Returns:
        even_newer_terms (QubitOperator): Updated operator with reduced terms.
        fixed_positions (list): Positions of qubits to be used for the
                                term reduction.
    Raises:
        StabilizerError: Trivial stabilizer (identity).
        StabilizerError: Stabilizer with complex coefficient.
    """"""
    # Initialize fixed_position as an empty list to avoid conflict with
    # fixed_positions.
    if manual_input is False:
        fixed_positions = []

    # We need the index of the stabilizer to connect it to the fixed qubit.
    for i, _ in enumerate(stabilizer_list):
        selected_stab = list(stabilizer_list[0].terms)[0]

        if manual_input is False:
            # Find first position non-fixed position with non-trivial Pauli.
            for qubit_pauli in selected_stab:
                if qubit_pauli[0] not in fixed_positions:
                    fixed_positions += [qubit_pauli[0]]
                    fixed_op = qubit_pauli[1]
                    break

        else:
            # Finds Pauli of the fixed qubit.
            for qubit_pauli in selected_stab:
                if qubit_pauli[0] == fixed_positions[i]:
                    fixed_op = qubit_pauli[1]
                    break

        if fixed_op in ['X', 'Z']:
            other_op = 'Y'
        else:
            other_op = 'X'

        new_terms = QubitOperator()
        for qubit_pauli in terms:
            new_terms += fix_single_term(qubit_pauli, fixed_positions[i],
                                         fixed_op, other_op, stabilizer_list[0])
        updated_stabilizers = []
        for update_stab in stabilizer_list[1:]:
            updated_stabilizers += [
                fix_single_term(update_stab, fixed_positions[i], fixed_op,
                                other_op, stabilizer_list[0])
            ]

        # Update terms and stabilizer list.
        terms = new_terms
        stabilizer_list = updated_stabilizers

        check_stabilizer_linearity(stabilizer_list,
                                   msg='Linearly dependent stabilizers.')
        check_commuting_stabilizers(stabilizer_list,
                                    msg='Stabilizers anti-commute.')

    return terms, fixed_positions","for qubit_pauli in selected_stab:
    if qubit_pauli[0] not in fixed_positions:
        fixed_positions += [qubit_pauli[0]]
        fixed_op = qubit_pauli[1]
        break","for qubit_pauli in selected_stab:
    (qubit_pauli_0, qubit_pauli_1, *_) = qubit_pauli
    if qubit_pauli[0] not in fixed_positions:
        fixed_positions += [qubit_pauli[0]]
        fixed_op = qubit_pauli[1]
        break","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,,_reduce_terms$138,"def _reduce_terms(terms, stabilizer_list, manual_input, fixed_positions):
    """"""
    Perform the term reduction using stabilizer conditions.

    Auxiliary function to reduce_number_of_terms.

    Args:
        terms (QubitOperator): Operator the number of terms is to be reduced.
        stabilizer_list (list): List of the stabilizers as QubitOperators.
        manual_input (Boolean): Option to pass the list of fixed qubits
                                positions manually. Set to False by default.
        fixed_positions (list): (optional) List of fixed qubit positions.
                                Passing a list is only effective if
                                manual_input is True.
    Returns:
        even_newer_terms (QubitOperator): Updated operator with reduced terms.
        fixed_positions (list): Positions of qubits to be used for the
                                term reduction.
    Raises:
        StabilizerError: Trivial stabilizer (identity).
        StabilizerError: Stabilizer with complex coefficient.
    """"""
    # Initialize fixed_position as an empty list to avoid conflict with
    # fixed_positions.
    if manual_input is False:
        fixed_positions = []

    # We need the index of the stabilizer to connect it to the fixed qubit.
    for i, _ in enumerate(stabilizer_list):
        selected_stab = list(stabilizer_list[0].terms)[0]

        if manual_input is False:
            # Find first position non-fixed position with non-trivial Pauli.
            for qubit_pauli in selected_stab:
                if qubit_pauli[0] not in fixed_positions:
                    fixed_positions += [qubit_pauli[0]]
                    fixed_op = qubit_pauli[1]
                    break

        else:
            # Finds Pauli of the fixed qubit.
            for qubit_pauli in selected_stab:
                if qubit_pauli[0] == fixed_positions[i]:
                    fixed_op = qubit_pauli[1]
                    break

        if fixed_op in ['X', 'Z']:
            other_op = 'Y'
        else:
            other_op = 'X'

        new_terms = QubitOperator()
        for qubit_pauli in terms:
            new_terms += fix_single_term(qubit_pauli, fixed_positions[i],
                                         fixed_op, other_op, stabilizer_list[0])
        updated_stabilizers = []
        for update_stab in stabilizer_list[1:]:
            updated_stabilizers += [
                fix_single_term(update_stab, fixed_positions[i], fixed_op,
                                other_op, stabilizer_list[0])
            ]

        # Update terms and stabilizer list.
        terms = new_terms
        stabilizer_list = updated_stabilizers

        check_stabilizer_linearity(stabilizer_list,
                                   msg='Linearly dependent stabilizers.')
        check_commuting_stabilizers(stabilizer_list,
                                    msg='Stabilizers anti-commute.')

    return terms, fixed_positions","for qubit_pauli in selected_stab:
    if qubit_pauli[0] == fixed_positions[i]:
        fixed_op = qubit_pauli[1]
        break","for qubit_pauli in selected_stab:
    (qubit_pauli_0, qubit_pauli_1, *_) = qubit_pauli
    if qubit_pauli[0] == fixed_positions[i]:
        fixed_op = qubit_pauli[1]
        break","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/FuzzParam.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/FuzzParam.py,FuzzerParam,result_method_1$355,"def result_method_1(self,str):
        result_key_list = []
        result_value_list = []
        regxs_1 = r'method\:.*?\,url\:.*?\,data\:({.*?})'
        if re.findall(regxs_1, str, re.S):
            result_json = re.findall(regxs_1, str, re.S)[0].replace("" "", """").replace(""\n"", """").replace(""{"", """").replace(""\t"",                                                                                                         """")
            regx_key = r'(.*?)\:.*?\,|(.*?)\:.*?\}'
            result_keys = re.findall(regx_key, result_json, re.S)
            for result_key in result_keys:
                if result_key[0] != """":
                    result_key_list.append(result_key[0])
                if result_key[1] != """":
                    result_key_list.append(result_key[1])
            regx_value = r'\:(.*?)\,|\:(.*?)\}'
            result_values = re.findall(regx_value, result_json, re.S)
            for para in result_values:
                if para[0] != '':
                    result_value_list.append(para[0])
                if para[1] != '':
                    result_value_list.append(para[1])

        # regxs_2 = r""""
        # if re.findall(regxs_2,str,re.S):
        #      result_json =

        result_list = [result_key_list,result_value_list]
        return result_list","for result_key in result_keys:
    if result_key[0] != '':
        result_key_list.append(result_key[0])
    if result_key[1] != '':
        result_key_list.append(result_key[1])","for result_key in result_keys:
    (result_key_0, result_key_1, *_) = result_key
    if result_key[0] != '':
        result_key_list.append(result_key[0])
    if result_key[1] != '':
        result_key_list.append(result_key[1])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/FuzzParam.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/FuzzParam.py,FuzzerParam,result_method_1$355,"def result_method_1(self,str):
        result_key_list = []
        result_value_list = []
        regxs_1 = r'method\:.*?\,url\:.*?\,data\:({.*?})'
        if re.findall(regxs_1, str, re.S):
            result_json = re.findall(regxs_1, str, re.S)[0].replace("" "", """").replace(""\n"", """").replace(""{"", """").replace(""\t"",                                                                                                         """")
            regx_key = r'(.*?)\:.*?\,|(.*?)\:.*?\}'
            result_keys = re.findall(regx_key, result_json, re.S)
            for result_key in result_keys:
                if result_key[0] != """":
                    result_key_list.append(result_key[0])
                if result_key[1] != """":
                    result_key_list.append(result_key[1])
            regx_value = r'\:(.*?)\,|\:(.*?)\}'
            result_values = re.findall(regx_value, result_json, re.S)
            for para in result_values:
                if para[0] != '':
                    result_value_list.append(para[0])
                if para[1] != '':
                    result_value_list.append(para[1])

        # regxs_2 = r""""
        # if re.findall(regxs_2,str,re.S):
        #      result_json =

        result_list = [result_key_list,result_value_list]
        return result_list","for para in result_values:
    if para[0] != '':
        result_value_list.append(para[0])
    if para[1] != '':
        result_value_list.append(para[1])","for para in result_values:
    (para_0, para_1, *_) = para
    if para[0] != '':
        result_value_list.append(para[0])
    if para[1] != '':
        result_value_list.append(para[1])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
DeepRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRec/utils/load_data/load_data_ranking.py,https://github.com/cheungdaven/DeepRec/tree/master/utils/load_data/load_data_ranking.py,,load_data_neg$64,"def load_data_neg(path=""../data/ml100k/movielens_100k.dat"", header=['user_id', 'item_id', 'rating', 'category'],
                  test_size=0.2, sep=""\t""):
    df = pd.read_csv(path, sep=sep, names=header, engine='python')

    n_users = df.user_id.unique().shape[0]
    n_items = df.item_id.unique().shape[0]

    train_data, test_data = train_test_split(df, test_size=test_size)
    train_data = pd.DataFrame(train_data)
    test_data = pd.DataFrame(test_data)

    train_row = []
    train_col = []
    train_rating = []

    for line in train_data.itertuples():
        u = line[1] - 1
        i = line[2] - 1
        train_row.append(u)
        train_col.append(i)
        train_rating.append(1)
    train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))

    # all_items = set(np.arange(n_items))
    # neg_items = {}
    # for u in range(n_users):
    #     neg_items[u] = list(all_items - set(train_matrix.getrow(u).nonzero()[1]))

    test_row = []
    test_col = []
    test_rating = []
    for line in test_data.itertuples():
        test_row.append(line[1] - 1)
        test_col.append(line[2] - 1)
        test_rating.append(1)
    test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))

    test_dict = {}
    for u in range(n_users):
        test_dict[u] = test_matrix.getrow(u).nonzero()[1]

    print(""Load data finished. Number of users:"", n_users, ""Number of items:"", n_items)
    return train_matrix.todok(), test_dict, n_users, n_items","for line in train_data.itertuples():
    u = line[1] - 1
    i = line[2] - 1
    train_row.append(u)
    train_col.append(i)
    train_rating.append(1)","for line in train_data.itertuples():
    (_, line_1, line_2, *line_rlinemaining) = line
    u = line[1] - 1
    i = line[2] - 1
    train_row.append(u)
    train_col.append(i)
    train_rating.append(1)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
DeepRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRec/utils/load_data/load_data_ranking.py,https://github.com/cheungdaven/DeepRec/tree/master/utils/load_data/load_data_ranking.py,,load_data_neg$64,"def load_data_neg(path=""../data/ml100k/movielens_100k.dat"", header=['user_id', 'item_id', 'rating', 'category'],
                  test_size=0.2, sep=""\t""):
    df = pd.read_csv(path, sep=sep, names=header, engine='python')

    n_users = df.user_id.unique().shape[0]
    n_items = df.item_id.unique().shape[0]

    train_data, test_data = train_test_split(df, test_size=test_size)
    train_data = pd.DataFrame(train_data)
    test_data = pd.DataFrame(test_data)

    train_row = []
    train_col = []
    train_rating = []

    for line in train_data.itertuples():
        u = line[1] - 1
        i = line[2] - 1
        train_row.append(u)
        train_col.append(i)
        train_rating.append(1)
    train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))

    # all_items = set(np.arange(n_items))
    # neg_items = {}
    # for u in range(n_users):
    #     neg_items[u] = list(all_items - set(train_matrix.getrow(u).nonzero()[1]))

    test_row = []
    test_col = []
    test_rating = []
    for line in test_data.itertuples():
        test_row.append(line[1] - 1)
        test_col.append(line[2] - 1)
        test_rating.append(1)
    test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))

    test_dict = {}
    for u in range(n_users):
        test_dict[u] = test_matrix.getrow(u).nonzero()[1]

    print(""Load data finished. Number of users:"", n_users, ""Number of items:"", n_items)
    return train_matrix.todok(), test_dict, n_users, n_items","for line in test_data.itertuples():
    test_row.append(line[1] - 1)
    test_col.append(line[2] - 1)
    test_rating.append(1)","for line in test_data.itertuples():
    (_, line_1, line_2, *line_rlinemaining) = line
    test_row.append(line[1] - 1)
    test_col.append(line[2] - 1)
    test_rating.append(1)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
HanLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HanLP/hanlp/components/srl/span_rank/srl_eval_utils.py,https://github.com/hankcs/HanLP/tree/master/hanlp/components/srl/span_rank/srl_eval_utils.py,,compute_srl_f1$162,"def compute_srl_f1(sentences, gold_srl, predictions, gold_path=None) -> SRLScores:
    assert len(gold_srl) == len(predictions)
    total_gold = 0
    total_predicted = 0
    total_matched = 0
    total_unlabeled_matched = 0
    num_sents = 0
    label_confusions = Counter()

    # Compute unofficial F1 of SRL relations.
    for gold, prediction in zip(gold_srl, predictions):
        gold_rels = 0
        pred_rels = 0
        matched = 0
        for pred_id, gold_args in gold.items():
            filtered_gold_args = [a for a in gold_args if a[2] not in [""V"", ""C-V""]]
            total_gold += len(filtered_gold_args)
            gold_rels += len(filtered_gold_args)
            if pred_id not in prediction:
                continue
            for a0 in filtered_gold_args:
                for a1 in prediction[pred_id]:
                    if a0[0] == a1[0] and a0[1] == a1[1]:
                        total_unlabeled_matched += 1
                        label_confusions.update([(a0[2], a1[2]), ])
                        if a0[2] == a1[2]:
                            total_matched += 1
                            matched += 1
        for pred_id, args in prediction.items():
            filtered_args = [a for a in args if a[2] not in [""V""]]  # ""C-V""]]
            total_predicted += len(filtered_args)
            pred_rels += len(filtered_args)

        if gold_rels == matched and pred_rels == matched:
            num_sents += 1

    precision, recall, f1 = _calc_f1(total_gold, total_predicted, total_matched,
                                     # ""SRL (unofficial)""
                                     )
    unlabeled_precision, unlabeled_recall, unlabeled_f1 = _calc_f1(total_gold, total_predicted,
                                                                   total_unlabeled_matched,
                                                                   # ""Unlabeled SRL (unofficial)""
                                                                   )

    # Prepare to compute official F1.
    if not gold_path:
        # print(""No gold conll_eval data provided. Recreating ..."")
        gold_path = tempfile.NamedTemporaryFile().name
        print_to_conll(sentences, gold_srl, gold_path, None)
        gold_predicates = None
    else:
        gold_predicates = read_gold_predicates(gold_path)

    temp_output = tempfile.NamedTemporaryFile().name
    # print((""Output temp outoput {}"".format(temp_output)))
    print_to_conll(sentences, predictions, temp_output, gold_predicates)

    # Evaluate twice with official script.
    conll_precision, conll_recall, conll_f1 = official_conll_05_evaluate(temp_output, gold_path)
    return SRLScores(unlabeled_precision, unlabeled_recall, unlabeled_f1, precision, recall, f1, conll_precision,
                     conll_recall, conll_f1, label_confusions, num_sents)","for (gold, prediction) in zip(gold_srl, predictions):
    gold_rels = 0
    pred_rels = 0
    matched = 0
    for (pred_id, gold_args) in gold.items():
        filtered_gold_args = [a for a in gold_args if a[2] not in ['V', 'C-V']]
        total_gold += len(filtered_gold_args)
        gold_rels += len(filtered_gold_args)
        if pred_id not in prediction:
            continue
        for a0 in filtered_gold_args:
            for a1 in prediction[pred_id]:
                if a0[0] == a1[0] and a0[1] == a1[1]:
                    total_unlabeled_matched += 1
                    label_confusions.update([(a0[2], a1[2])])
                    if a0[2] == a1[2]:
                        total_matched += 1
                        matched += 1
    for (pred_id, args) in prediction.items():
        filtered_args = [a for a in args if a[2] not in ['V']]
        total_predicted += len(filtered_args)
        pred_rels += len(filtered_args)
    if gold_rels == matched and pred_rels == matched:
        num_sents += 1",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: It depends on the type of ""e"" and ""pred_id"".
Iterable Unpacking: If ""e"" is a sequence type like a list or tuple and ""pred_id"" is an integer type constant that is within the range of indices of ""e"", then the answer is Yes. The Python code to unpack ""e"" to get unpacked elements and the corresponding variable mapping for each unpacked element is as follows:
e[pred_id]: e[pred_id]
variable mapping:
e[pred_id]: e[pred_id]

Otherwise, if ""e"" is not a sequence type or ""pred_id"" is not an integer type constant or it is out of range of indices of ""e"", then the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
HanLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HanLP/hanlp/components/srl/span_rank/srl_eval_utils.py,https://github.com/hankcs/HanLP/tree/master/hanlp/components/srl/span_rank/srl_eval_utils.py,,compute_srl_f1$162,"def compute_srl_f1(sentences, gold_srl, predictions, gold_path=None) -> SRLScores:
    assert len(gold_srl) == len(predictions)
    total_gold = 0
    total_predicted = 0
    total_matched = 0
    total_unlabeled_matched = 0
    num_sents = 0
    label_confusions = Counter()

    # Compute unofficial F1 of SRL relations.
    for gold, prediction in zip(gold_srl, predictions):
        gold_rels = 0
        pred_rels = 0
        matched = 0
        for pred_id, gold_args in gold.items():
            filtered_gold_args = [a for a in gold_args if a[2] not in [""V"", ""C-V""]]
            total_gold += len(filtered_gold_args)
            gold_rels += len(filtered_gold_args)
            if pred_id not in prediction:
                continue
            for a0 in filtered_gold_args:
                for a1 in prediction[pred_id]:
                    if a0[0] == a1[0] and a0[1] == a1[1]:
                        total_unlabeled_matched += 1
                        label_confusions.update([(a0[2], a1[2]), ])
                        if a0[2] == a1[2]:
                            total_matched += 1
                            matched += 1
        for pred_id, args in prediction.items():
            filtered_args = [a for a in args if a[2] not in [""V""]]  # ""C-V""]]
            total_predicted += len(filtered_args)
            pred_rels += len(filtered_args)

        if gold_rels == matched and pred_rels == matched:
            num_sents += 1

    precision, recall, f1 = _calc_f1(total_gold, total_predicted, total_matched,
                                     # ""SRL (unofficial)""
                                     )
    unlabeled_precision, unlabeled_recall, unlabeled_f1 = _calc_f1(total_gold, total_predicted,
                                                                   total_unlabeled_matched,
                                                                   # ""Unlabeled SRL (unofficial)""
                                                                   )

    # Prepare to compute official F1.
    if not gold_path:
        # print(""No gold conll_eval data provided. Recreating ..."")
        gold_path = tempfile.NamedTemporaryFile().name
        print_to_conll(sentences, gold_srl, gold_path, None)
        gold_predicates = None
    else:
        gold_predicates = read_gold_predicates(gold_path)

    temp_output = tempfile.NamedTemporaryFile().name
    # print((""Output temp outoput {}"".format(temp_output)))
    print_to_conll(sentences, predictions, temp_output, gold_predicates)

    # Evaluate twice with official script.
    conll_precision, conll_recall, conll_f1 = official_conll_05_evaluate(temp_output, gold_path)
    return SRLScores(unlabeled_precision, unlabeled_recall, unlabeled_f1, precision, recall, f1, conll_precision,
                     conll_recall, conll_f1, label_confusions, num_sents)","for a0 in filtered_gold_args:
    for a1 in prediction[pred_id]:
        if a0[0] == a1[0] and a0[1] == a1[1]:
            total_unlabeled_matched += 1
            label_confusions.update([(a0[2], a1[2])])
            if a0[2] == a1[2]:
                total_matched += 1
                matched += 1","for a0 in filtered_gold_args:
    (a0_0, a0_1, a0_2, *_) = a0
    for a1 in prediction[pred_id]:
        if a0[0] == a1[0] and a0[1] == a1[1]:
            total_unlabeled_matched += 1
            label_confusions.update([(a0[2], a1[2])])
            if a0[2] == a1[2]:
                total_matched += 1
                matched += 1","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
HanLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HanLP/hanlp/components/srl/span_rank/srl_eval_utils.py,https://github.com/hankcs/HanLP/tree/master/hanlp/components/srl/span_rank/srl_eval_utils.py,,compute_srl_f1$162,"def compute_srl_f1(sentences, gold_srl, predictions, gold_path=None) -> SRLScores:
    assert len(gold_srl) == len(predictions)
    total_gold = 0
    total_predicted = 0
    total_matched = 0
    total_unlabeled_matched = 0
    num_sents = 0
    label_confusions = Counter()

    # Compute unofficial F1 of SRL relations.
    for gold, prediction in zip(gold_srl, predictions):
        gold_rels = 0
        pred_rels = 0
        matched = 0
        for pred_id, gold_args in gold.items():
            filtered_gold_args = [a for a in gold_args if a[2] not in [""V"", ""C-V""]]
            total_gold += len(filtered_gold_args)
            gold_rels += len(filtered_gold_args)
            if pred_id not in prediction:
                continue
            for a0 in filtered_gold_args:
                for a1 in prediction[pred_id]:
                    if a0[0] == a1[0] and a0[1] == a1[1]:
                        total_unlabeled_matched += 1
                        label_confusions.update([(a0[2], a1[2]), ])
                        if a0[2] == a1[2]:
                            total_matched += 1
                            matched += 1
        for pred_id, args in prediction.items():
            filtered_args = [a for a in args if a[2] not in [""V""]]  # ""C-V""]]
            total_predicted += len(filtered_args)
            pred_rels += len(filtered_args)

        if gold_rels == matched and pred_rels == matched:
            num_sents += 1

    precision, recall, f1 = _calc_f1(total_gold, total_predicted, total_matched,
                                     # ""SRL (unofficial)""
                                     )
    unlabeled_precision, unlabeled_recall, unlabeled_f1 = _calc_f1(total_gold, total_predicted,
                                                                   total_unlabeled_matched,
                                                                   # ""Unlabeled SRL (unofficial)""
                                                                   )

    # Prepare to compute official F1.
    if not gold_path:
        # print(""No gold conll_eval data provided. Recreating ..."")
        gold_path = tempfile.NamedTemporaryFile().name
        print_to_conll(sentences, gold_srl, gold_path, None)
        gold_predicates = None
    else:
        gold_predicates = read_gold_predicates(gold_path)

    temp_output = tempfile.NamedTemporaryFile().name
    # print((""Output temp outoput {}"".format(temp_output)))
    print_to_conll(sentences, predictions, temp_output, gold_predicates)

    # Evaluate twice with official script.
    conll_precision, conll_recall, conll_f1 = official_conll_05_evaluate(temp_output, gold_path)
    return SRLScores(unlabeled_precision, unlabeled_recall, unlabeled_f1, precision, recall, f1, conll_precision,
                     conll_recall, conll_f1, label_confusions, num_sents)","for a1 in prediction[pred_id]:
    if a0[0] == a1[0] and a0[1] == a1[1]:
        total_unlabeled_matched += 1
        label_confusions.update([(a0[2], a1[2])])
        if a0[2] == a1[2]:
            total_matched += 1
            matched += 1","for a1 in prediction[pred_id]:
    (a1_0, a1_1, a1_2, *_) = a1
    if a0[0] == a1[0] and a0[1] == a1[1]:
        total_unlabeled_matched += 1
        label_confusions.update([(a0[2], a1[2])])
        if a0[2] == a1[2]:
            total_matched += 1
            matched += 1","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
BBTz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BBTz/gctexposer.py,https://github.com/m4ll0k/BBTz/tree/master//gctexposer.py,,GCTExposer$39,"def GCTExposer(domain,more):
    domains,next_ = contentParser(getContent(domain).content)
    for i in domains:
        if args.moreinfo:
            print(""{_} -> {__}"".format(_=i[1],__=i[2]))
        else:
            print(i[1])
    range_ = next_[-1]
    for i in range(range_ - 1):
        domains,next_= contentParser(getNextContent(next_[1]))
        next_ = next_ 
        for ii in domains:
            if args.moreinfo:
                print(""{_} -> {__}"".format(_=ii[1],__=ii[2]))
            else:
                print(ii[1])","for i in domains:
    if args.moreinfo:
        print('{_} -> {__}'.format(_=i[1], __=i[2]))
    else:
        print(i[1])","for i in domains:
    (_, i_1, i_2, *i_rimaining) = i
    if args.moreinfo:
        print('{_} -> {__}'.format(_=i[1], __=i[2]))
    else:
        print(i[1])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
BBTz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BBTz/gctexposer.py,https://github.com/m4ll0k/BBTz/tree/master//gctexposer.py,,GCTExposer$39,"def GCTExposer(domain,more):
    domains,next_ = contentParser(getContent(domain).content)
    for i in domains:
        if args.moreinfo:
            print(""{_} -> {__}"".format(_=i[1],__=i[2]))
        else:
            print(i[1])
    range_ = next_[-1]
    for i in range(range_ - 1):
        domains,next_= contentParser(getNextContent(next_[1]))
        next_ = next_ 
        for ii in domains:
            if args.moreinfo:
                print(""{_} -> {__}"".format(_=ii[1],__=ii[2]))
            else:
                print(ii[1])","for ii in domains:
    if args.moreinfo:
        print('{_} -> {__}'.format(_=ii[1], __=ii[2]))
    else:
        print(ii[1])","for ii in domains:
    (_, ii_1, ii_2, *ii_riimaining) = ii
    if args.moreinfo:
        print('{_} -> {__}'.format(_=ii[1], __=ii[2]))
    else:
        print(ii[1])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,
build-webos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/build-webos/scripts/webos-iot-scripts/set_webosiot_layer.py,https://github.com/webosose/build-webos/tree/master/scripts/webos-iot-scripts/set_webosiot_layer.py,,make_webosiot_layers_as_rule$9,"def make_webosiot_layers_as_rule(path):
    sys.path.insert(0,path)
    if not os.path.isfile(os.path.join(path,'weboslayers.py')):
        raise Exception(""Error: Configuration file %s does not exist!"" % os.path.join(path,'weboslayers.py'))

    from webosiot_rule import webosiot_layer_rules
    from weboslayers import webos_layers

    maxpriority = webos_layers[-1][1]

    for rule in webosiot_layer_rules:
        action = rule[0]
        if action == 'remove':
            del webos_layers[layer_index(webos_layers, rule[1])]
        elif action == 'insert':
            webos_layers.insert(layer_index(webos_layers, rule[1]) + 1, rule[2])
        elif action == 'append':
            appendlayer = list(rule[1])
            appendlayer[1] = maxpriority + 1
            webos_layers.append(tuple(appendlayer))
            maxpriority += 1

    return webos_layers","for rule in webosiot_layer_rules:
    action = rule[0]
    if action == 'remove':
        del webos_layers[layer_index(webos_layers, rule[1])]
    elif action == 'insert':
        webos_layers.insert(layer_index(webos_layers, rule[1]) + 1, rule[2])
    elif action == 'append':
        appendlayer = list(rule[1])
        appendlayer[1] = maxpriority + 1
        webos_layers.append(tuple(appendlayer))
        maxpriority += 1","for rule in webosiot_layer_rules:
    (rule_0, rule_1, rule_2, *_) = rule
    action = rule[0]
    if action == 'remove':
        del webos_layers[layer_index(webos_layers, rule[1])]
    elif action == 'insert':
        webos_layers.insert(layer_index(webos_layers, rule[1]) + 1, rule[2])
    elif action == 'append':
        appendlayer = list(rule[1])
        appendlayer[1] = maxpriority + 1
        webos_layers.append(tuple(appendlayer))
        maxpriority += 1","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
Octolapse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Octolapse/octoprint_octolapse/snapshot.py,https://github.com/FormerLurker/Octolapse/tree/master/octoprint_octolapse/snapshot.py,CaptureSnapshot,take_snapshots$111,"def take_snapshots(self, metadata={}, no_wait=False):
        logger.info(""Starting snapshot acquisition"")
        start_time = time()

        before_snapshot_threads = []
        snapshot_threads = []
        after_snapshot_threads = []
        results = []

        for current_camera in self.Cameras:
            camera_info = self.CameraInfos[""{}"".format(current_camera.guid)]

            # pre_snapshot threads
            if current_camera.on_before_snapshot_script:
                before_snapshot_job_info = SnapshotJobInfo(
                    self.TimelapseJobInfo,
                    self.temporary_directory,
                    camera_info.snapshot_attempt,
                    current_camera,
                    'before-snapshot',
                    metadata=metadata
                )
                thread = ExternalScriptSnapshotJob(before_snapshot_job_info, 'before-snapshot')
                thread.daemon = True
                before_snapshot_threads.append(
                    thread
                )

            snapshot_job_info = SnapshotJobInfo(
                self.TimelapseJobInfo,
                self.temporary_directory,
                camera_info.snapshot_attempt,
                current_camera,
                'snapshot',
                metadata=metadata
            )
            if current_camera.camera_type == ""script"":
                thread = ExternalScriptSnapshotJob(
                    snapshot_job_info,
                    'snapshot',
                    on_new_thumbnail_available_callback=self.OnNewThumbnailAvailableCallback,
                    on_post_processing_error_callback=self.on_post_processing_error_callback
                )
                thread.daemon = True
                snapshot_threads.append((thread, snapshot_job_info, None))
            elif current_camera.camera_type == ""webcam"":
                download_started_event = Event()
                thread = WebcamSnapshotJob(
                    snapshot_job_info,
                    download_started_event=download_started_event,
                    on_new_thumbnail_available_callback=self.OnNewThumbnailAvailableCallback,
                    on_post_processing_error_callback=self.on_post_processing_error_callback
                )
                thread.daemon = True
                snapshot_threads.append((thread, snapshot_job_info, download_started_event))

            # post_snapshot threads
            if current_camera.on_after_snapshot_script:
                after_snapshot_job_info = SnapshotJobInfo(
                    self.TimelapseJobInfo,
                    self.temporary_directory,
                    camera_info.snapshot_attempt,
                    current_camera,
                    'after-snapshot',
                    metadata=metadata
                )
                thread = ExternalScriptSnapshotJob(after_snapshot_job_info, 'after-snapshot')
                thread.daemon = True
                after_snapshot_threads.append(
                    thread
                )

        # Now that the before snapshot threads are prepared, send any before snapshot gcodes
        for current_camera in self.Cameras:
            if current_camera.on_before_snapshot_gcode:
                on_before_snapshot_gcode = Commands.string_to_gcode_array(current_camera.on_before_snapshot_gcode)
                if len(on_before_snapshot_gcode) > 0:
                    logger.info(""Sending on_before_snapshot_gcode for the %s camera."", current_camera.name)
                    self.SendGcodeArrayCallback(
                        on_before_snapshot_gcode,
                        current_camera.timeout_ms / 1000.0,
                        wait_for_completion=not no_wait,
                        tags={'before-snapshot-gcode'}
                    )

        if len(before_snapshot_threads) > 0:
            logger.info(""Starting %d before snapshot threads"", len(before_snapshot_threads))

        # start the pre-snapshot threads
        for t in before_snapshot_threads:
            t.start()

        # join the pre-snapshot threads
        for t in before_snapshot_threads:
            if not no_wait:
                snapshot_job_info = t.join()
                assert (isinstance(snapshot_job_info, SnapshotJobInfo))
                if t.snapshot_thread_error:
                    snapshot_job_info.success = False
                    snapshot_job_info.error = t.snapshot_thread_error
                else:
                    snapshot_job_info.success = True
            else:
                snapshot_job_info.success = True
            results.append(snapshot_job_info)

        if len(before_snapshot_threads) > 0:
            logger.info(""Before snapshot threads finished."")

        if len(snapshot_threads) > 0:
            logger.info(""Starting %d snapshot threads."", len(snapshot_threads))
        # start the snapshot threads, then wait for all threads to signal before continuing
        for t in snapshot_threads:
            t[0].start()

        # now send any gcode for gcode cameras
        for current_camera in self.Cameras:
            if current_camera.camera_type == ""gcode"":
                script_sent = False
                if current_camera.gcode_camera_script:
                    gcode_camera_script = Commands.string_to_gcode_array(current_camera.gcode_camera_script)
                    if len(gcode_camera_script) > 0:
                        logger.info(""Sending snapshot gcode array to %s."", current_camera.name)
                        # just send the gcode now so it all goes in order
                        self.SendGcodeArrayCallback(
                            Commands.string_to_gcode_array(current_camera.gcode_camera_script),
                            current_camera.timeout_ms/1000.0,
                            wait_for_completion=not no_wait
                        )
                        script_sent = True
                    if not script_sent:
                        logger.warning(""The gcode camera '%s' is enabled, but failed to produce any snapshot gcode."", current_camera.name)

        for t, snapshot_job_info, event in snapshot_threads:
            if not no_wait:
                if event:
                    event.wait()
                else:
                    snapshot_job_info = t.join()
                if t.snapshot_thread_error:
                    snapshot_job_info.success = False
                    snapshot_job_info.error = t.snapshot_thread_error
                elif t.post_processing_error:
                    snapshot_job_info.success = False
                    snapshot_job_info.error = t.post_processing_error
                else:
                    snapshot_job_info.success = True
            else:
                snapshot_job_info.success = True

            info = self.CameraInfos[snapshot_job_info.camera_guid]
            info.snapshot_attempt += 1
            if snapshot_job_info.success:
                info.snapshot_count += 1
                self.SnapshotsTotal += 1
            else:
                info.errors_count += 1
                self.ErrorsTotal += 1
            info.save(self.temporary_directory, self.TimelapseJobInfo.JobGuid, snapshot_job_info.camera_guid)
            results.append(snapshot_job_info)

        if len(snapshot_threads) > 0:
            logger.info(""Snapshot threads complete, but may be post-processing."")

        if len(after_snapshot_threads) > 0:
            logger.info(""Starting %d after snapshot threads."", len(after_snapshot_threads))

        # Now that the after snapshot threads are prepared, send any after snapshot gcodes
        for current_camera in self.Cameras:
            if current_camera.on_after_snapshot_gcode:
                on_after_snapshot_gcode = Commands.string_to_gcode_array(current_camera.on_after_snapshot_gcode)
                if len(on_after_snapshot_gcode) > 0:
                    logger.info(""Sending on_after_snapshot_gcode for the %s camera."", current_camera.name)
                    self.SendGcodeArrayCallback(
                        on_after_snapshot_gcode,
                        current_camera.timeout_ms / 1000.0,
                        wait_for_completion=not no_wait,
                        tags={'after-snapshot-gcode'}
                    )

        # start the after-snapshot threads
        for t in after_snapshot_threads:
            t.start()

        # join the after-snapshot threads
        for t in after_snapshot_threads:
            if not no_wait:
                snapshot_job_info = t.join()
                assert (isinstance(snapshot_job_info, SnapshotJobInfo))
                info = self.CameraInfos[snapshot_job_info.camera_guid]
                if t.snapshot_thread_error:
                    snapshot_job_info.success = False
                    snapshot_job_info.error = t.snapshot_thread_error
                else:
                    snapshot_job_info.success = True
            else:
                snapshot_job_info.success = True
            results.append(snapshot_job_info)

        if len(after_snapshot_threads) > 0:
            logger.info(""After snapshot threads complete."")

        logger.info(""Snapshot acquisition completed in %.3f seconds."", time()-start_time)

        return results","for t in snapshot_threads:
    t[0].start()","for t in snapshot_threads:
    (t_0, *t_rtmaining) = t
    t[0].start()","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
mmaction2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmaction2/tools/data/build_file_list.py,https://github.com/open-mmlab/mmaction2/tree/master/tools/data/build_file_list.py,,build_file_list$90,"def build_file_list(splits, frame_info, shuffle=False):
    """"""Build file list for a certain data split.

    Args:
        splits (tuple): Data split to generate file list.
        frame_info (dict): Dict mapping from frames to path. e.g.,
            'Skiing/v_Skiing_g18_c02': ('data/ucf101/rawframes/Skiing/v_Skiing_g18_c02', 0, 0).  # noqa: E501
        shuffle (bool): Whether to shuffle the file list.

    Returns:
        tuple: RGB file list for training and testing, together with
            Flow file list for training and testing.
    """"""

    def build_list(split):
        """"""Build RGB and Flow file list with a given split.

        Args:
            split (list): Split to be generate file list.

        Returns:
            tuple[list, list]: (rgb_list, flow_list), rgb_list is the
                generated file list for rgb, flow_list is the generated
                file list for flow.
        """"""
        rgb_list, flow_list = list(), list()
        for item in split:
            if item[0] not in frame_info:
                continue
            if frame_info[item[0]][1] > 0:
                # rawframes
                rgb_cnt = frame_info[item[0]][1]
                flow_cnt = frame_info[item[0]][2]
                if isinstance(item[1], int):
                    rgb_list.append(f'{item[0]} {rgb_cnt} {item[1]}\n')
                    flow_list.append(f'{item[0]} {flow_cnt} {item[1]}\n')
                elif isinstance(item[1], list):
                    # only for multi-label datasets like mmit
                    rgb_list.append(f'{item[0]} {rgb_cnt} ' +
                                    ' '.join([str(digit)
                                              for digit in item[1]]) + '\n')
                    rgb_list.append(f'{item[0]} {flow_cnt} ' +
                                    ' '.join([str(digit)
                                              for digit in item[1]]) + '\n')
                else:
                    raise ValueError(
                        'frame_info should be ' +
                        '[`video`(str), `label`(int)|`labels(list[int])`')
            else:
                # videos
                if isinstance(item[1], int):
                    rgb_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
                    flow_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
                elif isinstance(item[1], list):
                    # only for multi-label datasets like mmit
                    rgb_list.append(f'{frame_info[item[0]][0]} ' +
                                    ' '.join([str(digit)
                                              for digit in item[1]]) + '\n')
                    flow_list.append(
                        f'{frame_info[item[0]][0]} ' +
                        ' '.join([str(digit) for digit in item[1]]) + '\n')
                else:
                    raise ValueError(
                        'frame_info should be ' +
                        '[`video`(str), `label`(int)|`labels(list[int])`')
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return rgb_list, flow_list

    train_rgb_list, train_flow_list = build_list(splits[0])
    test_rgb_list, test_flow_list = build_list(splits[1])
    return (train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list)","for item in split:
    if item[0] not in frame_info:
        continue
    if frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        if isinstance(item[1], int):
            rgb_list.append(f'{item[0]} {rgb_cnt} {item[1]}\n')
            flow_list.append(f'{item[0]} {flow_cnt} {item[1]}\n')
        elif isinstance(item[1], list):
            rgb_list.append(f'{item[0]} {rgb_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
            rgb_list.append(f'{item[0]} {flow_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
        else:
            raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')
    elif isinstance(item[1], int):
        rgb_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
        flow_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
    elif isinstance(item[1], list):
        rgb_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
        flow_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
    else:
        raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')","for item in split:
    (item_0, item_1, *_) = item
    if item[0] not in frame_info:
        continue
    if frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        if isinstance(item[1], int):
            rgb_list.append(f'{item[0]} {rgb_cnt} {item[1]}\n')
            flow_list.append(f'{item[0]} {flow_cnt} {item[1]}\n')
        elif isinstance(item[1], list):
            rgb_list.append(f'{item[0]} {rgb_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
            rgb_list.append(f'{item[0]} {flow_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
        else:
            raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')
    elif isinstance(item[1], int):
        rgb_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
        flow_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
    elif isinstance(item[1], list):
        rgb_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
        flow_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
    else:
        raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pyOCD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyOCD/pyocd/probe/stlink/detect/windows.py,https://github.com/pyocd/pyOCD/tree/master/pyocd/probe/stlink/detect/windows.py,,_get_cached_mounted_points$70,"def _get_cached_mounted_points():
    """"""! Get the volumes present on the system
    @return List of mount points and their associated target id
      Ex. [{ 'mount_point': 'D:', 'target_id_usb_id': 'xxxx'}, ...]
    """"""
    result = []
    try:
        # Open the registry key for mounted devices
        mounted_devices_key = winreg.OpenKey(
            winreg.HKEY_LOCAL_MACHINE, ""SYSTEM\\MountedDevices""
        )
        for v in _iter_vals(mounted_devices_key):
            # Valid entries have the following format: \DosDevices\D:
            if ""DosDevices"" not in v[0]:
                continue

            volume_string = v[1].decode(""utf-16le"", ""ignore"")
            if not _is_mbed_volume(volume_string):
                continue

            mount_point_match = re.match("".*\\\\(.:)$"", v[0])

            if not mount_point_match:
                LOG.debug(""Invalid disk pattern for entry %s, skipping"", v[0])
                continue

            mount_point = mount_point_match.group(1)

            result.append({""mount_point"": mount_point, ""volume_string"": volume_string})
    except OSError:
        LOG.error('Failed to open ""MountedDevices"" in registry')

    return result","for v in _iter_vals(mounted_devices_key):
    if 'DosDevices' not in v[0]:
        continue
    volume_string = v[1].decode('utf-16le', 'ignore')
    if not _is_mbed_volume(volume_string):
        continue
    mount_point_match = re.match('.*\\\\(.:)$', v[0])
    if not mount_point_match:
        LOG.debug('Invalid disk pattern for entry %s, skipping', v[0])
        continue
    mount_point = mount_point_match.group(1)
    result.append({'mount_point': mount_point, 'volume_string': volume_string})","for v in _iter_vals(mounted_devices_key):
    (v_0, v_1, *_) = v
    if 'DosDevices' not in v[0]:
        continue
    volume_string = v[1].decode('utf-16le', 'ignore')
    if not _is_mbed_volume(volume_string):
        continue
    mount_point_match = re.match('.*\\\\(.:)$', v[0])
    if not mount_point_match:
        LOG.debug('Invalid disk pattern for entry %s, skipping', v[0])
        continue
    mount_point = mount_point_match.group(1)
    result.append({'mount_point': mount_point, 'volume_string': volume_string})","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/returners/django_return.py,https://github.com/saltstack/salt/tree/master/salt/returners/django_return.py,,returner$57,"def returner(ret):
    """"""
    Signal a Django server that a return is available
    """"""
    signaled = dispatch.Signal(providing_args=[""ret""]).send(sender=""returner"", ret=ret)

    for signal in signaled:
        log.debug(
            ""Django returner function 'returner' signaled %s which responded with %s"",
            signal[0],
            signal[1],
        )","for signal in signaled:
    log.debug(""Django returner function 'returner' signaled %s which responded with %s"", signal[0], signal[1])","for signal in signaled:
    (signal_0, signal_1, *_) = signal
    log.debug(""Django returner function 'returner' signaled %s which responded with %s"", signal[0], signal[1])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
centerNet-deep-sort,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/centerNet-deep-sort/CenterNet/src/tools/calc_coco_overlap.py,https://github.com/kimyoon-young/centerNet-deep-sort/tree/master/CenterNet/src/tools/calc_coco_overlap.py,,count_agnostic$117,"def count_agnostic(split):
  coco = COCO.COCO(ANN_PATH + ANN_FILES[split])
  images = coco.getImgIds()
  cnt = 0
  for img_id in images:
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    anns = coco.loadAnns(ids=ann_ids)
    centers = []
    for ann in anns:
      bbox = ann['bbox']
      center = ((bbox[0] + bbox[2] / 2) // 4, (bbox[1] + bbox[3] / 2) // 4)
      for c in centers:
        if center[0] == c[0] and center[1] == c[1]:
          cnt += 1
      centers.append(center)
  print('find {} collisions!'.format(cnt))","for ann in anns:
    bbox = ann['bbox']
    center = ((bbox[0] + bbox[2] / 2) // 4, (bbox[1] + bbox[3] / 2) // 4)
    for c in centers:
        if center[0] == c[0] and center[1] == c[1]:
            cnt += 1
    centers.append(center)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_bbox = e['bbox']
variable mapping:
e_bbox: e['bbox']",,,,,,,
centerNet-deep-sort,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/centerNet-deep-sort/CenterNet/src/tools/calc_coco_overlap.py,https://github.com/kimyoon-young/centerNet-deep-sort/tree/master/CenterNet/src/tools/calc_coco_overlap.py,,count_agnostic$117,"def count_agnostic(split):
  coco = COCO.COCO(ANN_PATH + ANN_FILES[split])
  images = coco.getImgIds()
  cnt = 0
  for img_id in images:
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    anns = coco.loadAnns(ids=ann_ids)
    centers = []
    for ann in anns:
      bbox = ann['bbox']
      center = ((bbox[0] + bbox[2] / 2) // 4, (bbox[1] + bbox[3] / 2) // 4)
      for c in centers:
        if center[0] == c[0] and center[1] == c[1]:
          cnt += 1
      centers.append(center)
  print('find {} collisions!'.format(cnt))","for c in centers:
    if center[0] == c[0] and center[1] == c[1]:
        cnt += 1","for c in centers:
    (c_0, c_1, *_) = c
    if center[0] == c[0] and center[1] == c[1]:
        cnt += 1","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Face_Pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Face_Pytorch/train_softmax.py,https://github.com/wujiyang/Face_Pytorch/tree/master//train_softmax.py,,train$38,"def train(args):
    # gpu init
    multi_gpus = False
    if len(args.gpus.split(',')) > 1:
        multi_gpus = True
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # log init
    save_dir = os.path.join(args.save_dir, args.model_pre + args.backbone.upper() + '_' + datetime.now().strftime('%Y%m%d_%H%M%S'))
    if os.path.exists(save_dir):
        raise NameError('model dir exists!')
    os.makedirs(save_dir)
    logging = init_log(save_dir)
    _print = logging.info

    # dataset loader
    transform = transforms.Compose([
        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]
        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]
    ])
    # validation dataset
    trainset = CASIAWebFace(args.train_root, args.train_file_list, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,
                                              shuffle=True, num_workers=8, drop_last=False)
    # test dataset
    lfwdataset = LFW(args.lfw_test_root, args.lfw_file_list, transform=transform)
    lfwloader = torch.utils.data.DataLoader(lfwdataset, batch_size=128,
                                             shuffle=False, num_workers=4, drop_last=False)
    agedbdataset = AgeDB30(args.agedb_test_root, args.agedb_file_list, transform=transform)
    agedbloader = torch.utils.data.DataLoader(agedbdataset, batch_size=128,
                                            shuffle=False, num_workers=4, drop_last=False)
    cfpfpdataset = CFP_FP(args.cfpfp_test_root, args.cfpfp_file_list, transform=transform)
    cfpfploader = torch.utils.data.DataLoader(cfpfpdataset, batch_size=128,
                                              shuffle=False, num_workers=4, drop_last=False)

    # define backbone and margin layer
    if args.backbone == 'MobileFace':
        net = MobileFaceNet()
    elif args.backbone == 'Res50':
        net = ResNet50()
    elif args.backbone == 'Res101':
        net = ResNet101()
    elif args.backbone == 'Res50_IR':
        net = SEResNet_IR(50, feature_dim=args.feature_dim, mode='ir')
    elif args.backbone == 'SERes50_IR':
        net = SEResNet_IR(50, feature_dim=args.feature_dim, mode='se_ir')
    elif args.backbone == 'SphereNet':
        net = SphereNet(num_layers=64, feature_dim=args.feature_dim)
    else:
        print(args.backbone, ' is not available!')

    if args.margin_type == 'ArcFace':
        margin = ArcMarginProduct(args.feature_dim, trainset.class_nums, s=args.scale_size)
    elif args.margin_type == 'CosFace':
        pass
    elif args.margin_type == 'SphereFace':
        pass
    elif args.margin_type == 'InnerProduct':
        margin = InnerProduct(args.feature_dim, trainset.class_nums)
    else:
        print(args.margin_type, 'is not available!')

    if args.resume:
        print('resume the model parameters from: ', args.net_path, args.margin_path)
        net.load_state_dict(torch.load(args.net_path)['net_state_dict'])
        margin.load_state_dict(torch.load(args.margin_path)['net_state_dict'])

    # define optimizers for different layer

    criterion_classi = torch.nn.CrossEntropyLoss().to(device)
    optimizer_classi = optim.SGD([
        {'params': net.parameters(), 'weight_decay': 5e-4},
        {'params': margin.parameters(), 'weight_decay': 5e-4}
    ], lr=0.1, momentum=0.9, nesterov=True)
    scheduler_classi = lr_scheduler.MultiStepLR(optimizer_classi, milestones=[20, 35, 45], gamma=0.1)

    if multi_gpus:
        net = DataParallel(net).to(device)
        margin = DataParallel(margin).to(device)
    else:
        net = net.to(device)
        margin = margin.to(device)

    best_lfw_acc = 0.0
    best_lfw_iters = 0
    best_agedb30_acc = 0.0
    best_agedb30_iters = 0
    best_cfp_fp_acc = 0.0
    best_cfp_fp_iters = 0
    total_iters = 0
    vis = Visualizer(env='softmax_train')
    for epoch in range(1, args.total_epoch + 1):
        scheduler_classi.step()
        # train model
        _print('Train Epoch: {}/{} ...'.format(epoch, args.total_epoch))
        net.train()

        since = time.time()
        for data in trainloader:
            img, label = data[0].to(device), data[1].to(device)
            feature = net(img)
            output = margin(feature)
            loss_classi = criterion_classi(output, label)
            total_loss = loss_classi

            optimizer_classi.zero_grad()
            total_loss.backward()
            optimizer_classi.step()

            total_iters += 1
            # print train information
            if total_iters % 100 == 0:
                #current training accuracy
                _, predict = torch.max(output.data, 1)
                total = label.size(0)
                correct = (np.array(predict) == np.array(label.data)).sum()
                time_cur = (time.time() - since) / 100
                since = time.time()
                vis.plot_curves({'train loss': loss_classi.item()}, iters=total_iters, title='train loss', xlabel='iters', ylabel='train loss')
                vis.plot_curves({'train accuracy': correct/total}, iters=total_iters, title='train accuracy', xlabel='iters', ylabel='train accuracy')
                print(""Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}"".format(total_iters,
                                                                                                                                          epoch,
                                                                                                                                          loss_classi.item(),
                                                                                                                                          correct/total,
                                                                                                                                          time_cur,
                                                                                                                                          scheduler_classi.get_lr()[
                                                                                                                                              0]))
            # save model
            if total_iters % args.save_freq == 0:
                msg = 'Saving checkpoint: {}'.format(total_iters)
                _print(msg)
                if multi_gpus:
                    net_state_dict = net.module.state_dict()
                    margin_state_dict = margin.module.state_dict()
                else:
                    net_state_dict = net.state_dict()
                    margin_state_dict = margin.state_dict()

                if not os.path.exists(save_dir):
                    os.mkdir(save_dir)
                torch.save({
                    'iters': total_iters,
                    'net_state_dict': net_state_dict},
                    os.path.join(save_dir, 'Iter_%06d_net.ckpt' % total_iters))
                torch.save({
                    'iters': total_iters,
                    'net_state_dict': margin_state_dict},
                    os.path.join(save_dir, 'Iter_%06d_margin.ckpt' % total_iters))

            # test accuracy
            if total_iters % args.test_freq == 0:
                # test model on lfw
                net.eval()
                getFeatureFromTorch('./result/cur_lfw_result.mat', net, device, lfwdataset, lfwloader)
                lfw_accs = evaluation_10_fold('./result/cur_lfw_result.mat')
                _print('LFW Ave Accuracy: {:.4f}'.format(np.mean(lfw_accs) * 100))
                if best_lfw_acc < np.mean(lfw_accs) * 100:
                    best_lfw_acc = np.mean(lfw_accs) * 100
                    best_lfw_iters = total_iters
                # test model on AgeDB30
                getFeatureFromTorch('./result/cur_agedb30_result.mat', net, device, agedbdataset, agedbloader)
                age_accs = evaluation_10_fold('./result/cur_agedb30_result.mat')
                _print('AgeDB-30 Ave Accuracy: {:.4f}'.format(np.mean(age_accs) * 100))
                if best_agedb30_acc < np.mean(age_accs) * 100:
                    best_agedb30_acc = np.mean(age_accs) * 100
                    best_agedb30_iters = total_iters
                # test model on CFP-FP
                getFeatureFromTorch('./result/cur_cfpfp_result.mat', net, device, cfpfpdataset, cfpfploader)
                cfp_accs = evaluation_10_fold('./result/cur_cfpfp_result.mat')
                _print('CFP-FP Ave Accuracy: {:.4f}'.format(np.mean(cfp_accs) * 100))
                if best_cfp_fp_acc < np.mean(cfp_accs) * 100:
                    best_cfp_fp_acc = np.mean(cfp_accs) * 100
                    best_cfp_fp_iters = total_iters
                _print('Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(
                    best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))
                vis.plot_curves({'lfw': np.mean(lfw_accs), 'agedb-30': np.mean(age_accs), 'cfp-fp': np.mean(cfp_accs)}, iters=total_iters, title='test accuracy', xlabel='iters', ylabel='test accuracy')
                net.train()

    _print('Finally Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(
        best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))
    print('finishing training')","for data in trainloader:
    (img, label) = (data[0].to(device), data[1].to(device))
    feature = net(img)
    output = margin(feature)
    loss_classi = criterion_classi(output, label)
    total_loss = loss_classi
    optimizer_classi.zero_grad()
    total_loss.backward()
    optimizer_classi.step()
    total_iters += 1
    if total_iters % 100 == 0:
        (_, predict) = torch.max(output.data, 1)
        total = label.size(0)
        correct = (np.array(predict) == np.array(label.data)).sum()
        time_cur = (time.time() - since) / 100
        since = time.time()
        vis.plot_curves({'train loss': loss_classi.item()}, iters=total_iters, title='train loss', xlabel='iters', ylabel='train loss')
        vis.plot_curves({'train accuracy': correct / total}, iters=total_iters, title='train accuracy', xlabel='iters', ylabel='train accuracy')
        print('Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}'.format(total_iters, epoch, loss_classi.item(), correct / total, time_cur, scheduler_classi.get_lr()[0]))
    if total_iters % args.save_freq == 0:
        msg = 'Saving checkpoint: {}'.format(total_iters)
        _print(msg)
        if multi_gpus:
            net_state_dict = net.module.state_dict()
            margin_state_dict = margin.module.state_dict()
        else:
            net_state_dict = net.state_dict()
            margin_state_dict = margin.state_dict()
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)
        torch.save({'iters': total_iters, 'net_state_dict': net_state_dict}, os.path.join(save_dir, 'Iter_%06d_net.ckpt' % total_iters))
        torch.save({'iters': total_iters, 'net_state_dict': margin_state_dict}, os.path.join(save_dir, 'Iter_%06d_margin.ckpt' % total_iters))
    if total_iters % args.test_freq == 0:
        net.eval()
        getFeatureFromTorch('./result/cur_lfw_result.mat', net, device, lfwdataset, lfwloader)
        lfw_accs = evaluation_10_fold('./result/cur_lfw_result.mat')
        _print('LFW Ave Accuracy: {:.4f}'.format(np.mean(lfw_accs) * 100))
        if best_lfw_acc < np.mean(lfw_accs) * 100:
            best_lfw_acc = np.mean(lfw_accs) * 100
            best_lfw_iters = total_iters
        getFeatureFromTorch('./result/cur_agedb30_result.mat', net, device, agedbdataset, agedbloader)
        age_accs = evaluation_10_fold('./result/cur_agedb30_result.mat')
        _print('AgeDB-30 Ave Accuracy: {:.4f}'.format(np.mean(age_accs) * 100))
        if best_agedb30_acc < np.mean(age_accs) * 100:
            best_agedb30_acc = np.mean(age_accs) * 100
            best_agedb30_iters = total_iters
        getFeatureFromTorch('./result/cur_cfpfp_result.mat', net, device, cfpfpdataset, cfpfploader)
        cfp_accs = evaluation_10_fold('./result/cur_cfpfp_result.mat')
        _print('CFP-FP Ave Accuracy: {:.4f}'.format(np.mean(cfp_accs) * 100))
        if best_cfp_fp_acc < np.mean(cfp_accs) * 100:
            best_cfp_fp_acc = np.mean(cfp_accs) * 100
            best_cfp_fp_iters = total_iters
        _print('Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))
        vis.plot_curves({'lfw': np.mean(lfw_accs), 'agedb-30': np.mean(age_accs), 'cfp-fp': np.mean(cfp_accs)}, iters=total_iters, title='test accuracy', xlabel='iters', ylabel='test accuracy')
        net.train()","for data in trainloader:
    (data_0, data_1, *_) = data
    (img, label) = (data[0].to(device), data[1].to(device))
    feature = net(img)
    output = margin(feature)
    loss_classi = criterion_classi(output, label)
    total_loss = loss_classi
    optimizer_classi.zero_grad()
    total_loss.backward()
    optimizer_classi.step()
    total_iters += 1
    if total_iters % 100 == 0:
        (_, predict) = torch.max(output.data, 1)
        total = label.size(0)
        correct = (np.array(predict) == np.array(label.data)).sum()
        time_cur = (time.time() - since) / 100
        since = time.time()
        vis.plot_curves({'train loss': loss_classi.item()}, iters=total_iters, title='train loss', xlabel='iters', ylabel='train loss')
        vis.plot_curves({'train accuracy': correct / total}, iters=total_iters, title='train accuracy', xlabel='iters', ylabel='train accuracy')
        print('Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}'.format(total_iters, epoch, loss_classi.item(), correct / total, time_cur, scheduler_classi.get_lr()[0]))
    if total_iters % args.save_freq == 0:
        msg = 'Saving checkpoint: {}'.format(total_iters)
        _print(msg)
        if multi_gpus:
            net_state_dict = net.module.state_dict()
            margin_state_dict = margin.module.state_dict()
        else:
            net_state_dict = net.state_dict()
            margin_state_dict = margin.state_dict()
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)
        torch.save({'iters': total_iters, 'net_state_dict': net_state_dict}, os.path.join(save_dir, 'Iter_%06d_net.ckpt' % total_iters))
        torch.save({'iters': total_iters, 'net_state_dict': margin_state_dict}, os.path.join(save_dir, 'Iter_%06d_margin.ckpt' % total_iters))
    if total_iters % args.test_freq == 0:
        net.eval()
        getFeatureFromTorch('./result/cur_lfw_result.mat', net, device, lfwdataset, lfwloader)
        lfw_accs = evaluation_10_fold('./result/cur_lfw_result.mat')
        _print('LFW Ave Accuracy: {:.4f}'.format(np.mean(lfw_accs) * 100))
        if best_lfw_acc < np.mean(lfw_accs) * 100:
            best_lfw_acc = np.mean(lfw_accs) * 100
            best_lfw_iters = total_iters
        getFeatureFromTorch('./result/cur_agedb30_result.mat', net, device, agedbdataset, agedbloader)
        age_accs = evaluation_10_fold('./result/cur_agedb30_result.mat')
        _print('AgeDB-30 Ave Accuracy: {:.4f}'.format(np.mean(age_accs) * 100))
        if best_agedb30_acc < np.mean(age_accs) * 100:
            best_agedb30_acc = np.mean(age_accs) * 100
            best_agedb30_iters = total_iters
        getFeatureFromTorch('./result/cur_cfpfp_result.mat', net, device, cfpfpdataset, cfpfploader)
        cfp_accs = evaluation_10_fold('./result/cur_cfpfp_result.mat')
        _print('CFP-FP Ave Accuracy: {:.4f}'.format(np.mean(cfp_accs) * 100))
        if best_cfp_fp_acc < np.mean(cfp_accs) * 100:
            best_cfp_fp_acc = np.mean(cfp_accs) * 100
            best_cfp_fp_iters = total_iters
        _print('Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))
        vis.plot_curves({'lfw': np.mean(lfw_accs), 'agedb-30': np.mean(age_accs), 'cfp-fp': np.mean(cfp_accs)}, iters=total_iters, title='test accuracy', xlabel='iters', ylabel='test accuracy')
        net.train()","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/bluez_bluetooth.py,https://github.com/saltstack/salt/tree/master/salt/modules/bluez_bluetooth.py,,scan$167,"def scan():
    """"""
    Scan for bluetooth devices in the area

    CLI Example:

    .. code-block:: bash

        salt '*' bluetooth.scan
    """"""
    ret = []
    devices = bluetooth.discover_devices(lookup_names=True)
    for device in devices:
        ret.append({device[0]: device[1]})
    return ret","for device in devices:
    ret.append({device[0]: device[1]})","for device in devices:
    (device_0, device_1, *_) = device
    ret.append({device[0]: device[1]})","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
underthesea,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/underthesea/underthesea/transformer/tagged_feature.py,https://github.com/undertheseanlp/underthesea/tree/master/underthesea/transformer/tagged_feature.py,,text_istitle$23,"def text_istitle(word):
    if len(word) == 0:
        return False
    try:
        titles = [s[0] for s in word.split("" "")]
        for token in titles:
            if token[0].istitle() is False:
                return False
        return True
    except Exception:
        return False","for token in titles:
    if token[0].istitle() is False:
        return False","for token in titles:
    (token_0, *token_rtokenmaining) = token
    if token[0].istitle() is False:
        return False","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
you-get,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/mgtv.py,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/mgtv.py,MGTV,prepare$93,"def prepare(self, **kwargs):
        if self.url:
            self.vid = self.get_vid_from_url(self.url)
        content_info = get_content(self.info_endpoint.format(video_id=self.vid))
        log.d(content_info)
        content_info = loads(content_info)
        self.title = content_info['data']['info']['videoName']

        content_player = get_content(self.player_endpoint.format(did=self.did, video_id=self.vid, tk2=self.tk2()))
        log.d(content_player)
        content_player = loads(content_player)
        pm2 = content_player['data']['atc']['pm2']

        content_source = get_content(self.source_endpoint.format(video_id=self.vid, tk2=self.tk2(), pm2=pm2))
        log.d(content_source)
        content_source = loads(content_source)
        domain = content_source['data']['stream_domain'][0]

        # stream_available = [i['name'] for i in content['data']['stream']]
        stream_available = {}
        for i in content_source['data']['stream']:
            stream_available[i['name']] = i['url']

        for s in self.stream_types:
            if s['video_profile'] in stream_available.keys():
                quality_id = self.id_dic[s['video_profile']]
                url = stream_available[s['video_profile']]
                if url is None or url == '':
                    # skip invalid profile with empty url
                    continue
                url = domain + re.sub(r'(\&arange\=\d+)', '', url)  # Un-Hum
                m3u8_url, m3u8_size, segment_list_this = self.get_mgtv_real_url(url)

                stream_fileid_list = []
                for i in segment_list_this:
                    stream_fileid_list.append(os.path.basename(i).split('.')[0])

                # make pieces
                pieces = []
                for i in zip(stream_fileid_list, segment_list_this):
                    pieces.append({'fileid': i[0], 'segs': i[1], })

                    self.streams[quality_id] = {
                        'container': s['container'],
                        'video_profile': s['video_profile'],
                        'size': m3u8_size,
                        'pieces': pieces,
                        'm3u8_url': m3u8_url
                    }

                if not kwargs['info_only']:
                    self.streams[quality_id]['src'] = segment_list_this","for i in content_source['data']['stream']:
    stream_available[i['name']] = i['url']",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['url'] is a dictionary value that can be accessed directly using the key 'url'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,
you-get,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/mgtv.py,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/mgtv.py,MGTV,prepare$93,"def prepare(self, **kwargs):
        if self.url:
            self.vid = self.get_vid_from_url(self.url)
        content_info = get_content(self.info_endpoint.format(video_id=self.vid))
        log.d(content_info)
        content_info = loads(content_info)
        self.title = content_info['data']['info']['videoName']

        content_player = get_content(self.player_endpoint.format(did=self.did, video_id=self.vid, tk2=self.tk2()))
        log.d(content_player)
        content_player = loads(content_player)
        pm2 = content_player['data']['atc']['pm2']

        content_source = get_content(self.source_endpoint.format(video_id=self.vid, tk2=self.tk2(), pm2=pm2))
        log.d(content_source)
        content_source = loads(content_source)
        domain = content_source['data']['stream_domain'][0]

        # stream_available = [i['name'] for i in content['data']['stream']]
        stream_available = {}
        for i in content_source['data']['stream']:
            stream_available[i['name']] = i['url']

        for s in self.stream_types:
            if s['video_profile'] in stream_available.keys():
                quality_id = self.id_dic[s['video_profile']]
                url = stream_available[s['video_profile']]
                if url is None or url == '':
                    # skip invalid profile with empty url
                    continue
                url = domain + re.sub(r'(\&arange\=\d+)', '', url)  # Un-Hum
                m3u8_url, m3u8_size, segment_list_this = self.get_mgtv_real_url(url)

                stream_fileid_list = []
                for i in segment_list_this:
                    stream_fileid_list.append(os.path.basename(i).split('.')[0])

                # make pieces
                pieces = []
                for i in zip(stream_fileid_list, segment_list_this):
                    pieces.append({'fileid': i[0], 'segs': i[1], })

                    self.streams[quality_id] = {
                        'container': s['container'],
                        'video_profile': s['video_profile'],
                        'size': m3u8_size,
                        'pieces': pieces,
                        'm3u8_url': m3u8_url
                    }

                if not kwargs['info_only']:
                    self.streams[quality_id]['src'] = segment_list_this","for s in self.stream_types:
    if s['video_profile'] in stream_available.keys():
        quality_id = self.id_dic[s['video_profile']]
        url = stream_available[s['video_profile']]
        if url is None or url == '':
            continue
        url = domain + re.sub('(\\&arange\\=\\d+)', '', url)
        (m3u8_url, m3u8_size, segment_list_this) = self.get_mgtv_real_url(url)
        stream_fileid_list = []
        for i in segment_list_this:
            stream_fileid_list.append(os.path.basename(i).split('.')[0])
        pieces = []
        for i in zip(stream_fileid_list, segment_list_this):
            pieces.append({'fileid': i[0], 'segs': i[1]})
            self.streams[quality_id] = {'container': s['container'], 'video_profile': s['video_profile'], 'size': m3u8_size, 'pieces': pieces, 'm3u8_url': m3u8_url}
        if not kwargs['info_only']:
            self.streams[quality_id]['src'] = segment_list_this",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_container, e_video_profile = e['container'], e['video_profile']
variable mapping:
e_container: e['container']
e_video_profile: e['video_profile']",,,,,,,
you-get,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/mgtv.py,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/mgtv.py,MGTV,prepare$93,"def prepare(self, **kwargs):
        if self.url:
            self.vid = self.get_vid_from_url(self.url)
        content_info = get_content(self.info_endpoint.format(video_id=self.vid))
        log.d(content_info)
        content_info = loads(content_info)
        self.title = content_info['data']['info']['videoName']

        content_player = get_content(self.player_endpoint.format(did=self.did, video_id=self.vid, tk2=self.tk2()))
        log.d(content_player)
        content_player = loads(content_player)
        pm2 = content_player['data']['atc']['pm2']

        content_source = get_content(self.source_endpoint.format(video_id=self.vid, tk2=self.tk2(), pm2=pm2))
        log.d(content_source)
        content_source = loads(content_source)
        domain = content_source['data']['stream_domain'][0]

        # stream_available = [i['name'] for i in content['data']['stream']]
        stream_available = {}
        for i in content_source['data']['stream']:
            stream_available[i['name']] = i['url']

        for s in self.stream_types:
            if s['video_profile'] in stream_available.keys():
                quality_id = self.id_dic[s['video_profile']]
                url = stream_available[s['video_profile']]
                if url is None or url == '':
                    # skip invalid profile with empty url
                    continue
                url = domain + re.sub(r'(\&arange\=\d+)', '', url)  # Un-Hum
                m3u8_url, m3u8_size, segment_list_this = self.get_mgtv_real_url(url)

                stream_fileid_list = []
                for i in segment_list_this:
                    stream_fileid_list.append(os.path.basename(i).split('.')[0])

                # make pieces
                pieces = []
                for i in zip(stream_fileid_list, segment_list_this):
                    pieces.append({'fileid': i[0], 'segs': i[1], })

                    self.streams[quality_id] = {
                        'container': s['container'],
                        'video_profile': s['video_profile'],
                        'size': m3u8_size,
                        'pieces': pieces,
                        'm3u8_url': m3u8_url
                    }

                if not kwargs['info_only']:
                    self.streams[quality_id]['src'] = segment_list_this","for i in zip(stream_fileid_list, segment_list_this):
    pieces.append({'fileid': i[0], 'segs': i[1]})
    self.streams[quality_id] = {'container': s['container'], 'video_profile': s['video_profile'], 'size': m3u8_size, 'pieces': pieces, 'm3u8_url': m3u8_url}","for i in zip(stream_fileid_list, segment_list_this):
    (i_0, i_1, *_) = i
    pieces.append({'fileid': i[0], 'segs': i[1]})
    self.streams[quality_id] = {'container': s['container'], 'video_profile': s['video_profile'], 'size': m3u8_size, 'pieces': pieces, 'm3u8_url': m3u8_url}","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,,get_sales_payment_data$66,"def get_sales_payment_data(filters, columns):
	data = []
	show_payment_detail = False

	sales_invoice_data = get_sales_invoice_data(filters)
	mode_of_payments = get_mode_of_payments(filters)
	mode_of_payment_details = get_mode_of_payment_details(filters)

	if filters.get(""payment_detail""):
		show_payment_detail = True
	else:
		show_payment_detail = False

	for inv in sales_invoice_data:
		owner_posting_date = inv[""owner""] + cstr(inv[""posting_date""])
		if show_payment_detail:
			row = [inv.posting_date, inv.owner, "" "", inv.net_total, inv.total_taxes, 0]
			data.append(row)
			for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
				row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
				data.append(row)
		else:
			total_payment = 0
			for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
				total_payment = total_payment + mop_detail[1]
			row = [
				inv.posting_date,
				inv.owner,
				"", "".join(mode_of_payments.get(owner_posting_date, [])),
				inv.net_total,
				inv.total_taxes,
				total_payment,
			]
			data.append(row)
	return data","for inv in sales_invoice_data:
    owner_posting_date = inv['owner'] + cstr(inv['posting_date'])
    if show_payment_detail:
        row = [inv.posting_date, inv.owner, ' ', inv.net_total, inv.total_taxes, 0]
        data.append(row)
        for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
            row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
            data.append(row)
    else:
        total_payment = 0
        for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
            total_payment = total_payment + mop_detail[1]
        row = [inv.posting_date, inv.owner, ', '.join(mode_of_payments.get(owner_posting_date, [])), inv.net_total, inv.total_taxes, total_payment]
        data.append(row)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_owner, e_posting_date = e['owner'], e['posting_date']
variable mapping:
e_owner: e['owner']
e_posting_date: e['posting_date']",,,,,,,
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,,get_sales_payment_data$66,"def get_sales_payment_data(filters, columns):
	data = []
	show_payment_detail = False

	sales_invoice_data = get_sales_invoice_data(filters)
	mode_of_payments = get_mode_of_payments(filters)
	mode_of_payment_details = get_mode_of_payment_details(filters)

	if filters.get(""payment_detail""):
		show_payment_detail = True
	else:
		show_payment_detail = False

	for inv in sales_invoice_data:
		owner_posting_date = inv[""owner""] + cstr(inv[""posting_date""])
		if show_payment_detail:
			row = [inv.posting_date, inv.owner, "" "", inv.net_total, inv.total_taxes, 0]
			data.append(row)
			for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
				row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
				data.append(row)
		else:
			total_payment = 0
			for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
				total_payment = total_payment + mop_detail[1]
			row = [
				inv.posting_date,
				inv.owner,
				"", "".join(mode_of_payments.get(owner_posting_date, [])),
				inv.net_total,
				inv.total_taxes,
				total_payment,
			]
			data.append(row)
	return data","for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
    row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
    data.append(row)","for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
    (mop_detail_0, mop_detail_1, *_) = mop_detail
    row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
    data.append(row)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,,get_sales_payment_data$66,"def get_sales_payment_data(filters, columns):
	data = []
	show_payment_detail = False

	sales_invoice_data = get_sales_invoice_data(filters)
	mode_of_payments = get_mode_of_payments(filters)
	mode_of_payment_details = get_mode_of_payment_details(filters)

	if filters.get(""payment_detail""):
		show_payment_detail = True
	else:
		show_payment_detail = False

	for inv in sales_invoice_data:
		owner_posting_date = inv[""owner""] + cstr(inv[""posting_date""])
		if show_payment_detail:
			row = [inv.posting_date, inv.owner, "" "", inv.net_total, inv.total_taxes, 0]
			data.append(row)
			for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
				row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
				data.append(row)
		else:
			total_payment = 0
			for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
				total_payment = total_payment + mop_detail[1]
			row = [
				inv.posting_date,
				inv.owner,
				"", "".join(mode_of_payments.get(owner_posting_date, [])),
				inv.net_total,
				inv.total_taxes,
				total_payment,
			]
			data.append(row)
	return data","for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
    total_payment = total_payment + mop_detail[1]","for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
    (_, mop_detail_1, *mop_detail_rmop_detailmaining) = mop_detail
    total_payment = total_payment + mop_detail[1]","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/vsphere/tests/legacy/test_vsphere.py,https://github.com/DataDog/integrations-core/tree/master/vsphere/tests/legacy/test_vsphere.py,,test__process_mor_objects_queue$379,"def test__process_mor_objects_queue(vsphere, instance):
    vsphere.log = MagicMock()
    vsphere._process_mor_objects_queue_async = MagicMock()
    vsphere._process_mor_objects_queue(instance)
    # Queue hasn't been initialized
    vsphere.log.debug.assert_called_once_with(
        ""Objects queue is not initialized yet for instance %s, skipping processing"", vsphere._instance_key(instance)
    )

    vsphere.batch_morlist_size = 1
    i_key = vsphere._instance_key(instance)
    with mock.patch('datadog_checks.vsphere.legacy.vsphere_legacy.vmodl'):
        vsphere._cache_morlist_raw(instance)
        assert sum(vsphere.mor_objects_queue.size(i_key, res_type) for res_type in RESOURCE_TYPE_METRICS) == 11
        vsphere._process_mor_objects_queue(instance)
        # Object queue should be empty after processing
        assert sum(vsphere.mor_objects_queue.size(i_key, res_type) for res_type in RESOURCE_TYPE_METRICS) == 0
        assert vsphere._process_mor_objects_queue_async.call_count == 0  # realtime only
        for call_args in vsphere._process_mor_objects_queue_async.call_args_list:
            # query_specs parameter should be a list of size 1 since the batch size is 1
            assert len(call_args[0][1]) == 1

        instance[""collect_realtime_only""] = False
        vsphere._cache_morlist_raw(instance)
        assert sum(vsphere.mor_objects_queue.size(i_key, res_type) for res_type in RESOURCE_TYPE_METRICS) == 11
        vsphere._process_mor_objects_queue(instance)
        # Object queue should be empty after processing
        assert sum(vsphere.mor_objects_queue.size(i_key, res_type) for res_type in RESOURCE_TYPE_METRICS) == 0
        assert vsphere._process_mor_objects_queue_async.call_count == 5","for call_args in vsphere._process_mor_objects_queue_async.call_args_list:
    assert len(call_args[0][1]) == 1","for call_args in vsphere._process_mor_objects_queue_async.call_args_list:
    ((call_args_0_0, call_args_0_1, *call_args_0_rcall_argsmaining), *call_args_rcall_argsmaining) = call_args
    assert len(call_args[0][1]) == 1","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: (e_0_0, e_0_1, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_1: e[0][1]",,,,,,,
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/meta_engine.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/meta_engine.py,MetaEngine,get_epn_arr_list$195,"def get_epn_arr_list(self, site, name, video_dir):
        epn_arr = []
        if site.lower() == 'video' and video_dir:
             video_db = os.path.join(ui.home_folder, 'VideoDB', 'Video.db')
             if os.path.exists(video_db):
                epn_arr_tmp = ui.media_data.get_video_db(video_db, ""Directory"", video_dir)
                for i in epn_arr_tmp:
                    epn_name = i[0]+'	'+i[1]
                    logger.debug(epn_name)
                    epn_arr.append(epn_name)
        elif video_dir:
            new_name_with_info = video_dir.strip()
            extra_info = ''
            if '	' in new_name_with_info:
                name_title = new_name_with_info.split('	')[0]
                extra_info = new_name_with_info.split('	')[1]
            else:
                name_title = new_name_with_info
            
            if site.lower() == 'subbedanime' or site.lower() == 'dubbedanime':
                siteName = ui.get_parameters_value(s='siteName')['siteName']
                hist_site = os.path.join(ui.home_folder, 'History', site, siteName, name_title)
            else:
                hist_site = os.path.join(ui.home_folder, 'History', site, name_title)
                
            hist_epn = os.path.join(hist_site, 'Ep.txt')
            logger.info(hist_epn)
            if os.path.exists(hist_epn):
                lines = open_files(hist_epn, True)
                for i in lines:
                    i = i.strip()
                    j = i.split('	')
                    if len(j) == 1:
                        epn_arr.append(i+'	'+i+'	'+name)
                    elif len(j) >= 2:
                        epn_arr.append(i+'	'+name)
        return epn_arr","for i in epn_arr_tmp:
    epn_name = i[0] + '\t' + i[1]
    logger.debug(epn_name)
    epn_arr.append(epn_name)","for i in epn_arr_tmp:
    (i_0, i_1, *_) = i
    epn_name = i[0] + '\t' + i[1]
    logger.debug(epn_name)
    epn_arr.append(epn_name)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
PGPortfolio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGPortfolio/pgportfolio/marketdata/globaldatamatrix.py,https://github.com/ZhengyaoJiang/PGPortfolio/tree/master/pgportfolio/marketdata/globaldatamatrix.py,HistoryManager,select_coins$125,"def select_coins(self, start, end):
        if not self._online:
            logging.info(""select coins offline from %s to %s"" % (datetime.fromtimestamp(start).strftime('%Y-%m-%d %H:%M'),
                                                                    datetime.fromtimestamp(end).strftime('%Y-%m-%d %H:%M')))
            connection = sqlite3.connect(DATABASE_DIR)
            try:
                cursor=connection.cursor()
                cursor.execute('SELECT coin,SUM(volume) AS total_volume FROM History WHERE'
                               ' date>=? and date<=? GROUP BY coin'
                               ' ORDER BY total_volume DESC LIMIT ?;',
                               (int(start), int(end), self._coin_number))
                coins_tuples = cursor.fetchall()

                if len(coins_tuples)!=self._coin_number:
                    logging.error(""the sqlite error happend"")
            finally:
                connection.commit()
                connection.close()
            coins = []
            for tuple in coins_tuples:
                coins.append(tuple[0])
        else:
            coins = list(self._coin_list.topNVolume(n=self._coin_number).index)
        logging.debug(""Selected coins are: ""+str(coins))
        return coins","for tuple in coins_tuples:
    coins.append(tuple[0])","for tuple in coins_tuples:
    (tuple_0, *tuple_rtuplemaining) = tuple
    coins.append(tuple[0])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
CMSmap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CMSmap/cmsmap/lib/jooscan.py,https://github.com/Dionach/CMSmap/tree/master/cmsmap/lib/jooscan.py,JooScan,JooFeed$140,"def JooFeed(self):
        requester.request(self.url + '/?format=feed', data=None)
        jooUsers = re.findall(""<author>(.+?) \((.+?)\)</author>"", requester.htmltext, re.IGNORECASE)
        if jooUsers:
            msg = ""Enumerating Joomla Usernames via \""Feed\"" ...""
            report.message(msg)
            jooUsers = sorted(set(jooUsers))
            for user in jooUsers:
                self.usernames.append(user[1])
                msg = user[1] + "": "" + user[0]
                report.info(msg)","for user in jooUsers:
    self.usernames.append(user[1])
    msg = user[1] + ': ' + user[0]
    report.info(msg)","for user in jooUsers:
    (user_0, user_1, *_) = user
    self.usernames.append(user[1])
    msg = user[1] + ': ' + user[0]
    report.info(msg)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
auto-editor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/auto-editor/auto_editor/__main__.py,https://github.com/WyattBlue/auto-editor/tree/master/auto_editor/__main__.py,,main$233,"def main():
    parser = vanparse.ArgumentParser('Auto-Editor', auto_editor.version,
        description='\nAuto-Editor is an automatic video/audio creator and editor. '
            'By default, it will detect silence and create a new video with those '
            'sections cut out. By changing some of the options, you can export to a '
            'traditional editor like Premiere Pro and adjust the edits there, adjust '
            'the pacing of the cuts, and change the method of editing like using audio '
            'loudness and video motion to judge making cuts.\nRun:\n    auto-editor '
            '--help\n\nTo get the list of options.\n')

    subcommands = ['create', 'test', 'info', 'levels', 'grep', 'subdump', 'desc']

    if(len(sys.argv) > 1 and sys.argv[1] in subcommands):
        obj = __import__('auto_editor.subcommands.{}'.format(sys.argv[1]),
            fromlist=['subcommands'])
        obj.main(sys.argv[2:])
        sys.exit()
    else:
        parser = main_options(parser)
        args = parser.parse_args(sys.argv[1:], Log(), 'auto-editor')

    timer = Timer(args.quiet)

    exporting_to_editor = (args.export_to_premiere or args.export_to_resolve or
        args.export_to_final_cut_pro or args.export_to_shotcut)
    making_data_file = exporting_to_editor or args.export_as_json

    is64bit = '64-bit' if sys.maxsize > 2**32 else '32-bit'

    ffmpeg = FFmpeg(args.ffmpeg_location, args.my_ffmpeg, args.show_ffmpeg_debug)

    if(args.debug and args.input == []):
        import platform

        dirpath = os.path.dirname(os.path.realpath(__file__))

        print('Python Version: {} {}'.format(platform.python_version(), is64bit))
        print('Platform: {} {} {}'.format(platform.system(), platform.release(), platform.machine().lower()))
        print('Config File path: {}'.format(os.path.join(dirpath, 'config.txt')))
        print('FFmpeg path: {}'.format(ffmpeg.path))
        print('FFmpeg version: {}'.format(ffmpeg.version))
        print('Auto-Editor version {}'.format(auto_editor.version))
        sys.exit()

    if(is64bit == '32-bit'):
        Log().warning('You have the 32-bit version of Python, which may lead to '
            'memory crashes.')

    if(args.version):
        print('Auto-Editor version {}'.format(auto_editor.version))
        sys.exit()

    if(args.temp_dir is None):
        TEMP = tempfile.mkdtemp()
    else:
        TEMP = args.temp_dir
        if(os.path.isfile(TEMP)):
            Log().error('Temp directory cannot be an already existing file.')
        if(os.path.isdir(TEMP)):
            if(len(os.listdir(TEMP)) != 0):
                Log().error('Temp directory should be empty!')
        else:
            os.mkdir(TEMP)

    log = Log(args.debug, args.quiet, temp=TEMP)
    log.debug('Temp Directory: {}'.format(TEMP))

    if(args.input == []):
        log.error('You need to give auto-editor an input file or folder so it can '
            'do the work for you.')

    if([args.export_to_premiere, args.export_to_resolve,
        args.export_to_final_cut_pro, args.export_as_audio,
        args.export_to_shotcut, args.export_as_clip_sequence].count(True) > 1):
        log.error('You must choose only one export option.')

    if(isinstance(args.frame_margin, str)):
        try:
            if(float(args.frame_margin) < 0):
                log.error('Frame margin cannot be negative.')
        except ValueError:
            log.error('Frame margin {}, is not valid.'.format(args.frame_margin))
    elif(args.frame_margin < 0):
        log.error('Frame margin cannot be negative.')
    if(args.constant_rate_factor != 'unset'):
        if(int(args.constant_rate_factor) < 0 or int(args.constant_rate_factor) > 51):
            log.error('Constant rate factor (crf) must be between 0-51.')
    if(args.width < 1):
        log.error('motionOps --width cannot be less than 1.')
    if(args.dilates < 0):
        log.error('motionOps --dilates cannot be less than 0')

    def write_starting_message(args):
        if(args.export_to_premiere):
            return 'Exporting to Adobe Premiere Pro XML file.'
        if(args.export_to_final_cut_pro):
            return 'Exporting to Final Cut Pro XML file.'
        if(args.export_to_resolve):
            return 'Exporting to DaVinci Resolve XML file.'
        if(args.export_to_shotcut):
            return 'Exporting to Shotcut XML Timeline file.'
        if(args.export_as_audio):
            return 'Exporting as audio.'
        return 'Starting.'

    if(not args.preview):
        log.conwrite(write_starting_message(args))

    if(args.preview or args.export_as_clip_sequence or making_data_file):
        args.no_open = True

    if(args.blur < 0):
        args.blur = 0

    if(args.silent_speed <= 0 or args.silent_speed > 99999):
        args.silent_speed = 99999

    if(args.video_speed <= 0 or args.video_speed > 99999):
        args.video_speed = 99999

    if(args.output_file is None):
        args.output_file = []

    from auto_editor.validate_input import valid_input
    input_list, segments = valid_input(args.input, ffmpeg, args, log)

    if(len(args.output_file) < len(input_list)):
        for i in range(len(input_list) - len(args.output_file)):
            args.output_file.append(set_output_name(input_list[i], None,
                making_data_file, args))

    if(args.combine_files):
        if(exporting_to_editor):
            temp_file = 'combined.mp4'
        else:
            temp_file = os.path.join(TEMP, 'combined.mp4')

        cmd = []
        for fileref in input_list:
            cmd.extend(['-i', fileref])
        cmd.extend(['-filter_complex', '[0:v]concat=n={}:v=1:a=1'.format(len(input_list)),
            '-codec:v', 'h264', '-pix_fmt', 'yuv420p', '-strict', '-2', temp_file])
        ffmpeg.run(cmd)
        del cmd
        input_list = [temp_file]

    speeds = [args.silent_speed, args.video_speed]
    if(args.cut_out != [] and 99999 not in speeds):
        speeds.append(99999)

    for item in args.set_speed_for_range:
        if(item[0] not in speeds):
            speeds.append(float(item[0]))

    log.debug('Speeds: {}'.format(speeds))

    def main_loop(input_list, ffmpeg, args, speeds, segments, log):
        num_cuts = 0

        progress = ProgressBar(args.machine_readable_progress, args.no_progress)

        for i, input_path in enumerate(input_list):
            inp = ffmpeg.file_info(input_path)

            if(len(input_list) > 1):
                log.conwrite('Working on {}'.format(inp.basename))

            cuts, output_path = edit_media(i, inp, ffmpeg, args, progress, speeds,
                segments[i], exporting_to_editor, making_data_file, TEMP, log)
            num_cuts += cuts

        if(not args.preview and not making_data_file):
            timer.stop()

        if(not args.preview and making_data_file):
            # Assume making each cut takes about 30 seconds.
            time_save = usefulfunctions.human_readable_time(num_cuts * 30)
            s = 's' if num_cuts != 1 else ''

            log.print('Auto-Editor made {} cut{}, which would have taken about {} if '
                'edited manually.'.format(num_cuts, s, time_save))

        if(not args.no_open):
            usefulfunctions.open_with_system_default(output_path, log)

    try:
        main_loop(input_list, ffmpeg, args, speeds, segments, log)
    except KeyboardInterrupt:
        log.error('Keyboard Interrupt')
    log.cleanup()","for item in args.set_speed_for_range:
    if item[0] not in speeds:
        speeds.append(float(item[0]))","for item in args.set_speed_for_range:
    (item_0, *item_ritemmaining) = item
    if item[0] not in speeds:
        speeds.append(float(item[0]))","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
networkx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/networkx/networkx/generators/line.py,https://github.com/networkx/networkx/tree/master/networkx/generators/line.py,,_odd_triangle$340,"def _odd_triangle(G, T):
    """"""Test whether T is an odd triangle in G

    Parameters
    ----------
    G : NetworkX Graph
    T : 3-tuple of vertices forming triangle in G

    Returns
    -------
    True is T is an odd triangle
    False otherwise

    Raises
    ------
    NetworkXError
        T is not a triangle in G

    Notes
    -----
    An odd triangle is one in which there exists another vertex in G which is
    adjacent to either exactly one or exactly all three of the vertices in the
    triangle.

    """"""
    for u in T:
        if u not in G.nodes():
            raise nx.NetworkXError(f""Vertex {u} not in graph"")
    for e in list(combinations(T, 2)):
        if e[0] not in G[e[1]]:
            raise nx.NetworkXError(f""Edge ({e[0]}, {e[1]}) not in graph"")

    T_neighbors = defaultdict(int)
    for t in T:
        for v in G[t]:
            if v not in T:
                T_neighbors[v] += 1
    for v in T_neighbors:
        if T_neighbors[v] in [1, 3]:
            return True
    return False","for e in list(combinations(T, 2)):
    if e[0] not in G[e[1]]:
        raise nx.NetworkXError(f'Edge ({e[0]}, {e[1]}) not in graph')","for e in list(combinations(T, 2)):
    (e_0, e_1, *_) = e
    if e_0 not in G[e_1]:
        raise nx.NetworkXError(f'Edge ({e_0}, {e_1}) not in graph')","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
PythonRobotics,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonRobotics/ArmNavigation/arm_obstacle_navigation/arm_obstacle_navigation_2.py,https://github.com/AtsushiSakai/PythonRobotics/tree/master/ArmNavigation/arm_obstacle_navigation/arm_obstacle_navigation_2.py,NLinkArm,plot_arm$278,"def plot_arm(self, myplt, obstacles=[]):  # pragma: no cover
        myplt.cla()

        for obstacle in obstacles:
            circle = myplt.Circle(
                (obstacle[0], obstacle[1]), radius=0.5 * obstacle[2], fc='k')
            myplt.gca().add_patch(circle)

        for i in range(self.n_links + 1):
            if i is not self.n_links:
                myplt.plot([self.points[i][0], self.points[i + 1][0]],
                           [self.points[i][1], self.points[i + 1][1]], 'r-')
            myplt.plot(self.points[i][0], self.points[i][1], 'k.')

        myplt.xlim([-self.lim, self.lim])
        myplt.ylim([-self.lim, self.lim])
        myplt.draw()","for obstacle in obstacles:
    circle = myplt.Circle((obstacle[0], obstacle[1]), radius=0.5 * obstacle[2], fc='k')
    myplt.gca().add_patch(circle)","for obstacle in obstacles:
    (obstacle_0, obstacle_1, obstacle_2, *_) = obstacle
    circle = myplt.Circle((obstacle[0], obstacle[1]), radius=0.5 * obstacle[2], fc='k')
    myplt.gca().add_patch(circle)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/ansible_test/unit/test_diff.py,https://github.com/ansible/ansible/tree/master/test/ansible_test/unit/test_diff.py,,get_parsed_diff$45,"def get_parsed_diff(base, head=None):
    """"""Return a parsed git diff between the base and head revision.
    :type base: str
    :type head: str | None
    :rtype: list[FileDiff]
    """"""
    lines = get_diff(base, head)
    items = parse_diff(lines)

    assert items

    for item in items:
        assert item.headers
        assert item.is_complete

        item.old.format_lines()
        item.new.format_lines()

        for line_range in item.old.ranges:
            assert line_range[1] >= line_range[0] > 0

        for line_range in item.new.ranges:
            assert line_range[1] >= line_range[0] > 0

    return items","for line_range in item.old.ranges:
    assert line_range[1] >= line_range[0] > 0","for line_range in item.old.ranges:
    (line_range_0, line_range_1, *_) = line_range
    assert line_range[1] >= line_range[0] > 0","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/ansible_test/unit/test_diff.py,https://github.com/ansible/ansible/tree/master/test/ansible_test/unit/test_diff.py,,get_parsed_diff$45,"def get_parsed_diff(base, head=None):
    """"""Return a parsed git diff between the base and head revision.
    :type base: str
    :type head: str | None
    :rtype: list[FileDiff]
    """"""
    lines = get_diff(base, head)
    items = parse_diff(lines)

    assert items

    for item in items:
        assert item.headers
        assert item.is_complete

        item.old.format_lines()
        item.new.format_lines()

        for line_range in item.old.ranges:
            assert line_range[1] >= line_range[0] > 0

        for line_range in item.new.ranges:
            assert line_range[1] >= line_range[0] > 0

    return items","for line_range in item.new.ranges:
    assert line_range[1] >= line_range[0] > 0","for line_range in item.new.ranges:
    (line_range_0, line_range_1, *_) = line_range
    assert line_range[1] >= line_range[0] > 0","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/head_detection/data_provider_farm/reformat_brainwash.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/head_detection/data_provider_farm/reformat_brainwash.py,,generate_data_list$12,"def generate_data_list():
    txt_file_path = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash/brainwash_test.idl'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash'

    list_file_path = './data_folder/data_list_brainwash_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fin = open(txt_file_path, 'r')
    fout = open(list_file_path, 'w')

    counter = 0
    for line in fin:
        line = line.strip(';\n')
        im_path = re.findall('[""](.*?)[""]', line)[0]
        im_path = os.path.join(image_root, im_path)
        if not os.path.exists(im_path):
            print('im file does not exist : %s'%im_path)
            continue
        bbox_str_list = re.findall('[(](.*?)[)]', line)
        bbox_list = []
        for bbox_str in bbox_str_list:
            bbox_str = bbox_str.split(', ')
            xmin = int(float(bbox_str[0]))
            ymin = int(float(bbox_str[1]))
            xmax = int(float(bbox_str[2]))
            ymax = int(float(bbox_str[3]))
            bbox_list.append((xmin, ymin, xmax-xmin+1, ymax-ymin+1))

        if len(bbox_list) == 0:
            line_str = im_path+',0,0'
            fout.write(line_str+'\n')
        else:
            line_str = im_path+',1,'+str(len(bbox_list))
            for bbox in bbox_list:
                line_str += ','+str(bbox[0])+','+str(bbox[1])+','+str(bbox[2])+','+str(bbox[3])
            fout.write(line_str + '\n')
        counter += 1
        print(counter)

    fout.close()
    fin.close()","for bbox_str in bbox_str_list:
    bbox_str = bbox_str.split(', ')
    xmin = int(float(bbox_str[0]))
    ymin = int(float(bbox_str[1]))
    xmax = int(float(bbox_str[2]))
    ymax = int(float(bbox_str[3]))
    bbox_list.append((xmin, ymin, xmax - xmin + 1, ymax - ymin + 1))","for bbox_str in bbox_str_list:
    (bbox_str_0, bbox_str_1, bbox_str_2, bbox_str_3, *_) = bbox_str
    bbox_str = bbox_str.split(', ')
    xmin = int(float(bbox_str[0]))
    ymin = int(float(bbox_str[1]))
    xmax = int(float(bbox_str[2]))
    ymax = int(float(bbox_str[3]))
    bbox_list.append((xmin, ymin, xmax - xmin + 1, ymax - ymin + 1))",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/head_detection/data_provider_farm/reformat_brainwash.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/head_detection/data_provider_farm/reformat_brainwash.py,,generate_data_list$12,"def generate_data_list():
    txt_file_path = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash/brainwash_test.idl'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash'

    list_file_path = './data_folder/data_list_brainwash_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fin = open(txt_file_path, 'r')
    fout = open(list_file_path, 'w')

    counter = 0
    for line in fin:
        line = line.strip(';\n')
        im_path = re.findall('[""](.*?)[""]', line)[0]
        im_path = os.path.join(image_root, im_path)
        if not os.path.exists(im_path):
            print('im file does not exist : %s'%im_path)
            continue
        bbox_str_list = re.findall('[(](.*?)[)]', line)
        bbox_list = []
        for bbox_str in bbox_str_list:
            bbox_str = bbox_str.split(', ')
            xmin = int(float(bbox_str[0]))
            ymin = int(float(bbox_str[1]))
            xmax = int(float(bbox_str[2]))
            ymax = int(float(bbox_str[3]))
            bbox_list.append((xmin, ymin, xmax-xmin+1, ymax-ymin+1))

        if len(bbox_list) == 0:
            line_str = im_path+',0,0'
            fout.write(line_str+'\n')
        else:
            line_str = im_path+',1,'+str(len(bbox_list))
            for bbox in bbox_list:
                line_str += ','+str(bbox[0])+','+str(bbox[1])+','+str(bbox[2])+','+str(bbox[3])
            fout.write(line_str + '\n')
        counter += 1
        print(counter)

    fout.close()
    fin.close()","for bbox in bbox_list:
    line_str += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])","for bbox in bbox_list:
    (bbox_0, bbox_1, bbox_2, bbox_3, *_) = bbox
    line_str += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/base/check_legacy_includes/fsc_sc2.py,https://github.com/tribe29/checkmk/tree/master/cmk/base/check_legacy_includes/fsc_sc2.py,,inventory_fsc_sc2_temp$318,"def inventory_fsc_sc2_temp(info):
    for line in info:
        if line[1] != ""2"":
            yield line[0], {}","for line in info:
    if line[1] != '2':
        yield (line[0], {})","for line in info:
    (line_0, line_1, *_) = line
    if line[1] != '2':
        yield (line[0], {})","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
werkzeug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/werkzeug/src/werkzeug/datastructures.py,https://github.com/pallets/werkzeug/tree/master/src/werkzeug/datastructures.py,MultiDict,values$497,"def values(self):
        """"""Returns an iterator of the first value on every key's value list.""""""
        for values in dict.values(self):
            yield values[0]","for values in dict.values(self):
    yield values[0]","for values in dict.values(self):
    (values_0, *values_rvaluesmaining) = values
    yield values[0]","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/units/module_utils/common/validation/test_check_type_dict.py,https://github.com/ansible/ansible/tree/master/test/units/module_utils/common/validation/test_check_type_dict.py,,test_check_type_dict$13,"def test_check_type_dict():
    test_cases = (
        ({'k1': 'v1'}, {'k1': 'v1'}),
        ('k1=v1,k2=v2', {'k1': 'v1', 'k2': 'v2'}),
        ('k1=v1, k2=v2', {'k1': 'v1', 'k2': 'v2'}),
        ('k1=v1,     k2=v2,  k3=v3', {'k1': 'v1', 'k2': 'v2', 'k3': 'v3'}),
        ('{""key"": ""value"", ""list"": [""one"", ""two""]}', {'key': 'value', 'list': ['one', 'two']})
    )
    for case in test_cases:
        assert case[1] == check_type_dict(case[0])","for case in test_cases:
    assert case[1] == check_type_dict(case[0])","for case in test_cases:
    (case_0, case_1, *_) = case
    assert case[1] == check_type_dict(case[0])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Scout2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Scout2/AWSScout2/output/console.py,https://github.com/nccgroup/Scout2/tree/master/AWSScout2/output/console.py,,format_listall_output$16,"def format_listall_output(format_file, format_item_dir, format, rule, option_prefix = None, template = None, skip_options = False):
    """"""
    Prepare listall output template

    :param format_file:
    :param format_item_dir:
    :param format:
    :param config:
    :param option_prefix:
    :param template:
    :param skip_options:
    :return:
    """"""
    # Set the list of keys if printing from a file spec
    # _LINE_(whatever)_EOL_
    # _ITEM_(resource)_METI_
    # _KEY_(path_to_value)
    if format_file and os.path.isfile(format_file):
        if not template:
            with open(format_file, 'rt') as f:
                template = f.read()
        # Optional files
        if not skip_options:
            re_option = re.compile(r'(%_OPTION_\((.*?)\)_NOITPO_)')
            optional_files = re_option.findall(template)
            for optional_file in optional_files:
                if optional_file[1].startswith(option_prefix + '-'):
                    with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:
                        template = template.replace(optional_file[0].strip(), f.read())
        # Include files if needed
        re_file = re.compile(r'(_FILE_\((.*?)\)_ELIF_)')
        while True:
            requested_files = re_file.findall(template)
            available_files = os.listdir(format_item_dir) if format_item_dir else []
            for requested_file in requested_files:
                if requested_file[1].strip() in available_files:
                    with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
                        template = template.replace(requested_file[0].strip(), f.read())
            # Find items and keys to be printed
            re_line = re.compile(r'(_ITEM_\((.*?)\)_METI_)')
            re_key = re.compile(r'_KEY_\(*(.*?)\)', re.DOTALL|re.MULTILINE) # Remove the multiline ?
            lines = re_line.findall(template)
            for (i, line) in enumerate(lines):
                lines[i] = line + (re_key.findall(line[1]),)
            requested_files = re_file.findall(template)
            if len(requested_files) == 0:
                break
    elif format and format[0] == 'csv':
        keys = rule.keys
        line = ', '.join('_KEY_(%s)' % k for k in keys)
        lines = [ (line, line, keys) ]
        template = line
    return (lines, template)","for optional_file in optional_files:
    if optional_file[1].startswith(option_prefix + '-'):
        with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:
            template = template.replace(optional_file[0].strip(), f.read())","for optional_file in optional_files:
    (optional_file_0, optional_file_1, *_) = optional_file
    if optional_file[1].startswith(option_prefix + '-'):
        with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:
            template = template.replace(optional_file[0].strip(), f.read())","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Scout2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Scout2/AWSScout2/output/console.py,https://github.com/nccgroup/Scout2/tree/master/AWSScout2/output/console.py,,format_listall_output$16,"def format_listall_output(format_file, format_item_dir, format, rule, option_prefix = None, template = None, skip_options = False):
    """"""
    Prepare listall output template

    :param format_file:
    :param format_item_dir:
    :param format:
    :param config:
    :param option_prefix:
    :param template:
    :param skip_options:
    :return:
    """"""
    # Set the list of keys if printing from a file spec
    # _LINE_(whatever)_EOL_
    # _ITEM_(resource)_METI_
    # _KEY_(path_to_value)
    if format_file and os.path.isfile(format_file):
        if not template:
            with open(format_file, 'rt') as f:
                template = f.read()
        # Optional files
        if not skip_options:
            re_option = re.compile(r'(%_OPTION_\((.*?)\)_NOITPO_)')
            optional_files = re_option.findall(template)
            for optional_file in optional_files:
                if optional_file[1].startswith(option_prefix + '-'):
                    with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:
                        template = template.replace(optional_file[0].strip(), f.read())
        # Include files if needed
        re_file = re.compile(r'(_FILE_\((.*?)\)_ELIF_)')
        while True:
            requested_files = re_file.findall(template)
            available_files = os.listdir(format_item_dir) if format_item_dir else []
            for requested_file in requested_files:
                if requested_file[1].strip() in available_files:
                    with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
                        template = template.replace(requested_file[0].strip(), f.read())
            # Find items and keys to be printed
            re_line = re.compile(r'(_ITEM_\((.*?)\)_METI_)')
            re_key = re.compile(r'_KEY_\(*(.*?)\)', re.DOTALL|re.MULTILINE) # Remove the multiline ?
            lines = re_line.findall(template)
            for (i, line) in enumerate(lines):
                lines[i] = line + (re_key.findall(line[1]),)
            requested_files = re_file.findall(template)
            if len(requested_files) == 0:
                break
    elif format and format[0] == 'csv':
        keys = rule.keys
        line = ', '.join('_KEY_(%s)' % k for k in keys)
        lines = [ (line, line, keys) ]
        template = line
    return (lines, template)","for requested_file in requested_files:
    if requested_file[1].strip() in available_files:
        with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
            template = template.replace(requested_file[0].strip(), f.read())","for requested_file in requested_files:
    (requested_file_0, requested_file_1, *_) = requested_file
    if requested_file[1].strip() in available_files:
        with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
            template = template.replace(requested_file[0].strip(), f.read())","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
Scout2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Scout2/AWSScout2/output/console.py,https://github.com/nccgroup/Scout2/tree/master/AWSScout2/output/console.py,,format_listall_output$16,"def format_listall_output(format_file, format_item_dir, format, rule, option_prefix = None, template = None, skip_options = False):
    """"""
    Prepare listall output template

    :param format_file:
    :param format_item_dir:
    :param format:
    :param config:
    :param option_prefix:
    :param template:
    :param skip_options:
    :return:
    """"""
    # Set the list of keys if printing from a file spec
    # _LINE_(whatever)_EOL_
    # _ITEM_(resource)_METI_
    # _KEY_(path_to_value)
    if format_file and os.path.isfile(format_file):
        if not template:
            with open(format_file, 'rt') as f:
                template = f.read()
        # Optional files
        if not skip_options:
            re_option = re.compile(r'(%_OPTION_\((.*?)\)_NOITPO_)')
            optional_files = re_option.findall(template)
            for optional_file in optional_files:
                if optional_file[1].startswith(option_prefix + '-'):
                    with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:
                        template = template.replace(optional_file[0].strip(), f.read())
        # Include files if needed
        re_file = re.compile(r'(_FILE_\((.*?)\)_ELIF_)')
        while True:
            requested_files = re_file.findall(template)
            available_files = os.listdir(format_item_dir) if format_item_dir else []
            for requested_file in requested_files:
                if requested_file[1].strip() in available_files:
                    with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
                        template = template.replace(requested_file[0].strip(), f.read())
            # Find items and keys to be printed
            re_line = re.compile(r'(_ITEM_\((.*?)\)_METI_)')
            re_key = re.compile(r'_KEY_\(*(.*?)\)', re.DOTALL|re.MULTILINE) # Remove the multiline ?
            lines = re_line.findall(template)
            for (i, line) in enumerate(lines):
                lines[i] = line + (re_key.findall(line[1]),)
            requested_files = re_file.findall(template)
            if len(requested_files) == 0:
                break
    elif format and format[0] == 'csv':
        keys = rule.keys
        line = ', '.join('_KEY_(%s)' % k for k in keys)
        lines = [ (line, line, keys) ]
        template = line
    return (lines, template)","for (i, line) in enumerate(lines):
    lines[i] = line + (re_key.findall(line[1]),)","for (i, line) in enumerate(lines):
    (_, line_1, *line_rlinemaining) = line
    lines[i] = line + (re_key.findall(line[1]),)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
flow-forecast,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow-forecast/flood_forecast/trainer.py,https://github.com/AIStream-Peelout/flow-forecast/tree/master/flood_forecast/trainer.py,,handle_model_evaluation1$16,"def handle_model_evaluation1(trained_model, params: Dict, model_type: str) -> None:
    """"""Utility function to help handle model evaluation. Primarily used at the moment for forcast

    :param trained_model: A PyTorchForecast model that has already been trained. 
    :type trained_model: PyTorchForecast
    :param params: A dictionary of the trained model parameters.
    :type params: Dict
    :param model_type: The type of model. Almost always PyTorch in practice.
    :type model_type: str
    """"""
    test_acc = evaluate_model(
            trained_model,
            model_type,
            params[""dataset_params""][""target_col""],
            params[""metrics""],
            params[""inference_params""],
            {})
    wandb.run.summary[""test_accuracy""] = test_acc[0]
    df_train_and_test = test_acc[1]
    forecast_start_idx = test_acc[2]
    df_prediction_samples = test_acc[3]
    mae = (df_train_and_test.loc[forecast_start_idx:, ""preds""] -
            df_train_and_test.loc[forecast_start_idx:, params[""dataset_params""][""target_col""][0]]).abs()
    inverse_mae = 1 / mae
    i = 0
    for df in df_prediction_samples:
        pred_std = df.std(axis=1)
        average_prediction_sharpe = (inverse_mae / pred_std).mean()
        wandb.log({'average_prediction_sharpe' + str(i): average_prediction_sharpe})
        i += 1
    df_train_and_test.to_csv(""temp_preds.csv"")
    # Log plots now
    if ""probabilistic"" in params[""inference_params""]:
        test_plot = plot_df_test_with_probabilistic_confidence_interval(
            df_train_and_test,
            forecast_start_idx,
            params,)
    elif len(df_prediction_samples) > 0:
        for thing in zip(df_prediction_samples, params[""dataset_params""][""target_col""]):
            thing[0].to_csv(thing[1] + "".csv"")
            test_plot = plot_df_test_with_confidence_interval(
                df_train_and_test,
                thing[0],
                forecast_start_idx,
                params,
                targ_col=thing[1],
                ci=95,
                alpha=0.25)
            wandb.log({""test_plot_"" + thing[1]: test_plot})
    else:
        pd.options.plotting.backend = ""plotly""
        t = params[""dataset_params""][""target_col""][0]
        test_plot = df_train_and_test[[t, ""preds""]].plot()
        wandb.log({""test_plot_"" + t: test_plot})
    print(""Now plotting final plots"")
    test_plot_all = go.Figure()
    for relevant_col in params[""dataset_params""][""relevant_cols""]:
        test_plot_all.add_trace(
            go.Scatter(
                x=df_train_and_test.index,
                y=df_train_and_test[relevant_col],
                name=relevant_col))
    wandb.log({""test_plot_all"": test_plot_all})","for thing in zip(df_prediction_samples, params['dataset_params']['target_col']):
    thing[0].to_csv(thing[1] + '.csv')
    test_plot = plot_df_test_with_confidence_interval(df_train_and_test, thing[0], forecast_start_idx, params, targ_col=thing[1], ci=95, alpha=0.25)
    wandb.log({'test_plot_' + thing[1]: test_plot})","for thing in zip(df_prediction_samples, params['dataset_params']['target_col']):
    (thing_0, thing_1, *_) = thing
    thing[0].to_csv(thing[1] + '.csv')
    test_plot = plot_df_test_with_confidence_interval(df_train_and_test, thing[0], forecast_start_idx, params, targ_col=thing[1], ci=95, alpha=0.25)
    wandb.log({'test_plot_' + thing[1]: test_plot})","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
xbmc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xbmc/plugin.video.amazon-test/resources/lib/amazontld.py,https://github.com/Sandmann79/xbmc/tree/master/plugin.video.amazon-test/resources/lib/amazontld.py,AmazonTLD,getcache$1065,"def getcache(uid):
            j = {}
            c = self._menuDb.cursor()
            for data in c.execute('select data from channels where uid = (?)', (uid,)).fetchall():
                j = json.loads(data[0])
            c.close()
            return j","for data in c.execute('select data from channels where uid = (?)', (uid,)).fetchall():
    j = json.loads(data[0])","for data in c.execute('select data from channels where uid = (?)', (uid,)).fetchall():
    (data_0, *data_rdatamaining) = data
    j = json.loads(data[0])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/full_sync.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/full_sync.py,FullSync,remove_library$524,"def remove_library(self, library_id, dialog):

        ''' Remove library by their id from the Kodi database.
        '''
        direct_path = self.library.direct_path

        with Database('jellyfin') as jellyfindb:

            db = jellyfin_db.JellyfinDatabase(jellyfindb.cursor)
            library = db.get_view(library_id.replace('Mixed:', """"))
            items = db.get_item_by_media_folder(library_id.replace('Mixed:', """"))
            media = 'music' if library.media_type == 'music' else 'video'

            if media == 'music':
                settings('MusicRescan.bool', False)

            if items:
                with self.library.music_database_lock if media == 'music' else self.library.database_lock:
                    with Database(media) as kodidb:

                        count = 0

                        if library.media_type == 'mixed':

                            movies = [x for x in items if x[1] == 'Movie']
                            tvshows = [x for x in items if x[1] == 'Series']

                            obj = Movies(self.server, jellyfindb, kodidb, direct_path, library).remove

                            for item in movies:

                                obj(item[0])
                                dialog.update(int((float(count) / float(len(items)) * 100)), heading=""%s: %s"" % (translate('addon_name'), library.view_name))
                                count += 1

                            obj = TVShows(self.server, jellyfindb, kodidb, direct_path, library).remove

                            for item in tvshows:

                                obj(item[0])
                                dialog.update(int((float(count) / float(len(items)) * 100)), heading=""%s: %s"" % (translate('addon_name'), library.view_name))
                                count += 1
                        else:
                            default_args = (self.server, jellyfindb, kodidb, direct_path)
                            for item in items:
                                if item[1] in ('Series', 'Season', 'Episode'):
                                    TVShows(*default_args).remove(item[0])
                                elif item[1] in ('Movie', 'BoxSet'):
                                    Movies(*default_args).remove(item[0])
                                elif item[1] in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):
                                    Music(*default_args).remove(item[0])
                                elif item[1] == 'MusicVideo':
                                    MusicVideos(*default_args).remove(item[0])

                                dialog.update(int((float(count) / float(len(items)) * 100)), heading=""%s: %s"" % (translate('addon_name'), library[0]))
                                count += 1

        self.sync = get_sync()

        if library_id in self.sync['Whitelist']:
            self.sync['Whitelist'].remove(library_id)

        elif 'Mixed:%s' % library_id in self.sync['Whitelist']:
            self.sync['Whitelist'].remove('Mixed:%s' % library_id)

        save_sync(self.sync)","for item in movies:
    obj(item[0])
    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
    count += 1","for item in movies:
    (item_0, *item_ritemmaining) = item
    obj(item[0])
    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
    count += 1","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/full_sync.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/full_sync.py,FullSync,remove_library$524,"def remove_library(self, library_id, dialog):

        ''' Remove library by their id from the Kodi database.
        '''
        direct_path = self.library.direct_path

        with Database('jellyfin') as jellyfindb:

            db = jellyfin_db.JellyfinDatabase(jellyfindb.cursor)
            library = db.get_view(library_id.replace('Mixed:', """"))
            items = db.get_item_by_media_folder(library_id.replace('Mixed:', """"))
            media = 'music' if library.media_type == 'music' else 'video'

            if media == 'music':
                settings('MusicRescan.bool', False)

            if items:
                with self.library.music_database_lock if media == 'music' else self.library.database_lock:
                    with Database(media) as kodidb:

                        count = 0

                        if library.media_type == 'mixed':

                            movies = [x for x in items if x[1] == 'Movie']
                            tvshows = [x for x in items if x[1] == 'Series']

                            obj = Movies(self.server, jellyfindb, kodidb, direct_path, library).remove

                            for item in movies:

                                obj(item[0])
                                dialog.update(int((float(count) / float(len(items)) * 100)), heading=""%s: %s"" % (translate('addon_name'), library.view_name))
                                count += 1

                            obj = TVShows(self.server, jellyfindb, kodidb, direct_path, library).remove

                            for item in tvshows:

                                obj(item[0])
                                dialog.update(int((float(count) / float(len(items)) * 100)), heading=""%s: %s"" % (translate('addon_name'), library.view_name))
                                count += 1
                        else:
                            default_args = (self.server, jellyfindb, kodidb, direct_path)
                            for item in items:
                                if item[1] in ('Series', 'Season', 'Episode'):
                                    TVShows(*default_args).remove(item[0])
                                elif item[1] in ('Movie', 'BoxSet'):
                                    Movies(*default_args).remove(item[0])
                                elif item[1] in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):
                                    Music(*default_args).remove(item[0])
                                elif item[1] == 'MusicVideo':
                                    MusicVideos(*default_args).remove(item[0])

                                dialog.update(int((float(count) / float(len(items)) * 100)), heading=""%s: %s"" % (translate('addon_name'), library[0]))
                                count += 1

        self.sync = get_sync()

        if library_id in self.sync['Whitelist']:
            self.sync['Whitelist'].remove(library_id)

        elif 'Mixed:%s' % library_id in self.sync['Whitelist']:
            self.sync['Whitelist'].remove('Mixed:%s' % library_id)

        save_sync(self.sync)","for item in tvshows:
    obj(item[0])
    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
    count += 1","for item in tvshows:
    (item_0, *item_ritemmaining) = item
    obj(item[0])
    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
    count += 1","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/full_sync.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/full_sync.py,FullSync,remove_library$524,"def remove_library(self, library_id, dialog):

        ''' Remove library by their id from the Kodi database.
        '''
        direct_path = self.library.direct_path

        with Database('jellyfin') as jellyfindb:

            db = jellyfin_db.JellyfinDatabase(jellyfindb.cursor)
            library = db.get_view(library_id.replace('Mixed:', """"))
            items = db.get_item_by_media_folder(library_id.replace('Mixed:', """"))
            media = 'music' if library.media_type == 'music' else 'video'

            if media == 'music':
                settings('MusicRescan.bool', False)

            if items:
                with self.library.music_database_lock if media == 'music' else self.library.database_lock:
                    with Database(media) as kodidb:

                        count = 0

                        if library.media_type == 'mixed':

                            movies = [x for x in items if x[1] == 'Movie']
                            tvshows = [x for x in items if x[1] == 'Series']

                            obj = Movies(self.server, jellyfindb, kodidb, direct_path, library).remove

                            for item in movies:

                                obj(item[0])
                                dialog.update(int((float(count) / float(len(items)) * 100)), heading=""%s: %s"" % (translate('addon_name'), library.view_name))
                                count += 1

                            obj = TVShows(self.server, jellyfindb, kodidb, direct_path, library).remove

                            for item in tvshows:

                                obj(item[0])
                                dialog.update(int((float(count) / float(len(items)) * 100)), heading=""%s: %s"" % (translate('addon_name'), library.view_name))
                                count += 1
                        else:
                            default_args = (self.server, jellyfindb, kodidb, direct_path)
                            for item in items:
                                if item[1] in ('Series', 'Season', 'Episode'):
                                    TVShows(*default_args).remove(item[0])
                                elif item[1] in ('Movie', 'BoxSet'):
                                    Movies(*default_args).remove(item[0])
                                elif item[1] in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):
                                    Music(*default_args).remove(item[0])
                                elif item[1] == 'MusicVideo':
                                    MusicVideos(*default_args).remove(item[0])

                                dialog.update(int((float(count) / float(len(items)) * 100)), heading=""%s: %s"" % (translate('addon_name'), library[0]))
                                count += 1

        self.sync = get_sync()

        if library_id in self.sync['Whitelist']:
            self.sync['Whitelist'].remove(library_id)

        elif 'Mixed:%s' % library_id in self.sync['Whitelist']:
            self.sync['Whitelist'].remove('Mixed:%s' % library_id)

        save_sync(self.sync)","for item in items:
    if item[1] in ('Series', 'Season', 'Episode'):
        TVShows(*default_args).remove(item[0])
    elif item[1] in ('Movie', 'BoxSet'):
        Movies(*default_args).remove(item[0])
    elif item[1] in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):
        Music(*default_args).remove(item[0])
    elif item[1] == 'MusicVideo':
        MusicVideos(*default_args).remove(item[0])
    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library[0]))
    count += 1","for item in items:
    (item_0, item_1, *_) = item
    if item[1] in ('Series', 'Season', 'Episode'):
        TVShows(*default_args).remove(item[0])
    elif item[1] in ('Movie', 'BoxSet'):
        Movies(*default_args).remove(item[0])
    elif item[1] in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):
        Music(*default_args).remove(item[0])
    elif item[1] == 'MusicVideo':
        MusicVideos(*default_args).remove(item[0])
    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library[0]))
    count += 1","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
ros,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros/tools/rosunit/src/rosunit/baretest.py,https://github.com/ros/ros/tree/master/tools/rosunit/src/rosunit/baretest.py,,_format_errors$528,"def _format_errors(errors):
    formatted = []
    for e in errors:
        if '_testMethodName' in e[0].__dict__:
            formatted.append(e[0]._testMethodName)
        elif 'description' in e[0].__dict__:
            formatted.append('%s: %s\n' % (str(e[0].description), str(e[1])))
        else:
            formatted.append(str(e[0].__dict__))
    return formatted","for e in errors:
    if '_testMethodName' in e[0].__dict__:
        formatted.append(e[0]._testMethodName)
    elif 'description' in e[0].__dict__:
        formatted.append('%s: %s\n' % (str(e[0].description), str(e[1])))
    else:
        formatted.append(str(e[0].__dict__))","for e in errors:
    (e_0, e_1, *_) = e
    if '_testMethodName' in e_0.__dict__:
        formatted.append(e_0._testMethodName)
    elif 'description' in e_0.__dict__:
        formatted.append('%s: %s\n' % (str(e_0.description), str(e_1)))
    else:
        formatted.append(str(e_0.__dict__))","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/homeassistant/components/danfoss_air/binary_sensor.py,https://github.com/home-assistant/core/tree/master/homeassistant/components/danfoss_air/binary_sensor.py,,setup_platform$12,"def setup_platform(hass, config, add_entities, discovery_info=None):
    """"""Set up the available Danfoss Air sensors etc.""""""
    data = hass.data[DANFOSS_AIR_DOMAIN]

    sensors = [
        [""Danfoss Air Bypass Active"", ReadCommand.bypass, DEVICE_CLASS_OPENING],
        [""Danfoss Air Away Mode Active"", ReadCommand.away_mode, None],
    ]

    dev = []

    for sensor in sensors:
        dev.append(DanfossAirBinarySensor(data, sensor[0], sensor[1], sensor[2]))

    add_entities(dev, True)","for sensor in sensors:
    dev.append(DanfossAirBinarySensor(data, sensor[0], sensor[1], sensor[2]))","for sensor in sensors:
    (sensor_0, sensor_1, sensor_2, *_) = sensor
    dev.append(DanfossAirBinarySensor(data, sensor[0], sensor[1], sensor[2]))","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
st2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/st2/contrib/runners/action_chain_runner/tests/unit/test_actionchain.py,https://github.com/StackStorm/st2/tree/master/contrib/runners/action_chain_runner/tests/unit/test_actionchain.py,TestActionChainRunner,test_chain_runner_dependent_param_temp$559,"def test_chain_runner_dependent_param_temp(self, request):
        chain_runner = acr.get_runner()
        chain_runner.entry_point = CHAIN_DEP_INPUT
        chain_runner.action = ACTION_1
        action_ref = ResourceReference.to_string_reference(
            name=ACTION_1.name, pack=ACTION_1.pack
        )
        chain_runner.liveaction = LiveActionDB(action=action_ref)
        chain_runner.pre_run()
        chain_runner.run({""s1"": 1, ""s2"": 2, ""s3"": 3, ""s4"": 4})
        self.assertNotEqual(chain_runner.chain_holder.actionchain, None)
        expected_values = [{""p1"": ""1""}, {""p1"": ""1""}, {""p2"": ""1"", ""p3"": ""1"", ""p1"": ""1""}]
        # Each of the call_args must be one of
        for call_args in request.call_args_list:
            self.assertIn(call_args[0][0].parameters, expected_values)
            expected_values.remove(call_args[0][0].parameters)
        self.assertEqual(len(expected_values), 0, ""Not all expected values received."")","for call_args in request.call_args_list:
    self.assertIn(call_args[0][0].parameters, expected_values)
    expected_values.remove(call_args[0][0].parameters)","for call_args in request.call_args_list:
    ((call_args_0_0, _, *call_args_0_rcall_argsmaining), *call_args_rcall_argsmaining) = call_args
    self.assertIn(call_args[0][0].parameters, expected_values)
    expected_values.remove(call_args[0][0].parameters)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",0,,,"Answer: Yes
Iterable Unpacking: (e_0_0, _, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]",,,,,,,
whatportis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/whatportis/tests/conftest.py,https://github.com/ncrocfer/whatportis/tree/master/tests/conftest.py,,create_ports$16,"def create_ports(tmpdir, monkeypatch):
    def _create_ports(ports):
        def get_db():
            tmp_db = tmpdir.join(""db.json"")
            db = TinyDB(str(tmp_db), storage=CachingMiddleware(JSONStorage))

            for port in ports:
                db.insert(
                    {
                        ""name"": port[0],
                        ""port"": port[1],
                        ""description"": port[2],
                        ""protocol"": port[3],
                    }
                )
            return db

        return monkeypatch.setattr(whatportis.db, ""get_database"", get_db)

    return _create_ports","for port in ports:
    db.insert({'name': port[0], 'port': port[1], 'description': port[2], 'protocol': port[3]})","for port in ports:
    (port_0, port_1, port_2, port_3, *_) = port
    db.insert({'name': port[0], 'port': port[1], 'description': port[2], 'protocol': port[3]})","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
freeipa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipatests/test_integration/test_epn.py,https://github.com/freeipa/freeipa/tree/master/ipatests/test_integration/test_epn.py,TestEPN,test_EPN_nbdays_1$559,"def test_EPN_nbdays_1(self, cleanupmail):
        """"""Test that for a given range, we find the users in that range""""""

        # Use hardcoded date ranges for now
        for date_range in [(0, 5), (7, 15), (1, 20)]:
            expected_user_list = [""user{i}"".format(i=i)
                                  for i in range(date_range[0], date_range[1])]
            (stdout_text_client, unused, _unused) = self._check_epn_output(
                self.clients[0],
                from_nbdays=date_range[0],
                to_nbdays=date_range[1]
            )
            user_list = [user[""uid""] for user in json.loads(stdout_text_client)]
            for user in expected_user_list:
                assert user in user_list
            for user in user_list:
                assert user in expected_user_list","for date_range in [(0, 5), (7, 15), (1, 20)]:
    expected_user_list = ['user{i}'.format(i=i) for i in range(date_range[0], date_range[1])]
    (stdout_text_client, unused, _unused) = self._check_epn_output(self.clients[0], from_nbdays=date_range[0], to_nbdays=date_range[1])
    user_list = [user['uid'] for user in json.loads(stdout_text_client)]
    for user in expected_user_list:
        assert user in user_list
    for user in user_list:
        assert user in expected_user_list","for date_range in [(0, 5), (7, 15), (1, 20)]:
    (date_range_0, date_range_1, *_) = date_range
    expected_user_list = ['user{i}'.format(i=i) for i in range(date_range[0], date_range[1])]
    (stdout_text_client, unused, _unused) = self._check_epn_output(self.clients[0], from_nbdays=date_range[0], to_nbdays=date_range[1])
    user_list = [user['uid'] for user in json.loads(stdout_text_client)]
    for user in expected_user_list:
        assert user in user_list
    for user in user_list:
        assert user in expected_user_list","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/tests/test_util.py,https://github.com/cloudtools/stacker/tree/master/stacker/tests/test_util.py,TestUtil,test_merge_map$82,"def test_merge_map(self):
        tests = [
            # 2 lists of stacks defined
            [{'stacks': [{'stack1': {'variables': {'a': 'b'}}}]},
             {'stacks': [{'stack2': {'variables': {'c': 'd'}}}]},
             {'stacks': [
                 {'stack1': {
                     'variables': {
                         'a': 'b'}}},
                 {'stack2': {
                     'variables': {
                         'c': 'd'}}}]}],
            # A list of stacks combined with a higher precedence dict of stacks
            [{'stacks': [{'stack1': {'variables': {'a': 'b'}}}]},
             {'stacks': {'stack2': {'variables': {'c': 'd'}}}},
             {'stacks': {'stack2': {'variables': {'c': 'd'}}}}],
            # 2 dicts of stacks with non-overlapping variables merged
            [{'stacks': {'stack1': {'variables': {'a': 'b'}}}},
             {'stacks': {'stack1': {'variables': {'c': 'd'}}}},
             {'stacks': {
                 'stack1': {
                     'variables': {
                         'a': 'b',
                         'c': 'd'}}}}],
            # 2 dicts of stacks with overlapping variables merged
            [{'stacks': {'stack1': {'variables': {'a': 'b'}}}},
             {'stacks': {'stack1': {'variables': {'a': 'c'}}}},
             {'stacks': {'stack1': {'variables': {'a': 'c'}}}}],
        ]
        for t in tests:
            self.assertEqual(merge_map(t[0], t[1]), t[2])","for t in tests:
    self.assertEqual(merge_map(t[0], t[1]), t[2])","for t in tests:
    (t_0, t_1, t_2, *_) = t
    self.assertEqual(merge_map(t[0], t[1]), t[2])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
sympy_gamma,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy_gamma/app/views.py,https://github.com/sympy/sympy_gamma/tree/master/app/views.py,,random_example$132,"def random_example(request):
    examples = []

    for category in EXAMPLES:
        for subcategory in category[1]:
            for example in subcategory[1]:
                if isinstance(example, tuple):
                    examples.append(example[1])
                else:
                    examples.append(example)

    return redirect('input/?i=' + six.moves.urllib.parse.quote(random.choice(examples)))","for category in EXAMPLES:
    for subcategory in category[1]:
        for example in subcategory[1]:
            if isinstance(example, tuple):
                examples.append(example[1])
            else:
                examples.append(example)","for category in EXAMPLES:
    (category_0, category_1, *category_rcategorymaining) = category
    for subcategory in category[1]:
        for example in subcategory[1]:
            if isinstance(example, tuple):
                examples.append(example[1])
            else:
                examples.append(example)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
sympy_gamma,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy_gamma/app/views.py,https://github.com/sympy/sympy_gamma/tree/master/app/views.py,,random_example$132,"def random_example(request):
    examples = []

    for category in EXAMPLES:
        for subcategory in category[1]:
            for example in subcategory[1]:
                if isinstance(example, tuple):
                    examples.append(example[1])
                else:
                    examples.append(example)

    return redirect('input/?i=' + six.moves.urllib.parse.quote(random.choice(examples)))","for subcategory in category[1]:
    for example in subcategory[1]:
        if isinstance(example, tuple):
            examples.append(example[1])
        else:
            examples.append(example)","for subcategory in category[1]:
    (_, subcategory_1, *subcategory_rsubcategorymaining) = subcategory
    for example in subcategory[1]:
        if isinstance(example, tuple):
            examples.append(example[1])
        else:
            examples.append(example)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
sympy_gamma,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy_gamma/app/views.py,https://github.com/sympy/sympy_gamma/tree/master/app/views.py,,random_example$132,"def random_example(request):
    examples = []

    for category in EXAMPLES:
        for subcategory in category[1]:
            for example in subcategory[1]:
                if isinstance(example, tuple):
                    examples.append(example[1])
                else:
                    examples.append(example)

    return redirect('input/?i=' + six.moves.urllib.parse.quote(random.choice(examples)))","for example in subcategory[1]:
    if isinstance(example, tuple):
        examples.append(example[1])
    else:
        examples.append(example)","for example in subcategory[1]:
    (_, example_1, *example_rexamplemaining) = example
    if isinstance(example, tuple):
        examples.append(example[1])
    else:
        examples.append(example)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/tests/fast_tests/test_traffic_lights.py,https://github.com/flow-project/flow/tree/master/tests/fast_tests/test_traffic_lights.py,TestPOEnv,compare_ordering$167,"def compare_ordering(ordering):
        # take in a list like [[bot0_0, right0_0, top0_1, left1_0], [bot....]
        # print(ordering)
        for x in ordering:
            # print(x)
            if not (x[0].startswith(""bot"") and x[1].startswith(""right"") and
                    x[2].startswith(""top"") and x[3].startswith(""left"")):
                return False
        return True","for x in ordering:
    if not (x[0].startswith('bot') and x[1].startswith('right') and x[2].startswith('top') and x[3].startswith('left')):
        return False","for x in ordering:
    (x_0, x_1, x_2, x_3, *_) = x
    if not (x[0].startswith('bot') and x[1].startswith('right') and x[2].startswith('top') and x[3].startswith('left')):
        return False","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli-core/azure/cli/core/commands/arm.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli-core/azure/cli/core/commands/arm.py,,_get_internal_path$637,"def _get_internal_path(path):
    # to handle indexing in the same way as other dot qualifiers,
    # we split paths like foo[0][1] into foo.[0].[1]
    path = path.replace('.[', '[').replace('[', '.[')
    path_segment_pairs = internal_path_regex.findall(path)
    final_paths = []
    for regex_result in path_segment_pairs:
        # the regex matches two capture group, one of which will be None
        segment = regex_result[0] or regex_result[1]
        final_paths.append(segment)
    return final_paths","for regex_result in path_segment_pairs:
    segment = regex_result[0] or regex_result[1]
    final_paths.append(segment)","for regex_result in path_segment_pairs:
    (regex_result_0, regex_result_1, *_) = regex_result
    segment = regex_result[0] or regex_result[1]
    final_paths.append(segment)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
folium,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/folium/folium/plugins/heat_map.py,https://github.com/python-visualization/folium/tree/master/folium/plugins/heat_map.py,HeatMap,_get_self_bounds$87,"def _get_self_bounds(self):
        """"""
        Computes the bounds of the object itself (not including it's children)
        in the form [[lat_min, lon_min], [lat_max, lon_max]].

        """"""

        bounds = [[None, None], [None, None]]
        for point in self.data:
            bounds = [
                [
                    none_min(bounds[0][0], point[0]),
                    none_min(bounds[0][1], point[1]),
                ],
                [
                    none_max(bounds[1][0], point[0]),
                    none_max(bounds[1][1], point[1]),
                ],
            ]
        return bounds","for point in self.data:
    bounds = [[none_min(bounds[0][0], point[0]), none_min(bounds[0][1], point[1])], [none_max(bounds[1][0], point[0]), none_max(bounds[1][1], point[1])]]","for point in self.data:
    (point_0, point_1, *_) = point
    bounds = [[none_min(bounds[0][0], point[0]), none_min(bounds[0][1], point[1])], [none_max(bounds[1][0], point[0]), none_max(bounds[1][1], point[1])]]","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
mssql-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mssql-cli/tests/test_sqlcompletion.py,https://github.com/dbcli/mssql-cli/tree/master/tests/test_sqlcompletion.py,SqlCompletionTests,test_distinct_and_order_by_suggestions_with_aliases$264,"def test_distinct_and_order_by_suggestions_with_aliases():
        test_args = [
            (
                'SELECT DISTINCT FROM tbl x JOIN tbl1 y',
                'SELECT DISTINCT',
                'SELECT',
            ),
            (
                'SELECT * FROM tbl x JOIN tbl1 y ORDER BY ',
                'SELECT * FROM tbl x JOIN tbl1 y ORDER BY ',
                'ORDER BY',
            )
        ]
        for arg in test_args:
            text = arg[0]
            text_before = arg[1]
            last_keyword = arg[2]

            suggestions = suggest_type(text, text_before)
            assert set(suggestions) == set([
                Column(
                    table_refs=(
                        TableReference(None, 'tbl', 'x', False),
                        TableReference(None, 'tbl1', 'y', False),
                    ),
                    local_tables=(),
                    qualifiable=True
                ),
                Function(schema=None),
                Keyword(last_keyword)
            ])","for arg in test_args:
    text = arg[0]
    text_before = arg[1]
    last_keyword = arg[2]
    suggestions = suggest_type(text, text_before)
    assert set(suggestions) == set([Column(table_refs=(TableReference(None, 'tbl', 'x', False), TableReference(None, 'tbl1', 'y', False)), local_tables=(), qualifiable=True), Function(schema=None), Keyword(last_keyword)])","for arg in test_args:
    (arg_0, arg_1, arg_2, *_) = arg
    text = arg[0]
    text_before = arg[1]
    last_keyword = arg[2]
    suggestions = suggest_type(text, text_before)
    assert set(suggestions) == set([Column(table_refs=(TableReference(None, 'tbl', 'x', False), TableReference(None, 'tbl1', 'y', False)), local_tables=(), qualifiable=True), Function(schema=None), Keyword(last_keyword)])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]",,,,,,,
oppia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/platform/taskqueue/cloud_tasks_emulator.py,https://github.com/oppia/oppia/tree/master/core/platform/taskqueue/cloud_tasks_emulator.py,Emulator,get_tasks$258,"def get_tasks(self, queue_name: Optional[str] = None) -> List[Task]:
        """"""Returns a list of the tasks in a single queue if a queue name is
        specified or a list of all of the tasks in the taskqueue if no queue
        name is specified.

        Args:
            queue_name: str|None. Name of the queue. Pass in None if no specific
                queue is designated.

        Returns:
            list(Task). List of tasks in a single queue or in the entire
            taskqueue.
        """"""
        if queue_name:
            return self._queues[queue_name]
        else:
            tasks_list = []
            for items in self._queues.items():
                tasks_list.extend(items[1])
            return tasks_list","for items in self._queues.items():
    tasks_list.extend(items[1])","for items in self._queues.items():
    (items_0, items_1, *items_ritemsmaining) = items
    tasks_list.extend(items[1])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
ros_comm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/test/test_rosmaster/test/testSlave.py,https://github.com/ros/ros_comm/tree/master/test/test_rosmaster/test/testSlave.py,SlaveTestCase,_sink_StartNodes$243,"def _sink_StartNodes(self, tests):
        """"""
        Test subroutine to startup all the nodes specified in the tests
        """"""
        master = self.master
        sourceUris = {}
        sinks = {}
        #start the nodes
        # - save the sink as we will be making calls on it
        pkg, node = testNode
        for test in tests:
            sourceName, sinkName = test[0]
            sourcePort = apiSuccess(master.addNode('', '', sourceName, pkg, node, TEST_MACHINE, 0))
            sinkPort = apiSuccess(master.addNode('', '', sinkName, pkg, node, TEST_MACHINE, 0))
            sourceUri = 'http://%s:%s/'%(testNodeAddr[0], sourcePort)
            sinkUri = 'http://%s:%s/'%(testNodeAddr[0], sinkPort)
            sourceUris[sourceName] = sourceUri
            sinks[sinkName] = ServerProxy(sinkUri)
        return sourceUris, sinks","for test in tests:
    (sourceName, sinkName) = test[0]
    sourcePort = apiSuccess(master.addNode('', '', sourceName, pkg, node, TEST_MACHINE, 0))
    sinkPort = apiSuccess(master.addNode('', '', sinkName, pkg, node, TEST_MACHINE, 0))
    sourceUri = 'http://%s:%s/' % (testNodeAddr[0], sourcePort)
    sinkUri = 'http://%s:%s/' % (testNodeAddr[0], sinkPort)
    sourceUris[sourceName] = sourceUri
    sinks[sinkName] = ServerProxy(sinkUri)","for test in tests:
    (test_0, *test_rtestmaining) = test
    (sourceName, sinkName) = test[0]
    sourcePort = apiSuccess(master.addNode('', '', sourceName, pkg, node, TEST_MACHINE, 0))
    sinkPort = apiSuccess(master.addNode('', '', sinkName, pkg, node, TEST_MACHINE, 0))
    sourceUri = 'http://%s:%s/' % (testNodeAddr[0], sourcePort)
    sinkUri = 'http://%s:%s/' % (testNodeAddr[0], sinkPort)
    sourceUris[sourceName] = sourceUri
    sinks[sinkName] = ServerProxy(sinkUri)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
wttr.in,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wttr.in/lib/wttr_srv.py,https://github.com/chubin/wttr.in/tree/master/lib/wttr_srv.py,,_parse_language_header$79,"def _parse_language_header(header):
    """"""
    >>> _parse_language_header(""en-US,en;q=0.9"")
    >>> _parse_language_header(""en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7"")
    >>> _parse_language_header(""xx, fr-CA;q=0.8, da-DK;q=0.9"")
    'da'
    """"""

    def _parse_accept_language(accept_language):
        languages = accept_language.split("","")
        locale_q_pairs = []

        for language in languages:
            try:
                if language.split("";"")[0] == language:
                    # no q => q = 1
                    locale_q_pairs.append((language.strip(), 1))
                else:
                    locale = language.split("";"")[0].strip()
                    weight = float(language.split("";"")[1].split(""="")[1])
                    locale_q_pairs.append((locale, weight))
            except (IndexError, ValueError):
                pass

        return locale_q_pairs

    def _find_supported_language(accepted_languages):

        def supported_langs():
            """"""Yields all pairs in the Accept-Language header
            supported in SUPPORTED_LANGS or None if 'en' is the preferred""""""
            for lang_tuple in accepted_languages:
                lang = lang_tuple[0]
                if '-' in lang:
                    lang = lang.split('-', 1)[0]
                if lang in SUPPORTED_LANGS:
                    yield lang, lang_tuple[1]
                elif lang == 'en':
                    yield None, lang_tuple[1]
        try:
            return max(supported_langs(), key=lambda lang_tuple: lang_tuple[1])[0]
        except ValueError:
            return None

    return _find_supported_language(_parse_accept_language(header))","for lang_tuple in accepted_languages:
    lang = lang_tuple[0]
    if '-' in lang:
        lang = lang.split('-', 1)[0]
    if lang in SUPPORTED_LANGS:
        yield (lang, lang_tuple[1])
    elif lang == 'en':
        yield (None, lang_tuple[1])","for lang_tuple in accepted_languages:
    (lang_tuple_0, lang_tuple_1, *_) = lang_tuple
    lang = lang_tuple[0]
    if '-' in lang:
        lang = lang.split('-', 1)[0]
    if lang in SUPPORTED_LANGS:
        yield (lang, lang_tuple[1])
    elif lang == 'en':
        yield (None, lang_tuple[1])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
pytextrank,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytextrank/deprecated/oldsrc.py,https://github.com/DerwenAI/pytextrank/tree/master/deprecated/oldsrc.py,TextRank,write_dot$455,"def write_dot (self, path=""graph.dot""):
        """"""
        output the lemma graph in Dot file format
        """"""
        keys = list(self.seen_lemma.keys())
        dot = graphviz.Digraph()

        for node_id in self.lemma_graph.nodes():
            text = keys[node_id][0].lower()
            rank = self.ranks[node_id]
            label = ""{} ({:.4f})"".format(text, rank)
            dot.node(str(node_id), label)

        for edge in self.lemma_graph.edges():
            dot.edge(str(edge[0]), str(edge[1]), constraint=""false"")

        with open(path, ""w"") as f:
            f.write(dot.source)","for edge in self.lemma_graph.edges():
    dot.edge(str(edge[0]), str(edge[1]), constraint='false')","for edge in self.lemma_graph.edges():
    (edge_0, edge_1, *_) = edge
    dot.edge(str(edge[0]), str(edge[1]), constraint='false')","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
flask-profiler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flask-profiler/flask_profiler/storage/sqlite.py,https://github.com/muatik/flask-profiler/tree/master/flask_profiler/storage/sqlite.py,Sqlite,getSummary$281,"def getSummary(self, kwds={}):
        filters = Sqlite.getFilters(kwds)

        conditions = ""WHERE 1=1 and ""

        if filters[""startedAt""]:
            conditions = conditions + ""startedAt>={0} AND "".format(
                filters[""startedAt""])
        if filters[""endedAt""]:
            conditions = conditions + ""endedAt<={0} AND "".format(
                filters[""endedAt""])
        if filters[""elapsed""]:
            conditions = conditions + ""elapsed>={0} AND"".format(
                filters[""elapsed""])

        conditions = conditions.rstrip("" AND"")
        with self.lock:
            sql = '''SELECT
                    method, name,
                    count(id) as count,
                    min(elapsed) as minElapsed,
                    max(elapsed) as maxElapsed,
                    avg(elapsed) as avgElapsed
                FROM ""{table_name}"" {conditions}
                group by method, name
                order by {sort_field} {sort_direction}
                '''.format(
                    table_name=self.table_name,
                    conditions=conditions,
                    sort_field=filters[""sort""][0],
                    sort_direction=filters[""sort""][1]
                    )

            self.cursor.execute(sql)
            rows = self.cursor.fetchall()

        result = []
        for r in rows:
            result.append({
                ""method"": r[0],
                ""name"": r[1],
                ""count"": r[2],
                ""minElapsed"": r[3],
                ""maxElapsed"": r[4],
                ""avgElapsed"": r[5]
            })
        return result","for r in rows:
    result.append({'method': r[0], 'name': r[1], 'count': r[2], 'minElapsed': r[3], 'maxElapsed': r[4], 'avgElapsed': r[5]})","for r in rows:
    (r_0, r_1, r_2, r_3, r_4, r_5, *_) = r
    result.append({'method': r[0], 'name': r[1], 'count': r[2], 'minElapsed': r[3], 'maxElapsed': r[4], 'avgElapsed': r[5]})","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4, e_5 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]
e_5: e[5]",,,,,,,
opentelemetry-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opentelemetry-python/opentelemetry-api/tests/baggage/test_baggage_propagation.py,https://github.com/open-telemetry/opentelemetry-python/tree/master/opentelemetry-api/tests/baggage/test_baggage_propagation.py,TestBaggagePropagation,test_fields$224,"def test_fields(self, mock_format_baggage, mock_baggage):

        mock_setter = Mock()

        self.propagator.inject({}, setter=mock_setter)

        inject_fields = set()

        for mock_call in mock_setter.mock_calls:
            inject_fields.add(mock_call[1][1])

        self.assertEqual(inject_fields, self.propagator.fields)","for mock_call in mock_setter.mock_calls:
    inject_fields.add(mock_call[1][1])","for mock_call in mock_setter.mock_calls:
    (_, mock_call_1, *mock_call_rmock_callmaining) = mock_call
    inject_fields.add(mock_call[1][1])","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",0,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1[1]: e[1][1]",,,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/webapps/galaxy/controllers/tag.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/galaxy/controllers/tag.py,TagsController,_get_tag_autocomplete_values$137,"def _get_tag_autocomplete_values(self, trans, q, limit, timestamp, user=None, item=None, item_class=None):
        """"""
        Returns autocomplete data for tag values ordered from most frequently used to
        least frequently used.
        """"""
        tag_name_and_value = q.split("":"")
        tag_name = tag_name_and_value[0]
        tag_value = tag_name_and_value[1]
        tag = self.get_tag_handler(trans).get_tag_by_name(tag_name)
        # Don't autocomplete if tag doesn't exist.
        if tag is None:
            return """"
        # Get item's class object and item-tag association class.
        if item is None and item_class is None:
            raise RuntimeError(""Both item and item_class cannot be None"")
        elif item is not None:
            item_class = item.__class__
        item_tag_assoc_class = self.get_tag_handler(trans).get_tag_assoc_class(item_class)
        # Build select statement.
        cols_to_select = [item_tag_assoc_class.table.c.value, func.count('*')]
        from_obj = item_tag_assoc_class.table.join(item_class.table).join(trans.app.model.Tag.table)
        where_clause = and_(item_tag_assoc_class.table.c.user_id == user.id,
                            trans.app.model.Tag.table.c.id == tag.id,
                            item_tag_assoc_class.table.c.value.like(f""{tag_value}%""))
        order_by = [func.count(""*"").desc(), item_tag_assoc_class.table.c.value]
        group_by = item_tag_assoc_class.table.c.value
        # Do query and get result set.
        query = select(columns=cols_to_select,
                       from_obj=from_obj,
                       whereclause=where_clause,
                       group_by=group_by,
                       order_by=order_by,
                       limit=limit)
        result_set = trans.sa_session.execute(query)
        # Create and return autocomplete data.
        ac_data = f""#Header|Your Values for '{tag_name}'\n""
        tag_uname = self._get_usernames_for_tag(trans, trans.user, tag, item_class, item_tag_assoc_class)[0]
        for row in result_set:
            ac_data += f""{tag_uname}:{row[0]}|{row[0]}\n""
        return ac_data","for row in result_set:
    ac_data += f'{tag_uname}:{row[0]}|{row[0]}\n'","for row in result_set:
    (row_0, *row_rrowmaining) = row
    ac_data += f'{tag_uname}:{row[0]}|{row[0]}\n'","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/backup/custom_wl.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/backup/custom_wl.py,,restore_azure_wl$615,"def restore_azure_wl(cmd, client, resource_group_name, vault_name, recovery_config, rehydration_duration=15,
                     rehydration_priority=None, use_secondary_region=None):

    recovery_config_object = cust_help.get_or_read_json(recovery_config)
    restore_mode = recovery_config_object['restore_mode']
    container_uri = recovery_config_object['container_uri']
    item_uri = recovery_config_object['item_uri']
    recovery_point_id = recovery_config_object['recovery_point_id']
    log_point_in_time = recovery_config_object['log_point_in_time']
    item_type = recovery_config_object['item_type']
    workload_type = recovery_config_object['workload_type']
    source_resource_id = recovery_config_object['source_resource_id']
    database_name = recovery_config_object['database_name']
    container_id = recovery_config_object['container_id']
    alternate_directory_paths = recovery_config_object['alternate_directory_paths']
    recovery_mode = recovery_config_object['recovery_mode']
    filepath = recovery_config_object['filepath']

    item = common.show_item(cmd, backup_protected_items_cf(cmd.cli_ctx), resource_group_name, vault_name,
                            container_uri, item_uri, ""AzureWorkload"")
    cust_help.validate_item(item)
    validate_wl_restore(item, item_type, restore_mode, recovery_mode)

    trigger_restore_properties = _get_restore_request_instance(item_type, log_point_in_time, None)
    if log_point_in_time is None:
        recovery_point = common.show_recovery_point(cmd, recovery_points_cf(cmd.cli_ctx), resource_group_name,
                                                    vault_name, container_uri, item_uri, recovery_point_id,
                                                    workload_type, ""AzureWorkload"", use_secondary_region)

        if recovery_point is None:
            raise InvalidArgumentValueError(""""""
            Specified recovery point not found. Please check the recovery config file
            or try removing --use-secondary-region if provided"""""")

        common.fetch_tier_for_rp(recovery_point)

        if (recovery_point.tier_type is not None and recovery_point.tier_type == 'VaultArchive'):
            if rehydration_priority is None:
                raise InvalidArgumentValueError(""""""The selected recovery point is in archive tier, provide additional
                parameters of rehydration duration and rehydration priority."""""")
            # normal rehydrated restore
            trigger_restore_properties = _get_restore_request_instance(item_type, log_point_in_time,
                                                                       rehydration_priority)

            rehyd_duration = 'P' + str(rehydration_duration) + 'D'
            rehydration_info = RecoveryPointRehydrationInfo(rehydration_retention_duration=rehyd_duration,
                                                            rehydration_priority=rehydration_priority)

            trigger_restore_properties.recovery_point_rehydration_info = rehydration_info

    trigger_restore_properties.recovery_type = restore_mode

    # Get target vm id
    if container_id is not None:
        target_container_name = cust_help.get_protection_container_uri_from_id(container_id)
        target_resource_group = cust_help.get_resource_group_from_id(container_id)
        target_vault_name = cust_help.get_vault_from_arm_id(container_id)
        target_container = common.show_container(cmd, backup_protection_containers_cf(cmd.cli_ctx),
                                                 target_container_name, target_resource_group, target_vault_name,
                                                 'AzureWorkload')
        setattr(trigger_restore_properties, 'target_virtual_machine_id', target_container.properties.source_resource_id)

    if restore_mode == 'AlternateLocation':
        if recovery_mode != ""FileRecovery"":
            setattr(trigger_restore_properties, 'source_resource_id', source_resource_id)
            setattr(trigger_restore_properties, 'target_info', TargetRestoreInfo(overwrite_option='Overwrite',
                                                                                 database_name=database_name,
                                                                                 container_id=container_id))
            if 'sql' in item_type.lower():
                directory_map = []
                for i in alternate_directory_paths:
                    directory_map.append(SQLDataDirectoryMapping(mapping_type=i[0], source_path=i[1],
                                                                 source_logical_name=i[2], target_path=i[3]))
                setattr(trigger_restore_properties, 'alternate_directory_paths', directory_map)
        else:
            target_info = TargetRestoreInfo(overwrite_option='Overwrite', container_id=container_id,
                                            target_directory_for_file_restore=filepath)
            setattr(trigger_restore_properties, 'target_info', target_info)
            trigger_restore_properties.recovery_mode = recovery_mode

    if log_point_in_time is not None:
        log_point_in_time = datetime_type(log_point_in_time)
        time_range_list = _get_log_time_range(cmd, resource_group_name, vault_name, item, use_secondary_region)
        validate_log_point_in_time(log_point_in_time, time_range_list)
        setattr(trigger_restore_properties, 'point_in_time', log_point_in_time)

    if 'sql' in item_type.lower():
        setattr(trigger_restore_properties, 'should_use_alternate_target_location', True)
        setattr(trigger_restore_properties, 'is_non_recoverable', False)

    trigger_restore_request = RestoreRequestResource(properties=trigger_restore_properties)

    if use_secondary_region:
        if rehydration_priority is not None:
            raise MutuallyExclusiveArgumentError(""Archive restore isn't supported for secondary region."")
        vault = vaults_cf(cmd.cli_ctx).get(resource_group_name, vault_name)
        vault_location = vault.location
        azure_region = custom.secondary_region_map[vault_location]
        aad_client = aad_properties_cf(cmd.cli_ctx)
        filter_string = cust_help.get_filter_string({'backupManagementType': 'AzureWorkload'})
        aad_result = aad_client.get(azure_region, filter_string)
        rp_client = recovery_points_passive_cf(cmd.cli_ctx)
        crr_access_token = rp_client.get_access_token(vault_name, resource_group_name, fabric_name, container_uri,
                                                      item_uri, recovery_point_id, aad_result).properties
        crr_client = cross_region_restore_cf(cmd.cli_ctx)
        trigger_restore_properties.region = azure_region
        trigger_crr_request = CrossRegionRestoreRequest(cross_region_restore_access_details=crr_access_token,
                                                        restore_request=trigger_restore_properties)
        result = crr_client.begin_trigger(azure_region, trigger_crr_request, cls=cust_help.get_pipeline_response,
                                          polling=False).result()
        return cust_help.track_backup_crr_job(cmd.cli_ctx, result, azure_region, vault.id)

    # Trigger restore and wait for completion
    result = client.begin_trigger(vault_name, resource_group_name, fabric_name, container_uri, item_uri,
                                  recovery_point_id, trigger_restore_request, cls=cust_help.get_pipeline_response,
                                  polling=False).result()
    return cust_help.track_backup_job(cmd.cli_ctx, result, vault_name, resource_group_name)","for i in alternate_directory_paths:
    directory_map.append(SQLDataDirectoryMapping(mapping_type=i[0], source_path=i[1], source_logical_name=i[2], target_path=i[3]))","for i in alternate_directory_paths:
    (i_0, i_1, i_2, i_3, *_) = i
    directory_map.append(SQLDataDirectoryMapping(mapping_type=i[0], source_path=i[1], source_logical_name=i[2], target_path=i[3]))","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""获取各种资源""""""

    outline = Outline()
    playlist = Playlist()
    counter = Counter()

    video_list = []
    pdf_list = []
    rich_text_list = []

    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190',
                 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean',
                 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id,
                 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr',
                     data=post_data).text.encode('utf_8').decode('unicode_escape')

    chapters = re.findall(r'homeworks=\w+;.+id=(\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)

        lessons = re.findall(r'chapterId=' + chapter[0] + r'.+contentType=1.+id=(\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)

            videos = re.findall(r'contentId=(\d+).+contentType=(1).+id=(\d+).+lessonId=' +
                                lesson[0] + r'.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()

            pdfs = re.findall(r'contentId=(\d+).+contentType=(3).+id=(\d+).+lessonId=' +
                              lesson[0] + r'.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()

            rich_text = re.findall(r'contentId=(\d+).+contentType=(4).+id=(\d+).+jsonContent=(.+);.+lessonId=' +
                                   lesson[0] + r'.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1),
                                  'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')

                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm',
                                           WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()

    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for chapter in chapters:
    counter.add(0)
    outline.write(chapter[1], counter, 0)
    lessons = re.findall('chapterId=' + chapter[0] + '.+contentType=1.+id=(\\d+).+name=""(.+)"".+test', res)
    for lesson in lessons:
        counter.add(1)
        outline.write(lesson[1], counter, 1)
        videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
        for video in videos:
            counter.add(2)
            outline.write(video[3], counter, 2, sign='#')
            video_list.append(Video(counter, video[3], video))
        counter.reset()
        pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
        for pdf in pdfs:
            counter.add(2)
            outline.write(pdf[3], counter, 2, sign='*')
            if CONFIG['doc']:
                pdf_list.append(Document(counter, pdf[3], pdf))
        counter.reset()
        rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
        for text in rich_text:
            counter.add(2)
            outline.write(text[4], counter, 2, sign='+')
            if CONFIG['text']:
                rich_text_list.append(RichText(counter, text[4], text))
            if CONFIG['file']:
                if text[3] != 'null' and text[3] != '""""':
                    params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                    file_name = Resource.file_to_save(params['fileName'])
                    outline.write(file_name, counter, 2, sign='!')
                    WORK_DIR.change('Files')
                    res_print(params['fileName'])
                    file_name = '%s %s' % (counter, file_name)
                    CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
        counter.reset()","for chapter in chapters:
    (chapter_0, chapter_1, *_) = chapter
    counter.add(0)
    outline.write(chapter[1], counter, 0)
    lessons = re.findall('chapterId=' + chapter[0] + '.+contentType=1.+id=(\\d+).+name=""(.+)"".+test', res)
    for lesson in lessons:
        counter.add(1)
        outline.write(lesson[1], counter, 1)
        videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
        for video in videos:
            counter.add(2)
            outline.write(video[3], counter, 2, sign='#')
            video_list.append(Video(counter, video[3], video))
        counter.reset()
        pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
        for pdf in pdfs:
            counter.add(2)
            outline.write(pdf[3], counter, 2, sign='*')
            if CONFIG['doc']:
                pdf_list.append(Document(counter, pdf[3], pdf))
        counter.reset()
        rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
        for text in rich_text:
            counter.add(2)
            outline.write(text[4], counter, 2, sign='+')
            if CONFIG['text']:
                rich_text_list.append(RichText(counter, text[4], text))
            if CONFIG['file']:
                if text[3] != 'null' and text[3] != '""""':
                    params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                    file_name = Resource.file_to_save(params['fileName'])
                    outline.write(file_name, counter, 2, sign='!')
                    WORK_DIR.change('Files')
                    res_print(params['fileName'])
                    file_name = '%s %s' % (counter, file_name)
                    CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
        counter.reset()","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""获取各种资源""""""

    outline = Outline()
    playlist = Playlist()
    counter = Counter()

    video_list = []
    pdf_list = []
    rich_text_list = []

    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190',
                 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean',
                 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id,
                 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr',
                     data=post_data).text.encode('utf_8').decode('unicode_escape')

    chapters = re.findall(r'homeworks=\w+;.+id=(\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)

        lessons = re.findall(r'chapterId=' + chapter[0] + r'.+contentType=1.+id=(\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)

            videos = re.findall(r'contentId=(\d+).+contentType=(1).+id=(\d+).+lessonId=' +
                                lesson[0] + r'.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()

            pdfs = re.findall(r'contentId=(\d+).+contentType=(3).+id=(\d+).+lessonId=' +
                              lesson[0] + r'.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()

            rich_text = re.findall(r'contentId=(\d+).+contentType=(4).+id=(\d+).+jsonContent=(.+);.+lessonId=' +
                                   lesson[0] + r'.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1),
                                  'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')

                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm',
                                           WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()

    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for lesson in lessons:
    counter.add(1)
    outline.write(lesson[1], counter, 1)
    videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
    for video in videos:
        counter.add(2)
        outline.write(video[3], counter, 2, sign='#')
        video_list.append(Video(counter, video[3], video))
    counter.reset()
    pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
    for pdf in pdfs:
        counter.add(2)
        outline.write(pdf[3], counter, 2, sign='*')
        if CONFIG['doc']:
            pdf_list.append(Document(counter, pdf[3], pdf))
    counter.reset()
    rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
    for text in rich_text:
        counter.add(2)
        outline.write(text[4], counter, 2, sign='+')
        if CONFIG['text']:
            rich_text_list.append(RichText(counter, text[4], text))
        if CONFIG['file']:
            if text[3] != 'null' and text[3] != '""""':
                params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                file_name = Resource.file_to_save(params['fileName'])
                outline.write(file_name, counter, 2, sign='!')
                WORK_DIR.change('Files')
                res_print(params['fileName'])
                file_name = '%s %s' % (counter, file_name)
                CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
    counter.reset()","for lesson in lessons:
    (lesson_0, lesson_1, *_) = lesson
    counter.add(1)
    outline.write(lesson[1], counter, 1)
    videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
    for video in videos:
        counter.add(2)
        outline.write(video[3], counter, 2, sign='#')
        video_list.append(Video(counter, video[3], video))
    counter.reset()
    pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
    for pdf in pdfs:
        counter.add(2)
        outline.write(pdf[3], counter, 2, sign='*')
        if CONFIG['doc']:
            pdf_list.append(Document(counter, pdf[3], pdf))
    counter.reset()
    rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
    for text in rich_text:
        counter.add(2)
        outline.write(text[4], counter, 2, sign='+')
        if CONFIG['text']:
            rich_text_list.append(RichText(counter, text[4], text))
        if CONFIG['file']:
            if text[3] != 'null' and text[3] != '""""':
                params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                file_name = Resource.file_to_save(params['fileName'])
                outline.write(file_name, counter, 2, sign='!')
                WORK_DIR.change('Files')
                res_print(params['fileName'])
                file_name = '%s %s' % (counter, file_name)
                CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
    counter.reset()","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""获取各种资源""""""

    outline = Outline()
    playlist = Playlist()
    counter = Counter()

    video_list = []
    pdf_list = []
    rich_text_list = []

    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190',
                 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean',
                 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id,
                 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr',
                     data=post_data).text.encode('utf_8').decode('unicode_escape')

    chapters = re.findall(r'homeworks=\w+;.+id=(\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)

        lessons = re.findall(r'chapterId=' + chapter[0] + r'.+contentType=1.+id=(\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)

            videos = re.findall(r'contentId=(\d+).+contentType=(1).+id=(\d+).+lessonId=' +
                                lesson[0] + r'.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()

            pdfs = re.findall(r'contentId=(\d+).+contentType=(3).+id=(\d+).+lessonId=' +
                              lesson[0] + r'.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()

            rich_text = re.findall(r'contentId=(\d+).+contentType=(4).+id=(\d+).+jsonContent=(.+);.+lessonId=' +
                                   lesson[0] + r'.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1),
                                  'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')

                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm',
                                           WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()

    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for video in videos:
    counter.add(2)
    outline.write(video[3], counter, 2, sign='#')
    video_list.append(Video(counter, video[3], video))","for video in videos:
    (_, _, _, video_3, *_) = video
    counter.add(2)
    outline.write(video[3], counter, 2, sign='#')
    video_list.append(Video(counter, video[3], video))",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, _, _, e_3 = e
variable mapping:
e_3: e[3]",,,,,,,
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""获取各种资源""""""

    outline = Outline()
    playlist = Playlist()
    counter = Counter()

    video_list = []
    pdf_list = []
    rich_text_list = []

    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190',
                 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean',
                 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id,
                 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr',
                     data=post_data).text.encode('utf_8').decode('unicode_escape')

    chapters = re.findall(r'homeworks=\w+;.+id=(\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)

        lessons = re.findall(r'chapterId=' + chapter[0] + r'.+contentType=1.+id=(\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)

            videos = re.findall(r'contentId=(\d+).+contentType=(1).+id=(\d+).+lessonId=' +
                                lesson[0] + r'.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()

            pdfs = re.findall(r'contentId=(\d+).+contentType=(3).+id=(\d+).+lessonId=' +
                              lesson[0] + r'.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()

            rich_text = re.findall(r'contentId=(\d+).+contentType=(4).+id=(\d+).+jsonContent=(.+);.+lessonId=' +
                                   lesson[0] + r'.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1),
                                  'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')

                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm',
                                           WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()

    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for pdf in pdfs:
    counter.add(2)
    outline.write(pdf[3], counter, 2, sign='*')
    if CONFIG['doc']:
        pdf_list.append(Document(counter, pdf[3], pdf))","for pdf in pdfs:
    (_, _, _, pdf_3, *_) = pdf
    counter.add(2)
    outline.write(pdf[3], counter, 2, sign='*')
    if CONFIG['doc']:
        pdf_list.append(Document(counter, pdf[3], pdf))",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, _, _, e_3 = e
variable mapping:
e_3: e[3]",,,,,,,
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""获取各种资源""""""

    outline = Outline()
    playlist = Playlist()
    counter = Counter()

    video_list = []
    pdf_list = []
    rich_text_list = []

    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190',
                 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean',
                 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id,
                 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr',
                     data=post_data).text.encode('utf_8').decode('unicode_escape')

    chapters = re.findall(r'homeworks=\w+;.+id=(\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)

        lessons = re.findall(r'chapterId=' + chapter[0] + r'.+contentType=1.+id=(\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)

            videos = re.findall(r'contentId=(\d+).+contentType=(1).+id=(\d+).+lessonId=' +
                                lesson[0] + r'.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()

            pdfs = re.findall(r'contentId=(\d+).+contentType=(3).+id=(\d+).+lessonId=' +
                              lesson[0] + r'.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()

            rich_text = re.findall(r'contentId=(\d+).+contentType=(4).+id=(\d+).+jsonContent=(.+);.+lessonId=' +
                                   lesson[0] + r'.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1),
                                  'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')

                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm',
                                           WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()

    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for text in rich_text:
    counter.add(2)
    outline.write(text[4], counter, 2, sign='+')
    if CONFIG['text']:
        rich_text_list.append(RichText(counter, text[4], text))
    if CONFIG['file']:
        if text[3] != 'null' and text[3] != '""""':
            params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
            file_name = Resource.file_to_save(params['fileName'])
            outline.write(file_name, counter, 2, sign='!')
            WORK_DIR.change('Files')
            res_print(params['fileName'])
            file_name = '%s %s' % (counter, file_name)
            CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})","for text in rich_text:
    (_, _, _, text_3, text_4, *_) = text
    counter.add(2)
    outline.write(text[4], counter, 2, sign='+')
    if CONFIG['text']:
        rich_text_list.append(RichText(counter, text[4], text))
    if CONFIG['file']:
        if text[3] != 'null' and text[3] != '""""':
            params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
            file_name = Resource.file_to_save(params['fileName'])
            outline.write(file_name, counter, 2, sign='!')
            WORK_DIR.change('Files')
            res_print(params['fileName'])
            file_name = '%s %s' % (counter, file_name)
            CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, _, _, e_3, e_4 = e
variable mapping:
e_3: e[3]
e_4: e[4]",,,,,,,
VTuber_Unity,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VTuber_Unity/head_pose_estimation/visualization.py,https://github.com/kwea123/VTuber_Unity/tree/master/head_pose_estimation/visualization.py,,draw_marks$10,"def draw_marks(image, marks, color=(255, 255, 255)):
    """"""Draw mark points on image""""""
    for mark in marks:
        cv2.circle(image, (int(mark[0]), int(
            mark[1])), 1, color, -1, cv2.LINE_AA)","for mark in marks:
    cv2.circle(image, (int(mark[0]), int(mark[1])), 1, color, -1, cv2.LINE_AA)","for mark in marks:
    (mark_0, mark_1, *_) = mark
    cv2.circle(image, (int(mark[0]), int(mark[1])), 1, color, -1, cv2.LINE_AA)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
veusz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/veusz/veusz/windows/mainwindow.py,https://github.com/veusz/veusz/tree/master/veusz/windows/mainwindow.py,MainWindow,definePlugins$378,"def definePlugins(self, pluginlist, actions, menuname):
        """"""Create menu items and actions for plugins.

        pluginlist: list of plugin classes
        actions: dict of actions to add new actions to
        menuname: string giving prefix for new menu entries (inside actions)
        """"""

        def getLoadDialog(pluginkls):
            def _loadPlugin():
                from ..dialogs.plugin import handlePlugin
                handlePlugin(self, self.document, pluginkls)
            return _loadPlugin

        menu = []
        for pluginkls in pluginlist:
            actname = menuname + '.' + '.'.join(pluginkls.menu)
            text = pluginkls.menu[-1]
            if pluginkls.has_parameters:
                text += '...'
            actions[actname] = utils.makeAction(
                self,
                pluginkls.description_short,
                text,
                getLoadDialog(pluginkls))

            # build up menu from tuple of names
            menulook = menu
            namebuild = [menuname]
            for cmpt in pluginkls.menu[:-1]:
                namebuild.append(cmpt)
                name = '.'.join(namebuild)

                for c in menulook:
                    if c[0] == name:
                        menulook = c[2]
                        break
                else:
                    menulook.append( [name, cmpt, []] )
                    menulook = menulook[-1][2]

            menulook.append(actname)

        return menu","for c in menulook:
    if c[0] == name:
        menulook = c[2]
        break
else:
    menulook.append([name, cmpt, []])
    menulook = menulook[-1][2]","for c in menulook:
    (c_0, _, c_2, *c_rcmaining) = c
    if c[0] == name:
        menulook = c[2]
        break
else:
    menulook.append([name, cmpt, []])
    menulook = menulook[-1][2]","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, _, e_2, *e_remaining = e
variable mapping:
e_0: e[0]
e_2: e[2]",,,,,,,
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/google/cloud/forseti/scanner/scanners/groups_settings_scanner.py,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/scanner/scanners/groups_settings_scanner.py,GroupsSettingsScanner,_retrieve$134,"def _retrieve(self):
        """"""Runs the data collection.

        Returns:
            tupl: 2 lists of GroupsSettings objects, 1 only for settings that
            have iam policies and 1 with all groups settings.
        Raises:
            ValueError: if resources have an unexpected type.
        """"""
        all_groups_settings = []
        iam_groups_settings = []

        model_manager = self.service_config.model_manager
        scoped_session, data_access = model_manager.get(self.model_name)
        with scoped_session as session:
            for settings in data_access.scanner_fetch_groups_settings(session,
                                                                      True):
                email = settings[0].split('group/')[1]
                iam_groups_settings.append(groups_settings.GroupsSettings
                                           .from_json(email, settings[1]))
            for settings in data_access.scanner_fetch_groups_settings(session,
                                                                      False):
                email = settings[0].split('group/')[1]
                all_groups_settings.append(groups_settings.GroupsSettings
                                           .from_json(email, settings[1]))

        return all_groups_settings, iam_groups_settings","for settings in data_access.scanner_fetch_groups_settings(session, True):
    email = settings[0].split('group/')[1]
    iam_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))","for settings in data_access.scanner_fetch_groups_settings(session, True):
    (settings_0, settings_1, *settings_rsettingsmaining) = settings
    email = settings[0].split('group/')[1]
    iam_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/google/cloud/forseti/scanner/scanners/groups_settings_scanner.py,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/scanner/scanners/groups_settings_scanner.py,GroupsSettingsScanner,_retrieve$134,"def _retrieve(self):
        """"""Runs the data collection.

        Returns:
            tupl: 2 lists of GroupsSettings objects, 1 only for settings that
            have iam policies and 1 with all groups settings.
        Raises:
            ValueError: if resources have an unexpected type.
        """"""
        all_groups_settings = []
        iam_groups_settings = []

        model_manager = self.service_config.model_manager
        scoped_session, data_access = model_manager.get(self.model_name)
        with scoped_session as session:
            for settings in data_access.scanner_fetch_groups_settings(session,
                                                                      True):
                email = settings[0].split('group/')[1]
                iam_groups_settings.append(groups_settings.GroupsSettings
                                           .from_json(email, settings[1]))
            for settings in data_access.scanner_fetch_groups_settings(session,
                                                                      False):
                email = settings[0].split('group/')[1]
                all_groups_settings.append(groups_settings.GroupsSettings
                                           .from_json(email, settings[1]))

        return all_groups_settings, iam_groups_settings","for settings in data_access.scanner_fetch_groups_settings(session, False):
    email = settings[0].split('group/')[1]
    all_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))","for settings in data_access.scanner_fetch_groups_settings(session, False):
    (_, settings_1, *settings_rsettingsmaining) = settings
    email = settings[0].split('group/')[1]
    all_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,
django-rosetta,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-rosetta/rosetta/views.py,https://github.com/mbi/django-rosetta/tree/master/rosetta/views.py,TranslationFileListView,get_context_data$200,"def get_context_data(self, **kwargs):
        context = super(TranslationFileListView, self).get_context_data(**kwargs)

        third_party_apps = self.po_filter in ('all', 'third-party')
        django_apps = self.po_filter in ('all', 'django')
        project_apps = self.po_filter in ('all', 'project')

        languages = []
        has_pos = False
        for language in rosetta_settings.ROSETTA_LANGUAGES:
            if not can_translate_language(self.request.user, language[0]):
                continue

            po_paths = find_pos(
                language[0],
                project_apps=project_apps,
                django_apps=django_apps,
                third_party_apps=third_party_apps,
            )
            po_files = [
                (get_app_name(lang), os.path.realpath(lang), pofile(lang))
                for lang in po_paths
            ]
            po_files.sort(key=lambda app: app[0])
            languages.append((language[0], _(language[1]), po_files))
            has_pos = has_pos or bool(po_paths)

        context['version'] = get_rosetta_version()
        context['languages'] = languages
        context['has_pos'] = has_pos
        context['po_filter'] = self.po_filter
        return context","for language in rosetta_settings.ROSETTA_LANGUAGES:
    if not can_translate_language(self.request.user, language[0]):
        continue
    po_paths = find_pos(language[0], project_apps=project_apps, django_apps=django_apps, third_party_apps=third_party_apps)
    po_files = [(get_app_name(lang), os.path.realpath(lang), pofile(lang)) for lang in po_paths]
    po_files.sort(key=lambda app: app[0])
    languages.append((language[0], _(language[1]), po_files))
    has_pos = has_pos or bool(po_paths)","for language in rosetta_settings.ROSETTA_LANGUAGES:
    (language_0, language_1, *_) = language
    if not can_translate_language(self.request.user, language[0]):
        continue
    po_paths = find_pos(language[0], project_apps=project_apps, django_apps=django_apps, third_party_apps=third_party_apps)
    po_files = [(get_app_name(lang), os.path.realpath(lang), pofile(lang)) for lang in po_paths]
    po_files.sort(key=lambda app: app[0])
    languages.append((language[0], _(language[1]), po_files))
    has_pos = has_pos or bool(po_paths)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/scripts/datasets/ucf101.py,https://github.com/dmlc/gluon-cv/tree/master/scripts/datasets/ucf101.py,,build_split_list$242,"def build_split_list(split, frame_info, shuffle=False):

    def build_set_list(set_list):
        rgb_list, flow_list = list(), list()
        for item in set_list:
            if item[0] not in frame_info:
                # print(""item:"", item)
                continue
            elif frame_info[item[0]][1] > 0:
                rgb_cnt = frame_info[item[0]][1]
                flow_cnt = frame_info[item[0]][2]
                rgb_list.append('{} {} {}\n'.format(
                    item[0], rgb_cnt, item[1]))
                flow_list.append('{} {} {}\n'.format(
                    item[0], flow_cnt, item[1]))
            else:
                rgb_list.append('{} {}\n'.format(
                    item[0], item[1]))
                flow_list.append('{} {}\n'.format(
                    item[0], item[1]))
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return rgb_list, flow_list

    train_rgb_list, train_flow_list = build_set_list(split[0])
    test_rgb_list, test_flow_list = build_set_list(split[1])
    return (train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list)","for item in set_list:
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))
        flow_list.append('{} {}\n'.format(item[0], item[1]))","for item in set_list:
    (item_0, item_1, *_) = item
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))
        flow_list.append('{} {}\n'.format(item[0], item[1]))","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
XlsxWriter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/XlsxWriter/xlsxwriter/test/worksheet/test_extract_filter_tokens.py,https://github.com/jmcnamara/XlsxWriter/tree/master/xlsxwriter/test/worksheet/test_extract_filter_tokens.py,TestExtractFilterTokens,test_extract_filter_tokens$25,"def test_extract_filter_tokens(self):
        """"""Test the _extract_filter_tokens() method""""""

        testcases = [
            [
                None,
                [],
            ],

            [
                '',
                [],
            ],

            [
                '0 <  2001',
                ['0', '<', '2001'],
            ],

            [
                'x <  2000',
                ['x', '<', '2000'],
            ],

            [
                'x >  2000',
                ['x', '>', '2000'],
            ],

            [
                'x == 2000',
                ['x', '==', '2000'],
            ],

            [
                'x >  2000 and x <  5000',
                ['x', '>', '2000', 'and', 'x', '<', '5000'],
            ],

            [
                'x = ""goo""',
                ['x', '=', 'goo'],
            ],

            [
                'x = moo',
                ['x', '=', 'moo'],
            ],

            [
                'x = ""foo baz""',
                ['x', '=', 'foo baz'],
            ],

            [
                'x = ""moo """" bar""',
                ['x', '=', 'moo "" bar'],
            ],

            [
                'x = ""foo bar"" or x = ""bar foo""',
                ['x', '=', 'foo bar', 'or', 'x', '=', 'bar foo'],
            ],

            [
                'x = ""foo """" bar"" or x = ""bar """" foo""',
                ['x', '=', 'foo "" bar', 'or', 'x', '=', 'bar "" foo'],
            ],

            [
                'x = """"""""""""""""',
                ['x', '=', '""""""'],
            ],

            [
                'x = Blanks',
                ['x', '=', 'Blanks'],
            ],

            [
                'x = NonBlanks',
                ['x', '=', 'NonBlanks'],
            ],

            [
                'top 10 %',
                ['top', '10', '%'],
            ],

            [
                'top 10 items',
                ['top', '10', 'items'],
            ],
        ]

        for testcase in testcases:
            expression = testcase[0]

            exp = testcase[1]
            got = self.worksheet._extract_filter_tokens(expression)

            self.assertEqual(got, exp)","for testcase in testcases:
    expression = testcase[0]
    exp = testcase[1]
    got = self.worksheet._extract_filter_tokens(expression)
    self.assertEqual(got, exp)","for testcase in testcases:
    (testcase_0, testcase_1, *_) = testcase
    expression = testcase[0]
    exp = testcase[1]
    got = self.worksheet._extract_filter_tokens(expression)
    self.assertEqual(got, exp)","(data, data, *data)","for (mapping_0, mapping_1, *mapping_len) in version_to_commit:
    if 
    mapping_0 == version:
        commit = 
        mapping_1",1,,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,
