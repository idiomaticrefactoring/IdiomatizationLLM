repo_name,file_path,file_html,class_name,me_name,me_code,old_code,new_code,bool_code,chatGPT_code,if_correct,,,reversed_code,non_replace_var_refactored_code,refactored_code,acc,instruction,sys_msg,exam_msg,user_msg,
find_or_refactor_wrong,,,,,,,,,,,,,,,,,,,,,
natlas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/natlas/natlas-server/app/api/routes.py,https://github.com/MJL85/natlas/tree/master/natlas-server/app/api/routes.py,,submit$94,"def submit():
    status_code = None
    response_body = None
    data = request.get_json()
    newhost = {}
    newhost = json.loads(data)
    newhost[""ctime""] = dt.now(tz.utc)
    if newhost[""scan_reason""] == ""requested"":
        mark_scan_completed(newhost[""ip""], newhost[""scan_id""])

    try:
        nmap = NmapParser.parse(newhost.get(""xml_data"", None))
        # If there's more or less than 1 host in the xml data, reject it (for now)
        if nmap.hosts_total != 1:
            status_code = 400
            response_body = json.dumps(
                {
                    ""status"": status_code,
                    ""message"": ""XML had too many hosts in it"",
                    ""retry"": False,
                }
            )

        # If it's not an acceptable target, tell the agent it's out of scope
        elif len(nmap.hosts) == 1 and not current_app.ScopeManager.is_acceptable_target(
            nmap.hosts[0].address
        ):
            status_code = 400
            response_body = json.dumps(
                {
                    ""status"": status_code,
                    ""message"": ""Out of scope: "" + nmap.hosts[0].address,
                    ""retry"": False,
                }
            )

        # If there's no further processing to do, store the host and prepare the response
        elif not newhost[""is_up""] or (newhost[""is_up""] and newhost[""port_count""] == 0):
            current_app.elastic.new_result(newhost)
            status_code = 200
            response_body = json.dumps(
                {""status"": status_code, ""message"": ""Received: "" + newhost[""ip""]}
            )
    except NmapParserException:
        status_code = 400
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": ""Invalid nmap xml data provided"",
                ""retry"": False,
            }
        )

    # If status_code and response_body have been set by this point, return a response.
    if status_code and response_body:
        response = Response(
            response=response_body, status=status_code, content_type=json_content
        )
        return response

    if newhost[""scan_start""] and newhost[""scan_stop""]:
        elapsed = dateutil.parser.parse(newhost[""scan_stop""]) - dateutil.parser.parse(
            newhost[""scan_start""]
        )
        newhost[""elapsed""] = elapsed.seconds

    newhost[""ip""] = nmap.hosts[0].address
    if len(nmap.hosts[0].hostnames) > 0:
        newhost[""hostname""] = nmap.hosts[0].hostnames[0]

    tmpports = []
    newhost[""ports""] = []

    for port in nmap.hosts[0].get_open_ports():
        tmpports.append(str(port[0]))
        srv = nmap.hosts[0].get_service(port[0], port[1])
        portinfo = srv.get_dict()
        portinfo[""service""] = srv.service_dict
        portinfo[""scripts""] = []
        for script in srv.scripts_results:
            scriptsave = {""id"": script[""id""], ""output"": script[""output""]}
            portinfo[""scripts""].append(scriptsave)
            if script[""id""] == ""ssl-cert"":
                portinfo[""ssl""] = parse_ssl_data(script)

        newhost[""ports""].append(portinfo)

    newhost[""port_str""] = "", "".join(tmpports)

    if ""screenshots"" in newhost and newhost[""screenshots""]:
        newhost[""screenshots""], newhost[""num_screenshots""] = process_screenshots(
            newhost[""screenshots""]
        )

    if len(newhost[""ports""]) == 0:
        status_code = 200
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": f""Expected open ports but didn't find any for {newhost['ip']}"",
            }
        )
    elif len(newhost[""ports""]) > 500:
        status_code = 200
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": ""More than 500 ports found, throwing data out"",
            }
        )
    else:
        status_code = 200
        current_app.elastic.new_result(newhost)
        response_body = json.dumps(
            {
                ""status"": status_code,
                ""message"": f""Received {len(newhost['ports'])} ports for {newhost['ip']}"",
            }
        )

    response = Response(
        response=response_body, status=status_code, content_type=json_content
    )
    return response","for script in srv.scripts_results:
    scriptsave = {'id': script['id'], 'output': script['output']}
    portinfo['scripts'].append(scriptsave)
    if script['id'] == 'ssl-cert':
        portinfo['ssl'] = parse_ssl_data(script)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked elements e['id'] and e['output'] are dictionary values that can be accessed directly using the keys 'id' and 'output'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/time/tests/test_basic.py,https://github.com/astropy/astropy/tree/master/astropy/time/tests/test_basic.py,TestSubFormat,test_fits_scale$963,"def test_fits_scale(self):
        """"""Test that the previous FITS-string formatting can still be handled
        but with a DeprecationWarning.""""""
        for inputs in (
            (""2000-01-02(TAI)"", ""tai""),
            (""1999-01-01T00:00:00.123(ET(NIST))"", ""tt""),
            (""2014-12-12T01:00:44.1(UTC)"", ""utc""),
        ):
            with pytest.warns(AstropyDeprecationWarning):
                t = Time(inputs[0])
            assert t.scale == inputs[1]

            # Create Time using normal ISOT syntax and compare with FITS
            t2 = Time(inputs[0][: inputs[0].index(""("")], format=""isot"", scale=inputs[1])
            assert t == t2

        # Explicit check that conversions still work despite warning
        with pytest.warns(AstropyDeprecationWarning):
            t = Time(""1999-01-01T00:00:00.123456789(UTC)"")
        t = t.tai
        assert t.isot == ""1999-01-01T00:00:32.123""

        with pytest.warns(AstropyDeprecationWarning):
            t = Time(""1999-01-01T00:00:32.123456789(TAI)"")
        t = t.utc
        assert t.isot == ""1999-01-01T00:00:00.123""

        # Check scale consistency
        with pytest.warns(AstropyDeprecationWarning):
            t = Time(""1999-01-01T00:00:32.123456789(TAI)"", scale=""tai"")
        assert t.scale == ""tai""
        with pytest.warns(AstropyDeprecationWarning):
            t = Time(""1999-01-01T00:00:32.123456789(ET)"", scale=""tt"")
        assert t.scale == ""tt""
        with pytest.raises(ValueError), pytest.warns(AstropyDeprecationWarning):
            t = Time(""1999-01-01T00:00:32.123456789(TAI)"", scale=""utc"")","for inputs in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(inputs[0])
    assert t.scale == inputs[1]
    t2 = Time(inputs[0][:inputs[0].index('(')], format='isot', scale=inputs[1])
    assert t == t2","for inputs in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    ((inputs_0_0, inputs_0_1, *inputs_0_rinputsmaining), inputs_1, *inputs_rinputsmaining) = inputs
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(inputs_0_0)
    assert t.scale == inputs_1
    t2 = Time(inputs_0_0[:inputs_0_0.index('(')], format='isot', scale=inputs_1)
    assert t == t2","(data, data, *data)","for (inputs_0, inputs_1, *inputs_len) in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(
        inputs_0)
    assert t.scale == 
    inputs_1
    t2 = Time(
    inputs_0[:
    inputs_0.index('(')], format='isot', scale=
    inputs_1)
    assert t == t2",0,,,"Answer: Yes
Iterable Unpacking: (e_0_0, e_0_1, *e_0_remaining), e_1, *e_remaining = e
variable mapping:
e_0_0: e[0]
e_0_1: e[0][:e[0].index('(')]
e_1: e[1]",,,,,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/mysql.py,https://github.com/saltstack/salt/tree/master/salt/modules/mysql.py,,__do_query_into_hash$2673,"def __do_query_into_hash(conn, sql_str):
    """"""
    Perform the query that is passed to it (sql_str).

    Returns:
       results in a dict.

    """"""
    mod = sys._getframe().f_code.co_name
    log.debug(""%s<--(%s)"", mod, sql_str)

    rtn_results = []

    try:
        cursor = conn.cursor()
    except MySQLdb.MySQLError:
        log.error(""%s: Can't get cursor for SQL->%s"", mod, sql_str)
        cursor.close()
        log.debug(""%s-->"", mod)
        return rtn_results

    try:
        _execute(cursor, sql_str)
    except MySQLdb.MySQLError:
        log.error(""%s: try to execute : SQL->%s"", mod, sql_str)
        cursor.close()
        log.debug(""%s-->"", mod)
        return rtn_results

    qrs = cursor.fetchall()

    for row_data in qrs:
        col_cnt = 0
        row = {}
        for col_data in cursor.description:
            col_name = col_data[0]
            row[col_name] = row_data[col_cnt]
            col_cnt += 1

        rtn_results.append(row)

    cursor.close()
    log.debug(""%s-->"", mod)
    return rtn_results","for row_data in qrs:
    col_cnt = 0
    row = {}
    for col_data in cursor.description:
        col_name = col_data[0]
        row[col_name] = row_data[col_cnt]
        col_cnt += 1
    rtn_results.append(row)",,Cannot refactor,Cannot refactor,2,,,"Answer: It depends on the value of col_cnt and the type of iterable object ""e"".
Iterable Unpacking: If the iterable object ""e"" is a sequence type like a list or tuple and col_cnt is an integer type constant that is within the range of the length of ""e"", then the answer is Yes. In this case, the Python code to unpack ""e"" to get unpacked elements and the corresponding variable mapping for each unpacked element is as follows:
e_col_cnt, *e_remaining = e
variable mapping:
e_col_cnt: e[col_cnt]
Otherwise, if the iterable object ""e"" is not a sequence type or col_cnt is not an integer type constant or it is out of range, then the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
chartpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chartpy/chartpy/engine.py,https://github.com/cuemacro/chartpy/tree/master/chartpy/engine.py,EnginePlotly,plot_chart$1458,"def plot_chart(self, data_frame, style, chart_type):

        # Special case if we have a pre-created Plotly object
        if isinstance(data_frame, Figure):
            return self.publish_plot(data_frame, style)

        mode = 'lines'

        if style is None: style = Style()

        marker_size = 1

        x = '';
        y = '';
        z = ''

        scale = 1

        try:
            # Adjust sizing if offline_html format
            if (
                    style.plotly_plot_mode == 'offline_html' and style.scale_factor > 0):
                scale = float(2.0 / 3.0)
        except:
            pass

        if style.plotly_webgl:
            plotly.graph_objs.Scatter = plotly.graph_objs.Scattergl

        # Check other plots implemented by Cufflinks
        cm = ColorMaster()

        # Create figure
        data_frame_list = self.split_data_frame_to_list(data_frame, style)
        fig_list = []
        cols = []

        # If we provide a list of Figures this will get ignored
        try:
            for data_frame in data_frame_list:
                cols.append(data_frame.columns)

            cols = list(numpy.array(cols).flat)

            # Get all the correct colors (and construct gradients if necessary eg. from 'Blues')
            # need to change to strings for cufflinks
            color_list = cm.create_color_list(style, [], cols=cols)
            color_spec = []

            # If no colors are specified then just use our default color set from chart constants
            if color_list == [None] * len(color_list):
                color_spec = [None] * len(color_list)

                for i in range(0, len(color_list)):

                    # Get the color
                    if color_spec[i] is None:
                        color_spec[i] = self.get_color_list(i)

                    try:
                        color_spec[i] = matplotlib.colors.rgb2hex(
                            color_spec[i])
                    except:
                        pass

            else:
                # Otherwise assume all the colors are rgba
                for color in color_list:
                    color = 'rgba' + str(color)
                    color_spec.append(color)
        except Exception as e:
            pass

        start = 0

        title_list = style.title

        if not (isinstance(title_list, list)):
            title_list = [style.title] * len(data_frame_list)

        # Go through each data_frame in the list and plot
        for i in range(0, len(data_frame_list)):
            data_frame = data_frame_list[i]
            title = title_list[i]

            if isinstance(data_frame, Figure):
                fig = data_frame
            else:

                if style.drop_na:
                    data_frame = data_frame.dropna()

                if isinstance(chart_type, list):
                    chart_type_ord = chart_type[i]
                else:
                    chart_type_ord = chart_type

                end = start + len(data_frame.columns)
                color_spec1 = color_spec[start:start + end]
                start = end

                # Special call for choropleth (uses Plotly API directly)
                # Special case for map/choropleth which has yet to be implemented in Cufflinks
                # will likely remove this in the future
                if chart_type_ord == 'choropleth':

                    for col in data_frame.columns:
                        try:
                            data_frame[col] = data_frame[col].astype(str)
                        except:
                            pass

                    if style.color != []:
                        color = style.color
                    else:
                        color = [[0.0, 'rgb(242,240,247)'],
                                 [0.2, 'rgb(218,218,235)'],
                                 [0.4, 'rgb(188,189,220)'], \
                                 [0.6, 'rgb(158,154,200)'],
                                 [0.8, 'rgb(117,107,177)'],
                                 [1.0, 'rgb(84,39,143)']]

                    text = ''

                    if 'text' in data_frame.columns:
                        text = data_frame['Text']

                    data = [dict(
                        type='choropleth',
                        colorscale=color,
                        autocolorscale=False,
                        locations=data_frame['Code'],
                        z=data_frame[style.plotly_choropleth_field].astype(
                            float),
                        locationmode=style.plotly_location_mode,
                        text=text,
                        marker=dict(
                            line=dict(
                                color='rgb(255,255,255)',
                                width=1
                            )
                        ),
                        colorbar=dict(
                            title=style.units
                        )
                    )]

                    layout = dict(
                        title=title,
                        geo=dict(
                            scope=style.plotly_scope,
                            projection=dict(type=style.plotly_projection),
                            showlakes=True,
                            lakecolor='rgb(255, 255, 255)',
                        ),
                    )

                    fig = dict(data=data, layout=layout)

                # Otherwise underlying Cufflinks library underneath
                elif style.plotly_helper == 'cufflinks':

                    # NOTE: we use cufflinks library, which simplifies plotting DataFrames in plotly
                    if chart_type_ord == 'surface':
                        fig = data_frame.iplot(kind=chart_type,
                                               title=title,
                                               xTitle=style.x_title,
                                               yTitle=style.y_title,
                                               zTitle=style.z_title,
                                               x=x, y=y, z=z,
                                               mode=mode,
                                               size=marker_size,
                                               sharing=style.plotly_sharing,
                                               theme=style.plotly_theme,
                                               bestfit=style.line_of_best_fit,
                                               legend=style.display_legend,
                                               colorscale=style.color,
                                               dimensions=(style.width * abs(
                                                   style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                               asFigure=True)

                        # Setting axis is different with a surface
                        if style.x_axis_range is not None:
                            fig.update_layout(scene=dict(
                                xaxis=dict(range=style.x_axis_range)))

                        if style.y_axis_range is not None:
                            fig.update_layout(scene=dict(
                                xaxis=dict(range=style.y_axis_range)))

                        if style.z_axis_range is not None:
                            fig.update_layout(scene=dict(
                                xaxis=dict(range=style.z_axis_range)))

                    elif chart_type_ord == 'heatmap':
                        fig = data_frame.iplot(kind=chart_type,
                                               title=title,
                                               xTitle=style.x_title,
                                               yTitle=style.y_title,
                                               x=x, y=y,
                                               mode=mode,
                                               size=marker_size,
                                               sharing=style.plotly_sharing,
                                               theme=style.plotly_theme,
                                               bestfit=style.line_of_best_fit,
                                               legend=style.display_legend,
                                               colorscale=style.color,
                                               dimensions=(style.width * abs(
                                                   style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                               asFigure=True)

                    elif chart_type_ord == ""annotated-heatmap"":

                        if style.color == []:
                            color = None
                        else:
                            color = style.color

                        fig = px.imshow(data_frame, text_auto=True,
                                        title=style.title,
                                        color_continuous_scale=color,
                                        width=(style.width *
                                               abs(style.scale_factor) * scale),
                                        height= style.height *
                                                abs(style.scale_factor) * scale)

                    # Otherwise we have a line plot (or similar such as a scatter plot, or bar chart etc)
                    else:

                        full_line = style.connect_line_gaps

                        if chart_type_ord == 'line':
                            full_line = True

                            # chart_type_ord = 'scatter'
                            mode = 'lines'
                        elif chart_type_ord in ['dash', 'dashdot', 'dot']:
                            chart_type_ord = 'scatter'

                        elif chart_type_ord == 'line+markers':
                            full_line = True

                            chart_type_ord = 'line'
                            mode = 'lines+markers'
                            marker_size = 5
                        elif chart_type_ord == 'scatter':
                            mode = 'markers'
                            marker_size = 5
                        elif chart_type_ord == 'bubble':
                            chart_type_ord = 'scatter'

                            mode = 'markers'

                        # TODO check this!
                        # Can have issues calling cufflinks with a theme which is None, so split up the cases
                        if style.plotly_theme is None:
                            plotly_theme = 'pearl'
                        else:
                            plotly_theme = style.plotly_theme

                        m = 0

                        y_axis_2_series = [x for x in style.y_axis_2_series if
                                           x in data_frame.columns]

                        vspan = None

                        if style.x_shade_dates is not None:
                            vspan = {
                                'x0': data_frame.index[0].strftime(""%Y-%m-%d""),
                                'x1': data_frame.index[-1].strftime(
                                    ""%Y-%m-%d""), 'color': 'rgba(30,30,30,0.3)',
                                'fill': True, 'opacity': .4}

                        # Sometimes Plotly has issues generating figures in dash, so if fails first, try again
                        while m < 10:

                            if True:
                                if vspan is None:
                                    fig = data_frame.iplot(kind=chart_type_ord,
                                                           title=title,
                                                           xTitle=style.x_title,
                                                           yTitle=style.y_title,
                                                           x=x, y=y, z=z,
                                                           subplots=False,
                                                           sharing=style.plotly_sharing,
                                                           mode=mode,
                                                           secondary_y=y_axis_2_series,
                                                           size=marker_size,
                                                           theme=plotly_theme,
                                                           colorscale='dflt',
                                                           bestfit=style.line_of_best_fit,
                                                           legend=style.display_legend,
                                                           width=style.linewidth,
                                                           color=color_spec1,
                                                           dimensions=(
                                                           style.width * abs(
                                                               style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                                           asFigure=True)
                                else:
                                    fig = data_frame.iplot(kind=chart_type_ord,
                                                           title=title,
                                                           xTitle=style.x_title,
                                                           yTitle=style.y_title,
                                                           x=x, y=y, z=z,
                                                           subplots=False,
                                                           sharing=style.plotly_sharing,
                                                           mode=mode,
                                                           secondary_y=y_axis_2_series,
                                                           size=marker_size,
                                                           theme=plotly_theme,
                                                           colorscale='dflt',
                                                           bestfit=style.line_of_best_fit,
                                                           legend=style.display_legend,
                                                           width=style.linewidth,
                                                           color=color_spec1,
                                                           dimensions=(
                                                           style.width * abs(
                                                               style.scale_factor) * scale,
                                                           style.height * abs(
                                                               style.scale_factor) * scale),
                                                           vspan=vspan,
                                                           asFigure=True)

                                m = 10;
                                break
                                # except Exception as e:
                                print(""Will attempt to re-render: "" + str(e))

                                import time
                                time.sleep(0.3)

                            m = m + 1

                        # For lines set the property of connectgaps (cannot specify directly in cufflinks)
                        if full_line:
                            for z in range(0, len(fig['data'])):
                                fig['data'][
                                    z].connectgaps = style.connect_line_gaps

                                for k in range(0, len(fig['data'])):
                                    if full_line:
                                        fig['data'][
                                            k].connectgaps = style.connect_line_gaps

                        if style.line_shape != None:
                            if isinstance(style.line_shape, str):
                                line_shape = [style.line_shape] * len(
                                    fig['data'])
                            else:
                                line_shape = style.line_shape

                            for k in range(0, len(fig['data'])):
                                fig['data'][k].line.shape = line_shape[k]

                        #if style.plotly_webgl:
                        #    for k in range(0, len(fig['data'])):
                        #        if fig['data'][k].type == 'scatter':
                        #            fig.update_traces(type=""scattergl"")
                                        #selector=dict(type=""bar""))
                                    #fig['data'][k].type = 'scattergl'

                        if style.stackgroup is not None:

                            if isinstance(style.stackgroup, list):
                                stackgroup = style.stackgroup
                            else:
                                stackgroup = ['A'] * len(fig['data'])

                            for k in range(0, len(fig['data'])):
                                fig['data'][k].stackgroup = stackgroup[k]


                # Use plotly express (not implemented yet)
                elif style.plotly_helper == 'plotly_express':
                    # TODO
                    pass

            # Common properties
            # Override other properties, which cannot be set directly by cufflinks/or you want to reset later
            if style.title is not None:
                try:
                    fig.update_layout(title=style.title)
                except:
                    pass

            # Add second y axis title
            if style.y_2_title is not None:
                if style.y_2_title != '':
                    try:
                        fig['layout'].update(
                            yaxis2=dict(title=style.y_2_title))
                    except:
                        pass

            if style.x_axis_range is not None:
                try:
                    fig['layout'].update(
                        xaxis=dict(range=style.x_axis_range, autorange=False))
                except:
                    pass

            if style.y_axis_range is not None:

                try:
                    fig['layout'].update(
                        yaxis=dict(range=style.y_axis_range, autorange=False))
                except:
                    pass

            if style.y_axis_2_range is not None:
                try:
                    fig['layout'].update(
                        yaxis2=dict(range=style.y_axis_2_range,
                                    autorange=False))
                except:
                    pass

            if style.z_axis_range is not None:
                try:
                    fig['layout'].update(
                        zaxis=dict(range=style.z_axis_range, autorange=False))
                except:
                    pass

            if style.font_family is not None:
                try:
                    fig.update_layout(font_family=style.font_family)
                except:
                    pass

            if style.x_axis_type is not None:
                try:
                    fig.update_xaxes(type=style.x_axis_type)
                except:
                    pass

            if style.y_axis_type is not None:
                try:
                    fig.update_yaxes(type=style.y_axis_type)
                except:
                    pass

            if style.x_dtick is not None:
                try:
                    fig.update_layout(
                        xaxis=dict(tickmode='linear', dtick=style.x_dtick))
                except:
                    pass

            if style.y_dtick is not None:
                try:
                    fig.update_layout(
                        yaxis=dict(tickmode='linear', dtick=style.y_dtick))
                except:
                    pass

            # Add shaded regions
            fig = self._multi_shade(fig, style)

            # Legend Properties
            if style.legend_x_anchor is not None:
                try:
                    fig.update_layout(
                        legend=dict(xanchor=style.legend_x_anchor))
                except:
                    pass

            if style.legend_y_anchor is not None:
                try:
                    fig.update_layout(
                        legend=dict(yanchor=style.legend_y_anchor))
                except:
                    pass

            if style.legend_x_pos is not None:
                try:
                    fig.update_layout(legend=dict(x=style.legend_x_pos))
                except:
                    pass

            if style.legend_y_pos is not None:
                try:
                    fig.update_layout(legend=dict(y=style.legend_y_pos))
                except:
                    pass

            if style.legend_bgcolor is not None:
                try:
                    fig.update_layout(
                        legend=dict(bgcolor=style.legend_bgcolor))
                except:
                    pass

            if style.legend_orientation is not None:
                try:
                    fig.update_layout(
                        legend=dict(orientation=style.legend_orientation))
                except:
                    pass

            if style.barmode is not None:
                try:
                    fig.update_layout(barmode=style.barmode)
                except:
                    pass

            fig_list.append(fig)

        #### Plotted all the lines

        # Create subplots if more than one figure
        if len(fig_list) > 1 and style.animate_figure == False \
                and style.subplots == True:
            from plotly.subplots import make_subplots

            if style.subplot_titles:
                fig = make_subplots(rows=len(fig_list), cols=1,
                                    subplot_titles=style.subplot_titles)
            else:
                fig = make_subplots(rows=len(fig_list), cols=1)

            # layout = fig_list[0]['layout']

            # fig.layout = fig_list[0].layout

            # for k, v in list(layout.items()):
            #     if 'xaxis' not in k and 'yaxis' not in k:
            #         fig['layout'].update({k: v})

            for i, f in enumerate(fig_list):
                f = f.data[0]
                f.update(legendgroup=i)
                fig.add_trace(f, row=i + 1, col=1)

            # fig = cf.subplots(fig_list, base_layout=fig_list[0].to_dict()['layout'], shape=(len(fig_list), 1),
            #                  shared_xaxes=False, shared_yaxes=False)

            if not (isinstance(style.title, list)):
                fig['layout'].update(title=style.title)

            fig.update_layout(
                height=style.height * abs(style.scale_factor),
                width=style.width * abs(style.scale_factor),
                showlegend=style.display_legend
            )

        elif style.animate_figure:

            fig = fig_list[0]

            # Add buttons to play/pause
            fig[""layout""][""updatemenus""] = [
                {
                    ""buttons"": [
                        {
                            ""args"": [None, {
                                ""frame"": {""duration"": style.animate_frame_ms,
                                          ""redraw"": True},
                                ""fromcurrent"": True, ""transition"": {
                                    ""duration"": style.animate_frame_ms,
                                    ""easing"": ""quadratic-in-out""}}],
                            ""label"": ""Play"",
                            ""method"": ""animate""
                        },
                        {
                            ""args"": [[None],
                                     {""frame"": {""duration"": 0, ""redraw"": True},
                                      ""mode"": ""immediate"",
                                      ""transition"": {""duration"": 0}}],
                            ""label"": ""Pause"",
                            ""method"": ""animate""
                        }
                    ],
                    ""direction"": ""left"",
                    ""pad"": {""r"": 10, ""t"": 87},
                    ""showactive"": False,
                    ""type"": ""buttons"",
                    ""x"": 0.1,
                    ""xanchor"": ""right"",
                    ""y"": 0,
                    ""yanchor"": ""top""
                }
            ]

            if style.animate_titles is not None:
                animate_titles = style.animate_titles
            else:
                animate_titles = list(range(0, len(fig_list)))

            # Add an animation frame for each data frame
            frames = []

            for fig_temp, title_temp in zip(fig_list, animate_titles):
                frames.append(go.Frame(data=fig_temp['data'],
                                       name=str(title_temp),
                                       layout=go.Layout(
                                           title=str(title_temp))))

            fig.update(frames=frames)

            # Add a slider, with the frame labels
            sliders_dict = {
                ""active"": 0,
                ""yanchor"": ""top"",
                ""xanchor"": ""left"",
                ""currentvalue"": {
                    ""visible"": True,
                    ""xanchor"": ""right""
                },
                ""transition"": {""duration"": style.animate_frame_ms,
                               ""easing"": ""cubic-in-out""},
                ""pad"": {""b"": 10, ""t"": 50},
                ""len"": 0.9,
                ""x"": 0.1,
                ""y"": 0,
                ""steps"": []
            }

            for i in range(0, len(fig_list)):
                slider_step = {""args"": [
                    [animate_titles[i]],
                    {""frame"": {""duration"": style.animate_frame_ms,
                               ""redraw"": True},
                     ""mode"": ""immediate"",
                     ""transition"": {""duration"": style.animate_frame_ms}}
                ],
                    ""label"": str(animate_titles[i]),
                    ""method"": ""animate""}

                sliders_dict[""steps""].append(slider_step)

            fig[""layout""][""sliders""] = [sliders_dict]

            # else:
            # Add an animation frame for each data frame
            #    fig.update(frames=[go.Frame(data=fig_temp['data']) for fig_temp in fig_list])

        else:
            fig = fig_list[0]

        fig.update(dict(layout=dict(legend=dict(
            x=0.05,
            y=1
        ))))

        # Adjust margins
        if style.thin_margin:
            fig.update(dict(layout=dict(margin=go.layout.Margin(
                l=20,
                r=20,
                b=40,
                t=40,
                pad=0
            ))))

        # Change background color
        fig.update(dict(layout=dict(paper_bgcolor='rgba(0,0,0,0)')))
        fig.update(dict(layout=dict(plot_bgcolor='rgba(0,0,0,0)')))

        # Deal with grids
        if (not (style.x_axis_showgrid)): fig.update(
            dict(layout=dict(xaxis=dict(showgrid=style.x_axis_showgrid))))
        if (not (style.y_axis_showgrid)): fig.update(
            dict(layout=dict(yaxis=dict(showgrid=style.y_axis_showgrid))))
        if (not (style.y_axis_2_showgrid)): fig.update(
            dict(layout=dict(yaxis2=dict(showgrid=style.y_axis_2_showgrid))))

        # Override properties, which cannot be set directly by cufflinks

        # For the type of line (ie. line or scatter)
        # For making the lined dashed, dotted etc.
        if style.subplots == False and isinstance(chart_type, list):
            for j in range(0, len(fig['data'])):
                mode = None;
                dash = None;
                line_shape = None;

                if chart_type[j] == 'line':
                    mode = 'lines'
                elif chart_type[j] == 'line+markers':
                    mode = 'lines+markers'
                elif chart_type[j] == 'scatter':
                    mode = 'markers'
                elif chart_type[j] in ['dash', 'dashdot', 'dot']:
                    dash = chart_type[j]
                    mode = 'lines'
                elif chart_type[j] in ['hv', 'vh', 'vhv', 'spline', 'linear']:
                    line_shape = chart_type[j]
                    mode = 'lines'
                elif chart_type[j] == 'bubble':
                    mode = 'markers'

                    bubble_series = style.bubble_series[cols[j]]
                    bubble_series = bubble_series.fillna(0)

                    # dash = chart_type[j]
                    # data_frame[bubble_series.name] = bubble_series
                    scale = float(bubble_series.max())

                    fig['data'][j].marker.size = \
                        (style.bubble_size_scalar * (
                                    bubble_series.values / scale)).tolist()

                if mode is not None:
                    fig['data'][j].mode = mode

                if dash is not None:
                    fig['data'][j].line.dash = dash

                if line_shape is not None:
                    fig['data'][j].line.shape = line_shape

        # If candlestick specified add that (needed to be appended on top of the Plotly figure's data
        if style.candlestick_series is not None and not (style.plotly_webgl):

            # self.logger.debug(""About to create candlesticks"")

            if isinstance(style.candlestick_series, Figure):
                fig_candle = style.candlestick_series
            else:
                # from plotly.tools import FigureFactory as FF
                fig_candle = create_candlestick(
                    style.candlestick_series['open'],
                    style.candlestick_series['high'],
                    style.candlestick_series['low'],
                    style.candlestick_series['close'],
                    dates=style.candlestick_series['close'].index
                    )

            if style.candlestick_increasing_color is not None:
                # Increasing
                fig_candle['data'][0].fillcolor = cm.get_color_code(
                    style.candlestick_increasing_color)
                fig_candle['data'][0].line.color = cm.get_color_code(
                    style.candlestick_increasing_line_color)

            if style.candlestick_decreasing_color is not None:
                # Decreasing
                fig_candle['data'][1].fillcolor = cm.get_color_code(
                    style.candlestick_decreasing_color)
                fig_candle['data'][1].line.color = cm.get_color_code(
                    style.candlestick_decreasing_line_color)

            try:
                # Append the data to the existing Plotly figure, plotted earlier
                fig.data.append(fig_candle.data[0]);
                fig.data.append(fig_candle.data[1])
            except:
                # Later version of Plotly
                fig.add_trace(fig_candle.data[0])
                fig.add_trace(fig_candle.data[1])

        # Overlay other Plotly figures on top of
        if style.overlay_fig is not None:

            for d in style.overlay_fig.data:
                fig.add_trace(d)

        x_y_line_list = []

        # fig.layout.yrange
        # add x-line:
        for x_y_line in style.x_y_line:
            start = x_y_line[0]
            finish = x_y_line[1]

            x_y_line_list.append(
                {
                    'type': 'line',
                    'x0': start[0],
                    'y0': start[1],
                    'x1': finish[0],
                    'y1': finish[1],
                    'line': {
                        'color': 'black',
                        'width': 0.5,
                        'dash': 'dot',
                    },
                }
            )

        # x_y_line_list =  [{
        #     'type': 'line',
        #     'x0': 1,
        #     'y0': 0,
        #     'x1': 1,
        #     'y1': 2,
        #     'li","for (fig_temp, title_temp) in zip(fig_list, animate_titles):
    frames.append(go.Frame(data=fig_temp['data'], name=str(title_temp), layout=go.Layout(title=str(title_temp))))",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['data'] is a dictionary value that can be accessed directly using the key 'data'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/smooth_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/smooth_uv.py,MUV_OT_SmoothUV,__smooth_wo_transmission$107,"def __smooth_wo_transmission(self, loop_seqs, uv_layer):
        # calculate path length
        loops = []
        for hseq in loop_seqs:
            loops.extend([hseq[0][0], hseq[0][1]])
        full_vlen = 0
        accm_vlens = [0.0]
        full_uvlen = 0
        accm_uvlens = [0.0]
        orig_uvs = [loop_seqs[0][0][0][uv_layer].uv.copy()]
        for l1, l2 in zip(loops[:-1], loops[1:]):
            diff_v = l2.vert.co - l1.vert.co
            full_vlen = full_vlen + diff_v.length
            accm_vlens.append(full_vlen)
            diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
            full_uvlen = full_uvlen + diff_uv.length
            accm_uvlens.append(full_uvlen)
            orig_uvs.append(l2[uv_layer].uv.copy())

        for hidx, hseq in enumerate(loop_seqs):
            pair = hseq[0]
            for pidx, l in enumerate(pair):
                if self.select:
                    l[uv_layer].select = True

                # ignore start/end loop
                if (hidx == 0 and pidx == 0) or\
                   ((hidx == len(loop_seqs) - 1) and (pidx == len(pair) - 1)):
                    continue

                # calculate target path length
                # target = no influenced * (1 - infl) + influenced * infl
                tgt_noinfl = full_uvlen * (hidx + pidx) / (len(loop_seqs))
                tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
                target_length = tgt_noinfl * (1 - self.mesh_infl) + \
                    tgt_infl * self.mesh_infl

                # get target UV
                for i in range(len(accm_uvlens[:-1])):
                    # get line segment which UV will be placed
                    if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                        tgt_seg_len = target_length - accm_uvlens[i]
                        seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                        uv1 = orig_uvs[i]
                        uv2 = orig_uvs[i + 1]
                        target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                        break
                else:
                    self.report({'ERROR'}, ""Failed to get target UV"")
                    return {'CANCELLED'}

                # update UV
                l[uv_layer].uv = target_uv","for hseq in loop_seqs:
    loops.extend([hseq[0][0], hseq[0][1]])","for hseq in loop_seqs:
    (((hseq_0_0, hseq_0_1), *hseq_0_rhseqmaining), *hseq_rhseqmaining) = hseq
    loops.extend([hseq_0_0, hseq_0_1])","((data, data, *data), *data)","for ((hseq_0_0, hseq_0_1, *hseq_0_len), *hseq_len) in loop_seqs:
    loops.extend([
    hseq_0_0, 
    hseq_0_1])",0,,,"Answer: Yes
Iterable Unpacking: ((e_0_0, e_0_1), *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]
e_0_1: e[0][1]",,,,,,,,
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/smooth_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/smooth_uv.py,MUV_OT_SmoothUV,__smooth_wo_transmission$107,"def __smooth_wo_transmission(self, loop_seqs, uv_layer):
        # calculate path length
        loops = []
        for hseq in loop_seqs:
            loops.extend([hseq[0][0], hseq[0][1]])
        full_vlen = 0
        accm_vlens = [0.0]
        full_uvlen = 0
        accm_uvlens = [0.0]
        orig_uvs = [loop_seqs[0][0][0][uv_layer].uv.copy()]
        for l1, l2 in zip(loops[:-1], loops[1:]):
            diff_v = l2.vert.co - l1.vert.co
            full_vlen = full_vlen + diff_v.length
            accm_vlens.append(full_vlen)
            diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
            full_uvlen = full_uvlen + diff_uv.length
            accm_uvlens.append(full_uvlen)
            orig_uvs.append(l2[uv_layer].uv.copy())

        for hidx, hseq in enumerate(loop_seqs):
            pair = hseq[0]
            for pidx, l in enumerate(pair):
                if self.select:
                    l[uv_layer].select = True

                # ignore start/end loop
                if (hidx == 0 and pidx == 0) or\
                   ((hidx == len(loop_seqs) - 1) and (pidx == len(pair) - 1)):
                    continue

                # calculate target path length
                # target = no influenced * (1 - infl) + influenced * infl
                tgt_noinfl = full_uvlen * (hidx + pidx) / (len(loop_seqs))
                tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
                target_length = tgt_noinfl * (1 - self.mesh_infl) + \
                    tgt_infl * self.mesh_infl

                # get target UV
                for i in range(len(accm_uvlens[:-1])):
                    # get line segment which UV will be placed
                    if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                        tgt_seg_len = target_length - accm_uvlens[i]
                        seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                        uv1 = orig_uvs[i]
                        uv2 = orig_uvs[i + 1]
                        target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                        break
                else:
                    self.report({'ERROR'}, ""Failed to get target UV"")
                    return {'CANCELLED'}

                # update UV
                l[uv_layer].uv = target_uv","for (l1, l2) in zip(loops[:-1], loops[1:]):
    diff_v = l2.vert.co - l1.vert.co
    full_vlen = full_vlen + diff_v.length
    accm_vlens.append(full_vlen)
    diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
    full_uvlen = full_uvlen + diff_uv.length
    accm_uvlens.append(full_uvlen)
    orig_uvs.append(l2[uv_layer].uv.copy())",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e[uv_layer], uv_layer is a variable that cannot be accessed directly using an integer type constant. Therefore, iterable unpacking is not applicable in this case as index is not an integer type constant.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.***************
Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e[uv_layer], uv_layer is a variable that cannot be accessed directly using an integer type constant. Therefore, iterable unpacking is not applicable in this case as index is not an integer type constant.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/smooth_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/smooth_uv.py,MUV_OT_SmoothUV,__smooth_wo_transmission$107,"def __smooth_wo_transmission(self, loop_seqs, uv_layer):
        # calculate path length
        loops = []
        for hseq in loop_seqs:
            loops.extend([hseq[0][0], hseq[0][1]])
        full_vlen = 0
        accm_vlens = [0.0]
        full_uvlen = 0
        accm_uvlens = [0.0]
        orig_uvs = [loop_seqs[0][0][0][uv_layer].uv.copy()]
        for l1, l2 in zip(loops[:-1], loops[1:]):
            diff_v = l2.vert.co - l1.vert.co
            full_vlen = full_vlen + diff_v.length
            accm_vlens.append(full_vlen)
            diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
            full_uvlen = full_uvlen + diff_uv.length
            accm_uvlens.append(full_uvlen)
            orig_uvs.append(l2[uv_layer].uv.copy())

        for hidx, hseq in enumerate(loop_seqs):
            pair = hseq[0]
            for pidx, l in enumerate(pair):
                if self.select:
                    l[uv_layer].select = True

                # ignore start/end loop
                if (hidx == 0 and pidx == 0) or\
                   ((hidx == len(loop_seqs) - 1) and (pidx == len(pair) - 1)):
                    continue

                # calculate target path length
                # target = no influenced * (1 - infl) + influenced * infl
                tgt_noinfl = full_uvlen * (hidx + pidx) / (len(loop_seqs))
                tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
                target_length = tgt_noinfl * (1 - self.mesh_infl) + \
                    tgt_infl * self.mesh_infl

                # get target UV
                for i in range(len(accm_uvlens[:-1])):
                    # get line segment which UV will be placed
                    if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                        tgt_seg_len = target_length - accm_uvlens[i]
                        seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                        uv1 = orig_uvs[i]
                        uv2 = orig_uvs[i + 1]
                        target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                        break
                else:
                    self.report({'ERROR'}, ""Failed to get target UV"")
                    return {'CANCELLED'}

                # update UV
                l[uv_layer].uv = target_uv","for (pidx, l) in enumerate(pair):
    if self.select:
        l[uv_layer].select = True
    if hidx == 0 and pidx == 0 or (hidx == len(loop_seqs) - 1 and pidx == len(pair) - 1):
        continue
    tgt_noinfl = full_uvlen * (hidx + pidx) / len(loop_seqs)
    tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
    target_length = tgt_noinfl * (1 - self.mesh_infl) + tgt_infl * self.mesh_infl
    for i in range(len(accm_uvlens[:-1])):
        if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
            tgt_seg_len = target_length - accm_uvlens[i]
            seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
            uv1 = orig_uvs[i]
            uv2 = orig_uvs[i + 1]
            target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
            break
    else:
        self.report({'ERROR'}, 'Failed to get target UV')
        return {'CANCELLED'}
    l[uv_layer].uv = target_uv",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: The given unpacked element e[uv_eayer] is not valid as the variable name ""uv_eayer"" is not defined. Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
qutip,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutip/qutip/legacy/bloch_redfield.py,https://github.com/qutip/qutip/tree/master/qutip/legacy/bloch_redfield.py,,bloch_redfield_tensor$21,"def bloch_redfield_tensor(H, a_ops, spectra_cb=None, c_ops=[], use_secular=True, sec_cutoff=0.1):
    """"""
    Calculate the Bloch-Redfield tensor for a system given a set of operators
    and corresponding spectral functions that describes the system's coupling
    to its environment.

    .. note::

        This tensor generation requires a time-independent Hamiltonian.

    Parameters
    ----------

    H : :class:`qutip.qobj`
        System Hamiltonian.

    a_ops : list of :class:`qutip.qobj`
        List of system operators that couple to the environment.

    spectra_cb : list of callback functions
        List of callback functions that evaluate the noise power spectrum
        at a given frequency.

    c_ops : list of :class:`qutip.qobj`
        List of system collapse operators.

    use_secular : bool
        Flag (True of False) that indicates if the secular approximation should
        be used.
    
    sec_cutoff : float {0.1}
        Threshold for secular approximation.

    Returns
    -------

    R, kets: :class:`qutip.Qobj`, list of :class:`qutip.Qobj`

        R is the Bloch-Redfield tensor and kets is a list eigenstates of the
        Hamiltonian.

    """"""
    
    if not (spectra_cb is None):
        warnings.warn(""The use of spectra_cb is depreciated."", DeprecationWarning)
        _a_ops = []
        for kk, a in enumerate(a_ops):
            _a_ops.append([a,spectra_cb[kk]])
        a_ops = _a_ops
    
    # Sanity checks for input parameters
    if not isinstance(H, Qobj):
        raise TypeError(""H must be an instance of Qobj"")

    for a in a_ops:
        if not isinstance(a[0], Qobj) or not a[0].isherm:
            raise TypeError(""Operators in a_ops must be Hermitian Qobj."")

    if c_ops is None:
        c_ops = []

    # use the eigenbasis
    evals, ekets = H.eigenstates()

    N = len(evals)
    K = len(a_ops)
    
    #only Lindblad collapse terms
    if K==0:
        Heb = qdiags(evals,0,dims=H.dims)
        L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])
        return L, ekets
    
    
    A = np.array([a_ops[k][0].transform(ekets).full() for k in range(K)])
    Jw = np.zeros((K, N, N), dtype=complex)

    # pre-calculate matrix elements and spectral densities
    # W[m,n] = real(evals[m] - evals[n])
    W = np.real(evals[:,np.newaxis] - evals[np.newaxis,:])

    for k in range(K):
        # do explicit loops here in case spectra_cb[k] can not deal with array arguments
        for n in range(N):
            for m in range(N):
                Jw[k, n, m] = a_ops[k][1](W[n, m])

    dw_min = np.abs(W[W.nonzero()]).min()

    # pre-calculate mapping between global index I and system indices a,b
    Iabs = np.empty((N*N,3),dtype=int)
    for I, Iab in enumerate(Iabs):
        # important: use [:] to change array values, instead of creating new variable Iab
        Iab[0]  = I
        Iab[1:] = vec2mat_index(N, I)

    # unitary part + dissipation from c_ops (if given):
    Heb = qdiags(evals,0,dims=H.dims)
    L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])
    
    # dissipative part:
    rows = []
    cols = []
    data = []
    for I, a, b in Iabs:
        # only check use_secular once per I
        if use_secular:
            # only loop over those indices J which actually contribute
            Jcds = Iabs[np.where(np.abs(W[a, b] - W[Iabs[:,1], Iabs[:,2]]) < dw_min * sec_cutoff)]
        else:
            Jcds = Iabs
        for J, c, d in Jcds:
            elem = 0+0j
            # summed over k, i.e., each operator coupling the system to the environment
            elem += 0.5 * np.sum(A[:, a, c] * A[:, d, b] * (Jw[:, c, a] + Jw[:, d, b]))
            if b==d:
                #                  sum_{k,n} A[k, a, n] * A[k, n, c] * Jw[k, c, n])
                elem -= 0.5 * np.sum(A[:, a, :] * A[:, :, c] * Jw[:, c, :])
            if a==c:
                #                  sum_{k,n} A[k, d, n] * A[k, n, b] * Jw[k, d, n])
                elem -= 0.5 * np.sum(A[:, d, :] * A[:, :, b] * Jw[:, d, :])
            if elem != 0:
                rows.append(I)
                cols.append(J)
                data.append(elem)

    R = arr_coo2fast(np.array(data, dtype=complex),
                    np.array(rows, dtype=np.int32),
                    np.array(cols, dtype=np.int32), N**2, N**2)
    
    L.data = L.data + R
    
    return L, ekets","for (I, Iab) in enumerate(Iabs):
    Iab[0] = I
    Iab[1:] = vec2mat_index(N, I)","for (I, Iab) in enumerate(Iabs):
    (Iab_0, *Iab_1) = Iab
    Iab_0 = I
    Iab[1:] = vec2mat_index(N, I)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, *e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1:]",,,,,,,,it actually cannot refactor
DeepNER,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepNER/src/utils/evaluator.py,https://github.com/z814081807/DeepNER/tree/master/src/utils/evaluator.py,,mrc_evaluation$240,"def mrc_evaluation(model, dev_info, device):
    dev_loader, (dev_callback_info, type_weight) = dev_info

    start_logits, end_logits = None, None

    model.eval()

    for tmp_pred in get_base_out(model, dev_loader, device):
        tmp_start_logits = tmp_pred[0].cpu().numpy()
        tmp_end_logits = tmp_pred[1].cpu().numpy()

        if start_logits is None:
            start_logits = tmp_start_logits
            end_logits = tmp_end_logits
        else:
            start_logits = np.append(start_logits, tmp_start_logits, axis=0)
            end_logits = np.append(end_logits, tmp_end_logits, axis=0)

    assert len(start_logits) == len(end_logits) == len(dev_callback_info)

    role_metric = np.zeros([13, 3])

    mirco_metrics = np.zeros(3)

    id2ent = {x: i for i, x in enumerate(ENTITY_TYPES)}

    for tmp_start_logits, tmp_end_logits, tmp_callback \
            in zip(start_logits, end_logits, dev_callback_info):

        text, text_offset, ent_type, gt_entities = tmp_callback

        tmp_start_logits = tmp_start_logits[text_offset:text_offset+len(text)]
        tmp_end_logits = tmp_end_logits[text_offset:text_offset+len(text)]

        pred_entities = mrc_decode(tmp_start_logits, tmp_end_logits, text)

        role_metric[id2ent[ent_type]] += calculate_metric(gt_entities, pred_entities)

    for idx, _type in enumerate(ENTITY_TYPES):
        temp_metric = get_p_r_f(role_metric[idx][0], role_metric[idx][1], role_metric[idx][2])

        mirco_metrics += temp_metric * type_weight[_type]

    metric_str = f'[MIRCO] precision: {mirco_metrics[0]:.4f}, ' \
                  f'recall: {mirco_metrics[1]:.4f}, f1: {mirco_metrics[2]:.4f}'

    return metric_str, mirco_metrics[2]","for (tmp_start_logits, tmp_end_logits, tmp_callback) in zip(start_logits, end_logits, dev_callback_info):
    (text, text_offset, ent_type, gt_entities) = tmp_callback
    tmp_start_logits = tmp_start_logits[text_offset:text_offset + len(text)]
    tmp_end_logits = tmp_end_logits[text_offset:text_offset + len(text)]
    pred_entities = mrc_decode(tmp_start_logits, tmp_end_logits, text)
    role_metric[id2ent[ent_type]] += calculate_metric(gt_entities, pred_entities)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e[text_offset:text_offset + len(text)] is a slice of the iterable object ""e"". However, iterable unpacking is not applicable in this case as the slice is not a fixed number of elements that can be unpacked.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.***************
Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e[text_offset:text_offset + len(text)] is a slice of the iterable object ""e"". However, iterable unpacking is not applicable in this case as the slice is not a fixed number of elements that can be unpacked.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
pandas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandas/pandas/core/internals/concat.py,https://github.com/pandas-dev/pandas/tree/master/pandas/core/internals/concat.py,,_combine_concat_plans$704,"def _combine_concat_plans(plans, concat_axis: AxisInt):
    """"""
    Combine multiple concatenation plans into one.

    existing_plan is updated in-place.
    """"""
    if len(plans) == 1:
        for p in plans[0]:
            yield p[0], [p[1]]

    elif concat_axis == 0:
        offset = 0
        for plan in plans:
            last_plc = None

            for plc, unit in plan:
                yield plc.add(offset), [unit]
                last_plc = plc

            if last_plc is not None:
                offset += last_plc.as_slice.stop

    else:
        # singleton list so we can modify it as a side-effect within _next_or_none
        num_ended = [0]

        def _next_or_none(seq):
            retval = next(seq, None)
            if retval is None:
                num_ended[0] += 1
            return retval

        plans = list(map(iter, plans))
        next_items = list(map(_next_or_none, plans))

        while num_ended[0] != len(next_items):
            if num_ended[0] > 0:
                raise ValueError(""Plan shapes are not aligned"")

            placements, units = zip(*next_items)

            lengths = list(map(len, placements))
            min_len, max_len = min(lengths), max(lengths)

            if min_len == max_len:
                yield placements[0], units
                next_items[:] = map(_next_or_none, plans)
            else:
                yielded_placement = None
                yielded_units = [None] * len(next_items)
                for i, (plc, unit) in enumerate(next_items):
                    yielded_units[i] = unit
                    if len(plc) > min_len:
                        # _trim_join_unit updates unit in place, so only
                        # placement needs to be sliced to skip min_len.
                        next_items[i] = (plc[min_len:], _trim_join_unit(unit, min_len))
                    else:
                        yielded_placement = plc
                        next_items[i] = _next_or_none(plans[i])

                yield yielded_placement, yielded_units","for (i, (plc, unit)) in enumerate(next_items):
    yielded_units[i] = unit
    if len(plc) > min_len:
        next_items[i] = (plc[min_len:], _trim_join_unit(unit, min_len))
    else:
        yielded_placement = plc
        next_items[i] = _next_or_none(plans[i])",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_min_len, = e[min_len:]
variable mapping:
e[min_len:]: e[min_len:]",,,,,,,,it actually cannot refactor
strictyaml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/strictyaml/strictyaml/ruamel/comments.py,https://github.com/crdoconnor/strictyaml/tree/master/strictyaml/ruamel/comments.py,CommentedMap,__getitem__$757,"def __getitem__(self, key):
        # type: (Any) -> Any
        try:
            return ordereddict.__getitem__(self, key)
        except KeyError:
            for merged in getattr(self, merge_attrib, []):
                if key in merged[1]:
                    return merged[1][key]
            raise","for merged in getattr(self, merge_attrib, []):
    if key in merged[1]:
        return merged[1][key]",It cannot be refactored by var unpacking,"(data, data, *data)","for (merged_0, merged_1, *merged_len) in getattr(self, merge_attrib, []):
    if key in 
    merged_1:
        return 
        merged_1[key]",0,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e[1][key], the key is not a constant integer value. Therefore, iterable unpacking is not applicable in this case. Instead, we can access the value using dictionary indexing as e[1]['key'].
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/CreatHtml.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/CreatHtml.py,CreatHtml,vuln_list$27,"def vuln_list(self):
        global creat_vuln_num
        creat_vuln_num = 1
        tr_whole_api_list = """"

        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        for vuln_info in vuln_infos:
            if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    tr_api_list = """"""
                    <tr>
                      <td>{vuln_num}</td>
                      <td>{vuln_api_name}</td>
                      <td>{vuln_url}</td>
                      <td>{vuln_url_type}</td>
                      <td>{vuln_risk}</td>
                      <td>{vuln_length}</td>
                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>
                    </tr>""""""
                    if api_info[5] == 1:
                        vuln_url_type = Utils().getMyWord(""{r_get}"")
                    else:
                        vuln_url_type = Utils().getMyWord(""{r_post}"")
                    tr_api_list = tr_api_list.replace(""{vuln_num}"", str(self.creat_api_num))
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    try:
                        api_length_info = len(api_info[4])
                    except:
                        api_length_info = 0
                    for js_path in js_paths:
                        tr_api_list = tr_api_list.replace(""{vuln_api_name}"",api_info[2])
                        tr_api_list = tr_api_list.replace(""{vuln_url}"", api_info[1])
                        tr_api_list = tr_api_list.replace(""{vuln_url_type}"", vuln_url_type)
                        tr_api_list = tr_api_list.replace(""{vuln_length}"", str(api_length_info))
                        tr_api_list = tr_api_list.replace(""{vuln_risk}"", Utils().getMyWord(""{r_l_m}""))
                        tr_api_list = tr_api_list.replace(""{vuln_id}"", ""vuln_"" + str(creat_vuln_num))
                        self.creat_api_num = self.creat_api_num + 1
                        creat_vuln_num = creat_vuln_num + 1
                        tr_whole_api_list = tr_whole_api_list + tr_api_list
            elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    tr_api_list = """"""
                    <tr>
                      <td>{vuln_num}</td>
                      <td>{vuln_api_name}</td>
                      <td>{vuln_url}</td>
                      <td>{vuln_url_type}</td>
                      <td>{vuln_risk}</td>
                      <td>{vuln_length}</td>
                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>
                    </tr>""""""
                    if api_info[5] == 1:
                        vuln_url_type = Utils().getMyWord(""{r_get}"")
                    else:
                        vuln_url_type = Utils().getMyWord(""{r_post}"")
                    tr_api_list = tr_api_list.replace(""{vuln_num}"", str(self.creat_api_num))
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    try:
                        api_length_info = len(api_info[4])
                    except:
                        api_length_info = 0
                    for js_path in js_paths:
                        tr_api_list = tr_api_list.replace(""{vuln_api_name}"", api_info[2])
                        tr_api_list = tr_api_list.replace(""{vuln_url}"", api_info[1])
                        tr_api_list = tr_api_list.replace(""{vuln_url_type}"", vuln_url_type)
                        tr_api_list = tr_api_list.replace(""{vuln_length}"", str(api_length_info))
                        tr_api_list = tr_api_list.replace(""{vuln_risk}"", Utils().getMyWord(""{r_l_l}"") + ""锛"" + Utils().getMyWord(""{r_vuln_maybe}""))
                        tr_api_list = tr_api_list.replace(""{vuln_id}"", ""vuln_"" + str(creat_vuln_num))
                        self.creat_api_num = self.creat_api_num + 1
                        creat_vuln_num = creat_vuln_num + 1
                        tr_whole_api_list = tr_whole_api_list + tr_api_list
        return tr_whole_api_list","for vuln_info in vuln_infos:
    if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list
    elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '锛' + Utils().getMyWord('{r_vuln_maybe}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list","for vuln_info in vuln_infos:
    (vuln_info_1, vuln_info_2, vuln_info_3, vuln_info_4, *_) = vuln_info
    if vuln_info_3 == 'unAuth' and vuln_info_4 == 1:
        sql = ""select * from api_tree where id='%s'"" % vuln_info_1
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info_2
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list
    elif vuln_info_3 == 'unAuth' and vuln_info_4 == 2:
        sql = ""select * from api_tree where id='%s'"" % vuln_info_1
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info_2
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '锛' + Utils().getMyWord('{r_vuln_maybe}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list","(data, data, data, data, data, *data)","for (vuln_info_0, vuln_info_1, vuln_info_2, vuln_info_3, vuln_info_4, *vuln_info_len) in vuln_infos:
    if 
    vuln_info_3 == 'unAuth' and 
    vuln_info_4 == 1:
        sql = ""select * from api_tree where id='%s'"" % 
        vuln_info_1
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % 
            vuln_info_2
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list
    elif 
    vuln_info_3 == 'unAuth' and 
    vuln_info_4 == 2:
        sql = ""select * from api_tree where id='%s'"" % 
        vuln_info_1
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % 
            vuln_info_2
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '锛' + Utils().getMyWord('{r_vuln_maybe}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list",0,,,"Answer: Yes
Iterable Unpacking: e_1, e_2, e_3, e_4 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,,
microk8s,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/microk8s/scripts/wrappers/status.py,https://github.com/ubuntu/microk8s/tree/master/scripts/wrappers/status.py,,print_pretty$33,"def print_pretty(isReady, enabled_addons, disabled_addons):
    console_formatter = ""{:>3} {:<20} # ({}) {}""
    if isReady:
        print(""microk8s is running"")
        if not is_ha_enabled():
            print(""high-availability: no"")
        else:
            info = get_dqlite_info()
            if ha_cluster_formed(info):
                print(""high-availability: yes"")
            else:
                print(""high-availability: no"")

            masters = ""none""
            standby = ""none""
            for node in info:
                if node[1] == ""voter"":
                    if masters == ""none"":
                        masters = ""{}"".format(node[0])
                    else:
                        masters = ""{} {}"".format(masters, node[0])
                if node[1] == ""standby"":
                    if standby == ""none"":
                        standby = ""{}"".format(node[0])
                    else:
                        standby = ""{} {}"".format(standby, node[0])

            print(""{:>2}{} {}"".format("""", ""datastore master nodes:"", masters))
            print(""{:>2}{} {}"".format("""", ""datastore standby nodes:"", standby))

        print(""addons:"")
        if enabled_addons and len(enabled_addons) > 0:
            print(""{:>2}{}"".format("""", ""enabled:""))
            for enabled in enabled_addons:
                print(
                    console_formatter.format(
                        """", enabled[""name""], enabled[""repository""], enabled[""description""]
                    )
                )
        if disabled_addons and len(disabled_addons) > 0:
            print(""{:>2}{}"".format("""", ""disabled:""))
            for disabled in disabled_addons:
                print(
                    console_formatter.format(
                        """", disabled[""name""], disabled[""repository""], disabled[""description""]
                    )
                )
    else:
        print(""microk8s is not running. Use microk8s inspect for a deeper inspection."")","for enabled in enabled_addons:
    print(console_formatter.format('', enabled['name'], enabled['repository'], enabled['description']))",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_description, e_name, e_repository = e['description'], e['name'], e['repository']
variable mapping:
e_description: e['description']
e_name: e['name']
e_repository: e['repository']",,,,,,,,it actually cannot refactor
microk8s,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/microk8s/scripts/wrappers/status.py,https://github.com/ubuntu/microk8s/tree/master/scripts/wrappers/status.py,,print_pretty$33,"def print_pretty(isReady, enabled_addons, disabled_addons):
    console_formatter = ""{:>3} {:<20} # ({}) {}""
    if isReady:
        print(""microk8s is running"")
        if not is_ha_enabled():
            print(""high-availability: no"")
        else:
            info = get_dqlite_info()
            if ha_cluster_formed(info):
                print(""high-availability: yes"")
            else:
                print(""high-availability: no"")

            masters = ""none""
            standby = ""none""
            for node in info:
                if node[1] == ""voter"":
                    if masters == ""none"":
                        masters = ""{}"".format(node[0])
                    else:
                        masters = ""{} {}"".format(masters, node[0])
                if node[1] == ""standby"":
                    if standby == ""none"":
                        standby = ""{}"".format(node[0])
                    else:
                        standby = ""{} {}"".format(standby, node[0])

            print(""{:>2}{} {}"".format("""", ""datastore master nodes:"", masters))
            print(""{:>2}{} {}"".format("""", ""datastore standby nodes:"", standby))

        print(""addons:"")
        if enabled_addons and len(enabled_addons) > 0:
            print(""{:>2}{}"".format("""", ""enabled:""))
            for enabled in enabled_addons:
                print(
                    console_formatter.format(
                        """", enabled[""name""], enabled[""repository""], enabled[""description""]
                    )
                )
        if disabled_addons and len(disabled_addons) > 0:
            print(""{:>2}{}"".format("""", ""disabled:""))
            for disabled in disabled_addons:
                print(
                    console_formatter.format(
                        """", disabled[""name""], disabled[""repository""], disabled[""description""]
                    )
                )
    else:
        print(""microk8s is not running. Use microk8s inspect for a deeper inspection."")","for disabled in disabled_addons:
    print(console_formatter.format('', disabled['name'], disabled['repository'], disabled['description']))",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_description, e_name, e_repository = e['description'], e['name'], e['repository']
variable mapping:
e_description: e['description']
e_name: e['name']
e_repository: e['repository']",,,,,,,,it actually cannot refactor
spaCy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spaCy/spacy/tests/pipeline/test_tok2vec.py,https://github.com/explosion/spaCy/tree/master/spacy/tests/pipeline/test_tok2vec.py,,test_tok2vec_listener$143,"def test_tok2vec_listener():
    orig_config = Config().from_str(cfg_string)
    nlp = util.load_model_from_config(orig_config, auto_fill=True, validate=True)
    assert nlp.pipe_names == [""tok2vec"", ""tagger""]
    tagger = nlp.get_pipe(""tagger"")
    tok2vec = nlp.get_pipe(""tok2vec"")
    tagger_tok2vec = tagger.model.get_ref(""tok2vec"")
    assert isinstance(tok2vec, Tok2Vec)
    assert isinstance(tagger_tok2vec, Tok2VecListener)
    train_examples = []
    for t in TRAIN_DATA:
        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
        for tag in t[1][""tags""]:
            tagger.add_label(tag)

    # Check that the Tok2Vec component finds it listeners
    assert tok2vec.listeners == []
    optimizer = nlp.initialize(lambda: train_examples)
    assert tok2vec.listeners == [tagger_tok2vec]

    for i in range(5):
        losses = {}
        nlp.update(train_examples, sgd=optimizer, losses=losses)

    doc = nlp(""Running the pipeline as a whole."")
    doc_tensor = tagger_tok2vec.predict([doc])[0]
    ops = get_current_ops()
    assert_array_equal(ops.to_numpy(doc.tensor), ops.to_numpy(doc_tensor))

    # TODO: should this warn or error?
    nlp.select_pipes(disable=""tok2vec"")
    assert nlp.pipe_names == [""tagger""]
    nlp(""Running the pipeline with the Tok2Vec component disabled."")","for t in TRAIN_DATA:
    train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
    for tag in t[1]['tags']:
        tagger.add_label(tag)","for t in TRAIN_DATA:
    (t_0, t_1, _, _, _, t_1_tags, *t_rtmaining) = t
    train_examples.append(Example.from_dict(nlp.make_doc(t_0), t_1))
    for tag in t_1['tags']:
        tagger.add_label(tag)","(data, data, *data)","for (t_0, t_1, *t_len) in TRAIN_DATA:
    train_examples.append(Example.from_dict(nlp.make_doc(
    t_0), 
    t_1))
    for tag in 
    t_1['tags']:
        tagger.add_label(tag)",0,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, _, _, _, e_1_eags, *e_remaining = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_1_eags: e[1]['eags']",,,,,,,,
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/pedestrian_detection/data_provider_farm/reformat_caltech.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/pedestrian_detection/data_provider_farm/reformat_caltech.py,,generate_data_list$8,"def generate_data_list():
    annotation_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_new_annotations/anno_test_1xnew'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_data/extracted_data'

    list_file_path = './data_folder/data_list_caltech_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fout = open(list_file_path, 'w')

    counter = 0
    for parent, dirnames, filenames in os.walk(annotation_root):
        for filename in filenames:
            if not filename.endswith('.txt'):
                continue

            filename_splits = filename[:-4].split('_')
            set_name = filename_splits[0]
            seq_name = filename_splits[1]
            img_name = filename_splits[2]

            img_path = os.path.join(image_root, set_name, seq_name, 'images', img_name)
            if not os.path.exists(img_path):
                print('The corresponding image does not exist! [%s]' % img_path)
                continue

            line = img_path

            fin_anno = open(os.path.join(parent, filename), 'r')

            bbox_list = []
            for i, anno in enumerate(fin_anno):
                if i == 0:
                    continue
                anno = anno.strip('\n').split(' ')
                if anno[0] != 'person':
                    continue
                x = math.floor(float(anno[1]))
                y = math.floor(float(anno[2]))
                width = math.ceil(float(anno[3]))
                height = math.ceil(float(anno[4]))

                width_vis = math.ceil(float(anno[8]))
                height_vis = math.ceil(float(anno[9]))

                if (width_vis*height_vis)/(width*height) < 0.2:
                    continue

                bbox_list.append((x, y, width, height))
            if len(bbox_list) == 0:
                line += ',0,0'
                fout.write(line + '\n')
            else:
                bbox_line = ''
                for bbox in bbox_list:
                    bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
                line += ',1,' + str(len(bbox_list)) + bbox_line
                fout.write(line + '\n')
            counter += 1
            print(counter)

    fout.close()","for filename in filenames:
    if not filename.endswith('.txt'):
        continue
    filename_splits = filename[:-4].split('_')
    set_name = filename_splits[0]
    seq_name = filename_splits[1]
    img_name = filename_splits[2]
    img_path = os.path.join(image_root, set_name, seq_name, 'images', img_name)
    if not os.path.exists(img_path):
        print('The corresponding image does not exist! [%s]' % img_path)
        continue
    line = img_path
    fin_anno = open(os.path.join(parent, filename), 'r')
    bbox_list = []
    for (i, anno) in enumerate(fin_anno):
        if i == 0:
            continue
        anno = anno.strip('\n').split(' ')
        if anno[0] != 'person':
            continue
        x = math.floor(float(anno[1]))
        y = math.floor(float(anno[2]))
        width = math.ceil(float(anno[3]))
        height = math.ceil(float(anno[4]))
        width_vis = math.ceil(float(anno[8]))
        height_vis = math.ceil(float(anno[9]))
        if width_vis * height_vis / (width * height) < 0.2:
            continue
        bbox_list.append((x, y, width, height))
    if len(bbox_list) == 0:
        line += ',0,0'
        fout.write(line + '\n')
    else:
        bbox_line = ''
        for bbox in bbox_list:
            bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
        line += ',1,' + str(len(bbox_list)) + bbox_line
        fout.write(line + '\n')
    counter += 1
    print(counter)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_unpacked, *_ = e[:-4]
variable mapping:
e_unpacked: e[:-4]
Explanation: The unpacked elements e[:-4] represent a slice of the iterable object ""e"" that can be accessed using the slice operator. Therefore, iterable unpacking is applicable in this case and we can use the slice operator to get the unpacked elements.",,,,,,,,it actually cannot refactor
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/pedestrian_detection/data_provider_farm/reformat_caltech.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/pedestrian_detection/data_provider_farm/reformat_caltech.py,,generate_data_list$8,"def generate_data_list():
    annotation_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_new_annotations/anno_test_1xnew'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_data/extracted_data'

    list_file_path = './data_folder/data_list_caltech_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fout = open(list_file_path, 'w')

    counter = 0
    for parent, dirnames, filenames in os.walk(annotation_root):
        for filename in filenames:
            if not filename.endswith('.txt'):
                continue

            filename_splits = filename[:-4].split('_')
            set_name = filename_splits[0]
            seq_name = filename_splits[1]
            img_name = filename_splits[2]

            img_path = os.path.join(image_root, set_name, seq_name, 'images', img_name)
            if not os.path.exists(img_path):
                print('The corresponding image does not exist! [%s]' % img_path)
                continue

            line = img_path

            fin_anno = open(os.path.join(parent, filename), 'r')

            bbox_list = []
            for i, anno in enumerate(fin_anno):
                if i == 0:
                    continue
                anno = anno.strip('\n').split(' ')
                if anno[0] != 'person':
                    continue
                x = math.floor(float(anno[1]))
                y = math.floor(float(anno[2]))
                width = math.ceil(float(anno[3]))
                height = math.ceil(float(anno[4]))

                width_vis = math.ceil(float(anno[8]))
                height_vis = math.ceil(float(anno[9]))

                if (width_vis*height_vis)/(width*height) < 0.2:
                    continue

                bbox_list.append((x, y, width, height))
            if len(bbox_list) == 0:
                line += ',0,0'
                fout.write(line + '\n')
            else:
                bbox_line = ''
                for bbox in bbox_list:
                    bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
                line += ',1,' + str(len(bbox_list)) + bbox_line
                fout.write(line + '\n')
            counter += 1
            print(counter)

    fout.close()","for (i, anno) in enumerate(fin_anno):
    if i == 0:
        continue
    anno = anno.strip('\n').split(' ')
    if anno[0] != 'person':
        continue
    x = math.floor(float(anno[1]))
    y = math.floor(float(anno[2]))
    width = math.ceil(float(anno[3]))
    height = math.ceil(float(anno[4]))
    width_vis = math.ceil(float(anno[8]))
    height_vis = math.ceil(float(anno[9]))
    if width_vis * height_vis / (width * height) < 0.2:
        continue
    bbox_list.append((x, y, width, height))","for (i, anno) in enumerate(fin_anno):
    (anno_0, anno_1, anno_2, anno_3, anno_4, *_, anno_8, anno_9) = anno
    if i == 0:
        continue
    anno = anno.strip('\n').split(' ')
    if anno_0 != 'person':
        continue
    x = math.floor(float(anno_1))
    y = math.floor(float(anno_2))
    width = math.ceil(float(anno_3))
    height = math.ceil(float(anno_4))
    width_vis = math.ceil(float(anno_8))
    height_vis = math.ceil(float(anno_9))
    if width_vis * height_vis / (width * height) < 0.2:
        continue
    bbox_list.append((x, y, width, height))",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4, *_, e_8, e_9 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]
e_8: e[8]
e_9: e[9]",,,,,,,,it actually cannot refactor
kivy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy/kivy/atlas.py,https://github.com/kivy/kivy/tree/master/kivy/atlas.py,Atlas,create$229,"def create(outname, filenames, size, padding=2, use_path=False):
        '''This method can be used to create an atlas manually from a set of
        images.

        :Parameters:
            `outname`: str
                Basename to use for ``.atlas`` creation and ``-<idx>.png``
                associated images.
            `filenames`: list
                List of filenames to put in the atlas.
            `size`: int or list (width, height)
                Size of the atlas image. If the size is not large enough to
                fit all of the source images, more atlas images will created
                as required.
            `padding`: int, defaults to 2
                Padding to put around each image.

                Be careful. If you're using a padding < 2, you might have
                issues with the borders of the images. Because of the OpenGL
                linearization, it might use the pixels of the adjacent image.

                If you're using a padding >= 2, we'll automatically generate a
                ""border"" of 1px around your image. If you look at
                the result, don't be scared if the image inside is not
                exactly the same as yours :).

            `use_path`: bool, defaults to False
                If True, the relative path of the source png
                file names will be included in the atlas ids rather
                that just in the file names. Leading dots and slashes will be
                excluded and all other slashes in the path will be replaced
                with underscores. For example, if `use_path` is False
                (the default) and the file name is
                ``../data/tiles/green_grass.png``, the id will be
                ``green_grass``. If `use_path` is True, it will be
                ``data_tiles_green_grass``.

            .. versionchanged:: 1.8.0
                Parameter use_path added
        '''
        # Thanks to
        # omnisaurusgames.com/2011/06/texture-atlas-generation-using-python/
        # for its initial implementation.
        try:
            from PIL import Image
        except ImportError:
            Logger.critical('Atlas: Imaging/PIL are missing')
            raise

        if isinstance(size, (tuple, list)):
            size_w, size_h = list(map(int, size))
        else:
            size_w = size_h = int(size)

        # open all of the images
        ims = list()
        for f in filenames:
            fp = open(f, 'rb')
            im = Image.open(fp)
            im.load()
            fp.close()
            ims.append((f, im))

        # sort by image area
        ims = sorted(ims, key=lambda im: im[1].size[0] * im[1].size[1],
                     reverse=True)

        # free boxes are empty space in our output image set
        # the freebox tuple format is: outidx, x, y, w, h
        freeboxes = [(0, 0, 0, size_w, size_h)]
        numoutimages = 1

        # full boxes are areas where we have placed images in the atlas
        # the full box tuple format is: image, outidx, x, y, w, h, filename
        fullboxes = []

        # do the actual atlasing by sticking the largest images we can
        # have into the smallest valid free boxes
        for imageinfo in ims:
            im = imageinfo[1]
            imw, imh = im.size
            imw += padding
            imh += padding
            if imw > size_w or imh > size_h:
                Logger.error(
                    'Atlas: image %s (%d by %d) is larger than the atlas size!'
                    % (imageinfo[0], imw, imh))
                return

            inserted = False
            while not inserted:
                for idx, fb in enumerate(freeboxes):
                    # find the smallest free box that will contain this image
                    if fb[3] >= imw and fb[4] >= imh:
                        # we found a valid spot! Remove the current
                        # freebox, and split the leftover space into (up to)
                        # two new freeboxes
                        del freeboxes[idx]
                        if fb[3] > imw:
                            freeboxes.append((
                                fb[0], fb[1] + imw, fb[2],
                                fb[3] - imw, imh))

                        if fb[4] > imh:
                            freeboxes.append((
                                fb[0], fb[1], fb[2] + imh,
                                fb[3], fb[4] - imh))

                        # keep this sorted!
                        freeboxes = sorted(freeboxes,
                                           key=lambda fb: fb[3] * fb[4])
                        fullboxes.append((im,
                                          fb[0], fb[1] + padding,
                                          fb[2] + padding, imw - padding,
                                          imh - padding, imageinfo[0]))
                        inserted = True
                        break

                if not inserted:
                    # oh crap - there isn't room in any of our free
                    # boxes, so we have to add a new output image
                    freeboxes.append((numoutimages, 0, 0, size_w, size_h))
                    numoutimages += 1

        # now that we've figured out where everything goes, make the output
        # images and blit the source images to the appropriate locations
        Logger.info('Atlas: create an {0}x{1} rgba image'.format(size_w,
                                                                 size_h))
        outimages = [Image.new('RGBA', (size_w, size_h))
                     for i in range(0, int(numoutimages))]
        for fb in fullboxes:
            x, y = fb[2], fb[3]
            out = outimages[fb[1]]
            out.paste(fb[0], (fb[2], fb[3]))
            w, h = fb[0].size
            if padding > 1:
                out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
                out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
                out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
                out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))

        # save the output images
        for idx, outimage in enumerate(outimages):
            outimage.save('%s-%d.png' % (outname, idx))

        # write out an json file that says where everything ended up
        meta = {}
        for fb in fullboxes:
            fn = '%s-%d.png' % (basename(outname), fb[1])
            if fn not in meta:
                d = meta[fn] = {}
            else:
                d = meta[fn]

            # fb[6] contain the filename
            if use_path:
                # use the path with separators replaced by _
                # example '../data/tiles/green_grass.png' becomes
                # 'data_tiles_green_grass'
                uid = splitext(fb[6])[0]
                # remove leading dots and slashes
                uid = uid.lstrip('./\\')
                # replace remaining slashes with _
                uid = uid.replace('/', '_').replace('\\', '_')
            else:
                # for example, '../data/tiles/green_grass.png'
                # just get only 'green_grass' as the uniq id.
                uid = splitext(basename(fb[6]))[0]

            x, y, w, h = fb[2:6]
            d[uid] = x, size_h - y - h, w, h

        outfn = '%s.atlas' % outname
        with open(outfn, 'w') as fd:
            json.dump(meta, fd)

        return outfn, meta","for fb in fullboxes:
    fn = '%s-%d.png' % (basename(outname), fb[1])
    if fn not in meta:
        d = meta[fn] = {}
    else:
        d = meta[fn]
    if use_path:
        uid = splitext(fb[6])[0]
        uid = uid.lstrip('./\\')
        uid = uid.replace('/', '_').replace('\\', '_')
    else:
        uid = splitext(basename(fb[6]))[0]
    (x, y, w, h) = fb[2:6]
    d[uid] = (x, size_h - y - h, w, h)","for fb in fullboxes:
    (_, fb_1, *fb_2_to_6, *fb_rfbmaining) = fb
    fn = '%s-%d.png' % (basename(outname), fb_1)
    if fn not in meta:
        d = meta[fn] = {}
    else:
        d = meta[fn]
    if use_path:
        uid = splitext(fb[6])[0]
        uid = uid.lstrip('./\\')
        uid = uid.replace('/', '_').replace('\\', '_')
    else:
        uid = splitext(basename(fb[6]))[0]
    (x, y, w, h) = fb[2:6]
    d[uid] = (x, size_h - y - h, w, h)",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_2_to_6, *e_remaining = e
variable mapping:
e_1: e[1]
e_2_to_6: e[2:6]",,,,,,,,it actually cannot refactor
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for edge in edges:
    points = GKPoints()
    if 'shape' in edge:
        for p in edge['shape']:
            new_point = GKPoint()
            new_point.set(p[0], p[1], 0)
            points.append(new_point)
        cmd = model.createNewCmd(model.getType('GKSection'))
        cmd.setPoints(edge['numLanes'], lane_width, points)
        model.getCommander().addCommand(cmd)
        section = cmd.createdObject()
        section.setName(edge['id'])
        edge_aimsun = model.getCatalog().findByName(edge['id'], type_section)
        edge_aimsun.setSpeed(edge['speed'] * 3.6)
    else:
        (first_node, last_node) = get_edge_nodes(edge, nodes)
        theta = get_edge_angle(first_node, last_node)
        first_node_offset = [0, 0]
        last_node_offset = [0, 0]
        if 'radius' in first_node:
            first_node_offset[0] = first_node['radius'] * np.cos(theta * np.pi / 180)
            first_node_offset[1] = first_node['radius'] * np.sin(theta * np.pi / 180)
        if 'radius' in last_node:
            last_node_offset[0] = -last_node['radius'] * np.cos(theta * np.pi / 180)
            last_node_offset[1] = -last_node['radius'] * np.sin(theta * np.pi / 180)
        edges_shared_node = [edg for edg in edges if first_node['id'] == edg['to'] or last_node['id'] == edg['from']]
        for new_edge in edges_shared_node:
            (new_first_node, new_last_node) = get_edge_nodes(new_edge, nodes)
            new_theta = get_edge_angle(new_first_node, new_last_node)
            if new_theta == theta - 180 or new_theta == theta + 180:
                first_node_offset[0] += lane_width * 0.5 * np.sin(theta * np.pi / 180)
                first_node_offset[1] -= lane_width * 0.5 * np.cos(theta * np.pi / 180)
                last_node_offset[0] += lane_width * 0.5 * np.sin(theta * np.pi / 180)
                last_node_offset[1] -= lane_width * 0.5 * np.cos(theta * np.pi / 180)
                break
        new_point = GKPoint()
        new_point.set(first_node['x'] + first_node_offset[0], first_node['y'] + first_node_offset[1], 0)
        points.append(new_point)
        new_point = GKPoint()
        new_point.set(last_node['x'] + last_node_offset[0], last_node['y'] + last_node_offset[1], 0)
        points.append(new_point)
        cmd = model.createNewCmd(type_section)
        cmd.setPoints(edge['numLanes'], lane_width, points)
        model.getCommander().addCommand(cmd)
        section = cmd.createdObject()
        section.setName(edge['id'])
        edge_aimsun = model.getCatalog().findByName(edge['id'], type_section)
        edge_aimsun.setSpeed(edge['speed'] * 3.6)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_id, e_numLanes, e_shape, e_speed = e.values()
variable mapping:
e_id: e['id']
e_numLanes: e['numLanes']
e_shape: e['shape']
e_speed: e['speed']",,,,,,,,it actually cannot refactor
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for node in nodes:
    node_pos = GKPoint()
    node_pos.set(node['x'], node['y'], 0)
    cmd = model.createNewCmd(type_node)
    cmd.setPosition(node_pos)
    model.getCommander().addCommand(cmd)
    new_node = cmd.createdObject()
    new_node.setName(node['id'])
    from_edges = [edge['id'] for edge in edges if edge['from'] == node['id']]
    to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]
    if len(to_edges) > 1 and len(from_edges) > 1 and (connections[node['id']] is not None):
        for connection in connections[node['id']]:
            cmd = model.createNewCmd(type_turn)
            from_section = model.getCatalog().findByName(connection['from'], type_section, True)
            to_section = model.getCatalog().findByName(connection['to'], type_section, True)
            cmd.setTurning(from_section, to_section)
            model.getCommander().addCommand(cmd)
            turn = cmd.createdObject()
            turn_name = '{}_to_{}'.format(connection['from'], connection['to'])
            turn.setName(turn_name)
            existing_node = turn.getNode()
            if existing_node is not None:
                existing_node.removeTurning(turn)
            new_node.addTurning(turn, False, True)
    else:
        for i in range(len(from_edges)):
            for j in range(len(to_edges)):
                cmd = model.createNewCmd(type_turn)
                to_section = model.getCatalog().findByName(from_edges[i], type_section, True)
                from_section = model.getCatalog().findByName(to_edges[j], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = '{}_to_{}'.format(from_edges[i], to_edges[j])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                new_node.addTurning(turn, False, True)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked elements, e['id'], e['x'], and e['y'] are dictionary values that can be accessed directly using the keys 'id', 'x', and 'y'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for node in junctions:
    phases = tls_properties[node['id']]['phases']
    print(phases)
    create_node_meters(model, control_plan, node['id'], phases)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['id'] is a dictionary value that can be accessed directly using the key 'id'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for veh_type in veh_types:
    new_state = create_state(model, veh_type['veh_id'])
    veh_type = model.getCatalog().findByName(veh_type['veh_id'], model.getType('GKVehicle'))
    new_state.setVehicle(veh_type)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['veh_id'] is a dictionary value that can be accessed directly using the key 'veh_id'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for inflow in inflows:
    traffic_state_aimsun = model.getCatalog().findByName(inflow['vtype'], type_traffic_state)
    edge_aimsun = model.getCatalog().findByName(inflow['edge'], type_section)
    traffic_state_aimsun.setEntranceFlow(edge_aimsun, None, inflow['vehsPerHour'])",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2 = e['edge'], e['vehsPerHour'], e['vtype']
variable mapping:
e_0: e['edge']
e_1: e['vehsPerHour']
e_2: e['vtype']",,,,,,,,it actually cannot refactor
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for veh_type in veh_types:
    state_car = model.getCatalog().findByName(veh_type['veh_id'], type_traffic_state)
    if demand is not None and demand.isA('GKTrafficDemand'):
        if state_car is not None and state_car.isA('GKTrafficState'):
            set_demand_item(model, demand, state_car)
        model.getCommander().addCommand(None)
    else:
        create_traffic_demand(model, veh_type['veh_id'])",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['veh_id'] is a dictionary value that can be accessed directly using the key 'veh_id'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for connection in connections[node['id']]:
    cmd = model.createNewCmd(type_turn)
    from_section = model.getCatalog().findByName(connection['from'], type_section, True)
    to_section = model.getCatalog().findByName(connection['to'], type_section, True)
    cmd.setTurning(from_section, to_section)
    model.getCommander().addCommand(cmd)
    turn = cmd.createdObject()
    turn_name = '{}_to_{}'.format(connection['from'], connection['to'])
    turn.setName(turn_name)
    existing_node = turn.getNode()
    if existing_node is not None:
        existing_node.removeTurning(turn)
    new_node.addTurning(turn, False, True)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_from, e_to = e['from'], e['to']
variable mapping:
e_from: e['from']
e_to: e['to']",,,,,,,,it actually cannot refactor
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes,
                 edges,
                 connections,
                 inflows,
                 veh_types,
                 traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6  # TODO additional params??
    type_section = model.getType(""GKSection"")
    type_node = model.getType(""GKNode"")
    type_turn = model.getType(""GKTurning"")
    type_traffic_state = model.getType(""GKTrafficState"")
    type_vehicle = model.getType(""GKVehicle"")
    type_demand = model.getType(""GKTrafficDemand"")

    # draw edges
    for edge in edges:
        points = GKPoints()
        if ""shape"" in edge:
            for p in edge[""shape""]:  # TODO add x, y offset (radius)
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)

            cmd = model.createNewCmd(model.getType(""GKSection""))
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)
        else:
            first_node, last_node = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]  # x, and y offset
            last_node_offset = [0, 0]  # x, and y offset

            # offset edge ends if there is a radius in the node
            if ""radius"" in first_node:
                first_node_offset[0] = first_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                first_node_offset[1] = first_node[""radius""] * \
                    np.sin(theta*np.pi/180)
            if ""radius"" in last_node:
                last_node_offset[0] = - last_node[""radius""] * \
                    np.cos(theta*np.pi/180)
                last_node_offset[1] = - last_node[""radius""] * \
                    np.sin(theta*np.pi/180)

            # offset edge ends if there are multiple edges between nodes
            # find the edges that share the first node
            edges_shared_node = [edg for edg in edges
                                 if first_node[""id""] == edg[""to""] or
                                 last_node[""id""] == edg[""from""]]
            for new_edge in edges_shared_node:
                new_first_node, new_last_node = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * \
                        np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 *\
                        np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 *\
                        np.cos(theta * np.pi / 180)
                    break

            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0],
                          first_node['y'] + first_node_offset[1],
                          0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0],
                          last_node['y'] + last_node_offset[1],
                          0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge[""numLanes""], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge[""id""])
            edge_aimsun = model.getCatalog().findByName(
                edge[""id""], type_section)
            edge_aimsun.setSpeed(edge[""speed""] * 3.6)

    # draw nodes and connections
    for node in nodes:
        # add a new node in Aimsun
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node[""id""])

        # list of edges from and to the node
        from_edges = [
            edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]

        # if the node is a junction with a list of connections
        if len(to_edges) > 1 and len(from_edges) > 1 \
                and connections[node['id']] is not None:
            # add connections
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(
                    connection[""from""], type_section, True)
                to_section = model.getCatalog().findByName(
                    connection[""to""], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = ""{}_to_{}"".format(connection[""from""],
                                              connection[""to""])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                # add the turning to the node
                new_node.addTurning(turn, False, True)

        # if the node is not a junction or connections is None
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(
                        from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(
                        to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = ""{}_to_{}"".format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)

                    # add the turning to the node
                    new_node.addTurning(turn, False, True)

    # get the control plan
    control_plan = model.getCatalog().findByName(
            ""Control Plan"", model.getType(""GKControlPlan""))

    # add traffic lights
    tls_properties = traffic_lights.get_properties()
    # determine junctions
    junctions = get_junctions(nodes)
    # add meters for all nodes in junctions
    for node in junctions:
        phases = tls_properties[node['id']][""phases""]
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)

    # set vehicle types
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == ""Car"":
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type[""veh_id""])

    # Create new states based on vehicle types
    for veh_type in veh_types:
        new_state = create_state(model, veh_type[""veh_id""])
        # find vehicle type
        veh_type = model.getCatalog().findByName(
            veh_type[""veh_id""], model.getType(""GKVehicle""))
        # set state vehicles
        new_state.setVehicle(veh_type)

    # add traffic inflows to traffic states
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(
            inflow[""vtype""], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(
            inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(
            edge_aimsun, None, inflow['vehsPerHour'])

    # get traffic demand
    demand = model.getCatalog().findByName(
        ""Traffic Demand 864"", type_demand)
    # clear the demand of any previous item
    demand.removeSchedule()

    # set traffic demand
    for veh_type in veh_types:
        # find the state for each vehicle type
        state_car = model.getCatalog().findByName(
            veh_type[""veh_id""], type_traffic_state)
        if demand is not None and demand.isA(""GKTrafficDemand""):
            # Add the state
            if state_car is not None and state_car.isA(""GKTrafficState""):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type[""veh_id""])  # TODO debug

    # set the view to ""whole world"" in Aimsun
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()

    # set view mode, each vehicle type with different color
    set_vehicles_color(model)

    # set API
    network_name = data[""network_name""]
    scenario = model.getCatalog().findByName(
        network_name, model.getType(""GKScenario""))  # find scenario
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(
        config.PROJECT_PATH, ""flow/utils/aimsun/run.py""), True)

    # save
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for veh_type in veh_types:
    cmd = GKObjectDuplicateCmd()
    cmd.init(vehicle)
    model.getCommander().addCommand(cmd)
    new_veh = cmd.createdObject()
    new_veh.setName(veh_type['veh_id'])",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['veh_id'] is a dictionary value that can be accessed directly using the key 'veh_id'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
PaddleVideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleVideo/paddlevideo/utils/multigrid/multigrid.py,https://github.com/PaddlePaddle/PaddleVideo/tree/master/paddlevideo/utils/multigrid/multigrid.py,MultigridSchedule,get_long_cycle_schedule$110,"def get_long_cycle_schedule(self, cfg):
        """"""
        Based on multigrid hyperparameters, define the schedule of a long cycle.
        Args:
            cfg (configs): configs that contains training and multigrid specific
                hyperparameters.
        Returns:
            schedule (list): Specifies a list long cycle base shapes and their
                corresponding training epochs.
        """"""

        steps = cfg.OPTIMIZER.learning_rate.steps

        default_size = float(
            cfg.PIPELINE.train.decode_sampler.num_frames *
            cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size']**
            2)  # 32 * 224 * 224  C*H*W
        default_iters = steps[-1]  # 196

        # Get shapes and average batch size for each long cycle shape.
        avg_bs = []
        all_shapes = []
        #        for t_factor, s_factor in cfg.MULTIGRID.long_cycle_factors:
        for item in cfg.MULTIGRID.long_cycle_factors:
            t_factor, s_factor = item[""value""]
            base_t = int(
                round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
            base_s = int(
                round(
                    cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size']
                    * s_factor))
            if cfg.MULTIGRID.SHORT_CYCLE:
                shapes = [
                    [
                        base_t,
                        cfg.MULTIGRID.default_crop_size *
                        cfg.MULTIGRID.short_cycle_factors[0],
                    ],
                    [
                        base_t,
                        cfg.MULTIGRID.default_crop_size *
                        cfg.MULTIGRID.short_cycle_factors[1],
                    ],
                    [base_t, base_s],
                ]  #first two is short_cycle, last is the base long_cycle
            else:
                shapes = [[base_t, base_s]]

            # (T, S) -> (B, T, S)
            shapes = [[
                int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]
            ] for s in shapes]
            avg_bs.append(np.mean([s[0] for s in shapes]))
            all_shapes.append(shapes)

        # Get schedule regardless of cfg.MULTIGRID.epoch_factor.
        total_iters = 0
        schedule = []
        for step_index in range(len(steps) - 1):
            step_epochs = steps[step_index + 1] - steps[step_index]

            for long_cycle_index, shapes in enumerate(all_shapes):
                #ensure each of 4 sequences run the same num of iters
                cur_epochs = (step_epochs * avg_bs[long_cycle_index] /
                              sum(avg_bs))

                # get cur_iters from cur_epochs
                cur_iters = cur_epochs / avg_bs[long_cycle_index]
                total_iters += cur_iters
                schedule.append((step_index, shapes[-1], cur_epochs))

        iter_saving = default_iters / total_iters  # ratio between default iters and real iters

        final_step_epochs = cfg.OPTIMIZER.learning_rate.max_epoch - steps[-1]

        # We define the fine-tuning phase to have the same amount of iteration
        # saving as the rest of the training.
        #final_step_epochs / iter_saving make fine-tune having the same iters as training
        ft_epochs = final_step_epochs / iter_saving * avg_bs[-1]

        #        schedule.append((step_index + 1, all_shapes[-1][2], ft_epochs))
        schedule.append((step_index + 1, all_shapes[-1][-1], ft_epochs))

        # Obtrain final schedule given desired cfg.MULTIGRID.epoch_factor.
        x = (cfg.OPTIMIZER.learning_rate.max_epoch *
             cfg.MULTIGRID.epoch_factor / sum(s[-1] for s in schedule))

        final_schedule = []
        total_epochs = 0
        for s in schedule:
            epochs = s[2] * x
            total_epochs += epochs
            final_schedule.append((s[0], s[1], int(round(total_epochs))))
        print_schedule(final_schedule)
        return final_schedule","for item in cfg.MULTIGRID.long_cycle_factors:
    (t_factor, s_factor) = item['value']
    base_t = int(round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
    base_s = int(round(cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size'] * s_factor))
    if cfg.MULTIGRID.SHORT_CYCLE:
        shapes = [[base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[0]], [base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[1]], [base_t, base_s]]
    else:
        shapes = [[base_t, base_s]]
    shapes = [[int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]] for s in shapes]
    avg_bs.append(np.mean([s[0] for s in shapes]))
    all_shapes.append(shapes)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['value'] is a dictionary value that can be accessed directly using the key 'value'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
	dbg.createLogWindow()
	global currentArgs
	currentArgs = copy.copy(args)
	try:
		starttime = datetime.datetime.now()
		ptr_counter = 0
		
		# initialize list of commands
		commands = {}
		
		# ----- HELP ----- #
		def getBanner():
			banners = {}
			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                         __               __                      |\n""
			bannertext += ""    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n""
			bannertext += ""    |  / ___/ __ \/ ___/ _ \/ / __ `/ __ \   / __/ _ \/ __ `/ __ `__ \ |\n""
			bannertext += ""    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n""
			bannertext += ""    | \___/\____/_/   \___/_/\__,_/_/ /_/   \__/\___/\__,_/_/ /_/ /_/  |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |     https://www.corelan.be | https://www.corelan-training.com    |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[0] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""			
			bannertext += ""    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n""
			bannertext += ""    |       | '_ ` _ \  / _ \ | '_ \  / _` |   | '_ \ | | | |          |\n""
			bannertext += ""    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n""
			bannertext += ""    |       |_| |_| |_| \___/ |_| |_| \__,_|(_)| .__/  \__, |          |\n""
			bannertext += ""    |                                          |_|     |___/           |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""	
			banners[1] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |    _____ ___  ____  ____  ____ _                                 |\n""
			bannertext += ""    |    / __ `__ \/ __ \/ __ \/ __ `/  https://www.corelan.be         |\n""
			bannertext += ""    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n""
			bannertext += ""    |  /_/ /_/ /_/\____/_/ /_/\__,_/  #corelan (Freenode IRC)          |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[2] = bannertext

			bannertext = """"
			bannertext += ""\n    .##.....##..#######..##....##....###........########..##....##\n""
			bannertext += ""    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n""
			bannertext += ""    .####.####.##.....##.####..##..##...##......##.....##...####..\n""
			bannertext += ""    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n""
			bannertext += ""    .##.....##.##.....##.##..####.#########.....##...........##...\n""
			bannertext += ""    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n""
			bannertext += ""    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n""
			banners[3] = bannertext


			# pick random banner
			bannerlist = []
			for i in range (0, len(banners)):
				bannerlist.append(i)

			random.shuffle(bannerlist)
			return banners[bannerlist[0]]

		
		def procHelp(args):
			dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__,str(arch)))
			dbg.log(""     Plugin version : %s r%s"" % (__VERSION__,__REV__))
			dbg.log(""     Python version : %s"" % (getPythonVersion()))
			if __DEBUGGERAPP__ == ""WinDBG"":
				pykdversion = dbg.getPyKDVersionNr()
				dbg.log(""     PyKD version %s"" % pykdversion)
			dbg.log(""     Written by Corelan - https://www.corelan.be"")
			dbg.log(""     Project page : https://github.com/corelan/mona"")
			dbg.logLines(getBanner(),highlight=1)
			dbg.log(""Global options :"")
			dbg.log(""----------------"")
			dbg.log(""You can use one or more of the following global options on any command that will perform"")
			dbg.log(""a search in one or more modules, returning a list of pointers :"")
			dbg.log("" -n                     : Skip modules that start with a null byte. If this is too broad, use"")
			dbg.log(""                          option -cp nonull instead"")
			dbg.log("" -o                     : Ignore OS modules"")
			dbg.log("" -p <nr>                : Stop search after <nr> pointers."")
			dbg.log("" -m <module,module,...> : only query the given modules. Be sure what you are doing !"")
			dbg.log(""                          You can specify multiple modules (comma separated)"")
			dbg.log(""                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored"")
			dbg.log(""                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,"")
			dbg.log(""                          blah or *blah* = contains blah"")
			dbg.log("" -cm <crit,crit,...>    : Apply some additional criteria to the modules to query."")
			dbg.log(""                          You can use one or more of the following criteria :"")
			dbg.log(""                          aslr,safeseh,rebase,nx,os"")
			dbg.log(""                          You can enable or disable a certain criterium by setting it to true or false"")
			dbg.log(""                          Example :  -cm aslr=true,safeseh=false"")
			dbg.log(""                          Suppose you want to search for p/p/r in aslr enabled modules, you could call"")
			dbg.log(""                          !mona seh -cm aslr"")
			dbg.log("" -cp <crit,crit,...>    : Apply some criteria to the pointers to return"")
			dbg.log(""                          Available options are :"")
			dbg.log(""                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev"")
			dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
			dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
			dbg.log(""                          You can use .. to indicate a range of bytes (in between 2 bad chars)"")
			dbg.log("" -x <access>            : Specify desired access level of the returning pointers. If not specified,"")
			dbg.log(""                          only executable pointers will be returned."")
			dbg.log(""                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *"")
			
			if not args:
				args = []
			if len(args) > 1:
				thiscmd = args[1].lower().strip()
				if thiscmd in commands:
					dbg.log("""")
					dbg.log(""Usage of command '%s' :"" % thiscmd)
					dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
					dbg.logLines(commands[thiscmd].usage)
					dbg.log("""")
				else:
					aliasfound = False
					for cmd in commands:
						if commands[cmd].alias == thiscmd:
							dbg.log("""")
							dbg.log(""Usage of command '%s' :"" % thiscmd)
							dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
							dbg.logLines(commands[cmd].usage)
							dbg.log("""")
							aliasfound = True
					if not aliasfound:
						dbg.logLines(""\nCommand %s does not exist. Run !mona to get a list of available commands\n"" % thiscmd,highlight=1)
			else:
				dbg.logLines(""\nUsage :"")
				dbg.logLines(""-------\n"")
				dbg.log("" !mona <command> <parameter>"")
				dbg.logLines(""\nAvailable commands and parameters :\n"")

				items = commands.items()
				items.sort(key = itemgetter(0))
				for item in items:
					if commands[item[0]].usage != """":
						aliastxt = """"
						if commands[item[0]].alias != """":
							aliastxt = "" / "" + commands[item[0]].alias
						dbg.logLines(""%s | %s"" % (item[0] + aliastxt + ("" "" * (20 - len(item[0]+aliastxt))), commands[item[0]].description))
				dbg.log("""")
				dbg.log(""Want more info about a given command ?  Run !mona help <command>"",highlight=1)
				dbg.log("""")
		
		commands[""help""] = MnCommand(""help"", ""show help"", ""!mona help [command]"",procHelp)
		
		# ----- Config file management ----- #
		
		def procConfig(args):
			#did we specify -get, -set or -add?
			showerror = False
			if not ""set"" in args and not ""get"" in args and not ""add"" in args:
				showerror = True
				
			if ""set"" in args:
				if type(args[""set""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""set""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""add"" in args:
				if type(args[""add""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""add""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""get"" in args:
				if type(args[""get""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""get""].split("" "")
					if len(params) < 1:
						showerror = True
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(configUsage,highlight=1)
				return
			else:
				if ""get"" in args:
					dbg.log(""Reading value from configuration file"")
					monaConfig = MnConfig()
					thevalue = monaConfig.get(args[""get""])
					dbg.log(""Parameter %s = %s"" % (args[""get""],thevalue))
				
				if ""set"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""set""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = args[""set""][0+len(configparam):len(args[""set""])]
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
				if ""add"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""add""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = monaConfig.get(configparam).strip() + "","" + args[""add""][0+len(configparam):len(args[""add""])].strip()
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
		# ----- Jump to register ----- #
	
		def procFindJ(args):
			return procFindJMP(args)
		
		def procFindJMP(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			
			if (inspect.stack()[1][3] == ""procFindJ""):
				dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."",highlight=1)

			criteria={}
			all_opcodes={}
			
			global ptr_to_get
			ptr_to_get = -1
			
			distancestr = """"
			mindistance = 0
			maxdistance = 0
			
			#did user specify -r <reg> ?
			showerror = False
			if ""r"" in args:
				if type(args[""r""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#valid register ?
					thisreg = args[""r""].upper().strip()
					validregs = dbglib.Registers32BitsOrder
					if not thisreg in validregs:
						showerror = True
			else:
				showerror = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0		
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0						
			
			if maxdistance < mindistance:
				tmp = maxdistance
				maxdistance = mindistance
				mindistance = tmp
			
			criteria[""mindistance""] = mindistance
			criteria[""maxdistance""] = maxdistance
			
			
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(jmpUsage,highlight=1)
				return				
			else:
				modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
				# go for it !	
				all_opcodes=findJMP(modulecriteria,criteria,args[""r""].lower().strip())
			
			# write to log
			logfile = MnLog(""jmp.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog)
		
		# ----- Exception Handler Overwrites ----- #
		
					
		def procFindSEH(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""safeseh""] = False
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False

			criteria = {}
			specialcases = {}
			all_opcodes = {}
			
			global ptr_to_get
			ptr_to_get = -1
			
			#what is the caller function (backwards compatibility with pvefindaddr)
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if ""rop"" in args:
				criteria[""rop""] = True
			
			if ""all"" in args:
				criteria[""all""] = True
				specialcases[""maponly""] = True
			else:
				criteria[""all""] = False
				specialcases[""maponly""] = False
			
			# go for it !	
			all_opcodes = findSEH(modulecriteria,criteria)
			#report findings to log
			logfile = MnLog(""seh.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog,specialcases)
			
			
		# ----- MODULES ------ #
		def procShowMODULES(args):
			modulecriteria={}
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			modulestosearch = getModulesToQuery(modulecriteria)
			showModuleTable("""",modulestosearch)

		# ----- ROP ----- #
		def procFindROPFUNC(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			#modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			ropfuncs = {}
			ropfuncoffsets ={}
			ropfuncs,ropfuncoffsets = findROPFUNC(modulecriteria,criteria)
			#report findings to log
			dbg.log(""[+] Processing pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc.txt"")
			thislog = logfile.reset()
			processResults(ropfuncs,logfile,thislog)
			global silent
			silent = True
			dbg.log(""[+] Processing offsets to pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc_offset.txt"")
			thislog = logfile.reset()
			processResults(ropfuncoffsets,logfile,thislog)			
			
		def procStackPivots(args):
			procROP(args,""stackpivot"")
			
		def procROP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			maxoffset = 40
			thedistance = 8
			split = False
			fast = False
			sortedprint = False
			endingstr = """"
			endings = []
			technique = """"            
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass
			
			if ""offset"" in args:
				if type(args[""offset""]).__name__.lower() != ""bool"":
					try:
						maxoffset = int(args[""offset""])
					except:
						pass
			
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() != ""bool"":
					try:
						thedistance = args[""distance""]
					except:
						pass
			
			if ""split"" in args:
				if type(args[""split""]).__name__.lower() == ""bool"":
					split = args[""split""]

			if ""s"" in args:
				if type(args[""s""]).__name__.lower() != ""bool"":
					technique = args[""s""].replace(""'"","""").replace('""',"""").strip().lower()                   
					
			if ""fast"" in args:
				if type(args[""fast""]).__name__.lower() == ""bool"":
					fast = args[""fast""]
			
			if ""end"" in args:
				if type(args[""end""]).__name__.lower() == ""str"":
					endingstr = args[""end""].replace(""'"","""").replace('""',"""").strip()
					endings = endingstr.split(""#"")
					
			if ""f"" in args:
				if args[""f""] != """":
					criteria[""f""] = args[""f""]
			
			if ""sort"" in args:
				sortedprint = True
			
			if ""rva"" in args:
				criteria[""rva""] = True
			
			if mode == ""stackpivot"":
				fast = False
				endings = """"
				split = False
			else:
				mode = ""all""
			
			findROPGADGETS(modulecriteria,criteria,endings,maxoffset,depth,split,thedistance,fast,mode,sortedprint,technique)
			

		def procJseh(args):
			results = []
			showred=0
			showall=False
			if ""all"" in args:
				showall = True
			nrfound = 0
			dbg.log(""-----------------------------------------------------------------------"")
			dbg.log(""Search for jmp/call dword[ebp/esp+nn] (and other) combinations started "")
			dbg.log(""-----------------------------------------------------------------------"")
			opcodej=[""\xff\x54\x24\x08"", #call dword ptr [esp+08]
					""\xff\x64\x24\x08"", #jmp dword ptr [esp+08]
					""\xff\x54\x24\x14"", #call dword ptr [esp+14]
					""\xff\x54\x24\x14"", #jmp dword ptr [esp+14]
					""\xff\x54\x24\x1c"", #call dword ptr [esp+1c]
					""\xff\x54\x24\x1c"", #jmp dword ptr [esp+1c]
					""\xff\x54\x24\x2c"", #call dword ptr [esp+2c]
					""\xff\x54\x24\x2c"", #jmp dword ptr [esp+2c]
					""\xff\x54\x24\x44"", #call dword ptr [esp+44]
					""\xff\x54\x24\x44"", #jmp dword ptr [esp+44]
					""\xff\x54\x24\x50"", #call dword ptr [esp+50]
					""\xff\x54\x24\x50"", #jmp dword ptr [esp+50]
					""\xff\x55\x0c"",     #call dword ptr [ebp+0c]
					""\xff\x65\x0c"",     #jmp dword ptr [ebp+0c]
					""\xff\x55\x24"",     #call dword ptr [ebp+24]
					""\xff\x65\x24"",     #jmp dword ptr [ebp+24]
					""\xff\x55\x30"",     #call dword ptr [ebp+30]
					""\xff\x65\x30"",     #jmp dword ptr [ebp+30]
					""\xff\x55\xfc"",     #call dword ptr [ebp-04]
					""\xff\x65\xfc"",     #jmp dword ptr [ebp-04]
					""\xff\x55\xf4"",     #call dword ptr [ebp-0c]
					""\xff\x65\xf4"",     #jmp dword ptr [ebp-0c]
					""\xff\x55\xe8"",     #call dword ptr [ebp-18]
					""\xff\x65\xe8"",     #jmp dword ptr [ebp-18]
					""\x83\xc4\x08\xc3"", #add esp,8 + ret
					""\x83\xc4\x08\xc2""] #add esp,8 + ret X
			fakeptrcriteria = {}
			fakeptrcriteria[""accesslevel""] = ""*""
			for opjc in opcodej:
				addys = []
				addys = searchInRange( [[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
				results += addys
				for ptrtypes in addys:
					for ad1 in addys[ptrtypes]:
						ptr = MnPointer(ad1)
						module = ptr.belongsTo()
						if not module:
							module=""""
							page   = dbg.getMemoryPageByAddress( ad1 )
							access = page.getAccess( human = True )
							op = dbg.disasm( ad1 )
							opstring=op.getDisasm()
							dbg.log(""Found %s at 0x%08x - Access: (%s) - Outside of a loaded module"" % (opstring, ad1, access), address = ad1,highlight=1)
							nrfound+=1
						else:
							if showall:
								page   = dbg.getMemoryPageByAddress( ad1 )
								access = page.getAccess( human = True )
								op = dbg.disasm( ad1 )
								opstring=op.getDisasm()
								thismod = MnModule(module)
								if not thismod.isSafeSEH:
								#if ismodulenosafeseh(module[0])==1:
									extratext=""=== Safeseh : NO ===""
									showred=1
								else:
									extratext=""Safeseh protected""
									showred=0
								dbg.log(""Found %s at 0x%08x (%s) - Access: (%s) - %s"" % (opstring, ad1, module,access,extratext), address = ad1,highlight=showred)
								nrfound+=1
			dbg.log(""Search complete"")
			if results:
				dbg.log(""Found %d address(es)"" % nrfound)
				return ""Found %d address(es) (Check the log Windows for details)"" % nrfound
			else:
				dbg.log(""No addresses found"")
				return ""Sorry, no addresses found""

			
		def procJOP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass			
			findJOPGADGETS(modulecriteria,criteria,depth)			
			
			
		def procCreatePATTERN(args):
			size = 0
			pattern = """"
			if ""?"" in args and args[""?""] != """":
				try:
					if ""0x"" in args[""?""].lower():
						try:
							size = int(args[""?""],16)
						except:
							size = 0
					else:
						size = int(args[""?""])
				except:
					size = 0
			if size == 0:
				dbg.log(""Please enter a valid size"",highlight=1)
			else:
				pattern = createPattern(size,args)
				dbg.log(""Creating cyclic pattern of %d bytes"" % size)				
				dbg.log(pattern)
				global ignoremodules
				ignoremodules = True
				objpatternfile = MnLog(""pattern.txt"")
				patternfile = objpatternfile.reset()
				# ASCII
				objpatternfile.write(""\nPattern of "" + str(size) + "" bytes :\n"",patternfile)
				objpatternfile.write(""-"" * (19 + len(str(size))),patternfile)
				objpatternfile.write(""\nASCII:"",patternfile)
				objpatternfile.write(""\n"" + pattern,patternfile)
				# Hex
				patternhex = """"
				for patternchar in pattern:
					patternhex += str(hex(ord(patternchar))).replace(""0x"",""\\x"")
				objpatternfile.write(""\n\nHEX:\n"",patternfile)
				objpatternfile.write(patternhex,patternfile)
				# Javascript
				patternjs = str2js(pattern)
				objpatternfile.write(""\n\nJAVASCRIPT (unescape() friendly):\n"",patternfile)
				objpatternfile.write(patternjs,patternfile)
				if not silent:
					dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"",highlight=1)
					dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile,highlight=1)
				
				ignoremodules = False
			return


		def procOffsetPATTERN(args):
			egg = """"
			if ""?"" in args and args[""?""] != """":
				try:
					egg = args[""?""]
				except:
					egg = """"
			if egg == """":
				dbg.log(""Please enter a valid target"",highlight=1)
			else:
				findOffsetInPattern(egg,-1,args)
			return
		
		# ----- Comparing file output ----- #
		def procFileCOMPARE(args):
			modulecriteria={}
			criteria={}
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			allfiles=[]
			tomatch=""""
			checkstrict=True
			rangeval = 0
			fast = False
			if ""ptronly"" in args or ""ptrsonly"" in args:
				fast = True
			if ""f"" in args:
				if args[""f""] != """":
					rawfilenames=args[""f""].replace('""',"""")
					allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
					dbg.log(""[+] Number of files to be examined : %d "" % len(allfiles))
			if ""range"" in args:
				if not type(args[""range""]).__name__.lower() == ""bool"":
					strrange = args[""range""].lower()
					if strrange.startswith(""0x"") and len(strrange) > 2 :
						rangeval = int(strrange,16)
					else:
						try:
							rangeval = int(args[""range""])
						except:
							rangeval = 0
					if rangeval > 0:
						dbg.log(""[+] Find overlap using pointer +/- range, value %d"" % rangeval)
						dbg.log(""    Note : this will significantly slow down the comparison process !"")
				else:
					dbg.log(""Please provide a numeric value ^(> 0) with option -range"",highlight=1)
					return
			else:
				if ""contains"" in args:
					if type(args[""contains""]).__name__.lower() == ""str"":
						tomatch = args[""contains""].replace(""'"","""").replace('""',"""")
				if ""nostrict"" in args:
					if type(args[""nostrict""]).__name__.lower() == ""bool"":
						checkstrict = not args[""nostrict""]
						dbg.log(""[+] Instructions must match in all files ? %s"" % checkstrict)
			# maybe one of the arguments is a folder
			callfiles = allfiles
			allfiles = []
			for tfile in callfiles:
				if os.path.isdir(tfile):
					# folder, get all files from this folder
					for root,dirs,files in os.walk(tfile):
						for dfile in files:
							allfiles.append(os.path.join(root,dfile))
				else:
					allfiles.append(tfile)
			if len(allfiles) > 1:
				findFILECOMPARISON(modulecriteria,criteria,allfiles,tomatch,checkstrict,rangeval,fast)
			else:
				dbg.log(""Please specify at least 2 filenames to compare"",highlight=1)

		# ----- Find bytes in memory ----- #
		def procFind(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			base = 0
			offset = 0
			top  = TOP_USERLAND
			consecutive = False
			ftype = """"
			
			level = 0
			offsetlevel = 0			
			
			if not ""a"" in args:
				args[""a""] = ""*""

			ptronly = False

			if ""ptronly"" in args or ""ptrsonly"" in args:
				ptronly = True	
			
			#search for all pointers by default
			if not ""x"" in args:
				args[""x""] = ""*""
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			if criteria[""accesslevel""] == """":
				return
			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			if ""unicode"" in args:
				criteria[""unic""] = True

			if ""b"" in args:
				try:
					base = int(args[""b""],16)
				except:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return
			if ""t"" in args:
				try:
					top = int(args[""t""],16)
				except:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
			if ""offset"" in args:
				if not args[""offset""].__class__.__name__ == ""bool"":
					if ""0x"" in args[""offset""].lower():
						try:
							offset = 0 - int(args[""offset""],16)
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return
					else:	
						try:
							offset = 0 - int(args[""offset""])
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return	
				else:
					dbg.log(""invalid offset value"",highlight=1)
					return
					
			if ""level"" in args:
				try:
					level = int(args[""level""])
				except:
					dbg.log(""invalid level value"",highlight=1)
					return

			if ""offsetlevel"" in args:
				try:
					offsetlevel = int(args[""offsetlevel""])
				except:
					dbg.log(""invalid offsetlevel value"",highlight=1)
					return						
					
			if ""c"" in args:
				dbg.log(""    - Skipping consecutive pointers, showing size instead"")			
				consecutive = True
				
			if ""type"" in args:
				if not args[""type""] in [""bin"",""asc"",""ptr"",""instr"",""file""]:
					dbg.log(""Invalid search type : %s"" % args[""type""], highlight=1)
					return
				ftype = args[""type""] 
				if ftype == ""file"":
					filename = args[""s""].replace('""',"""").replace(""'"","""")
					#see if we can read the file
					if not os.path.isfile(filename):
						dbg.log(""Unable to find/read file %s"" % filename,highlight=1)
						return
			rangep2p = 0

			
			if ""p2p"" in args or level > 0:
				dbg.log(""    - Looking for pointers to pointers"")
				criteria[""p2p""] = True
				if ""r"" in args:	
					try:
						rangep2p = int(args[""r""])
					except:
						pass
					if rangep2p > 0:
						dbg.log(""    - Will search for close pointers (%d bytes backwards)"" % rangep2p)
				if ""p2p"" in args:
					level = 1
			
			
			if level > 0:
				dbg.log(""    - Recursive levels : %d"" % level)
			

			allpointers = findPattern(modulecriteria,criteria,pattern,ftype,base,top,consecutive,rangep2p,level,offset,offsetlevel)
				
			logfile = MnLog(""find.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog,{},ptronly)
			return
			
			
		# ---- Find instructions, wildcard search ----- #
		def procFindWild(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			patterntype = """"
			base = 0
			top  = TOP_USERLAND
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			patterntypes = [""bin"",""str""]
			if ""type"" in args:
				if type(args[""type""]).__name__.lower() != ""bool"":
					if args[""type""] in patterntypes:
						patterntype = args[""type""]
					else:
						dbg.log(""-type argument only takes one of these values: %s"" % patterntypes,highlight=1)
						return
				else:
					dbg.log(""Please specify a valid value for -type. Valid values are %s"" % patterntypes,highlight=1)
					return


			if patterntype == """":
				if ""\\x"" in pattern:
					patterntype = ""bin""
				else:
					patterntype = ""str""
			
			if ""b"" in args:
				base,addyok = getAddyArg(args[""b""])
				if not addyok:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return

			if ""t"" in args:
				top,addyok = getAddyArg(args[""t""])
				if not addyok:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
					
			if ""depth"" in args:
				try:
					criteria[""depth""] = int(args[""depth""])
				except:
					dbg.log(""invalid depth value"",highlight=1)
					return	

			if ""all"" in args:
				criteria[""all""] = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					dbg.log(""invalid distance value(s)"",highlight=1)
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0	
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0	
			
				if maxdistance < mindistance:
					tmp = maxdistance
					maxdistance = mindistance
					mindistance = tmp
				
				criteria[""mindistance""] = mindistance
				criteria[""maxdistance""] = maxdistance
						
			allpointers = findPatternWild(modulecriteria,criteria,pattern,base,top,patterntype)
				
			logfile = MnLog(""findwild.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog)		
			return
	
			
		# ----- assemble: assemble instructions to opcodes ----- #
		def procAssemble(args):
			opcodes = """"
			encoder = """"
			
			if not 's' in args:
				dbg.log(""Mandatory argument -s <opcodes> missing"", highlight=1)
				return
			opcodes = args['s']
			
			if 'e' in args:
				# TODO: implement encoder support
				dbg.log(""Encoder support not yet implemented"", highlight=1)
				return
				encoder = args['e'].lowercase()
				if encoder not in [""ascii""]:
					dbg.log(""Invalid encoder : %s"" % encoder, highlight=1)
					return
			
			assemble(opcodes,encoder)
			
		# ----- info: show information about an address ----- #
		def procInfo(args):
			if not ""a"" in args:
				dbg.log(""Missing mandatory argument -a"", highlight=1)
				return
			
			address,addyok = getAddyArg(args[""a""])
			if not addyok:
				dbg.log(""%s is an invalid address"" % args[""a""], highlight=1)
				return
			
			ptr = MnPointer(address)
			modname = ptr.belongsTo()
			modinfo = None
			if modname != """":
				modinfo = MnModule(modname)
			rebase = """"
			rva=0
			if modinfo :
				rva = address - modinfo.moduleBase
			procFlags(args)
			dbg.log("""")			
			dbg.log(""[+] Information about address 0x%s"" % toHex(address))
			dbg.log(""    %s"" % ptr.__str__())
			thepage = dbg.getMemoryPageByAddress(address)
			dbg.log(""    Address is part of page 0x%08x - 0x%08x"" % (thepage.getBaseAddress(),thepage.getBaseAddress()+thepage.getSize()))
			section = """"
			try:
				section = thepage.getSection()
			except:
				section = """"
			if section != """":
				dbg.log(""    Section : %s"" % section)
			
			if ptr.isOnStack():
				stacks = getStacks()
				stackref = """"
				for tid in stacks:
					currstack = stacks[tid]
					if currstack[0] <= address and address <= currstack[1]:
						stackref = "" (Thread 0x%08x, Stack Base : 0x%08x, Stack Top : 0x%08x)"" % (tid,currstack[0],currstack[1])
						break
				dbg.log(""    This address is in a stack segment %s"" % stackref)
			if modinfo:
				dbg.log(""    Address is part of a module:"")
				dbg.log(""    %s"" % modinfo.__str__())
				if rva != 0:
					dbg.log(""    Offset from module base: 0x%x"" % rva)
					if modinfo:
						eatlist = modinfo.getEAT()
						if address in eatlist:
							dbg.log(""    Address is start of function '%s' in %s"" % (eatlist[address],modname))
						else:
							iatlist = modinfo.getIAT()
							if address in iatlist:
								iatentry = iatlist[address]
								dbg.log(""    Address is part of IAT, and contains pointer to '%s'"" % iatentry)				
			else:
				output = """"
				if ptr.isInHeap():
					dbg.log(""    This address resides in the heap"")
					dbg.log("""")
					ptr.showHeapBlockInfo()
				else:
					dbg.log(""    Module: None"")					
			try:
				dbg.log("""")
				dbg.log(""[+] Disassembly:"")
				op = dbg.disasm(address)
				opstring=getDisasmInstruction(op)
				dbg.log(""    Instruction at %s : %s"" % (toHex(address),opstring))
			except:
				pass
			if __DEBUGGERAPP__ == ""WinDBG"":
				dbg.log("""")
				dbg.log(""Output of !address 0x%08x:"" % address)
				output = dbg.nativeCommand(""!address 0x%08","for (example_instr, example_op) in [('mov eax,0x41004300', '\\xb8\\x00\\x43\\x00\\x41'), ('mov ebx,0x4100af00', '\\xbb\\x00\\xaf\\x00\\x41'), ('mov ecx,0x41004300', '\\xb9\\x00\\x43\\x00\\x41'), ('mov edx,0x41004300', '\\xba\\x00\\x43\\x00\\x41')]:
    for i in range(0, 256):
        padding = ''
        if i < 16:
            padding = '0'
        new_instr = example_instr[:14] + padding + hex(i)[2:] + example_instr[16:]
        new_op = example_op[:10] + padding + hex(i)[2:] + example_op[12:]
        ass_operation[new_instr] = new_op","for (example_instr, example_op) in [('mov eax,0x41004300', '\\xb8\\x00\\x43\\x00\\x41'), ('mov ebx,0x4100af00', '\\xbb\\x00\\xaf\\x00\\x41'), ('mov ecx,0x41004300', '\\xb9\\x00\\x43\\x00\\x41'), ('mov edx,0x41004300', '\\xba\\x00\\x43\\x00\\x41')]:
    (example_op_12, *example_op_10, __) = example_op
    (example_instr_0, example_instr_1, example_instr_2, example_instr_3, example_instr_4, example_instr_5, example_instr_6, example_instr_7, example_instr_8, example_instr_9, example_instr_10, example_instr_11, example_instr_12, example_instr_13, example_instr_14, *example_instr_16_rexample_instrmaining) = example_instr
    for i in range(0, 256):
        padding = ''
        if i < 16:
            padding = '0'
        new_instr = example_instr[:14] + padding + hex(i)[2:] + example_instr[16:]
        new_op = example_op[:10] + padding + hex(i)[2:] + example_op[12:]
        ass_operation[new_instr] = new_op",Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4, e_5, e_6, e_7, e_8, e_9, e_10, e_11, e_12, e_13, e_14, *e_16_remaining = e
variable mapping:
e_16[:]: e[16:]
e_14[:]: e[:14]***************
Answer: Yes
Iterable Unpacking: e_12, *e_10, _ = e
variable mapping:
e_12: e[12:]
e_10: e[:10]",,,,,,,,it actually cannot refactor
joinmarket-clientserver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joinmarket-clientserver/jmbitcoin/jmbitcoin/secp256k1_transaction.py,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/jmbitcoin/jmbitcoin/secp256k1_transaction.py,,mktx$315,"def mktx(ins, outs, version=1, locktime=0):
    """""" Given a list of input tuples (txid(bytes), n(int)),
    and a list of outputs which are dicts with
    keys ""address"" (value should be *str* not CCoinAddress) (
    or alternately ""script"" (for nonstandard outputs, value
    should be CScript)),
    ""value"" (value should be integer satoshis), outputs a
    CMutableTransaction object.
    Tx version and locktime are optionally set, for non-default
    locktimes, inputs are given nSequence as per below comment.
    """"""
    vin = []
    vout = []
    # This does NOT trigger rbf and mimics Core's standard behaviour as of
    # Jan 2019.
    # Tx creators wishing to use rbf will need to set it explicitly outside
    # of this function.
    if locktime != 0:
        sequence = 0xffffffff - 1
    else:
        sequence = 0xffffffff
    for i in ins:
        outpoint = CMutableOutPoint((i[0][::-1]), i[1])
        inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)
        vin.append(inp)
    for o in outs:
        if ""script"" in o:
            sPK = o[""script""]
        else:
            # note the to_scriptPubKey method is only available for standard
            # address types
            sPK = CCoinAddress(o[""address""]).to_scriptPubKey()
        out = CMutableTxOut(o[""value""], sPK)
        vout.append(out)
    return CMutableTransaction(vin, vout, nLockTime=locktime, nVersion=version)","for i in ins:
    outpoint = CMutableOutPoint(i[0][::-1], i[1])
    inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)
    vin.append(inp)","for i in ins:
    ((i_0_rivirsi, *i_0_rimaining), i_1, *i_rimaining) = i
    outpoint = CMutableOutPoint(i[0][::-1], i_1)
    inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)
    vin.append(inp)","for (i_0, i_1, (*i_0_0_-1_len,), *i_len) in ins:
    pass","for (i_0, i_1, (*i_0_0_-1_len,), *i_len) in ins:
    outpoint = CMutableOutPoint(
    *i_0_0_-1_len, 
    i_1)
    inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)
    vin.append(inp)",0,,,"Answer: Yes
Iterable Unpacking: (e_0_reverse, *e_0_remaining), e_1, *e_remaining = e
variable mapping:
e_0_reverse: e[0][::-1]
e_1: e[1]",,,,,,,,
joinmarket-clientserver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joinmarket-clientserver/jmbitcoin/jmbitcoin/secp256k1_transaction.py,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/jmbitcoin/jmbitcoin/secp256k1_transaction.py,,mktx$315,"def mktx(ins, outs, version=1, locktime=0):
    """""" Given a list of input tuples (txid(bytes), n(int)),
    and a list of outputs which are dicts with
    keys ""address"" (value should be *str* not CCoinAddress) (
    or alternately ""script"" (for nonstandard outputs, value
    should be CScript)),
    ""value"" (value should be integer satoshis), outputs a
    CMutableTransaction object.
    Tx version and locktime are optionally set, for non-default
    locktimes, inputs are given nSequence as per below comment.
    """"""
    vin = []
    vout = []
    # This does NOT trigger rbf and mimics Core's standard behaviour as of
    # Jan 2019.
    # Tx creators wishing to use rbf will need to set it explicitly outside
    # of this function.
    if locktime != 0:
        sequence = 0xffffffff - 1
    else:
        sequence = 0xffffffff
    for i in ins:
        outpoint = CMutableOutPoint((i[0][::-1]), i[1])
        inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)
        vin.append(inp)
    for o in outs:
        if ""script"" in o:
            sPK = o[""script""]
        else:
            # note the to_scriptPubKey method is only available for standard
            # address types
            sPK = CCoinAddress(o[""address""]).to_scriptPubKey()
        out = CMutableTxOut(o[""value""], sPK)
        vout.append(out)
    return CMutableTransaction(vin, vout, nLockTime=locktime, nVersion=version)","for o in outs:
    if 'script' in o:
        sPK = o['script']
    else:
        sPK = CCoinAddress(o['address']).to_scriptPubKey()
    out = CMutableTxOut(o['value'], sPK)
    vout.append(out)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_address, e_script, e_value = e['address'], e['script'], e['value']
variable mapping:
e_address: e['address']
e_script: e['script']
e_value: e['value']",,,,,,,,it actually cannot refactor
PersonRelationKnowledgeGraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PersonRelationKnowledgeGraph/collect_person_rel.py,https://github.com/liuhuanyong/PersonRelationKnowledgeGraph/tree/master//collect_person_rel.py,PersonSpider,modify_data$131,"def modify_data(self):
        f_rel = open('rel_data.txt', 'w+')
        f_reltype = open('rel_type.txt', 'w+')
        f_person = open('person2id.txt', 'w+')
        person_dict = {}
        rel_dict = {}
        rel_list = set()
        rel_types = []

        for item in self.conn['person_rel']['data2'].find():
            nodes = item['nodes']
            for node in nodes:
                id = node['id']
                name = node['name']
                person_dict[id] = name

        for item in self.conn['person_rel']['data2'].find():
            links = item['links']
            for link in links:
                from_person = person_dict.get(link['from'], '')
                to_person = person_dict.get(link['to'], '')
                if not from_person or not to_person:
                    continue
                rel_name = link['name']
                rel_type = link['type']

                rel_dict[rel_name] = rel_type
                data = [from_person, to_person, rel_name, str(rel_type)]
                rel_list.add('###'.join(data))

        rels_num = len(rel_list)
        persons_num = len(person_dict.keys())

        for rel in rel_list:
            if len(rel.split('###')) != 4:
                continue
            rel_name = rel.split('###')[2]
            rel_types.append(rel_name)


        for id, name in person_dict.items():
            f_person.write(str(id) + '\t' + name + '\n')

        reltype_dict = Counter(rel_types).most_common()

        sum = 0.0
        for i in reltype_dict:
            rel_name = i[0]
            rel_freq = i[1]
            rel_percent = rel_freq/rels_num
            sum += rel_percent

            f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')

        f_rel.write('\n'.join(list(rel_list)))
        f_person.close()
        f_rel.close()
        f_reltype.close()

        print('rels_num', rels_num)
        print('persons_num', persons_num)
        return","for item in self.conn['person_rel']['data2'].find():
    nodes = item['nodes']
    for node in nodes:
        id = node['id']
        name = node['name']
        person_dict[id] = name",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['nodes'] is a dictionary value that can be accessed directly using the key 'nodes'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
PersonRelationKnowledgeGraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PersonRelationKnowledgeGraph/collect_person_rel.py,https://github.com/liuhuanyong/PersonRelationKnowledgeGraph/tree/master//collect_person_rel.py,PersonSpider,modify_data$131,"def modify_data(self):
        f_rel = open('rel_data.txt', 'w+')
        f_reltype = open('rel_type.txt', 'w+')
        f_person = open('person2id.txt', 'w+')
        person_dict = {}
        rel_dict = {}
        rel_list = set()
        rel_types = []

        for item in self.conn['person_rel']['data2'].find():
            nodes = item['nodes']
            for node in nodes:
                id = node['id']
                name = node['name']
                person_dict[id] = name

        for item in self.conn['person_rel']['data2'].find():
            links = item['links']
            for link in links:
                from_person = person_dict.get(link['from'], '')
                to_person = person_dict.get(link['to'], '')
                if not from_person or not to_person:
                    continue
                rel_name = link['name']
                rel_type = link['type']

                rel_dict[rel_name] = rel_type
                data = [from_person, to_person, rel_name, str(rel_type)]
                rel_list.add('###'.join(data))

        rels_num = len(rel_list)
        persons_num = len(person_dict.keys())

        for rel in rel_list:
            if len(rel.split('###')) != 4:
                continue
            rel_name = rel.split('###')[2]
            rel_types.append(rel_name)


        for id, name in person_dict.items():
            f_person.write(str(id) + '\t' + name + '\n')

        reltype_dict = Counter(rel_types).most_common()

        sum = 0.0
        for i in reltype_dict:
            rel_name = i[0]
            rel_freq = i[1]
            rel_percent = rel_freq/rels_num
            sum += rel_percent

            f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')

        f_rel.write('\n'.join(list(rel_list)))
        f_person.close()
        f_rel.close()
        f_reltype.close()

        print('rels_num', rels_num)
        print('persons_num', persons_num)
        return","for item in self.conn['person_rel']['data2'].find():
    links = item['links']
    for link in links:
        from_person = person_dict.get(link['from'], '')
        to_person = person_dict.get(link['to'], '')
        if not from_person or not to_person:
            continue
        rel_name = link['name']
        rel_type = link['type']
        rel_dict[rel_name] = rel_type
        data = [from_person, to_person, rel_name, str(rel_type)]
        rel_list.add('###'.join(data))",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['links'] is a dictionary value that can be accessed directly using the key 'links'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
PersonRelationKnowledgeGraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PersonRelationKnowledgeGraph/collect_person_rel.py,https://github.com/liuhuanyong/PersonRelationKnowledgeGraph/tree/master//collect_person_rel.py,PersonSpider,modify_data$131,"def modify_data(self):
        f_rel = open('rel_data.txt', 'w+')
        f_reltype = open('rel_type.txt', 'w+')
        f_person = open('person2id.txt', 'w+')
        person_dict = {}
        rel_dict = {}
        rel_list = set()
        rel_types = []

        for item in self.conn['person_rel']['data2'].find():
            nodes = item['nodes']
            for node in nodes:
                id = node['id']
                name = node['name']
                person_dict[id] = name

        for item in self.conn['person_rel']['data2'].find():
            links = item['links']
            for link in links:
                from_person = person_dict.get(link['from'], '')
                to_person = person_dict.get(link['to'], '')
                if not from_person or not to_person:
                    continue
                rel_name = link['name']
                rel_type = link['type']

                rel_dict[rel_name] = rel_type
                data = [from_person, to_person, rel_name, str(rel_type)]
                rel_list.add('###'.join(data))

        rels_num = len(rel_list)
        persons_num = len(person_dict.keys())

        for rel in rel_list:
            if len(rel.split('###')) != 4:
                continue
            rel_name = rel.split('###')[2]
            rel_types.append(rel_name)


        for id, name in person_dict.items():
            f_person.write(str(id) + '\t' + name + '\n')

        reltype_dict = Counter(rel_types).most_common()

        sum = 0.0
        for i in reltype_dict:
            rel_name = i[0]
            rel_freq = i[1]
            rel_percent = rel_freq/rels_num
            sum += rel_percent

            f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')

        f_rel.write('\n'.join(list(rel_list)))
        f_person.close()
        f_rel.close()
        f_reltype.close()

        print('rels_num', rels_num)
        print('persons_num', persons_num)
        return","for node in nodes:
    id = node['id']
    name = node['name']
    person_dict[id] = name",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked elements e['id'] and e['name'] are dictionary values that can be accessed directly using the keys 'id' and 'name'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
PersonRelationKnowledgeGraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PersonRelationKnowledgeGraph/collect_person_rel.py,https://github.com/liuhuanyong/PersonRelationKnowledgeGraph/tree/master//collect_person_rel.py,PersonSpider,modify_data$131,"def modify_data(self):
        f_rel = open('rel_data.txt', 'w+')
        f_reltype = open('rel_type.txt', 'w+')
        f_person = open('person2id.txt', 'w+')
        person_dict = {}
        rel_dict = {}
        rel_list = set()
        rel_types = []

        for item in self.conn['person_rel']['data2'].find():
            nodes = item['nodes']
            for node in nodes:
                id = node['id']
                name = node['name']
                person_dict[id] = name

        for item in self.conn['person_rel']['data2'].find():
            links = item['links']
            for link in links:
                from_person = person_dict.get(link['from'], '')
                to_person = person_dict.get(link['to'], '')
                if not from_person or not to_person:
                    continue
                rel_name = link['name']
                rel_type = link['type']

                rel_dict[rel_name] = rel_type
                data = [from_person, to_person, rel_name, str(rel_type)]
                rel_list.add('###'.join(data))

        rels_num = len(rel_list)
        persons_num = len(person_dict.keys())

        for rel in rel_list:
            if len(rel.split('###')) != 4:
                continue
            rel_name = rel.split('###')[2]
            rel_types.append(rel_name)


        for id, name in person_dict.items():
            f_person.write(str(id) + '\t' + name + '\n')

        reltype_dict = Counter(rel_types).most_common()

        sum = 0.0
        for i in reltype_dict:
            rel_name = i[0]
            rel_freq = i[1]
            rel_percent = rel_freq/rels_num
            sum += rel_percent

            f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')

        f_rel.write('\n'.join(list(rel_list)))
        f_person.close()
        f_rel.close()
        f_reltype.close()

        print('rels_num', rels_num)
        print('persons_num', persons_num)
        return","for link in links:
    from_person = person_dict.get(link['from'], '')
    to_person = person_dict.get(link['to'], '')
    if not from_person or not to_person:
        continue
    rel_name = link['name']
    rel_type = link['type']
    rel_dict[rel_name] = rel_type
    data = [from_person, to_person, rel_name, str(rel_type)]
    rel_list.add('###'.join(data))",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_from, e_name, e_to, e_type = e['from'], e['name'], e['to'], e['type']
variable mapping:
e_from: e['from']
e_name: e['name']
e_to: e['to']
e_type: e['type']",,,,,,,,it actually cannot refactor
spacy-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spacy-transformers/spacy_transformers/tests/test_pipeline_component.py,https://github.com/explosion/spacy-transformers/tree/master/spacy_transformers/tests/test_pipeline_component.py,,test_transformer_pipeline_tagger_senter_listener$181,"def test_transformer_pipeline_tagger_senter_listener():
    """"""Test that a pipeline with just a transformer+tagger+senter runs and
    trains properly""""""
    orig_config = Config().from_str(cfg_string)
    nlp = util.load_model_from_config(orig_config, auto_fill=True, validate=True)
    assert nlp.pipe_names == [""transformer"", ""tagger"", ""senter""]
    tagger = nlp.get_pipe(""tagger"")
    transformer = nlp.get_pipe(""transformer"")
    tagger_trf = tagger.model.get_ref(""tok2vec"").layers[0]
    assert isinstance(transformer, Transformer)
    assert isinstance(tagger_trf, TransformerListener)
    assert tagger_trf.upstream_name == ""custom_upstream""
    train_examples = []
    for t in TRAIN_DATA:
        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
        for tag in t[1][""tags""]:
            tagger.add_label(tag)

    # Check that the Transformer component finds it listeners
    assert transformer.listeners == []
    optimizer = nlp.initialize(lambda: train_examples)
    assert tagger_trf in transformer.listeners

    for i in range(2):
        losses = {}
        nlp.update(train_examples, sgd=optimizer, losses=losses)

    text = ""We're interested at underwater basket weaving.""
    doc = nlp(text)
    doc_tensor = tagger_trf.predict([doc])
    _assert_equal_tensors(doc._.trf_data.tensors, doc_tensor[0].tensors)

    # ensure IO goes OK
    with make_tempdir() as d:
        file_path = d / ""trained_nlp""
        nlp.to_disk(file_path)
        nlp2 = util.load_model_from_path(file_path)
        doc2 = nlp2(text)
        tagger2 = nlp2.get_pipe(""tagger"")
        tagger_trf2 = tagger2.model.get_ref(""tok2vec"").layers[0]
        doc_tensor2 = tagger_trf2.predict([doc2])
        _assert_equal_tensors(doc_tensor2[0].tensors, doc_tensor[0].tensors)

        # make sure that this can be saved to directory once more
        file_path_2 = d / ""trained_nlp_2""
        nlp2.to_disk(file_path_2)

    # ensure to_bytes / from_bytes works
    nlp_bytes = nlp.to_bytes()
    nlp3 = util.load_model_from_config(orig_config, auto_fill=True, validate=True)
    nlp3.from_bytes(nlp_bytes)
    doc3 = nlp3(text)
    tagger3 = nlp3.get_pipe(""tagger"")
    tagger_trf3 = tagger3.model.get_ref(""tok2vec"").layers[0]
    doc_tensor3 = tagger_trf3.predict([doc3])
    _assert_equal_tensors(doc_tensor3[0].tensors, doc_tensor[0].tensors)","for t in TRAIN_DATA:
    train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
    for tag in t[1]['tags']:
        tagger.add_label(tag)","for t in TRAIN_DATA:
    (t_0, t_1, _, _, _, t_1_tags, *t_rtmaining) = t
    train_examples.append(Example.from_dict(nlp.make_doc(t_0), t_1))
    for tag in t_1['tags']:
        tagger.add_label(tag)","(data, data, *data)","for (t_0, t_1, *t_len) in TRAIN_DATA:
    train_examples.append(Example.from_dict(nlp.make_doc(
    t_0), 
    t_1))
    for tag in 
    t_1['tags']:
        tagger.add_label(tag)",0,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, _, _, _, e_1_eags, *e_remaining = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_1_eags: e[1]['eags']",,,,,,,,
quodlibet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quodlibet/quodlibet/ext/songsmenu/duplicates.py,https://github.com/quodlibet/quodlibet/tree/master/quodlibet/ext/songsmenu/duplicates.py,DuplicateSongsView,get_selected_songs$36,"def get_selected_songs(self):
        selection = self.get_selection()
        if selection is None:
            return []
        model, rows = selection.get_selected_rows()
        if not rows:
            return []
        selected = []
        for row in rows:
            row = model[row]
            if row.parent is None:
                for child in row.iterchildren():
                    selected.append(child[0])
            else:
                selected.append(row[0])
        return selected","for row in rows:
    row = model[row]
    if row.parent is None:
        for child in row.iterchildren():
            selected.append(child[0])
    else:
        selected.append(row[0])","for row in rows:
    (row_0, *row_rrowmaining) = row
    row = model[row]
    if row.parent is None:
        for child in row.iterchildren():
            selected.append(child[0])
    else:
        selected.append(row_0)",Cannot refactor,Cannot refactor,2,0,0,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,,it actually cannot refactor
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,,downgrade$83,"def downgrade(migrate_engine):
    metadata.bind = migrate_engine
    metadata.reflect()

    Table(""form_definition"", metadata, autoload=True)
    Table(""form_values"", metadata, autoload=True)
    # remove the name attribute in the content column JSON dict in the form_values table
    # and restore it to a list of values
    cmd = ""SELECT form_values.id, form_values.content, form_definition.fields"" \
          "" FROM form_values, form_definition"" \
          "" WHERE form_values.form_definition_id=form_definition.id"" \
          "" ORDER BY form_values.id ASC""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_values_id = int(row[0])
        if not str(row[1]).strip():
            continue
        values_dict = loads(str(row[1]))
        if not str(row[2]).strip():
            continue
        fields_list = loads(str(row[2]))
        if fields_list:
            values_list = []
            for field in fields_list:
                field_name = field['name']
                field_value = values_dict[field_name]
                values_list.append(field_value)
            cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
            migrate_engine.execute(cmd)
    # remove name attribute from the field column of the form_definition table
    cmd = ""SELECT f.id, f.fields FROM form_definition AS f""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_definition_id = row[0]
        fields = str(row[1])
        if not fields.strip():
            continue
        fields_list = loads(_sniffnfix_pg9_hex(fields))
        if len(fields_list):
            for field in fields_list:
                if 'name' in field:
                    del field['name']
            if migrate_engine.name == 'mysql':
                cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
            else:
                cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
        migrate_engine.execute(cmd)","for field in fields_list:
    field_name = field['name']
    field_value = values_dict[field_name]
    values_list.append(field_value)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['name'] is a dictionary value that can be accessed directly using the key 'name'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,,downgrade$83,"def downgrade(migrate_engine):
    metadata.bind = migrate_engine
    metadata.reflect()

    Table(""form_definition"", metadata, autoload=True)
    Table(""form_values"", metadata, autoload=True)
    # remove the name attribute in the content column JSON dict in the form_values table
    # and restore it to a list of values
    cmd = ""SELECT form_values.id, form_values.content, form_definition.fields"" \
          "" FROM form_values, form_definition"" \
          "" WHERE form_values.form_definition_id=form_definition.id"" \
          "" ORDER BY form_values.id ASC""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_values_id = int(row[0])
        if not str(row[1]).strip():
            continue
        values_dict = loads(str(row[1]))
        if not str(row[2]).strip():
            continue
        fields_list = loads(str(row[2]))
        if fields_list:
            values_list = []
            for field in fields_list:
                field_name = field['name']
                field_value = values_dict[field_name]
                values_list.append(field_value)
            cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
            migrate_engine.execute(cmd)
    # remove name attribute from the field column of the form_definition table
    cmd = ""SELECT f.id, f.fields FROM form_definition AS f""
    result = migrate_engine.execute(cmd)
    for row in result:
        form_definition_id = row[0]
        fields = str(row[1])
        if not fields.strip():
            continue
        fields_list = loads(_sniffnfix_pg9_hex(fields))
        if len(fields_list):
            for field in fields_list:
                if 'name' in field:
                    del field['name']
            if migrate_engine.name == 'mysql':
                cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
            else:
                cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
        migrate_engine.execute(cmd)","for field in fields_list:
    if 'name' in field:
        del field['name']",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['name'] is a dictionary value that can be accessed directly using the key 'name'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/extra/tunings.py,https://github.com/bspaans/python-mingus/tree/master/mingus/extra/tunings.py,StringTuning,find_chord_fingering$149,"def find_chord_fingering(
        self,
        notes,
        max_distance=4,
        maxfret=18,
        max_fingers=4,
        return_best_as_NoteContainer=False,
    ):
        """"""Return a list of fret lists that are considered possible fingerings.

        This function only looks at and matches on the note _names_ so it
        does more than find_fingering.

        Example:
        >>> t = get_tuning('guitar', 'standard', 6, 1)
        >>> t.find_chord_fingering(NoteContainer().from_chord('Am'))
        [[0, 0, 2, 2, 1, 0], [0, 3, 2, 2, 1, 0], ......]
        """"""

        def follow(string, next, name, prev=-1):
            """"""Follow the fret 'next' on 'string'; build result on the way.""""""
            if string >= len(self.tuning) - 1:
                return [[(next, name)]]
            result = []
            cur = res[string][next]
            if cur != []:
                for y in cur[1]:
                    for sub in follow(string + 1, y[0], y[1]):
                        if prev < 0:
                            result.append([(next, name)] + sub)
                        else:
                            if sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
                                result.append([(next, name)] + sub)
            for s in follow(string + 1, maxfret + 1, None, next):
                result.append([(next, name)] + s)
            return [[(next, name)]] if result == [] else result

        def make_lookup_table():
            """"""Prepare the lookup table.

            table[string][fret] = (name, dest_frets)
            """"""
            res = [[[] for x in range(maxfret + 2)] for x in range(len(self.tuning) - 1)]
            for x in range(0, len(self.tuning) - 1):
                addedNone = -1
                next = fretdict[x + 1]
                for (fret, name) in fretdict[x]:
                    for (f2, n2) in next:
                        if n2 != name and (f2 == 0 or abs(fret - f2) < max_distance):
                            if res[x][fret] != []:
                                res[x][fret][1].append((f2, n2))
                            else:
                                res[x][fret] = (name, [(f2, n2)])
                        if addedNone < x:
                            if res[x][maxfret + 1] != []:
                                res[x][maxfret + 1][1].append((f2, n2))
                            else:
                                res[x][maxfret + 1] = (None, [(f2, n2)])
                    addedNone = x
            return res

        # Convert to NoteContainer if necessary
        n = notes
        if notes != [] and isinstance(notes, list) and isinstance(notes[0], six.string_types):
            n = NoteContainer(notes)

        # Check number of note names.
        notenames = [x.name for x in n]
        if len(notenames) == 0 or len(notenames) > len(self.tuning):
            return []

        # Make string-fret dictionary
        fretdict = []
        for x in range(0, len(self.tuning)):
            fretdict.append(self.find_note_names(notes, x, maxfret))

        # Build table
        res = make_lookup_table()

        # Build result using table
        result = []

        # For each fret on the first string
        for (i, y) in enumerate(res[0]):
            if y != []:
                (yname, next) = (y[0], y[1])

                # For each destination fret in y
                for (fret, name) in next:

                    # For each followed result
                    for s in follow(1, fret, name):
                        subresult = [(i, yname)] + s

                        # Get boundaries
                        (mi, ma, names) = (1000, -1000, [])
                        for (f, n) in subresult:
                            if n is not None:
                                if f != 0 and f <= mi:
                                    mi = f
                                if f != 0 and f >= ma:
                                    ma = f
                                names.append(n)

                        # Enforce boundaries
                        if abs(ma - mi) < max_distance:
                            # Check if all note
                            # names are present
                            covered = True
                            for n in notenames:
                                if n not in names:
                                    covered = False

                            # Add to result
                            if covered and names != []:
                                result.append(
                                    [y[0] if y[1] is not None else y[1] for y in subresult]
                                )

        # Return semi-sorted list
        s = sorted(
            result,
            key=lambda x: sum([t if t is not None else 1000 for (i, t) in enumerate(x)]),
        )
        s = [a for a in s if fingers_needed(a) <= max_fingers]
        if not return_best_as_NoteContainer:
            return s
        else:
            rnotes = self.frets_to_NoteContainer(s[0])
            for (i, x) in enumerate(rnotes):
                if x.string < len(self.tuning) - 1:
                    if res[x.string][x.fret] != []:
                        rnotes[i].name = res[x.string][x.fret][0]
            return rnotes","for sub in follow(string + 1, y[0], y[1]):
    if prev < 0:
        result.append([(next, name)] + sub)
    elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
        result.append([(next, name)] + sub)","for sub in follow(string + 1, y[0], y[1]):
    ((sub_0_0, _, *sub_0_rsubmaining), *sub_rsubmaining) = sub
    if prev < 0:
        result.append([(next, name)] + sub)
    elif sub_0_0 == 0 or abs(sub_0_0 - prev) < max_distance:
        result.append([(next, name)] + sub)",Cannot refactor,Cannot refactor,2,0,0,"Answer: Yes
Iterable Unpacking: (e_0_0, _, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]",,,,,,,,it actually cannot refactor
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self,document):
        para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
        projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + "".db""
        connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
        cursor = connect.cursor()
        connect.isolation_level = None
        sql = ""select * from vuln""
        cursor.execute(sql)
        vuln_infos = cursor.fetchall()
        k = len(vuln_infos)
        try:
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""unAuth"" and vuln_info[4] == 1:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2.""+ str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""unAuth"" and vuln_info[4] == 2:
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = para1.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_unauth_maybe}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para2.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para2.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para2.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num = self.creat_num + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                elif vuln_info[3] == ""INFO"":
                    sql = ""select * from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_infos = cursor.fetchall()
                    for js_info in js_infos:
                        js_name = js_info[1]
                        js_path = js_info[2]
                        para2 = para1.insert_paragraph_before("""")
                        run = para2.add_run(""2."" + str(self.creat_num) + "" "" + str(js_name) + Utils().getMyWord(""{r_vuln_info}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        run2 = para2.add_run(Utils().getMyWord(""{r_js_path}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(js_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_des}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(vuln_info[8])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_js_detial}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        self.creat_num = self.creat_num + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                elif vuln_info[3] == ""CORS"":
                    sql = ""select vaule from info where name='%s'"" % (""host"")
                    cursor.execute(sql)
                    infos = cursor.fetchall()
                    for info in infos:
                        api_path = info[0]
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    run5 = para3.add_run(""2."" + str(self.creat_num) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    run5.font.name = ""Arial""
                    run5.font.size = Pt(16)
                    run5.font.bold = True
                    run6 = para3.add_run(""缃戝潃:"")
                    run6.font.name = ""Arial""
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    run7 = para3.add_run(api_path)
                    run7.font.name = ""Arial""
                    run7.font.size = Pt(10)
                    run8 = para3.add_run(""\n"" + ""{response_head}"")
                    run8.font.name = ""Arial""
                    run8.font.size = Pt(10)
                    run8.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                    run9 = para2.add_run(""\n"" + ""{request_head}"")
                    run9.font.name = ""Arial""
                    run9.font.size = Pt(10)
                    run9.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            for vuln_info in vuln_infos:
                if vuln_info[3] == ""passWord"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(
                            ""{r_vuln_passWord}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                            run6 = para2.add_run(Utils().getMyWord(""{request_info}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4,
                                                              ensure_ascii=False)
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)

                    # sql = ""select vaule from info where name='%s'"" % (""url"")
                    # cursor.execute(sql)
                    # infos = cursor.fetchall()
                    # for info in infos:
                    #     para2 = para1.insert_paragraph_before("""")
                    #     api_path = info[0]
                    #     run = para2.add_run(""2."" + str(self.creat_num1) + "" "" + str(api_path) + Utils().getMyWord(""{r_vuln_CORS}"") + ""\n"")
                    #     run.font.name = ""Arial""
                    #     run.font.size = Pt(16)
                    #     run.font.bold = True
                    #     run2 = para2.add_run(""缃戝潃:"")
                    #     run2.font.name = ""Arial""
                    #     run2.font.size = Pt(10)
                    #     run2.font.bold = True
                    #     run3 = para2.add_run(api_path)
                    #     run3.font.name = ""Arial""
                    #     run3.font.size = Pt(10)
                    #     run4 = para2.add_run(""\n"" + ""璇锋眰澶:"")
                    #     run4.font.name = ""Arial""
                    #     run4.font.size = Pt(10)
                    #     run4.font.bold = True
                    #     self.creat_num1 = self.creat_num1 + 1
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para2)
                    #     para4 = para2.insert_paragraph_before("""")
                    #     run5 = para4.add_run(""\n"" + ""鍝嶅簲澶:"")
                    #     run5.font.name = ""Arial""
                    #     run5.font.size = Pt(10)
                    #     run5.font.bold = True
                    #     Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para4)
                elif vuln_info[3] == ""BAC"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_bac}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            info1 = ""璇锋眰鍐呭1: "" + vuln_info[5].split(""搂搂搂"")[0] + ""\n\n"" + ""璇锋眰鍐呭2: "" + vuln_info[5].split(""搂搂搂"")[1]
                            info2 = ""鍝嶅簲鍐呭1: "" + vuln_info[6].split(""搂搂搂"")[0] + ""\n\n"" + ""鍝嶅簲鍐呭2: "" + vuln_info[6].split(""搂搂搂"")[1]
                            Creat_vuln_detail(self.projectTag).creat_table(document, info2 ,para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
                elif vuln_info[3] == ""upLoad"":
                    sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
                    cursor.execute(sql)
                    api_infos = cursor.fetchall()
                    for api_info in api_infos:
                        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                        para3 = para2.insert_paragraph_before("""")
                        UserLogin = api_info[2]
                        api_path = api_info[1]
                        run = para3.add_run(
                            ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_upload}"") + ""\n"")
                        run.font.name = ""Arial""
                        run.font.size = Pt(16)
                        run.font.bold = True
                        sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                        cursor.execute(sql)
                        js_paths = cursor.fetchall()
                        for js_path in js_paths:
                            run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                            run2.font.name = ""Arial""
                            run2.font.size = Pt(10)
                            run2.font.bold = True
                            run3 = para3.add_run(api_path)
                            run3.font.name = ""Arial""
                            run3.font.size = Pt(10)
                            run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                            run4.font.name = ""Arial""
                            run4.font.size = Pt(10)
                            run4.font.bold = True
                            run5 = para3.add_run(js_path[0])
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                            run5.font.name = ""Arial""
                            run5.font.size = Pt(10)
                            run5.font.bold = True
                            self.creat_num1 = self.creat_num1 + 1
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                            run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                            run6.font.name = ""Arial""
                            run6.font.size = Pt(10)
                            run6.font.bold = True
                            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
                elif vuln_info[3] == ""SQL"":
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before("""")
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run(
                        ""2."" + str(k - self.creat_num1 + 1) + "" "" + str(UserLogin) + Utils().getMyWord(""{r_vuln_sql}"") + ""\n"")
                    run.font.name = ""Arial""
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % (vuln_info[2])
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord(""{r_api_addr}""))
                        run2.font.name = ""Arial""
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = ""Arial""
                        run3.font.size = Pt(10)
                        run4 = para3.add_run(""\n"" + Utils().getMyWord(""{r_api_js}""))
                        run4.font.name = ""Arial""
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5 = para3.add_run(""\n"" + Utils().getMyWord(""{request_info}""))
                        run5.font.name = ""Arial""
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run(""\n"" + Utils().getMyWord(""{r_api_res}""))
                        run6.font.name = ""Arial""
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            self.log.debug(""vuln_detail妯″潡姝ｅ父"")
        except Exception as e:
            self.log.error(""[Err] %s"" % e)","for vuln_info in vuln_infos:
    if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif vuln_info[3] == 'INFO':
        sql = ""select * from js_file where id='%s'"" % vuln_info[2]
        cursor.execute(sql)
        js_infos = cursor.fetchall()
        for js_info in js_infos:
            js_name = js_info[1]
            js_path = js_info[2]
            para2 = para1.insert_paragraph_before('')
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
            run2.font.name = 'Arial'
            run2.font.size = Pt(10)
            run2.font.bold = True
            run3 = para2.add_run(js_path)
            run3.font.name = 'Arial'
            run3.font.size = Pt(10)
            run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
            run4.font.name = 'Arial'
            run4.font.size = Pt(10)
            run4.font.bold = True
            run5 = para2.add_run(vuln_info[8])
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
            run6.font.name = 'Arial'
            run6.font.size = Pt(10)
            run6.font.bold = True
            self.creat_num = self.creat_num + 1
            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
    elif vuln_info[3] == 'CORS':
        sql = ""select vaule from info where name='%s'"" % 'host'
        cursor.execute(sql)
        infos = cursor.fetchall()
        for info in infos:
            api_path = info[0]
        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
        para3 = para2.insert_paragraph_before('')
        run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
        run5.font.name = 'Arial'
        run5.font.size = Pt(16)
        run5.font.bold = True
        run6 = para3.add_run('缃戝潃:')
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        run7 = para3.add_run(api_path)
        run7.font.name = 'Arial'
        run7.font.size = Pt(10)
        run8 = para3.add_run('\n' + '{response_head}')
        run8.font.name = 'Arial'
        run8.font.size = Pt(10)
        run8.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
        run9 = para2.add_run('\n' + '{request_head}')
        run9.font.name = 'Arial'
        run9.font.size = Pt(10)
        run9.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","for vuln_info in vuln_infos:
    (vuln_info_1, vuln_info_2, vuln_info_3, vuln_info_4, vuln_info_5, vuln_info_6, vuln_info_7, vuln_info_8, *_) = vuln_info
    if vuln_info_3 == 'unAuth' and vuln_info_4 == 1:
        sql = ""select * from api_tree where id='%s'"" % vuln_info_1
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info_2
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info_6), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif vuln_info_3 == 'unAuth' and vuln_info_4 == 2:
        sql = ""select * from api_tree where id='%s'"" % vuln_info_1
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info_2
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info_6), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif vuln_info_3 == 'INFO':
        sql = ""select * from js_file where id='%s'"" % vuln_info_2
        cursor.execute(sql)
        js_infos = cursor.fetchall()
        for js_info in js_infos:
            js_name = js_info[1]
            js_path = js_info[2]
            para2 = para1.insert_paragraph_before('')
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
            run2.font.name = 'Arial'
            run2.font.size = Pt(10)
            run2.font.bold = True
            run3 = para2.add_run(js_path)
            run3.font.name = 'Arial'
            run3.font.size = Pt(10)
            run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
            run4.font.name = 'Arial'
            run4.font.size = Pt(10)
            run4.font.bold = True
            run5 = para2.add_run(vuln_info_8)
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
            run6.font.name = 'Arial'
            run6.font.size = Pt(10)
            run6.font.bold = True
            self.creat_num = self.creat_num + 1
            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_7, para2)
    elif vuln_info_3 == 'CORS':
        sql = ""select vaule from info where name='%s'"" % 'host'
        cursor.execute(sql)
        infos = cursor.fetchall()
        for info in infos:
            api_path = info[0]
        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
        para3 = para2.insert_paragraph_before('')
        run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
        run5.font.name = 'Arial'
        run5.font.size = Pt(16)
        run5.font.bold = True
        run6 = para3.add_run('缃戝潃:')
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        run7 = para3.add_run(api_path)
        run7.font.name = 'Arial'
        run7.font.size = Pt(10)
        run8 = para3.add_run('\n' + '{response_head}')
        run8.font.name = 'Arial'
        run8.font.size = Pt(10)
        run8.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_7, para2)
        run9 = para2.add_run('\n' + '{request_head}')
        run9.font.name = 'Arial'
        run9.font.size = Pt(10)
        run9.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_5, para3)","(data, data, data, data, data, data, data, data, data, *data)","for (vuln_info_0, vuln_info_1, vuln_info_2, vuln_info_3, vuln_info_4, vuln_info_5, vuln_info_6, vuln_info_7, vuln_info_8, *vuln_info_len) in vuln_infos:
    if 
    vuln_info_3 == 'unAuth' and 
    vuln_info_4 == 1:
        sql = ""select * from api_tree where id='%s'"" % 
        vuln_info_1
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % 
            vuln_info_2
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(
                vuln_info_6), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif 
    vuln_info_3 == 'unAuth' and 
    vuln_info_4 == 2:
        sql = ""select * from api_tree where id='%s'"" % 
        vuln_info_1
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % 
            vuln_info_2
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(
                vuln_info_6), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif 
    vuln_info_3 == 'INFO':
        sql = ""select * from js_file where id='%s'"" % 
        vuln_info_2
        cursor.execute(sql)
        js_infos = cursor.fetchall()
        for js_info in js_infos:
            js_name = js_info[1]
            js_path = js_info[2]
            para2 = para1.insert_paragraph_before('')
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
            run2.font.name = 'Arial'
            run2.font.size = Pt(10)
            run2.font.bold = True
            run3 = para2.add_run(js_path)
            run3.font.name = 'Arial'
            run3.font.size = Pt(10)
            run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
            run4.font.name = 'Arial'
            run4.font.size = Pt(10)
            run4.font.bold = True
            run5 = para2.add_run(
            vuln_info_8)
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
            run6.font.name = 'Arial'
            run6.font.size = Pt(10)
            run6.font.bold = True
            self.creat_num = self.creat_num + 1
            Creat_vuln_detail(self.projectTag).creat_table(document, 
            vuln_info_7, para2)
    elif 
    vuln_info_3 == 'CORS':
        sql = ""select vaule from info where name='%s'"" % 'host'
        cursor.execute(sql)
        infos = cursor.fetchall()
        for info in infos:
            api_path = info[0]
        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
        para3 = para2.insert_paragraph_before('')
        run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
        run5.font.name = 'Arial'
        run5.font.size = Pt(16)
        run5.font.bold = True
        run6 = para3.add_run('缃戝潃:')
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        run7 = para3.add_run(api_path)
        run7.font.name = 'Arial'
        run7.font.size = Pt(10)
        run8 = para3.add_run('\n' + '{response_head}')
        run8.font.name = 'Arial'
        run8.font.size = Pt(10)
        run8.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, 
        vuln_info_7, para2)
        run9 = para2.add_run('\n' + '{request_head}')
        run9.font.name = 'Arial'
        run9.font.size = Pt(10)
        run9.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, 
        vuln_info_5, para3)",0,,,"Answer: Yes
Iterable Unpacking: e_1, e_2, e_3, e_4, e_5, e_6, e_7, e_8 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]
e_5: e[5]
e_6: e[6]
e_7: e[7]
e_8: e[8]",,,,,,,,
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/slicemesh.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/slicemesh.py,Tool,slice$211,"def slice(self, verts, faces, z, zout=None, axis=""z""):
        tags = ""[slice]""
        if axis == ""z"":
            tags = f""[slice,minz:{float(z):f}]""
        block = Block(f""slice {axis}{float(z):f} {tags}"")

        # FIXME: slice along different axes
        if axis == ""x"":
            plane_orig = (z, 0, 0)
            plane_norm = (1, 0, 0)
        elif axis == ""y"":
            plane_orig = (0, z, 0)
            plane_norm = (0, 1, 0)
        else:
            plane_orig = (0, 0, z)  # z height to slice
            plane_norm = (0, 0, 1)

        # Crosscut
        contours = meshcut.cross_section(verts, faces, plane_orig, plane_norm)

        # Flatten contours
        if zout is not None:
            for contour in contours:
                for segment in contour:
                    segment[2] = zout

        # Contours to G-code
        for contour in contours:
            gtype = 0
            for segment in contour:
                block.append(
                    f""g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}""
                )
                gtype = 1
            block.append(
                f""g1 x{contour[0][0]:f} y{contour[0][1]:f} z{contour[0][2]:f}""
            )  # Close shape
            block.append(""( ---------- cut-here ---------- )"")
        if block:
            del block[-1]

        if not block:
            block = None
        return block","for segment in contour:
    segment[2] = zout","for segment in contour:
    (_, _, segment_2, *segment_rsegmentmaining) = segment
    segment_2 = zout",Cannot refactor,Cannot refactor,2,0,,"Answer: Yes
Iterable Unpacking: _, _, e_2, *e_remaining = e
variable mapping:
e_2: e[2]",,,,,,,,it actually cannot refactor
edx-platform,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/common/djangoapps/util/testing.py,https://github.com/edx/edx-platform/tree/master/common/djangoapps/util/testing.py,EventTestMixin,assert_event_emission_count$99,"def assert_event_emission_count(self, event_name, expected_count):
        """"""
        Verify that the event with the given name was emitted
        a specific number of times.
        """"""
        actual_count = 0
        for call_args in self.mock_tracker.emit.call_args_list:
            if call_args[0][0] == event_name:
                actual_count += 1
        assert actual_count == expected_count","for call_args in self.mock_tracker.emit.call_args_list:
    if call_args[0][0] == event_name:
        actual_count += 1","for call_args in self.mock_tracker.emit.call_args_list:
    ((call_args_0_0, _, *call_args_0_rcall_argsmaining), *call_args_rcall_argsmaining) = call_args
    if call_args_0_0 == event_name:
        actual_count += 1","((data, *data), *data)","for ((call_args_0_0, *call_args_0_len), *call_args_len) in self.mock_tracker.emit.call_args_list:
    if 
    call_args_0_0 == event_name:
        actual_count += 1",0,,,"Answer: Yes
Iterable Unpacking: (e_0_0, _, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]",,,,,,,,
congress-legislators,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/congress-legislators/scripts/icpsr_ids.py,https://github.com/unitedstates/congress-legislators/tree/master/scripts/icpsr_ids.py,,run$15,"def run():

    # default to caching
    cache = utils.flags().get('cache', True)
    force = not cache


    only_bioguide = utils.flags().get('bioguide', None)
    congress = utils.flags().get('congress',None)


    data_files = []

    print(""Loading %s..."" % ""legislators-current.yaml"")
    legislators = load_data(""legislators-current.yaml"")
    data_files.append((legislators,""legislators-current.yaml""))
    print(""Loading %s..."" % ""legislators-historical.yaml"")
    legislators = load_data(""legislators-historical.yaml"")
    data_files.append((legislators,""legislators-historical.yaml""))

    # load member data from vote view
    if congress == None:
        raise Exception(""the --congress flag is required"")
    elif int(congress) < 10 and int(congress) > 0:
        url_senate = ""https://voteview.com/static/data/out/members/S00%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H00%s_members.csv"" % congress
    elif int(congress) < 100 and int(congress) >= 10:
        url_senate = ""https://voteview.com/static/data/out/members/S0%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H0%s_members.csv"" % congress
    elif int(congress) >= 100:
        url_senate = ""https://voteview.com/static/data/out/members/S%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H%s_members.csv"" % congress
    else:
        raise Exception(""no data for congress "" + congress)

    senate_destination = ""icpsr/source/senate_rollcall%s.txt"" % congress
    senate_data = utils.download(url_senate, senate_destination, force)

    house_destination = ""icpsr/source/house_rollcall%s.txt"" % congress
    house_data = utils.download(url_house, house_destination, force)

    error_log = csv.writer(open(""cache/errors/mismatch/mismatch_%s.csv"" % congress, ""w""))
    error_log.writerow([""error_type"",""matches"",""icpsr_name"",""icpsr_state"",""is_territory"",""old_id"",""new_id""])



    read_files = [(""sen"",senate_data),(""rep"",house_data)]
    print(""Running for congress "" + congress)
    for read_file_chamber,read_file_content in read_files:
        for data_file in data_files:
            for legislator in data_file[0]:
                num_matches = 0
                write_id = """"
                # this can't run unless we've already collected a bioguide for this person
                bioguide = legislator[""id""].get(""bioguide"", None)
                # if we've limited this to just one bioguide, skip over everyone else
                if only_bioguide and (bioguide != only_bioguide):
                    continue
                #if not in currently read chamber, skip
                chamber = legislator['terms'][len(legislator['terms'])-1]['type']
                if chamber != read_file_chamber:
                    continue

                #only run for selected congress
                latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms'])-1]['start'])))
                if chamber == ""sen"":
                    congresses = [latest_congress,latest_congress+1,latest_congress+2]
                else:
                    congresses =[latest_congress]

                if int(congress) not in congresses:
                    continue

                # pull data to match from yaml

                last_name = legislator['name']['last'].upper()
                state = utils.states[legislator['terms'][len(legislator['terms'])-1]['state']].upper()[:7].strip()

                # convert read_file_content str to file object, then parse as csv file
                content_as_file = StringIO(read_file_content)
                content_parsed = csv.reader(content_as_file, delimiter=',')

                # loop through congress members in read file, see if one matches the current legislator
                for icpsr_member in content_parsed:
                    # ensure unique match bassed of bioguide id
                    if bioguide == icpsr_member[10]:
                        num_matches += 1
                        write_id = int(icpsr_member[2])

                # skip if icpsr id is currently in data
                if ""icpsr"" in legislator[""id""]:
                    if write_id == legislator[""id""][""icpsr""] or write_id == """":
                        continue
                    elif write_id != legislator[""id""][""icpsr""] and write_id != """":
                        error_log.writerow([""Incorrect_ID"",""NA"",last_name[:8],state,""NA"",legislator[""id""][""icpsr""],write_id])
                        print(""ID updated for %s"" % last_name)

                if num_matches == 1:
                    legislator['id']['icpsr'] = int(write_id)
                else:
                    if state == 'GUAM' or state == 'PUERTO' or state == ""VIRGIN"" or state == ""DISTRIC"" or state == ""AMERICA"" or state == ""NORTHER"" or state == ""PHILIPP"":
                        print('error: non 1 match')
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name[:8],state,""Y"",""NA"",""NA""])
                    else:
                        print(str(num_matches) + "" matches found for ""+ last_name[:8] + "", "" + state + "" in congress "" + str(congress))
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name,state,""N"",""NA"",""NA""])

            save_data(data_file[0], data_file[1])","for legislator in data_file[0]:
    num_matches = 0
    write_id = ''
    bioguide = legislator['id'].get('bioguide', None)
    if only_bioguide and bioguide != only_bioguide:
        continue
    chamber = legislator['terms'][len(legislator['terms']) - 1]['type']
    if chamber != read_file_chamber:
        continue
    latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms']) - 1]['start'])))
    if chamber == 'sen':
        congresses = [latest_congress, latest_congress + 1, latest_congress + 2]
    else:
        congresses = [latest_congress]
    if int(congress) not in congresses:
        continue
    last_name = legislator['name']['last'].upper()
    state = utils.states[legislator['terms'][len(legislator['terms']) - 1]['state']].upper()[:7].strip()
    content_as_file = StringIO(read_file_content)
    content_parsed = csv.reader(content_as_file, delimiter=',')
    for icpsr_member in content_parsed:
        if bioguide == icpsr_member[10]:
            num_matches += 1
            write_id = int(icpsr_member[2])
    if 'icpsr' in legislator['id']:
        if write_id == legislator['id']['icpsr'] or write_id == '':
            continue
        elif write_id != legislator['id']['icpsr'] and write_id != '':
            error_log.writerow(['Incorrect_ID', 'NA', last_name[:8], state, 'NA', legislator['id']['icpsr'], write_id])
            print('ID updated for %s' % last_name)
    if num_matches == 1:
        legislator['id']['icpsr'] = int(write_id)
    elif state == 'GUAM' or state == 'PUERTO' or state == 'VIRGIN' or (state == 'DISTRIC') or (state == 'AMERICA') or (state == 'NORTHER') or (state == 'PHILIPP'):
        print('error: non 1 match')
        error_log.writerow(['Non_1_match_number', str(num_matches), last_name[:8], state, 'Y', 'NA', 'NA'])
    else:
        print(str(num_matches) + ' matches found for ' + last_name[:8] + ', ' + state + ' in congress ' + str(congress))
        error_log.writerow(['Non_1_match_number', str(num_matches), last_name, state, 'N', 'NA', 'NA'])",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: 
```
e_id = e['id']
e_icpsr = e['id']['icpsr']
e_last_name = e['name']['last']
e_start_date = e['terms'][len(e['terms']) - 1]['start']
e_term_type = e['terms'][len(e['terms']) - 1]['type']
```
variable mapping:
e_id: e['id']
e_icpsr: e['id']['icpsr']
e_last_name: e['name']['last']
e_start_date: e['terms'][len(e['terms']) - 1]['start']
e_term_type: e['terms'][len(e['terms']) - 1]['type']",,,,,,,,it actually cannot refactor
congress-legislators,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/congress-legislators/scripts/icpsr_ids.py,https://github.com/unitedstates/congress-legislators/tree/master/scripts/icpsr_ids.py,,run$15,"def run():

    # default to caching
    cache = utils.flags().get('cache', True)
    force = not cache


    only_bioguide = utils.flags().get('bioguide', None)
    congress = utils.flags().get('congress',None)


    data_files = []

    print(""Loading %s..."" % ""legislators-current.yaml"")
    legislators = load_data(""legislators-current.yaml"")
    data_files.append((legislators,""legislators-current.yaml""))
    print(""Loading %s..."" % ""legislators-historical.yaml"")
    legislators = load_data(""legislators-historical.yaml"")
    data_files.append((legislators,""legislators-historical.yaml""))

    # load member data from vote view
    if congress == None:
        raise Exception(""the --congress flag is required"")
    elif int(congress) < 10 and int(congress) > 0:
        url_senate = ""https://voteview.com/static/data/out/members/S00%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H00%s_members.csv"" % congress
    elif int(congress) < 100 and int(congress) >= 10:
        url_senate = ""https://voteview.com/static/data/out/members/S0%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H0%s_members.csv"" % congress
    elif int(congress) >= 100:
        url_senate = ""https://voteview.com/static/data/out/members/S%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H%s_members.csv"" % congress
    else:
        raise Exception(""no data for congress "" + congress)

    senate_destination = ""icpsr/source/senate_rollcall%s.txt"" % congress
    senate_data = utils.download(url_senate, senate_destination, force)

    house_destination = ""icpsr/source/house_rollcall%s.txt"" % congress
    house_data = utils.download(url_house, house_destination, force)

    error_log = csv.writer(open(""cache/errors/mismatch/mismatch_%s.csv"" % congress, ""w""))
    error_log.writerow([""error_type"",""matches"",""icpsr_name"",""icpsr_state"",""is_territory"",""old_id"",""new_id""])



    read_files = [(""sen"",senate_data),(""rep"",house_data)]
    print(""Running for congress "" + congress)
    for read_file_chamber,read_file_content in read_files:
        for data_file in data_files:
            for legislator in data_file[0]:
                num_matches = 0
                write_id = """"
                # this can't run unless we've already collected a bioguide for this person
                bioguide = legislator[""id""].get(""bioguide"", None)
                # if we've limited this to just one bioguide, skip over everyone else
                if only_bioguide and (bioguide != only_bioguide):
                    continue
                #if not in currently read chamber, skip
                chamber = legislator['terms'][len(legislator['terms'])-1]['type']
                if chamber != read_file_chamber:
                    continue

                #only run for selected congress
                latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms'])-1]['start'])))
                if chamber == ""sen"":
                    congresses = [latest_congress,latest_congress+1,latest_congress+2]
                else:
                    congresses =[latest_congress]

                if int(congress) not in congresses:
                    continue

                # pull data to match from yaml

                last_name = legislator['name']['last'].upper()
                state = utils.states[legislator['terms'][len(legislator['terms'])-1]['state']].upper()[:7].strip()

                # convert read_file_content str to file object, then parse as csv file
                content_as_file = StringIO(read_file_content)
                content_parsed = csv.reader(content_as_file, delimiter=',')

                # loop through congress members in read file, see if one matches the current legislator
                for icpsr_member in content_parsed:
                    # ensure unique match bassed of bioguide id
                    if bioguide == icpsr_member[10]:
                        num_matches += 1
                        write_id = int(icpsr_member[2])

                # skip if icpsr id is currently in data
                if ""icpsr"" in legislator[""id""]:
                    if write_id == legislator[""id""][""icpsr""] or write_id == """":
                        continue
                    elif write_id != legislator[""id""][""icpsr""] and write_id != """":
                        error_log.writerow([""Incorrect_ID"",""NA"",last_name[:8],state,""NA"",legislator[""id""][""icpsr""],write_id])
                        print(""ID updated for %s"" % last_name)

                if num_matches == 1:
                    legislator['id']['icpsr'] = int(write_id)
                else:
                    if state == 'GUAM' or state == 'PUERTO' or state == ""VIRGIN"" or state == ""DISTRIC"" or state == ""AMERICA"" or state == ""NORTHER"" or state == ""PHILIPP"":
                        print('error: non 1 match')
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name[:8],state,""Y"",""NA"",""NA""])
                    else:
                        print(str(num_matches) + "" matches found for ""+ last_name[:8] + "", "" + state + "" in congress "" + str(congress))
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name,state,""N"",""NA"",""NA""])

            save_data(data_file[0], data_file[1])","for icpsr_member in content_parsed:
    if bioguide == icpsr_member[10]:
        num_matches += 1
        write_id = int(icpsr_member[2])","for icpsr_member in content_parsed:
    (*icpsr_member_ricpsr_membermaining, icpsr_member_10, _, icpsr_member_2, *icpsr_member_ricpsr_membermaining_2) = icpsr_member
    if bioguide == icpsr_member_10:
        num_matches += 1
        write_id = int(icpsr_member_2)",Cannot refactor,Cannot refactor,2,0,0,"Answer: Yes
Iterable Unpacking: *e_remaining, e_10, _, e_2, *e_remaining_2 = e
variable mapping:
e_10: e[10]
e_2: e[2]",,,,,,,,it actually cannot refactor
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/core/chords.py,https://github.com/bspaans/python-mingus/tree/master/mingus/core/chords.py,,inversion_exhauster$1183,"def inversion_exhauster(
        chord,
        shorthand,
        tries,
        result,
        polychords,
    ):
        """"""Recursive helper function""""""

        # Determine polychords
        if tries == 1 and not no_polychords:
            polychords += determine_polychords(chord, shorthand)

        def add_result(short):
            result.append((short, tries, chord[0]))

        ch = determine_extended_chord5(chord[:5], True, True, True)
        intval5 = intervals.determine(chord[0], chord[5])
        for c in ch:
            c = c[len(chord[0]) :]
            if c == ""9"":
                if intval5 == ""perfect fourth"":
                    add_result(""11"")
                elif intval5 == ""augmented fourth"":
                    add_result(""7#11"")
                elif intval5 == ""major sixth"":
                    add_result(""13"")
            elif c == ""m9"":
                if intval5 == ""perfect fourth"":
                    add_result(""m11"")
                elif intval5 == ""major sixth"":
                    add_result(""m13"")
            elif c == ""M9"":
                if intval5 == ""perfect fourth"":
                    add_result(""M11"")
                elif intval5 == ""major sixth"":
                    add_result(""M13"")
        if tries != 6 and not no_inversions:
            return inversion_exhauster(
                [chord[-1]] + chord[:-1], shorthand, tries + 1, result, polychords
            )
        else:
            res = []
            for r in result:
                if shorthand:
                    res.append(r[2] + r[0])
                else:
                    res.append(r[2] + chord_shorthand_meaning[r[0]] + int_desc(r[1]))
            return res + polychords","for c in ch:
    c = c[len(chord[0]):]
    if c == '9':
        if intval5 == 'perfect fourth':
            add_result('11')
        elif intval5 == 'augmented fourth':
            add_result('7#11')
        elif intval5 == 'major sixth':
            add_result('13')
    elif c == 'm9':
        if intval5 == 'perfect fourth':
            add_result('m11')
        elif intval5 == 'major sixth':
            add_result('m13')
    elif c == 'M9':
        if intval5 == 'perfect fourth':
            add_result('M11')
        elif intval5 == 'major sixth':
            add_result('M13')",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, = e[len(ehord[0]):]
variable mapping:
e[len(ehord[0]):]: e[len(ehord[0]):]",,,,,,,,it actually cannot refactor
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/special_agents/agent_vsphere.py,https://github.com/tribe29/checkmk/tree/master/cmk/special_agents/agent_vsphere.py,,eval_snapshot_list$1689,"def eval_snapshot_list(info, _datastores):
    response = []
    snapshot_info = get_pattern(
        ""<name>(.*?)</name>.*?<id>(.*?)</id><createTime>(.*?)</createTime><state>(.*?)</state>"",
        info,
    )
    for entry in snapshot_info:
        try:
            # 2013-11-06T15:39:39.347543Z
            creation_time = int(time.mktime(time.strptime(entry[2][:19], ""%Y-%m-%dT%H:%M:%S"")))
        except ValueError:
            creation_time = 0
        response.append(
            ""%s %s %s %s"" % (entry[1], creation_time, entry[3], entry[0].replace(""|"", "" ""))
        )
    return ""|"".join(response)","for entry in snapshot_info:
    try:
        creation_time = int(time.mktime(time.strptime(entry[2][:19], '%Y-%m-%dT%H:%M:%S')))
    except ValueError:
        creation_time = 0
    response.append('%s %s %s %s' % (entry[1], creation_time, entry[3], entry[0].replace('|', ' ')))","for entry in snapshot_info:
    (entry_0, entry_1, (entry_2_0, *entry_2_rentrymaining), entry_3, *entry_rentrymaining) = entry
    try:
        creation_time = int(time.mktime(time.strptime(entry[2][:19], '%Y-%m-%dT%H:%M:%S')))
    except ValueError:
        creation_time = 0
    response.append('%s %s %s %s' % (entry_1, creation_time, entry_3, entry_0.replace('|', ' ')))","(data, data, data, data, (*data, *data), *data)","for (entry_0, entry_1, entry_2, entry_3, (*entry_2_0_1_19, *entry_2_0_1_len), *entry_len) in snapshot_info:
    try:
        creation_time = int(time.mktime(time.strptime(
        *entry_2_0_1_19, '%Y-%m-%dT%H:%M:%S')))
    except ValueError:
        creation_time = 0
    response.append('%s %s %s %s' % (
    entry_1, creation_time, 
    entry_3, 
    entry_0.replace('|', ' ')))",0,,,"Answer: Yes
Iterable Unpacking: e_0, e_1, (e_2_0, *e_2_remaining), e_3, *e_remaining = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2[:19]: e[2][:19]
e_3: e[3]",,,,,,,,
frigate,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frigate/frigate/edgetpu.py,https://github.com/blakeblackshear/frigate/tree/master/frigate/edgetpu.py,RemoteObjectDetector,detect$260,"def detect(self, tensor_input, threshold=0.4):
        detections = []

        # copy input to shared memory
        self.np_shm[:] = tensor_input[:]
        self.event.clear()
        self.detection_queue.put(self.name)
        result = self.event.wait(timeout=10.0)

        # if it timed out
        if result is None:
            return detections

        for d in self.out_np_shm:
            if d[1] < threshold:
                break
            detections.append(
                (self.labels[int(d[0])], float(d[1]), (d[2], d[3], d[4], d[5]))
            )
        self.fps.update()
        return detections","for d in self.out_np_shm:
    if d[1] < threshold:
        break
    detections.append((self.labels[int(d[0])], float(d[1]), (d[2], d[3], d[4], d[5])))","for d in self.out_np_shm:
    (d_1, d_2, d_3, d_4, d_5, *_) = d
    if d_1 < threshold:
        break
    detections.append((self.labels[int(d[0])], float(d_1), (d_2, d_3, d_4, d_5)))","(data, data, data, data, data, data, *data)","for (d_0, d_1, d_2, d_3, d_4, d_5, *d_len) in self.out_np_shm:
    if 
    d_1 < threshold:
        break
    detections.append((self.labels[int(
    d_0)], float(
    d_1), (
    d_2, 
    d_3, 
    d_4, 
    d_5)))",0,,,"Answer: Yes
Iterable Unpacking: e_1, e_2, e_3, e_4, e_5 = e
variable mapping:
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]
e_5: e[5]",,,,,,,,
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/common.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/common.py,,__get_loop_sequence_internal$824,"def __get_loop_sequence_internal(uv_layer, pairs, island_info, closed):
    loop_sequences = []
    for pair in pairs:
        seqs = [pair]
        p = pair
        isl_grp = __get_island_group_include_pair(pair, island_info)
        if isl_grp == -1:
            return None, ""Can not find the island or invalid island""

        while True:
            nlp = __get_next_loop_pair(p)
            if not nlp:
                break       # no more loop pair
            nlp_isl_grp = __get_island_group_include_pair(nlp, island_info)
            if nlp_isl_grp != isl_grp:
                break       # another island
            for nlpl in nlp:
                if nlpl[uv_layer].select:
                    return None, ""Do not select UV which does not belong to "" \
                                 ""the end edge""

            seqs.append(nlp)

            # when face is triangle, it indicates CLOSED
            if (len(nlp) == 1) and closed:
                break

            nplp = __get_next_poly_loop_pair(nlp)
            if not nplp:
                break       # no more loop pair
            nplp_isl_grp = __get_island_group_include_pair(nplp, island_info)
            if nplp_isl_grp != isl_grp:
                break       # another island

            # check if the UVs are already parsed.
            # this check is needed for the mesh which has the circular
            # sequence of the vertices
            matched = False
            for p1 in seqs:
                p2 = nplp
                if ((p1[0] == p2[0]) and (p1[1] == p2[1])) or \
                   ((p1[0] == p2[1]) and (p1[1] == p2[0])):
                    matched = True
            if matched:
                debug_print(""This is a circular sequence"")
                break

            for nlpl in nplp:
                if nlpl[uv_layer].select:
                    return None, ""Do not select UV which does not belong to "" \
                                 ""the end edge""

            seqs.append(nplp)

            p = nplp

        loop_sequences.append(seqs)
    return loop_sequences, """"","for nlpl in nlp:
    if nlpl[uv_layer].select:
        return (None, 'Do not select UV which does not belong to the end edge')",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: It depends on the value of ""uv_layer"" and the type of iterable object ""e"".
Iterable Unpacking: If the iterable object ""e"" is a sequence type like a list or tuple and ""uv_layer"" is an integer type constant, then the answer is Yes and the iterable unpacking code is:

e_uv_layer, *e_remaining = e
variable mapping:
e_uv_layer: e[uv_layer]

If the iterable object ""e"" is a dictionary type and ""uv_layer"" is a string type constant, then the answer is Yes and the iterable unpacking code is:

e_uv_layer = e.get(uv_layer)
variable mapping:
e_uv_layer: e[uv_layer]

Otherwise, if the iterable object ""e"" is not a sequence or dictionary type or ""uv_layer"" is not an integer or string type constant, then the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/common.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/common.py,,__get_loop_sequence_internal$824,"def __get_loop_sequence_internal(uv_layer, pairs, island_info, closed):
    loop_sequences = []
    for pair in pairs:
        seqs = [pair]
        p = pair
        isl_grp = __get_island_group_include_pair(pair, island_info)
        if isl_grp == -1:
            return None, ""Can not find the island or invalid island""

        while True:
            nlp = __get_next_loop_pair(p)
            if not nlp:
                break       # no more loop pair
            nlp_isl_grp = __get_island_group_include_pair(nlp, island_info)
            if nlp_isl_grp != isl_grp:
                break       # another island
            for nlpl in nlp:
                if nlpl[uv_layer].select:
                    return None, ""Do not select UV which does not belong to "" \
                                 ""the end edge""

            seqs.append(nlp)

            # when face is triangle, it indicates CLOSED
            if (len(nlp) == 1) and closed:
                break

            nplp = __get_next_poly_loop_pair(nlp)
            if not nplp:
                break       # no more loop pair
            nplp_isl_grp = __get_island_group_include_pair(nplp, island_info)
            if nplp_isl_grp != isl_grp:
                break       # another island

            # check if the UVs are already parsed.
            # this check is needed for the mesh which has the circular
            # sequence of the vertices
            matched = False
            for p1 in seqs:
                p2 = nplp
                if ((p1[0] == p2[0]) and (p1[1] == p2[1])) or \
                   ((p1[0] == p2[1]) and (p1[1] == p2[0])):
                    matched = True
            if matched:
                debug_print(""This is a circular sequence"")
                break

            for nlpl in nplp:
                if nlpl[uv_layer].select:
                    return None, ""Do not select UV which does not belong to "" \
                                 ""the end edge""

            seqs.append(nplp)

            p = nplp

        loop_sequences.append(seqs)
    return loop_sequences, """"","for nlpl in nplp:
    if nlpl[uv_layer].select:
        return (None, 'Do not select UV which does not belong to the end edge')",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: It depends on the value of ""uv_layer"" and the type of iterable object ""e"".
Iterable Unpacking: If the iterable object ""e"" is a sequence type like a list or tuple and ""uv_layer"" is an integer type constant, then the answer is Yes and the iterable unpacking code is:

e_uv_layer, *e_remaining = e
variable mapping:
e_uv_layer: e[uv_layer]

If the iterable object ""e"" is a dictionary and ""uv_layer"" is a string type constant, then the answer is Yes and the iterable unpacking code is:

e_uv_layer = e.get(uv_layer)
variable mapping:
e_uv_layer: e[uv_layer]

Otherwise, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
parliament,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/parliament/parliament/statement.py,https://github.com/duo-labs/parliament/tree/master/parliament/statement.py,Statement,_check_condition$496,"def _check_condition(self, operator, condition_block, expanded_actions):
        """"""
        operator is something like ""StringLike""
        condition_block is something like {""s3:prefix"":[""home/${aws:username}/*""]}
        """"""

        operator_type_requirement = None
        for documented_operator in OPERATORS:
            op = documented_operator.lower()
            if operator.lower() in [
                op,
                op + ""ifexists"",
                ""forallvalues:"" + op,
                ""foranyvalue:"" + op,
                ""forallvalues:"" + op + ""ifexists"",
                ""foranyvalue:"" + op + ""ifexists"",
            ]:
                operator_type_requirement = OPERATORS[documented_operator]
                break

        if operator_type_requirement is None:
            self.add_finding(
                ""UNKNOWN_OPERATOR"",
                detail=operator,
                location=condition_block,
            )

        if operator_type_requirement == ""Bool"":
            # Get the value that is being compared against
            for c in condition_block:
                value = str(c[1].value).lower()
                if value != ""true"" and value != ""false"":
                    self.add_finding(
                        ""MISMATCHED_TYPE_OPERATION_TO_NULL"", location=condition_block
                    )
                    return False

        for block in condition_block:
            key = block[0]
            values = []
            for v in make_list(block[1]):
                values.append(v.value)

            # Check for known bad pattern
            if operator.lower() == ""bool"":
                if (
                    key.lower() == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.',
                        location=condition_block,
                    )
            elif operator.lower() == ""null"":
                if (
                    key.lower == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.',
                        location=condition_block,
                    )

            if operator.lower() in [""null""]:
                # The following condition is valid:
                # ""Condition"": { ""Null"": { ""aws:MultiFactorAuthAge"": true }
                # If we check further we'll get a MISMATCHED_TYPE finding due to
                # aws:MultiFactorAuthAge being checked against a bool value instead of a date
                continue

            # The key here from the example is ""s3:prefix""
            condition_type = get_global_key_type(key)
            if condition_type:
                # This is a global key, like aws:CurrentTime
                # Check if the values match the type (ex. must all be Date values)
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding(
                        ""MISMATCHED_TYPE"",
                        detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                            key, condition_type, values
                        ),
                        location=condition_block,
                    )
            else:
                # See if this is a service specific key
                for action_struct in expanded_actions:
                    privilege_info = get_privilege_info(
                        action_struct[""service""], action_struct[""action""]
                    )

                    # Ensure the condition_key exists
                    match = None
                    for resource_type in privilege_info[""resource_types""]:
                        for condition_key in resource_type[""condition_keys""]:
                            if is_condition_key_match(condition_key, key):
                                match = condition_key

                    if match is None:
                        self.add_finding(
                            ""UNKNOWN_CONDITION_FOR_ACTION"",
                            detail=""Unknown condition {} for action {}:{}"".format(
                                key, action_struct[""service""], action_struct[""action""]
                            ),
                            location=condition_block,
                        )
                        continue

                    condition_type = None
                    for condition in privilege_info[""service_conditions""]:
                        if condition[""condition""] == match:
                            condition_type = condition[""type""]

                    if condition_type is None:
                        raise Exception(
                            ""Action condition not found in service definition for {}"".format(
                                match
                            )
                        )

                    if not is_value_in_correct_format_for_type(condition_type, values):
                        self.add_finding(
                            ""MISMATCHED_TYPE"",
                            detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                key, condition_type, values
                            ),
                            location=condition_block,
                        )

                if condition_type is not None:
                    # if operator_type_requirement.lower() == 'string' and condition_type.lower() = 'arn':
                    #     # Ignore these.
                    #     pass
                    documenation_condition_type = translate_documentation_types(
                        condition_type
                    )
                    if operator_type_requirement != documenation_condition_type:
                        if (
                            operator_type_requirement == ""String""
                            and documenation_condition_type == ""Arn""
                        ):
                            self.add_finding(
                                ""MISMATCHED_TYPE_BUT_USABLE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )
                        else:
                            self.add_finding(
                                ""MISMATCHED_TYPE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )

        return","for action_struct in expanded_actions:
    privilege_info = get_privilege_info(action_struct['service'], action_struct['action'])
    match = None
    for resource_type in privilege_info['resource_types']:
        for condition_key in resource_type['condition_keys']:
            if is_condition_key_match(condition_key, key):
                match = condition_key
    if match is None:
        self.add_finding('UNKNOWN_CONDITION_FOR_ACTION', detail='Unknown condition {} for action {}:{}'.format(key, action_struct['service'], action_struct['action']), location=condition_block)
        continue
    condition_type = None
    for condition in privilege_info['service_conditions']:
        if condition['condition'] == match:
            condition_type = condition['type']
    if condition_type is None:
        raise Exception('Action condition not found in service definition for {}'.format(match))
    if not is_value_in_correct_format_for_type(condition_type, values):
        self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_action, e_service = e['action'], e['service']
variable mapping:
e_action: e['action']
e_service: e['service']",,,,,,,,it actually cannot refactor
parliament,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/parliament/parliament/statement.py,https://github.com/duo-labs/parliament/tree/master/parliament/statement.py,Statement,_check_condition$496,"def _check_condition(self, operator, condition_block, expanded_actions):
        """"""
        operator is something like ""StringLike""
        condition_block is something like {""s3:prefix"":[""home/${aws:username}/*""]}
        """"""

        operator_type_requirement = None
        for documented_operator in OPERATORS:
            op = documented_operator.lower()
            if operator.lower() in [
                op,
                op + ""ifexists"",
                ""forallvalues:"" + op,
                ""foranyvalue:"" + op,
                ""forallvalues:"" + op + ""ifexists"",
                ""foranyvalue:"" + op + ""ifexists"",
            ]:
                operator_type_requirement = OPERATORS[documented_operator]
                break

        if operator_type_requirement is None:
            self.add_finding(
                ""UNKNOWN_OPERATOR"",
                detail=operator,
                location=condition_block,
            )

        if operator_type_requirement == ""Bool"":
            # Get the value that is being compared against
            for c in condition_block:
                value = str(c[1].value).lower()
                if value != ""true"" and value != ""false"":
                    self.add_finding(
                        ""MISMATCHED_TYPE_OPERATION_TO_NULL"", location=condition_block
                    )
                    return False

        for block in condition_block:
            key = block[0]
            values = []
            for v in make_list(block[1]):
                values.append(v.value)

            # Check for known bad pattern
            if operator.lower() == ""bool"":
                if (
                    key.lower() == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.',
                        location=condition_block,
                    )
            elif operator.lower() == ""null"":
                if (
                    key.lower == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.',
                        location=condition_block,
                    )

            if operator.lower() in [""null""]:
                # The following condition is valid:
                # ""Condition"": { ""Null"": { ""aws:MultiFactorAuthAge"": true }
                # If we check further we'll get a MISMATCHED_TYPE finding due to
                # aws:MultiFactorAuthAge being checked against a bool value instead of a date
                continue

            # The key here from the example is ""s3:prefix""
            condition_type = get_global_key_type(key)
            if condition_type:
                # This is a global key, like aws:CurrentTime
                # Check if the values match the type (ex. must all be Date values)
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding(
                        ""MISMATCHED_TYPE"",
                        detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                            key, condition_type, values
                        ),
                        location=condition_block,
                    )
            else:
                # See if this is a service specific key
                for action_struct in expanded_actions:
                    privilege_info = get_privilege_info(
                        action_struct[""service""], action_struct[""action""]
                    )

                    # Ensure the condition_key exists
                    match = None
                    for resource_type in privilege_info[""resource_types""]:
                        for condition_key in resource_type[""condition_keys""]:
                            if is_condition_key_match(condition_key, key):
                                match = condition_key

                    if match is None:
                        self.add_finding(
                            ""UNKNOWN_CONDITION_FOR_ACTION"",
                            detail=""Unknown condition {} for action {}:{}"".format(
                                key, action_struct[""service""], action_struct[""action""]
                            ),
                            location=condition_block,
                        )
                        continue

                    condition_type = None
                    for condition in privilege_info[""service_conditions""]:
                        if condition[""condition""] == match:
                            condition_type = condition[""type""]

                    if condition_type is None:
                        raise Exception(
                            ""Action condition not found in service definition for {}"".format(
                                match
                            )
                        )

                    if not is_value_in_correct_format_for_type(condition_type, values):
                        self.add_finding(
                            ""MISMATCHED_TYPE"",
                            detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                key, condition_type, values
                            ),
                            location=condition_block,
                        )

                if condition_type is not None:
                    # if operator_type_requirement.lower() == 'string' and condition_type.lower() = 'arn':
                    #     # Ignore these.
                    #     pass
                    documenation_condition_type = translate_documentation_types(
                        condition_type
                    )
                    if operator_type_requirement != documenation_condition_type:
                        if (
                            operator_type_requirement == ""String""
                            and documenation_condition_type == ""Arn""
                        ):
                            self.add_finding(
                                ""MISMATCHED_TYPE_BUT_USABLE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )
                        else:
                            self.add_finding(
                                ""MISMATCHED_TYPE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )

        return","for resource_type in privilege_info['resource_types']:
    for condition_key in resource_type['condition_keys']:
        if is_condition_key_match(condition_key, key):
            match = condition_key",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_condition_keys, *_ = e.values()
variable mapping:
e_condition_keys: e['condition_keys']",,,,,,,,it actually cannot refactor
parliament,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/parliament/parliament/statement.py,https://github.com/duo-labs/parliament/tree/master/parliament/statement.py,Statement,_check_condition$496,"def _check_condition(self, operator, condition_block, expanded_actions):
        """"""
        operator is something like ""StringLike""
        condition_block is something like {""s3:prefix"":[""home/${aws:username}/*""]}
        """"""

        operator_type_requirement = None
        for documented_operator in OPERATORS:
            op = documented_operator.lower()
            if operator.lower() in [
                op,
                op + ""ifexists"",
                ""forallvalues:"" + op,
                ""foranyvalue:"" + op,
                ""forallvalues:"" + op + ""ifexists"",
                ""foranyvalue:"" + op + ""ifexists"",
            ]:
                operator_type_requirement = OPERATORS[documented_operator]
                break

        if operator_type_requirement is None:
            self.add_finding(
                ""UNKNOWN_OPERATOR"",
                detail=operator,
                location=condition_block,
            )

        if operator_type_requirement == ""Bool"":
            # Get the value that is being compared against
            for c in condition_block:
                value = str(c[1].value).lower()
                if value != ""true"" and value != ""false"":
                    self.add_finding(
                        ""MISMATCHED_TYPE_OPERATION_TO_NULL"", location=condition_block
                    )
                    return False

        for block in condition_block:
            key = block[0]
            values = []
            for v in make_list(block[1]):
                values.append(v.value)

            # Check for known bad pattern
            if operator.lower() == ""bool"":
                if (
                    key.lower() == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.',
                        location=condition_block,
                    )
            elif operator.lower() == ""null"":
                if (
                    key.lower == ""aws:MultiFactorAuthPresent"".lower()
                    and ""false"" in values
                ):
                    self.add_finding(
                        ""BAD_PATTERN_FOR_MFA"",
                        detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.',
                        location=condition_block,
                    )

            if operator.lower() in [""null""]:
                # The following condition is valid:
                # ""Condition"": { ""Null"": { ""aws:MultiFactorAuthAge"": true }
                # If we check further we'll get a MISMATCHED_TYPE finding due to
                # aws:MultiFactorAuthAge being checked against a bool value instead of a date
                continue

            # The key here from the example is ""s3:prefix""
            condition_type = get_global_key_type(key)
            if condition_type:
                # This is a global key, like aws:CurrentTime
                # Check if the values match the type (ex. must all be Date values)
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding(
                        ""MISMATCHED_TYPE"",
                        detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                            key, condition_type, values
                        ),
                        location=condition_block,
                    )
            else:
                # See if this is a service specific key
                for action_struct in expanded_actions:
                    privilege_info = get_privilege_info(
                        action_struct[""service""], action_struct[""action""]
                    )

                    # Ensure the condition_key exists
                    match = None
                    for resource_type in privilege_info[""resource_types""]:
                        for condition_key in resource_type[""condition_keys""]:
                            if is_condition_key_match(condition_key, key):
                                match = condition_key

                    if match is None:
                        self.add_finding(
                            ""UNKNOWN_CONDITION_FOR_ACTION"",
                            detail=""Unknown condition {} for action {}:{}"".format(
                                key, action_struct[""service""], action_struct[""action""]
                            ),
                            location=condition_block,
                        )
                        continue

                    condition_type = None
                    for condition in privilege_info[""service_conditions""]:
                        if condition[""condition""] == match:
                            condition_type = condition[""type""]

                    if condition_type is None:
                        raise Exception(
                            ""Action condition not found in service definition for {}"".format(
                                match
                            )
                        )

                    if not is_value_in_correct_format_for_type(condition_type, values):
                        self.add_finding(
                            ""MISMATCHED_TYPE"",
                            detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                key, condition_type, values
                            ),
                            location=condition_block,
                        )

                if condition_type is not None:
                    # if operator_type_requirement.lower() == 'string' and condition_type.lower() = 'arn':
                    #     # Ignore these.
                    #     pass
                    documenation_condition_type = translate_documentation_types(
                        condition_type
                    )
                    if operator_type_requirement != documenation_condition_type:
                        if (
                            operator_type_requirement == ""String""
                            and documenation_condition_type == ""Arn""
                        ):
                            self.add_finding(
                                ""MISMATCHED_TYPE_BUT_USABLE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )
                        else:
                            self.add_finding(
                                ""MISMATCHED_TYPE"",
                                detail=""Type mismatch: {} requires a value of type {} but given {}"".format(
                                    operator,
                                    operator_type_requirement,
                                    translate_documentation_types(condition_type),
                                ),
                                location=condition_block,
                            )

        return","for condition in privilege_info['service_conditions']:
    if condition['condition'] == match:
        condition_type = condition['type']",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_e, e_type = e['e'], e['type']
variable mapping:
e_e: e['e']
e_type: e['type']",,,,,,,,it actually cannot refactor
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/thread_modules.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/thread_modules.py,FindPosterThread,run$393,"def run(self):
        name = self.name
        url = self.url
        direct_url = self.direct_url
        #print(name, url, direct_url, '--poster--thread--')
        fanart = os.path.join(TMPDIR, name+'-fanart.jpg')
        thumb = os.path.join(TMPDIR, name+'.jpg')
        fan_text = os.path.join(TMPDIR, name+'-fanart.txt')
        post_text = os.path.join(TMPDIR, name+'-poster.txt')
        logger.info(fanart)
        logger.info(thumb)
        final_link = """"
        m = []

        if site == 'Music':
            final = ''
            if (self.copy_fanart and self.copy_poster and self.copy_summary):
                if not direct_url and not url:
                    nam = ui.metaengine.name_adjust(name)
                    url = ""http://www.last.fm/search?q=""+nam
                    logger.info(url)
                wiki = """"
                content = ccurl(url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('div', {'class':'row clearfix'})
                name3 = """"
                for i in link:
                    j = i.findAll('a')
                    for k in j:
                        try:
                            url = k['href']
                            if '?q=' not in url:
                                logger.info(url)
                                break
                        except Exception as err:
                            print(err, '--108--')
                logger.info(url)
                if url.startswith('http'):
                    url = url
                else:
                    url = ""http://www.last.fm"" + url
                logger.info(url)
                img_url = url+'/+images'
                wiki_url = url + '/+wiki'
                logger.info(wiki_url)
                content = ccurl(wiki_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.find('div', {'class':'wiki-content'})
                if link:
                    wiki = link.text
                    self.summary_signal.emit(name, wiki, 'summary')
                content = ccurl(img_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('ul', {'class':'image-list'})
                img = []
                for i in link:
                    j = i.findAll('img')
                    for k in j:
                        l = k['src']
                        u1 = l.rsplit('/', 2)[0]
                        u2 = l.split('/')[-1]
                        u = u1 + '/770x0/'+u2
                        img.append(u)
                img = list(set(img))
                logger.info(len(img))
                thumb = os.path.join(TMPDIR, name+'.jpg')
                if img:
                    url = img[0]
                    try:
                        ccurl(url, curl_opt='-o', out_file=thumb)
                    except Exception as err:
                        print(err, '--151--')
            elif (self.copy_poster or self.copy_fanart) and url and direct_url:
                if 'last.fm' in url:
                    logger.info('--artist-link---{0}'.format(url))
                    content = ccurl(url)
                    soup = BeautifulSoup(content, 'lxml')
                    link = soup.findAll('img')
                    url1Code = url.split('/')[-1]
                    found = None
                    for i in link:
                        if 'src' in str(i):
                            j = i['src']
                            k = j.split('/')[-1]
                            if url1Code == k:
                                found = j
                                break
                    logger.info(str(found))
                    if found:
                        u1 = found.rsplit('/', 2)[0]
                        u2 = found.split('/')[-1]
                        final = u1 + '/770x0/'+u2
                        logger.info(final)
                elif ("".jpg"" in url or "".png"" in url) and url.startswith('http'):
                    final = url
                else:
                    final = ''
                try:
                    if final.startswith('http'):
                        ccurl(final, curl_opt='-o', out_file=thumb)
                except Exception as e:
                    print(e)
        else:
            nam = ui.metaengine.name_adjust(name)
            src_site = 'tvdb'
            epn_arr = []
            post_val = ''
            fan_val = ''
            logger.debug('\nvideo_dir={0}\n'.format(self.video_dir))
            if site.lower() == 'video' and self.video_dir:
                 video_db = os.path.join(ui.home_folder, 'VideoDB', 'Video.db')
                 if os.path.exists(video_db):
                    epn_arr_tmp = ui.media_data.get_video_db(video_db, ""Directory"", self.video_dir)
                    for i in epn_arr_tmp:
                        epn_name = i[0]+'	'+i[1]
                        logger.debug(epn_name)
                        epn_arr.append(epn_name)
            elif self.video_dir:
                new_name_with_info = self.video_dir.strip()
                extra_info = ''
                if '	' in new_name_with_info:
                    name_title = new_name_with_info.split('	')[0]
                    extra_info = new_name_with_info.split('	')[1]
                else:
                    name_title = new_name_with_info
                
                if site.lower() == 'subbedanime' or site.lower() == 'dubbedanime' and siteName:
                    hist_site = os.path.join(ui.home_folder, 'History', site, siteName, name_title)
                else:
                    hist_site = os.path.join(ui.home_folder, 'History', site, name_title)
                    
                hist_epn = os.path.join(hist_site, 'Ep.txt')
                logger.info(hist_epn)
                if os.path.exists(hist_epn):
                    lines = open_files(hist_epn, True)
                    for i in lines:
                        i = i.strip()
                        j = i.split('	')
                        if len(j) == 1:
                            epn_arr.append(i+'	'+i+'	'+name)
                        elif len(j) >= 2:
                            epn_arr.append(i+'	'+name)
                            
            if ui.series_info_dict.get(name) and not epn_arr:
                logger.debug('getting values from cache')
                dict_val = ui.series_info_dict.get(name)
                post_arr = dict_val.get('poster')
                fan_arr = dict_val.get('fanart')
                fan_index = dict_val.get('f')
                post_index = dict_val.get('p')
                if fan_index < len(fan_arr):
                    fan_val = fan_arr[fan_index]
                    fan_index = (fan_index + 1) % len(fan_arr)
                    dict_val.update({'f':fan_index})
                if post_index < len(post_arr):
                    post_val = post_arr[post_index]
                    post_index = (post_index + 1) % len(post_arr)
                    dict_val.update({'p':post_index})
                ui.series_info_dict.update({name:dict_val})
                if isinstance(self.use_search, bool):
                    src_site = 'tvdb'
                else:
                    src_site = self.use_search
            else:
                m, final_link, src_site = self.init_search(
                    nam, url, direct_url, thumb, fanart, src_site
                    )
            if (m and src_site in ['tvdb', 'tvdb+g', 'tvdb+ddg']) or post_val or fan_val:
                if post_val or fan_val:
                    if post_val:
                        url = ""http://thetvdb.com/"" + post_val
                        ccurl(url+'#'+'-o'+'#'+thumb)
                    if fan_val:
                        url = ""http://thetvdb.com/"" + fan_val
                        ccurl(url+'#'+'-o'+'#'+fanart)
                else:
                    if not final_link:
                        n = re.sub('amp;', '', m[0])
                        elist = re.sub('tab=series', 'tab=seasonall', n)
                        url = ""http://thetvdb.com"" + n
                        logger.info(url)
                        elist_url = ""http://thetvdb.com"" + elist
                    else:
                        url = final_link
                    post_arr, fan_arr = self.parse_tvdb(name, url)
                    if post_arr:
                        url = ""http://thetvdb.com/"" + post_arr[0]
                        ccurl(url+'#'+'-o'+'#'+thumb)
                        logger.info(post_arr)
                    if fan_arr:
                        #if ui.player_theme != 'default':
                        fan_arr = [i for i in fan_arr if 'vignette' not in i]
                        if fan_arr:
                            url = ""http://thetvdb.com/"" + fan_arr[0]
                            ccurl(url+'#'+'-o'+'#'+fanart)
                        logger.debug(fan_arr)
                    fan_arr.sort()
                    post_arr.sort()
                    ui.series_info_dict.update(
                            {
                            name:{
                                'fanart':fan_arr.copy(), 'poster':post_arr.copy(),
                                'f':0, 'p':0
                                }
                            }
                        )
                    elist_url = re.sub('tab=series', 'tab=seasonall', final_link)
                    if epn_arr:
                        ui.metaengine.getTvdbEpnInfo(
                            elist_url, epn_arr=epn_arr.copy(), site=site,
                            name=name, thread=self, video_dir=self.video_dir
                            )
                        image_dict = self.image_dict_list.copy()
                        dest_dir = self.dest_dir
                        self.imagesignal.emit(image_dict, dest_dir, site)
            elif m and src_site in ['tmdb', 'tmdb+g', 'tmdb+ddg']:
                self.parse_tmdb(name, final_link, thumb, fanart)","for k in j:
    try:
        url = k['href']
        if '?q=' not in url:
            logger.info(url)
            break
    except Exception as err:
        print(err, '--108--')",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['href'] is a dictionary value that can be accessed directly using the key 'href'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/thread_modules.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/thread_modules.py,FindPosterThread,run$393,"def run(self):
        name = self.name
        url = self.url
        direct_url = self.direct_url
        #print(name, url, direct_url, '--poster--thread--')
        fanart = os.path.join(TMPDIR, name+'-fanart.jpg')
        thumb = os.path.join(TMPDIR, name+'.jpg')
        fan_text = os.path.join(TMPDIR, name+'-fanart.txt')
        post_text = os.path.join(TMPDIR, name+'-poster.txt')
        logger.info(fanart)
        logger.info(thumb)
        final_link = """"
        m = []

        if site == 'Music':
            final = ''
            if (self.copy_fanart and self.copy_poster and self.copy_summary):
                if not direct_url and not url:
                    nam = ui.metaengine.name_adjust(name)
                    url = ""http://www.last.fm/search?q=""+nam
                    logger.info(url)
                wiki = """"
                content = ccurl(url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('div', {'class':'row clearfix'})
                name3 = """"
                for i in link:
                    j = i.findAll('a')
                    for k in j:
                        try:
                            url = k['href']
                            if '?q=' not in url:
                                logger.info(url)
                                break
                        except Exception as err:
                            print(err, '--108--')
                logger.info(url)
                if url.startswith('http'):
                    url = url
                else:
                    url = ""http://www.last.fm"" + url
                logger.info(url)
                img_url = url+'/+images'
                wiki_url = url + '/+wiki'
                logger.info(wiki_url)
                content = ccurl(wiki_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.find('div', {'class':'wiki-content'})
                if link:
                    wiki = link.text
                    self.summary_signal.emit(name, wiki, 'summary')
                content = ccurl(img_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('ul', {'class':'image-list'})
                img = []
                for i in link:
                    j = i.findAll('img')
                    for k in j:
                        l = k['src']
                        u1 = l.rsplit('/', 2)[0]
                        u2 = l.split('/')[-1]
                        u = u1 + '/770x0/'+u2
                        img.append(u)
                img = list(set(img))
                logger.info(len(img))
                thumb = os.path.join(TMPDIR, name+'.jpg')
                if img:
                    url = img[0]
                    try:
                        ccurl(url, curl_opt='-o', out_file=thumb)
                    except Exception as err:
                        print(err, '--151--')
            elif (self.copy_poster or self.copy_fanart) and url and direct_url:
                if 'last.fm' in url:
                    logger.info('--artist-link---{0}'.format(url))
                    content = ccurl(url)
                    soup = BeautifulSoup(content, 'lxml')
                    link = soup.findAll('img')
                    url1Code = url.split('/')[-1]
                    found = None
                    for i in link:
                        if 'src' in str(i):
                            j = i['src']
                            k = j.split('/')[-1]
                            if url1Code == k:
                                found = j
                                break
                    logger.info(str(found))
                    if found:
                        u1 = found.rsplit('/', 2)[0]
                        u2 = found.split('/')[-1]
                        final = u1 + '/770x0/'+u2
                        logger.info(final)
                elif ("".jpg"" in url or "".png"" in url) and url.startswith('http'):
                    final = url
                else:
                    final = ''
                try:
                    if final.startswith('http'):
                        ccurl(final, curl_opt='-o', out_file=thumb)
                except Exception as e:
                    print(e)
        else:
            nam = ui.metaengine.name_adjust(name)
            src_site = 'tvdb'
            epn_arr = []
            post_val = ''
            fan_val = ''
            logger.debug('\nvideo_dir={0}\n'.format(self.video_dir))
            if site.lower() == 'video' and self.video_dir:
                 video_db = os.path.join(ui.home_folder, 'VideoDB', 'Video.db')
                 if os.path.exists(video_db):
                    epn_arr_tmp = ui.media_data.get_video_db(video_db, ""Directory"", self.video_dir)
                    for i in epn_arr_tmp:
                        epn_name = i[0]+'	'+i[1]
                        logger.debug(epn_name)
                        epn_arr.append(epn_name)
            elif self.video_dir:
                new_name_with_info = self.video_dir.strip()
                extra_info = ''
                if '	' in new_name_with_info:
                    name_title = new_name_with_info.split('	')[0]
                    extra_info = new_name_with_info.split('	')[1]
                else:
                    name_title = new_name_with_info
                
                if site.lower() == 'subbedanime' or site.lower() == 'dubbedanime' and siteName:
                    hist_site = os.path.join(ui.home_folder, 'History', site, siteName, name_title)
                else:
                    hist_site = os.path.join(ui.home_folder, 'History', site, name_title)
                    
                hist_epn = os.path.join(hist_site, 'Ep.txt')
                logger.info(hist_epn)
                if os.path.exists(hist_epn):
                    lines = open_files(hist_epn, True)
                    for i in lines:
                        i = i.strip()
                        j = i.split('	')
                        if len(j) == 1:
                            epn_arr.append(i+'	'+i+'	'+name)
                        elif len(j) >= 2:
                            epn_arr.append(i+'	'+name)
                            
            if ui.series_info_dict.get(name) and not epn_arr:
                logger.debug('getting values from cache')
                dict_val = ui.series_info_dict.get(name)
                post_arr = dict_val.get('poster')
                fan_arr = dict_val.get('fanart')
                fan_index = dict_val.get('f')
                post_index = dict_val.get('p')
                if fan_index < len(fan_arr):
                    fan_val = fan_arr[fan_index]
                    fan_index = (fan_index + 1) % len(fan_arr)
                    dict_val.update({'f':fan_index})
                if post_index < len(post_arr):
                    post_val = post_arr[post_index]
                    post_index = (post_index + 1) % len(post_arr)
                    dict_val.update({'p':post_index})
                ui.series_info_dict.update({name:dict_val})
                if isinstance(self.use_search, bool):
                    src_site = 'tvdb'
                else:
                    src_site = self.use_search
            else:
                m, final_link, src_site = self.init_search(
                    nam, url, direct_url, thumb, fanart, src_site
                    )
            if (m and src_site in ['tvdb', 'tvdb+g', 'tvdb+ddg']) or post_val or fan_val:
                if post_val or fan_val:
                    if post_val:
                        url = ""http://thetvdb.com/"" + post_val
                        ccurl(url+'#'+'-o'+'#'+thumb)
                    if fan_val:
                        url = ""http://thetvdb.com/"" + fan_val
                        ccurl(url+'#'+'-o'+'#'+fanart)
                else:
                    if not final_link:
                        n = re.sub('amp;', '', m[0])
                        elist = re.sub('tab=series', 'tab=seasonall', n)
                        url = ""http://thetvdb.com"" + n
                        logger.info(url)
                        elist_url = ""http://thetvdb.com"" + elist
                    else:
                        url = final_link
                    post_arr, fan_arr = self.parse_tvdb(name, url)
                    if post_arr:
                        url = ""http://thetvdb.com/"" + post_arr[0]
                        ccurl(url+'#'+'-o'+'#'+thumb)
                        logger.info(post_arr)
                    if fan_arr:
                        #if ui.player_theme != 'default':
                        fan_arr = [i for i in fan_arr if 'vignette' not in i]
                        if fan_arr:
                            url = ""http://thetvdb.com/"" + fan_arr[0]
                            ccurl(url+'#'+'-o'+'#'+fanart)
                        logger.debug(fan_arr)
                    fan_arr.sort()
                    post_arr.sort()
                    ui.series_info_dict.update(
                            {
                            name:{
                                'fanart':fan_arr.copy(), 'poster':post_arr.copy(),
                                'f':0, 'p':0
                                }
                            }
                        )
                    elist_url = re.sub('tab=series', 'tab=seasonall', final_link)
                    if epn_arr:
                        ui.metaengine.getTvdbEpnInfo(
                            elist_url, epn_arr=epn_arr.copy(), site=site,
                            name=name, thread=self, video_dir=self.video_dir
                            )
                        image_dict = self.image_dict_list.copy()
                        dest_dir = self.dest_dir
                        self.imagesignal.emit(image_dict, dest_dir, site)
            elif m and src_site in ['tmdb', 'tmdb+g', 'tmdb+ddg']:
                self.parse_tmdb(name, final_link, thumb, fanart)","for k in j:
    l = k['src']
    u1 = l.rsplit('/', 2)[0]
    u2 = l.split('/')[-1]
    u = u1 + '/770x0/' + u2
    img.append(u)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: The given unpacked element e['src'] is a dictionary value that can be accessed directly using the key 'src'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/thread_modules.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/thread_modules.py,FindPosterThread,run$393,"def run(self):
        name = self.name
        url = self.url
        direct_url = self.direct_url
        #print(name, url, direct_url, '--poster--thread--')
        fanart = os.path.join(TMPDIR, name+'-fanart.jpg')
        thumb = os.path.join(TMPDIR, name+'.jpg')
        fan_text = os.path.join(TMPDIR, name+'-fanart.txt')
        post_text = os.path.join(TMPDIR, name+'-poster.txt')
        logger.info(fanart)
        logger.info(thumb)
        final_link = """"
        m = []

        if site == 'Music':
            final = ''
            if (self.copy_fanart and self.copy_poster and self.copy_summary):
                if not direct_url and not url:
                    nam = ui.metaengine.name_adjust(name)
                    url = ""http://www.last.fm/search?q=""+nam
                    logger.info(url)
                wiki = """"
                content = ccurl(url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('div', {'class':'row clearfix'})
                name3 = """"
                for i in link:
                    j = i.findAll('a')
                    for k in j:
                        try:
                            url = k['href']
                            if '?q=' not in url:
                                logger.info(url)
                                break
                        except Exception as err:
                            print(err, '--108--')
                logger.info(url)
                if url.startswith('http'):
                    url = url
                else:
                    url = ""http://www.last.fm"" + url
                logger.info(url)
                img_url = url+'/+images'
                wiki_url = url + '/+wiki'
                logger.info(wiki_url)
                content = ccurl(wiki_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.find('div', {'class':'wiki-content'})
                if link:
                    wiki = link.text
                    self.summary_signal.emit(name, wiki, 'summary')
                content = ccurl(img_url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('ul', {'class':'image-list'})
                img = []
                for i in link:
                    j = i.findAll('img')
                    for k in j:
                        l = k['src']
                        u1 = l.rsplit('/', 2)[0]
                        u2 = l.split('/')[-1]
                        u = u1 + '/770x0/'+u2
                        img.append(u)
                img = list(set(img))
                logger.info(len(img))
                thumb = os.path.join(TMPDIR, name+'.jpg')
                if img:
                    url = img[0]
                    try:
                        ccurl(url, curl_opt='-o', out_file=thumb)
                    except Exception as err:
                        print(err, '--151--')
            elif (self.copy_poster or self.copy_fanart) and url and direct_url:
                if 'last.fm' in url:
                    logger.info('--artist-link---{0}'.format(url))
                    content = ccurl(url)
                    soup = BeautifulSoup(content, 'lxml')
                    link = soup.findAll('img')
                    url1Code = url.split('/')[-1]
                    found = None
                    for i in link:
                        if 'src' in str(i):
                            j = i['src']
                            k = j.split('/')[-1]
                            if url1Code == k:
                                found = j
                                break
                    logger.info(str(found))
                    if found:
                        u1 = found.rsplit('/', 2)[0]
                        u2 = found.split('/')[-1]
                        final = u1 + '/770x0/'+u2
                        logger.info(final)
                elif ("".jpg"" in url or "".png"" in url) and url.startswith('http'):
                    final = url
                else:
                    final = ''
                try:
                    if final.startswith('http'):
                        ccurl(final, curl_opt='-o', out_file=thumb)
                except Exception as e:
                    print(e)
        else:
            nam = ui.metaengine.name_adjust(name)
            src_site = 'tvdb'
            epn_arr = []
            post_val = ''
            fan_val = ''
            logger.debug('\nvideo_dir={0}\n'.format(self.video_dir))
            if site.lower() == 'video' and self.video_dir:
                 video_db = os.path.join(ui.home_folder, 'VideoDB', 'Video.db')
                 if os.path.exists(video_db):
                    epn_arr_tmp = ui.media_data.get_video_db(video_db, ""Directory"", self.video_dir)
                    for i in epn_arr_tmp:
                        epn_name = i[0]+'	'+i[1]
                        logger.debug(epn_name)
                        epn_arr.append(epn_name)
            elif self.video_dir:
                new_name_with_info = self.video_dir.strip()
                extra_info = ''
                if '	' in new_name_with_info:
                    name_title = new_name_with_info.split('	')[0]
                    extra_info = new_name_with_info.split('	')[1]
                else:
                    name_title = new_name_with_info
                
                if site.lower() == 'subbedanime' or site.lower() == 'dubbedanime' and siteName:
                    hist_site = os.path.join(ui.home_folder, 'History', site, siteName, name_title)
                else:
                    hist_site = os.path.join(ui.home_folder, 'History', site, name_title)
                    
                hist_epn = os.path.join(hist_site, 'Ep.txt')
                logger.info(hist_epn)
                if os.path.exists(hist_epn):
                    lines = open_files(hist_epn, True)
                    for i in lines:
                        i = i.strip()
                        j = i.split('	')
                        if len(j) == 1:
                            epn_arr.append(i+'	'+i+'	'+name)
                        elif len(j) >= 2:
                            epn_arr.append(i+'	'+name)
                            
            if ui.series_info_dict.get(name) and not epn_arr:
                logger.debug('getting values from cache')
                dict_val = ui.series_info_dict.get(name)
                post_arr = dict_val.get('poster')
                fan_arr = dict_val.get('fanart')
                fan_index = dict_val.get('f')
                post_index = dict_val.get('p')
                if fan_index < len(fan_arr):
                    fan_val = fan_arr[fan_index]
                    fan_index = (fan_index + 1) % len(fan_arr)
                    dict_val.update({'f':fan_index})
                if post_index < len(post_arr):
                    post_val = post_arr[post_index]
                    post_index = (post_index + 1) % len(post_arr)
                    dict_val.update({'p':post_index})
                ui.series_info_dict.update({name:dict_val})
                if isinstance(self.use_search, bool):
                    src_site = 'tvdb'
                else:
                    src_site = self.use_search
            else:
                m, final_link, src_site = self.init_search(
                    nam, url, direct_url, thumb, fanart, src_site
                    )
            if (m and src_site in ['tvdb', 'tvdb+g', 'tvdb+ddg']) or post_val or fan_val:
                if post_val or fan_val:
                    if post_val:
                        url = ""http://thetvdb.com/"" + post_val
                        ccurl(url+'#'+'-o'+'#'+thumb)
                    if fan_val:
                        url = ""http://thetvdb.com/"" + fan_val
                        ccurl(url+'#'+'-o'+'#'+fanart)
                else:
                    if not final_link:
                        n = re.sub('amp;', '', m[0])
                        elist = re.sub('tab=series', 'tab=seasonall', n)
                        url = ""http://thetvdb.com"" + n
                        logger.info(url)
                        elist_url = ""http://thetvdb.com"" + elist
                    else:
                        url = final_link
                    post_arr, fan_arr = self.parse_tvdb(name, url)
                    if post_arr:
                        url = ""http://thetvdb.com/"" + post_arr[0]
                        ccurl(url+'#'+'-o'+'#'+thumb)
                        logger.info(post_arr)
                    if fan_arr:
                        #if ui.player_theme != 'default':
                        fan_arr = [i for i in fan_arr if 'vignette' not in i]
                        if fan_arr:
                            url = ""http://thetvdb.com/"" + fan_arr[0]
                            ccurl(url+'#'+'-o'+'#'+fanart)
                        logger.debug(fan_arr)
                    fan_arr.sort()
                    post_arr.sort()
                    ui.series_info_dict.update(
                            {
                            name:{
                                'fanart':fan_arr.copy(), 'poster':post_arr.copy(),
                                'f':0, 'p':0
                                }
                            }
                        )
                    elist_url = re.sub('tab=series', 'tab=seasonall', final_link)
                    if epn_arr:
                        ui.metaengine.getTvdbEpnInfo(
                            elist_url, epn_arr=epn_arr.copy(), site=site,
                            name=name, thread=self, video_dir=self.video_dir
                            )
                        image_dict = self.image_dict_list.copy()
                        dest_dir = self.dest_dir
                        self.imagesignal.emit(image_dict, dest_dir, site)
            elif m and src_site in ['tmdb', 'tmdb+g', 'tmdb+ddg']:
                self.parse_tmdb(name, final_link, thumb, fanart)","for i in link:
    if 'src' in str(i):
        j = i['src']
        k = j.split('/')[-1]
        if url1Code == k:
            found = j
            break",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['src'] is a dictionary value that can be accessed directly using the key 'src'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
gif-for-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gif-for-cli/tests/test_display.py,https://github.com/google/gif-for-cli/tree/master/tests/test_display.py,TestDisplayTxtFrames,test_0_loops$58,"def test_0_loops(self):
        stdout = io.StringIO()

        txt_frames = self.txt_frames
        num_loops = 0
        error_after_num_loops = 5
        error_after_num_sleep_calls = error_after_num_loops * len(txt_frames)

        with patch('time.sleep') as mock_sleep:
            num_sleep_calls = 0

            def sleep_side_effect(s):
                nonlocal num_sleep_calls
                num_sleep_calls += 1
                if num_sleep_calls >= error_after_num_sleep_calls:
                    raise KeyboardInterrupt()
                return
            mock_sleep.side_effect = sleep_side_effect

            display_txt_frames(txt_frames, stdout, num_loops, self.seconds_per_frame)

        self.assertEqual(mock_sleep.call_count, error_after_num_loops * len(txt_frames))
        for call in mock_sleep.call_args_list:
            self.assertEqual(call[0][0], self.seconds_per_frame)

        output_ending = '\n' + ANSI_RESET + '\n'
        output = stdout.getvalue()

        self.assertEqual(output[-len(output_ending):], output_ending)

        output = output[:-len(output_ending)]
        output = output.split('\n' + (ANSI_CURSOR_UP * self.height))

        self.assertEqual(output, self.txt_frames * error_after_num_loops)","for call in mock_sleep.call_args_list:
    self.assertEqual(call[0][0], self.seconds_per_frame)","for call in mock_sleep.call_args_list:
    ((call_0_0, _, *call_0_rcallmaining), *call_rcallmaining) = call
    self.assertEqual(call_0_0, self.seconds_per_frame)","((data, *data), *data)","for ((call_0_0, *call_0_len), *call_len) in mock_sleep.call_args_list:
    self.assertEqual(
    call_0_0, self.seconds_per_frame)",0,,,"Answer: Yes
Iterable Unpacking: (e_0_0, _, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]",,,,,,,,
sparrow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparrow/views/report.py,https://github.com/wylok/sparrow/tree/master/views/report.py,,work_order_report$287,"def work_order_report(start_time=None,end_time=None,source='all_order'):
    INFOS = []
    db_sso = db_op.user_sso
    db_work_order = db_op.work_order
    dm_key = 'op_work_order_report_dm'
    stat_key = 'op_work_order_report_status'
    dep_key = 'op_work_order_report_department'
    if not start_time or not end_time:
        tm = datetime.datetime.now() - datetime.timedelta(days=7)
        start_time = tm.strftime('%Y-%m-%d')
        end_time = time.strftime('%Y-%m-%d',time.localtime())
    try:
        infos = db_sso.query.with_entities(db_sso.dingunionid, db_sso.department,db_sso.realName).all()
        departments = {info[0]: info[1] for info in infos}
        users = {info[0]: info[-1] for info in infos}
        #缁熻¤繍缁村伐鍗曠姸鎬
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.status,func.count(db_work_order.status)).filter(and_(
                db_work_order.source == source,db_work_order.date >=start_time,db_work_order.date<=end_time)).group_by(db_work_order.status).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.status,func.count(db_work_order.status)).filter(and_(
                    db_work_order.date >=start_time,db_work_order.date<=end_time)).group_by(db_work_order.status).all()
            pie = Pie(""杩愮淮宸ュ崟鐘舵佺粺璁"", width='100%', height='100%', title_pos='center', title_text_size=14)
            pie_vals = [val[0] for val in vals]
            pie_counts = [int(val[1]) for val in vals]
            pie.add("""", pie_vals, pie_counts, is_label_show=True, is_toolbox_show=False, legend_orient='vertical',
                           legend_pos='right',radius=[1, 65], is_random=True)
            INFOS.append(pie)
        except Exception as e:
            logging.error(e)
        #缁熻℃湀搴﹀伐鍗曟暟閲忓強鍙楃悊鐜
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.date,db_work_order.status).filter(and_(
                db_work_order.source == source,db_work_order.date >=start_time,db_work_order.date<=end_time)).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.date, db_work_order.status).filter(and_(
                    db_work_order.date >= start_time,db_work_order.date <= end_time)).all()
            if vals:
                for val in vals:
                    dm,status = val
                    dm = dm.split('-')[1]
                    RC.hincrby(dm_key,dm)
                    if status not in ('鏈鍙楃悊', '鏈瀹℃牳'):
                        RC.hincrby(stat_key,dm)
            line = Line(""鏈堝害宸ュ崟鏁伴噺鍙婂彈鐞嗙巼缁熻"", width='100%', height='100%', title_pos='center',title_text_size=14)
            total_vals = RC.hgetall(dm_key)
            vals = sorted(total_vals.items(), key=lambda item: int(item[0]))
            dm_vals = [val[0] for val in vals]
            dm_counts = [int(val[1]) for val in vals]
            line.add('宸ュ崟鏁伴噺', dm_vals, dm_counts, is_label_show=True, is_toolbox_show=False,
                         legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            stat_vals = RC.hgetall(stat_key)
            stat_counts = [round((float(stat_vals[val])/float(total_vals[val]))*100,1) for val in stat_vals]
            line.add('鍙楃悊鐜', dm_vals, stat_counts, is_label_show=True, is_toolbox_show=False,
                         legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            RC.delete(stat_key)
            RC.delete(dm_key)
            INFOS.append(line)
        except Exception as e:
            logging.error(e)
        #宸ュ崟鐢宠锋暟閲忛儴闂ㄦ帓鍚
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.applicant).filter(and_(
                db_work_order.source == source,db_work_order.date >=start_time,db_work_order.date<=end_time)).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.applicant).filter(and_(
                db_work_order.date >=start_time,db_work_order.date<=end_time)).all()
            if vals:
                for val in vals:
                    RC.hincrby(dep_key,departments[val[0]])
            bar = Bar(""閮ㄩ棬鎻愪氦宸ュ崟缁熻"", width='100%', height='100%', title_pos='center', title_text_size=14)
            vals = RC.hgetall(dep_key)
            dep_vals = [val for val in vals]
            dep_counts = [int(vals[val]) for val in vals]
            bar.add('', dep_vals, dep_counts, is_label_show=True, is_toolbox_show=False,
                         legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            RC.delete(dep_key)
            INFOS.append(bar)
        except Exception as e:
            logging.error(e)
        #宸ュ崟鐢宠锋暟閲忎釜浜烘帓鍚
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.applicant,func.count(db_work_order.applicant)).filter(and_(
                db_work_order.source == source,db_work_order.date >=start_time,db_work_order.date<=end_time)).group_by(
                db_work_order.applicant).order_by(desc(func.count(db_work_order.applicant))).limit(15).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.applicant,
                                                         func.count(db_work_order.applicant)).filter(and_(
                    db_work_order.date >= start_time,db_work_order.date <= end_time)).group_by(
                    db_work_order.applicant).order_by(desc(func.count(db_work_order.applicant))).limit(15).all()
            vals = [list(val) for val in vals]
            for val in vals:
                val[0] = users[val[0]]
            bar = Bar(""涓浜烘彁浜ゅ伐鍗曠粺璁"", width='100%', height='100%', title_pos='center', title_text_size=14)
            dep_vals = [val[0] for val in vals]
            dep_counts = [int(val[1]) for val in vals]
            bar.add('', dep_vals, dep_counts, is_label_show=True, is_toolbox_show=False,
                    legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            INFOS.append(bar)
        except Exception as e:
            logging.error(e)
    except Exception as e:
        logging.error(e)
    return render_template('work_order_report.html',INFOS=INFOS,tt=(start_time,end_time))","for val in vals:
    val[0] = users[val[0]]","for val in vals:
    (val_0, *val_rvalmaining) = val
    val_0 = users[val_0]",Cannot refactor,Cannot refactor,2,0,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,,it actually cannot refactor
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for file in files:
    if file[-4:] == '.wav':
        sound_files[ExtractAudioID(file)] = os.path.join(root, file)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg_4, e_neg_3, e_neg_2, e_neg_1 = e[-4:]
variable mapping:
e[-4:]: e[-4:]",,,,,,,,it actually cannot refactor
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for file in files:
    if file[-4:] == '.trs':
        XMLRefine(os.path.join(root, file), os.path.join(target_dir, 'temp', file))
        annotation_files[file] = os.path.join(target_dir, 'temp', file)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg_4, e_neg_3, e_neg_2, e_neg_1 = e[-4:]
variable mapping:
e[-4:]: e[-4:]",,,,,,,,it actually cannot refactor
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for file in files:
    if file[-4:] == '.eaf':
        annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg_4, e_neg_3, e_neg_2, e_neg_1 = e[-4:]
variable mapping:
e[-4:]: e[-4:]",,,,,,,,it actually cannot refactor
HanLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HanLP/hanlp/components/srl/span_rank/srl_eval_utils.py,https://github.com/hankcs/HanLP/tree/master/hanlp/components/srl/span_rank/srl_eval_utils.py,,compute_srl_f1$162,"def compute_srl_f1(sentences, gold_srl, predictions, gold_path=None) -> SRLScores:
    assert len(gold_srl) == len(predictions)
    total_gold = 0
    total_predicted = 0
    total_matched = 0
    total_unlabeled_matched = 0
    num_sents = 0
    label_confusions = Counter()

    # Compute unofficial F1 of SRL relations.
    for gold, prediction in zip(gold_srl, predictions):
        gold_rels = 0
        pred_rels = 0
        matched = 0
        for pred_id, gold_args in gold.items():
            filtered_gold_args = [a for a in gold_args if a[2] not in [""V"", ""C-V""]]
            total_gold += len(filtered_gold_args)
            gold_rels += len(filtered_gold_args)
            if pred_id not in prediction:
                continue
            for a0 in filtered_gold_args:
                for a1 in prediction[pred_id]:
                    if a0[0] == a1[0] and a0[1] == a1[1]:
                        total_unlabeled_matched += 1
                        label_confusions.update([(a0[2], a1[2]), ])
                        if a0[2] == a1[2]:
                            total_matched += 1
                            matched += 1
        for pred_id, args in prediction.items():
            filtered_args = [a for a in args if a[2] not in [""V""]]  # ""C-V""]]
            total_predicted += len(filtered_args)
            pred_rels += len(filtered_args)

        if gold_rels == matched and pred_rels == matched:
            num_sents += 1

    precision, recall, f1 = _calc_f1(total_gold, total_predicted, total_matched,
                                     # ""SRL (unofficial)""
                                     )
    unlabeled_precision, unlabeled_recall, unlabeled_f1 = _calc_f1(total_gold, total_predicted,
                                                                   total_unlabeled_matched,
                                                                   # ""Unlabeled SRL (unofficial)""
                                                                   )

    # Prepare to compute official F1.
    if not gold_path:
        # print(""No gold conll_eval data provided. Recreating ..."")
        gold_path = tempfile.NamedTemporaryFile().name
        print_to_conll(sentences, gold_srl, gold_path, None)
        gold_predicates = None
    else:
        gold_predicates = read_gold_predicates(gold_path)

    temp_output = tempfile.NamedTemporaryFile().name
    # print((""Output temp outoput {}"".format(temp_output)))
    print_to_conll(sentences, predictions, temp_output, gold_predicates)

    # Evaluate twice with official script.
    conll_precision, conll_recall, conll_f1 = official_conll_05_evaluate(temp_output, gold_path)
    return SRLScores(unlabeled_precision, unlabeled_recall, unlabeled_f1, precision, recall, f1, conll_precision,
                     conll_recall, conll_f1, label_confusions, num_sents)","for (gold, prediction) in zip(gold_srl, predictions):
    gold_rels = 0
    pred_rels = 0
    matched = 0
    for (pred_id, gold_args) in gold.items():
        filtered_gold_args = [a for a in gold_args if a[2] not in ['V', 'C-V']]
        total_gold += len(filtered_gold_args)
        gold_rels += len(filtered_gold_args)
        if pred_id not in prediction:
            continue
        for a0 in filtered_gold_args:
            for a1 in prediction[pred_id]:
                if a0[0] == a1[0] and a0[1] == a1[1]:
                    total_unlabeled_matched += 1
                    label_confusions.update([(a0[2], a1[2])])
                    if a0[2] == a1[2]:
                        total_matched += 1
                        matched += 1
    for (pred_id, args) in prediction.items():
        filtered_args = [a for a in args if a[2] not in ['V']]
        total_predicted += len(filtered_args)
        pred_rels += len(filtered_args)
    if gold_rels == matched and pred_rels == matched:
        num_sents += 1",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: It depends on the type of ""e"" and ""pred_id"".
Iterable Unpacking: If ""e"" is a sequence type like a list or tuple and ""pred_id"" is an integer type constant that is within the range of indices of ""e"", then the answer is Yes. The Python code to unpack ""e"" to get unpacked elements and the corresponding variable mapping for each unpacked element is as follows:
e[pred_id]: e[pred_id]
variable mapping:
e[pred_id]: e[pred_id]

Otherwise, if ""e"" is not a sequence type or ""pred_id"" is not an integer type constant or it is out of range of indices of ""e"", then the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
centerNet-deep-sort,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/centerNet-deep-sort/CenterNet/src/tools/calc_coco_overlap.py,https://github.com/kimyoon-young/centerNet-deep-sort/tree/master/CenterNet/src/tools/calc_coco_overlap.py,,count_agnostic$117,"def count_agnostic(split):
  coco = COCO.COCO(ANN_PATH + ANN_FILES[split])
  images = coco.getImgIds()
  cnt = 0
  for img_id in images:
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    anns = coco.loadAnns(ids=ann_ids)
    centers = []
    for ann in anns:
      bbox = ann['bbox']
      center = ((bbox[0] + bbox[2] / 2) // 4, (bbox[1] + bbox[3] / 2) // 4)
      for c in centers:
        if center[0] == c[0] and center[1] == c[1]:
          cnt += 1
      centers.append(center)
  print('find {} collisions!'.format(cnt))","for ann in anns:
    bbox = ann['bbox']
    center = ((bbox[0] + bbox[2] / 2) // 4, (bbox[1] + bbox[3] / 2) // 4)
    for c in centers:
        if center[0] == c[0] and center[1] == c[1]:
            cnt += 1
    centers.append(center)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_bbox = e['bbox']
variable mapping:
e_bbox: e['bbox']",,,,,,,,it actually cannot refactor
you-get,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/mgtv.py,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/mgtv.py,MGTV,prepare$93,"def prepare(self, **kwargs):
        if self.url:
            self.vid = self.get_vid_from_url(self.url)
        content_info = get_content(self.info_endpoint.format(video_id=self.vid))
        log.d(content_info)
        content_info = loads(content_info)
        self.title = content_info['data']['info']['videoName']

        content_player = get_content(self.player_endpoint.format(did=self.did, video_id=self.vid, tk2=self.tk2()))
        log.d(content_player)
        content_player = loads(content_player)
        pm2 = content_player['data']['atc']['pm2']

        content_source = get_content(self.source_endpoint.format(video_id=self.vid, tk2=self.tk2(), pm2=pm2))
        log.d(content_source)
        content_source = loads(content_source)
        domain = content_source['data']['stream_domain'][0]

        # stream_available = [i['name'] for i in content['data']['stream']]
        stream_available = {}
        for i in content_source['data']['stream']:
            stream_available[i['name']] = i['url']

        for s in self.stream_types:
            if s['video_profile'] in stream_available.keys():
                quality_id = self.id_dic[s['video_profile']]
                url = stream_available[s['video_profile']]
                if url is None or url == '':
                    # skip invalid profile with empty url
                    continue
                url = domain + re.sub(r'(\&arange\=\d+)', '', url)  # Un-Hum
                m3u8_url, m3u8_size, segment_list_this = self.get_mgtv_real_url(url)

                stream_fileid_list = []
                for i in segment_list_this:
                    stream_fileid_list.append(os.path.basename(i).split('.')[0])

                # make pieces
                pieces = []
                for i in zip(stream_fileid_list, segment_list_this):
                    pieces.append({'fileid': i[0], 'segs': i[1], })

                    self.streams[quality_id] = {
                        'container': s['container'],
                        'video_profile': s['video_profile'],
                        'size': m3u8_size,
                        'pieces': pieces,
                        'm3u8_url': m3u8_url
                    }

                if not kwargs['info_only']:
                    self.streams[quality_id]['src'] = segment_list_this","for i in content_source['data']['stream']:
    stream_available[i['name']] = i['url']",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: No
Iterable Unpacking: None
Explanation: For the given unpacked element e['url'] is a dictionary value that can be accessed directly using the key 'url'. However, iterable unpacking is not applicable in this case as the iterable object ""e"" is not a sequence type like a list or tuple.
Therefore, the answer is No and the iterable unpacking code is not applicable in this case.",,,,,,,,it actually cannot refactor
you-get,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/mgtv.py,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/mgtv.py,MGTV,prepare$93,"def prepare(self, **kwargs):
        if self.url:
            self.vid = self.get_vid_from_url(self.url)
        content_info = get_content(self.info_endpoint.format(video_id=self.vid))
        log.d(content_info)
        content_info = loads(content_info)
        self.title = content_info['data']['info']['videoName']

        content_player = get_content(self.player_endpoint.format(did=self.did, video_id=self.vid, tk2=self.tk2()))
        log.d(content_player)
        content_player = loads(content_player)
        pm2 = content_player['data']['atc']['pm2']

        content_source = get_content(self.source_endpoint.format(video_id=self.vid, tk2=self.tk2(), pm2=pm2))
        log.d(content_source)
        content_source = loads(content_source)
        domain = content_source['data']['stream_domain'][0]

        # stream_available = [i['name'] for i in content['data']['stream']]
        stream_available = {}
        for i in content_source['data']['stream']:
            stream_available[i['name']] = i['url']

        for s in self.stream_types:
            if s['video_profile'] in stream_available.keys():
                quality_id = self.id_dic[s['video_profile']]
                url = stream_available[s['video_profile']]
                if url is None or url == '':
                    # skip invalid profile with empty url
                    continue
                url = domain + re.sub(r'(\&arange\=\d+)', '', url)  # Un-Hum
                m3u8_url, m3u8_size, segment_list_this = self.get_mgtv_real_url(url)

                stream_fileid_list = []
                for i in segment_list_this:
                    stream_fileid_list.append(os.path.basename(i).split('.')[0])

                # make pieces
                pieces = []
                for i in zip(stream_fileid_list, segment_list_this):
                    pieces.append({'fileid': i[0], 'segs': i[1], })

                    self.streams[quality_id] = {
                        'container': s['container'],
                        'video_profile': s['video_profile'],
                        'size': m3u8_size,
                        'pieces': pieces,
                        'm3u8_url': m3u8_url
                    }

                if not kwargs['info_only']:
                    self.streams[quality_id]['src'] = segment_list_this","for s in self.stream_types:
    if s['video_profile'] in stream_available.keys():
        quality_id = self.id_dic[s['video_profile']]
        url = stream_available[s['video_profile']]
        if url is None or url == '':
            continue
        url = domain + re.sub('(\\&arange\\=\\d+)', '', url)
        (m3u8_url, m3u8_size, segment_list_this) = self.get_mgtv_real_url(url)
        stream_fileid_list = []
        for i in segment_list_this:
            stream_fileid_list.append(os.path.basename(i).split('.')[0])
        pieces = []
        for i in zip(stream_fileid_list, segment_list_this):
            pieces.append({'fileid': i[0], 'segs': i[1]})
            self.streams[quality_id] = {'container': s['container'], 'video_profile': s['video_profile'], 'size': m3u8_size, 'pieces': pieces, 'm3u8_url': m3u8_url}
        if not kwargs['info_only']:
            self.streams[quality_id]['src'] = segment_list_this",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_container, e_video_profile = e['container'], e['video_profile']
variable mapping:
e_container: e['container']
e_video_profile: e['video_profile']",,,,,,,,it actually cannot refactor
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,,get_sales_payment_data$66,"def get_sales_payment_data(filters, columns):
	data = []
	show_payment_detail = False

	sales_invoice_data = get_sales_invoice_data(filters)
	mode_of_payments = get_mode_of_payments(filters)
	mode_of_payment_details = get_mode_of_payment_details(filters)

	if filters.get(""payment_detail""):
		show_payment_detail = True
	else:
		show_payment_detail = False

	for inv in sales_invoice_data:
		owner_posting_date = inv[""owner""] + cstr(inv[""posting_date""])
		if show_payment_detail:
			row = [inv.posting_date, inv.owner, "" "", inv.net_total, inv.total_taxes, 0]
			data.append(row)
			for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
				row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
				data.append(row)
		else:
			total_payment = 0
			for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
				total_payment = total_payment + mop_detail[1]
			row = [
				inv.posting_date,
				inv.owner,
				"", "".join(mode_of_payments.get(owner_posting_date, [])),
				inv.net_total,
				inv.total_taxes,
				total_payment,
			]
			data.append(row)
	return data","for inv in sales_invoice_data:
    owner_posting_date = inv['owner'] + cstr(inv['posting_date'])
    if show_payment_detail:
        row = [inv.posting_date, inv.owner, ' ', inv.net_total, inv.total_taxes, 0]
        data.append(row)
        for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
            row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
            data.append(row)
    else:
        total_payment = 0
        for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
            total_payment = total_payment + mop_detail[1]
        row = [inv.posting_date, inv.owner, ', '.join(mode_of_payments.get(owner_posting_date, [])), inv.net_total, inv.total_taxes, total_payment]
        data.append(row)",It cannot be refactored by var unpacking,Cannot refactor,Cannot refactor,2,,,"Answer: Yes
Iterable Unpacking: e_owner, e_posting_date = e['owner'], e['posting_date']
variable mapping:
e_owner: e['owner']
e_posting_date: e['posting_date']",,,,,,,,it actually cannot refactor
st2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/st2/contrib/runners/action_chain_runner/tests/unit/test_actionchain.py,https://github.com/StackStorm/st2/tree/master/contrib/runners/action_chain_runner/tests/unit/test_actionchain.py,TestActionChainRunner,test_chain_runner_dependent_param_temp$559,"def test_chain_runner_dependent_param_temp(self, request):
        chain_runner = acr.get_runner()
        chain_runner.entry_point = CHAIN_DEP_INPUT
        chain_runner.action = ACTION_1
        action_ref = ResourceReference.to_string_reference(
            name=ACTION_1.name, pack=ACTION_1.pack
        )
        chain_runner.liveaction = LiveActionDB(action=action_ref)
        chain_runner.pre_run()
        chain_runner.run({""s1"": 1, ""s2"": 2, ""s3"": 3, ""s4"": 4})
        self.assertNotEqual(chain_runner.chain_holder.actionchain, None)
        expected_values = [{""p1"": ""1""}, {""p1"": ""1""}, {""p2"": ""1"", ""p3"": ""1"", ""p1"": ""1""}]
        # Each of the call_args must be one of
        for call_args in request.call_args_list:
            self.assertIn(call_args[0][0].parameters, expected_values)
            expected_values.remove(call_args[0][0].parameters)
        self.assertEqual(len(expected_values), 0, ""Not all expected values received."")","for call_args in request.call_args_list:
    self.assertIn(call_args[0][0].parameters, expected_values)
    expected_values.remove(call_args[0][0].parameters)","for call_args in request.call_args_list:
    ((call_args_0_0, _, *call_args_0_rcall_argsmaining), *call_args_rcall_argsmaining) = call_args
    self.assertIn(call_args_0_0.parameters, expected_values)
    expected_values.remove(call_args_0_0.parameters)","((data, *data), *data)","for ((call_args_0_0, *call_args_0_len), *call_args_len) in request.call_args_list:
    self.assertIn(
    call_args_0_0.parameters, expected_values)
    expected_values.remove(
    call_args_0_0.parameters)",0,,,"Answer: Yes
Iterable Unpacking: (e_0_0, _, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]",,,,,,,,
new_find,,,,,,,,,,,,,,,,,,,,,
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2020/med_test.py,https://github.com/3b1b/videos/tree/master/_2020/med_test.py,SamplePopulationBreastCancer,construct$820,"def construct(self):
        # Introduce population
        title = TexText(
            ""Sample of "", ""$1{,}000$"", "" women"",
            font_size=72,
        )
        title.add(Underline(title, color=GREY_B))
        title.to_edge(UP, buff=MED_SMALL_BUFF)
        self.add(title)

        woman = WomanIcon()
        globals()['woman'] = woman
        population = VGroup(*[woman.copy() for x in range(1000)])
        population.arrange_in_grid(
            25, 40,
            buff=LARGE_BUFF,
            fill_rows_first=False,
        )
        population.set_height(6)
        population.next_to(title, DOWN)

        counter = Integer(1000, edge_to_fix=UL)
        counter.replace(title[1])
        counter.set_value(0)

        title[1].set_opacity(0)
        self.play(
            ShowIncreasingSubsets(population),
            ChangeDecimalToValue(counter, 1000),
            run_time=5
        )
        self.remove(counter)
        title[1].set_opacity(1)
        self.wait()

        # Show true positives
        rects = VGroup(Rectangle(), Rectangle())
        rects.set_height(6)
        rects[0].set_width(4, stretch=True)
        rects[1].set_width(8, stretch=True)
        rects[0].set_stroke(YELLOW, 3)
        rects[1].set_stroke(GREY, 3)
        rects.arrange(RIGHT)
        rects.center().to_edge(DOWN, buff=MED_SMALL_BUFF)

        positive_cases = population[:10]
        negative_cases = population[10:]

        positive_cases.generate_target()
        positive_cases.target.move_to(rects[0])
        positive_cases.target.set_color(YELLOW)

        negative_cases.generate_target()
        negative_cases.target.set_height(rects[1].get_height() * 0.8)
        negative_cases.target.move_to(rects[1])

        positive_words = TexText(r""1\% "", ""Have breast cancer"", font_size=36)
        positive_words.set_color(YELLOW)
        positive_words.next_to(rects[0], UP, SMALL_BUFF)

        negative_words = TexText(r""99\% "", ""Do not have cancer"", font_size=36)
        negative_words.set_color(GREY_B)
        negative_words.next_to(rects[1], UP, SMALL_BUFF)

        self.play(
            MoveToTarget(positive_cases),
            MoveToTarget(negative_cases),
            Write(positive_words, run_time=1),
            Write(negative_words, run_time=1),
            FadeIn(rects),
        )
        self.wait()

        # Show screening
        scan_lines = VGroup(*(
            Line(
                # FRAME_WIDTH * LEFT / 2,
                FRAME_HEIGHT * DOWN / 2,
                icon.get_center(),
                stroke_width=1,
                stroke_color=interpolate_color(BLUE, GREEN, random.random())
            )
            for icon in population
        ))
        self.play(
            LaggedStartMap(
                ShowCreationThenFadeOut, scan_lines,
                lag_ratio=1 / len(scan_lines),
                run_time=3,
            )
        )
        self.wait()

        # Test results on cancer population
        tpr_words = TexText(""9 True positives"", font_size=36)
        fnr_words = TexText(""1 False negative"", font_size=36)
        tnr_words = TexText(""901 True negatives"", font_size=36)
        fpr_words = TexText(""89 False positives"", font_size=36)

        tpr_words.set_color(GREEN_B)
        fnr_words.set_color(RED_D)
        tnr_words.set_color(RED_B)
        fpr_words.set_color(GREEN_D)

        tp_cases = positive_cases[:9]
        fn_cases = positive_cases[9:]

        tpr_words.next_to(tp_cases, UP)
        fnr_words.next_to(fn_cases, DOWN)

        signs = VGroup()
        for woman in tp_cases:
            sign = Tex(""+"")
            sign.set_color(GREEN_B)
            sign.match_height(woman)
            sign.next_to(woman, RIGHT, SMALL_BUFF)
            woman.sign = sign
            signs.add(sign)
        for woman in fn_cases:
            sign = Tex(""-"")
            sign.set_color(RED)
            sign.match_width(signs[0])
            sign.next_to(woman, RIGHT, SMALL_BUFF)
            woman.sign = sign
            signs.add(sign)

        boxes = VGroup()
        for n, woman in enumerate(positive_cases):
            box = SurroundingRectangle(woman, buff=0)
            box.set_stroke(width=2)
            if woman in tp_cases:
                box.set_color(GREEN)
            else:
                box.set_color(RED)
            woman.box = box
            boxes.add(box)

        self.play(
            FadeIn(tpr_words, shift=0.2 * UP),
            ShowIncreasingSubsets(signs[:9]),
            ShowIncreasingSubsets(boxes[:9]),
        )
        self.wait()
        self.play(
            FadeIn(fnr_words, shift=0.2 * DOWN),
            Write(signs[9:]),
            ShowCreation(boxes[9:]),
        )
        self.wait()

        # Test results on cancer-free population
        negative_cases.sort(lambda p: -p[1])

        num_fp = int(len(negative_cases) * 0.09)
        fp_cases = negative_cases[:num_fp]
        tn_cases = negative_cases[num_fp:]

        new_boxes = VGroup()
        for n, woman in enumerate(negative_cases):
            box = SurroundingRectangle(woman, buff=0)
            box.set_stroke(width=2)
            if woman in fp_cases:
                box.set_color(GREEN)
            else:
                box.set_color(RED)
            woman.box = box
            new_boxes.add(box)

        fpr_words.next_to(fp_cases, UP, buff=SMALL_BUFF)
        tnr_words.next_to(tn_cases, DOWN, buff=0.2)

        self.play(
            FadeIn(fpr_words, shift=0.2 * UP),
            ShowIncreasingSubsets(new_boxes[:num_fp])
        )
        self.wait()
        self.play(
            FadeIn(tnr_words, shift=0.2 * DOWN),
            ShowIncreasingSubsets(new_boxes[num_fp:])
        )
        self.wait()

        # Consolidate boxes
        self.remove(boxes, new_boxes, population)
        for woman in population:
            woman.add(woman.box)
        self.add(population)

        # Limit view to positive cases
        for cases, nr, rect in zip([tp_cases, fp_cases], [3, 7], rects):
            cases.save_state()
            cases.generate_target()
            for case in cases.target:
                case[-1].set_stroke(width=3)
                case[-1].scale(1.1)
            cases.target.arrange_in_grid(
                n_rows=nr,
                buff=0.5 * cases[0].get_width()
            )
            cases.target.scale(0.5 / cases.target[0].get_height())
            cases.target.move_to(rect)

        fp_cases.target.shift(0.4 * DOWN)
        positive_words.save_state()
        negative_words.save_state()
        tpr_words.save_state()
        fpr_words.save_state()

        self.play(
            MoveToTarget(tp_cases),
            MoveToTarget(fp_cases),
            tpr_words.next_to, tp_cases.target, UP,
            fpr_words.next_to, fp_cases.target, UP,
            FadeOut(signs),
            positive_words[0].set_opacity, 0,
            negative_words[0].set_opacity, 0,
            positive_words[1].match_x, rects[0],
            negative_words[1].match_x, rects[1],
            LaggedStart(
                FadeOut(fn_cases, shift=DOWN),
                FadeOut(fnr_words, shift=DOWN),
                FadeOut(tn_cases, shift=DOWN),
                FadeOut(tnr_words, shift=DOWN),
            ),
        )
        self.wait()

        # Emphasize groups counts
        self.play(
            ShowCreationThenFadeOut(SurroundingRectangle(
                tpr_words[0][:1],
                stroke_width=2,
                stroke_color=WHITE,
                buff=0.05,
            )),
            LaggedStartMap(Indicate, tp_cases, color=YELLOW, lag_ratio=0.3, run_time=1),
        )
        self.wait()
        self.play(
            ShowCreationThenFadeOut(SurroundingRectangle(
                fpr_words[0][:2],
                stroke_width=2,
                stroke_color=WHITE,
                buff=0.05,
            )),
            LaggedStartMap(
                Indicate, fp_cases,
                color=GREEN_A,
                lag_ratio=0.05,
                run_time=3
            )
        )
        self.wait()

        # Final equation
        equation = Tex(
            ""P("",
            ""\\text{Have cancer }"",
            ""|"",
            ""\\text{ positive test})"",
            ""\\approx"",
            ""\\frac{9}{9 + 89}"",
            ""\\approx \\frac{1}{11}""
        )
        equation.set_color_by_tex(""cancer"", YELLOW)
        equation.set_color_by_tex(""positive"", GREEN)
        equation.to_edge(UP, buff=SMALL_BUFF)

        self.play(
            FadeIn(equation[:-1], shift=UP),
            FadeOut(title, shift=UP),
        )
        self.wait()
        self.play(Write(equation[-1]))
        self.wait()

        # Label PPV
        frame = self.camera.frame
        frame.save_state()

        ppv_words = TexText(
            ""Positive\\\\"",
            ""Predictive\\\\"",
            ""Value\\\\"",
            alignment="""",
        )
        ppv_words.next_to(equation, RIGHT, LARGE_BUFF, DOWN)
        for word in ppv_words:
            word[0].set_color(BLUE)

        ppv_rhs = Tex(
            ""={\\text{TP} \\over \\text{TP} + \\text{FP}}"",
            tex_to_color_map={
                ""\\text{TP}"": GREEN_B,
                ""\\text{FP}"": GREEN_C,
            }
        )
        ppv_rhs.next_to(ppv_words, RIGHT)
        ppv_rhs.shift(1.5 * LEFT)

        self.play(frame.scale, 1.1, {""about_edge"": DL})
        self.play(ShowIncreasingSubsets(ppv_words))
        self.wait()

        self.play(
            equation.shift, 1.5 * LEFT + 0.5 * UP,
            ppv_words.shift, 1.5 * LEFT,
            FadeIn(ppv_rhs, lag_ratio=0.1),
            frame.scale, 1.1, {""about_edge"": DL},
        )
        self.wait()

        # Go back to earlier state
        self.play(
            frame.restore,
            frame.shift, 0.5 * DOWN,
            LaggedStartMap(FadeOut, VGroup(equation, ppv_words, ppv_rhs)),
            LaggedStartMap(Restore, VGroup(
                tpr_words, tp_cases,
                fpr_words, fp_cases,
            )),
            run_time=3,
        )
        self.play(
            LaggedStartMap(FadeIn, VGroup(
                fnr_words, fn_cases,
                tnr_words, tn_cases,
            )),
        )
        self.wait()

        # Fade rects
        fade_rects = VGroup(*(
            BackgroundRectangle(
                VGroup(rect, words),
                fill_opacity=0.9,
                fill_color=BLACK,
                buff=SMALL_BUFF,
            )
            for rect, words in zip(rects, [positive_words, negative_words])
        ))

        # Sensitivity
        sens_eq = Tex(
            ""\\text{Sensitivity}"",
            ""= {9 \\over 10}"",
            ""= 90\\%""
        )
        sens_eq.next_to(rects[0], LEFT, MED_LARGE_BUFF, aligned_edge=UP)
        sens_eq.shift(DOWN)

        fnr_eq = Tex(
            ""\\text{False Negative Rate}"", ""= 10\\%""
        )
        fnr_eq.set_color(RED)
        fnr_eq.scale(0.9)
        equiv = Tex(""\\Leftrightarrow"")
        equiv.scale(1.5)
        equiv.rotate(90 * DEGREES)
        equiv.next_to(sens_eq, DOWN, MED_LARGE_BUFF)
        fnr_eq.next_to(equiv, DOWN, MED_LARGE_BUFF)

        self.play(
            frame.shift, 5 * LEFT,
            FadeIn(fade_rects[1]),
            Write(sens_eq[0]),
        )
        self.wait()
        self.play(
            TransformFromCopy(tpr_words[0][0], sens_eq[1][1]),
            Write(sens_eq[1][0]),
            Write(sens_eq[1][2:]),
        )
        self.play(Write(sens_eq[2]))
        self.wait()

        self.play(
            FadeIn(equiv, shift=0.5 * DOWN),
            FadeIn(fnr_eq, shift=1.0 * DOWN),
        )
        self.wait()

        # Transition to right side
        fade_rects[0].stretch(5, 0, about_edge=RIGHT)
        self.play(
            ApplyMethod(frame.shift, 10 * RIGHT, run_time=4),
            FadeIn(fade_rects[0], run_time=2),
            FadeOut(fade_rects[1], run_time=2),
        )

        # Specificity
        spec_eq = Tex(
            ""\\text{Specificity}"",
            ""= {901 \\over 990}"",
            ""\\approx 91\\%""
        )
        spec_eq.next_to(rects[1], RIGHT, MED_LARGE_BUFF, aligned_edge=DOWN)
        spec_eq.shift(UP)

        fpr_eq = Tex(
            ""\\text{False Positive Rate}"", ""= 9\\%""
        )
        fpr_eq.set_color(GREEN)
        fpr_eq.scale(0.9)
        equiv2 = Tex(""\\Leftrightarrow"")
        equiv2.scale(1.5)
        equiv2.rotate(90 * DEGREES)
        equiv2.next_to(spec_eq, UP, MED_LARGE_BUFF)
        fpr_eq.next_to(equiv2, UP, MED_LARGE_BUFF)

        self.play(Write(spec_eq[0]))
        self.wait()
        self.play(
            Write(spec_eq[1][0]),
            TransformFromCopy(
                tnr_words[0][:3],
                spec_eq[1][1:4],
                run_time=2,
                path_arc=30 * DEGREES,
            ),
            Write(spec_eq[1][4:]),
        )
        self.wait()
        self.play(Write(spec_eq[2]))
        self.wait()

        self.play(
            FadeIn(equiv2, shift=0.5 * UP),
            FadeIn(fpr_eq, shift=1.0 * UP),
        )
        self.wait()

        # Reset to show both kinds of accuracy
        eqs = [sens_eq, spec_eq]
        for eq, word in zip(eqs, [positive_words, negative_words]):
            eq.generate_target()
            eq.target[1].set_opacity(0)
            eq.target[2].move_to(eq.target[1], LEFT),
            eq.target.next_to(word, UP, buff=0.3)

        self.play(
            FadeOut(fade_rects[0]),
            frame.shift, 5 * LEFT,
            frame.scale, 1.1, {""about_edge"": DOWN},
            MoveToTarget(sens_eq),
            MoveToTarget(spec_eq),
            *map(FadeOut, (fnr_eq, fpr_eq, equiv, equiv2)),
            run_time=2,
        )
        self.wait()

        self.play(
            VGroup(
                fn_cases, fnr_words,
                fp_cases, fpr_words,
            ).set_opacity, 0.2,
            rate_func=there_and_back_with_pause,
            run_time=3
        )","for (cases, nr, rect) in zip([tp_cases, fp_cases], [3, 7], rects):
    cases.save_state()
    cases.generate_target()
    for case in cases.target:
        case[-1].set_stroke(width=3)
        case[-1].scale(1.1)
    cases.target.arrange_in_grid(n_rows=nr, buff=0.5 * cases[0].get_width())
    cases.target.scale(0.5 / cases.target[0].get_height())
    cases.target.move_to(rect)","for (cases, nr, rect) in zip([tp_cases, fp_cases], [3, 7], rects):
    (cases_0, *cases_rcasesmaining) = cases
    cases.save_state()
    cases.generate_target()
    for case in cases.target:
        case[-1].set_stroke(width=3)
        case[-1].scale(1.1)
    cases.target.arrange_in_grid(n_rows=nr, buff=0.5 * cases_0.get_width())
    cases.target.scale(0.5 / cases.target[0].get_height())
    cases.target.move_to(rect)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,,
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2020/med_test.py,https://github.com/3b1b/videos/tree/master/_2020/med_test.py,SamplePopulationBreastCancer,construct$820,"def construct(self):
        # Introduce population
        title = TexText(
            ""Sample of "", ""$1{,}000$"", "" women"",
            font_size=72,
        )
        title.add(Underline(title, color=GREY_B))
        title.to_edge(UP, buff=MED_SMALL_BUFF)
        self.add(title)

        woman = WomanIcon()
        globals()['woman'] = woman
        population = VGroup(*[woman.copy() for x in range(1000)])
        population.arrange_in_grid(
            25, 40,
            buff=LARGE_BUFF,
            fill_rows_first=False,
        )
        population.set_height(6)
        population.next_to(title, DOWN)

        counter = Integer(1000, edge_to_fix=UL)
        counter.replace(title[1])
        counter.set_value(0)

        title[1].set_opacity(0)
        self.play(
            ShowIncreasingSubsets(population),
            ChangeDecimalToValue(counter, 1000),
            run_time=5
        )
        self.remove(counter)
        title[1].set_opacity(1)
        self.wait()

        # Show true positives
        rects = VGroup(Rectangle(), Rectangle())
        rects.set_height(6)
        rects[0].set_width(4, stretch=True)
        rects[1].set_width(8, stretch=True)
        rects[0].set_stroke(YELLOW, 3)
        rects[1].set_stroke(GREY, 3)
        rects.arrange(RIGHT)
        rects.center().to_edge(DOWN, buff=MED_SMALL_BUFF)

        positive_cases = population[:10]
        negative_cases = population[10:]

        positive_cases.generate_target()
        positive_cases.target.move_to(rects[0])
        positive_cases.target.set_color(YELLOW)

        negative_cases.generate_target()
        negative_cases.target.set_height(rects[1].get_height() * 0.8)
        negative_cases.target.move_to(rects[1])

        positive_words = TexText(r""1\% "", ""Have breast cancer"", font_size=36)
        positive_words.set_color(YELLOW)
        positive_words.next_to(rects[0], UP, SMALL_BUFF)

        negative_words = TexText(r""99\% "", ""Do not have cancer"", font_size=36)
        negative_words.set_color(GREY_B)
        negative_words.next_to(rects[1], UP, SMALL_BUFF)

        self.play(
            MoveToTarget(positive_cases),
            MoveToTarget(negative_cases),
            Write(positive_words, run_time=1),
            Write(negative_words, run_time=1),
            FadeIn(rects),
        )
        self.wait()

        # Show screening
        scan_lines = VGroup(*(
            Line(
                # FRAME_WIDTH * LEFT / 2,
                FRAME_HEIGHT * DOWN / 2,
                icon.get_center(),
                stroke_width=1,
                stroke_color=interpolate_color(BLUE, GREEN, random.random())
            )
            for icon in population
        ))
        self.play(
            LaggedStartMap(
                ShowCreationThenFadeOut, scan_lines,
                lag_ratio=1 / len(scan_lines),
                run_time=3,
            )
        )
        self.wait()

        # Test results on cancer population
        tpr_words = TexText(""9 True positives"", font_size=36)
        fnr_words = TexText(""1 False negative"", font_size=36)
        tnr_words = TexText(""901 True negatives"", font_size=36)
        fpr_words = TexText(""89 False positives"", font_size=36)

        tpr_words.set_color(GREEN_B)
        fnr_words.set_color(RED_D)
        tnr_words.set_color(RED_B)
        fpr_words.set_color(GREEN_D)

        tp_cases = positive_cases[:9]
        fn_cases = positive_cases[9:]

        tpr_words.next_to(tp_cases, UP)
        fnr_words.next_to(fn_cases, DOWN)

        signs = VGroup()
        for woman in tp_cases:
            sign = Tex(""+"")
            sign.set_color(GREEN_B)
            sign.match_height(woman)
            sign.next_to(woman, RIGHT, SMALL_BUFF)
            woman.sign = sign
            signs.add(sign)
        for woman in fn_cases:
            sign = Tex(""-"")
            sign.set_color(RED)
            sign.match_width(signs[0])
            sign.next_to(woman, RIGHT, SMALL_BUFF)
            woman.sign = sign
            signs.add(sign)

        boxes = VGroup()
        for n, woman in enumerate(positive_cases):
            box = SurroundingRectangle(woman, buff=0)
            box.set_stroke(width=2)
            if woman in tp_cases:
                box.set_color(GREEN)
            else:
                box.set_color(RED)
            woman.box = box
            boxes.add(box)

        self.play(
            FadeIn(tpr_words, shift=0.2 * UP),
            ShowIncreasingSubsets(signs[:9]),
            ShowIncreasingSubsets(boxes[:9]),
        )
        self.wait()
        self.play(
            FadeIn(fnr_words, shift=0.2 * DOWN),
            Write(signs[9:]),
            ShowCreation(boxes[9:]),
        )
        self.wait()

        # Test results on cancer-free population
        negative_cases.sort(lambda p: -p[1])

        num_fp = int(len(negative_cases) * 0.09)
        fp_cases = negative_cases[:num_fp]
        tn_cases = negative_cases[num_fp:]

        new_boxes = VGroup()
        for n, woman in enumerate(negative_cases):
            box = SurroundingRectangle(woman, buff=0)
            box.set_stroke(width=2)
            if woman in fp_cases:
                box.set_color(GREEN)
            else:
                box.set_color(RED)
            woman.box = box
            new_boxes.add(box)

        fpr_words.next_to(fp_cases, UP, buff=SMALL_BUFF)
        tnr_words.next_to(tn_cases, DOWN, buff=0.2)

        self.play(
            FadeIn(fpr_words, shift=0.2 * UP),
            ShowIncreasingSubsets(new_boxes[:num_fp])
        )
        self.wait()
        self.play(
            FadeIn(tnr_words, shift=0.2 * DOWN),
            ShowIncreasingSubsets(new_boxes[num_fp:])
        )
        self.wait()

        # Consolidate boxes
        self.remove(boxes, new_boxes, population)
        for woman in population:
            woman.add(woman.box)
        self.add(population)

        # Limit view to positive cases
        for cases, nr, rect in zip([tp_cases, fp_cases], [3, 7], rects):
            cases.save_state()
            cases.generate_target()
            for case in cases.target:
                case[-1].set_stroke(width=3)
                case[-1].scale(1.1)
            cases.target.arrange_in_grid(
                n_rows=nr,
                buff=0.5 * cases[0].get_width()
            )
            cases.target.scale(0.5 / cases.target[0].get_height())
            cases.target.move_to(rect)

        fp_cases.target.shift(0.4 * DOWN)
        positive_words.save_state()
        negative_words.save_state()
        tpr_words.save_state()
        fpr_words.save_state()

        self.play(
            MoveToTarget(tp_cases),
            MoveToTarget(fp_cases),
            tpr_words.next_to, tp_cases.target, UP,
            fpr_words.next_to, fp_cases.target, UP,
            FadeOut(signs),
            positive_words[0].set_opacity, 0,
            negative_words[0].set_opacity, 0,
            positive_words[1].match_x, rects[0],
            negative_words[1].match_x, rects[1],
            LaggedStart(
                FadeOut(fn_cases, shift=DOWN),
                FadeOut(fnr_words, shift=DOWN),
                FadeOut(tn_cases, shift=DOWN),
                FadeOut(tnr_words, shift=DOWN),
            ),
        )
        self.wait()

        # Emphasize groups counts
        self.play(
            ShowCreationThenFadeOut(SurroundingRectangle(
                tpr_words[0][:1],
                stroke_width=2,
                stroke_color=WHITE,
                buff=0.05,
            )),
            LaggedStartMap(Indicate, tp_cases, color=YELLOW, lag_ratio=0.3, run_time=1),
        )
        self.wait()
        self.play(
            ShowCreationThenFadeOut(SurroundingRectangle(
                fpr_words[0][:2],
                stroke_width=2,
                stroke_color=WHITE,
                buff=0.05,
            )),
            LaggedStartMap(
                Indicate, fp_cases,
                color=GREEN_A,
                lag_ratio=0.05,
                run_time=3
            )
        )
        self.wait()

        # Final equation
        equation = Tex(
            ""P("",
            ""\\text{Have cancer }"",
            ""|"",
            ""\\text{ positive test})"",
            ""\\approx"",
            ""\\frac{9}{9 + 89}"",
            ""\\approx \\frac{1}{11}""
        )
        equation.set_color_by_tex(""cancer"", YELLOW)
        equation.set_color_by_tex(""positive"", GREEN)
        equation.to_edge(UP, buff=SMALL_BUFF)

        self.play(
            FadeIn(equation[:-1], shift=UP),
            FadeOut(title, shift=UP),
        )
        self.wait()
        self.play(Write(equation[-1]))
        self.wait()

        # Label PPV
        frame = self.camera.frame
        frame.save_state()

        ppv_words = TexText(
            ""Positive\\\\"",
            ""Predictive\\\\"",
            ""Value\\\\"",
            alignment="""",
        )
        ppv_words.next_to(equation, RIGHT, LARGE_BUFF, DOWN)
        for word in ppv_words:
            word[0].set_color(BLUE)

        ppv_rhs = Tex(
            ""={\\text{TP} \\over \\text{TP} + \\text{FP}}"",
            tex_to_color_map={
                ""\\text{TP}"": GREEN_B,
                ""\\text{FP}"": GREEN_C,
            }
        )
        ppv_rhs.next_to(ppv_words, RIGHT)
        ppv_rhs.shift(1.5 * LEFT)

        self.play(frame.scale, 1.1, {""about_edge"": DL})
        self.play(ShowIncreasingSubsets(ppv_words))
        self.wait()

        self.play(
            equation.shift, 1.5 * LEFT + 0.5 * UP,
            ppv_words.shift, 1.5 * LEFT,
            FadeIn(ppv_rhs, lag_ratio=0.1),
            frame.scale, 1.1, {""about_edge"": DL},
        )
        self.wait()

        # Go back to earlier state
        self.play(
            frame.restore,
            frame.shift, 0.5 * DOWN,
            LaggedStartMap(FadeOut, VGroup(equation, ppv_words, ppv_rhs)),
            LaggedStartMap(Restore, VGroup(
                tpr_words, tp_cases,
                fpr_words, fp_cases,
            )),
            run_time=3,
        )
        self.play(
            LaggedStartMap(FadeIn, VGroup(
                fnr_words, fn_cases,
                tnr_words, tn_cases,
            )),
        )
        self.wait()

        # Fade rects
        fade_rects = VGroup(*(
            BackgroundRectangle(
                VGroup(rect, words),
                fill_opacity=0.9,
                fill_color=BLACK,
                buff=SMALL_BUFF,
            )
            for rect, words in zip(rects, [positive_words, negative_words])
        ))

        # Sensitivity
        sens_eq = Tex(
            ""\\text{Sensitivity}"",
            ""= {9 \\over 10}"",
            ""= 90\\%""
        )
        sens_eq.next_to(rects[0], LEFT, MED_LARGE_BUFF, aligned_edge=UP)
        sens_eq.shift(DOWN)

        fnr_eq = Tex(
            ""\\text{False Negative Rate}"", ""= 10\\%""
        )
        fnr_eq.set_color(RED)
        fnr_eq.scale(0.9)
        equiv = Tex(""\\Leftrightarrow"")
        equiv.scale(1.5)
        equiv.rotate(90 * DEGREES)
        equiv.next_to(sens_eq, DOWN, MED_LARGE_BUFF)
        fnr_eq.next_to(equiv, DOWN, MED_LARGE_BUFF)

        self.play(
            frame.shift, 5 * LEFT,
            FadeIn(fade_rects[1]),
            Write(sens_eq[0]),
        )
        self.wait()
        self.play(
            TransformFromCopy(tpr_words[0][0], sens_eq[1][1]),
            Write(sens_eq[1][0]),
            Write(sens_eq[1][2:]),
        )
        self.play(Write(sens_eq[2]))
        self.wait()

        self.play(
            FadeIn(equiv, shift=0.5 * DOWN),
            FadeIn(fnr_eq, shift=1.0 * DOWN),
        )
        self.wait()

        # Transition to right side
        fade_rects[0].stretch(5, 0, about_edge=RIGHT)
        self.play(
            ApplyMethod(frame.shift, 10 * RIGHT, run_time=4),
            FadeIn(fade_rects[0], run_time=2),
            FadeOut(fade_rects[1], run_time=2),
        )

        # Specificity
        spec_eq = Tex(
            ""\\text{Specificity}"",
            ""= {901 \\over 990}"",
            ""\\approx 91\\%""
        )
        spec_eq.next_to(rects[1], RIGHT, MED_LARGE_BUFF, aligned_edge=DOWN)
        spec_eq.shift(UP)

        fpr_eq = Tex(
            ""\\text{False Positive Rate}"", ""= 9\\%""
        )
        fpr_eq.set_color(GREEN)
        fpr_eq.scale(0.9)
        equiv2 = Tex(""\\Leftrightarrow"")
        equiv2.scale(1.5)
        equiv2.rotate(90 * DEGREES)
        equiv2.next_to(spec_eq, UP, MED_LARGE_BUFF)
        fpr_eq.next_to(equiv2, UP, MED_LARGE_BUFF)

        self.play(Write(spec_eq[0]))
        self.wait()
        self.play(
            Write(spec_eq[1][0]),
            TransformFromCopy(
                tnr_words[0][:3],
                spec_eq[1][1:4],
                run_time=2,
                path_arc=30 * DEGREES,
            ),
            Write(spec_eq[1][4:]),
        )
        self.wait()
        self.play(Write(spec_eq[2]))
        self.wait()

        self.play(
            FadeIn(equiv2, shift=0.5 * UP),
            FadeIn(fpr_eq, shift=1.0 * UP),
        )
        self.wait()

        # Reset to show both kinds of accuracy
        eqs = [sens_eq, spec_eq]
        for eq, word in zip(eqs, [positive_words, negative_words]):
            eq.generate_target()
            eq.target[1].set_opacity(0)
            eq.target[2].move_to(eq.target[1], LEFT),
            eq.target.next_to(word, UP, buff=0.3)

        self.play(
            FadeOut(fade_rects[0]),
            frame.shift, 5 * LEFT,
            frame.scale, 1.1, {""about_edge"": DOWN},
            MoveToTarget(sens_eq),
            MoveToTarget(spec_eq),
            *map(FadeOut, (fnr_eq, fpr_eq, equiv, equiv2)),
            run_time=2,
        )
        self.wait()

        self.play(
            VGroup(
                fn_cases, fnr_words,
                fp_cases, fpr_words,
            ).set_opacity, 0.2,
            rate_func=there_and_back_with_pause,
            run_time=3
        )","for case in cases.target:
    case[-1].set_stroke(width=3)
    case[-1].scale(1.1)","for case in cases.target:
    (*case_rcasemaining, case_last) = case
    case_last.set_stroke(width=3)
    case_last.scale(1.1)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_last = e
variable mapping:
e_last: e[-1]",,,,,,,,
transitions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transitions/transitions/extensions/diagrams_pygraphviz.py,https://github.com/pytransitions/transitions/tree/master/transitions/extensions/diagrams_pygraphviz.py,Graph,get_graph$55,"def get_graph(self, title=None, roi_state=None):
        if title:
            self.fsm_graph.graph_attr['label'] = title
        if roi_state:
            filtered = _copy_agraph(self.fsm_graph)
            kept_nodes = set()
            active_state = roi_state.name if hasattr(roi_state, 'name') else roi_state
            if not filtered.has_node(roi_state):
                active_state += '_anchor'
            kept_nodes.add(active_state)

            # remove all edges that have no connection to the currently active state
            for edge in filtered.edges():
                if active_state not in edge:
                    filtered.delete_edge(edge)

            # find the ingoing edge by color; remove the rest
            for edge in filtered.in_edges(active_state):
                if edge.attr['color'] == self.fsm_graph.style_attributes['edge']['previous']['color']:
                    kept_nodes.add(edge[0])
                else:
                    filtered.delete_edge(edge)

            # remove outgoing edges from children
            for edge in filtered.out_edges_iter(active_state):
                kept_nodes.add(edge[1])

            for node in filtered.nodes():
                if node not in kept_nodes:
                    filtered.delete_node(node)

            return filtered
        return self.fsm_graph","for edge in filtered.in_edges(active_state):
    if edge.attr['color'] == self.fsm_graph.style_attributes['edge']['previous']['color']:
        kept_nodes.add(edge[0])
    else:
        filtered.delete_edge(edge)","for edge in filtered.in_edges(active_state):
    (edge_0, *edge_redgemaining) = edge
    if edge.attr['color'] == self.fsm_graph.style_attributes['edge']['previous']['color']:
        kept_nodes.add(edge_0)
    else:
        filtered.delete_edge(edge)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,,
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/smooth_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/smooth_uv.py,MUV_OT_SmoothUV,__smooth_wo_transmission$107,"def __smooth_wo_transmission(self, loop_seqs, uv_layer):
        # calculate path length
        loops = []
        for hseq in loop_seqs:
            loops.extend([hseq[0][0], hseq[0][1]])
        full_vlen = 0
        accm_vlens = [0.0]
        full_uvlen = 0
        accm_uvlens = [0.0]
        orig_uvs = [loop_seqs[0][0][0][uv_layer].uv.copy()]
        for l1, l2 in zip(loops[:-1], loops[1:]):
            diff_v = l2.vert.co - l1.vert.co
            full_vlen = full_vlen + diff_v.length
            accm_vlens.append(full_vlen)
            diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
            full_uvlen = full_uvlen + diff_uv.length
            accm_uvlens.append(full_uvlen)
            orig_uvs.append(l2[uv_layer].uv.copy())

        for hidx, hseq in enumerate(loop_seqs):
            pair = hseq[0]
            for pidx, l in enumerate(pair):
                if self.select:
                    l[uv_layer].select = True

                # ignore start/end loop
                if (hidx == 0 and pidx == 0) or\
                   ((hidx == len(loop_seqs) - 1) and (pidx == len(pair) - 1)):
                    continue

                # calculate target path length
                # target = no influenced * (1 - infl) + influenced * infl
                tgt_noinfl = full_uvlen * (hidx + pidx) / (len(loop_seqs))
                tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
                target_length = tgt_noinfl * (1 - self.mesh_infl) + \
                    tgt_infl * self.mesh_infl

                # get target UV
                for i in range(len(accm_uvlens[:-1])):
                    # get line segment which UV will be placed
                    if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                        tgt_seg_len = target_length - accm_uvlens[i]
                        seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                        uv1 = orig_uvs[i]
                        uv2 = orig_uvs[i + 1]
                        target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                        break
                else:
                    self.report({'ERROR'}, ""Failed to get target UV"")
                    return {'CANCELLED'}

                # update UV
                l[uv_layer].uv = target_uv","for (hidx, hseq) in enumerate(loop_seqs):
    pair = hseq[0]
    for (pidx, l) in enumerate(pair):
        if self.select:
            l[uv_layer].select = True
        if hidx == 0 and pidx == 0 or (hidx == len(loop_seqs) - 1 and pidx == len(pair) - 1):
            continue
        tgt_noinfl = full_uvlen * (hidx + pidx) / len(loop_seqs)
        tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
        target_length = tgt_noinfl * (1 - self.mesh_infl) + tgt_infl * self.mesh_infl
        for i in range(len(accm_uvlens[:-1])):
            if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                tgt_seg_len = target_length - accm_uvlens[i]
                seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                uv1 = orig_uvs[i]
                uv2 = orig_uvs[i + 1]
                target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                break
        else:
            self.report({'ERROR'}, 'Failed to get target UV')
            return {'CANCELLED'}
        l[uv_layer].uv = target_uv","for (hidx, hseq) in enumerate(loop_seqs):
    (hseq_0, *hseq_rhseqmaining) = hseq
    pair = hseq_0
    for (pidx, l) in enumerate(pair):
        if self.select:
            l[uv_layer].select = True
        if hidx == 0 and pidx == 0 or (hidx == len(loop_seqs) - 1 and pidx == len(pair) - 1):
            continue
        tgt_noinfl = full_uvlen * (hidx + pidx) / len(loop_seqs)
        tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
        target_length = tgt_noinfl * (1 - self.mesh_infl) + tgt_infl * self.mesh_infl
        for i in range(len(accm_uvlens[:-1])):
            if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                tgt_seg_len = target_length - accm_uvlens[i]
                seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                uv1 = orig_uvs[i]
                uv2 = orig_uvs[i + 1]
                target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                break
        else:
            self.report({'ERROR'}, 'Failed to get target UV')
            return {'CANCELLED'}
        l[uv_layer].uv = target_uv",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,,
OnlyFans,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OnlyFans/modules/onlyfans.py,https://github.com/DIGITALCRIMINAL/OnlyFans/tree/master/modules/onlyfans.py,,scrape_choice$166,"def scrape_choice(authed: create_auth, subscription):
    user_id = subscription.id
    post_count = subscription.postsCount
    archived_count = subscription.archivedPostsCount
    message = ""Scrape: 0 = All | 1 = Images | 2 = Videos | 3 = Audios | 4 = Texts""
    media_types = [
        [["""", ""All""], ["""", ""Images""], ["""", ""Videos""], ["""", ""Audios""], ["""", ""Texts""]],
        message,
    ]
    choice_list = main_helper.choose_option(media_types, auto_media_choice)
    user_api = OnlyFans.endpoint_links(user_id).users
    message_api = OnlyFans.endpoint_links(user_id).message_api
    # mass_messages_api = OnlyFans.endpoint_links().mass_messages_api
    stories_api = OnlyFans.endpoint_links(user_id).stories_api
    list_highlights = OnlyFans.endpoint_links(user_id).list_highlights
    post_api = OnlyFans.endpoint_links(user_id).post_api
    archived_api = OnlyFans.endpoint_links(user_id).archived_posts
    # ARGUMENTS
    only_links = False
    mandatory = [download_directory, only_links]
    y = [""photo"", ""video"", ""stream"", ""gif"", ""audio"", ""text""]
    u_array = [
        ""You have chosen to scrape {}"",
        [user_api, media_types, *mandatory, post_count],
        ""Profile"",
    ]
    s_array = [
        ""You have chosen to scrape {}"",
        [stories_api, media_types, *mandatory, post_count],
        ""Stories"",
    ]
    h_array = [
        ""You have chosen to scrape {}"",
        [list_highlights, media_types, *mandatory, post_count],
        ""Highlights"",
    ]
    p_array = [
        ""You have chosen to scrape {}"",
        [post_api, media_types, *mandatory, post_count],
        ""Posts"",
    ]
    m_array = [
        ""You have chosen to scrape {}"",
        [message_api, media_types, *mandatory, post_count],
        ""Messages"",
    ]
    a_array = [
        ""You have chosen to scrape {}"",
        [archived_api, media_types, *mandatory, archived_count],
        ""Archived"",
    ]
    array = [u_array, s_array, p_array, a_array, m_array]
    # array = [u_array, s_array, p_array, a_array, m_array]
    # array = [s_array, h_array, p_array, a_array, m_array]
    # array = [s_array]
    # array = [u_array]
    # array = [p_array]
    # array = [a_array]
    # array = [m_array]
    new_array = []
    valid_input = True
    for xxx in array:
        if xxx[2] == ""Mass Messages"":
            if not subscription.is_me():
                continue
        new_item = dict()
        new_item[""api_message""] = xxx[0]
        new_item[""api_array""] = {}
        new_item[""api_array""][""api_link""] = xxx[1][0]
        new_item[""api_array""][""media_types""] = xxx[1][1]
        new_item[""api_array""][""directory""] = xxx[1][2]
        new_item[""api_array""][""only_links""] = xxx[1][3]
        new_item[""api_array""][""post_count""] = xxx[1][4]
        formatted = format_media_types()
        final_format = []
        for choice in choice_list:
            choice = choice[1]
            final_format.extend([result for result in formatted if result[0] == choice])
        new_item[""api_array""][""media_types""] = final_format
        new_item[""api_type""] = xxx[2]
        if valid_input:
            new_array.append(new_item)
    return new_array","for choice in choice_list:
    choice = choice[1]
    final_format.extend([result for result in formatted if result[0] == choice])","for choice in choice_list:
    (choice_0, choice_1, *choice_rchoicemaining) = choice
    choice = choice_1
    final_format.extend([result for result in formatted if result[0] == choice])",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for i in encoding:
    if i[2] < 128:
        pitch_sum[i[-1]] += i[3]
        note_cnt[i[-1]] += 1","for i in encoding:
    (_, _, i_2, i_3, *_) = i
    if i_2 < 128:
        pitch_sum[i[-1]] += i_3
        note_cnt[i[-1]] += 1",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: _, _, e_2, e_3 = e
variable mapping:
e_2: e[2]
e_3: e[3]",,,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for note in encoding:
    if note[6] != allowed_ts:
        continue
    if note[3] < 0 or note[3] > 127:
        continue
    if len(tmp):
        cur_pos = bar_to_pos[note[0]] + note[1]
        last_note = tmp[-1]
        last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
        if cur_pos - last_pos >= 0:
            tmp.append(note)
    else:
        tmp.append(note)","for note in encoding:
    (_, note_1, _, note_3, _, _, note_6, *note_rnotemaining) = note
    if note_6 != allowed_ts:
        continue
    if note_3 < 0 or note_3 > 127:
        continue
    if len(tmp):
        cur_pos = bar_to_pos[note[0]] + note_1
        last_note = tmp[-1]
        last_pos = bar_to_pos[last_note[0]] + last_note_1 + dec_dur(last_note[4])
        if cur_pos - last_pos >= 0:
            tmp.append(note)
    else:
        tmp.append(note)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: _, e_1, _, e_3, _, _, e_6, *e_remaining = e
variable mapping:
e_1: e[1]
e_3: e[3]
e_6: e[6]",,,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for enc in encoding:
    if boundry_idx >= len(boundry):
        break
    cur_pos = bar_to_pos[enc[0]] + enc[1]
    while cur_pos >= boundry[boundry_idx]:
        boundry_idx += 1
        if boundry_idx >= len(boundry):
            break
        segments.append([])
    if len(segments):
        segments[-1].append(enc)","for enc in encoding:
    (_, enc_1, *enc_rencmaining) = enc
    if boundry_idx >= len(boundry):
        break
    cur_pos = bar_to_pos[enc[0]] + enc_1
    while cur_pos >= boundry[boundry_idx]:
        boundry_idx += 1
        if boundry_idx >= len(boundry):
            break
        segments.append([])
    if len(segments):
        segments[-1].append(enc)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for note in raw_notes:
    if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
        notes_list.append([])
    notes_list[-1].append(note)
    cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])","for note in raw_notes:
    (_, note_1, *_, note_4) = note
    if len(notes_list) == 0 or bar_to_pos[note[0]] + note_1 - cur_pos > 2 * pos_resolution:
        notes_list.append([])
    notes_list[-1].append(note)
    cur_pos = bar_to_pos[note[0]] + note_1 + dec_dur(note_4)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: _, e_1, *_, e_4 = e
variable mapping:
e_1: e[1]
e_4: e[4]",,,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for notes in notes_list:
    if len(notes) < min_notes or len(notes) > max_notes:
        continue
    src_words = []
    if is_major:
        src_words.append('MAJ')
    else:
        src_words.append('MIN')
    tgt_words = []
    first_note = notes[0]
    min_bar = first_note[0]
    for (note_idx, note) in enumerate(notes):
        cur_pos = bar_to_pos[note[0]] + note[1]
        chord_idx = 2 * note[0]
        if note[1] >= 2 * pos_resolution:
            chord_idx += 1
        cur_chord = lead_chords[chord_idx]
        src_words.append(f'Chord_{cur_chord}')
        if note_idx != len(notes) - 1:
            nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
            if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                pitch_type = note[3] % 12
                if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                    src_words.append('AUT')
                else:
                    src_words.append('HALF')
            else:
                src_words.append('NOT')
        elif dec_dur(note[4]) >= pos_resolution:
            pitch_type = note[3] % 12
            if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                src_words.append('AUT')
            else:
                src_words.append('HALF')
        else:
            src_words.append('NOT')
        beat_idx = note[1] // pos_resolution
        beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
        src_words.append(f'BEAT_{beat_idx}')
        tgt_words.append(f'Bar_{note[0] - min_bar}')
        tgt_words.append(f'Pos_{note[1]}')
        tgt_words.append(f'Pitch_{note[3]}')
        tgt_words.append(f'Dur_{note[4]}')
    src_strs.append(' '.join(src_words))
    tgt_strs.append(' '.join(tgt_words))","for notes in notes_list:
    (notes_0, *notes_rnotesmaining) = notes
    if len(notes) < min_notes or len(notes) > max_notes:
        continue
    src_words = []
    if is_major:
        src_words.append('MAJ')
    else:
        src_words.append('MIN')
    tgt_words = []
    first_note = notes_0
    min_bar = first_note[0]
    for (note_idx, note) in enumerate(notes):
        cur_pos = bar_to_pos[note[0]] + note[1]
        chord_idx = 2 * note[0]
        if note[1] >= 2 * pos_resolution:
            chord_idx += 1
        cur_chord = lead_chords[chord_idx]
        src_words.append(f'Chord_{cur_chord}')
        if note_idx != len(notes) - 1:
            nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
            if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                pitch_type = note[3] % 12
                if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                    src_words.append('AUT')
                else:
                    src_words.append('HALF')
            else:
                src_words.append('NOT')
        elif dec_dur(note[4]) >= pos_resolution:
            pitch_type = note[3] % 12
            if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                src_words.append('AUT')
            else:
                src_words.append('HALF')
        else:
            src_words.append('NOT')
        beat_idx = note[1] // pos_resolution
        beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
        src_words.append(f'BEAT_{beat_idx}')
        tgt_words.append(f'Bar_{note[0] - min_bar}')
        tgt_words.append(f'Pos_{note[1]}')
        tgt_words.append(f'Pitch_{note[3]}')
        tgt_words.append(f'Dur_{note[4]}')
    src_strs.append(' '.join(src_words))
    tgt_strs.append(' '.join(tgt_words))",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
_, e_note_1, *e_note_remaining = e[note_idx + 1]
_, e_note_1_1, *e_note_1_remaining = e_note_1
variable mapping:
e_0: e[0]
e_note_1_1: e[note_idx + 1][1]
",,,,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):
    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(
        set(i.key_number for i in midi_obj.key_signature_changes))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(
        key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start)
                       for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(
        max_pos)]  # (Measure, TimeSig, Pos, Tempo)
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j))
                       for i, j in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(
                    tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            # MIDI default time signature
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)  # MIDI default tempo (BPM)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(
                j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)

    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for idx, inst in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program,
                             note.pitch + 128 if inst.is_drum else note.pitch,
                             enc_dur(max(1, time_to_pos(note.end - note.start))),
                             enc_vel(note.velocity),
                             info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *
                          math.log2((x / tot)) for x in start_distribution))

    # filter unaligned music
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(
            start_ppl)

    encoding.sort()
    encoding, is_major = normalize_to_c_major(encoding)

    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]

    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items,
                                       pos_per_chord=pos_per_chord,
                                       max_chords=max_chords,
                                       key_chord_loglik=key_chord_loglik,
                                       key_chord_transition_loglik=key_chord_transition_loglik
                                       )
    boundry = []
    chord_int = 2
    for chord_idx, chord in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(
        boundry) >= 2, f'segement must start and end in chords: {target_chords}'

    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i]
                 if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]

    # filter overlap
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + \
                last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp

    # normalize pitch
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch +
                           encoding[i][3] % 12, *encoding[i][4:])

    # infer chords for lead
    lead_notes = []
    for note in encoding:
        max_pos = max(
            max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(
                name='On',
                start=bar_to_pos[note[0]] + note[1],
                end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]),
                vel=dec_vel(note[5]),
                pitch=note[3],
                track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)

    lead_chords = infer_chords_for_sequence(lead_notes,
                                            pos_per_chord=pos_per_chord,
                                            max_chords=max_chords,
                                            key_chord_loglik=key_chord_loglik,
                                            key_chord_transition_loglik=key_chord_transition_loglik
                                            )

    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)

    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        src_strs, tgt_strs = [], []
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for note_idx, note in enumerate(notes):

                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1]
                                         [0]] + notes[note_idx+1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or
                                                                        (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or
                                                                        cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')

                    else:
                        src_words.append('NOT')
                else:
                    if dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if pitch_type == period_pitch or \
                                (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or\
                                cur_chord in target_chords:
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return src_strs, tgt_strs

    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                src_strs, tgt_strs = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue

        if (cur_len + last_len) >= target_len:
            if cur_len + last_len <= max_notes:
                src_strs, tgt_strs = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    src_strs, tgt_strs = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment

    if max_notes >= last_len >= min_notes:
        src_strs, tgt_strs = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs

    assert len(src_str_list) == len(tgt_str_list)
    return src_str_list, tgt_str_list, get_hash(encoding)","for (note_idx, note) in enumerate(notes):
    cur_pos = bar_to_pos[note[0]] + note[1]
    chord_idx = 2 * note[0]
    if note[1] >= 2 * pos_resolution:
        chord_idx += 1
    cur_chord = lead_chords[chord_idx]
    src_words.append(f'Chord_{cur_chord}')
    if note_idx != len(notes) - 1:
        nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
        if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
            pitch_type = note[3] % 12
            if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                src_words.append('AUT')
            else:
                src_words.append('HALF')
        else:
            src_words.append('NOT')
    elif dec_dur(note[4]) >= pos_resolution:
        pitch_type = note[3] % 12
        if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
            src_words.append('AUT')
        else:
            src_words.append('HALF')
    else:
        src_words.append('NOT')
    beat_idx = note[1] // pos_resolution
    beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
    src_words.append(f'BEAT_{beat_idx}')
    tgt_words.append(f'Bar_{note[0] - min_bar}')
    tgt_words.append(f'Pos_{note[1]}')
    tgt_words.append(f'Pitch_{note[3]}')
    tgt_words.append(f'Dur_{note[4]}')","for (note_idx, note) in enumerate(notes):
    (note_0, note_1, *_, note_3, note_4) = note
    cur_pos = bar_to_pos[note_0] + note_1
    chord_idx = 2 * note_0
    if note_1 >= 2 * pos_resolution:
        chord_idx += 1
    cur_chord = lead_chords[chord_idx]
    src_words.append(f'Chord_{cur_chord}')
    if note_idx != len(notes) - 1:
        nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
        if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note_4) >= pos_resolution:
            pitch_type = note_3 % 12
            if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                src_words.append('AUT')
            else:
                src_words.append('HALF')
        else:
            src_words.append('NOT')
    elif dec_dur(note_4) >= pos_resolution:
        pitch_type = note_3 % 12
        if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
            src_words.append('AUT')
        else:
            src_words.append('HALF')
    else:
        src_words.append('NOT')
    beat_idx = note_1 // pos_resolution
    beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
    src_words.append(f'BEAT_{beat_idx}')
    tgt_words.append(f'Bar_{note_0 - min_bar}')
    tgt_words.append(f'Pos_{note_1}')
    tgt_words.append(f'Pitch_{note_3}')
    tgt_words.append(f'Dur_{note_4}')",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *_, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_3: e[3]
e_4: e[4]",,,,,,,,
kivy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy/kivy/atlas.py,https://github.com/kivy/kivy/tree/master/kivy/atlas.py,Atlas,create$229,"def create(outname, filenames, size, padding=2, use_path=False):
        '''This method can be used to create an atlas manually from a set of
        images.

        :Parameters:
            `outname`: str
                Basename to use for ``.atlas`` creation and ``-<idx>.png``
                associated images.
            `filenames`: list
                List of filenames to put in the atlas.
            `size`: int or list (width, height)
                Size of the atlas image. If the size is not large enough to
                fit all of the source images, more atlas images will created
                as required.
            `padding`: int, defaults to 2
                Padding to put around each image.

                Be careful. If you're using a padding < 2, you might have
                issues with the borders of the images. Because of the OpenGL
                linearization, it might use the pixels of the adjacent image.

                If you're using a padding >= 2, we'll automatically generate a
                ""border"" of 1px around your image. If you look at
                the result, don't be scared if the image inside is not
                exactly the same as yours :).

            `use_path`: bool, defaults to False
                If True, the relative path of the source png
                file names will be included in the atlas ids rather
                that just in the file names. Leading dots and slashes will be
                excluded and all other slashes in the path will be replaced
                with underscores. For example, if `use_path` is False
                (the default) and the file name is
                ``../data/tiles/green_grass.png``, the id will be
                ``green_grass``. If `use_path` is True, it will be
                ``data_tiles_green_grass``.

            .. versionchanged:: 1.8.0
                Parameter use_path added
        '''
        # Thanks to
        # omnisaurusgames.com/2011/06/texture-atlas-generation-using-python/
        # for its initial implementation.
        try:
            from PIL import Image
        except ImportError:
            Logger.critical('Atlas: Imaging/PIL are missing')
            raise

        if isinstance(size, (tuple, list)):
            size_w, size_h = list(map(int, size))
        else:
            size_w = size_h = int(size)

        # open all of the images
        ims = list()
        for f in filenames:
            fp = open(f, 'rb')
            im = Image.open(fp)
            im.load()
            fp.close()
            ims.append((f, im))

        # sort by image area
        ims = sorted(ims, key=lambda im: im[1].size[0] * im[1].size[1],
                     reverse=True)

        # free boxes are empty space in our output image set
        # the freebox tuple format is: outidx, x, y, w, h
        freeboxes = [(0, 0, 0, size_w, size_h)]
        numoutimages = 1

        # full boxes are areas where we have placed images in the atlas
        # the full box tuple format is: image, outidx, x, y, w, h, filename
        fullboxes = []

        # do the actual atlasing by sticking the largest images we can
        # have into the smallest valid free boxes
        for imageinfo in ims:
            im = imageinfo[1]
            imw, imh = im.size
            imw += padding
            imh += padding
            if imw > size_w or imh > size_h:
                Logger.error(
                    'Atlas: image %s (%d by %d) is larger than the atlas size!'
                    % (imageinfo[0], imw, imh))
                return

            inserted = False
            while not inserted:
                for idx, fb in enumerate(freeboxes):
                    # find the smallest free box that will contain this image
                    if fb[3] >= imw and fb[4] >= imh:
                        # we found a valid spot! Remove the current
                        # freebox, and split the leftover space into (up to)
                        # two new freeboxes
                        del freeboxes[idx]
                        if fb[3] > imw:
                            freeboxes.append((
                                fb[0], fb[1] + imw, fb[2],
                                fb[3] - imw, imh))

                        if fb[4] > imh:
                            freeboxes.append((
                                fb[0], fb[1], fb[2] + imh,
                                fb[3], fb[4] - imh))

                        # keep this sorted!
                        freeboxes = sorted(freeboxes,
                                           key=lambda fb: fb[3] * fb[4])
                        fullboxes.append((im,
                                          fb[0], fb[1] + padding,
                                          fb[2] + padding, imw - padding,
                                          imh - padding, imageinfo[0]))
                        inserted = True
                        break

                if not inserted:
                    # oh crap - there isn't room in any of our free
                    # boxes, so we have to add a new output image
                    freeboxes.append((numoutimages, 0, 0, size_w, size_h))
                    numoutimages += 1

        # now that we've figured out where everything goes, make the output
        # images and blit the source images to the appropriate locations
        Logger.info('Atlas: create an {0}x{1} rgba image'.format(size_w,
                                                                 size_h))
        outimages = [Image.new('RGBA', (size_w, size_h))
                     for i in range(0, int(numoutimages))]
        for fb in fullboxes:
            x, y = fb[2], fb[3]
            out = outimages[fb[1]]
            out.paste(fb[0], (fb[2], fb[3]))
            w, h = fb[0].size
            if padding > 1:
                out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
                out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
                out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
                out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))

        # save the output images
        for idx, outimage in enumerate(outimages):
            outimage.save('%s-%d.png' % (outname, idx))

        # write out an json file that says where everything ended up
        meta = {}
        for fb in fullboxes:
            fn = '%s-%d.png' % (basename(outname), fb[1])
            if fn not in meta:
                d = meta[fn] = {}
            else:
                d = meta[fn]

            # fb[6] contain the filename
            if use_path:
                # use the path with separators replaced by _
                # example '../data/tiles/green_grass.png' becomes
                # 'data_tiles_green_grass'
                uid = splitext(fb[6])[0]
                # remove leading dots and slashes
                uid = uid.lstrip('./\\')
                # replace remaining slashes with _
                uid = uid.replace('/', '_').replace('\\', '_')
            else:
                # for example, '../data/tiles/green_grass.png'
                # just get only 'green_grass' as the uniq id.
                uid = splitext(basename(fb[6]))[0]

            x, y, w, h = fb[2:6]
            d[uid] = x, size_h - y - h, w, h

        outfn = '%s.atlas' % outname
        with open(outfn, 'w') as fd:
            json.dump(meta, fd)

        return outfn, meta","for (idx, fb) in enumerate(freeboxes):
    if fb[3] >= imw and fb[4] >= imh:
        del freeboxes[idx]
        if fb[3] > imw:
            freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
        if fb[4] > imh:
            freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
        freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
        fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
        inserted = True
        break","for (idx, fb) in enumerate(freeboxes):
    (fb_0, fb_1, fb_2, fb_3, fb_4, *_) = fb
    if fb_3 >= imw and fb_4 >= imh:
        del freeboxes[idx]
        if fb_3 > imw:
            freeboxes.append((fb_0, fb_1 + imw, fb_2, fb_3 - imw, imh))
        if fb_4 > imh:
            freeboxes.append((fb_0, fb_1, fb_2 + imh, fb_3, fb_4 - imh))
        freeboxes = sorted(freeboxes, key=lambda fb: fb_3 * fb_4)
        fullboxes.append((im, fb_0, fb_1 + padding, fb_2 + padding, imw - padding, imh - padding, imageinfo[0]))
        inserted = True
        break",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]",,,,,,,,
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/examples/pathfinding/mob_graph_test_navmesh.py,https://github.com/kidscancode/pygame_tutorials/tree/master/examples/pathfinding/mob_graph_test_navmesh.py,WeightedMesh,draw$121,"def draw(self):
        for node in self.edges.keys():
            x = int(node[0] * TILESIZE + TILESIZE / 2)
            y = int(node[1] * TILESIZE + TILESIZE / 2)
            pg.draw.circle(screen, CYAN, (x, y), 10)
            for c in self.edges[node]:
                cx = c[0] * TILESIZE + TILESIZE / 2
                cy = c[1] * TILESIZE + TILESIZE / 2
                pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)","for node in self.edges.keys():
    x = int(node[0] * TILESIZE + TILESIZE / 2)
    y = int(node[1] * TILESIZE + TILESIZE / 2)
    pg.draw.circle(screen, CYAN, (x, y), 10)
    for c in self.edges[node]:
        cx = c[0] * TILESIZE + TILESIZE / 2
        cy = c[1] * TILESIZE + TILESIZE / 2
        pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)","for node in self.edges.keys():
    (node_0, node_1, *_) = node
    x = int(node_0 * TILESIZE + TILESIZE / 2)
    y = int(node_1 * TILESIZE + TILESIZE / 2)
    pg.draw.circle(screen, CYAN, (x, y), 10)
    for c in self.edges[node]:
        cx = c[0] * TILESIZE + TILESIZE / 2
        cy = c[1] * TILESIZE + TILESIZE / 2
        pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,,
yolov5-face,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yolov5-face/test_widerface.py,https://github.com/deepcam-cn/yolov5-face/tree/master//test_widerface.py,,if_main_my$113,"if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default='runs/train/exp5/weights/last.pt', help='model.pt path(s)')
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.02, help='object confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
    parser.add_argument('--project', default='runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--save_folder', default='./widerface_evaluate/widerface_txt/', type=str, help='Dir to save txt results')
    parser.add_argument('--dataset_folder', default='../WiderFace/val/images/', type=str, help='dataset path')
    parser.add_argument('--folder_pict', default='/yolov5-face/data/widerface/val/wider_val.txt', type=str, help='folder_pict')
    opt = parser.parse_args()
    print(opt)

    # changhy : read folder_pict
    pict_folder = {}
    with open(opt.folder_pict, 'r') as f:
        lines = f.readlines()
        for line in lines:
            line = line.strip().split('/')
            pict_folder[line[-1]] = line[-2]

    # Load model
    device = select_device(opt.device)
    model = attempt_load(opt.weights, map_location=device)  # load FP32 model
    with torch.no_grad():
        # testing dataset
        testset_folder = opt.dataset_folder

        for image_path in tqdm(glob.glob(os.path.join(testset_folder, '*'))):
            if image_path.endswith('.txt'):
                continue
            img0 = cv2.imread(image_path)  # BGR
            if img0 is None:
                print(f'ignore : {image_path}')
                continue
            boxes = detect(model, img0)
            # --------------------------------------------------------------------
            image_name = os.path.basename(image_path)
            txt_name = os.path.splitext(image_name)[0] + "".txt""
            save_name = os.path.join(opt.save_folder, pict_folder[image_name], txt_name)
            dirname = os.path.dirname(save_name)
            if not os.path.isdir(dirname):
                os.makedirs(dirname)
            with open(save_name, ""w"") as fd:
                file_name = os.path.basename(save_name)[:-4] + ""\n""            
                bboxs_num = str(len(boxes)) + ""\n""
                fd.write(file_name)
                fd.write(bboxs_num)
                for box in boxes:
                    fd.write('%d %d %d %d %.03f' % (box[0], box[1], box[2], box[3], box[4] if box[4] <= 1 else 1) + '\n')
        print('done.')","for line in lines:
    line = line.strip().split('/')
    pict_folder[line[-1]] = line[-2]","for line in lines:
    (*line_rlinemaining, line_nlineg2) = line
    line = line.strip().split('/')
    pict_folder[line[-1]] = line_nlineg2",Cannot refactor,Cannot refactor,2,1,0,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg2 = e
variable mapping:
e_neg2: e[-2]",,,,,,,,
PaddleVideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleVideo/paddlevideo/utils/multigrid/multigrid.py,https://github.com/PaddlePaddle/PaddleVideo/tree/master/paddlevideo/utils/multigrid/multigrid.py,MultigridSchedule,get_long_cycle_schedule$110,"def get_long_cycle_schedule(self, cfg):
        """"""
        Based on multigrid hyperparameters, define the schedule of a long cycle.
        Args:
            cfg (configs): configs that contains training and multigrid specific
                hyperparameters.
        Returns:
            schedule (list): Specifies a list long cycle base shapes and their
                corresponding training epochs.
        """"""

        steps = cfg.OPTIMIZER.learning_rate.steps

        default_size = float(
            cfg.PIPELINE.train.decode_sampler.num_frames *
            cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size']**
            2)  # 32 * 224 * 224  C*H*W
        default_iters = steps[-1]  # 196

        # Get shapes and average batch size for each long cycle shape.
        avg_bs = []
        all_shapes = []
        #        for t_factor, s_factor in cfg.MULTIGRID.long_cycle_factors:
        for item in cfg.MULTIGRID.long_cycle_factors:
            t_factor, s_factor = item[""value""]
            base_t = int(
                round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
            base_s = int(
                round(
                    cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size']
                    * s_factor))
            if cfg.MULTIGRID.SHORT_CYCLE:
                shapes = [
                    [
                        base_t,
                        cfg.MULTIGRID.default_crop_size *
                        cfg.MULTIGRID.short_cycle_factors[0],
                    ],
                    [
                        base_t,
                        cfg.MULTIGRID.default_crop_size *
                        cfg.MULTIGRID.short_cycle_factors[1],
                    ],
                    [base_t, base_s],
                ]  #first two is short_cycle, last is the base long_cycle
            else:
                shapes = [[base_t, base_s]]

            # (T, S) -> (B, T, S)
            shapes = [[
                int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]
            ] for s in shapes]
            avg_bs.append(np.mean([s[0] for s in shapes]))
            all_shapes.append(shapes)

        # Get schedule regardless of cfg.MULTIGRID.epoch_factor.
        total_iters = 0
        schedule = []
        for step_index in range(len(steps) - 1):
            step_epochs = steps[step_index + 1] - steps[step_index]

            for long_cycle_index, shapes in enumerate(all_shapes):
                #ensure each of 4 sequences run the same num of iters
                cur_epochs = (step_epochs * avg_bs[long_cycle_index] /
                              sum(avg_bs))

                # get cur_iters from cur_epochs
                cur_iters = cur_epochs / avg_bs[long_cycle_index]
                total_iters += cur_iters
                schedule.append((step_index, shapes[-1], cur_epochs))

        iter_saving = default_iters / total_iters  # ratio between default iters and real iters

        final_step_epochs = cfg.OPTIMIZER.learning_rate.max_epoch - steps[-1]

        # We define the fine-tuning phase to have the same amount of iteration
        # saving as the rest of the training.
        #final_step_epochs / iter_saving make fine-tune having the same iters as training
        ft_epochs = final_step_epochs / iter_saving * avg_bs[-1]

        #        schedule.append((step_index + 1, all_shapes[-1][2], ft_epochs))
        schedule.append((step_index + 1, all_shapes[-1][-1], ft_epochs))

        # Obtrain final schedule given desired cfg.MULTIGRID.epoch_factor.
        x = (cfg.OPTIMIZER.learning_rate.max_epoch *
             cfg.MULTIGRID.epoch_factor / sum(s[-1] for s in schedule))

        final_schedule = []
        total_epochs = 0
        for s in schedule:
            epochs = s[2] * x
            total_epochs += epochs
            final_schedule.append((s[0], s[1], int(round(total_epochs))))
        print_schedule(final_schedule)
        return final_schedule","for (long_cycle_index, shapes) in enumerate(all_shapes):
    cur_epochs = step_epochs * avg_bs[long_cycle_index] / sum(avg_bs)
    cur_iters = cur_epochs / avg_bs[long_cycle_index]
    total_iters += cur_iters
    schedule.append((step_index, shapes[-1], cur_epochs))","for (long_cycle_index, shapes) in enumerate(all_shapes):
    (*shapes_rshapesmaining, shapes_last) = shapes
    cur_epochs = step_epochs * avg_bs[long_cycle_index] / sum(avg_bs)
    cur_iters = cur_epochs / avg_bs[long_cycle_index]
    total_iters += cur_iters
    schedule.append((step_index, shapes_last, cur_epochs))",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_last = e
variable mapping:
e_last: e[-1]",,,,,,,,
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
	dbg.createLogWindow()
	global currentArgs
	currentArgs = copy.copy(args)
	try:
		starttime = datetime.datetime.now()
		ptr_counter = 0
		
		# initialize list of commands
		commands = {}
		
		# ----- HELP ----- #
		def getBanner():
			banners = {}
			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                         __               __                      |\n""
			bannertext += ""    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n""
			bannertext += ""    |  / ___/ __ \/ ___/ _ \/ / __ `/ __ \   / __/ _ \/ __ `/ __ `__ \ |\n""
			bannertext += ""    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n""
			bannertext += ""    | \___/\____/_/   \___/_/\__,_/_/ /_/   \__/\___/\__,_/_/ /_/ /_/  |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |     https://www.corelan.be | https://www.corelan-training.com    |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[0] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""			
			bannertext += ""    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n""
			bannertext += ""    |       | '_ ` _ \  / _ \ | '_ \  / _` |   | '_ \ | | | |          |\n""
			bannertext += ""    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n""
			bannertext += ""    |       |_| |_| |_| \___/ |_| |_| \__,_|(_)| .__/  \__, |          |\n""
			bannertext += ""    |                                          |_|     |___/           |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""	
			banners[1] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |    _____ ___  ____  ____  ____ _                                 |\n""
			bannertext += ""    |    / __ `__ \/ __ \/ __ \/ __ `/  https://www.corelan.be         |\n""
			bannertext += ""    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n""
			bannertext += ""    |  /_/ /_/ /_/\____/_/ /_/\__,_/  #corelan (Freenode IRC)          |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[2] = bannertext

			bannertext = """"
			bannertext += ""\n    .##.....##..#######..##....##....###........########..##....##\n""
			bannertext += ""    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n""
			bannertext += ""    .####.####.##.....##.####..##..##...##......##.....##...####..\n""
			bannertext += ""    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n""
			bannertext += ""    .##.....##.##.....##.##..####.#########.....##...........##...\n""
			bannertext += ""    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n""
			bannertext += ""    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n""
			banners[3] = bannertext


			# pick random banner
			bannerlist = []
			for i in range (0, len(banners)):
				bannerlist.append(i)

			random.shuffle(bannerlist)
			return banners[bannerlist[0]]

		
		def procHelp(args):
			dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__,str(arch)))
			dbg.log(""     Plugin version : %s r%s"" % (__VERSION__,__REV__))
			dbg.log(""     Python version : %s"" % (getPythonVersion()))
			if __DEBUGGERAPP__ == ""WinDBG"":
				pykdversion = dbg.getPyKDVersionNr()
				dbg.log(""     PyKD version %s"" % pykdversion)
			dbg.log(""     Written by Corelan - https://www.corelan.be"")
			dbg.log(""     Project page : https://github.com/corelan/mona"")
			dbg.logLines(getBanner(),highlight=1)
			dbg.log(""Global options :"")
			dbg.log(""----------------"")
			dbg.log(""You can use one or more of the following global options on any command that will perform"")
			dbg.log(""a search in one or more modules, returning a list of pointers :"")
			dbg.log("" -n                     : Skip modules that start with a null byte. If this is too broad, use"")
			dbg.log(""                          option -cp nonull instead"")
			dbg.log("" -o                     : Ignore OS modules"")
			dbg.log("" -p <nr>                : Stop search after <nr> pointers."")
			dbg.log("" -m <module,module,...> : only query the given modules. Be sure what you are doing !"")
			dbg.log(""                          You can specify multiple modules (comma separated)"")
			dbg.log(""                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored"")
			dbg.log(""                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,"")
			dbg.log(""                          blah or *blah* = contains blah"")
			dbg.log("" -cm <crit,crit,...>    : Apply some additional criteria to the modules to query."")
			dbg.log(""                          You can use one or more of the following criteria :"")
			dbg.log(""                          aslr,safeseh,rebase,nx,os"")
			dbg.log(""                          You can enable or disable a certain criterium by setting it to true or false"")
			dbg.log(""                          Example :  -cm aslr=true,safeseh=false"")
			dbg.log(""                          Suppose you want to search for p/p/r in aslr enabled modules, you could call"")
			dbg.log(""                          !mona seh -cm aslr"")
			dbg.log("" -cp <crit,crit,...>    : Apply some criteria to the pointers to return"")
			dbg.log(""                          Available options are :"")
			dbg.log(""                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev"")
			dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
			dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
			dbg.log(""                          You can use .. to indicate a range of bytes (in between 2 bad chars)"")
			dbg.log("" -x <access>            : Specify desired access level of the returning pointers. If not specified,"")
			dbg.log(""                          only executable pointers will be returned."")
			dbg.log(""                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *"")
			
			if not args:
				args = []
			if len(args) > 1:
				thiscmd = args[1].lower().strip()
				if thiscmd in commands:
					dbg.log("""")
					dbg.log(""Usage of command '%s' :"" % thiscmd)
					dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
					dbg.logLines(commands[thiscmd].usage)
					dbg.log("""")
				else:
					aliasfound = False
					for cmd in commands:
						if commands[cmd].alias == thiscmd:
							dbg.log("""")
							dbg.log(""Usage of command '%s' :"" % thiscmd)
							dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
							dbg.logLines(commands[cmd].usage)
							dbg.log("""")
							aliasfound = True
					if not aliasfound:
						dbg.logLines(""\nCommand %s does not exist. Run !mona to get a list of available commands\n"" % thiscmd,highlight=1)
			else:
				dbg.logLines(""\nUsage :"")
				dbg.logLines(""-------\n"")
				dbg.log("" !mona <command> <parameter>"")
				dbg.logLines(""\nAvailable commands and parameters :\n"")

				items = commands.items()
				items.sort(key = itemgetter(0))
				for item in items:
					if commands[item[0]].usage != """":
						aliastxt = """"
						if commands[item[0]].alias != """":
							aliastxt = "" / "" + commands[item[0]].alias
						dbg.logLines(""%s | %s"" % (item[0] + aliastxt + ("" "" * (20 - len(item[0]+aliastxt))), commands[item[0]].description))
				dbg.log("""")
				dbg.log(""Want more info about a given command ?  Run !mona help <command>"",highlight=1)
				dbg.log("""")
		
		commands[""help""] = MnCommand(""help"", ""show help"", ""!mona help [command]"",procHelp)
		
		# ----- Config file management ----- #
		
		def procConfig(args):
			#did we specify -get, -set or -add?
			showerror = False
			if not ""set"" in args and not ""get"" in args and not ""add"" in args:
				showerror = True
				
			if ""set"" in args:
				if type(args[""set""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""set""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""add"" in args:
				if type(args[""add""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""add""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""get"" in args:
				if type(args[""get""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""get""].split("" "")
					if len(params) < 1:
						showerror = True
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(configUsage,highlight=1)
				return
			else:
				if ""get"" in args:
					dbg.log(""Reading value from configuration file"")
					monaConfig = MnConfig()
					thevalue = monaConfig.get(args[""get""])
					dbg.log(""Parameter %s = %s"" % (args[""get""],thevalue))
				
				if ""set"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""set""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = args[""set""][0+len(configparam):len(args[""set""])]
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
				if ""add"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""add""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = monaConfig.get(configparam).strip() + "","" + args[""add""][0+len(configparam):len(args[""add""])].strip()
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
		# ----- Jump to register ----- #
	
		def procFindJ(args):
			return procFindJMP(args)
		
		def procFindJMP(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			
			if (inspect.stack()[1][3] == ""procFindJ""):
				dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."",highlight=1)

			criteria={}
			all_opcodes={}
			
			global ptr_to_get
			ptr_to_get = -1
			
			distancestr = """"
			mindistance = 0
			maxdistance = 0
			
			#did user specify -r <reg> ?
			showerror = False
			if ""r"" in args:
				if type(args[""r""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#valid register ?
					thisreg = args[""r""].upper().strip()
					validregs = dbglib.Registers32BitsOrder
					if not thisreg in validregs:
						showerror = True
			else:
				showerror = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0		
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0						
			
			if maxdistance < mindistance:
				tmp = maxdistance
				maxdistance = mindistance
				mindistance = tmp
			
			criteria[""mindistance""] = mindistance
			criteria[""maxdistance""] = maxdistance
			
			
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(jmpUsage,highlight=1)
				return				
			else:
				modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
				# go for it !	
				all_opcodes=findJMP(modulecriteria,criteria,args[""r""].lower().strip())
			
			# write to log
			logfile = MnLog(""jmp.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog)
		
		# ----- Exception Handler Overwrites ----- #
		
					
		def procFindSEH(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""safeseh""] = False
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False

			criteria = {}
			specialcases = {}
			all_opcodes = {}
			
			global ptr_to_get
			ptr_to_get = -1
			
			#what is the caller function (backwards compatibility with pvefindaddr)
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if ""rop"" in args:
				criteria[""rop""] = True
			
			if ""all"" in args:
				criteria[""all""] = True
				specialcases[""maponly""] = True
			else:
				criteria[""all""] = False
				specialcases[""maponly""] = False
			
			# go for it !	
			all_opcodes = findSEH(modulecriteria,criteria)
			#report findings to log
			logfile = MnLog(""seh.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog,specialcases)
			
			
		# ----- MODULES ------ #
		def procShowMODULES(args):
			modulecriteria={}
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			modulestosearch = getModulesToQuery(modulecriteria)
			showModuleTable("""",modulestosearch)

		# ----- ROP ----- #
		def procFindROPFUNC(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			#modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			ropfuncs = {}
			ropfuncoffsets ={}
			ropfuncs,ropfuncoffsets = findROPFUNC(modulecriteria,criteria)
			#report findings to log
			dbg.log(""[+] Processing pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc.txt"")
			thislog = logfile.reset()
			processResults(ropfuncs,logfile,thislog)
			global silent
			silent = True
			dbg.log(""[+] Processing offsets to pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc_offset.txt"")
			thislog = logfile.reset()
			processResults(ropfuncoffsets,logfile,thislog)			
			
		def procStackPivots(args):
			procROP(args,""stackpivot"")
			
		def procROP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			maxoffset = 40
			thedistance = 8
			split = False
			fast = False
			sortedprint = False
			endingstr = """"
			endings = []
			technique = """"            
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass
			
			if ""offset"" in args:
				if type(args[""offset""]).__name__.lower() != ""bool"":
					try:
						maxoffset = int(args[""offset""])
					except:
						pass
			
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() != ""bool"":
					try:
						thedistance = args[""distance""]
					except:
						pass
			
			if ""split"" in args:
				if type(args[""split""]).__name__.lower() == ""bool"":
					split = args[""split""]

			if ""s"" in args:
				if type(args[""s""]).__name__.lower() != ""bool"":
					technique = args[""s""].replace(""'"","""").replace('""',"""").strip().lower()                   
					
			if ""fast"" in args:
				if type(args[""fast""]).__name__.lower() == ""bool"":
					fast = args[""fast""]
			
			if ""end"" in args:
				if type(args[""end""]).__name__.lower() == ""str"":
					endingstr = args[""end""].replace(""'"","""").replace('""',"""").strip()
					endings = endingstr.split(""#"")
					
			if ""f"" in args:
				if args[""f""] != """":
					criteria[""f""] = args[""f""]
			
			if ""sort"" in args:
				sortedprint = True
			
			if ""rva"" in args:
				criteria[""rva""] = True
			
			if mode == ""stackpivot"":
				fast = False
				endings = """"
				split = False
			else:
				mode = ""all""
			
			findROPGADGETS(modulecriteria,criteria,endings,maxoffset,depth,split,thedistance,fast,mode,sortedprint,technique)
			

		def procJseh(args):
			results = []
			showred=0
			showall=False
			if ""all"" in args:
				showall = True
			nrfound = 0
			dbg.log(""-----------------------------------------------------------------------"")
			dbg.log(""Search for jmp/call dword[ebp/esp+nn] (and other) combinations started "")
			dbg.log(""-----------------------------------------------------------------------"")
			opcodej=[""\xff\x54\x24\x08"", #call dword ptr [esp+08]
					""\xff\x64\x24\x08"", #jmp dword ptr [esp+08]
					""\xff\x54\x24\x14"", #call dword ptr [esp+14]
					""\xff\x54\x24\x14"", #jmp dword ptr [esp+14]
					""\xff\x54\x24\x1c"", #call dword ptr [esp+1c]
					""\xff\x54\x24\x1c"", #jmp dword ptr [esp+1c]
					""\xff\x54\x24\x2c"", #call dword ptr [esp+2c]
					""\xff\x54\x24\x2c"", #jmp dword ptr [esp+2c]
					""\xff\x54\x24\x44"", #call dword ptr [esp+44]
					""\xff\x54\x24\x44"", #jmp dword ptr [esp+44]
					""\xff\x54\x24\x50"", #call dword ptr [esp+50]
					""\xff\x54\x24\x50"", #jmp dword ptr [esp+50]
					""\xff\x55\x0c"",     #call dword ptr [ebp+0c]
					""\xff\x65\x0c"",     #jmp dword ptr [ebp+0c]
					""\xff\x55\x24"",     #call dword ptr [ebp+24]
					""\xff\x65\x24"",     #jmp dword ptr [ebp+24]
					""\xff\x55\x30"",     #call dword ptr [ebp+30]
					""\xff\x65\x30"",     #jmp dword ptr [ebp+30]
					""\xff\x55\xfc"",     #call dword ptr [ebp-04]
					""\xff\x65\xfc"",     #jmp dword ptr [ebp-04]
					""\xff\x55\xf4"",     #call dword ptr [ebp-0c]
					""\xff\x65\xf4"",     #jmp dword ptr [ebp-0c]
					""\xff\x55\xe8"",     #call dword ptr [ebp-18]
					""\xff\x65\xe8"",     #jmp dword ptr [ebp-18]
					""\x83\xc4\x08\xc3"", #add esp,8 + ret
					""\x83\xc4\x08\xc2""] #add esp,8 + ret X
			fakeptrcriteria = {}
			fakeptrcriteria[""accesslevel""] = ""*""
			for opjc in opcodej:
				addys = []
				addys = searchInRange( [[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
				results += addys
				for ptrtypes in addys:
					for ad1 in addys[ptrtypes]:
						ptr = MnPointer(ad1)
						module = ptr.belongsTo()
						if not module:
							module=""""
							page   = dbg.getMemoryPageByAddress( ad1 )
							access = page.getAccess( human = True )
							op = dbg.disasm( ad1 )
							opstring=op.getDisasm()
							dbg.log(""Found %s at 0x%08x - Access: (%s) - Outside of a loaded module"" % (opstring, ad1, access), address = ad1,highlight=1)
							nrfound+=1
						else:
							if showall:
								page   = dbg.getMemoryPageByAddress( ad1 )
								access = page.getAccess( human = True )
								op = dbg.disasm( ad1 )
								opstring=op.getDisasm()
								thismod = MnModule(module)
								if not thismod.isSafeSEH:
								#if ismodulenosafeseh(module[0])==1:
									extratext=""=== Safeseh : NO ===""
									showred=1
								else:
									extratext=""Safeseh protected""
									showred=0
								dbg.log(""Found %s at 0x%08x (%s) - Access: (%s) - %s"" % (opstring, ad1, module,access,extratext), address = ad1,highlight=showred)
								nrfound+=1
			dbg.log(""Search complete"")
			if results:
				dbg.log(""Found %d address(es)"" % nrfound)
				return ""Found %d address(es) (Check the log Windows for details)"" % nrfound
			else:
				dbg.log(""No addresses found"")
				return ""Sorry, no addresses found""

			
		def procJOP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass			
			findJOPGADGETS(modulecriteria,criteria,depth)			
			
			
		def procCreatePATTERN(args):
			size = 0
			pattern = """"
			if ""?"" in args and args[""?""] != """":
				try:
					if ""0x"" in args[""?""].lower():
						try:
							size = int(args[""?""],16)
						except:
							size = 0
					else:
						size = int(args[""?""])
				except:
					size = 0
			if size == 0:
				dbg.log(""Please enter a valid size"",highlight=1)
			else:
				pattern = createPattern(size,args)
				dbg.log(""Creating cyclic pattern of %d bytes"" % size)				
				dbg.log(pattern)
				global ignoremodules
				ignoremodules = True
				objpatternfile = MnLog(""pattern.txt"")
				patternfile = objpatternfile.reset()
				# ASCII
				objpatternfile.write(""\nPattern of "" + str(size) + "" bytes :\n"",patternfile)
				objpatternfile.write(""-"" * (19 + len(str(size))),patternfile)
				objpatternfile.write(""\nASCII:"",patternfile)
				objpatternfile.write(""\n"" + pattern,patternfile)
				# Hex
				patternhex = """"
				for patternchar in pattern:
					patternhex += str(hex(ord(patternchar))).replace(""0x"",""\\x"")
				objpatternfile.write(""\n\nHEX:\n"",patternfile)
				objpatternfile.write(patternhex,patternfile)
				# Javascript
				patternjs = str2js(pattern)
				objpatternfile.write(""\n\nJAVASCRIPT (unescape() friendly):\n"",patternfile)
				objpatternfile.write(patternjs,patternfile)
				if not silent:
					dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"",highlight=1)
					dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile,highlight=1)
				
				ignoremodules = False
			return


		def procOffsetPATTERN(args):
			egg = """"
			if ""?"" in args and args[""?""] != """":
				try:
					egg = args[""?""]
				except:
					egg = """"
			if egg == """":
				dbg.log(""Please enter a valid target"",highlight=1)
			else:
				findOffsetInPattern(egg,-1,args)
			return
		
		# ----- Comparing file output ----- #
		def procFileCOMPARE(args):
			modulecriteria={}
			criteria={}
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			allfiles=[]
			tomatch=""""
			checkstrict=True
			rangeval = 0
			fast = False
			if ""ptronly"" in args or ""ptrsonly"" in args:
				fast = True
			if ""f"" in args:
				if args[""f""] != """":
					rawfilenames=args[""f""].replace('""',"""")
					allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
					dbg.log(""[+] Number of files to be examined : %d "" % len(allfiles))
			if ""range"" in args:
				if not type(args[""range""]).__name__.lower() == ""bool"":
					strrange = args[""range""].lower()
					if strrange.startswith(""0x"") and len(strrange) > 2 :
						rangeval = int(strrange,16)
					else:
						try:
							rangeval = int(args[""range""])
						except:
							rangeval = 0
					if rangeval > 0:
						dbg.log(""[+] Find overlap using pointer +/- range, value %d"" % rangeval)
						dbg.log(""    Note : this will significantly slow down the comparison process !"")
				else:
					dbg.log(""Please provide a numeric value ^(> 0) with option -range"",highlight=1)
					return
			else:
				if ""contains"" in args:
					if type(args[""contains""]).__name__.lower() == ""str"":
						tomatch = args[""contains""].replace(""'"","""").replace('""',"""")
				if ""nostrict"" in args:
					if type(args[""nostrict""]).__name__.lower() == ""bool"":
						checkstrict = not args[""nostrict""]
						dbg.log(""[+] Instructions must match in all files ? %s"" % checkstrict)
			# maybe one of the arguments is a folder
			callfiles = allfiles
			allfiles = []
			for tfile in callfiles:
				if os.path.isdir(tfile):
					# folder, get all files from this folder
					for root,dirs,files in os.walk(tfile):
						for dfile in files:
							allfiles.append(os.path.join(root,dfile))
				else:
					allfiles.append(tfile)
			if len(allfiles) > 1:
				findFILECOMPARISON(modulecriteria,criteria,allfiles,tomatch,checkstrict,rangeval,fast)
			else:
				dbg.log(""Please specify at least 2 filenames to compare"",highlight=1)

		# ----- Find bytes in memory ----- #
		def procFind(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			base = 0
			offset = 0
			top  = TOP_USERLAND
			consecutive = False
			ftype = """"
			
			level = 0
			offsetlevel = 0			
			
			if not ""a"" in args:
				args[""a""] = ""*""

			ptronly = False

			if ""ptronly"" in args or ""ptrsonly"" in args:
				ptronly = True	
			
			#search for all pointers by default
			if not ""x"" in args:
				args[""x""] = ""*""
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			if criteria[""accesslevel""] == """":
				return
			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			if ""unicode"" in args:
				criteria[""unic""] = True

			if ""b"" in args:
				try:
					base = int(args[""b""],16)
				except:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return
			if ""t"" in args:
				try:
					top = int(args[""t""],16)
				except:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
			if ""offset"" in args:
				if not args[""offset""].__class__.__name__ == ""bool"":
					if ""0x"" in args[""offset""].lower():
						try:
							offset = 0 - int(args[""offset""],16)
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return
					else:	
						try:
							offset = 0 - int(args[""offset""])
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return	
				else:
					dbg.log(""invalid offset value"",highlight=1)
					return
					
			if ""level"" in args:
				try:
					level = int(args[""level""])
				except:
					dbg.log(""invalid level value"",highlight=1)
					return

			if ""offsetlevel"" in args:
				try:
					offsetlevel = int(args[""offsetlevel""])
				except:
					dbg.log(""invalid offsetlevel value"",highlight=1)
					return						
					
			if ""c"" in args:
				dbg.log(""    - Skipping consecutive pointers, showing size instead"")			
				consecutive = True
				
			if ""type"" in args:
				if not args[""type""] in [""bin"",""asc"",""ptr"",""instr"",""file""]:
					dbg.log(""Invalid search type : %s"" % args[""type""], highlight=1)
					return
				ftype = args[""type""] 
				if ftype == ""file"":
					filename = args[""s""].replace('""',"""").replace(""'"","""")
					#see if we can read the file
					if not os.path.isfile(filename):
						dbg.log(""Unable to find/read file %s"" % filename,highlight=1)
						return
			rangep2p = 0

			
			if ""p2p"" in args or level > 0:
				dbg.log(""    - Looking for pointers to pointers"")
				criteria[""p2p""] = True
				if ""r"" in args:	
					try:
						rangep2p = int(args[""r""])
					except:
						pass
					if rangep2p > 0:
						dbg.log(""    - Will search for close pointers (%d bytes backwards)"" % rangep2p)
				if ""p2p"" in args:
					level = 1
			
			
			if level > 0:
				dbg.log(""    - Recursive levels : %d"" % level)
			

			allpointers = findPattern(modulecriteria,criteria,pattern,ftype,base,top,consecutive,rangep2p,level,offset,offsetlevel)
				
			logfile = MnLog(""find.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog,{},ptronly)
			return
			
			
		# ---- Find instructions, wildcard search ----- #
		def procFindWild(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			patterntype = """"
			base = 0
			top  = TOP_USERLAND
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			patterntypes = [""bin"",""str""]
			if ""type"" in args:
				if type(args[""type""]).__name__.lower() != ""bool"":
					if args[""type""] in patterntypes:
						patterntype = args[""type""]
					else:
						dbg.log(""-type argument only takes one of these values: %s"" % patterntypes,highlight=1)
						return
				else:
					dbg.log(""Please specify a valid value for -type. Valid values are %s"" % patterntypes,highlight=1)
					return


			if patterntype == """":
				if ""\\x"" in pattern:
					patterntype = ""bin""
				else:
					patterntype = ""str""
			
			if ""b"" in args:
				base,addyok = getAddyArg(args[""b""])
				if not addyok:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return

			if ""t"" in args:
				top,addyok = getAddyArg(args[""t""])
				if not addyok:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
					
			if ""depth"" in args:
				try:
					criteria[""depth""] = int(args[""depth""])
				except:
					dbg.log(""invalid depth value"",highlight=1)
					return	

			if ""all"" in args:
				criteria[""all""] = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					dbg.log(""invalid distance value(s)"",highlight=1)
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0	
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0	
			
				if maxdistance < mindistance:
					tmp = maxdistance
					maxdistance = mindistance
					mindistance = tmp
				
				criteria[""mindistance""] = mindistance
				criteria[""maxdistance""] = maxdistance
						
			allpointers = findPatternWild(modulecriteria,criteria,pattern,base,top,patterntype)
				
			logfile = MnLog(""findwild.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog)		
			return
	
			
		# ----- assemble: assemble instructions to opcodes ----- #
		def procAssemble(args):
			opcodes = """"
			encoder = """"
			
			if not 's' in args:
				dbg.log(""Mandatory argument -s <opcodes> missing"", highlight=1)
				return
			opcodes = args['s']
			
			if 'e' in args:
				# TODO: implement encoder support
				dbg.log(""Encoder support not yet implemented"", highlight=1)
				return
				encoder = args['e'].lowercase()
				if encoder not in [""ascii""]:
					dbg.log(""Invalid encoder : %s"" % encoder, highlight=1)
					return
			
			assemble(opcodes,encoder)
			
		# ----- info: show information about an address ----- #
		def procInfo(args):
			if not ""a"" in args:
				dbg.log(""Missing mandatory argument -a"", highlight=1)
				return
			
			address,addyok = getAddyArg(args[""a""])
			if not addyok:
				dbg.log(""%s is an invalid address"" % args[""a""], highlight=1)
				return
			
			ptr = MnPointer(address)
			modname = ptr.belongsTo()
			modinfo = None
			if modname != """":
				modinfo = MnModule(modname)
			rebase = """"
			rva=0
			if modinfo :
				rva = address - modinfo.moduleBase
			procFlags(args)
			dbg.log("""")			
			dbg.log(""[+] Information about address 0x%s"" % toHex(address))
			dbg.log(""    %s"" % ptr.__str__())
			thepage = dbg.getMemoryPageByAddress(address)
			dbg.log(""    Address is part of page 0x%08x - 0x%08x"" % (thepage.getBaseAddress(),thepage.getBaseAddress()+thepage.getSize()))
			section = """"
			try:
				section = thepage.getSection()
			except:
				section = """"
			if section != """":
				dbg.log(""    Section : %s"" % section)
			
			if ptr.isOnStack():
				stacks = getStacks()
				stackref = """"
				for tid in stacks:
					currstack = stacks[tid]
					if currstack[0] <= address and address <= currstack[1]:
						stackref = "" (Thread 0x%08x, Stack Base : 0x%08x, Stack Top : 0x%08x)"" % (tid,currstack[0],currstack[1])
						break
				dbg.log(""    This address is in a stack segment %s"" % stackref)
			if modinfo:
				dbg.log(""    Address is part of a module:"")
				dbg.log(""    %s"" % modinfo.__str__())
				if rva != 0:
					dbg.log(""    Offset from module base: 0x%x"" % rva)
					if modinfo:
						eatlist = modinfo.getEAT()
						if address in eatlist:
							dbg.log(""    Address is start of function '%s' in %s"" % (eatlist[address],modname))
						else:
							iatlist = modinfo.getIAT()
							if address in iatlist:
								iatentry = iatlist[address]
								dbg.log(""    Address is part of IAT, and contains pointer to '%s'"" % iatentry)				
			else:
				output = """"
				if ptr.isInHeap():
					dbg.log(""    This address resides in the heap"")
					dbg.log("""")
					ptr.showHeapBlockInfo()
				else:
					dbg.log(""    Module: None"")					
			try:
				dbg.log("""")
				dbg.log(""[+] Disassembly:"")
				op = dbg.disasm(address)
				opstring=getDisasmInstruction(op)
				dbg.log(""    Instruction at %s : %s"" % (toHex(address),opstring))
			except:
				pass
			if __DEBUGGERAPP__ == ""WinDBG"":
				dbg.log("""")
				dbg.log(""Output of !address 0x%08x:"" % address)
				output = dbg.nativeCommand(""!address 0x%08","for word in arguments:
    if word[0] == '-':
        word = word.lstrip('-')
        opts[word] = True
        last = word
    elif last != '':
        if str(opts[last]) == 'True':
            opts[last] = word
        else:
            opts[last] = opts[last] + ' ' + word","for word in arguments:
    (word_0, *word_rwordmaining) = word
    if word_0 == '-':
        word = word.lstrip('-')
        opts[word] = True
        last = word
    elif last != '':
        if str(opts[last]) == 'True':
            opts[last] = word
        else:
            opts[last] = opts[last] + ' ' + word",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,,
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
	dbg.createLogWindow()
	global currentArgs
	currentArgs = copy.copy(args)
	try:
		starttime = datetime.datetime.now()
		ptr_counter = 0
		
		# initialize list of commands
		commands = {}
		
		# ----- HELP ----- #
		def getBanner():
			banners = {}
			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                         __               __                      |\n""
			bannertext += ""    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n""
			bannertext += ""    |  / ___/ __ \/ ___/ _ \/ / __ `/ __ \   / __/ _ \/ __ `/ __ `__ \ |\n""
			bannertext += ""    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n""
			bannertext += ""    | \___/\____/_/   \___/_/\__,_/_/ /_/   \__/\___/\__,_/_/ /_/ /_/  |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |     https://www.corelan.be | https://www.corelan-training.com    |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[0] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""			
			bannertext += ""    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n""
			bannertext += ""    |       | '_ ` _ \  / _ \ | '_ \  / _` |   | '_ \ | | | |          |\n""
			bannertext += ""    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n""
			bannertext += ""    |       |_| |_| |_| \___/ |_| |_| \__,_|(_)| .__/  \__, |          |\n""
			bannertext += ""    |                                          |_|     |___/           |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""	
			banners[1] = bannertext

			bannertext = """"
			bannertext += ""    |------------------------------------------------------------------|\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |    _____ ___  ____  ____  ____ _                                 |\n""
			bannertext += ""    |    / __ `__ \/ __ \/ __ \/ __ `/  https://www.corelan.be         |\n""
			bannertext += ""    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n""
			bannertext += ""    |  /_/ /_/ /_/\____/_/ /_/\__,_/  #corelan (Freenode IRC)          |\n""
			bannertext += ""    |                                                                  |\n""
			bannertext += ""    |------------------------------------------------------------------|\n""
			banners[2] = bannertext

			bannertext = """"
			bannertext += ""\n    .##.....##..#######..##....##....###........########..##....##\n""
			bannertext += ""    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n""
			bannertext += ""    .####.####.##.....##.####..##..##...##......##.....##...####..\n""
			bannertext += ""    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n""
			bannertext += ""    .##.....##.##.....##.##..####.#########.....##...........##...\n""
			bannertext += ""    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n""
			bannertext += ""    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n""
			banners[3] = bannertext


			# pick random banner
			bannerlist = []
			for i in range (0, len(banners)):
				bannerlist.append(i)

			random.shuffle(bannerlist)
			return banners[bannerlist[0]]

		
		def procHelp(args):
			dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__,str(arch)))
			dbg.log(""     Plugin version : %s r%s"" % (__VERSION__,__REV__))
			dbg.log(""     Python version : %s"" % (getPythonVersion()))
			if __DEBUGGERAPP__ == ""WinDBG"":
				pykdversion = dbg.getPyKDVersionNr()
				dbg.log(""     PyKD version %s"" % pykdversion)
			dbg.log(""     Written by Corelan - https://www.corelan.be"")
			dbg.log(""     Project page : https://github.com/corelan/mona"")
			dbg.logLines(getBanner(),highlight=1)
			dbg.log(""Global options :"")
			dbg.log(""----------------"")
			dbg.log(""You can use one or more of the following global options on any command that will perform"")
			dbg.log(""a search in one or more modules, returning a list of pointers :"")
			dbg.log("" -n                     : Skip modules that start with a null byte. If this is too broad, use"")
			dbg.log(""                          option -cp nonull instead"")
			dbg.log("" -o                     : Ignore OS modules"")
			dbg.log("" -p <nr>                : Stop search after <nr> pointers."")
			dbg.log("" -m <module,module,...> : only query the given modules. Be sure what you are doing !"")
			dbg.log(""                          You can specify multiple modules (comma separated)"")
			dbg.log(""                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored"")
			dbg.log(""                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,"")
			dbg.log(""                          blah or *blah* = contains blah"")
			dbg.log("" -cm <crit,crit,...>    : Apply some additional criteria to the modules to query."")
			dbg.log(""                          You can use one or more of the following criteria :"")
			dbg.log(""                          aslr,safeseh,rebase,nx,os"")
			dbg.log(""                          You can enable or disable a certain criterium by setting it to true or false"")
			dbg.log(""                          Example :  -cm aslr=true,safeseh=false"")
			dbg.log(""                          Suppose you want to search for p/p/r in aslr enabled modules, you could call"")
			dbg.log(""                          !mona seh -cm aslr"")
			dbg.log("" -cp <crit,crit,...>    : Apply some criteria to the pointers to return"")
			dbg.log(""                          Available options are :"")
			dbg.log(""                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev"")
			dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
			dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
			dbg.log(""                          You can use .. to indicate a range of bytes (in between 2 bad chars)"")
			dbg.log("" -x <access>            : Specify desired access level of the returning pointers. If not specified,"")
			dbg.log(""                          only executable pointers will be returned."")
			dbg.log(""                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *"")
			
			if not args:
				args = []
			if len(args) > 1:
				thiscmd = args[1].lower().strip()
				if thiscmd in commands:
					dbg.log("""")
					dbg.log(""Usage of command '%s' :"" % thiscmd)
					dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
					dbg.logLines(commands[thiscmd].usage)
					dbg.log("""")
				else:
					aliasfound = False
					for cmd in commands:
						if commands[cmd].alias == thiscmd:
							dbg.log("""")
							dbg.log(""Usage of command '%s' :"" % thiscmd)
							dbg.log(""%s"" % (""-"" * (22 + len(thiscmd))))
							dbg.logLines(commands[cmd].usage)
							dbg.log("""")
							aliasfound = True
					if not aliasfound:
						dbg.logLines(""\nCommand %s does not exist. Run !mona to get a list of available commands\n"" % thiscmd,highlight=1)
			else:
				dbg.logLines(""\nUsage :"")
				dbg.logLines(""-------\n"")
				dbg.log("" !mona <command> <parameter>"")
				dbg.logLines(""\nAvailable commands and parameters :\n"")

				items = commands.items()
				items.sort(key = itemgetter(0))
				for item in items:
					if commands[item[0]].usage != """":
						aliastxt = """"
						if commands[item[0]].alias != """":
							aliastxt = "" / "" + commands[item[0]].alias
						dbg.logLines(""%s | %s"" % (item[0] + aliastxt + ("" "" * (20 - len(item[0]+aliastxt))), commands[item[0]].description))
				dbg.log("""")
				dbg.log(""Want more info about a given command ?  Run !mona help <command>"",highlight=1)
				dbg.log("""")
		
		commands[""help""] = MnCommand(""help"", ""show help"", ""!mona help [command]"",procHelp)
		
		# ----- Config file management ----- #
		
		def procConfig(args):
			#did we specify -get, -set or -add?
			showerror = False
			if not ""set"" in args and not ""get"" in args and not ""add"" in args:
				showerror = True
				
			if ""set"" in args:
				if type(args[""set""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""set""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""add"" in args:
				if type(args[""add""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""add""].split("" "")
					if len(params) < 2:
						showerror = True
			if ""get"" in args:
				if type(args[""get""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#count nr of words
					params = args[""get""].split("" "")
					if len(params) < 1:
						showerror = True
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(configUsage,highlight=1)
				return
			else:
				if ""get"" in args:
					dbg.log(""Reading value from configuration file"")
					monaConfig = MnConfig()
					thevalue = monaConfig.get(args[""get""])
					dbg.log(""Parameter %s = %s"" % (args[""get""],thevalue))
				
				if ""set"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""set""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = args[""set""][0+len(configparam):len(args[""set""])]
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
				if ""add"" in args:
					dbg.log(""Writing value to configuration file"")
					monaConfig = MnConfig()
					value = args[""add""].split("" "")
					configparam = value[0].strip()
					dbg.log(""Old value of parameter %s = %s"" % (configparam,monaConfig.get(configparam)))
					configvalue = monaConfig.get(configparam).strip() + "","" + args[""add""][0+len(configparam):len(args[""add""])].strip()
					monaConfig.set(configparam,configvalue)
					dbg.log(""New value of parameter %s = %s"" % (configparam,configvalue))
				
		# ----- Jump to register ----- #
	
		def procFindJ(args):
			return procFindJMP(args)
		
		def procFindJMP(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			
			if (inspect.stack()[1][3] == ""procFindJ""):
				dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."",highlight=1)

			criteria={}
			all_opcodes={}
			
			global ptr_to_get
			ptr_to_get = -1
			
			distancestr = """"
			mindistance = 0
			maxdistance = 0
			
			#did user specify -r <reg> ?
			showerror = False
			if ""r"" in args:
				if type(args[""r""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					#valid register ?
					thisreg = args[""r""].upper().strip()
					validregs = dbglib.Registers32BitsOrder
					if not thisreg in validregs:
						showerror = True
			else:
				showerror = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					showerror = True
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0		
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0						
			
			if maxdistance < mindistance:
				tmp = maxdistance
				maxdistance = mindistance
				mindistance = tmp
			
			criteria[""mindistance""] = mindistance
			criteria[""maxdistance""] = maxdistance
			
			
			if showerror:
				dbg.log(""Usage :"")
				dbg.logLines(jmpUsage,highlight=1)
				return				
			else:
				modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
				# go for it !	
				all_opcodes=findJMP(modulecriteria,criteria,args[""r""].lower().strip())
			
			# write to log
			logfile = MnLog(""jmp.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog)
		
		# ----- Exception Handler Overwrites ----- #
		
					
		def procFindSEH(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""safeseh""] = False
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False

			criteria = {}
			specialcases = {}
			all_opcodes = {}
			
			global ptr_to_get
			ptr_to_get = -1
			
			#what is the caller function (backwards compatibility with pvefindaddr)
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if ""rop"" in args:
				criteria[""rop""] = True
			
			if ""all"" in args:
				criteria[""all""] = True
				specialcases[""maponly""] = True
			else:
				criteria[""all""] = False
				specialcases[""maponly""] = False
			
			# go for it !	
			all_opcodes = findSEH(modulecriteria,criteria)
			#report findings to log
			logfile = MnLog(""seh.txt"")
			thislog = logfile.reset()
			processResults(all_opcodes,logfile,thislog,specialcases)
			
			
		# ----- MODULES ------ #
		def procShowMODULES(args):
			modulecriteria={}
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			modulestosearch = getModulesToQuery(modulecriteria)
			showModuleTable("""",modulestosearch)

		# ----- ROP ----- #
		def procFindROPFUNC(args):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			#modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False
			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			ropfuncs = {}
			ropfuncoffsets ={}
			ropfuncs,ropfuncoffsets = findROPFUNC(modulecriteria,criteria)
			#report findings to log
			dbg.log(""[+] Processing pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc.txt"")
			thislog = logfile.reset()
			processResults(ropfuncs,logfile,thislog)
			global silent
			silent = True
			dbg.log(""[+] Processing offsets to pointers to interesting rop functions"")
			logfile = MnLog(""ropfunc_offset.txt"")
			thislog = logfile.reset()
			processResults(ropfuncoffsets,logfile,thislog)			
			
		def procStackPivots(args):
			procROP(args,""stackpivot"")
			
		def procROP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			maxoffset = 40
			thedistance = 8
			split = False
			fast = False
			sortedprint = False
			endingstr = """"
			endings = []
			technique = """"            
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass
			
			if ""offset"" in args:
				if type(args[""offset""]).__name__.lower() != ""bool"":
					try:
						maxoffset = int(args[""offset""])
					except:
						pass
			
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() != ""bool"":
					try:
						thedistance = args[""distance""]
					except:
						pass
			
			if ""split"" in args:
				if type(args[""split""]).__name__.lower() == ""bool"":
					split = args[""split""]

			if ""s"" in args:
				if type(args[""s""]).__name__.lower() != ""bool"":
					technique = args[""s""].replace(""'"","""").replace('""',"""").strip().lower()                   
					
			if ""fast"" in args:
				if type(args[""fast""]).__name__.lower() == ""bool"":
					fast = args[""fast""]
			
			if ""end"" in args:
				if type(args[""end""]).__name__.lower() == ""str"":
					endingstr = args[""end""].replace(""'"","""").replace('""',"""").strip()
					endings = endingstr.split(""#"")
					
			if ""f"" in args:
				if args[""f""] != """":
					criteria[""f""] = args[""f""]
			
			if ""sort"" in args:
				sortedprint = True
			
			if ""rva"" in args:
				criteria[""rva""] = True
			
			if mode == ""stackpivot"":
				fast = False
				endings = """"
				split = False
			else:
				mode = ""all""
			
			findROPGADGETS(modulecriteria,criteria,endings,maxoffset,depth,split,thedistance,fast,mode,sortedprint,technique)
			

		def procJseh(args):
			results = []
			showred=0
			showall=False
			if ""all"" in args:
				showall = True
			nrfound = 0
			dbg.log(""-----------------------------------------------------------------------"")
			dbg.log(""Search for jmp/call dword[ebp/esp+nn] (and other) combinations started "")
			dbg.log(""-----------------------------------------------------------------------"")
			opcodej=[""\xff\x54\x24\x08"", #call dword ptr [esp+08]
					""\xff\x64\x24\x08"", #jmp dword ptr [esp+08]
					""\xff\x54\x24\x14"", #call dword ptr [esp+14]
					""\xff\x54\x24\x14"", #jmp dword ptr [esp+14]
					""\xff\x54\x24\x1c"", #call dword ptr [esp+1c]
					""\xff\x54\x24\x1c"", #jmp dword ptr [esp+1c]
					""\xff\x54\x24\x2c"", #call dword ptr [esp+2c]
					""\xff\x54\x24\x2c"", #jmp dword ptr [esp+2c]
					""\xff\x54\x24\x44"", #call dword ptr [esp+44]
					""\xff\x54\x24\x44"", #jmp dword ptr [esp+44]
					""\xff\x54\x24\x50"", #call dword ptr [esp+50]
					""\xff\x54\x24\x50"", #jmp dword ptr [esp+50]
					""\xff\x55\x0c"",     #call dword ptr [ebp+0c]
					""\xff\x65\x0c"",     #jmp dword ptr [ebp+0c]
					""\xff\x55\x24"",     #call dword ptr [ebp+24]
					""\xff\x65\x24"",     #jmp dword ptr [ebp+24]
					""\xff\x55\x30"",     #call dword ptr [ebp+30]
					""\xff\x65\x30"",     #jmp dword ptr [ebp+30]
					""\xff\x55\xfc"",     #call dword ptr [ebp-04]
					""\xff\x65\xfc"",     #jmp dword ptr [ebp-04]
					""\xff\x55\xf4"",     #call dword ptr [ebp-0c]
					""\xff\x65\xf4"",     #jmp dword ptr [ebp-0c]
					""\xff\x55\xe8"",     #call dword ptr [ebp-18]
					""\xff\x65\xe8"",     #jmp dword ptr [ebp-18]
					""\x83\xc4\x08\xc3"", #add esp,8 + ret
					""\x83\xc4\x08\xc2""] #add esp,8 + ret X
			fakeptrcriteria = {}
			fakeptrcriteria[""accesslevel""] = ""*""
			for opjc in opcodej:
				addys = []
				addys = searchInRange( [[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
				results += addys
				for ptrtypes in addys:
					for ad1 in addys[ptrtypes]:
						ptr = MnPointer(ad1)
						module = ptr.belongsTo()
						if not module:
							module=""""
							page   = dbg.getMemoryPageByAddress( ad1 )
							access = page.getAccess( human = True )
							op = dbg.disasm( ad1 )
							opstring=op.getDisasm()
							dbg.log(""Found %s at 0x%08x - Access: (%s) - Outside of a loaded module"" % (opstring, ad1, access), address = ad1,highlight=1)
							nrfound+=1
						else:
							if showall:
								page   = dbg.getMemoryPageByAddress( ad1 )
								access = page.getAccess( human = True )
								op = dbg.disasm( ad1 )
								opstring=op.getDisasm()
								thismod = MnModule(module)
								if not thismod.isSafeSEH:
								#if ismodulenosafeseh(module[0])==1:
									extratext=""=== Safeseh : NO ===""
									showred=1
								else:
									extratext=""Safeseh protected""
									showred=0
								dbg.log(""Found %s at 0x%08x (%s) - Access: (%s) - %s"" % (opstring, ad1, module,access,extratext), address = ad1,highlight=showred)
								nrfound+=1
			dbg.log(""Search complete"")
			if results:
				dbg.log(""Found %d address(es)"" % nrfound)
				return ""Found %d address(es) (Check the log Windows for details)"" % nrfound
			else:
				dbg.log(""No addresses found"")
				return ""Sorry, no addresses found""

			
		def procJOP(args,mode=""all""):
			#default criteria
			modulecriteria={}
			modulecriteria[""aslr""] = False
			modulecriteria[""rebase""] = False
			modulecriteria[""os""] = False

			criteria={}
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			
			# handle optional arguments
			
			depth = 6
			
			if ""depth"" in args:
				if type(args[""depth""]).__name__.lower() != ""bool"":
					try:
						depth = int(args[""depth""])
					except:
						pass			
			findJOPGADGETS(modulecriteria,criteria,depth)			
			
			
		def procCreatePATTERN(args):
			size = 0
			pattern = """"
			if ""?"" in args and args[""?""] != """":
				try:
					if ""0x"" in args[""?""].lower():
						try:
							size = int(args[""?""],16)
						except:
							size = 0
					else:
						size = int(args[""?""])
				except:
					size = 0
			if size == 0:
				dbg.log(""Please enter a valid size"",highlight=1)
			else:
				pattern = createPattern(size,args)
				dbg.log(""Creating cyclic pattern of %d bytes"" % size)				
				dbg.log(pattern)
				global ignoremodules
				ignoremodules = True
				objpatternfile = MnLog(""pattern.txt"")
				patternfile = objpatternfile.reset()
				# ASCII
				objpatternfile.write(""\nPattern of "" + str(size) + "" bytes :\n"",patternfile)
				objpatternfile.write(""-"" * (19 + len(str(size))),patternfile)
				objpatternfile.write(""\nASCII:"",patternfile)
				objpatternfile.write(""\n"" + pattern,patternfile)
				# Hex
				patternhex = """"
				for patternchar in pattern:
					patternhex += str(hex(ord(patternchar))).replace(""0x"",""\\x"")
				objpatternfile.write(""\n\nHEX:\n"",patternfile)
				objpatternfile.write(patternhex,patternfile)
				# Javascript
				patternjs = str2js(pattern)
				objpatternfile.write(""\n\nJAVASCRIPT (unescape() friendly):\n"",patternfile)
				objpatternfile.write(patternjs,patternfile)
				if not silent:
					dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"",highlight=1)
					dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile,highlight=1)
				
				ignoremodules = False
			return


		def procOffsetPATTERN(args):
			egg = """"
			if ""?"" in args and args[""?""] != """":
				try:
					egg = args[""?""]
				except:
					egg = """"
			if egg == """":
				dbg.log(""Please enter a valid target"",highlight=1)
			else:
				findOffsetInPattern(egg,-1,args)
			return
		
		# ----- Comparing file output ----- #
		def procFileCOMPARE(args):
			modulecriteria={}
			criteria={}
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			allfiles=[]
			tomatch=""""
			checkstrict=True
			rangeval = 0
			fast = False
			if ""ptronly"" in args or ""ptrsonly"" in args:
				fast = True
			if ""f"" in args:
				if args[""f""] != """":
					rawfilenames=args[""f""].replace('""',"""")
					allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
					dbg.log(""[+] Number of files to be examined : %d "" % len(allfiles))
			if ""range"" in args:
				if not type(args[""range""]).__name__.lower() == ""bool"":
					strrange = args[""range""].lower()
					if strrange.startswith(""0x"") and len(strrange) > 2 :
						rangeval = int(strrange,16)
					else:
						try:
							rangeval = int(args[""range""])
						except:
							rangeval = 0
					if rangeval > 0:
						dbg.log(""[+] Find overlap using pointer +/- range, value %d"" % rangeval)
						dbg.log(""    Note : this will significantly slow down the comparison process !"")
				else:
					dbg.log(""Please provide a numeric value ^(> 0) with option -range"",highlight=1)
					return
			else:
				if ""contains"" in args:
					if type(args[""contains""]).__name__.lower() == ""str"":
						tomatch = args[""contains""].replace(""'"","""").replace('""',"""")
				if ""nostrict"" in args:
					if type(args[""nostrict""]).__name__.lower() == ""bool"":
						checkstrict = not args[""nostrict""]
						dbg.log(""[+] Instructions must match in all files ? %s"" % checkstrict)
			# maybe one of the arguments is a folder
			callfiles = allfiles
			allfiles = []
			for tfile in callfiles:
				if os.path.isdir(tfile):
					# folder, get all files from this folder
					for root,dirs,files in os.walk(tfile):
						for dfile in files:
							allfiles.append(os.path.join(root,dfile))
				else:
					allfiles.append(tfile)
			if len(allfiles) > 1:
				findFILECOMPARISON(modulecriteria,criteria,allfiles,tomatch,checkstrict,rangeval,fast)
			else:
				dbg.log(""Please specify at least 2 filenames to compare"",highlight=1)

		# ----- Find bytes in memory ----- #
		def procFind(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			base = 0
			offset = 0
			top  = TOP_USERLAND
			consecutive = False
			ftype = """"
			
			level = 0
			offsetlevel = 0			
			
			if not ""a"" in args:
				args[""a""] = ""*""

			ptronly = False

			if ""ptronly"" in args or ""ptrsonly"" in args:
				ptronly = True	
			
			#search for all pointers by default
			if not ""x"" in args:
				args[""x""] = ""*""
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)
			if criteria[""accesslevel""] == """":
				return
			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			if ""unicode"" in args:
				criteria[""unic""] = True

			if ""b"" in args:
				try:
					base = int(args[""b""],16)
				except:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return
			if ""t"" in args:
				try:
					top = int(args[""t""],16)
				except:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
			if ""offset"" in args:
				if not args[""offset""].__class__.__name__ == ""bool"":
					if ""0x"" in args[""offset""].lower():
						try:
							offset = 0 - int(args[""offset""],16)
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return
					else:	
						try:
							offset = 0 - int(args[""offset""])
						except:
							dbg.log(""invalid offset value"",highlight=1)
							return	
				else:
					dbg.log(""invalid offset value"",highlight=1)
					return
					
			if ""level"" in args:
				try:
					level = int(args[""level""])
				except:
					dbg.log(""invalid level value"",highlight=1)
					return

			if ""offsetlevel"" in args:
				try:
					offsetlevel = int(args[""offsetlevel""])
				except:
					dbg.log(""invalid offsetlevel value"",highlight=1)
					return						
					
			if ""c"" in args:
				dbg.log(""    - Skipping consecutive pointers, showing size instead"")			
				consecutive = True
				
			if ""type"" in args:
				if not args[""type""] in [""bin"",""asc"",""ptr"",""instr"",""file""]:
					dbg.log(""Invalid search type : %s"" % args[""type""], highlight=1)
					return
				ftype = args[""type""] 
				if ftype == ""file"":
					filename = args[""s""].replace('""',"""").replace(""'"","""")
					#see if we can read the file
					if not os.path.isfile(filename):
						dbg.log(""Unable to find/read file %s"" % filename,highlight=1)
						return
			rangep2p = 0

			
			if ""p2p"" in args or level > 0:
				dbg.log(""    - Looking for pointers to pointers"")
				criteria[""p2p""] = True
				if ""r"" in args:	
					try:
						rangep2p = int(args[""r""])
					except:
						pass
					if rangep2p > 0:
						dbg.log(""    - Will search for close pointers (%d bytes backwards)"" % rangep2p)
				if ""p2p"" in args:
					level = 1
			
			
			if level > 0:
				dbg.log(""    - Recursive levels : %d"" % level)
			

			allpointers = findPattern(modulecriteria,criteria,pattern,ftype,base,top,consecutive,rangep2p,level,offset,offsetlevel)
				
			logfile = MnLog(""find.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog,{},ptronly)
			return
			
			
		# ---- Find instructions, wildcard search ----- #
		def procFindWild(args):
			modulecriteria={}
			criteria={}
			pattern = """"
			patterntype = """"
			base = 0
			top  = TOP_USERLAND
			
			modulecriteria,criteria = args2criteria(args,modulecriteria,criteria)

			if not ""s"" in args:
				dbg.log(""-s <search pattern (or filename)> is a mandatory argument"",highlight=1)
				return
			pattern = args[""s""]
			
			patterntypes = [""bin"",""str""]
			if ""type"" in args:
				if type(args[""type""]).__name__.lower() != ""bool"":
					if args[""type""] in patterntypes:
						patterntype = args[""type""]
					else:
						dbg.log(""-type argument only takes one of these values: %s"" % patterntypes,highlight=1)
						return
				else:
					dbg.log(""Please specify a valid value for -type. Valid values are %s"" % patterntypes,highlight=1)
					return


			if patterntype == """":
				if ""\\x"" in pattern:
					patterntype = ""bin""
				else:
					patterntype = ""str""
			
			if ""b"" in args:
				base,addyok = getAddyArg(args[""b""])
				if not addyok:
					dbg.log(""invalid base address: %s"" % args[""b""],highlight=1)
					return

			if ""t"" in args:
				top,addyok = getAddyArg(args[""t""])
				if not addyok:
					dbg.log(""invalid top address: %s"" % args[""t""],highlight=1)
					return
					
			if ""depth"" in args:
				try:
					criteria[""depth""] = int(args[""depth""])
				except:
					dbg.log(""invalid depth value"",highlight=1)
					return	

			if ""all"" in args:
				criteria[""all""] = True
				
			if ""distance"" in args:
				if type(args[""distance""]).__name__.lower() == ""bool"":
					dbg.log(""invalid distance value(s)"",highlight=1)
				else:
					distancestr = args[""distance""]
					distanceparts = distancestr.split("","")
					for parts in distanceparts:
						valueparts = parts.split(""="")
						if len(valueparts) > 1:
							if valueparts[0].lower() == ""min"":
								try:
									mindistance = int(valueparts[1])
								except:
									mindistance = 0	
							if valueparts[0].lower() == ""max"":
								try:
									maxdistance = int(valueparts[1])
								except:
									maxdistance = 0	
			
				if maxdistance < mindistance:
					tmp = maxdistance
					maxdistance = mindistance
					mindistance = tmp
				
				criteria[""mindistance""] = mindistance
				criteria[""maxdistance""] = maxdistance
						
			allpointers = findPatternWild(modulecriteria,criteria,pattern,base,top,patterntype)
				
			logfile = MnLog(""findwild.txt"")
			thislog = logfile.reset()
			processResults(allpointers,logfile,thislog)		
			return
	
			
		# ----- assemble: assemble instructions to opcodes ----- #
		def procAssemble(args):
			opcodes = """"
			encoder = """"
			
			if not 's' in args:
				dbg.log(""Mandatory argument -s <opcodes> missing"", highlight=1)
				return
			opcodes = args['s']
			
			if 'e' in args:
				# TODO: implement encoder support
				dbg.log(""Encoder support not yet implemented"", highlight=1)
				return
				encoder = args['e'].lowercase()
				if encoder not in [""ascii""]:
					dbg.log(""Invalid encoder : %s"" % encoder, highlight=1)
					return
			
			assemble(opcodes,encoder)
			
		# ----- info: show information about an address ----- #
		def procInfo(args):
			if not ""a"" in args:
				dbg.log(""Missing mandatory argument -a"", highlight=1)
				return
			
			address,addyok = getAddyArg(args[""a""])
			if not addyok:
				dbg.log(""%s is an invalid address"" % args[""a""], highlight=1)
				return
			
			ptr = MnPointer(address)
			modname = ptr.belongsTo()
			modinfo = None
			if modname != """":
				modinfo = MnModule(modname)
			rebase = """"
			rva=0
			if modinfo :
				rva = address - modinfo.moduleBase
			procFlags(args)
			dbg.log("""")			
			dbg.log(""[+] Information about address 0x%s"" % toHex(address))
			dbg.log(""    %s"" % ptr.__str__())
			thepage = dbg.getMemoryPageByAddress(address)
			dbg.log(""    Address is part of page 0x%08x - 0x%08x"" % (thepage.getBaseAddress(),thepage.getBaseAddress()+thepage.getSize()))
			section = """"
			try:
				section = thepage.getSection()
			except:
				section = """"
			if section != """":
				dbg.log(""    Section : %s"" % section)
			
			if ptr.isOnStack():
				stacks = getStacks()
				stackref = """"
				for tid in stacks:
					currstack = stacks[tid]
					if currstack[0] <= address and address <= currstack[1]:
						stackref = "" (Thread 0x%08x, Stack Base : 0x%08x, Stack Top : 0x%08x)"" % (tid,currstack[0],currstack[1])
						break
				dbg.log(""    This address is in a stack segment %s"" % stackref)
			if modinfo:
				dbg.log(""    Address is part of a module:"")
				dbg.log(""    %s"" % modinfo.__str__())
				if rva != 0:
					dbg.log(""    Offset from module base: 0x%x"" % rva)
					if modinfo:
						eatlist = modinfo.getEAT()
						if address in eatlist:
							dbg.log(""    Address is start of function '%s' in %s"" % (eatlist[address],modname))
						else:
							iatlist = modinfo.getIAT()
							if address in iatlist:
								iatentry = iatlist[address]
								dbg.log(""    Address is part of IAT, and contains pointer to '%s'"" % iatentry)				
			else:
				output = """"
				if ptr.isInHeap():
					dbg.log(""    This address resides in the heap"")
					dbg.log("""")
					ptr.showHeapBlockInfo()
				else:
					dbg.log(""    Module: None"")					
			try:
				dbg.log("""")
				dbg.log(""[+] Disassembly:"")
				op = dbg.disasm(address)
				opstring=getDisasmInstruction(op)
				dbg.log(""    Instruction at %s : %s"" % (toHex(address),opstring))
			except:
				pass
			if __DEBUGGERAPP__ == ""WinDBG"":
				dbg.log("""")
				dbg.log(""Output of !address 0x%08x:"" % address)
				output = dbg.nativeCommand(""!address 0x%08","for (name, value) in zip(registers_to_fill, values_to_generate_all_255_values):
    padding = ''
    if value < 16:
        padding = '0'
    if 'h' in name:
        prefix += 'mov e%sx,0x4100%s%s00; ' % (name[0], padding, hex(value)[2:])
        prefix += 'add [ebp],ch; '
        additionalLength += 8
    if 'l' in name:
        prefix += 'mov e%sx,0x4100%s%s00; ' % (buf_sig, padding, hex(value)[2:])
        prefix += 'add %s,%sh; ' % (name, buf_sig)
        prefix += 'add [ebp],ch; '
        additionalLength += 10","for (name, value) in zip(registers_to_fill, values_to_generate_all_255_values):
    (name_0, *name_rnamemaining) = name
    padding = ''
    if value < 16:
        padding = '0'
    if 'h' in name:
        prefix += 'mov e%sx,0x4100%s%s00; ' % (name_0, padding, hex(value)[2:])
        prefix += 'add [ebp],ch; '
        additionalLength += 8
    if 'l' in name:
        prefix += 'mov e%sx,0x4100%s%s00; ' % (buf_sig, padding, hex(value)[2:])
        prefix += 'add %s,%sh; ' % (name, buf_sig)
        prefix += 'add [ebp],ch; '
        additionalLength += 10",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,,
meld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meld/meld/matchers/merge.py,https://github.com/GNOME/meld/tree/master/meld/matchers/merge.py,AutoMergeDiffer,_auto_merge$32,"def _auto_merge(self, using, texts):
        for out0, out1 in super()._auto_merge(using, texts):
            if self.auto_merge and out0[0] == 'conflict':
                # we will try to resolve more complex conflicts automatically
                # here... if possible
                l0, h0, l1, h1, l2, h2 = (
                    out0[3], out0[4], out0[1], out0[2], out1[3], out1[4])
                len0 = h0 - l0
                len1 = h1 - l1
                len2 = h2 - l2
                if (len0 > 0 and len2 > 0) and (
                        len0 == len1 or len2 == len1 or len1 == 0):
                    matcher = self._matcher(
                        None, texts[0][l0:h0], texts[2][l2:h2])
                    for chunk in matcher.get_opcodes():
                        s1 = l1
                        e1 = l1
                        if len0 == len1:
                            s1 += chunk[1]
                            e1 += chunk[2]
                        elif len2 == len1:
                            s1 += chunk[3]
                            e1 += chunk[4]
                        out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                        out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                        if chunk[0] == 'equal':
                            out0 = ('replace',) + out0_bounds
                            out1 = ('replace',) + out1_bounds
                            yield out0, out1
                        else:
                            out0 = ('conflict',) + out0_bounds
                            out1 = ('conflict',) + out1_bounds
                            yield out0, out1
                    return
                # elif len0 > 0 and len2 > 0:
                #     # this logic will resolve more conflicts automatically,
                #     # but unresolved conflicts may sometimes look confusing
                #     # as the line numbers in ancestor file will be
                #     # interpolated and may not reflect the actual changes
                #     matcher = self._matcher(
                #         None, texts[0][l0:h0], texts[2][l2:h2])
                #     if len0 > len2:
                #         maxindex = 1
                #         maxlen = len0
                #     else:
                #         maxindex = 3
                #         maxlen = len2
                #     for chunk in matcher.get_opcodes():
                #         new_start = l1 + len1 * chunk[maxindex] / maxlen
                #         new_end = l1 + len1 * chunk[maxindex + 1] / maxlen
                #         out0_bounds = (
                #             new_start, new_end, l0 + chunk[1], l0 + chunk[2])
                #         out1_bounds = (
                #             new_start, new_end, l2 + chunk[3], l2 + chunk[4])
                #         if chunk[0] == 'equal':
                #             out0 = ('replace',) + out0_bounds
                #             out1 = ('replace',) + out1_bounds
                #             yield out0, out1
                #         else:
                #             out0 = ('conflict',) + out0_bounds
                #             out1 = ('conflict',) + out1_bounds
                #             yield out0, out1
                #     return
                else:
                    # some tricks to resolve even more conflicts automatically
                    # unfortunately the resulting chunks cannot be used to
                    # highlight changes but hey, they are good enough to merge
                    # the resulting file :)
                    chunktype = using[0][0][0]
                    for chunkarr in using:
                        for chunk in chunkarr:
                            if chunk[0] != chunktype:
                                chunktype = None
                                break
                        if not chunktype:
                            break
                    if chunktype == 'delete':
                        # delete + delete -> split into delete/conflict
                        seq0 = seq1 = None
                        while 1:
                            if seq0 is None:
                                try:
                                    seq0 = using[0].pop(0)
                                    i0 = seq0[1]
                                    end0 = seq0[4]
                                except IndexError:
                                    break
                            if seq1 is None:
                                try:
                                    seq1 = using[1].pop(0)
                                    i1 = seq1[1]
                                    end1 = seq1[4]
                                except IndexError:
                                    break
                            highstart = max(i0, i1)
                            if i0 != i1:
                                out0 = (
                                    'conflict', i0 - highstart + i1, highstart,
                                    seq0[3] - highstart + i1, seq0[3]
                                )
                                out1 = (
                                    'conflict', i1 - highstart + i0, highstart,
                                    seq1[3] - highstart + i0, seq1[3]
                                )
                                yield out0, out1
                            lowend = min(seq0[2], seq1[2])
                            if highstart != lowend:
                                out0 = (
                                    'delete', highstart, lowend,
                                    seq0[3], seq0[4]
                                )
                                out1 = (
                                    'delete', highstart, lowend,
                                    seq1[3], seq1[4]
                                )
                                yield out0, out1
                            i0 = i1 = lowend
                            if lowend == seq0[2]:
                                seq0 = None
                            if lowend == seq1[2]:
                                seq1 = None

                        if seq0:
                            out0 = (
                                'conflict', i0, seq0[2],
                                seq0[3], seq0[4]
                            )
                            out1 = (
                                'conflict', i0, seq0[2],
                                end1, end1 + seq0[2] - i0
                            )
                            yield out0, out1
                        elif seq1:
                            out0 = (
                                'conflict', i1, seq1[2],
                                end0, end0 + seq1[2] - i1
                            )
                            out1 = (
                                'conflict', i1,
                                seq1[2], seq1[3], seq1[4]
                            )
                            yield out0, out1
                        return
            yield out0, out1","for (out0, out1) in super()._auto_merge(using, texts):
    if self.auto_merge and out0[0] == 'conflict':
        (l0, h0, l1, h1, l2, h2) = (out0[3], out0[4], out0[1], out0[2], out1[3], out1[4])
        len0 = h0 - l0
        len1 = h1 - l1
        len2 = h2 - l2
        if (len0 > 0 and len2 > 0) and (len0 == len1 or len2 == len1 or len1 == 0):
            matcher = self._matcher(None, texts[0][l0:h0], texts[2][l2:h2])
            for chunk in matcher.get_opcodes():
                s1 = l1
                e1 = l1
                if len0 == len1:
                    s1 += chunk[1]
                    e1 += chunk[2]
                elif len2 == len1:
                    s1 += chunk[3]
                    e1 += chunk[4]
                out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                if chunk[0] == 'equal':
                    out0 = ('replace',) + out0_bounds
                    out1 = ('replace',) + out1_bounds
                    yield (out0, out1)
                else:
                    out0 = ('conflict',) + out0_bounds
                    out1 = ('conflict',) + out1_bounds
                    yield (out0, out1)
            return
        else:
            chunktype = using[0][0][0]
            for chunkarr in using:
                for chunk in chunkarr:
                    if chunk[0] != chunktype:
                        chunktype = None
                        break
                if not chunktype:
                    break
            if chunktype == 'delete':
                seq0 = seq1 = None
                while 1:
                    if seq0 is None:
                        try:
                            seq0 = using[0].pop(0)
                            i0 = seq0[1]
                            end0 = seq0[4]
                        except IndexError:
                            break
                    if seq1 is None:
                        try:
                            seq1 = using[1].pop(0)
                            i1 = seq1[1]
                            end1 = seq1[4]
                        except IndexError:
                            break
                    highstart = max(i0, i1)
                    if i0 != i1:
                        out0 = ('conflict', i0 - highstart + i1, highstart, seq0[3] - highstart + i1, seq0[3])
                        out1 = ('conflict', i1 - highstart + i0, highstart, seq1[3] - highstart + i0, seq1[3])
                        yield (out0, out1)
                    lowend = min(seq0[2], seq1[2])
                    if highstart != lowend:
                        out0 = ('delete', highstart, lowend, seq0[3], seq0[4])
                        out1 = ('delete', highstart, lowend, seq1[3], seq1[4])
                        yield (out0, out1)
                    i0 = i1 = lowend
                    if lowend == seq0[2]:
                        seq0 = None
                    if lowend == seq1[2]:
                        seq1 = None
                if seq0:
                    out0 = ('conflict', i0, seq0[2], seq0[3], seq0[4])
                    out1 = ('conflict', i0, seq0[2], end1, end1 + seq0[2] - i0)
                    yield (out0, out1)
                elif seq1:
                    out0 = ('conflict', i1, seq1[2], end0, end0 + seq1[2] - i1)
                    out1 = ('conflict', i1, seq1[2], seq1[3], seq1[4])
                    yield (out0, out1)
                return
    yield (out0, out1)","for (out0, out1) in super()._auto_merge(using, texts):
    (_, _, _, out1_3, out1_4, *_) = out1
    (out0_0, out0_1, out0_2, out0_3, out0_4, *_) = out0
    if self.auto_merge and out0_0 == 'conflict':
        (l0, h0, l1, h1, l2, h2) = (out0_3, out0_4, out0_1, out0_2, out1_3, out1_4)
        len0 = h0 - l0
        len1 = h1 - l1
        len2 = h2 - l2
        if (len0 > 0 and len2 > 0) and (len0 == len1 or len2 == len1 or len1 == 0):
            matcher = self._matcher(None, texts[0][l0:h0], texts[2][l2:h2])
            for chunk in matcher.get_opcodes():
                s1 = l1
                e1 = l1
                if len0 == len1:
                    s1 += chunk[1]
                    e1 += chunk[2]
                elif len2 == len1:
                    s1 += chunk[3]
                    e1 += chunk[4]
                out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                if chunk[0] == 'equal':
                    out0 = ('replace',) + out0_bounds
                    out1 = ('replace',) + out1_bounds
                    yield (out0, out1)
                else:
                    out0 = ('conflict',) + out0_bounds
                    out1 = ('conflict',) + out1_bounds
                    yield (out0, out1)
            return
        else:
            chunktype = using[0][0][0]
            for chunkarr in using:
                for chunk in chunkarr:
                    if chunk[0] != chunktype:
                        chunktype = None
                        break
                if not chunktype:
                    break
            if chunktype == 'delete':
                seq0 = seq1 = None
                while 1:
                    if seq0 is None:
                        try:
                            seq0 = using[0].pop(0)
                            i0 = seq0[1]
                            end0 = seq0[4]
                        except IndexError:
                            break
                    if seq1 is None:
                        try:
                            seq1 = using[1].pop(0)
                            i1 = seq1[1]
                            end1 = seq1[4]
                        except IndexError:
                            break
                    highstart = max(i0, i1)
                    if i0 != i1:
                        out0 = ('conflict', i0 - highstart + i1, highstart, seq0[3] - highstart + i1, seq0[3])
                        out1 = ('conflict', i1 - highstart + i0, highstart, seq1[3] - highstart + i0, seq1[3])
                        yield (out0, out1)
                    lowend = min(seq0[2], seq1[2])
                    if highstart != lowend:
                        out0 = ('delete', highstart, lowend, seq0[3], seq0[4])
                        out1 = ('delete', highstart, lowend, seq1[3], seq1[4])
                        yield (out0, out1)
                    i0 = i1 = lowend
                    if lowend == seq0[2]:
                        seq0 = None
                    if lowend == seq1[2]:
                        seq1 = None
                if seq0:
                    out0 = ('conflict', i0, seq0[2], seq0[3], seq0[4])
                    out1 = ('conflict', i0, seq0[2], end1, end1 + seq0[2] - i0)
                    yield (out0, out1)
                elif seq1:
                    out0 = ('conflict', i1, seq1[2], end0, end0 + seq1[2] - i1)
                    out1 = ('conflict', i1, seq1[2], seq1[3], seq1[4])
                    yield (out0, out1)
                return
    yield (out0, out1)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3, e_4 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]
e_4: e[4]***************
Answer: Yes
Iterable Unpacking: _, _, _, e_3, e_4 = e
variable mapping:
e_3: e[3]
e_4: e[4]",,,,,,,,
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/fix_annotations.py,https://github.com/nlplab/brat/tree/master/tools/fix_annotations.py,,correct_annotations$28,"def correct_annotations(orig_fn, ann_fn, change_fn):
    with annotation.TextAnnotations(ann_fn) as anns:
        orig_text = anns.get_document_text()
        with annotation.open_textfile(change_fn, 'r') as f:
            changed_text = f.read()
        diffs = diff_match_patch().diff_main(orig_text, changed_text)
        orig_offset = 0
        offsets = []
        for diff in diffs:
            kind = diff[0]
            text = diff[1]
            size = len(text)
            delta = size * kind
            offsets.append((orig_offset, delta))
            if kind != 1:
                orig_offset += size
        offsets = offsets[::-1]
        tbs = list(anns.get_textbounds())
        indices = []
        for tbi, tb in enumerate(tbs):
            for spani, span in enumerate(tb.spans):
                indices.append((span[0], tbi, spani, 0))
                indices.append((span[1], tbi, spani, 1))
        indices.sort(reverse=True)
        for orig_offset, delta in offsets:
            for index in indices:
                if index[0] < orig_offset:
                    break
                frag = list(tbs[index[1]].spans[index[2]])
                frag[index[3]] += delta
                tbs[index[1]].spans[index[2]] = tuple(frag)
        for tb in tbs:
            if isinstance(tb, annotation.TextBoundAnnotationWithText):
                tb.text = annotation.DISCONT_SEP.join(
                    (changed_text[start:end] for start, end in tb.spans))
    copy(change_fn, orig_fn)","for (spani, span) in enumerate(tb.spans):
    indices.append((span[0], tbi, spani, 0))
    indices.append((span[1], tbi, spani, 1))","for (spani, span) in enumerate(tb.spans):
    (span_0, span_1, *span_rspanmaining) = span
    indices.append((span_0, tbi, spani, 0))
    indices.append((span_1, tbi, spani, 1))",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, e_1, *e_remaining = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,,
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/fix_annotations.py,https://github.com/nlplab/brat/tree/master/tools/fix_annotations.py,,correct_annotations$28,"def correct_annotations(orig_fn, ann_fn, change_fn):
    with annotation.TextAnnotations(ann_fn) as anns:
        orig_text = anns.get_document_text()
        with annotation.open_textfile(change_fn, 'r') as f:
            changed_text = f.read()
        diffs = diff_match_patch().diff_main(orig_text, changed_text)
        orig_offset = 0
        offsets = []
        for diff in diffs:
            kind = diff[0]
            text = diff[1]
            size = len(text)
            delta = size * kind
            offsets.append((orig_offset, delta))
            if kind != 1:
                orig_offset += size
        offsets = offsets[::-1]
        tbs = list(anns.get_textbounds())
        indices = []
        for tbi, tb in enumerate(tbs):
            for spani, span in enumerate(tb.spans):
                indices.append((span[0], tbi, spani, 0))
                indices.append((span[1], tbi, spani, 1))
        indices.sort(reverse=True)
        for orig_offset, delta in offsets:
            for index in indices:
                if index[0] < orig_offset:
                    break
                frag = list(tbs[index[1]].spans[index[2]])
                frag[index[3]] += delta
                tbs[index[1]].spans[index[2]] = tuple(frag)
        for tb in tbs:
            if isinstance(tb, annotation.TextBoundAnnotationWithText):
                tb.text = annotation.DISCONT_SEP.join(
                    (changed_text[start:end] for start, end in tb.spans))
    copy(change_fn, orig_fn)","for index in indices:
    if index[0] < orig_offset:
        break
    frag = list(tbs[index[1]].spans[index[2]])
    frag[index[3]] += delta
    tbs[index[1]].spans[index[2]] = tuple(frag)","for index in indices:
    (index_0, *index_rindexmaining) = index
    if index_0 < orig_offset:
        break
    frag = list(tbs[index[1]].spans[index[2]])
    frag[index[3]] += delta
    tbs[index[1]].spans[index[2]] = tuple(frag)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, *e_remaining = e
variable mapping:
e_0: e[0]",,,,,,,,
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/extra/tunings.py,https://github.com/bspaans/python-mingus/tree/master/mingus/extra/tunings.py,StringTuning,find_chord_fingering$149,"def find_chord_fingering(
        self,
        notes,
        max_distance=4,
        maxfret=18,
        max_fingers=4,
        return_best_as_NoteContainer=False,
    ):
        """"""Return a list of fret lists that are considered possible fingerings.

        This function only looks at and matches on the note _names_ so it
        does more than find_fingering.

        Example:
        >>> t = get_tuning('guitar', 'standard', 6, 1)
        >>> t.find_chord_fingering(NoteContainer().from_chord('Am'))
        [[0, 0, 2, 2, 1, 0], [0, 3, 2, 2, 1, 0], ......]
        """"""

        def follow(string, next, name, prev=-1):
            """"""Follow the fret 'next' on 'string'; build result on the way.""""""
            if string >= len(self.tuning) - 1:
                return [[(next, name)]]
            result = []
            cur = res[string][next]
            if cur != []:
                for y in cur[1]:
                    for sub in follow(string + 1, y[0], y[1]):
                        if prev < 0:
                            result.append([(next, name)] + sub)
                        else:
                            if sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
                                result.append([(next, name)] + sub)
            for s in follow(string + 1, maxfret + 1, None, next):
                result.append([(next, name)] + s)
            return [[(next, name)]] if result == [] else result

        def make_lookup_table():
            """"""Prepare the lookup table.

            table[string][fret] = (name, dest_frets)
            """"""
            res = [[[] for x in range(maxfret + 2)] for x in range(len(self.tuning) - 1)]
            for x in range(0, len(self.tuning) - 1):
                addedNone = -1
                next = fretdict[x + 1]
                for (fret, name) in fretdict[x]:
                    for (f2, n2) in next:
                        if n2 != name and (f2 == 0 or abs(fret - f2) < max_distance):
                            if res[x][fret] != []:
                                res[x][fret][1].append((f2, n2))
                            else:
                                res[x][fret] = (name, [(f2, n2)])
                        if addedNone < x:
                            if res[x][maxfret + 1] != []:
                                res[x][maxfret + 1][1].append((f2, n2))
                            else:
                                res[x][maxfret + 1] = (None, [(f2, n2)])
                    addedNone = x
            return res

        # Convert to NoteContainer if necessary
        n = notes
        if notes != [] and isinstance(notes, list) and isinstance(notes[0], six.string_types):
            n = NoteContainer(notes)

        # Check number of note names.
        notenames = [x.name for x in n]
        if len(notenames) == 0 or len(notenames) > len(self.tuning):
            return []

        # Make string-fret dictionary
        fretdict = []
        for x in range(0, len(self.tuning)):
            fretdict.append(self.find_note_names(notes, x, maxfret))

        # Build table
        res = make_lookup_table()

        # Build result using table
        result = []

        # For each fret on the first string
        for (i, y) in enumerate(res[0]):
            if y != []:
                (yname, next) = (y[0], y[1])

                # For each destination fret in y
                for (fret, name) in next:

                    # For each followed result
                    for s in follow(1, fret, name):
                        subresult = [(i, yname)] + s

                        # Get boundaries
                        (mi, ma, names) = (1000, -1000, [])
                        for (f, n) in subresult:
                            if n is not None:
                                if f != 0 and f <= mi:
                                    mi = f
                                if f != 0 and f >= ma:
                                    ma = f
                                names.append(n)

                        # Enforce boundaries
                        if abs(ma - mi) < max_distance:
                            # Check if all note
                            # names are present
                            covered = True
                            for n in notenames:
                                if n not in names:
                                    covered = False

                            # Add to result
                            if covered and names != []:
                                result.append(
                                    [y[0] if y[1] is not None else y[1] for y in subresult]
                                )

        # Return semi-sorted list
        s = sorted(
            result,
            key=lambda x: sum([t if t is not None else 1000 for (i, t) in enumerate(x)]),
        )
        s = [a for a in s if fingers_needed(a) <= max_fingers]
        if not return_best_as_NoteContainer:
            return s
        else:
            rnotes = self.frets_to_NoteContainer(s[0])
            for (i, x) in enumerate(rnotes):
                if x.string < len(self.tuning) - 1:
                    if res[x.string][x.fret] != []:
                        rnotes[i].name = res[x.string][x.fret][0]
            return rnotes","for (i, y) in enumerate(res[0]):
    if y != []:
        (yname, next) = (y[0], y[1])
        for (fret, name) in next:
            for s in follow(1, fret, name):
                subresult = [(i, yname)] + s
                (mi, ma, names) = (1000, -1000, [])
                for (f, n) in subresult:
                    if n is not None:
                        if f != 0 and f <= mi:
                            mi = f
                        if f != 0 and f >= ma:
                            ma = f
                        names.append(n)
                if abs(ma - mi) < max_distance:
                    covered = True
                    for n in notenames:
                        if n not in names:
                            covered = False
                    if covered and names != []:
                        result.append([y[0] if y[1] is not None else y[1] for y in subresult])","for (i, y) in enumerate(res[0]):
    (y_0, y_1, *_) = y
    if y != []:
        (yname, next) = (y_0, y_1)
        for (fret, name) in next:
            for s in follow(1, fret, name):
                subresult = [(i, yname)] + s
                (mi, ma, names) = (1000, -1000, [])
                for (f, n) in subresult:
                    if n is not None:
                        if f != 0 and f <= mi:
                            mi = f
                        if f != 0 and f >= ma:
                            ma = f
                        names.append(n)
                if abs(ma - mi) < max_distance:
                    covered = True
                    for n in notenames:
                        if n not in names:
                            covered = False
                    if covered and names != []:
                        result.append([y_0 if y_1 is not None else y_1 for y in subresult])",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, e_1 = e
variable mapping:
e_0: e[0]
e_1: e[1]",,,,,,,,
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/printing/pretty/pretty.py,https://github.com/sympy/sympy/tree/master/sympy/printing/pretty/pretty.py,PrettyPrinter,_print_Integral$426,"def _print_Integral(self, integral):
        f = integral.function

        # Add parentheses if arg involves addition of terms and
        # create a pretty form for the argument
        prettyF = self._print(f)
        # XXX generalize parens
        if f.is_Add:
            prettyF = prettyForm(*prettyF.parens())

        # dx dy dz ...
        arg = prettyF
        for x in integral.limits:
            prettyArg = self._print(x[0])
            # XXX qparens (parens if needs-parens)
            if prettyArg.width() > 1:
                prettyArg = prettyForm(*prettyArg.parens())

            arg = prettyForm(*arg.right(' d', prettyArg))

        # \int \int \int ...
        firstterm = True
        s = None
        for lim in integral.limits:
            # Create bar based on the height of the argument
            h = arg.height()
            H = h + 2

            # XXX hack!
            ascii_mode = not self._use_unicode
            if ascii_mode:
                H += 2

            vint = vobj('int', H)

            # Construct the pretty form with the integral sign and the argument
            pform = prettyForm(vint)
            pform.baseline = arg.baseline + (
                H - h)//2    # covering the whole argument

            if len(lim) > 1:
                # Create pretty forms for endpoints, if definite integral.
                # Do not print empty endpoints.
                if len(lim) == 2:
                    prettyA = prettyForm("""")
                    prettyB = self._print(lim[1])
                if len(lim) == 3:
                    prettyA = self._print(lim[1])
                    prettyB = self._print(lim[2])

                if ascii_mode:  # XXX hack
                    # Add spacing so that endpoint can more easily be
                    # identified with the correct integral sign
                    spc = max(1, 3 - prettyB.width())
                    prettyB = prettyForm(*prettyB.left(' ' * spc))

                    spc = max(1, 4 - prettyA.width())
                    prettyA = prettyForm(*prettyA.right(' ' * spc))

                pform = prettyForm(*pform.above(prettyB))
                pform = prettyForm(*pform.below(prettyA))

            if not ascii_mode:  # XXX hack
                pform = prettyForm(*pform.right(' '))

            if firstterm:
                s = pform   # first term
                firstterm = False
            else:
                s = prettyForm(*s.left(pform))

        pform = prettyForm(*arg.left(s))
        pform.binding = prettyForm.MUL
        return pform","for lim in integral.limits:
    h = arg.height()
    H = h + 2
    ascii_mode = not self._use_unicode
    if ascii_mode:
        H += 2
    vint = vobj('int', H)
    pform = prettyForm(vint)
    pform.baseline = arg.baseline + (H - h) // 2
    if len(lim) > 1:
        if len(lim) == 2:
            prettyA = prettyForm('')
            prettyB = self._print(lim[1])
        if len(lim) == 3:
            prettyA = self._print(lim[1])
            prettyB = self._print(lim[2])
        if ascii_mode:
            spc = max(1, 3 - prettyB.width())
            prettyB = prettyForm(*prettyB.left(' ' * spc))
            spc = max(1, 4 - prettyA.width())
            prettyA = prettyForm(*prettyA.right(' ' * spc))
        pform = prettyForm(*pform.above(prettyB))
        pform = prettyForm(*pform.below(prettyA))
    if not ascii_mode:
        pform = prettyForm(*pform.right(' '))
    if firstterm:
        s = pform
        firstterm = False
    else:
        s = prettyForm(*s.left(pform))","for lim in integral.limits:
    (_, lim_1, lim_2, *lim_rlimmaining) = lim
    h = arg.height()
    H = h + 2
    ascii_mode = not self._use_unicode
    if ascii_mode:
        H += 2
    vint = vobj('int', H)
    pform = prettyForm(vint)
    pform.baseline = arg.baseline + (H - h) // 2
    if len(lim) > 1:
        if len(lim) == 2:
            prettyA = prettyForm('')
            prettyB = self._print(lim_1)
        if len(lim) == 3:
            prettyA = self._print(lim_1)
            prettyB = self._print(lim_2)
        if ascii_mode:
            spc = max(1, 3 - prettyB.width())
            prettyB = prettyForm(*prettyB.left(' ' * spc))
            spc = max(1, 4 - prettyA.width())
            prettyA = prettyForm(*prettyA.right(' ' * spc))
        pform = prettyForm(*pform.above(prettyB))
        pform = prettyForm(*pform.below(prettyA))
    if not ascii_mode:
        pform = prettyForm(*pform.right(' '))
    if firstterm:
        s = pform
        firstterm = False
    else:
        s = prettyForm(*s.left(pform))",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: _, e_1, e_2, *e_remaining = e
variable mapping:
e_1: e[1]
e_2: e[2]",,,,,,,,
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/slicemesh.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/slicemesh.py,Tool,slice$211,"def slice(self, verts, faces, z, zout=None, axis=""z""):
        tags = ""[slice]""
        if axis == ""z"":
            tags = f""[slice,minz:{float(z):f}]""
        block = Block(f""slice {axis}{float(z):f} {tags}"")

        # FIXME: slice along different axes
        if axis == ""x"":
            plane_orig = (z, 0, 0)
            plane_norm = (1, 0, 0)
        elif axis == ""y"":
            plane_orig = (0, z, 0)
            plane_norm = (0, 1, 0)
        else:
            plane_orig = (0, 0, z)  # z height to slice
            plane_norm = (0, 0, 1)

        # Crosscut
        contours = meshcut.cross_section(verts, faces, plane_orig, plane_norm)

        # Flatten contours
        if zout is not None:
            for contour in contours:
                for segment in contour:
                    segment[2] = zout

        # Contours to G-code
        for contour in contours:
            gtype = 0
            for segment in contour:
                block.append(
                    f""g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}""
                )
                gtype = 1
            block.append(
                f""g1 x{contour[0][0]:f} y{contour[0][1]:f} z{contour[0][2]:f}""
            )  # Close shape
            block.append(""( ---------- cut-here ---------- )"")
        if block:
            del block[-1]

        if not block:
            block = None
        return block","for contour in contours:
    gtype = 0
    for segment in contour:
        block.append(f'g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}')
        gtype = 1
    block.append(f'g1 x{contour[0][0]:f} y{contour[0][1]:f} z{contour[0][2]:f}')
    block.append('( ---------- cut-here ---------- )')","for contour in contours:
    ((contour_0_0, contour_0_1, contour_0_2, *contour_0_rcontourmaining), *contour_rcontourmaining) = contour
    gtype = 0
    for segment in contour:
        block.append(f'g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}')
        gtype = 1
    block.append(f'g1 x{contour_0_0:f} y{contour_0_1:f} z{contour_0_2:f}')
    block.append('( ---------- cut-here ---------- )')",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: (e_0_0, e_0_1, e_0_2, *e_0_remaining), *e_remaining = e
variable mapping:
e_0_0: e[0][0]
e_0_1: e[0][1]
e_0_2: e[0][2]",,,,,,,,
gluon-nlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/scripts/processing/learn_subword.py,https://github.com/dmlc/gluon-nlp/tree/master/scripts/processing/learn_subword.py,,main$103,"def main(args):
    corpus_path_list = args.corpus
    if args.save_dir is None:
        args.save_dir = args.model
    for corpus_path in corpus_path_list:
        if not os.path.exists(corpus_path):
            raise ValueError('The path=""{}"" provided by --corpus does not exist!'
                             .format(corpus_path))
    print('Learn the ""{}""s subword model based on {}.'.format(args.model, args.corpus))
    os.makedirs(args.save_dir, exist_ok=True)
    model_prefix = os.path.join(args.save_dir, args.model)
    print('Save the subword model to {}.model'.format(model_prefix))
    print('Save the vocabulary to {}.vocab'.format(model_prefix))
    print()
    print('------- Start Training -------------')
    special_tokens_kv = OrderedDict()
    if not args.disable_unk:
        special_tokens_kv['unk_token'] = Vocab.UNK_TOKEN
    if not args.disable_bos:
        special_tokens_kv['bos_token'] = Vocab.BOS_TOKEN
    if not args.disable_eos:
        special_tokens_kv['eos_token'] = Vocab.EOS_TOKEN
    if not args.disable_pad:
        special_tokens_kv['pad_token'] = Vocab.PAD_TOKEN
    # split custom special tokens
    if args.model in ['yttm'] and len(args.custom_special_tokens) > 0:
        raise ValueError('model {} do not support custom_special_tokens'.format(args.model))
    additional_custom_special_token = OrderedDict()
    for custom_special_token in args.custom_special_tokens:
        kv = custom_special_token.split('=')
        if not len(kv) == 2:
            raise ValueError('parameter {} has wrong format'.format(custom_special_token))
        k, v = kv[0], kv[1]
        if k in special_tokens_kv:
            warnings.warn(f'There are overlaps between the custom special tokens and the'
                          f' unk, bos, eos, pad tokens. Currently, we will overwrite the '
                          f'default tokens. We will overwrite ""{k}"" to ""{v}""')
        special_tokens_kv[k] = v
        additional_custom_special_token[k] = v
    if args.model == 'hf_wordpiece':
        tokenizers = try_import_huggingface_tokenizers()
        if 'unk_token' not in special_tokens_kv or special_tokens_kv['unk_token'] != '[UNK]':
            # TODO, HF Tokenizer must have the unk token.
            special_tokens_kv['unk_token'] = '[UNK]'
        if parse_version(tokenizers.__version__) < parse_version('0.8'):
            # The older version of Tokenizers
            # hf_wordpiece must contain mask, cls and sep tokens
            # the custom defined mask,cls,sep can overwrite the default settings
            if 'mask_token' not in special_tokens_kv:
                special_tokens_kv['mask_token'] = Vocab.MASK_TOKEN
            if 'cls_token' not in special_tokens_kv:
                special_tokens_kv['cls_token'] = Vocab.CLS_TOKEN
            if 'sep_token' not in special_tokens_kv:
                special_tokens_kv['sep_token'] = Vocab.SEP_TOKEN
    special_tokens = list(special_tokens_kv.values())
    print('special tokens: ' + ', '.join(special_tokens))
    vocab = []
    if args.model == 'spm':
        try_import_sentencepiece()
        import sentencepiece as spm
        corpus_path = ','.join(corpus_path_list)
        script = '--input={} --model_prefix={} --vocab_size={} --character_coverage={} --input_sentence_size={}' \
                 .format(corpus_path, model_prefix, args.vocab_size, args.coverage, args.input_sentence_size)
        script += (' --unk_id=' + str(list(special_tokens_kv.keys()).index('unk_token')))
        script += (' --bos_id=' + ('-1' if args.disable_bos else str(list(special_tokens_kv.keys()).index('bos_token'))))
        script += (' --eos_id=' + ('-1' if args.disable_eos else str(list(special_tokens_kv.keys()).index('eos_token'))))
        script += (' --pad_id=' + ('-1' if args.disable_pad else str(list(special_tokens_kv.keys()).index('pad_token'))))
        if len(additional_custom_special_token) > 0:
            script += (' --control_symbols=' + ','.join(list(additional_custom_special_token.values())))
        print(script)
        spm.SentencePieceTrainer.Train(script)
        if 'bos_token' in special_tokens_kv:
            special_tokens_kv['bos_token'] = '<s>'
        if 'eos_token' in special_tokens_kv:
            special_tokens_kv['eos_token'] = '</s>'
        # build spm vocab
        spm_model = spm.SentencePieceProcessor()
        spm_model.load(model_prefix + '.model')
        vocab = [spm_model.id_to_piece(i) for i in range(len(spm_model))]
        os.remove(model_prefix + '.vocab')
    elif args.model == 'subword_nmt':
        try_import_subword_nmt()
        from subword_nmt import learn_bpe
        corpus_path = cat_corpus(corpus_path_list)\
            if len(corpus_path_list) > 1 else corpus_path_list[0]
        # build model
        with open(corpus_path, 'r', encoding='utf-8') as fc,\
             open(model_prefix + '.model', 'w', encoding='utf-8') as fm:
            learn_bpe.learn_bpe(fc, fm, args.vocab_size - len(special_tokens), total_symbols=True)
        # build vocab
        with open(corpus_path, 'r', encoding='utf-8') as fc, \
             open(model_prefix + '.model', 'r', encoding='utf-8') as fm:
            vocab.extend(special_tokens)
            uniq_chars_internal = set()
            uniq_chars_final = set()
            uniq_words = set()
            for line in fc:
                for word in line.strip('\r\n ').split(' '):
                    if word:
                        uniq_words.add(word)
            # this code piece is same as 
            # https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/learn_bpe.py shows
            uniq_words = [tuple(x[:-1]) + (x[-1]+'</w>',) for x in uniq_words]
            for word in uniq_words:
                for char in word[:-1]:
                    uniq_chars_internal.add(char)
                uniq_chars_final.add(word[-1])
            # sort to ensure the same settings produce the same vocab
            vocab.extend(sorted(list(uniq_chars_internal)))
            vocab.extend(sorted(list(uniq_chars_final)))
            fm.readline()
            pair = fm.readline()
            while pair:
                vocab.append(pair.replace(' ', '', 1).strip())
                pair = fm.readline()
        if len(corpus_path_list) > 1:
            os.remove(corpus_path)
    elif args.model == 'yttm':
        try_import_yttm()
        import youtokentome as yttm
        corpus_path = cat_corpus(corpus_path_list)\
            if len(corpus_path_list) > 1 else corpus_path_list[0]
        tokenizer = yttm.BPE.train(
            data=corpus_path, 
            model=model_prefix + '.model',
            vocab_size=args.vocab_size, 
            coverage=args.coverage, 
            n_threads=args.n_threads,
            unk_id=special_tokens.index(Vocab.UNK_TOKEN),
            bos_id=-1 if args.disable_bos else special_tokens.index(Vocab.BOS_TOKEN),
            eos_id=-1 if args.disable_eos else special_tokens.index(Vocab.EOS_TOKEN),
            pad_id=-1 if args.disable_pad else special_tokens.index(Vocab.PAD_TOKEN))
        vocab = tokenizer.vocab()
        if 'unk_token' in special_tokens_kv:
            special_tokens_kv['unk_token'] = '<UNK>'
        if 'bos_token' in special_tokens_kv:
            special_tokens_kv['bos_token'] = '<BOS>'
        if 'eos_token' in special_tokens_kv:
            special_tokens_kv['eos_token'] = '<EOS>'        
        if 'pad_token' in special_tokens_kv:
            special_tokens_kv['pad_token'] = '<PAD>'
        if len(corpus_path_list) > 1:
            os.remove(corpus_path)
    elif args.model in ['hf_bpe', 'hf_bytebpe', 'hf_wordpiece']:
        tokenizers = try_import_huggingface_tokenizers()
        if args.model == 'hf_bpe':
            split_on_whitespace_only = not args.split_punctuation
            tokenizer = tokenizers.CharBPETokenizer(
                lowercase=args.lowercase,
                bert_normalizer=args.bert_normalizer,
                split_on_whitespace_only=split_on_whitespace_only)
        elif args.model == 'hf_bytebpe':
            tokenizer = tokenizers.ByteLevelBPETokenizer(lowercase=args.lowercase)
        elif args.model == 'hf_wordpiece':
            unk_token = special_tokens_kv.get('unk_token', None)
            sep_token = special_tokens_kv.get('sep_token', None)
            cls_token = special_tokens_kv.get('cls_token', None)
            pad_token = special_tokens_kv.get('pad_token', None)
            mask_token = special_tokens_kv.get('mask_token', None)
            if args.bert_normalizer:
                strip_accents = None
                clean_text = True
                handle_chinese_chars = True
            else:
                strip_accents = False
                clean_text = False
                handle_chinese_chars = False
            tokenizer = tokenizers.BertWordPieceTokenizer(
                unk_token=unk_token,
                sep_token=sep_token,
                cls_token=cls_token,
                pad_token=pad_token,
                mask_token=mask_token,
                lowercase=args.lowercase,
                strip_accents=strip_accents,
                handle_chinese_chars=handle_chinese_chars,
                clean_text=clean_text
            )
        else:
            raise NotImplementedError
        tokenizer.train(
            corpus_path_list,
            vocab_size=args.vocab_size,
            show_progress=True,
            special_tokens=special_tokens)
        # Deal with the API change of tokenizers >= 0.8
        if version.parse(tokenizers.__version__) >= version.parse('0.8'):
            save_model_path = model_prefix + '.model'
            tokenizer.save(save_model_path)
            model_info = json.load(open(save_model_path, encoding='utf-8'))
            special_tokens_in_tokenizer = model_info['added_tokens']
            assert len(special_tokens_in_tokenizer) == len(special_tokens)
            hf_vocab = model_info['model']['vocab']
            hf_vocab_sorted = sorted(list(hf_vocab.items()), key=lambda x: x[1])
            hf_vocab_ids = [ele[1] for ele in hf_vocab_sorted]
            assert min(hf_vocab_ids) == 0 and max(hf_vocab_ids) == len(hf_vocab_ids) - 1
            vocab = [ele[0] for ele in hf_vocab_sorted]
        else:
            tokenizer.save(args.save_dir, args.model)
            # we replace the huggingface vocab file with our Vocab implementation
            if args.model == 'hf_wordpiece':
                hf_vocab_file = model_prefix + '-vocab.txt'
                with open(hf_vocab_file, 'r', encoding='utf-8') as fv:
                    for line in fv:
                        vocab.append(line.strip())
            else:
                # Move the hf_${model}-merges.txt to hf_${model}.models
                os.rename(os.path.join(args.save_dir, '{}-merges.txt'.format(args.model)),
                          os.path.join(args.save_dir, '{}.model'.format(args.model)))
                hf_vocab_file = model_prefix + '-vocab.json'
                with open(hf_vocab_file, 'r', encoding='utf-8') as fv:
                    vocab_kv = json.load(fv)
                    vocab_kv = sorted(list(vocab_kv.items()), key=lambda x: x[1])
                    for kv in vocab_kv:
                        vocab.append(kv[0])
            os.remove(hf_vocab_file)
    else:
        raise NotImplementedError
    vocab_obj = Vocab(vocab, **special_tokens_kv)
    vocab_obj.save(model_prefix + '.vocab')
    print('-------- Done Training -------------')","for word in uniq_words:
    for char in word[:-1]:
        uniq_chars_internal.add(char)
    uniq_chars_final.add(word[-1])","for word in uniq_words:
    (*word_rwordmaining, word_nwordg_1) = word
    for char in word[:-1]:
        uniq_chars_internal.add(char)
    uniq_chars_final.add(word_nwordg_1)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg_1 = e
variable mapping:
e_neg_1: e[-1]
e[:-1]: e_remaining",,,,,,,,
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):

        ''' Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        '''
        obj = {'Id': item_id}

        try:
            obj['KodiId'] = e_item[0]
            obj['FileId'] = e_item[1]
            obj['ParentId'] = e_item[3]
            obj['Media'] = e_item[4]
        except TypeError:
            return

        if obj['Media'] == 'episode':

            temp_obj = dict(obj)
            self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
            season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            try:
                temp_obj['Id'] = season[0]
                temp_obj['ParentId'] = season[1]
            except TypeError:
                return

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):

                self.remove_season(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

            temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))

            if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):

                for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                    self.remove_season(season[1], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))

                self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))

        elif obj['Media'] == 'tvshow':
            obj['ParentId'] = obj['KodiId']

            for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):

                temp_obj = dict(obj)
                temp_obj['ParentId'] = season[1]

                for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                    self.remove_episode(episode[1], episode[2], obj['Id'])
                else:
                    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))

            self.remove_tvshow(obj['KodiId'], obj['Id'])

        elif obj['Media'] == 'season':

            for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))

            self.remove_season(obj['KodiId'], obj['Id'])

            if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):

                self.remove_tvshow(obj['ParentId'], obj['Id'])
                self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))

        # Remove any series pooling episodes
        for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
            self.remove_episode(episode[2], episode[3], obj['Id'])
        else:
            self.jellyfin_db.remove_media_by_parent_id(obj['Id'])

        self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
    self.remove_episode(episode[2], episode[3], obj['Id'])
else:
    self.jellyfin_db.remove_media_by_parent_id(obj['Id'])","for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
    (_, _, episode_2, episode_3, *_) = episode
    self.remove_episode(episode_2, episode_3, obj['Id'])
else:
    self.jellyfin_db.remove_media_by_parent_id(obj['Id'])",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: _, _, e_2, e_3 = e
variable mapping:
e_2: e[2]
e_3: e[3]",,,,,,,,
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for segment in segment_info:
    if segment[0] == 'None':
        spk = spk_id
        spk_id += 1
    else:
        spk = temp_speaker_id[segment[0]]
    segment_id = '%s_%s_%s' % (PackZero(spk), audio_name, PackZero(segment_number))
    skip = False
    for seg in segment:
        if len(seg) < 1:
            print('warning segment %s in %s' % (segment_id, audio_name))
            skip = True
    if skip:
        continue
    print('%s %s %s %s' % (segment_id, audio_name, segment[2], segment[3]), file=segments)
    print('%s %s' % (segment_id, PackZero(spk)), file=utt2spk)
    print('%s %s' % (segment_id, segment[1]), file=text)
    spk2utt_prep[spk] = spk2utt_prep.get(spk, '') + ' %s' % segment_id
    segment_number += 1","for segment in segment_info:
    (segment_0, segment_1, segment_2, segment_3, *_) = segment
    if segment_0 == 'None':
        spk = spk_id
        spk_id += 1
    else:
        spk = temp_speaker_id[segment_0]
    segment_id = '%s_%s_%s' % (PackZero(spk), audio_name, PackZero(segment_number))
    skip = False
    for seg in segment:
        if len(seg) < 1:
            print('warning segment %s in %s' % (segment_id, audio_name))
            skip = True
    if skip:
        continue
    print('%s %s %s %s' % (segment_id, audio_name, segment_2, segment_3), file=segments)
    print('%s %s' % (segment_id, PackZero(spk)), file=utt2spk)
    print('%s %s' % (segment_id, segment_1), file=text)
    spk2utt_prep[spk] = spk2utt_prep.get(spk, '') + ' %s' % segment_id
    segment_number += 1",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,,
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(
    sound_dir,
    annotation_dir,
    target_dir,
    mode,
    speaker_info,
    new_data_dir,
    speaker_details,
    text_format,
):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    segments = open(os.path.join(target_dir, ""segments""), ""w"", encoding=""utf-8"")
    wavscp = open(os.path.join(target_dir, ""wav.scp""), ""w"", encoding=""utf-8"")
    utt2spk = open(os.path.join(target_dir, ""utt2spk""), ""w"", encoding=""utf-8"")
    spk2utt = open(os.path.join(target_dir, ""spk2utt""), ""w"", encoding=""utf-8"")
    text = open(os.path.join(target_dir, ""text""), ""w"", encoding=""utf-8"")
    name2spk = open(os.path.join(target_dir, ""name2spk""), ""w"", encoding=""utf-8"")
    remix_script = open(
        os.path.join(target_dir, ""remix_script.sh""), ""w"", encoding=""utf-8""
    )

    # get relationship
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}

    if mode == ""trs"":
        if not os.path.exists(os.path.join(target_dir, ""temp"")):
            os.mkdir(os.path.join(target_dir, ""temp""))
        audio_set = set()
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".trs"":
                    XMLRefine(
                        os.path.join(root, file), os.path.join(target_dir, ""temp"", file)
                    )
                    annotation_files[file] = os.path.join(target_dir, ""temp"", file)
        for afile in annotation_files.keys():
            if afile == ""error"":
                continue
            try:
                audio_name, speakers, segment_info = XMLProcessing(
                    annotation_files[afile]
                )
            except Exception:
                print(""error process %s"" % annotation_files[afile])
            audio_name = audio_name.replace("" "", """")
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if ""%s.wav"" % audio_name not in sound_files.keys():
                print(""no audio found for annotation: %s"" % afile)
                continue
                # write wav.scp & segments & text files
            print(
                ""%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |""
                % (audio_name, sound_files[""%s.wav"" % audio_name]),
                file=wavscp,
            )
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker][""name""]] = name2spk_prep.get(
                    speakers[speaker][""name""], spk_id
                )
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker][""name""]]
                if name2spk_prep[speakers[speaker][""name""]] == spk_id:
                    print(
                        ""%s %s"" % (speakers[speaker][""name""], PackZero(spk_id)),
                        file=name2spk,
                    )
                    spk_id += 1
            for segment in segment_info:
                # segment: [spk, text, start_time, end_time]
                if segment[0] == ""None"":
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = ""%s_%s_%s"" % (
                    PackZero(spk),
                    audio_name,
                    PackZero(segment_number),
                )

                # skip data error
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print(""warning segment %s in %s"" % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue

                print(
                    ""%s %s %s %s"" % (segment_id, audio_name, segment[2], segment[3]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, PackZero(spk)), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[1]), file=text)

                spk2utt_prep[spk] = spk2utt_prep.get(spk, """") + "" %s"" % (segment_id)
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
            print(""successfully processing %s"" % afile)
        shutil.rmtree(os.path.join(target_dir, ""temp""))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for root, dirs, files in os.walk(sound_dir):
            for file in files:
                if file[-4:] == "".wav"":
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for root, dirs, files in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == "".eaf"":
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(
                        root, file
                    )
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == ""error"":
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            left_channel_segments, right_channel_segments = segment_info

            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print(
                'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1'
                % (sound_files[afile], os.path.join(new_data_dir, afile)),
                file=remix_script,
            )

            print(
                ""%s-L %s-L.wav"" % (afile, os.path.join(new_data_dir, afile)),
                file=wavscp,
            )
            segment_number = 0
            for segment in left_channel_segments:
                # segments: start end text
                segment_id = ""%s_%s-L_%s"" % (
                    spk_info[0],
                    afile,
                    PackZero(segment_number),
                )
                if float(segment[1]) > max_length:
                    continue
                print(
                    ""%s %s-L %s %s"" % (segment_id, afile, segment[0], segment[1]),
                    file=segments,
                )
                print(""%s %s"" % (segment_id, spk_info[0]), file=utt2spk)
                print(""%s %s"" % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(
                    spk_info[0], """"
                ) + "" %s"" % (segment_id)
                segment_number += 1

            if len(right_channel_segments) > 0:
                print(
                    'sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2'
                    % (sound_files[afile], os.path.join(new_data_dir, afile)),
                    file=remix_script,
                )
                print(
                    ""%s-R %s-R.wav"" % (afile, os.path.join(new_data_dir, afile)),
                    file=wavscp,
                )
                for segment in right_channel_segments:
                    # segments: start end text
                    segment_id = ""%s_%s-R_%s"" % (
                        spk_info[1],
                        afile,
                        PackZero(segment_number),
                    )
                    if float(segment[1]) > max_length:
                        continue
                    print(
                        ""%s %s-R %s %s"" % (segment_id, afile, segment[0], segment[1]),
                        file=segments,
                    )
                    print(""%s %s"" % (segment_id, spk_info[1]), file=utt2spk)
                    print(""%s %s"" % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(
                        spk_info[1], """"
                    ) + "" %s"" % (segment_id)
                    segment_number += 1
            print(""successfully processing %s"" % afile)
        for spk in spk2utt_prep.keys():
            print(""%s %s"" % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for file in files:
    if file[-4:] == '.wav':
        sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)","for file in files:
    (*file_rfilemaining, file_nfileg_4, file_nfileg_3, file_nfileg_2, file_nfileg_1) = file
    if file[-4:] == '.wav':
        sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: *e_remaining, e_neg_4, e_neg_3, e_neg_2, e_neg_1 = e
variable mapping:
e[-4:]: e[-4:] (This unpacked element can be directly assigned to a variable using slicing)
",,,,,,,,
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/head_detection/data_provider_farm/reformat_brainwash.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/head_detection/data_provider_farm/reformat_brainwash.py,,generate_data_list$12,"def generate_data_list():
    txt_file_path = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash/brainwash_test.idl'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash'

    list_file_path = './data_folder/data_list_brainwash_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fin = open(txt_file_path, 'r')
    fout = open(list_file_path, 'w')

    counter = 0
    for line in fin:
        line = line.strip(';\n')
        im_path = re.findall('[""](.*?)[""]', line)[0]
        im_path = os.path.join(image_root, im_path)
        if not os.path.exists(im_path):
            print('im file does not exist : %s'%im_path)
            continue
        bbox_str_list = re.findall('[(](.*?)[)]', line)
        bbox_list = []
        for bbox_str in bbox_str_list:
            bbox_str = bbox_str.split(', ')
            xmin = int(float(bbox_str[0]))
            ymin = int(float(bbox_str[1]))
            xmax = int(float(bbox_str[2]))
            ymax = int(float(bbox_str[3]))
            bbox_list.append((xmin, ymin, xmax-xmin+1, ymax-ymin+1))

        if len(bbox_list) == 0:
            line_str = im_path+',0,0'
            fout.write(line_str+'\n')
        else:
            line_str = im_path+',1,'+str(len(bbox_list))
            for bbox in bbox_list:
                line_str += ','+str(bbox[0])+','+str(bbox[1])+','+str(bbox[2])+','+str(bbox[3])
            fout.write(line_str + '\n')
        counter += 1
        print(counter)

    fout.close()
    fin.close()","for bbox_str in bbox_str_list:
    bbox_str = bbox_str.split(', ')
    xmin = int(float(bbox_str[0]))
    ymin = int(float(bbox_str[1]))
    xmax = int(float(bbox_str[2]))
    ymax = int(float(bbox_str[3]))
    bbox_list.append((xmin, ymin, xmax - xmin + 1, ymax - ymin + 1))","for bbox_str in bbox_str_list:
    (bbox_str_0, bbox_str_1, bbox_str_2, bbox_str_3, *_) = bbox_str
    bbox_str = bbox_str.split(', ')
    xmin = int(float(bbox_str_0))
    ymin = int(float(bbox_str_1))
    xmax = int(float(bbox_str_2))
    ymax = int(float(bbox_str_3))
    bbox_list.append((xmin, ymin, xmax - xmin + 1, ymax - ymin + 1))",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: e_0, e_1, e_2, e_3 = e
variable mapping:
e_0: e[0]
e_1: e[1]
e_2: e[2]
e_3: e[3]",,,,,,,,
Scout2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Scout2/AWSScout2/output/console.py,https://github.com/nccgroup/Scout2/tree/master/AWSScout2/output/console.py,,format_listall_output$16,"def format_listall_output(format_file, format_item_dir, format, rule, option_prefix = None, template = None, skip_options = False):
    """"""
    Prepare listall output template

    :param format_file:
    :param format_item_dir:
    :param format:
    :param config:
    :param option_prefix:
    :param template:
    :param skip_options:
    :return:
    """"""
    # Set the list of keys if printing from a file spec
    # _LINE_(whatever)_EOL_
    # _ITEM_(resource)_METI_
    # _KEY_(path_to_value)
    if format_file and os.path.isfile(format_file):
        if not template:
            with open(format_file, 'rt') as f:
                template = f.read()
        # Optional files
        if not skip_options:
            re_option = re.compile(r'(%_OPTION_\((.*?)\)_NOITPO_)')
            optional_files = re_option.findall(template)
            for optional_file in optional_files:
                if optional_file[1].startswith(option_prefix + '-'):
                    with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:
                        template = template.replace(optional_file[0].strip(), f.read())
        # Include files if needed
        re_file = re.compile(r'(_FILE_\((.*?)\)_ELIF_)')
        while True:
            requested_files = re_file.findall(template)
            available_files = os.listdir(format_item_dir) if format_item_dir else []
            for requested_file in requested_files:
                if requested_file[1].strip() in available_files:
                    with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
                        template = template.replace(requested_file[0].strip(), f.read())
            # Find items and keys to be printed
            re_line = re.compile(r'(_ITEM_\((.*?)\)_METI_)')
            re_key = re.compile(r'_KEY_\(*(.*?)\)', re.DOTALL|re.MULTILINE) # Remove the multiline ?
            lines = re_line.findall(template)
            for (i, line) in enumerate(lines):
                lines[i] = line + (re_key.findall(line[1]),)
            requested_files = re_file.findall(template)
            if len(requested_files) == 0:
                break
    elif format and format[0] == 'csv':
        keys = rule.keys
        line = ', '.join('_KEY_(%s)' % k for k in keys)
        lines = [ (line, line, keys) ]
        template = line
    return (lines, template)","for (i, line) in enumerate(lines):
    lines[i] = line + (re_key.findall(line[1]),)","for (i, line) in enumerate(lines):
    (_, line_1, *line_rlinemaining) = line
    lines[i] = line + (re_key.findall(line_1),)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,,
sympy_gamma,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy_gamma/app/views.py,https://github.com/sympy/sympy_gamma/tree/master/app/views.py,,random_example$132,"def random_example(request):
    examples = []

    for category in EXAMPLES:
        for subcategory in category[1]:
            for example in subcategory[1]:
                if isinstance(example, tuple):
                    examples.append(example[1])
                else:
                    examples.append(example)

    return redirect('input/?i=' + six.moves.urllib.parse.quote(random.choice(examples)))","for example in subcategory[1]:
    if isinstance(example, tuple):
        examples.append(example[1])
    else:
        examples.append(example)","for example in subcategory[1]:
    (_, example_1, *example_rexamplemaining) = example
    if isinstance(example, tuple):
        examples.append(example_1)
    else:
        examples.append(example)",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: _, e_1, *e_remaining = e
variable mapping:
e_1: e[1]",,,,,,,,
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""鑾峰彇鍚勭嶈祫婧""""""

    outline = Outline()
    playlist = Playlist()
    counter = Counter()

    video_list = []
    pdf_list = []
    rich_text_list = []

    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190',
                 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean',
                 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id,
                 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr',
                     data=post_data).text.encode('utf_8').decode('unicode_escape')

    chapters = re.findall(r'homeworks=\w+;.+id=(\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)

        lessons = re.findall(r'chapterId=' + chapter[0] + r'.+contentType=1.+id=(\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)

            videos = re.findall(r'contentId=(\d+).+contentType=(1).+id=(\d+).+lessonId=' +
                                lesson[0] + r'.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()

            pdfs = re.findall(r'contentId=(\d+).+contentType=(3).+id=(\d+).+lessonId=' +
                              lesson[0] + r'.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()

            rich_text = re.findall(r'contentId=(\d+).+contentType=(4).+id=(\d+).+jsonContent=(.+);.+lessonId=' +
                                   lesson[0] + r'.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1),
                                  'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')

                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm',
                                           WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()

    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for video in videos:
    counter.add(2)
    outline.write(video[3], counter, 2, sign='#')
    video_list.append(Video(counter, video[3], video))","for video in videos:
    (_, _, _, video_3, *_) = video
    counter.add(2)
    outline.write(video_3, counter, 2, sign='#')
    video_list.append(Video(counter, video_3, video))",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: _, _, _, e_3 = e
variable mapping:
e_3: e[3]",,,,,,,,
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""鑾峰彇鍚勭嶈祫婧""""""

    outline = Outline()
    playlist = Playlist()
    counter = Counter()

    video_list = []
    pdf_list = []
    rich_text_list = []

    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190',
                 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean',
                 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id,
                 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr',
                     data=post_data).text.encode('utf_8').decode('unicode_escape')

    chapters = re.findall(r'homeworks=\w+;.+id=(\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)

        lessons = re.findall(r'chapterId=' + chapter[0] + r'.+contentType=1.+id=(\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)

            videos = re.findall(r'contentId=(\d+).+contentType=(1).+id=(\d+).+lessonId=' +
                                lesson[0] + r'.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()

            pdfs = re.findall(r'contentId=(\d+).+contentType=(3).+id=(\d+).+lessonId=' +
                              lesson[0] + r'.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()

            rich_text = re.findall(r'contentId=(\d+).+contentType=(4).+id=(\d+).+jsonContent=(.+);.+lessonId=' +
                                   lesson[0] + r'.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1),
                                  'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')

                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm',
                                           WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()

    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for pdf in pdfs:
    counter.add(2)
    outline.write(pdf[3], counter, 2, sign='*')
    if CONFIG['doc']:
        pdf_list.append(Document(counter, pdf[3], pdf))","for pdf in pdfs:
    (_, _, _, pdf_3, *_) = pdf
    counter.add(2)
    outline.write(pdf_3, counter, 2, sign='*')
    if CONFIG['doc']:
        pdf_list.append(Document(counter, pdf_3, pdf))",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: _, _, _, e_3 = e
variable mapping:
e_3: e[3]",,,,,,,,
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""鑾峰彇鍚勭嶈祫婧""""""

    outline = Outline()
    playlist = Playlist()
    counter = Counter()

    video_list = []
    pdf_list = []
    rich_text_list = []

    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190',
                 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean',
                 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id,
                 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr',
                     data=post_data).text.encode('utf_8').decode('unicode_escape')

    chapters = re.findall(r'homeworks=\w+;.+id=(\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)

        lessons = re.findall(r'chapterId=' + chapter[0] + r'.+contentType=1.+id=(\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)

            videos = re.findall(r'contentId=(\d+).+contentType=(1).+id=(\d+).+lessonId=' +
                                lesson[0] + r'.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()

            pdfs = re.findall(r'contentId=(\d+).+contentType=(3).+id=(\d+).+lessonId=' +
                              lesson[0] + r'.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()

            rich_text = re.findall(r'contentId=(\d+).+contentType=(4).+id=(\d+).+jsonContent=(.+);.+lessonId=' +
                                   lesson[0] + r'.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1),
                                  'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')

                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm',
                                           WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()

    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for text in rich_text:
    counter.add(2)
    outline.write(text[4], counter, 2, sign='+')
    if CONFIG['text']:
        rich_text_list.append(RichText(counter, text[4], text))
    if CONFIG['file']:
        if text[3] != 'null' and text[3] != '""""':
            params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
            file_name = Resource.file_to_save(params['fileName'])
            outline.write(file_name, counter, 2, sign='!')
            WORK_DIR.change('Files')
            res_print(params['fileName'])
            file_name = '%s %s' % (counter, file_name)
            CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})","for text in rich_text:
    (_, _, _, text_3, text_4, *_) = text
    counter.add(2)
    outline.write(text_4, counter, 2, sign='+')
    if CONFIG['text']:
        rich_text_list.append(RichText(counter, text_4, text))
    if CONFIG['file']:
        if text_3 != 'null' and text_3 != '""""':
            params = {'nosKey': re.search('nosKey"":""(.+?)""', text_3).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text_3).group(1)}
            file_name = Resource.file_to_save(params['fileName'])
            outline.write(file_name, counter, 2, sign='!')
            WORK_DIR.change('Files')
            res_print(params['fileName'])
            file_name = '%s %s' % (counter, file_name)
            CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})",Cannot refactor,Cannot refactor,2,1,,"Answer: Yes
Iterable Unpacking: _, _, _, e_3, e_4 = e
variable mapping:
e_3: e[3]
e_4: e[4]",,,,,,,,
no_find,,,,,,,,,,,,,,,,,,,,,
