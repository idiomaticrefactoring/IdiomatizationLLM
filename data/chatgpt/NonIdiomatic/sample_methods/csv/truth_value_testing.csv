repo_name,file_html,class_name,me_name,me_code,old_code,new_code
pycls,https://github.com/facebookresearch/pycls/tree/master/pycls/models/blocks.py,,patchify2d_cx$88,"def patchify2d_cx(cx, w_in, w_out, k, *, bias=True):
    """"""Accumulates complexity of patchify2d into cx = (h, w, flops, params, acts).""""""
    err_str = ""Only kernel sizes divisible by the input size are supported.""
    assert cx[""h""] % k == 0 and cx[""w""] % k == 0, err_str
    h, w, flops, params, acts = cx[""h""], cx[""w""], cx[""flops""], cx[""params""], cx[""acts""]
    h, w = h // k, w // k
    flops += k * k * w_in * w_out * h * w + (w_out * h * w if bias else 0)
    params += k * k * w_in * w_out + (w_out if bias else 0)
    acts += w_out * h * w
    return {""h"": h, ""w"": w, ""flops"": flops, ""params"": params, ""acts"": acts}",cx['h'] % k == 0,not cx['h'] % k
pycls,https://github.com/facebookresearch/pycls/tree/master/pycls/models/blocks.py,,patchify2d_cx$88,"def patchify2d_cx(cx, w_in, w_out, k, *, bias=True):
    """"""Accumulates complexity of patchify2d into cx = (h, w, flops, params, acts).""""""
    err_str = ""Only kernel sizes divisible by the input size are supported.""
    assert cx[""h""] % k == 0 and cx[""w""] % k == 0, err_str
    h, w, flops, params, acts = cx[""h""], cx[""w""], cx[""flops""], cx[""params""], cx[""acts""]
    h, w = h // k, w // k
    flops += k * k * w_in * w_out * h * w + (w_out * h * w if bias else 0)
    params += k * k * w_in * w_out + (w_out if bias else 0)
    acts += w_out * h * w
    return {""h"": h, ""w"": w, ""flops"": flops, ""params"": params, ""acts"": acts}",cx['w'] % k == 0,not cx['w'] % k
LinOTP,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/rw_lock.py,RWLock,release$105,"def release(self):
        """"""Release a lock, whether read or write.""""""
        self.monitor.acquire()
        if self.rwlock < 0:
            self.rwlock = 0
        else:
            self.rwlock -= 1
        wake_writers = self.writers_waiting and self.rwlock == 0
        wake_readers = self.writers_waiting == 0
        self.monitor.release()
        if wake_writers:
            self.writers_ok.acquire()
            self.writers_ok.notify()
            self.writers_ok.release()
        elif wake_readers:
            self.readers_ok.acquire()
            self.readers_ok.notifyAll()
            self.readers_ok.release()",self.rwlock == 0,not self.rwlock
graph-rcnn.pytorch,https://github.com/jwyang/graph-rcnn.pytorch/tree/master/lib/data/evaluation/coco/coco_eval.py,,evaluate_box_proposals$189,"def evaluate_box_proposals(
    predictions, dataset, thresholds=None, area=""all"", limit=None
):
    """"""Evaluate detection proposal recall metrics. This function is a much
    faster alternative to the official COCO API recall evaluation code. However,
    it produces slightly different results.
    """"""
    # Record max overlap value for each gt box
    # Return vector of overlap values
    areas = {
        ""all"": 0,
        ""small"": 1,
        ""medium"": 2,
        ""large"": 3,
        ""96-128"": 4,
        ""128-256"": 5,
        ""256-512"": 6,
        ""512-inf"": 7,
    }
    area_ranges = [
        [0 ** 2, 1e5 ** 2],  # all
        [0 ** 2, 32 ** 2],  # small
        [32 ** 2, 96 ** 2],  # medium
        [96 ** 2, 1e5 ** 2],  # large
        [96 ** 2, 128 ** 2],  # 96-128
        [128 ** 2, 256 ** 2],  # 128-256
        [256 ** 2, 512 ** 2],  # 256-512
        [512 ** 2, 1e5 ** 2],
    ]  # 512-inf
    assert area in areas, ""Unknown area range: {}"".format(area)
    area_range = area_ranges[areas[area]]
    gt_overlaps = []
    num_pos = 0

    for image_id, prediction in enumerate(predictions):
        original_id = image_id # dataset.id_to_img_map[image_id]

        img_info = dataset.get_img_info(image_id)
        image_width = img_info[""width""]
        image_height = img_info[""height""]
        prediction = prediction.resize((image_width, image_height))

        # sort predictions in descending order
        # TODO maybe remove this and make it explicit in the documentation
        inds = prediction.get_field(""objectness"").sort(descending=True)[1]
        prediction = prediction[inds]

        ann_ids = dataset.coco.getAnnIds(imgIds=original_id)
        anno = dataset.coco.loadAnns(ann_ids)
        gt_boxes = [obj[""bbox""] for obj in anno if obj[""iscrowd""] == 0]
        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes
        gt_boxes = BoxList(gt_boxes, (image_width, image_height), mode=""xywh"").convert(
            ""xyxy""
        )
        gt_areas = torch.as_tensor([obj[""area""] for obj in anno if obj[""iscrowd""] == 0])

        if len(gt_boxes) == 0:
            continue

        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])
        gt_boxes = gt_boxes[valid_gt_inds]

        num_pos += len(gt_boxes)

        if len(gt_boxes) == 0:
            continue

        if len(prediction) == 0:
            continue

        if limit is not None and len(prediction) > limit:
            prediction = prediction[:limit]

        overlaps = boxlist_iou(prediction, gt_boxes)

        _gt_overlaps = torch.zeros(len(gt_boxes))
        for j in range(min(len(prediction), len(gt_boxes))):
            # find which proposal box maximally covers each gt box
            # and get the iou amount of coverage for each gt box
            max_overlaps, argmax_overlaps = overlaps.max(dim=0)

            # find which gt box is 'best' covered (i.e. 'best' = most iou)
            gt_ovr, gt_ind = max_overlaps.max(dim=0)
            assert gt_ovr >= 0
            # find the proposal box that covers the best covered gt box
            box_ind = argmax_overlaps[gt_ind]
            # record the iou coverage of this gt box
            _gt_overlaps[j] = overlaps[box_ind, gt_ind]
            assert _gt_overlaps[j] == gt_ovr
            # mark the proposal box and the gt box as used
            overlaps[box_ind, :] = -1
            overlaps[:, gt_ind] = -1

        # append recorded iou coverage level
        gt_overlaps.append(_gt_overlaps)
    gt_overlaps = torch.cat(gt_overlaps, dim=0)
    gt_overlaps, _ = torch.sort(gt_overlaps)

    if thresholds is None:
        step = 0.05
        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)
    recalls = torch.zeros_like(thresholds)
    # compute recall for each iou threshold
    for i, t in enumerate(thresholds):
        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)
    # ar = 2 * np.trapz(recalls, thresholds)
    ar = recalls.mean()
    return {
        ""ar"": ar,
        ""recalls"": recalls,
        ""thresholds"": thresholds,
        ""gt_overlaps"": gt_overlaps,
        ""num_pos"": num_pos,
    }",len(gt_boxes) == 0,not gt_boxes
graph-rcnn.pytorch,https://github.com/jwyang/graph-rcnn.pytorch/tree/master/lib/data/evaluation/coco/coco_eval.py,,evaluate_box_proposals$189,"def evaluate_box_proposals(
    predictions, dataset, thresholds=None, area=""all"", limit=None
):
    """"""Evaluate detection proposal recall metrics. This function is a much
    faster alternative to the official COCO API recall evaluation code. However,
    it produces slightly different results.
    """"""
    # Record max overlap value for each gt box
    # Return vector of overlap values
    areas = {
        ""all"": 0,
        ""small"": 1,
        ""medium"": 2,
        ""large"": 3,
        ""96-128"": 4,
        ""128-256"": 5,
        ""256-512"": 6,
        ""512-inf"": 7,
    }
    area_ranges = [
        [0 ** 2, 1e5 ** 2],  # all
        [0 ** 2, 32 ** 2],  # small
        [32 ** 2, 96 ** 2],  # medium
        [96 ** 2, 1e5 ** 2],  # large
        [96 ** 2, 128 ** 2],  # 96-128
        [128 ** 2, 256 ** 2],  # 128-256
        [256 ** 2, 512 ** 2],  # 256-512
        [512 ** 2, 1e5 ** 2],
    ]  # 512-inf
    assert area in areas, ""Unknown area range: {}"".format(area)
    area_range = area_ranges[areas[area]]
    gt_overlaps = []
    num_pos = 0

    for image_id, prediction in enumerate(predictions):
        original_id = image_id # dataset.id_to_img_map[image_id]

        img_info = dataset.get_img_info(image_id)
        image_width = img_info[""width""]
        image_height = img_info[""height""]
        prediction = prediction.resize((image_width, image_height))

        # sort predictions in descending order
        # TODO maybe remove this and make it explicit in the documentation
        inds = prediction.get_field(""objectness"").sort(descending=True)[1]
        prediction = prediction[inds]

        ann_ids = dataset.coco.getAnnIds(imgIds=original_id)
        anno = dataset.coco.loadAnns(ann_ids)
        gt_boxes = [obj[""bbox""] for obj in anno if obj[""iscrowd""] == 0]
        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes
        gt_boxes = BoxList(gt_boxes, (image_width, image_height), mode=""xywh"").convert(
            ""xyxy""
        )
        gt_areas = torch.as_tensor([obj[""area""] for obj in anno if obj[""iscrowd""] == 0])

        if len(gt_boxes) == 0:
            continue

        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])
        gt_boxes = gt_boxes[valid_gt_inds]

        num_pos += len(gt_boxes)

        if len(gt_boxes) == 0:
            continue

        if len(prediction) == 0:
            continue

        if limit is not None and len(prediction) > limit:
            prediction = prediction[:limit]

        overlaps = boxlist_iou(prediction, gt_boxes)

        _gt_overlaps = torch.zeros(len(gt_boxes))
        for j in range(min(len(prediction), len(gt_boxes))):
            # find which proposal box maximally covers each gt box
            # and get the iou amount of coverage for each gt box
            max_overlaps, argmax_overlaps = overlaps.max(dim=0)

            # find which gt box is 'best' covered (i.e. 'best' = most iou)
            gt_ovr, gt_ind = max_overlaps.max(dim=0)
            assert gt_ovr >= 0
            # find the proposal box that covers the best covered gt box
            box_ind = argmax_overlaps[gt_ind]
            # record the iou coverage of this gt box
            _gt_overlaps[j] = overlaps[box_ind, gt_ind]
            assert _gt_overlaps[j] == gt_ovr
            # mark the proposal box and the gt box as used
            overlaps[box_ind, :] = -1
            overlaps[:, gt_ind] = -1

        # append recorded iou coverage level
        gt_overlaps.append(_gt_overlaps)
    gt_overlaps = torch.cat(gt_overlaps, dim=0)
    gt_overlaps, _ = torch.sort(gt_overlaps)

    if thresholds is None:
        step = 0.05
        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)
    recalls = torch.zeros_like(thresholds)
    # compute recall for each iou threshold
    for i, t in enumerate(thresholds):
        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)
    # ar = 2 * np.trapz(recalls, thresholds)
    ar = recalls.mean()
    return {
        ""ar"": ar,
        ""recalls"": recalls,
        ""thresholds"": thresholds,
        ""gt_overlaps"": gt_overlaps,
        ""num_pos"": num_pos,
    }",len(gt_boxes) == 0,not gt_boxes
graph-rcnn.pytorch,https://github.com/jwyang/graph-rcnn.pytorch/tree/master/lib/data/evaluation/coco/coco_eval.py,,evaluate_box_proposals$189,"def evaluate_box_proposals(
    predictions, dataset, thresholds=None, area=""all"", limit=None
):
    """"""Evaluate detection proposal recall metrics. This function is a much
    faster alternative to the official COCO API recall evaluation code. However,
    it produces slightly different results.
    """"""
    # Record max overlap value for each gt box
    # Return vector of overlap values
    areas = {
        ""all"": 0,
        ""small"": 1,
        ""medium"": 2,
        ""large"": 3,
        ""96-128"": 4,
        ""128-256"": 5,
        ""256-512"": 6,
        ""512-inf"": 7,
    }
    area_ranges = [
        [0 ** 2, 1e5 ** 2],  # all
        [0 ** 2, 32 ** 2],  # small
        [32 ** 2, 96 ** 2],  # medium
        [96 ** 2, 1e5 ** 2],  # large
        [96 ** 2, 128 ** 2],  # 96-128
        [128 ** 2, 256 ** 2],  # 128-256
        [256 ** 2, 512 ** 2],  # 256-512
        [512 ** 2, 1e5 ** 2],
    ]  # 512-inf
    assert area in areas, ""Unknown area range: {}"".format(area)
    area_range = area_ranges[areas[area]]
    gt_overlaps = []
    num_pos = 0

    for image_id, prediction in enumerate(predictions):
        original_id = image_id # dataset.id_to_img_map[image_id]

        img_info = dataset.get_img_info(image_id)
        image_width = img_info[""width""]
        image_height = img_info[""height""]
        prediction = prediction.resize((image_width, image_height))

        # sort predictions in descending order
        # TODO maybe remove this and make it explicit in the documentation
        inds = prediction.get_field(""objectness"").sort(descending=True)[1]
        prediction = prediction[inds]

        ann_ids = dataset.coco.getAnnIds(imgIds=original_id)
        anno = dataset.coco.loadAnns(ann_ids)
        gt_boxes = [obj[""bbox""] for obj in anno if obj[""iscrowd""] == 0]
        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes
        gt_boxes = BoxList(gt_boxes, (image_width, image_height), mode=""xywh"").convert(
            ""xyxy""
        )
        gt_areas = torch.as_tensor([obj[""area""] for obj in anno if obj[""iscrowd""] == 0])

        if len(gt_boxes) == 0:
            continue

        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])
        gt_boxes = gt_boxes[valid_gt_inds]

        num_pos += len(gt_boxes)

        if len(gt_boxes) == 0:
            continue

        if len(prediction) == 0:
            continue

        if limit is not None and len(prediction) > limit:
            prediction = prediction[:limit]

        overlaps = boxlist_iou(prediction, gt_boxes)

        _gt_overlaps = torch.zeros(len(gt_boxes))
        for j in range(min(len(prediction), len(gt_boxes))):
            # find which proposal box maximally covers each gt box
            # and get the iou amount of coverage for each gt box
            max_overlaps, argmax_overlaps = overlaps.max(dim=0)

            # find which gt box is 'best' covered (i.e. 'best' = most iou)
            gt_ovr, gt_ind = max_overlaps.max(dim=0)
            assert gt_ovr >= 0
            # find the proposal box that covers the best covered gt box
            box_ind = argmax_overlaps[gt_ind]
            # record the iou coverage of this gt box
            _gt_overlaps[j] = overlaps[box_ind, gt_ind]
            assert _gt_overlaps[j] == gt_ovr
            # mark the proposal box and the gt box as used
            overlaps[box_ind, :] = -1
            overlaps[:, gt_ind] = -1

        # append recorded iou coverage level
        gt_overlaps.append(_gt_overlaps)
    gt_overlaps = torch.cat(gt_overlaps, dim=0)
    gt_overlaps, _ = torch.sort(gt_overlaps)

    if thresholds is None:
        step = 0.05
        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)
    recalls = torch.zeros_like(thresholds)
    # compute recall for each iou threshold
    for i, t in enumerate(thresholds):
        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)
    # ar = 2 * np.trapz(recalls, thresholds)
    ar = recalls.mean()
    return {
        ""ar"": ar,
        ""recalls"": recalls,
        ""thresholds"": thresholds,
        ""gt_overlaps"": gt_overlaps,
        ""num_pos"": num_pos,
    }",len(prediction) == 0,not prediction
elasticsearch-dsl-py,https://github.com/elastic/elasticsearch-dsl-py/tree/master/tests/test_query.py,,test_empty_bool_has_min_should_match_0$176,"def test_empty_bool_has_min_should_match_0():
    assert 0 == query.Bool()._min_should_match",0 == query.Bool()._min_should_match,not query.Bool()._min_should_match
mmocr,https://github.com/open-mmlab/mmocr/tree/master/mmocr/datasets/pipelines/textdet_targets/textsnake_targets.py,TextSnakeTargets,find_head_tail$63,"def find_head_tail(self, points, orientation_thr):
        """"""Find the head edge and tail edge of a text polygon.

        Args:
            points (ndarray): The points composing a text polygon.
            orientation_thr (float): The threshold for distinguishing between
                head edge and tail edge among the horizontal and vertical edges
                of a quadrangle.

        Returns:
            head_inds (list): The indexes of two points composing head edge.
            tail_inds (list): The indexes of two points composing tail edge.
        """"""

        assert points.ndim == 2
        assert points.shape[0] >= 4
        assert points.shape[1] == 2
        assert isinstance(orientation_thr, float)

        if len(points) > 4:
            pad_points = np.vstack([points, points[0]])
            edge_vec = pad_points[1:] - pad_points[:-1]

            theta_sum = []
            adjacent_vec_theta = []
            for i, edge_vec1 in enumerate(edge_vec):
                adjacent_ind = [x % len(edge_vec) for x in [i - 1, i + 1]]
                adjacent_edge_vec = edge_vec[adjacent_ind]
                temp_theta_sum = np.sum(
                    self.vector_angle(edge_vec1, adjacent_edge_vec))
                temp_adjacent_theta = self.vector_angle(
                    adjacent_edge_vec[0], adjacent_edge_vec[1])
                theta_sum.append(temp_theta_sum)
                adjacent_vec_theta.append(temp_adjacent_theta)
            theta_sum_score = np.array(theta_sum) / np.pi
            adjacent_theta_score = np.array(adjacent_vec_theta) / np.pi
            poly_center = np.mean(points, axis=0)
            edge_dist = np.maximum(
                norm(pad_points[1:] - poly_center, axis=-1),
                norm(pad_points[:-1] - poly_center, axis=-1))
            dist_score = edge_dist / (np.max(edge_dist) + self.eps)
            position_score = np.zeros(len(edge_vec))
            score = 0.5 * theta_sum_score + 0.15 * adjacent_theta_score
            score += 0.35 * dist_score
            if len(points) % 2 == 0:
                position_score[(len(score) // 2 - 1)] += 1
                position_score[-1] += 1
            score += 0.1 * position_score
            pad_score = np.concatenate([score, score])
            score_matrix = np.zeros((len(score), len(score) - 3))
            x = np.arange(len(score) - 3) / float(len(score) - 4)
            gaussian = 1. / (np.sqrt(2. * np.pi) * 0.5) * np.exp(-np.power(
                (x - 0.5) / 0.5, 2.) / 2)
            gaussian = gaussian / np.max(gaussian)
            for i in range(len(score)):
                score_matrix[i, :] = score[i] + pad_score[
                    (i + 2):(i + len(score) - 1)] * gaussian * 0.3

            head_start, tail_increment = np.unravel_index(
                score_matrix.argmax(), score_matrix.shape)
            tail_start = (head_start + tail_increment + 2) % len(points)
            head_end = (head_start + 1) % len(points)
            tail_end = (tail_start + 1) % len(points)

            if head_end > tail_end:
                head_start, tail_start = tail_start, head_start
                head_end, tail_end = tail_end, head_end
            head_inds = [head_start, head_end]
            tail_inds = [tail_start, tail_end]
        else:
            if self.vector_slope(points[1] - points[0]) + self.vector_slope(
                    points[3] - points[2]) < self.vector_slope(
                        points[2] - points[1]) + self.vector_slope(points[0] -
                                                                   points[3]):
                horizontal_edge_inds = [[0, 1], [2, 3]]
                vertical_edge_inds = [[3, 0], [1, 2]]
            else:
                horizontal_edge_inds = [[3, 0], [1, 2]]
                vertical_edge_inds = [[0, 1], [2, 3]]

            vertical_len_sum = norm(points[vertical_edge_inds[0][0]] -
                                    points[vertical_edge_inds[0][1]]) + norm(
                                        points[vertical_edge_inds[1][0]] -
                                        points[vertical_edge_inds[1][1]])
            horizontal_len_sum = norm(
                points[horizontal_edge_inds[0][0]] -
                points[horizontal_edge_inds[0][1]]) + norm(
                    points[horizontal_edge_inds[1][0]] -
                    points[horizontal_edge_inds[1][1]])

            if vertical_len_sum > horizontal_len_sum * orientation_thr:
                head_inds = horizontal_edge_inds[0]
                tail_inds = horizontal_edge_inds[1]
            else:
                head_inds = vertical_edge_inds[0]
                tail_inds = vertical_edge_inds[1]

        return head_inds, tail_inds",len(points) % 2 == 0,not len(points) % 2
adb-enhanced,https://github.com/ashishb/adb-enhanced/tree/master/adbe/adb_enhanced.py,,alarm_manager$1925,"def alarm_manager(param):
    cmd = ""dumpsys alarm""
    api_version = get_device_android_api_version()
    err_msg_api = ""Your Android version (API 28 and bellow) does not support "" \
                  ""listing pending alarm""

    c, o, e = execute_adb_shell_command2(cmd)
    if c != 0:
        print_error_and_exit(""Something gone wrong on ""
                             ""dumping alarms. Error: %s"" % e)
        return

    if not isinstance(param, AlarmEnum):
        print_error(""Not supported parameter"")
        return

    run_all = 0
    padding = """"
    if param == AlarmEnum.ALL:
        run_all = 1
        padding = ""\t""

    if param == AlarmEnum.TOP or run_all == 1:
        print_top_alarms(o, padding)

    if param == AlarmEnum.PENDING or run_all == 1:
        if api_version > 28:
            print_pending_alarms(o, padding)
        else:
            print_error(err_msg_api)

    if param == AlarmEnum.HISTORY or run_all == 1:
        if api_version > 28:
            print_history_alarms(o, padding)
        else:
            print_error(err_msg_api)",c != 0,c
CompilerGym,https://github.com/facebookresearch/CompilerGym/tree/master/tests/datasets/benchmark_test.py,,test_add_validation_callbacks_values$54,"def test_add_validation_callbacks_values():
    """"""Test methods for adding and checking custom validation callbacks.""""""

    def a(env):
        pass

    benchmark = Benchmark(BenchmarkProto(uri=""benchmark://example-compiler-v0/foobar""))
    assert benchmark.validation_callbacks() == []
    assert not benchmark.is_validatable()

    benchmark.add_validation_callback(a)
    assert benchmark.validation_callbacks() == [a]
    assert benchmark.is_validatable()

    benchmark.add_validation_callback(a)
    assert benchmark.validation_callbacks() == [a, a]",benchmark.validation_callbacks() == [],not benchmark.validation_callbacks()
katana,https://github.com/JohnHammond/katana/tree/master/katana/units/esoteric/ook.py,Unit,__init__$87,"def __init__(self, *args, **kwargs):
        super(Unit, self).__init__(*args, **kwargs)

        matches = OOK_REGEX.findall(self.target.raw)

        if matches is None or matches == []:
            raise NotApplicable(""no ook potential found"")
        else:
            self.code = b"""".join([m[-1] for m in matches])",matches == [],not matches
great_expectations,https://github.com/great-expectations/great_expectations/tree/master/tests/datasource/data_connector/test_configured_asset_filesystem_data_connector.py,,test_basic_instantiation$28,"def test_basic_instantiation(tmp_path_factory):
    base_directory = str(tmp_path_factory.mktemp(""test_basic_instantiation""))
    create_files_in_directory(
        directory=base_directory,
        file_name_list=[
            ""alpha-1.csv"",
            ""alpha-2.csv"",
            ""alpha-3.csv"",
        ],
    )

    my_data_connector = ConfiguredAssetFilesystemDataConnector(
        name=""my_data_connector"",
        datasource_name=""FAKE_DATASOURCE_NAME"",
        execution_engine=PandasExecutionEngine(),
        default_regex={
            ""pattern"": ""alpha-(.*)\\.csv"",
            ""group_names"": [""index""],
        },
        base_directory=base_directory,
        assets={""alpha"": {}},
    )

    assert my_data_connector.self_check() == {
        ""class_name"": ""ConfiguredAssetFilesystemDataConnector"",
        ""data_asset_count"": 1,
        ""example_data_asset_names"": [
            ""alpha"",
        ],
        ""data_assets"": {
            ""alpha"": {
                ""example_data_references"": [
                    ""alpha-1.csv"",
                    ""alpha-2.csv"",
                    ""alpha-3.csv"",
                ],
                ""batch_definition_count"": 3,
            },
        },
        ""example_unmatched_data_references"": [],
        ""unmatched_data_reference_count"": 0,
        # FIXME: (Sam) example_data_reference removed temporarily in PR #2590:
        # ""example_data_reference"": {},
    }

    # noinspection PyProtectedMember
    my_data_connector._refresh_data_references_cache()
    assert my_data_connector.get_data_reference_list_count() == 3
    assert my_data_connector.get_unmatched_data_references() == []

    # Illegal execution environment name
    with pytest.raises(ValueError):
        print(
            my_data_connector.get_batch_definition_list_from_batch_request(
                BatchRequest(
                    datasource_name=""something"",
                    data_connector_name=""my_data_connector"",
                    data_asset_name=""something"",
                )
            )
        )",my_data_connector.get_unmatched_data_references() == [],not my_data_connector.get_unmatched_data_references()
sawtooth-core,https://github.com/hyperledger/sawtooth-core/tree/master/validator/sawtooth_validator/journal/consensus/dev_mode/dev_mode_consensus.py,BlockPublisher,check_publish_block$103,"def check_publish_block(self, block_header):
        """"""Check if a candidate block is ready to be claimed.

        block_header (BlockHeader): the block_header to be checked if it
            should be claimed
        Returns:
            Boolean: True if the candidate block_header should be claimed.
        """"""
        if any(publisher_key != block_header.signer_public_key
               for publisher_key in self._valid_block_publishers):
            return False

        if self._min_wait_time == 0:
            return True

        if self._min_wait_time < 0:
            return False

        assert self._min_wait_time > 0

        if self._max_wait_time <= 0:
            return self._start_time + self._min_wait_time <= time.time()

        assert self._max_wait_time > 0

        if self._max_wait_time <= self._min_wait_time:
            return False

        assert 0 < self._min_wait_time < self._max_wait_time

        return self._start_time + self._wait_time <= time.time()",self._min_wait_time == 0,not self._min_wait_time
gixy,https://github.com/yandex/gixy/tree/master/gixy/core/regexp.py,MaxRepeatToken,must_contain$364,"def must_contain(self, char):
        if self.max == 0:
            # [a-z]{0}
            return False
        if self.min == 0:
            # [a-z]?
            return False
        for child in self.childs:
            if child.must_contain(char):
                return True
        return False",self.max == 0,not self.max
gixy,https://github.com/yandex/gixy/tree/master/gixy/core/regexp.py,MaxRepeatToken,must_contain$364,"def must_contain(self, char):
        if self.max == 0:
            # [a-z]{0}
            return False
        if self.min == 0:
            # [a-z]?
            return False
        for child in self.childs:
            if child.must_contain(char):
                return True
        return False",self.min == 0,not self.min
NLP-Tutorials,https://github.com/MorvanZhou/NLP-Tutorials/tree/master/pytorch/CBOW.py,,train$62,"def train(model,data):
    if torch.cuda.is_available():
        print(""GPU train avaliable"")
        device =torch.device(""cuda"")
        model = model.cuda()
    else:
        device = torch.device(""cpu"")
        model = model.cpu()
    for t in range(2500):
        bx,by = data.sample(16)
        bx,by = torch.from_numpy(bx).to(device), torch.from_numpy(by).to(device)
        loss = model.step(bx,by)
        if t%200 == 0:
            print(f""step: {t}  |  loss: {loss}"")",t % 200 == 0,not t % 200
eliot,https://github.com/itamarst/eliot/tree/master//versioneer.py,,run_command$384,"def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,
                env=None):
    """"""Call the given command(s).""""""
    assert isinstance(commands, list)
    p = None
    for c in commands:
        try:
            dispcmd = str([c] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            p = subprocess.Popen([c] + args, cwd=cwd, env=env,
                                 stdout=subprocess.PIPE,
                                 stderr=(subprocess.PIPE if hide_stderr
                                         else None))
            break
        except EnvironmentError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print(""unable to run %s"" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print(""unable to find command, tried %s"" % (commands,))
        return None, None
    stdout = p.communicate()[0].strip()
    if sys.version_info[0] >= 3:
        stdout = stdout.decode()
    if p.returncode != 0:
        if verbose:
            print(""unable to run %s (error)"" % dispcmd)
            print(""stdout was %s"" % stdout)
        return None, p.returncode
    return stdout, p.returncode",p.returncode != 0,p.returncode
napari,https://github.com/napari/napari/tree/master/napari/layers/shapes/_tests/test_shapes.py,,test_adding_shapes_to_empty$1139,"def test_adding_shapes_to_empty():
    """"""Test adding shapes to empty.""""""
    data = np.empty((0, 0, 2))
    np.random.seed(0)
    layer = Shapes(np.empty((0, 0, 2)))
    assert len(layer.data) == 0

    data = [
        20 * np.random.random((np.random.randint(2, 12), 2)) for i in range(5)
    ] + list(np.random.random((5, 4, 2)))
    shape_type = ['path'] * 5 + ['rectangle'] * 3 + ['ellipse'] * 2

    layer.add(data, shape_type=shape_type)
    assert layer.nshapes == len(data)
    assert np.all([np.all(ld == d) for ld, d in zip(layer.data, data)])
    assert layer.ndim == 2
    assert np.all([s == so for s, so in zip(layer.shape_type, shape_type)])",len(layer.data) == 0,not layer.data
bindsnet,https://github.com/BindsNET/bindsnet/tree/master/bindsnet/environment/dot_simulator.py,DotSimulator,movePoint$283,"def movePoint(self, d: Dot, dotDir: int = -1):
        """"""
        Apply clockwise directional enumeration.

        :param dotDir: enumerated movement as described above.
        :param/return r: current row    => next row
        :param/return c: current column => next column
        """"""

        # If not provided, use the known current direction.
        targetDir = False  # flag if we're using the target's direction.
        if dotDir < 0:
            dotDir = self.dotDir
            targetDir = True

        r, c = d.row[0], d.col[0]

        """""" Apply clockwise directional enumeration. """"""
        # 0 means stay, though we also won't go past the edge.
        if dotDir == 1:  # up
            r += self.speed
        elif dotDir == 2:  # right
            c += self.speed
        elif dotDir == 3:  # down
            r -= self.speed
        elif dotDir == 4:  # left
            c -= self.speed
        elif dotDir == 5:  # up and right
            r += self.speed
            c += self.speed
        elif dotDir == 6:  # down and right
            r -= self.speed
            c += self.speed
        elif dotDir == 7:  # down and left
            r -= self.speed
            c -= self.speed
        elif dotDir == 8:  # up and left
            r += self.speed
            c -= self.speed
        elif dotDir != 0:  # Woops
            assert False, ""Unsupported dot direction""

        """""" When a dot attempts to move past an edge... """"""
        # Stay put.
        if self.b_handling == 0:
            r = max(min(r, self.h - 1), 0)
            c = max(min(c, self.w - 1), 0)
            # direction stays the same.

        # Bounce: reflect its coordinates back into the region.
        elif self.b_handling == 1:
            if r < 0 or self.h <= r:
                r = self.h - 1 - r % self.h  # reflect row
                if targetDir:
                    self.dotDir += ROW_CROSSING[dotDir]

            if c < 0 or self.w <= c:
                c = self.w - 1 - c % self.w  # reflect column
                if targetDir:
                    self.dotDir += COL_CROSSING[dotDir]

        # Translate: the dot will continue in the same direction
        #            from the opposite side of the region.
        elif self.b_handling == 2:
            r = r % self.h  # Mirror row
            c = c % self.w  # Mirror column
            # direction stays the same.

        # Woops
        else:
            assert False, ""Unsupported bounds handling""

        # Update the saved point in the Dot class.
        # This also cycles the path history.
        d.move(r, c)

        # Update the grid with this point and its decaying trail.
        for t in range(self.decay):
            self.obs[d.row[t], d.col[t]] = min(
                self.obs[d.row[t], d.col[t]] + 1 - t / self.decay, 1
            )",self.b_handling == 0,not self.b_handling
bindsnet,https://github.com/BindsNET/bindsnet/tree/master/bindsnet/environment/dot_simulator.py,DotSimulator,movePoint$283,"def movePoint(self, d: Dot, dotDir: int = -1):
        """"""
        Apply clockwise directional enumeration.

        :param dotDir: enumerated movement as described above.
        :param/return r: current row    => next row
        :param/return c: current column => next column
        """"""

        # If not provided, use the known current direction.
        targetDir = False  # flag if we're using the target's direction.
        if dotDir < 0:
            dotDir = self.dotDir
            targetDir = True

        r, c = d.row[0], d.col[0]

        """""" Apply clockwise directional enumeration. """"""
        # 0 means stay, though we also won't go past the edge.
        if dotDir == 1:  # up
            r += self.speed
        elif dotDir == 2:  # right
            c += self.speed
        elif dotDir == 3:  # down
            r -= self.speed
        elif dotDir == 4:  # left
            c -= self.speed
        elif dotDir == 5:  # up and right
            r += self.speed
            c += self.speed
        elif dotDir == 6:  # down and right
            r -= self.speed
            c += self.speed
        elif dotDir == 7:  # down and left
            r -= self.speed
            c -= self.speed
        elif dotDir == 8:  # up and left
            r += self.speed
            c -= self.speed
        elif dotDir != 0:  # Woops
            assert False, ""Unsupported dot direction""

        """""" When a dot attempts to move past an edge... """"""
        # Stay put.
        if self.b_handling == 0:
            r = max(min(r, self.h - 1), 0)
            c = max(min(c, self.w - 1), 0)
            # direction stays the same.

        # Bounce: reflect its coordinates back into the region.
        elif self.b_handling == 1:
            if r < 0 or self.h <= r:
                r = self.h - 1 - r % self.h  # reflect row
                if targetDir:
                    self.dotDir += ROW_CROSSING[dotDir]

            if c < 0 or self.w <= c:
                c = self.w - 1 - c % self.w  # reflect column
                if targetDir:
                    self.dotDir += COL_CROSSING[dotDir]

        # Translate: the dot will continue in the same direction
        #            from the opposite side of the region.
        elif self.b_handling == 2:
            r = r % self.h  # Mirror row
            c = c % self.w  # Mirror column
            # direction stays the same.

        # Woops
        else:
            assert False, ""Unsupported bounds handling""

        # Update the saved point in the Dot class.
        # This also cycles the path history.
        d.move(r, c)

        # Update the grid with this point and its decaying trail.
        for t in range(self.decay):
            self.obs[d.row[t], d.col[t]] = min(
                self.obs[d.row[t], d.col[t]] + 1 - t / self.decay, 1
            )",dotDir != 0,dotDir
wand,https://github.com/emcconville/wand/tree/master/wand/image.py,BaseImage,extent$4927,"def extent(self, width=None, height=None, x=None, y=None, gravity=None):
        """"""Adjust the canvas size of the image. Use ``x`` & ``y`` to offset
        the image's relative placement in the canvas, or ``gravity`` helper
        for quick position placement.

        :param width: the target width of the extended image.
                      Default is the :attr:`width` of the image.
        :type width: :class:`numbers.Integral`
        :param height: the target height of the extended image.
                       Default is the :attr:`height` of the image.
        :type height: :class:`numbers.Integral`
        :param x: the x-axis offset of the extended image.
                      Default is 0, and can not be used with ``gravity``.
        :type x: :class:`numbers.Integral`
        :param y: the :attr:`y` offset of the extended image.
                       Default is 0, and can not be used with ``gravity``.
        :type y: :class:`numbers.Integral`
        :param gravity: position of the item extent when not using ``x`` &
                        ``y``. See :const:`GRAVITY_TYPES`.
        :type gravity: :class:`basestring`

        .. versionadded:: 0.4.5

        .. versionchanged:: 0.6.8
           Added ``gravity`` argument.
        """"""
        if width is None or width == 0:
            width = self.width
        if height is None or height == 0:
            height = self.height
        assertions.assert_unsigned_integer(width=width, height=height)
        if gravity is None:
            if x is None:
                x = 0
            if y is None:
                y = 0
        else:
            if x is not None or y is not None:
                raise ValueError('x & y can not be used with gravity.')
            y, x = self._gravity_to_offset(gravity, width, height)
        assertions.assert_integer(x=x, y=y)
        return library.MagickExtentImage(self.wand, width, height, x, y)",width == 0,not width
wand,https://github.com/emcconville/wand/tree/master/wand/image.py,BaseImage,extent$4927,"def extent(self, width=None, height=None, x=None, y=None, gravity=None):
        """"""Adjust the canvas size of the image. Use ``x`` & ``y`` to offset
        the image's relative placement in the canvas, or ``gravity`` helper
        for quick position placement.

        :param width: the target width of the extended image.
                      Default is the :attr:`width` of the image.
        :type width: :class:`numbers.Integral`
        :param height: the target height of the extended image.
                       Default is the :attr:`height` of the image.
        :type height: :class:`numbers.Integral`
        :param x: the x-axis offset of the extended image.
                      Default is 0, and can not be used with ``gravity``.
        :type x: :class:`numbers.Integral`
        :param y: the :attr:`y` offset of the extended image.
                       Default is 0, and can not be used with ``gravity``.
        :type y: :class:`numbers.Integral`
        :param gravity: position of the item extent when not using ``x`` &
                        ``y``. See :const:`GRAVITY_TYPES`.
        :type gravity: :class:`basestring`

        .. versionadded:: 0.4.5

        .. versionchanged:: 0.6.8
           Added ``gravity`` argument.
        """"""
        if width is None or width == 0:
            width = self.width
        if height is None or height == 0:
            height = self.height
        assertions.assert_unsigned_integer(width=width, height=height)
        if gravity is None:
            if x is None:
                x = 0
            if y is None:
                y = 0
        else:
            if x is not None or y is not None:
                raise ValueError('x & y can not be used with gravity.')
            y, x = self._gravity_to_offset(gravity, width, height)
        assertions.assert_integer(x=x, y=y)
        return library.MagickExtentImage(self.wand, width, height, x, y)",height == 0,not height
GotoX,https://github.com/SeaHOH/GotoX/tree/master/local/common/internet_active.py,InternetActiveCheck,set_dns_servers_v6$176,"def set_dns_servers_v6(self):
        if '6' not in GC.LINK_PROFILE:
            return
        addr6 = get_wan_ipv6()
        if addr6:
            if addr6.teredo:
                if self.type != 'IPv6 Teredo':
                    if self.type != 'IPv6':
                        logging.warning('妫娴嬪埌 IPv6 缃戠粶鍙樺姩锛屽綋鍓嶄娇鐢 Teredo 闅ч亾锛孖P锛%s', addr6)
                    self.type = 'IPv6 Teredo'
                if not (self._dns_servers or self.only_check_ip):
                    self.set_dns_servers(dns_ips_v6w)
            elif addr6.sixtofour:
                if self.type != 'IPv6 6to4':
                    if self.type != 'IPv6':
                        logging.warning('妫娴嬪埌 IPv6 缃戠粶鍙樺姩锛屽綋鍓嶄娇鐢 6to4 闅ч亾锛孖P锛%s', addr6)
                    self.type = 'IPv6 6to4'
                if not (self._dns_servers or self.only_check_ip):
                    self.set_dns_servers(dns_ips_v6w)
            else:
                if self.type != 'IPv6 Global':
                    if self.type != 'IPv6':
                        logging.warning('妫娴嬪埌 IPv6 缃戠粶鍙樺姩锛屽綋鍓嶄娇鐢ㄥ師鐢熺綉缁滐紝IP锛%s', addr6)
                    self.type = 'IPv6 Global'
                if not (self._dns_servers or self.only_check_ip):
                    self.set_dns_servers(dns_ips_v6)
            if self.only_check_ip and self.last_stat != 1:
                if self.last_stat is not None:
                    logging.warning('IPv6 缃戠粶鎭㈠嶈繛鎺')
                self.last_stat = 1
        else:
            if self.only_check_ip and self.last_stat != 0:
                logging.error('IPv6 缃戠粶鐜板湪涓嶅彲鐢锛屽皢姣 10 绉掓娴嬩竴娆♀︹')
            self.last_stat = 0
            self._dns_servers = None
            return",self.last_stat != 0,self.last_stat
Wave-U-Net,https://github.com/f90/Wave-U-Net/tree/master/Models/InterpolationLayer.py,,learned_interpolation_layer$4,"def learned_interpolation_layer(input, padding, level):
    '''
    Implements a trainable upsampling layer by interpolation by a factor of two, from N samples to N*2 - 1.
    Interpolation of intermediate feature vectors v_1 and v_2 (of dimensionality F) is performed by
     w \cdot v_1 + (1-w) \cdot v_2, where \cdot is point-wise multiplication, and w an F-dimensional weight vector constrained to [0,1]
    :param input: Input features of shape [batch_size, 1, width, F]
    :param padding:
    :param level:
    :return:
    '''
    assert(padding == ""valid"" or padding == ""same"")
    features = input.get_shape().as_list()[3]

    # Construct 2FxF weight matrix, where F is the number of feature channels in the feature map.
    # Matrix is constrained, made up out of two diagonal FxF matrices with diagonal weights w and 1-w. w is constrained to be in [0,1] # mioid
    weights = tf.get_variable(""interp_"" + str(level), shape=[features], dtype=tf.float32)
    weights_scaled = tf.nn.sigmoid(weights) # Constrain weights to [0,1]
    counter_weights = 1.0 - weights_scaled # Mirrored weights for the features from the other time step
    conv_weights = tf.expand_dims(tf.concat([tf.expand_dims(tf.diag(weights_scaled), axis=0), tf.expand_dims(tf.diag(counter_weights), axis=0)], axis=0), axis=0)
    intermediate_vals = tf.nn.conv2d(input, conv_weights, strides=[1,1,1,1], padding=padding.upper())

    intermediate_vals = tf.transpose(intermediate_vals, [2, 0, 1, 3])
    out = tf.transpose(input, [2, 0, 1, 3])
    num_entries = out.get_shape().as_list()[0]
    out = tf.concat([out, intermediate_vals], axis=0)
    indices = list()

    # Interleave interpolated features with original ones, starting with the first original one
    num_outputs = (2*num_entries - 1) if padding == ""valid"" else 2*num_entries
    for idx in range(num_outputs):
        if idx % 2 == 0:
            indices.append(idx // 2)
        else:
            indices.append(num_entries + idx//2)
    out = tf.gather(out, indices)
    current_layer = tf.transpose(out, [1, 2, 0, 3])
    return current_layer",idx % 2 == 0,not idx % 2
typer,https://github.com/tiangolo/typer/tree/master/tests/test_tutorial/test_first_steps/test_tutorial005.py,,test_option_lastname$33,"def test_option_lastname():
    result = runner.invoke(app, [""Camila"", ""--lastname"", ""Guti茅rrez""])
    assert result.exit_code == 0
    assert ""Hello Camila Guti茅rrez"" in result.output",result.exit_code == 0,not result.exit_code
socorro,https://github.com/mozilla-services/socorro/tree/master/webapp-django/crashstats/authentication/tests/test_auditgroups.py,TestAuditGroupsCommand,test_user_with_invalid_email_removed$57,"def test_user_with_invalid_email_removed(self, db):
        hackers_group = Group.objects.get(name=""Hackers"")

        bob = User.objects.create(username=""bob"", email=""bob@example.com"")
        bob.last_login = timezone.now()
        bob.groups.add(hackers_group)
        bob.save()

        buffer = StringIO()
        call_command(""auditgroups"", dry_run=False, stdout=buffer)
        assert [u.email for u in hackers_group.user_set.all()] == []
        assert (
            ""Removing: bob@example.com (not employee or exception)"" in buffer.getvalue()
        )",[u.email for u in hackers_group.user_set.all()] == [],not [u.email for u in hackers_group.user_set.all()]
nix-gui,https://github.com/nix-gui/nix-gui/tree/master/nixui/options/option_definition.py,OptionDefinition,get_object_type$103,"def get_object_type(cls, obj):
        if obj == Unresolvable:
            raise ValueError
        elif isinstance(obj, list):
            subtypes = set([cls.get_object_type(elem) for elem in obj])
            if len(subtypes) == 0:
                return types.ListOfType()
            if len(subtypes) == 1:
                return types.ListOfType(subtypes.pop())
            else:
                return types.ListOfType(types.EitherType(subtypes))
        elif isinstance(obj, dict):
            subtypes = set([cls.get_object_type(v) for v in obj.values()])
            if len(subtypes) == 0:
                return types.AttrsOfType()
            if len(subtypes) == 1:
                return types.AttrsOfType(subtypes.pop())
            else:
                return types.AttrsOfType(types.EitherType(subtypes))
        elif isinstance(obj, bool):
            return types.BoolType()
        elif isinstance(obj, int):
            return types.IntType()
        elif isinstance(obj, float):
            return types.FloatType()
        elif isinstance(obj, str):
            return types.StrType()
        elif isinstance(obj, Path):
            return types.PathType()
        elif obj is None:
            return types.NullType()
        elif obj == Undefined:
            return types.AnythingType()
        else:
            raise NotImplementedError(obj)",len(subtypes) == 0,not subtypes
nix-gui,https://github.com/nix-gui/nix-gui/tree/master/nixui/options/option_definition.py,OptionDefinition,get_object_type$103,"def get_object_type(cls, obj):
        if obj == Unresolvable:
            raise ValueError
        elif isinstance(obj, list):
            subtypes = set([cls.get_object_type(elem) for elem in obj])
            if len(subtypes) == 0:
                return types.ListOfType()
            if len(subtypes) == 1:
                return types.ListOfType(subtypes.pop())
            else:
                return types.ListOfType(types.EitherType(subtypes))
        elif isinstance(obj, dict):
            subtypes = set([cls.get_object_type(v) for v in obj.values()])
            if len(subtypes) == 0:
                return types.AttrsOfType()
            if len(subtypes) == 1:
                return types.AttrsOfType(subtypes.pop())
            else:
                return types.AttrsOfType(types.EitherType(subtypes))
        elif isinstance(obj, bool):
            return types.BoolType()
        elif isinstance(obj, int):
            return types.IntType()
        elif isinstance(obj, float):
            return types.FloatType()
        elif isinstance(obj, str):
            return types.StrType()
        elif isinstance(obj, Path):
            return types.PathType()
        elif obj is None:
            return types.NullType()
        elif obj == Undefined:
            return types.AnythingType()
        else:
            raise NotImplementedError(obj)",len(subtypes) == 0,not subtypes
CenterNet-better,https://github.com/FateScript/CenterNet-better/tree/master/dl_lib/evaluation/coco_evaluation.py,,_evaluate_box_proposals$354,"def _evaluate_box_proposals(dataset_predictions, coco_api, thresholds=None, area=""all"", limit=None):
    """"""
    Evaluate detection proposal recall metrics. This function is a much
    faster alternative to the official COCO API recall evaluation code. However,
    it produces slightly different results.
    """"""
    # Record max overlap value for each gt box
    # Return vector of overlap values
    areas = {
        ""all"": 0,
        ""small"": 1,
        ""medium"": 2,
        ""large"": 3,
        ""96-128"": 4,
        ""128-256"": 5,
        ""256-512"": 6,
        ""512-inf"": 7,
    }
    area_ranges = [
        [0 ** 2, 1e5 ** 2],  # all
        [0 ** 2, 32 ** 2],  # small
        [32 ** 2, 96 ** 2],  # medium
        [96 ** 2, 1e5 ** 2],  # large
        [96 ** 2, 128 ** 2],  # 96-128
        [128 ** 2, 256 ** 2],  # 128-256
        [256 ** 2, 512 ** 2],  # 256-512
        [512 ** 2, 1e5 ** 2],
    ]  # 512-inf
    assert area in areas, ""Unknown area range: {}"".format(area)
    area_range = area_ranges[areas[area]]
    gt_overlaps = []
    num_pos = 0

    for prediction_dict in dataset_predictions:
        predictions = prediction_dict[""proposals""]

        # sort predictions in descending order
        # TODO maybe remove this and make it explicit in the documentation
        inds = predictions.objectness_logits.sort(descending=True)[1]
        predictions = predictions[inds]

        ann_ids = coco_api.getAnnIds(imgIds=prediction_dict[""image_id""])
        anno = coco_api.loadAnns(ann_ids)
        gt_boxes = [
            BoxMode.convert(obj[""bbox""], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)
            for obj in anno
            if obj[""iscrowd""] == 0
        ]
        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes
        gt_boxes = Boxes(gt_boxes)
        gt_areas = torch.as_tensor([obj[""area""] for obj in anno if obj[""iscrowd""] == 0])

        if len(gt_boxes) == 0 or len(predictions) == 0:
            continue

        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])
        gt_boxes = gt_boxes[valid_gt_inds]

        num_pos += len(gt_boxes)

        if len(gt_boxes) == 0:
            continue

        if limit is not None and len(predictions) > limit:
            predictions = predictions[:limit]

        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)

        _gt_overlaps = torch.zeros(len(gt_boxes))
        for j in range(min(len(predictions), len(gt_boxes))):
            # find which proposal box maximally covers each gt box
            # and get the iou amount of coverage for each gt box
            max_overlaps, argmax_overlaps = overlaps.max(dim=0)

            # find which gt box is 'best' covered (i.e. 'best' = most iou)
            gt_ovr, gt_ind = max_overlaps.max(dim=0)
            assert gt_ovr >= 0
            # find the proposal box that covers the best covered gt box
            box_ind = argmax_overlaps[gt_ind]
            # record the iou coverage of this gt box
            _gt_overlaps[j] = overlaps[box_ind, gt_ind]
            assert _gt_overlaps[j] == gt_ovr
            # mark the proposal box and the gt box as used
            overlaps[box_ind, :] = -1
            overlaps[:, gt_ind] = -1

        # append recorded iou coverage level
        gt_overlaps.append(_gt_overlaps)
    gt_overlaps = torch.cat(gt_overlaps, dim=0)
    gt_overlaps, _ = torch.sort(gt_overlaps)

    if thresholds is None:
        step = 0.05
        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)
    recalls = torch.zeros_like(thresholds)
    # compute recall for each iou threshold
    for i, t in enumerate(thresholds):
        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)
    # ar = 2 * np.trapz(recalls, thresholds)
    ar = recalls.mean()
    return {
        ""ar"": ar,
        ""recalls"": recalls,
        ""thresholds"": thresholds,
        ""gt_overlaps"": gt_overlaps,
        ""num_pos"": num_pos,
    }",len(gt_boxes) == 0,not gt_boxes
CenterNet-better,https://github.com/FateScript/CenterNet-better/tree/master/dl_lib/evaluation/coco_evaluation.py,,_evaluate_box_proposals$354,"def _evaluate_box_proposals(dataset_predictions, coco_api, thresholds=None, area=""all"", limit=None):
    """"""
    Evaluate detection proposal recall metrics. This function is a much
    faster alternative to the official COCO API recall evaluation code. However,
    it produces slightly different results.
    """"""
    # Record max overlap value for each gt box
    # Return vector of overlap values
    areas = {
        ""all"": 0,
        ""small"": 1,
        ""medium"": 2,
        ""large"": 3,
        ""96-128"": 4,
        ""128-256"": 5,
        ""256-512"": 6,
        ""512-inf"": 7,
    }
    area_ranges = [
        [0 ** 2, 1e5 ** 2],  # all
        [0 ** 2, 32 ** 2],  # small
        [32 ** 2, 96 ** 2],  # medium
        [96 ** 2, 1e5 ** 2],  # large
        [96 ** 2, 128 ** 2],  # 96-128
        [128 ** 2, 256 ** 2],  # 128-256
        [256 ** 2, 512 ** 2],  # 256-512
        [512 ** 2, 1e5 ** 2],
    ]  # 512-inf
    assert area in areas, ""Unknown area range: {}"".format(area)
    area_range = area_ranges[areas[area]]
    gt_overlaps = []
    num_pos = 0

    for prediction_dict in dataset_predictions:
        predictions = prediction_dict[""proposals""]

        # sort predictions in descending order
        # TODO maybe remove this and make it explicit in the documentation
        inds = predictions.objectness_logits.sort(descending=True)[1]
        predictions = predictions[inds]

        ann_ids = coco_api.getAnnIds(imgIds=prediction_dict[""image_id""])
        anno = coco_api.loadAnns(ann_ids)
        gt_boxes = [
            BoxMode.convert(obj[""bbox""], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)
            for obj in anno
            if obj[""iscrowd""] == 0
        ]
        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes
        gt_boxes = Boxes(gt_boxes)
        gt_areas = torch.as_tensor([obj[""area""] for obj in anno if obj[""iscrowd""] == 0])

        if len(gt_boxes) == 0 or len(predictions) == 0:
            continue

        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])
        gt_boxes = gt_boxes[valid_gt_inds]

        num_pos += len(gt_boxes)

        if len(gt_boxes) == 0:
            continue

        if limit is not None and len(predictions) > limit:
            predictions = predictions[:limit]

        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)

        _gt_overlaps = torch.zeros(len(gt_boxes))
        for j in range(min(len(predictions), len(gt_boxes))):
            # find which proposal box maximally covers each gt box
            # and get the iou amount of coverage for each gt box
            max_overlaps, argmax_overlaps = overlaps.max(dim=0)

            # find which gt box is 'best' covered (i.e. 'best' = most iou)
            gt_ovr, gt_ind = max_overlaps.max(dim=0)
            assert gt_ovr >= 0
            # find the proposal box that covers the best covered gt box
            box_ind = argmax_overlaps[gt_ind]
            # record the iou coverage of this gt box
            _gt_overlaps[j] = overlaps[box_ind, gt_ind]
            assert _gt_overlaps[j] == gt_ovr
            # mark the proposal box and the gt box as used
            overlaps[box_ind, :] = -1
            overlaps[:, gt_ind] = -1

        # append recorded iou coverage level
        gt_overlaps.append(_gt_overlaps)
    gt_overlaps = torch.cat(gt_overlaps, dim=0)
    gt_overlaps, _ = torch.sort(gt_overlaps)

    if thresholds is None:
        step = 0.05
        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)
    recalls = torch.zeros_like(thresholds)
    # compute recall for each iou threshold
    for i, t in enumerate(thresholds):
        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)
    # ar = 2 * np.trapz(recalls, thresholds)
    ar = recalls.mean()
    return {
        ""ar"": ar,
        ""recalls"": recalls,
        ""thresholds"": thresholds,
        ""gt_overlaps"": gt_overlaps,
        ""num_pos"": num_pos,
    }",len(gt_boxes) == 0,not gt_boxes
CenterNet-better,https://github.com/FateScript/CenterNet-better/tree/master/dl_lib/evaluation/coco_evaluation.py,,_evaluate_box_proposals$354,"def _evaluate_box_proposals(dataset_predictions, coco_api, thresholds=None, area=""all"", limit=None):
    """"""
    Evaluate detection proposal recall metrics. This function is a much
    faster alternative to the official COCO API recall evaluation code. However,
    it produces slightly different results.
    """"""
    # Record max overlap value for each gt box
    # Return vector of overlap values
    areas = {
        ""all"": 0,
        ""small"": 1,
        ""medium"": 2,
        ""large"": 3,
        ""96-128"": 4,
        ""128-256"": 5,
        ""256-512"": 6,
        ""512-inf"": 7,
    }
    area_ranges = [
        [0 ** 2, 1e5 ** 2],  # all
        [0 ** 2, 32 ** 2],  # small
        [32 ** 2, 96 ** 2],  # medium
        [96 ** 2, 1e5 ** 2],  # large
        [96 ** 2, 128 ** 2],  # 96-128
        [128 ** 2, 256 ** 2],  # 128-256
        [256 ** 2, 512 ** 2],  # 256-512
        [512 ** 2, 1e5 ** 2],
    ]  # 512-inf
    assert area in areas, ""Unknown area range: {}"".format(area)
    area_range = area_ranges[areas[area]]
    gt_overlaps = []
    num_pos = 0

    for prediction_dict in dataset_predictions:
        predictions = prediction_dict[""proposals""]

        # sort predictions in descending order
        # TODO maybe remove this and make it explicit in the documentation
        inds = predictions.objectness_logits.sort(descending=True)[1]
        predictions = predictions[inds]

        ann_ids = coco_api.getAnnIds(imgIds=prediction_dict[""image_id""])
        anno = coco_api.loadAnns(ann_ids)
        gt_boxes = [
            BoxMode.convert(obj[""bbox""], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)
            for obj in anno
            if obj[""iscrowd""] == 0
        ]
        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes
        gt_boxes = Boxes(gt_boxes)
        gt_areas = torch.as_tensor([obj[""area""] for obj in anno if obj[""iscrowd""] == 0])

        if len(gt_boxes) == 0 or len(predictions) == 0:
            continue

        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])
        gt_boxes = gt_boxes[valid_gt_inds]

        num_pos += len(gt_boxes)

        if len(gt_boxes) == 0:
            continue

        if limit is not None and len(predictions) > limit:
            predictions = predictions[:limit]

        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)

        _gt_overlaps = torch.zeros(len(gt_boxes))
        for j in range(min(len(predictions), len(gt_boxes))):
            # find which proposal box maximally covers each gt box
            # and get the iou amount of coverage for each gt box
            max_overlaps, argmax_overlaps = overlaps.max(dim=0)

            # find which gt box is 'best' covered (i.e. 'best' = most iou)
            gt_ovr, gt_ind = max_overlaps.max(dim=0)
            assert gt_ovr >= 0
            # find the proposal box that covers the best covered gt box
            box_ind = argmax_overlaps[gt_ind]
            # record the iou coverage of this gt box
            _gt_overlaps[j] = overlaps[box_ind, gt_ind]
            assert _gt_overlaps[j] == gt_ovr
            # mark the proposal box and the gt box as used
            overlaps[box_ind, :] = -1
            overlaps[:, gt_ind] = -1

        # append recorded iou coverage level
        gt_overlaps.append(_gt_overlaps)
    gt_overlaps = torch.cat(gt_overlaps, dim=0)
    gt_overlaps, _ = torch.sort(gt_overlaps)

    if thresholds is None:
        step = 0.05
        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)
    recalls = torch.zeros_like(thresholds)
    # compute recall for each iou threshold
    for i, t in enumerate(thresholds):
        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)
    # ar = 2 * np.trapz(recalls, thresholds)
    ar = recalls.mean()
    return {
        ""ar"": ar,
        ""recalls"": recalls,
        ""thresholds"": thresholds,
        ""gt_overlaps"": gt_overlaps,
        ""num_pos"": num_pos,
    }",len(predictions) == 0,not predictions
DBFace,https://github.com/dlunion/DBFace/tree/master/train/small/common.py,,pad$477,"def pad(image, stride=32):

    hasChange = False
    stdw = image.shape[1]
    if stdw % stride != 0:
        stdw += stride - (stdw % stride)
        hasChange = True 

    stdh = image.shape[0]
    if stdh % stride != 0:
        stdh += stride - (stdh % stride)
        hasChange = True

    if hasChange:
        newImage = np.zeros((stdh, stdw, 3), np.uint8)
        newImage[:image.shape[0], :image.shape[1], :] = image
        return newImage
    else:
        return image",stdw % stride != 0,stdw % stride
DBFace,https://github.com/dlunion/DBFace/tree/master/train/small/common.py,,pad$477,"def pad(image, stride=32):

    hasChange = False
    stdw = image.shape[1]
    if stdw % stride != 0:
        stdw += stride - (stdw % stride)
        hasChange = True 

    stdh = image.shape[0]
    if stdh % stride != 0:
        stdh += stride - (stdh % stride)
        hasChange = True

    if hasChange:
        newImage = np.zeros((stdh, stdw, 3), np.uint8)
        newImage[:image.shape[0], :image.shape[1], :] = image
        return newImage
    else:
        return image",stdh % stride != 0,stdh % stride
OpenFermion,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/circuits/primitives/ffft_test.py,,append_in_normal_order$67,"def append_in_normal_order(index, mode):
        phase = 1
        mode = n - 1 - mode
        for i in range(n):
            bit = 1 << i
            if i == mode:
                if index & bit != 0:
                    return None, None
                return index | bit, phase
            elif index & bit:
                phase *= -1",index & bit != 0,index & bit
nibabel,https://github.com/nipy/nibabel/tree/master/nibabel/tests/test_fileslice.py,,test_slice2len$170,"def test_slice2len():
    # Test slice length calculation
    assert slice2len(slice(None), 10) == 10
    assert slice2len(slice(11), 10) == 10
    assert slice2len(slice(1, 11), 10) == 9
    assert slice2len(slice(1, 1), 10) == 0
    assert slice2len(slice(1, 11, 2), 10) == 5
    assert slice2len(slice(0, 11, 3), 10) == 4
    assert slice2len(slice(1, 11, 3), 10) == 3
    assert slice2len(slice(None, None, -1), 10) == 10
    assert slice2len(slice(11, None, -1), 10) == 10
    assert slice2len(slice(None, 1, -1), 10) == 8
    assert slice2len(slice(None, None, -2), 10) == 5
    assert slice2len(slice(None, None, -3), 10) == 4
    assert slice2len(slice(None, 0, -3), 10) == 3
    # Start, end are always taken to be relative if negative
    assert slice2len(slice(None, -4, -1), 10) == 3
    assert slice2len(slice(-4, -2, 1), 10) == 2
    # start after stop
    assert slice2len(slice(3, 2, 1), 10) == 0
    assert slice2len(slice(2, 3, -1), 10) == 0","slice2len(slice(1, 1), 10) == 0","not slice2len(slice(1, 1), 10)"
nibabel,https://github.com/nipy/nibabel/tree/master/nibabel/tests/test_fileslice.py,,test_slice2len$170,"def test_slice2len():
    # Test slice length calculation
    assert slice2len(slice(None), 10) == 10
    assert slice2len(slice(11), 10) == 10
    assert slice2len(slice(1, 11), 10) == 9
    assert slice2len(slice(1, 1), 10) == 0
    assert slice2len(slice(1, 11, 2), 10) == 5
    assert slice2len(slice(0, 11, 3), 10) == 4
    assert slice2len(slice(1, 11, 3), 10) == 3
    assert slice2len(slice(None, None, -1), 10) == 10
    assert slice2len(slice(11, None, -1), 10) == 10
    assert slice2len(slice(None, 1, -1), 10) == 8
    assert slice2len(slice(None, None, -2), 10) == 5
    assert slice2len(slice(None, None, -3), 10) == 4
    assert slice2len(slice(None, 0, -3), 10) == 3
    # Start, end are always taken to be relative if negative
    assert slice2len(slice(None, -4, -1), 10) == 3
    assert slice2len(slice(-4, -2, 1), 10) == 2
    # start after stop
    assert slice2len(slice(3, 2, 1), 10) == 0
    assert slice2len(slice(2, 3, -1), 10) == 0","slice2len(slice(3, 2, 1), 10) == 0","not slice2len(slice(3, 2, 1), 10)"
nibabel,https://github.com/nipy/nibabel/tree/master/nibabel/tests/test_fileslice.py,,test_slice2len$170,"def test_slice2len():
    # Test slice length calculation
    assert slice2len(slice(None), 10) == 10
    assert slice2len(slice(11), 10) == 10
    assert slice2len(slice(1, 11), 10) == 9
    assert slice2len(slice(1, 1), 10) == 0
    assert slice2len(slice(1, 11, 2), 10) == 5
    assert slice2len(slice(0, 11, 3), 10) == 4
    assert slice2len(slice(1, 11, 3), 10) == 3
    assert slice2len(slice(None, None, -1), 10) == 10
    assert slice2len(slice(11, None, -1), 10) == 10
    assert slice2len(slice(None, 1, -1), 10) == 8
    assert slice2len(slice(None, None, -2), 10) == 5
    assert slice2len(slice(None, None, -3), 10) == 4
    assert slice2len(slice(None, 0, -3), 10) == 3
    # Start, end are always taken to be relative if negative
    assert slice2len(slice(None, -4, -1), 10) == 3
    assert slice2len(slice(-4, -2, 1), 10) == 2
    # start after stop
    assert slice2len(slice(3, 2, 1), 10) == 0
    assert slice2len(slice(2, 3, -1), 10) == 0","slice2len(slice(2, 3, -1), 10) == 0","not slice2len(slice(2, 3, -1), 10)"
prototypical-networks,https://github.com/jakesnell/prototypical-networks/tree/master/scripts/predict/few_shot/eval.py,,main$13,"def main(opt):
    # load model
    model = torch.load(opt['model.model_path'])
    model.eval()

    # load opts
    model_opt_file = os.path.join(os.path.dirname(opt['model.model_path']), 'opt.json')
    with open(model_opt_file, 'r') as f:
        model_opt = json.load(f)

    # Postprocess arguments
    model_opt['model.x_dim'] = map(int, model_opt['model.x_dim'].split(','))
    model_opt['log.fields'] = model_opt['log.fields'].split(',')

    # construct data
    data_opt = { 'data.' + k: v for k,v in filter_opt(model_opt, 'data').items() }

    episode_fields = {
        'data.test_way': 'data.way',
        'data.test_shot': 'data.shot',
        'data.test_query': 'data.query',
        'data.test_episodes': 'data.train_episodes'
    }

    for k,v in episode_fields.items():
        if opt[k] != 0:
            data_opt[k] = opt[k]
        elif model_opt[k] != 0:
            data_opt[k] = model_opt[k]
        else:
            data_opt[k] = model_opt[v]

    print(""Evaluating {:d}-way, {:d}-shot with {:d} query examples/class over {:d} episodes"".format(
        data_opt['data.test_way'], data_opt['data.test_shot'],
        data_opt['data.test_query'], data_opt['data.test_episodes']))

    torch.manual_seed(1234)
    if data_opt['data.cuda']:
        torch.cuda.manual_seed(1234)

    data = data_utils.load(data_opt, ['test'])

    if data_opt['data.cuda']:
        model.cuda()

    meters = { field: tnt.meter.AverageValueMeter() for field in model_opt['log.fields'] }

    model_utils.evaluate(model, data['test'], meters, desc=""test"")

    for field,meter in meters.items():
        mean, std = meter.value()
        print(""test {:s}: {:0.6f} +/- {:0.6f}"".format(field, mean, 1.96 * std / math.sqrt(data_opt['data.test_episodes'])))",opt[k] != 0,opt[k]
prototypical-networks,https://github.com/jakesnell/prototypical-networks/tree/master/scripts/predict/few_shot/eval.py,,main$13,"def main(opt):
    # load model
    model = torch.load(opt['model.model_path'])
    model.eval()

    # load opts
    model_opt_file = os.path.join(os.path.dirname(opt['model.model_path']), 'opt.json')
    with open(model_opt_file, 'r') as f:
        model_opt = json.load(f)

    # Postprocess arguments
    model_opt['model.x_dim'] = map(int, model_opt['model.x_dim'].split(','))
    model_opt['log.fields'] = model_opt['log.fields'].split(',')

    # construct data
    data_opt = { 'data.' + k: v for k,v in filter_opt(model_opt, 'data').items() }

    episode_fields = {
        'data.test_way': 'data.way',
        'data.test_shot': 'data.shot',
        'data.test_query': 'data.query',
        'data.test_episodes': 'data.train_episodes'
    }

    for k,v in episode_fields.items():
        if opt[k] != 0:
            data_opt[k] = opt[k]
        elif model_opt[k] != 0:
            data_opt[k] = model_opt[k]
        else:
            data_opt[k] = model_opt[v]

    print(""Evaluating {:d}-way, {:d}-shot with {:d} query examples/class over {:d} episodes"".format(
        data_opt['data.test_way'], data_opt['data.test_shot'],
        data_opt['data.test_query'], data_opt['data.test_episodes']))

    torch.manual_seed(1234)
    if data_opt['data.cuda']:
        torch.cuda.manual_seed(1234)

    data = data_utils.load(data_opt, ['test'])

    if data_opt['data.cuda']:
        model.cuda()

    meters = { field: tnt.meter.AverageValueMeter() for field in model_opt['log.fields'] }

    model_utils.evaluate(model, data['test'], meters, desc=""test"")

    for field,meter in meters.items():
        mean, std = meter.value()
        print(""test {:s}: {:0.6f} +/- {:0.6f}"".format(field, mean, 1.96 * std / math.sqrt(data_opt['data.test_episodes'])))",model_opt[k] != 0,model_opt[k]
ds4drv,https://github.com/chrippa/ds4drv/tree/master/ds4drv/backends/bluetooth.py,BluetoothDS4Device,read_report$47,"def read_report(self):
        try:
            ret = self.int_sock.recv_into(self.buf)
        except IOError:
            return

        # Disconnection
        if ret == 0:
            return

        # Invalid report size or id, just ignore it
        if ret < REPORT_SIZE or self.buf[1] != REPORT_ID:
            return False

        # Cut off bluetooth data
        buf = zero_copy_slice(self.buf, 3)

        return self.parse_report(buf)",ret == 0,not ret
cms,https://github.com/amfoss/cms/tree/master/cmscontrib/updaters/update_32.py,Updater,run$40,"def run(self):
        for k, v in self.objs.items():
            if k.startswith(""_""):
                continue

            if v[""_class""] == ""Dataset"":

                limit = v[""time_limit""]
                # Zero explicitly meant no limits.
                if limit == 0.0:
                    limit = None
                # Negative was undefined though.
                if limit is not None and limit <= 0.0:
                    logger.warning(""Previous time limit %s was updated, ""
                                   ""no time limit is enforced now."",
                                   limit)
                    limit = None
                v[""time_limit""] = limit

                limit = v[""memory_limit""]
                # Zero explicitly meant no limits.
                if limit == 0:
                    limit = None
                # Negative was undefined though.
                if limit is not None and limit <= 0:
                    logger.warning(""Previous memory limit %s was updated, ""
                                   ""no memory limit is enforced now."",
                                   limit)
                    limit = None
                v[""memory_limit""] = limit

        return self.objs",limit == 0.0,not limit
cms,https://github.com/amfoss/cms/tree/master/cmscontrib/updaters/update_32.py,Updater,run$40,"def run(self):
        for k, v in self.objs.items():
            if k.startswith(""_""):
                continue

            if v[""_class""] == ""Dataset"":

                limit = v[""time_limit""]
                # Zero explicitly meant no limits.
                if limit == 0.0:
                    limit = None
                # Negative was undefined though.
                if limit is not None and limit <= 0.0:
                    logger.warning(""Previous time limit %s was updated, ""
                                   ""no time limit is enforced now."",
                                   limit)
                    limit = None
                v[""time_limit""] = limit

                limit = v[""memory_limit""]
                # Zero explicitly meant no limits.
                if limit == 0:
                    limit = None
                # Negative was undefined though.
                if limit is not None and limit <= 0:
                    logger.warning(""Previous memory limit %s was updated, ""
                                   ""no memory limit is enforced now."",
                                   limit)
                    limit = None
                v[""memory_limit""] = limit

        return self.objs",limit == 0,not limit
atlassian-python-api,https://github.com/atlassian-api/atlassian-python-api/tree/master/atlassian/confluence.py,Confluence,has_unknown_attachment_error$1472,"def has_unknown_attachment_error(self, page_id):
        """"""
        Check has unknown attachment error on page
        :param page_id:
        :return:
        """"""
        unknown_attachment_identifier = ""plugins/servlet/confluence/placeholder/unknown-attachment""
        result = self.get_page_by_id(page_id, expand=""body.view"")
        if len(result) == 0:
            return """"
        body = ((result.get(""body"") or {}).get(""view"") or {}).get(""value"") or {}
        if unknown_attachment_identifier in body:
            return result.get(""_links"").get(""base"") + result.get(""_links"").get(""tinyui"")
        return """"",len(result) == 0,not result
xonsh,https://github.com/xonsh/xonsh/tree/master//amalgamate.py,,module_is_package$172,"def module_is_package(module, pkg, level):
    """"""Returns whether or not the module name refers to the package.""""""
    if level == 0:
        return module == pkg
    elif level == 1:
        return module is None
    else:
        return False",level == 0,not level
OWOD,https://github.com/JosephKJ/OWOD/tree/master/detectron2/data/datasets/lvis.py,,load_lvis_json$40,"def load_lvis_json(json_file, image_root, dataset_name=None):
    """"""
    Load a json file in LVIS's annotation format.

    Args:
        json_file (str): full path to the LVIS json annotation file.
        image_root (str): the directory where the images in this json file exists.
        dataset_name (str): the name of the dataset (e.g., ""lvis_v0.5_train"").
            If provided, this function will put ""thing_classes"" into the metadata
            associated with this dataset.

    Returns:
        list[dict]: a list of dicts in Detectron2 standard format. (See
        `Using Custom Datasets </tutorials/datasets.html>`_ )

    Notes:
        1. This function does not read the image files.
           The results do not have the ""image"" field.
    """"""
    from lvis import LVIS

    json_file = PathManager.get_local_path(json_file)

    timer = Timer()
    lvis_api = LVIS(json_file)
    if timer.seconds() > 1:
        logger.info(""Loading {} takes {:.2f} seconds."".format(json_file, timer.seconds()))

    if dataset_name is not None:
        meta = get_lvis_instances_meta(dataset_name)
        MetadataCatalog.get(dataset_name).set(**meta)

    # sort indices for reproducible results
    img_ids = sorted(lvis_api.imgs.keys())
    # imgs is a list of dicts, each looks something like:
    # {'license': 4,
    #  'url': 'http://farm6.staticflickr.com/5454/9413846304_881d5e5c3b_z.jpg',
    #  'file_name': 'COCO_val2014_000000001268.jpg',
    #  'height': 427,
    #  'width': 640,
    #  'date_captured': '2013-11-17 05:57:24',
    #  'id': 1268}
    imgs = lvis_api.load_imgs(img_ids)
    # anns is a list[list[dict]], where each dict is an annotation
    # record for an object. The inner list enumerates the objects in an image
    # and the outer list enumerates over images. Example of anns[0]:
    # [{'segmentation': [[192.81,
    #     247.09,
    #     ...
    #     219.03,
    #     249.06]],
    #   'area': 1035.749,
    #   'image_id': 1268,
    #   'bbox': [192.81, 224.8, 74.73, 33.43],
    #   'category_id': 16,
    #   'id': 42986},
    #  ...]
    anns = [lvis_api.img_ann_map[img_id] for img_id in img_ids]

    # Sanity check that each annotation has a unique id
    ann_ids = [ann[""id""] for anns_per_image in anns for ann in anns_per_image]
    assert len(set(ann_ids)) == len(ann_ids), ""Annotation ids in '{}' are not unique"".format(
        json_file
    )

    imgs_anns = list(zip(imgs, anns))

    logger.info(""Loaded {} images in the LVIS format from {}"".format(len(imgs_anns), json_file))

    def get_file_name(img_root, img_dict):
        # Determine the path including the split folder (""train2017"", ""val2017"", ""test2017"") from
        # the coco_url field. Example:
        #   'coco_url': 'http://images.cocodataset.org/train2017/000000155379.jpg'
        split_folder, file_name = img_dict[""coco_url""].split(""/"")[-2:]
        return os.path.join(img_root + split_folder, file_name)

    dataset_dicts = []

    for (img_dict, anno_dict_list) in imgs_anns:
        record = {}
        record[""file_name""] = get_file_name(image_root, img_dict)
        record[""height""] = img_dict[""height""]
        record[""width""] = img_dict[""width""]
        record[""not_exhaustive_category_ids""] = img_dict.get(""not_exhaustive_category_ids"", [])
        record[""neg_category_ids""] = img_dict.get(""neg_category_ids"", [])
        image_id = record[""image_id""] = img_dict[""id""]

        objs = []
        for anno in anno_dict_list:
            # Check that the image_id in this annotation is the same as
            # the image_id we're looking at.
            # This fails only when the data parsing logic or the annotation file is buggy.
            assert anno[""image_id""] == image_id
            obj = {""bbox"": anno[""bbox""], ""bbox_mode"": BoxMode.XYWH_ABS}
            obj[""category_id""] = anno[""category_id""] - 1  # Convert 1-indexed to 0-indexed
            segm = anno[""segmentation""]  # list[list[float]]
            # filter out invalid polygons (< 3 points)
            valid_segm = [poly for poly in segm if len(poly) % 2 == 0 and len(poly) >= 6]
            assert len(segm) == len(
                valid_segm
            ), ""Annotation contains an invalid polygon with < 3 points""
            assert len(segm) > 0
            obj[""segmentation""] = segm
            objs.append(obj)
        record[""annotations""] = objs
        dataset_dicts.append(record)

    return dataset_dicts",len(poly) % 2 == 0,not len(poly) % 2
GFocal,https://github.com/implus/GFocal/tree/master/mmdet/ops/carafe/carafe.py,CARAFEFunction,forward$79,"def forward(ctx, features, masks, kernel_size, group_size, scale_factor):
        assert scale_factor >= 1
        assert masks.size(1) == kernel_size * kernel_size * group_size
        assert masks.size(-1) == features.size(-1) * scale_factor
        assert masks.size(-2) == features.size(-2) * scale_factor
        assert features.size(1) % group_size == 0
        assert (kernel_size - 1) % 2 == 0 and kernel_size >= 1
        ctx.kernel_size = kernel_size
        ctx.group_size = group_size
        ctx.scale_factor = scale_factor
        ctx.feature_size = features.size()
        ctx.mask_size = masks.size()

        n, c, h, w = features.size()
        output = features.new_zeros((n, c, h * scale_factor, w * scale_factor))
        routput = features.new_zeros(output.size(), requires_grad=False)
        rfeatures = features.new_zeros(features.size(), requires_grad=False)
        rmasks = masks.new_zeros(masks.size(), requires_grad=False)
        if features.is_cuda:
            carafe_cuda.forward(features, rfeatures, masks, rmasks,
                                kernel_size, group_size, scale_factor, routput,
                                output)
        else:
            raise NotImplementedError

        if features.requires_grad or masks.requires_grad:
            ctx.save_for_backward(features, masks, rfeatures)
        return output",features.size(1) % group_size == 0,not features.size(1) % group_size
GFocal,https://github.com/implus/GFocal/tree/master/mmdet/ops/carafe/carafe.py,CARAFEFunction,forward$79,"def forward(ctx, features, masks, kernel_size, group_size, scale_factor):
        assert scale_factor >= 1
        assert masks.size(1) == kernel_size * kernel_size * group_size
        assert masks.size(-1) == features.size(-1) * scale_factor
        assert masks.size(-2) == features.size(-2) * scale_factor
        assert features.size(1) % group_size == 0
        assert (kernel_size - 1) % 2 == 0 and kernel_size >= 1
        ctx.kernel_size = kernel_size
        ctx.group_size = group_size
        ctx.scale_factor = scale_factor
        ctx.feature_size = features.size()
        ctx.mask_size = masks.size()

        n, c, h, w = features.size()
        output = features.new_zeros((n, c, h * scale_factor, w * scale_factor))
        routput = features.new_zeros(output.size(), requires_grad=False)
        rfeatures = features.new_zeros(features.size(), requires_grad=False)
        rmasks = masks.new_zeros(masks.size(), requires_grad=False)
        if features.is_cuda:
            carafe_cuda.forward(features, rfeatures, masks, rmasks,
                                kernel_size, group_size, scale_factor, routput,
                                output)
        else:
            raise NotImplementedError

        if features.requires_grad or masks.requires_grad:
            ctx.save_for_backward(features, masks, rfeatures)
        return output",(kernel_size - 1) % 2 == 0,not (kernel_size - 1) % 2
pygame_tutorials,https://github.com/kidscancode/pygame_tutorials/tree/master/examples/hash example.py,Player,move_8way$155,"def move_8way(self):
        keystate = pg.key.get_pressed()
        if keystate[pg.K_UP]:
            self.vy = -5
        if keystate[pg.K_DOWN]:
            self.vy = 5
        if keystate[pg.K_LEFT]:
            self.vx = -5
        if keystate[pg.K_RIGHT]:
            self.vx = 5
        if self.vx != 0 and self.vy != 0:
            self.vx *= 0.7071
            self.vy *= 0.7071",self.vx != 0,self.vx
pygame_tutorials,https://github.com/kidscancode/pygame_tutorials/tree/master/examples/hash example.py,Player,move_8way$155,"def move_8way(self):
        keystate = pg.key.get_pressed()
        if keystate[pg.K_UP]:
            self.vy = -5
        if keystate[pg.K_DOWN]:
            self.vy = 5
        if keystate[pg.K_LEFT]:
            self.vx = -5
        if keystate[pg.K_RIGHT]:
            self.vx = 5
        if self.vx != 0 and self.vy != 0:
            self.vx *= 0.7071
            self.vy *= 0.7071",self.vy != 0,self.vy
CrackMapExec,https://github.com/byt3bl33d3r/CrackMapExec/tree/master/cme/protocols/smb/passpol.py,PassPolDump,fetchList$123,"def fetchList(self, rpctransport):
        dce = DCERPC_v5(rpctransport)
        dce.connect()
        dce.bind(samr.MSRPC_UUID_SAMR)

        # Setup Connection
        resp = samr.hSamrConnect2(dce)
        if resp['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp2 = samr.hSamrEnumerateDomainsInSamServer(dce, serverHandle=resp['ServerHandle'],
                                                      enumerationContext=0,
                                                      preferedMaximumLength=500)
        if resp2['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp3 = samr.hSamrLookupDomainInSamServer(dce, serverHandle=resp['ServerHandle'],
                                                  name=resp2['Buffer']['Buffer'][0]['Name'])
        if resp3['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp4 = samr.hSamrOpenDomain(dce, serverHandle=resp['ServerHandle'],
                                     desiredAccess=samr.MAXIMUM_ALLOWED,
                                     domainId=resp3['DomainId'])
        if resp4['ErrorCode'] != 0:
            raise Exception('Connect error')

        self.__domains = resp2['Buffer']['Buffer']
        domainHandle = resp4['DomainHandle']
        # End Setup

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainPasswordInformation)
        self.__min_pass_len = re['Buffer']['Password']['MinPasswordLength'] or ""None""
        self.__pass_hist_len = re['Buffer']['Password']['PasswordHistoryLength'] or ""None""
        self.__max_pass_age = convert(int(re['Buffer']['Password']['MaxPasswordAge']['LowPart']), int(re['Buffer']['Password']['MaxPasswordAge']['HighPart']))
        self.__min_pass_age = convert(int(re['Buffer']['Password']['MinPasswordAge']['LowPart']), int(re['Buffer']['Password']['MinPasswordAge']['HighPart']))
        self.__pass_prop = d2b(re['Buffer']['Password']['PasswordProperties'])

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainLockoutInformation)
        self.__rst_accnt_lock_counter = convert(0, re['Buffer']['Lockout']['LockoutObservationWindow'], lockout=True)
        self.__lock_accnt_dur = convert(0, re['Buffer']['Lockout']['LockoutDuration'], lockout=True)
        self.__accnt_lock_thres = re['Buffer']['Lockout']['LockoutThreshold'] or ""None""

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainLogoffInformation)
        self.__force_logoff_time = convert(re['Buffer']['Logoff']['ForceLogoff']['LowPart'], re['Buffer']['Logoff']['ForceLogoff']['HighPart'])

        self.pass_pol = {'min_pass_len': self.__min_pass_len, 'pass_hist_len': self.__pass_hist_len, 
                         'max_pass_age': self.__max_pass_age, 'min_pass_age': self.__min_pass_age, 
                         'pass_prop': self.__pass_prop, 'rst_accnt_lock_counter': self.__rst_accnt_lock_counter,
                         'lock_accnt_dur': self.__lock_accnt_dur, 'accnt_lock_thres': self.__accnt_lock_thres,
                         'force_logoff_time': self.__force_logoff_time}

        dce.disconnect()",resp['ErrorCode'] != 0,resp['ErrorCode']
CrackMapExec,https://github.com/byt3bl33d3r/CrackMapExec/tree/master/cme/protocols/smb/passpol.py,PassPolDump,fetchList$123,"def fetchList(self, rpctransport):
        dce = DCERPC_v5(rpctransport)
        dce.connect()
        dce.bind(samr.MSRPC_UUID_SAMR)

        # Setup Connection
        resp = samr.hSamrConnect2(dce)
        if resp['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp2 = samr.hSamrEnumerateDomainsInSamServer(dce, serverHandle=resp['ServerHandle'],
                                                      enumerationContext=0,
                                                      preferedMaximumLength=500)
        if resp2['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp3 = samr.hSamrLookupDomainInSamServer(dce, serverHandle=resp['ServerHandle'],
                                                  name=resp2['Buffer']['Buffer'][0]['Name'])
        if resp3['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp4 = samr.hSamrOpenDomain(dce, serverHandle=resp['ServerHandle'],
                                     desiredAccess=samr.MAXIMUM_ALLOWED,
                                     domainId=resp3['DomainId'])
        if resp4['ErrorCode'] != 0:
            raise Exception('Connect error')

        self.__domains = resp2['Buffer']['Buffer']
        domainHandle = resp4['DomainHandle']
        # End Setup

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainPasswordInformation)
        self.__min_pass_len = re['Buffer']['Password']['MinPasswordLength'] or ""None""
        self.__pass_hist_len = re['Buffer']['Password']['PasswordHistoryLength'] or ""None""
        self.__max_pass_age = convert(int(re['Buffer']['Password']['MaxPasswordAge']['LowPart']), int(re['Buffer']['Password']['MaxPasswordAge']['HighPart']))
        self.__min_pass_age = convert(int(re['Buffer']['Password']['MinPasswordAge']['LowPart']), int(re['Buffer']['Password']['MinPasswordAge']['HighPart']))
        self.__pass_prop = d2b(re['Buffer']['Password']['PasswordProperties'])

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainLockoutInformation)
        self.__rst_accnt_lock_counter = convert(0, re['Buffer']['Lockout']['LockoutObservationWindow'], lockout=True)
        self.__lock_accnt_dur = convert(0, re['Buffer']['Lockout']['LockoutDuration'], lockout=True)
        self.__accnt_lock_thres = re['Buffer']['Lockout']['LockoutThreshold'] or ""None""

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainLogoffInformation)
        self.__force_logoff_time = convert(re['Buffer']['Logoff']['ForceLogoff']['LowPart'], re['Buffer']['Logoff']['ForceLogoff']['HighPart'])

        self.pass_pol = {'min_pass_len': self.__min_pass_len, 'pass_hist_len': self.__pass_hist_len, 
                         'max_pass_age': self.__max_pass_age, 'min_pass_age': self.__min_pass_age, 
                         'pass_prop': self.__pass_prop, 'rst_accnt_lock_counter': self.__rst_accnt_lock_counter,
                         'lock_accnt_dur': self.__lock_accnt_dur, 'accnt_lock_thres': self.__accnt_lock_thres,
                         'force_logoff_time': self.__force_logoff_time}

        dce.disconnect()",resp2['ErrorCode'] != 0,resp2['ErrorCode']
CrackMapExec,https://github.com/byt3bl33d3r/CrackMapExec/tree/master/cme/protocols/smb/passpol.py,PassPolDump,fetchList$123,"def fetchList(self, rpctransport):
        dce = DCERPC_v5(rpctransport)
        dce.connect()
        dce.bind(samr.MSRPC_UUID_SAMR)

        # Setup Connection
        resp = samr.hSamrConnect2(dce)
        if resp['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp2 = samr.hSamrEnumerateDomainsInSamServer(dce, serverHandle=resp['ServerHandle'],
                                                      enumerationContext=0,
                                                      preferedMaximumLength=500)
        if resp2['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp3 = samr.hSamrLookupDomainInSamServer(dce, serverHandle=resp['ServerHandle'],
                                                  name=resp2['Buffer']['Buffer'][0]['Name'])
        if resp3['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp4 = samr.hSamrOpenDomain(dce, serverHandle=resp['ServerHandle'],
                                     desiredAccess=samr.MAXIMUM_ALLOWED,
                                     domainId=resp3['DomainId'])
        if resp4['ErrorCode'] != 0:
            raise Exception('Connect error')

        self.__domains = resp2['Buffer']['Buffer']
        domainHandle = resp4['DomainHandle']
        # End Setup

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainPasswordInformation)
        self.__min_pass_len = re['Buffer']['Password']['MinPasswordLength'] or ""None""
        self.__pass_hist_len = re['Buffer']['Password']['PasswordHistoryLength'] or ""None""
        self.__max_pass_age = convert(int(re['Buffer']['Password']['MaxPasswordAge']['LowPart']), int(re['Buffer']['Password']['MaxPasswordAge']['HighPart']))
        self.__min_pass_age = convert(int(re['Buffer']['Password']['MinPasswordAge']['LowPart']), int(re['Buffer']['Password']['MinPasswordAge']['HighPart']))
        self.__pass_prop = d2b(re['Buffer']['Password']['PasswordProperties'])

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainLockoutInformation)
        self.__rst_accnt_lock_counter = convert(0, re['Buffer']['Lockout']['LockoutObservationWindow'], lockout=True)
        self.__lock_accnt_dur = convert(0, re['Buffer']['Lockout']['LockoutDuration'], lockout=True)
        self.__accnt_lock_thres = re['Buffer']['Lockout']['LockoutThreshold'] or ""None""

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainLogoffInformation)
        self.__force_logoff_time = convert(re['Buffer']['Logoff']['ForceLogoff']['LowPart'], re['Buffer']['Logoff']['ForceLogoff']['HighPart'])

        self.pass_pol = {'min_pass_len': self.__min_pass_len, 'pass_hist_len': self.__pass_hist_len, 
                         'max_pass_age': self.__max_pass_age, 'min_pass_age': self.__min_pass_age, 
                         'pass_prop': self.__pass_prop, 'rst_accnt_lock_counter': self.__rst_accnt_lock_counter,
                         'lock_accnt_dur': self.__lock_accnt_dur, 'accnt_lock_thres': self.__accnt_lock_thres,
                         'force_logoff_time': self.__force_logoff_time}

        dce.disconnect()",resp3['ErrorCode'] != 0,resp3['ErrorCode']
CrackMapExec,https://github.com/byt3bl33d3r/CrackMapExec/tree/master/cme/protocols/smb/passpol.py,PassPolDump,fetchList$123,"def fetchList(self, rpctransport):
        dce = DCERPC_v5(rpctransport)
        dce.connect()
        dce.bind(samr.MSRPC_UUID_SAMR)

        # Setup Connection
        resp = samr.hSamrConnect2(dce)
        if resp['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp2 = samr.hSamrEnumerateDomainsInSamServer(dce, serverHandle=resp['ServerHandle'],
                                                      enumerationContext=0,
                                                      preferedMaximumLength=500)
        if resp2['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp3 = samr.hSamrLookupDomainInSamServer(dce, serverHandle=resp['ServerHandle'],
                                                  name=resp2['Buffer']['Buffer'][0]['Name'])
        if resp3['ErrorCode'] != 0:
            raise Exception('Connect error')

        resp4 = samr.hSamrOpenDomain(dce, serverHandle=resp['ServerHandle'],
                                     desiredAccess=samr.MAXIMUM_ALLOWED,
                                     domainId=resp3['DomainId'])
        if resp4['ErrorCode'] != 0:
            raise Exception('Connect error')

        self.__domains = resp2['Buffer']['Buffer']
        domainHandle = resp4['DomainHandle']
        # End Setup

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainPasswordInformation)
        self.__min_pass_len = re['Buffer']['Password']['MinPasswordLength'] or ""None""
        self.__pass_hist_len = re['Buffer']['Password']['PasswordHistoryLength'] or ""None""
        self.__max_pass_age = convert(int(re['Buffer']['Password']['MaxPasswordAge']['LowPart']), int(re['Buffer']['Password']['MaxPasswordAge']['HighPart']))
        self.__min_pass_age = convert(int(re['Buffer']['Password']['MinPasswordAge']['LowPart']), int(re['Buffer']['Password']['MinPasswordAge']['HighPart']))
        self.__pass_prop = d2b(re['Buffer']['Password']['PasswordProperties'])

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainLockoutInformation)
        self.__rst_accnt_lock_counter = convert(0, re['Buffer']['Lockout']['LockoutObservationWindow'], lockout=True)
        self.__lock_accnt_dur = convert(0, re['Buffer']['Lockout']['LockoutDuration'], lockout=True)
        self.__accnt_lock_thres = re['Buffer']['Lockout']['LockoutThreshold'] or ""None""

        re = samr.hSamrQueryInformationDomain2(dce, domainHandle=domainHandle,
                                               domainInformationClass=samr.DOMAIN_INFORMATION_CLASS.DomainLogoffInformation)
        self.__force_logoff_time = convert(re['Buffer']['Logoff']['ForceLogoff']['LowPart'], re['Buffer']['Logoff']['ForceLogoff']['HighPart'])

        self.pass_pol = {'min_pass_len': self.__min_pass_len, 'pass_hist_len': self.__pass_hist_len, 
                         'max_pass_age': self.__max_pass_age, 'min_pass_age': self.__min_pass_age, 
                         'pass_prop': self.__pass_prop, 'rst_accnt_lock_counter': self.__rst_accnt_lock_counter,
                         'lock_accnt_dur': self.__lock_accnt_dur, 'accnt_lock_thres': self.__accnt_lock_thres,
                         'force_logoff_time': self.__force_logoff_time}

        dce.disconnect()",resp4['ErrorCode'] != 0,resp4['ErrorCode']
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/downloader/common.py,FileDownloader,try_utime$206,"def try_utime(self, filename, last_modified_hdr):
        """"""Try to set the last-modified time of the given file.""""""
        if last_modified_hdr is None:
            return
        if not os.path.isfile(encodeFilename(filename)):
            return
        timestr = last_modified_hdr
        if timestr is None:
            return
        filetime = timeconvert(timestr)
        if filetime is None:
            return filetime
        # Ignore obviously invalid dates
        if filetime == 0:
            return
        try:
            os.utime(filename, (time.time(), filetime))
        except Exception:
            pass
        return filetime",filetime == 0,not filetime
faker,https://github.com/joke2k/faker/tree/master/tests/providers/test_ssn.py,TestPtBR,test_pt_BR_ssn_checksum$858,"def test_pt_BR_ssn_checksum(self):
        assert pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2]) == 2
        assert pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2, 2]) == 0","pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2, 2]) == 0","not pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2, 2])"
koalas,https://github.com/databricks/koalas/tree/master/databricks/koalas/frame.py,DataFrame,quantile$10687,"def quantile(
        self,
        q: Union[float, Iterable[float]] = 0.5,
        axis: Union[int, str] = 0,
        numeric_only: bool = True,
        accuracy: int = 10000,
    ) -> Union[""DataFrame"", ""Series""]:
        """"""
        Return value at the given quantile.

        .. note:: Unlike pandas', the quantile in Koalas is an approximated quantile based upon
            approximate percentile computation because computing quantile across a large dataset
            is extremely expensive.

        Parameters
        ----------
        q : float or array-like, default 0.5 (50% quantile)
            0 <= q <= 1, the quantile(s) to compute.
        axis : int or str, default 0 or 'index'
            Can only be set to 0 at the moment.
        numeric_only : bool, default True
            If False, the quantile of datetime and timedelta data will be computed as well.
            Can only be set to True at the moment.
        accuracy : int, optional
            Default accuracy of approximation. Larger value means better accuracy.
            The relative error can be deduced by 1.0 / accuracy.

        Returns
        -------
        Series or DataFrame
            If q is an array, a DataFrame will be returned where the
            index is q, the columns are the columns of self, and the values are the quantiles.
            If q is a float, a Series will be returned where the
            index is the columns of self and the values are the quantiles.

        Examples
        --------
        >>> kdf = ks.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [6, 7, 8, 9, 0]})
        >>> kdf
           a  b
        0  1  6
        1  2  7
        2  3  8
        3  4  9
        4  5  0

        >>> kdf.quantile(.5)
        a    3.0
        b    7.0
        Name: 0.5, dtype: float64

        >>> kdf.quantile([.25, .5, .75])
                a    b
        0.25  2.0  6.0
        0.50  3.0  7.0
        0.75  4.0  8.0
        """"""
        axis = validate_axis(axis)
        if axis != 0:
            raise NotImplementedError('axis should be either 0 or ""index"" currently.')

        if not isinstance(accuracy, int):
            raise ValueError(
                ""accuracy must be an integer; however, got [%s]"" % type(accuracy).__name__
            )

        if isinstance(q, Iterable):
            q = list(q)

        for v in q if isinstance(q, list) else [q]:
            if not isinstance(v, float):
                raise ValueError(
                    ""q must be a float or an array of floats; however, [%s] found."" % type(v)
                )
            if v < 0.0 or v > 1.0:
                raise ValueError(""percentiles should all be in the interval [0, 1]."")

        def quantile(spark_column, spark_type):
            if isinstance(spark_type, (BooleanType, NumericType)):
                return SF.percentile_approx(spark_column.cast(DoubleType()), q, accuracy)
            else:
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )

        if isinstance(q, list):
            # First calculate the percentiles from all columns and map it to each `quantiles`
            # by creating each entry as a struct. So, it becomes an array of structs as below:
            #
            # +-----------------------------------------+
            # |                                   arrays|
            # +-----------------------------------------+
            # |[[0.25, 2, 6], [0.5, 3, 7], [0.75, 4, 8]]|
            # +-----------------------------------------+

            percentile_cols = []
            percentile_col_names = []
            column_labels = []
            for label, column in zip(
                self._internal.column_labels, self._internal.data_spark_column_names
            ):
                spark_type = self._internal.spark_type_for(label)

                is_numeric_or_boolean = isinstance(spark_type, (NumericType, BooleanType))
                keep_column = not numeric_only or is_numeric_or_boolean

                if keep_column:
                    percentile_col = quantile(self._internal.spark_column_for(label), spark_type)
                    percentile_cols.append(percentile_col.alias(column))
                    percentile_col_names.append(column)
                    column_labels.append(label)

            if len(percentile_cols) == 0:
                return DataFrame(index=q)

            sdf = self._internal.spark_frame.select(percentile_cols)
            # Here, after select percentile cols, a spark_frame looks like below:
            # +---------+---------+
            # |        a|        b|
            # +---------+---------+
            # |[2, 3, 4]|[6, 7, 8]|
            # +---------+---------+

            cols_dict = OrderedDict()  # type: OrderedDict
            for column in percentile_col_names:
                cols_dict[column] = list()
                for i in range(len(q)):
                    cols_dict[column].append(scol_for(sdf, column).getItem(i).alias(column))

            internal_index_column = SPARK_DEFAULT_INDEX_NAME
            cols = []
            for i, col in enumerate(zip(*cols_dict.values())):
                cols.append(F.struct(F.lit(q[i]).alias(internal_index_column), *col))
            sdf = sdf.select(F.array(*cols).alias(""arrays""))

            # And then, explode it and manually set the index.
            # +-----------------+---+---+
            # |__index_level_0__|  a|  b|
            # +-----------------+---+---+
            # |             0.25|  2|  6|
            # |              0.5|  3|  7|
            # |             0.75|  4|  8|
            # +-----------------+---+---+
            sdf = sdf.select(F.explode(F.col(""arrays""))).selectExpr(""col.*"")

            internal = InternalFrame(
                spark_frame=sdf,
                index_spark_columns=[scol_for(sdf, internal_index_column)],
                column_labels=column_labels,
                data_spark_columns=[scol_for(sdf, col) for col in percentile_col_names],
            )
            return DataFrame(internal)
        else:
            return self._reduce_for_stat_function(
                quantile, name=""quantile"", numeric_only=numeric_only
            ).rename(q)",axis != 0,axis
koalas,https://github.com/databricks/koalas/tree/master/databricks/koalas/frame.py,DataFrame,quantile$10687,"def quantile(
        self,
        q: Union[float, Iterable[float]] = 0.5,
        axis: Union[int, str] = 0,
        numeric_only: bool = True,
        accuracy: int = 10000,
    ) -> Union[""DataFrame"", ""Series""]:
        """"""
        Return value at the given quantile.

        .. note:: Unlike pandas', the quantile in Koalas is an approximated quantile based upon
            approximate percentile computation because computing quantile across a large dataset
            is extremely expensive.

        Parameters
        ----------
        q : float or array-like, default 0.5 (50% quantile)
            0 <= q <= 1, the quantile(s) to compute.
        axis : int or str, default 0 or 'index'
            Can only be set to 0 at the moment.
        numeric_only : bool, default True
            If False, the quantile of datetime and timedelta data will be computed as well.
            Can only be set to True at the moment.
        accuracy : int, optional
            Default accuracy of approximation. Larger value means better accuracy.
            The relative error can be deduced by 1.0 / accuracy.

        Returns
        -------
        Series or DataFrame
            If q is an array, a DataFrame will be returned where the
            index is q, the columns are the columns of self, and the values are the quantiles.
            If q is a float, a Series will be returned where the
            index is the columns of self and the values are the quantiles.

        Examples
        --------
        >>> kdf = ks.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [6, 7, 8, 9, 0]})
        >>> kdf
           a  b
        0  1  6
        1  2  7
        2  3  8
        3  4  9
        4  5  0

        >>> kdf.quantile(.5)
        a    3.0
        b    7.0
        Name: 0.5, dtype: float64

        >>> kdf.quantile([.25, .5, .75])
                a    b
        0.25  2.0  6.0
        0.50  3.0  7.0
        0.75  4.0  8.0
        """"""
        axis = validate_axis(axis)
        if axis != 0:
            raise NotImplementedError('axis should be either 0 or ""index"" currently.')

        if not isinstance(accuracy, int):
            raise ValueError(
                ""accuracy must be an integer; however, got [%s]"" % type(accuracy).__name__
            )

        if isinstance(q, Iterable):
            q = list(q)

        for v in q if isinstance(q, list) else [q]:
            if not isinstance(v, float):
                raise ValueError(
                    ""q must be a float or an array of floats; however, [%s] found."" % type(v)
                )
            if v < 0.0 or v > 1.0:
                raise ValueError(""percentiles should all be in the interval [0, 1]."")

        def quantile(spark_column, spark_type):
            if isinstance(spark_type, (BooleanType, NumericType)):
                return SF.percentile_approx(spark_column.cast(DoubleType()), q, accuracy)
            else:
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )

        if isinstance(q, list):
            # First calculate the percentiles from all columns and map it to each `quantiles`
            # by creating each entry as a struct. So, it becomes an array of structs as below:
            #
            # +-----------------------------------------+
            # |                                   arrays|
            # +-----------------------------------------+
            # |[[0.25, 2, 6], [0.5, 3, 7], [0.75, 4, 8]]|
            # +-----------------------------------------+

            percentile_cols = []
            percentile_col_names = []
            column_labels = []
            for label, column in zip(
                self._internal.column_labels, self._internal.data_spark_column_names
            ):
                spark_type = self._internal.spark_type_for(label)

                is_numeric_or_boolean = isinstance(spark_type, (NumericType, BooleanType))
                keep_column = not numeric_only or is_numeric_or_boolean

                if keep_column:
                    percentile_col = quantile(self._internal.spark_column_for(label), spark_type)
                    percentile_cols.append(percentile_col.alias(column))
                    percentile_col_names.append(column)
                    column_labels.append(label)

            if len(percentile_cols) == 0:
                return DataFrame(index=q)

            sdf = self._internal.spark_frame.select(percentile_cols)
            # Here, after select percentile cols, a spark_frame looks like below:
            # +---------+---------+
            # |        a|        b|
            # +---------+---------+
            # |[2, 3, 4]|[6, 7, 8]|
            # +---------+---------+

            cols_dict = OrderedDict()  # type: OrderedDict
            for column in percentile_col_names:
                cols_dict[column] = list()
                for i in range(len(q)):
                    cols_dict[column].append(scol_for(sdf, column).getItem(i).alias(column))

            internal_index_column = SPARK_DEFAULT_INDEX_NAME
            cols = []
            for i, col in enumerate(zip(*cols_dict.values())):
                cols.append(F.struct(F.lit(q[i]).alias(internal_index_column), *col))
            sdf = sdf.select(F.array(*cols).alias(""arrays""))

            # And then, explode it and manually set the index.
            # +-----------------+---+---+
            # |__index_level_0__|  a|  b|
            # +-----------------+---+---+
            # |             0.25|  2|  6|
            # |              0.5|  3|  7|
            # |             0.75|  4|  8|
            # +-----------------+---+---+
            sdf = sdf.select(F.explode(F.col(""arrays""))).selectExpr(""col.*"")

            internal = InternalFrame(
                spark_frame=sdf,
                index_spark_columns=[scol_for(sdf, internal_index_column)],
                column_labels=column_labels,
                data_spark_columns=[scol_for(sdf, col) for col in percentile_col_names],
            )
            return DataFrame(internal)
        else:
            return self._reduce_for_stat_function(
                quantile, name=""quantile"", numeric_only=numeric_only
            ).rename(q)",len(percentile_cols) == 0,not percentile_cols
investpy,https://github.com/alvarobartt/investpy/tree/master/investpy/stocks.py,,search_stocks$1756,"def search_stocks(by, value):
    """"""
    This function searches stocks by the introduced value for the specified field. This means that this function
    is going to search if there is a value that matches the introduced one for the specified field which is the
    `stocks.csv` column name to search in. Available fields to search stocks are 'name', 'full_name' and 'isin'.

    Args:
        by (:obj:`str`): name of the field to search for, which is the column name which can be: 'name', 'full_name' or 'isin'.
        value (:obj:`str`): value of the field to search for, which is the value that is going to be searched.

    Returns:
        :obj:`pandas.DataFrame` - search_result:
            The resulting :obj:`pandas.DataFrame` contains the search results from the given query, which is
            any match of the specified value in the specified field. If there are no results for the given query,
            an error will be raised, but otherwise the resulting :obj:`pandas.DataFrame` will contain all the
            available stocks that match the introduced query.

    Raises:
        ValueError: raised if any of the introduced parameters is not valid or errored.
        FileNotFoundError: raised if `stocks.csv` file is missing.
        IOError: raised if data could not be retrieved due to file error.
        RuntimeError: raised if no results were found for the introduced value in the introduced field.

    """"""

    if not by:
        raise ValueError(
            ""ERR#0006: the introduced field to search is mandatory and should be a str.""
        )

    if not isinstance(by, str):
        raise ValueError(
            ""ERR#0006: the introduced field to search is mandatory and should be a str.""
        )

    if not value:
        raise ValueError(
            ""ERR#0017: the introduced value to search is mandatory and should be a str.""
        )

    if not isinstance(value, str):
        raise ValueError(
            ""ERR#0017: the introduced value to search is mandatory and should be a str.""
        )

    resource_package = ""investpy""
    resource_path = ""/"".join(((""resources"", ""stocks.csv"")))
    if pkg_resources.resource_exists(resource_package, resource_path):
        stocks = pd.read_csv(
            pkg_resources.resource_filename(resource_package, resource_path),
            keep_default_na=False,
        )
    else:
        raise FileNotFoundError(""ERR#0056: stocks file not found or errored."")

    if stocks is None:
        raise IOError(""ERR#0001: stocks object not found or unable to retrieve."")

    stocks.drop(columns=[""tag"", ""id""], inplace=True)

    available_search_fields = stocks.columns.tolist()

    if isinstance(by, str) and by not in available_search_fields:
        raise ValueError(
            ""ERR#0026: the introduced field to search can either just be ""
            + "" or "".join(available_search_fields)
        )

    stocks[""matches""] = stocks[by].str.contains(value, case=False)

    search_result = stocks.loc[stocks[""matches""] == True].copy()

    if len(search_result) == 0:
        raise RuntimeError(
            ""ERR#0043: no results were found for the introduced "" + str(by) + "".""
        )

    search_result.drop(columns=[""matches""], inplace=True)
    search_result.reset_index(drop=True, inplace=True)

    return search_result",len(search_result) == 0,not search_result
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/strategy/cuda.py,,conv3d_strategy_cuda$716,"def conv3d_strategy_cuda(attrs, inputs, out_type, target):
    """"""conv3d cuda strategy""""""
    strategy = _op.OpStrategy()
    data, kernel = inputs
    layout = attrs.data_layout
    _, stride_h, stride_w = attrs.get_int_tuple(""strides"")
    _, dilation_h, dilation_w = attrs.get_int_tuple(""dilation"")
    assert layout in [""NCDHW"", ""NDHWC""], ""Not support this layout {} yet"".format(layout)
    if layout == ""NCDHW"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ncdhw),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw),
            name=""conv3d_ncdhw.cuda"",
            plevel=10,
        )
        _, _, _, kh, kw = get_const_tuple(kernel.shape)
        if (
            2 < kh < 8
            and 2 < kw < 8
            and kh == kw
            and stride_h == 1
            and stride_w == 1
            and dilation_h == 1
            and dilation_w == 1
            and attrs[""groups""] == 1
        ):
            strategy.add_implementation(
                wrap_compute_conv3d(topi.cuda.conv3d_ncdhw_winograd),
                wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw_winograd),
                name=""conv3d_ncdhw_winograd.cuda"",
                plevel=5,
            )
    else:  # layout == ""NDHWC"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ndhwc),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc),
            name=""conv3d_ndhwc.cuda"",
            plevel=10,
        )
        N, _, _, _, _ = get_const_tuple(data.shape)
        _, _, _, CI, CO = get_const_tuple(kernel.shape)
        if target.kind.name == ""cuda"":
            if nvcc.have_tensorcore(target=target):
                if (
                    (N % 16 == 0 and CI % 16 == 0 and CO % 16 == 0)
                    or (N % 8 == 0 and CI % 16 == 0 and CO % 32 == 0)
                    or (N % 32 == 0 and CI % 16 == 0 and CO % 8 == 0)
                ) and out_type == ""float16"":
                    strategy.add_implementation(
                        wrap_compute_conv3d(topi.cuda.conv3d_ndhwc_tensorcore),
                        wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc_tensorcore),
                        name=""conv3d_ndhwc_tensorcore.cuda"",
                        plevel=20,
                    )

    if target.kind.name == ""cuda"" and ""cudnn"" in target.libs:
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_cudnn, True),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_cudnn),
            name=""conv3d_cudnn.cuda"",
            plevel=25,
        )
    return strategy",N % 16 == 0,not N % 16
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/strategy/cuda.py,,conv3d_strategy_cuda$716,"def conv3d_strategy_cuda(attrs, inputs, out_type, target):
    """"""conv3d cuda strategy""""""
    strategy = _op.OpStrategy()
    data, kernel = inputs
    layout = attrs.data_layout
    _, stride_h, stride_w = attrs.get_int_tuple(""strides"")
    _, dilation_h, dilation_w = attrs.get_int_tuple(""dilation"")
    assert layout in [""NCDHW"", ""NDHWC""], ""Not support this layout {} yet"".format(layout)
    if layout == ""NCDHW"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ncdhw),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw),
            name=""conv3d_ncdhw.cuda"",
            plevel=10,
        )
        _, _, _, kh, kw = get_const_tuple(kernel.shape)
        if (
            2 < kh < 8
            and 2 < kw < 8
            and kh == kw
            and stride_h == 1
            and stride_w == 1
            and dilation_h == 1
            and dilation_w == 1
            and attrs[""groups""] == 1
        ):
            strategy.add_implementation(
                wrap_compute_conv3d(topi.cuda.conv3d_ncdhw_winograd),
                wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw_winograd),
                name=""conv3d_ncdhw_winograd.cuda"",
                plevel=5,
            )
    else:  # layout == ""NDHWC"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ndhwc),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc),
            name=""conv3d_ndhwc.cuda"",
            plevel=10,
        )
        N, _, _, _, _ = get_const_tuple(data.shape)
        _, _, _, CI, CO = get_const_tuple(kernel.shape)
        if target.kind.name == ""cuda"":
            if nvcc.have_tensorcore(target=target):
                if (
                    (N % 16 == 0 and CI % 16 == 0 and CO % 16 == 0)
                    or (N % 8 == 0 and CI % 16 == 0 and CO % 32 == 0)
                    or (N % 32 == 0 and CI % 16 == 0 and CO % 8 == 0)
                ) and out_type == ""float16"":
                    strategy.add_implementation(
                        wrap_compute_conv3d(topi.cuda.conv3d_ndhwc_tensorcore),
                        wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc_tensorcore),
                        name=""conv3d_ndhwc_tensorcore.cuda"",
                        plevel=20,
                    )

    if target.kind.name == ""cuda"" and ""cudnn"" in target.libs:
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_cudnn, True),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_cudnn),
            name=""conv3d_cudnn.cuda"",
            plevel=25,
        )
    return strategy",CI % 16 == 0,not CI % 16
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/strategy/cuda.py,,conv3d_strategy_cuda$716,"def conv3d_strategy_cuda(attrs, inputs, out_type, target):
    """"""conv3d cuda strategy""""""
    strategy = _op.OpStrategy()
    data, kernel = inputs
    layout = attrs.data_layout
    _, stride_h, stride_w = attrs.get_int_tuple(""strides"")
    _, dilation_h, dilation_w = attrs.get_int_tuple(""dilation"")
    assert layout in [""NCDHW"", ""NDHWC""], ""Not support this layout {} yet"".format(layout)
    if layout == ""NCDHW"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ncdhw),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw),
            name=""conv3d_ncdhw.cuda"",
            plevel=10,
        )
        _, _, _, kh, kw = get_const_tuple(kernel.shape)
        if (
            2 < kh < 8
            and 2 < kw < 8
            and kh == kw
            and stride_h == 1
            and stride_w == 1
            and dilation_h == 1
            and dilation_w == 1
            and attrs[""groups""] == 1
        ):
            strategy.add_implementation(
                wrap_compute_conv3d(topi.cuda.conv3d_ncdhw_winograd),
                wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw_winograd),
                name=""conv3d_ncdhw_winograd.cuda"",
                plevel=5,
            )
    else:  # layout == ""NDHWC"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ndhwc),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc),
            name=""conv3d_ndhwc.cuda"",
            plevel=10,
        )
        N, _, _, _, _ = get_const_tuple(data.shape)
        _, _, _, CI, CO = get_const_tuple(kernel.shape)
        if target.kind.name == ""cuda"":
            if nvcc.have_tensorcore(target=target):
                if (
                    (N % 16 == 0 and CI % 16 == 0 and CO % 16 == 0)
                    or (N % 8 == 0 and CI % 16 == 0 and CO % 32 == 0)
                    or (N % 32 == 0 and CI % 16 == 0 and CO % 8 == 0)
                ) and out_type == ""float16"":
                    strategy.add_implementation(
                        wrap_compute_conv3d(topi.cuda.conv3d_ndhwc_tensorcore),
                        wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc_tensorcore),
                        name=""conv3d_ndhwc_tensorcore.cuda"",
                        plevel=20,
                    )

    if target.kind.name == ""cuda"" and ""cudnn"" in target.libs:
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_cudnn, True),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_cudnn),
            name=""conv3d_cudnn.cuda"",
            plevel=25,
        )
    return strategy",CO % 16 == 0,not CO % 16
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/strategy/cuda.py,,conv3d_strategy_cuda$716,"def conv3d_strategy_cuda(attrs, inputs, out_type, target):
    """"""conv3d cuda strategy""""""
    strategy = _op.OpStrategy()
    data, kernel = inputs
    layout = attrs.data_layout
    _, stride_h, stride_w = attrs.get_int_tuple(""strides"")
    _, dilation_h, dilation_w = attrs.get_int_tuple(""dilation"")
    assert layout in [""NCDHW"", ""NDHWC""], ""Not support this layout {} yet"".format(layout)
    if layout == ""NCDHW"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ncdhw),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw),
            name=""conv3d_ncdhw.cuda"",
            plevel=10,
        )
        _, _, _, kh, kw = get_const_tuple(kernel.shape)
        if (
            2 < kh < 8
            and 2 < kw < 8
            and kh == kw
            and stride_h == 1
            and stride_w == 1
            and dilation_h == 1
            and dilation_w == 1
            and attrs[""groups""] == 1
        ):
            strategy.add_implementation(
                wrap_compute_conv3d(topi.cuda.conv3d_ncdhw_winograd),
                wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw_winograd),
                name=""conv3d_ncdhw_winograd.cuda"",
                plevel=5,
            )
    else:  # layout == ""NDHWC"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ndhwc),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc),
            name=""conv3d_ndhwc.cuda"",
            plevel=10,
        )
        N, _, _, _, _ = get_const_tuple(data.shape)
        _, _, _, CI, CO = get_const_tuple(kernel.shape)
        if target.kind.name == ""cuda"":
            if nvcc.have_tensorcore(target=target):
                if (
                    (N % 16 == 0 and CI % 16 == 0 and CO % 16 == 0)
                    or (N % 8 == 0 and CI % 16 == 0 and CO % 32 == 0)
                    or (N % 32 == 0 and CI % 16 == 0 and CO % 8 == 0)
                ) and out_type == ""float16"":
                    strategy.add_implementation(
                        wrap_compute_conv3d(topi.cuda.conv3d_ndhwc_tensorcore),
                        wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc_tensorcore),
                        name=""conv3d_ndhwc_tensorcore.cuda"",
                        plevel=20,
                    )

    if target.kind.name == ""cuda"" and ""cudnn"" in target.libs:
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_cudnn, True),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_cudnn),
            name=""conv3d_cudnn.cuda"",
            plevel=25,
        )
    return strategy",N % 8 == 0,not N % 8
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/strategy/cuda.py,,conv3d_strategy_cuda$716,"def conv3d_strategy_cuda(attrs, inputs, out_type, target):
    """"""conv3d cuda strategy""""""
    strategy = _op.OpStrategy()
    data, kernel = inputs
    layout = attrs.data_layout
    _, stride_h, stride_w = attrs.get_int_tuple(""strides"")
    _, dilation_h, dilation_w = attrs.get_int_tuple(""dilation"")
    assert layout in [""NCDHW"", ""NDHWC""], ""Not support this layout {} yet"".format(layout)
    if layout == ""NCDHW"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ncdhw),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw),
            name=""conv3d_ncdhw.cuda"",
            plevel=10,
        )
        _, _, _, kh, kw = get_const_tuple(kernel.shape)
        if (
            2 < kh < 8
            and 2 < kw < 8
            and kh == kw
            and stride_h == 1
            and stride_w == 1
            and dilation_h == 1
            and dilation_w == 1
            and attrs[""groups""] == 1
        ):
            strategy.add_implementation(
                wrap_compute_conv3d(topi.cuda.conv3d_ncdhw_winograd),
                wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw_winograd),
                name=""conv3d_ncdhw_winograd.cuda"",
                plevel=5,
            )
    else:  # layout == ""NDHWC"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ndhwc),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc),
            name=""conv3d_ndhwc.cuda"",
            plevel=10,
        )
        N, _, _, _, _ = get_const_tuple(data.shape)
        _, _, _, CI, CO = get_const_tuple(kernel.shape)
        if target.kind.name == ""cuda"":
            if nvcc.have_tensorcore(target=target):
                if (
                    (N % 16 == 0 and CI % 16 == 0 and CO % 16 == 0)
                    or (N % 8 == 0 and CI % 16 == 0 and CO % 32 == 0)
                    or (N % 32 == 0 and CI % 16 == 0 and CO % 8 == 0)
                ) and out_type == ""float16"":
                    strategy.add_implementation(
                        wrap_compute_conv3d(topi.cuda.conv3d_ndhwc_tensorcore),
                        wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc_tensorcore),
                        name=""conv3d_ndhwc_tensorcore.cuda"",
                        plevel=20,
                    )

    if target.kind.name == ""cuda"" and ""cudnn"" in target.libs:
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_cudnn, True),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_cudnn),
            name=""conv3d_cudnn.cuda"",
            plevel=25,
        )
    return strategy",CI % 16 == 0,not CI % 16
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/strategy/cuda.py,,conv3d_strategy_cuda$716,"def conv3d_strategy_cuda(attrs, inputs, out_type, target):
    """"""conv3d cuda strategy""""""
    strategy = _op.OpStrategy()
    data, kernel = inputs
    layout = attrs.data_layout
    _, stride_h, stride_w = attrs.get_int_tuple(""strides"")
    _, dilation_h, dilation_w = attrs.get_int_tuple(""dilation"")
    assert layout in [""NCDHW"", ""NDHWC""], ""Not support this layout {} yet"".format(layout)
    if layout == ""NCDHW"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ncdhw),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw),
            name=""conv3d_ncdhw.cuda"",
            plevel=10,
        )
        _, _, _, kh, kw = get_const_tuple(kernel.shape)
        if (
            2 < kh < 8
            and 2 < kw < 8
            and kh == kw
            and stride_h == 1
            and stride_w == 1
            and dilation_h == 1
            and dilation_w == 1
            and attrs[""groups""] == 1
        ):
            strategy.add_implementation(
                wrap_compute_conv3d(topi.cuda.conv3d_ncdhw_winograd),
                wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw_winograd),
                name=""conv3d_ncdhw_winograd.cuda"",
                plevel=5,
            )
    else:  # layout == ""NDHWC"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ndhwc),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc),
            name=""conv3d_ndhwc.cuda"",
            plevel=10,
        )
        N, _, _, _, _ = get_const_tuple(data.shape)
        _, _, _, CI, CO = get_const_tuple(kernel.shape)
        if target.kind.name == ""cuda"":
            if nvcc.have_tensorcore(target=target):
                if (
                    (N % 16 == 0 and CI % 16 == 0 and CO % 16 == 0)
                    or (N % 8 == 0 and CI % 16 == 0 and CO % 32 == 0)
                    or (N % 32 == 0 and CI % 16 == 0 and CO % 8 == 0)
                ) and out_type == ""float16"":
                    strategy.add_implementation(
                        wrap_compute_conv3d(topi.cuda.conv3d_ndhwc_tensorcore),
                        wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc_tensorcore),
                        name=""conv3d_ndhwc_tensorcore.cuda"",
                        plevel=20,
                    )

    if target.kind.name == ""cuda"" and ""cudnn"" in target.libs:
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_cudnn, True),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_cudnn),
            name=""conv3d_cudnn.cuda"",
            plevel=25,
        )
    return strategy",CO % 32 == 0,not CO % 32
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/strategy/cuda.py,,conv3d_strategy_cuda$716,"def conv3d_strategy_cuda(attrs, inputs, out_type, target):
    """"""conv3d cuda strategy""""""
    strategy = _op.OpStrategy()
    data, kernel = inputs
    layout = attrs.data_layout
    _, stride_h, stride_w = attrs.get_int_tuple(""strides"")
    _, dilation_h, dilation_w = attrs.get_int_tuple(""dilation"")
    assert layout in [""NCDHW"", ""NDHWC""], ""Not support this layout {} yet"".format(layout)
    if layout == ""NCDHW"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ncdhw),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw),
            name=""conv3d_ncdhw.cuda"",
            plevel=10,
        )
        _, _, _, kh, kw = get_const_tuple(kernel.shape)
        if (
            2 < kh < 8
            and 2 < kw < 8
            and kh == kw
            and stride_h == 1
            and stride_w == 1
            and dilation_h == 1
            and dilation_w == 1
            and attrs[""groups""] == 1
        ):
            strategy.add_implementation(
                wrap_compute_conv3d(topi.cuda.conv3d_ncdhw_winograd),
                wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw_winograd),
                name=""conv3d_ncdhw_winograd.cuda"",
                plevel=5,
            )
    else:  # layout == ""NDHWC"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ndhwc),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc),
            name=""conv3d_ndhwc.cuda"",
            plevel=10,
        )
        N, _, _, _, _ = get_const_tuple(data.shape)
        _, _, _, CI, CO = get_const_tuple(kernel.shape)
        if target.kind.name == ""cuda"":
            if nvcc.have_tensorcore(target=target):
                if (
                    (N % 16 == 0 and CI % 16 == 0 and CO % 16 == 0)
                    or (N % 8 == 0 and CI % 16 == 0 and CO % 32 == 0)
                    or (N % 32 == 0 and CI % 16 == 0 and CO % 8 == 0)
                ) and out_type == ""float16"":
                    strategy.add_implementation(
                        wrap_compute_conv3d(topi.cuda.conv3d_ndhwc_tensorcore),
                        wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc_tensorcore),
                        name=""conv3d_ndhwc_tensorcore.cuda"",
                        plevel=20,
                    )

    if target.kind.name == ""cuda"" and ""cudnn"" in target.libs:
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_cudnn, True),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_cudnn),
            name=""conv3d_cudnn.cuda"",
            plevel=25,
        )
    return strategy",N % 32 == 0,not N % 32
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/strategy/cuda.py,,conv3d_strategy_cuda$716,"def conv3d_strategy_cuda(attrs, inputs, out_type, target):
    """"""conv3d cuda strategy""""""
    strategy = _op.OpStrategy()
    data, kernel = inputs
    layout = attrs.data_layout
    _, stride_h, stride_w = attrs.get_int_tuple(""strides"")
    _, dilation_h, dilation_w = attrs.get_int_tuple(""dilation"")
    assert layout in [""NCDHW"", ""NDHWC""], ""Not support this layout {} yet"".format(layout)
    if layout == ""NCDHW"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ncdhw),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw),
            name=""conv3d_ncdhw.cuda"",
            plevel=10,
        )
        _, _, _, kh, kw = get_const_tuple(kernel.shape)
        if (
            2 < kh < 8
            and 2 < kw < 8
            and kh == kw
            and stride_h == 1
            and stride_w == 1
            and dilation_h == 1
            and dilation_w == 1
            and attrs[""groups""] == 1
        ):
            strategy.add_implementation(
                wrap_compute_conv3d(topi.cuda.conv3d_ncdhw_winograd),
                wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw_winograd),
                name=""conv3d_ncdhw_winograd.cuda"",
                plevel=5,
            )
    else:  # layout == ""NDHWC"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ndhwc),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc),
            name=""conv3d_ndhwc.cuda"",
            plevel=10,
        )
        N, _, _, _, _ = get_const_tuple(data.shape)
        _, _, _, CI, CO = get_const_tuple(kernel.shape)
        if target.kind.name == ""cuda"":
            if nvcc.have_tensorcore(target=target):
                if (
                    (N % 16 == 0 and CI % 16 == 0 and CO % 16 == 0)
                    or (N % 8 == 0 and CI % 16 == 0 and CO % 32 == 0)
                    or (N % 32 == 0 and CI % 16 == 0 and CO % 8 == 0)
                ) and out_type == ""float16"":
                    strategy.add_implementation(
                        wrap_compute_conv3d(topi.cuda.conv3d_ndhwc_tensorcore),
                        wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc_tensorcore),
                        name=""conv3d_ndhwc_tensorcore.cuda"",
                        plevel=20,
                    )

    if target.kind.name == ""cuda"" and ""cudnn"" in target.libs:
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_cudnn, True),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_cudnn),
            name=""conv3d_cudnn.cuda"",
            plevel=25,
        )
    return strategy",CI % 16 == 0,not CI % 16
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/strategy/cuda.py,,conv3d_strategy_cuda$716,"def conv3d_strategy_cuda(attrs, inputs, out_type, target):
    """"""conv3d cuda strategy""""""
    strategy = _op.OpStrategy()
    data, kernel = inputs
    layout = attrs.data_layout
    _, stride_h, stride_w = attrs.get_int_tuple(""strides"")
    _, dilation_h, dilation_w = attrs.get_int_tuple(""dilation"")
    assert layout in [""NCDHW"", ""NDHWC""], ""Not support this layout {} yet"".format(layout)
    if layout == ""NCDHW"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ncdhw),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw),
            name=""conv3d_ncdhw.cuda"",
            plevel=10,
        )
        _, _, _, kh, kw = get_const_tuple(kernel.shape)
        if (
            2 < kh < 8
            and 2 < kw < 8
            and kh == kw
            and stride_h == 1
            and stride_w == 1
            and dilation_h == 1
            and dilation_w == 1
            and attrs[""groups""] == 1
        ):
            strategy.add_implementation(
                wrap_compute_conv3d(topi.cuda.conv3d_ncdhw_winograd),
                wrap_topi_schedule(topi.cuda.schedule_conv3d_ncdhw_winograd),
                name=""conv3d_ncdhw_winograd.cuda"",
                plevel=5,
            )
    else:  # layout == ""NDHWC"":
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_ndhwc),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc),
            name=""conv3d_ndhwc.cuda"",
            plevel=10,
        )
        N, _, _, _, _ = get_const_tuple(data.shape)
        _, _, _, CI, CO = get_const_tuple(kernel.shape)
        if target.kind.name == ""cuda"":
            if nvcc.have_tensorcore(target=target):
                if (
                    (N % 16 == 0 and CI % 16 == 0 and CO % 16 == 0)
                    or (N % 8 == 0 and CI % 16 == 0 and CO % 32 == 0)
                    or (N % 32 == 0 and CI % 16 == 0 and CO % 8 == 0)
                ) and out_type == ""float16"":
                    strategy.add_implementation(
                        wrap_compute_conv3d(topi.cuda.conv3d_ndhwc_tensorcore),
                        wrap_topi_schedule(topi.cuda.schedule_conv3d_ndhwc_tensorcore),
                        name=""conv3d_ndhwc_tensorcore.cuda"",
                        plevel=20,
                    )

    if target.kind.name == ""cuda"" and ""cudnn"" in target.libs:
        strategy.add_implementation(
            wrap_compute_conv3d(topi.cuda.conv3d_cudnn, True),
            wrap_topi_schedule(topi.cuda.schedule_conv3d_cudnn),
            name=""conv3d_cudnn.cuda"",
            plevel=25,
        )
    return strategy",CO % 8 == 0,not CO % 8
aster,https://github.com/bgshih/aster/tree/master/meta_architectures/multi_predictors_recognition_model.py,MultiPredictorsRecognitionModel,provide_groundtruth$97,"def provide_groundtruth(self, groundtruth_lists, scope=None):
    with tf.variable_scope(scope, 'ProvideGroundtruth', list(groundtruth_lists.values())):
      # provide groundtruth_text to all predictors
      groundtruth_text = tf.stack(
        groundtruth_lists[fields.InputDataFields.groundtruth_text], axis=0)
      for name, predictor in self._predictors_dict.items():
        predictor.provide_groundtruth(
          groundtruth_text,
          scope='{}/ProvideGroundtruth'.format(name))

      # provide groundtruth keypoints
      if self._spatial_transformer is not None and self._keypoint_supervision == True:
        groundtruth_keypoints_lengths = tf.stack([
          tf.shape(keypoints)[0] for keypoints in
          groundtruth_lists[fields.InputDataFields.groundtruth_keypoints]
        ])
        max_keypoints_length = tf.reduce_max(groundtruth_keypoints_lengths)
        has_groundtruth_keypoints = tf.equal(groundtruth_keypoints_lengths, max_keypoints_length)
        groundtruth_keypoints = tf.stack([
          tf.pad(keypoints, [[0, max_keypoints_length - tf.shape(keypoints)[0]]])
          for keypoints in groundtruth_lists[fields.InputDataFields.groundtruth_keypoints]
        ])
        groundtruth_keypoints = tf.boolean_mask(groundtruth_keypoints, has_groundtruth_keypoints)
        groundtruth_control_points = ops.divide_curve(
          groundtruth_keypoints,
          num_key_points=self._spatial_transformer._num_control_points)
        self._groundtruth_dict['control_points_mask'] = has_groundtruth_keypoints
        self._groundtruth_dict['control_points'] = groundtruth_control_points",self._keypoint_supervision == True,self._keypoint_supervision
pymc,https://github.com/pymc-devs/pymc/tree/master/pymc/backends/base.py,,merge_traces$565,"def merge_traces(mtraces: List[MultiTrace]) -> MultiTrace:
    """"""Merge MultiTrace objects.

    Parameters
    ----------
    mtraces: list of MultiTraces
        Each instance should have unique chain numbers.

    Raises
    ------
    A ValueError is raised if any traces have overlapping chain numbers,
    or if chains are of different lengths.

    Returns
    -------
    A MultiTrace instance with merged chains
    """"""
    if len(mtraces) == 0:
        raise ValueError(""Cannot merge an empty set of traces."")
    base_mtrace = mtraces[0]
    chain_len = len(base_mtrace)
    # check base trace
    if any(
        len(st) != chain_len for _, st in base_mtrace._straces.items()
    ):  # pylint: disable=line-too-long
        raise ValueError(""Chains are of different lengths."")
    for new_mtrace in mtraces[1:]:
        for new_chain, strace in new_mtrace._straces.items():
            if new_chain in base_mtrace._straces:
                raise ValueError(""Chains are not unique."")
            if len(strace) != chain_len:
                raise ValueError(""Chains are of different lengths."")
            base_mtrace._straces[new_chain] = strace
    base_mtrace._report = merge_reports([trace.report for trace in mtraces])
    return base_mtrace",len(mtraces) == 0,not mtraces
openpilot,https://github.com/commaai/openpilot/tree/master/selfdrive/loggerd/uploader.py,Uploader,upload$186,"def upload(self, key, fn):
    try:
      sz = os.path.getsize(fn)
    except OSError:
      cloudlog.exception(""upload: getsize failed"")
      return False

    cloudlog.event(""upload"", key=key, fn=fn, sz=sz)

    cloudlog.debug(""checking %r with size %r"", key, sz)

    if sz == 0:
      try:
        # tag files of 0 size as uploaded
        setxattr(fn, UPLOAD_ATTR_NAME, UPLOAD_ATTR_VALUE)
      except OSError:
        cloudlog.event(""uploader_setxattr_failed"", exc=self.last_exc, key=key, fn=fn, sz=sz)
      success = True
    else:
      start_time = time.monotonic()
      cloudlog.debug(""uploading %r"", fn)
      stat = self.normal_upload(key, fn)
      if stat is not None and stat.status_code in (200, 201, 403, 412):
        cloudlog.event(""upload_success"" if stat.status_code != 412 else ""upload_ignored"", key=key, fn=fn, sz=sz, debug=True)
        try:
          # tag file as uploaded
          setxattr(fn, UPLOAD_ATTR_NAME, UPLOAD_ATTR_VALUE)
        except OSError:
          cloudlog.event(""uploader_setxattr_failed"", exc=self.last_exc, key=key, fn=fn, sz=sz)

        self.last_filename = fn
        self.last_time = time.monotonic() - start_time
        self.last_speed = (sz / 1e6) / self.last_time
        success = True
      else:
        cloudlog.event(""upload_failed"", stat=stat, exc=self.last_exc, key=key, fn=fn, sz=sz, debug=True)
        success = False

    return success",sz == 0,not sz
gpiozero,https://github.com/gpiozero/gpiozero/tree/master/tests/test_tools.py,,test_sin_values$294,"def test_sin_values():
    for e, v in zip([0, 0], sin_values(2)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    for e, v in zip([0, 1, 0, -1], sin_values(4)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    for e, v in zip([0, 2**0.5/2, 1, 2**0.5/2, 0, -2**0.5/2, -1, -2**0.5/2], sin_values(8)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    firstval = None
    for i, v in zip(range(1000), sin_values()):
        assert -1 <= v <= 1
        assert isclose(v, sin(radians(i)), abs_tol=1e-9)
        if i == 0:
            firstval = v
        else:
            if i % 360 == 0:
                assert v == firstval
    for period in (360, 100):
        firstval = None
        for i, v in zip(range(1000), sin_values(period)):
            assert -1 <= v <= 1
            if i == 0:
                firstval = v
            else:
                if i % period == 0:
                    assert v == firstval",i == 0,not i
gpiozero,https://github.com/gpiozero/gpiozero/tree/master/tests/test_tools.py,,test_sin_values$294,"def test_sin_values():
    for e, v in zip([0, 0], sin_values(2)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    for e, v in zip([0, 1, 0, -1], sin_values(4)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    for e, v in zip([0, 2**0.5/2, 1, 2**0.5/2, 0, -2**0.5/2, -1, -2**0.5/2], sin_values(8)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    firstval = None
    for i, v in zip(range(1000), sin_values()):
        assert -1 <= v <= 1
        assert isclose(v, sin(radians(i)), abs_tol=1e-9)
        if i == 0:
            firstval = v
        else:
            if i % 360 == 0:
                assert v == firstval
    for period in (360, 100):
        firstval = None
        for i, v in zip(range(1000), sin_values(period)):
            assert -1 <= v <= 1
            if i == 0:
                firstval = v
            else:
                if i % period == 0:
                    assert v == firstval",i % 360 == 0,not i % 360
gpiozero,https://github.com/gpiozero/gpiozero/tree/master/tests/test_tools.py,,test_sin_values$294,"def test_sin_values():
    for e, v in zip([0, 0], sin_values(2)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    for e, v in zip([0, 1, 0, -1], sin_values(4)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    for e, v in zip([0, 2**0.5/2, 1, 2**0.5/2, 0, -2**0.5/2, -1, -2**0.5/2], sin_values(8)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    firstval = None
    for i, v in zip(range(1000), sin_values()):
        assert -1 <= v <= 1
        assert isclose(v, sin(radians(i)), abs_tol=1e-9)
        if i == 0:
            firstval = v
        else:
            if i % 360 == 0:
                assert v == firstval
    for period in (360, 100):
        firstval = None
        for i, v in zip(range(1000), sin_values(period)):
            assert -1 <= v <= 1
            if i == 0:
                firstval = v
            else:
                if i % period == 0:
                    assert v == firstval",i == 0,not i
gpiozero,https://github.com/gpiozero/gpiozero/tree/master/tests/test_tools.py,,test_sin_values$294,"def test_sin_values():
    for e, v in zip([0, 0], sin_values(2)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    for e, v in zip([0, 1, 0, -1], sin_values(4)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    for e, v in zip([0, 2**0.5/2, 1, 2**0.5/2, 0, -2**0.5/2, -1, -2**0.5/2], sin_values(8)):
        assert -1 <= v <= 1
        assert isclose(e, v, abs_tol=1e-9)
    firstval = None
    for i, v in zip(range(1000), sin_values()):
        assert -1 <= v <= 1
        assert isclose(v, sin(radians(i)), abs_tol=1e-9)
        if i == 0:
            firstval = v
        else:
            if i % 360 == 0:
                assert v == firstval
    for period in (360, 100):
        firstval = None
        for i, v in zip(range(1000), sin_values(period)):
            assert -1 <= v <= 1
            if i == 0:
                firstval = v
            else:
                if i % period == 0:
                    assert v == firstval",i % period == 0,not i % period
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/marian/modeling_tf_marian.py,TFMarianSinusoidalPositionalEmbedding,__init__$123,"def __init__(self, num_positions: int, embedding_dim: int, **kwargs):
        super().__init__(**kwargs)

        if embedding_dim % 2 != 0:
            raise NotImplementedError(f""odd embedding_dim {embedding_dim} not supported"")

        self.embedding_dim = embedding_dim
        self.num_positions = num_positions",embedding_dim % 2 != 0,embedding_dim % 2
Lasagne,https://github.com/Lasagne/Lasagne/tree/master/lasagne/tests/layers/test_helper.py,TestGetOutput_MergeLayer,test_get_output_input_is_a_mapping_for_layer$381,"def test_get_output_input_is_a_mapping_for_layer(self, layers, get_output):
        l1, l2, l3 = layers
        p = PropertyMock()
        type(l1[0]).input_var = p
        input_expr, kwarg = theano.tensor.matrix(), object()
        inputs = {l2[0]: input_expr}
        output = get_output(l3, inputs, kwarg=kwarg)
        # expected: l3.get_output_for([input_expr,
        #                              l2[1].get_output_for(l1[1].input_var,
        #                                                   kwarg=kwarg)],
        #                              kwarg=kwarg)
        assert output is l3.get_output_for.return_value
        l3.get_output_for.assert_called_with([
            input_expr,
            l2[1].get_output_for.return_value,
            ], kwarg=kwarg)
        l2[1].get_output_for.assert_called_with(
            l1[1].input_var, kwarg=kwarg)
        # l2[0].get_output_for should not have been called
        assert l2[0].get_output_for.call_count == 0
        # l1[0].input_var should not have been accessed
        assert p.call_count == 0",l2[0].get_output_for.call_count == 0,not l2[0].get_output_for.call_count
Lasagne,https://github.com/Lasagne/Lasagne/tree/master/lasagne/tests/layers/test_helper.py,TestGetOutput_MergeLayer,test_get_output_input_is_a_mapping_for_layer$381,"def test_get_output_input_is_a_mapping_for_layer(self, layers, get_output):
        l1, l2, l3 = layers
        p = PropertyMock()
        type(l1[0]).input_var = p
        input_expr, kwarg = theano.tensor.matrix(), object()
        inputs = {l2[0]: input_expr}
        output = get_output(l3, inputs, kwarg=kwarg)
        # expected: l3.get_output_for([input_expr,
        #                              l2[1].get_output_for(l1[1].input_var,
        #                                                   kwarg=kwarg)],
        #                              kwarg=kwarg)
        assert output is l3.get_output_for.return_value
        l3.get_output_for.assert_called_with([
            input_expr,
            l2[1].get_output_for.return_value,
            ], kwarg=kwarg)
        l2[1].get_output_for.assert_called_with(
            l1[1].input_var, kwarg=kwarg)
        # l2[0].get_output_for should not have been called
        assert l2[0].get_output_for.call_count == 0
        # l1[0].input_var should not have been accessed
        assert p.call_count == 0",p.call_count == 0,not p.call_count
data-science-competition,https://github.com/DLLXW/data-science-competition/tree/master/else/澶╅┈鏉--AI+z鏅鸿兘璐ㄦ/code/run_mt_classifier.py,,main$88,"def main():
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    # Path options.
    parser.add_argument(""--pretrained_model_path"", default=None, type=str,
                        help=""Path of the pretrained model."")
    parser.add_argument(""--dataset_path_list"", default=[], nargs='+', type=str, help=""Dataset path list."")
    parser.add_argument(""--output_model_path"", default=""./models/multitask_classifier_model.bin"", type=str,
                        help=""Path of the output model."")
    parser.add_argument(""--vocab_path"", default=None, type=str,
                        help=""Path of the vocabulary file."")
    parser.add_argument(""--spm_model_path"", default=None, type=str,
                        help=""Path of the sentence piece model."")    
    parser.add_argument(""--config_path"", default=""./models/bert_base_config.json"", type=str,
                        help=""Path of the config file."")

    # Model options.
    parser.add_argument(""--batch_size"", type=int, default=32,
                        help=""Batch size."")
    parser.add_argument(""--seq_length"", type=int, default=128,
                        help=""Sequence length."")
    parser.add_argument(""--embedding"", choices=[""bert"", ""word""], default=""bert"",
                        help=""Emebdding type."")
    parser.add_argument(""--encoder"", choices=[""bert"", ""lstm"", ""gru"", \
                                              ""cnn"", ""gatedcnn"", ""attn"", ""synt"", \
                                              ""rcnn"", ""crnn"", ""gpt"", ""bilstm""], \
                                              default=""bert"", help=""Encoder type."")
    parser.add_argument(""--bidirectional"", action=""store_true"", help=""Specific to recurrent model."")
    parser.add_argument(""--pooling"", choices=[""mean"", ""max"", ""first"", ""last""], default=""first"",
                        help=""Pooling type."")
    parser.add_argument(""--factorized_embedding_parameterization"", action=""store_true"", help=""Factorized embedding parameterization."")
    parser.add_argument(""--parameter_sharing"", action=""store_true"", help=""Parameter sharing."")
    
    # Tokenizer options.
    parser.add_argument(""--tokenizer"", choices=[""bert"", ""char"", ""space""], default=""bert"",
                        help=""Specify the tokenizer."" 
                             ""Original Google BERT uses bert tokenizer on Chinese corpus.""
                             ""Char tokenizer segments sentences into characters.""
                             ""Space tokenizer segments sentences into words according to space.""
                             )

    # Optimizer options.
    parser.add_argument(""--soft_targets"", action='store_true',
                        help=""Train model with logits."")
    parser.add_argument(""--learning_rate"", type=float, default=2e-5,
                        help=""Learning rate."")
    parser.add_argument(""--warmup"", type=float, default=0.1,
                        help=""Warm up value."")
    parser.add_argument(""--fp16"", action='store_true',
                        help=""Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit."")
    parser.add_argument(""--fp16_opt_level"", choices=[""O0"", ""O1"", ""O2"", ""O3"" ], default='O1',
                        help=""For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].""
                             ""See details at https://nvidia.github.io/apex/amp.html"")

    # Training options.
    parser.add_argument(""--dropout"", type=float, default=0.5,
                        help=""Dropout."")
    parser.add_argument(""--epochs_num"", type=int, default=3,
                        help=""Number of epochs."")
    parser.add_argument(""--report_steps"", type=int, default=100,
                        help=""Specific steps to print prompt."")
    parser.add_argument(""--seed"", type=int, default=7,
                        help=""Random seed."")
    
    args = parser.parse_args()

    # Load the hyperparameters from the config file.
    args = load_hyperparam(args)

    set_seed(args.seed)

    # Count the number of labels. 
    args.labels_num_list = [count_labels_num(os.path.join(path, ""train.tsv"")) for path in args.dataset_path_list]

    args.datasets_num = len(args.dataset_path_list)

    # Build tokenizer.
    args.tokenizer = globals()[args.tokenizer.capitalize() + ""Tokenizer""](args)
    
    # Build multi-task classification model.
    model = MultitaskClassifier(args)

    # Load or initialize parameters.
    load_or_initialize_parameters(args, model)

    args.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
    model = model.to(args.device)
    args.model = model
     


    # Training phase.
    dataset_list = [read_dataset(args, os.path.join(path, ""train.tsv"")) for path in args.dataset_path_list]
    packed_dataset_list = [pack_dataset(dataset, i, args.batch_size) for i, dataset in enumerate(dataset_list)] 
    
    packed_dataset_all = []
    for packed_dataset in packed_dataset_list:
        packed_dataset_all += packed_dataset

    random.shuffle(packed_dataset_all)    
    instances_num = sum([len(dataset) for dataset in dataset_list])
    batch_size = args.batch_size

    args.train_steps = int(instances_num * args.epochs_num / batch_size) + 1

    print(""Batch size: "", batch_size)
    print(""The number of training instances:"", instances_num)

    optimizer, scheduler = build_optimizer(args, model)
    
    if args.fp16:
        try:
            from apex import amp
        except ImportError:
            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")
        model, optimizer = amp.initialize(model, optimizer,opt_level = args.fp16_opt_level)
        args.amp = amp

    if torch.cuda.device_count() > 1:
        print(""{} GPUs are available. Let's use them."".format(torch.cuda.device_count()))
        model = torch.nn.DataParallel(model)

    total_loss, result, best_result = 0., 0., 0.
    
    print(""Start training."")

    for epoch in range(1, args.epochs_num+1):
        model.train()
        for i, (dataset_id, src_batch, tgt_batch, seg_batch) in enumerate(packed_dataset_all):
            if hasattr(model, ""module""):
                model.module.change_dataset(dataset_id)
            else:
                model.change_dataset(dataset_id)
            loss = train_model(args, model, optimizer, scheduler, src_batch, tgt_batch, seg_batch, None)
            total_loss += loss.item()
            if (i + 1) % args.report_steps == 0:
                print(""Epoch id: {}, Training steps: {}, Avg loss: {:.3f}"".format(epoch, i+1, total_loss / args.report_steps))
                total_loss = 0.

        for dataset_id, path in enumerate(args.dataset_path_list):
            args.labels_num = args.labels_num_list[dataset_id]
            if hasattr(model, ""module""):
                model.module.change_dataset(dataset_id)
            else:
                model.change_dataset(dataset_id)
            result = evaluate(args, read_dataset(args, os.path.join(path, ""dev.tsv"")))

    save_model(model, args.output_model_path)",(i + 1) % args.report_steps == 0,not (i + 1) % args.report_steps
PettingZoo,https://github.com/Farama-Foundation/PettingZoo/tree/master/pettingzoo/classic/rps/rps.py,raw_env,step$440,"def step(self, action):
        if (
            self.terminations[self.agent_selection]
            or self.truncations[self.agent_selection]
        ):
            self._was_dead_step(action)
            return

        agent = self.agent_selection

        self.state[self.agent_selection] = action

        # collect reward if it is the last agent to act
        if self._agent_selector.is_last():

            # same action => 0 reward each agent
            if self.state[self.agents[0]] == self.state[self.agents[1]]:
                rewards = (0, 0)
            else:
                # same action parity => lower action number wins
                if (self.state[self.agents[0]] + self.state[self.agents[1]]) % 2 == 0:
                    if self.state[self.agents[0]] > self.state[self.agents[1]]:
                        rewards = (-1, 1)
                    else:
                        rewards = (1, -1)
                # different action parity => higher action number wins
                else:
                    if self.state[self.agents[0]] > self.state[self.agents[1]]:
                        rewards = (1, -1)
                    else:
                        rewards = (-1, 1)
            self.rewards[self.agents[0]], self.rewards[self.agents[1]] = rewards

            self.num_moves += 1

            self.truncations = {
                agent: self.num_moves >= self.max_cycles for agent in self.agents
            }
            for i in self.agents:
                self.observations[i] = self.state[
                    self.agents[1 - self.agent_name_mapping[i]]
                ]

            # record history by pushing back
            self.history[2:] = self.history[:-2]
            self.history[0] = self.state[self.agents[0]]
            self.history[1] = self.state[self.agents[1]]

        else:
            self.state[self.agents[1 - self.agent_name_mapping[agent]]] = self._none

            self._clear_rewards()

        self._cumulative_rewards[self.agent_selection] = 0
        self.agent_selection = self._agent_selector.next()
        self._accumulate_rewards()

        if self.render_mode == ""human"":
            self.render()",(self.state[self.agents[0]] + self.state[self.agents[1]]) % 2 == 0,not (self.state[self.agents[0]] + self.state[self.agents[1]]) % 2
netzob,https://github.com/netzob/netzob/tree/master/netzob/src/netzob/Model/Vocabulary/Domain/Variables/Nodes/Alt.py,Alt,specialize$145,"def specialize(self, specializingPath):
        """"""Specializes an Alt""""""

        if specializingPath is None:
            raise Exception(""SpecializingPath cannot be None"")

        if len(self.children) == 0:
            raise Exception(""Cannot specialize ALT if its has no children"")

        specializingPaths = []

        # parse each child according to its definition
        for i_child, child in enumerate(self.children):
            newSpecializingPath = specializingPath.duplicate()
            self._logger.debug(""ALT Specialize of {0}/{1} with {2}"".format(
                i_child + 1, len(self.children), newSpecializingPath))

            childSpecializingPaths = child.specialize(newSpecializingPath)
            if len(childSpecializingPaths) == 0:
                self._logger.debug(""Path {0} on child {1} didn't succeed."".
                                   format(newSpecializingPath, child))
            else:
                self._logger.debug(""Path {0} on child {1} succeed."".format(
                    newSpecializingPath, child))
                for childSpecializingPath in childSpecializingPaths:
                    childSpecializingPath.addResult(
                        self,
                        childSpecializingPath.getDataAssignedToVariable(child))

                specializingPaths.extend(childSpecializingPaths)

        if len(specializingPaths) == 0:
            self._logger.debug(
                ""No children of {0} successfuly specialized"".format(self))

        # lets shuffle this ( :) ) >>> by default we only consider the first valid parsing path.
        random.shuffle(specializingPaths)
        return specializingPaths",len(self.children) == 0,not self.children
netzob,https://github.com/netzob/netzob/tree/master/netzob/src/netzob/Model/Vocabulary/Domain/Variables/Nodes/Alt.py,Alt,specialize$145,"def specialize(self, specializingPath):
        """"""Specializes an Alt""""""

        if specializingPath is None:
            raise Exception(""SpecializingPath cannot be None"")

        if len(self.children) == 0:
            raise Exception(""Cannot specialize ALT if its has no children"")

        specializingPaths = []

        # parse each child according to its definition
        for i_child, child in enumerate(self.children):
            newSpecializingPath = specializingPath.duplicate()
            self._logger.debug(""ALT Specialize of {0}/{1} with {2}"".format(
                i_child + 1, len(self.children), newSpecializingPath))

            childSpecializingPaths = child.specialize(newSpecializingPath)
            if len(childSpecializingPaths) == 0:
                self._logger.debug(""Path {0} on child {1} didn't succeed."".
                                   format(newSpecializingPath, child))
            else:
                self._logger.debug(""Path {0} on child {1} succeed."".format(
                    newSpecializingPath, child))
                for childSpecializingPath in childSpecializingPaths:
                    childSpecializingPath.addResult(
                        self,
                        childSpecializingPath.getDataAssignedToVariable(child))

                specializingPaths.extend(childSpecializingPaths)

        if len(specializingPaths) == 0:
            self._logger.debug(
                ""No children of {0} successfuly specialized"".format(self))

        # lets shuffle this ( :) ) >>> by default we only consider the first valid parsing path.
        random.shuffle(specializingPaths)
        return specializingPaths",len(specializingPaths) == 0,not specializingPaths
netzob,https://github.com/netzob/netzob/tree/master/netzob/src/netzob/Model/Vocabulary/Domain/Variables/Nodes/Alt.py,Alt,specialize$145,"def specialize(self, specializingPath):
        """"""Specializes an Alt""""""

        if specializingPath is None:
            raise Exception(""SpecializingPath cannot be None"")

        if len(self.children) == 0:
            raise Exception(""Cannot specialize ALT if its has no children"")

        specializingPaths = []

        # parse each child according to its definition
        for i_child, child in enumerate(self.children):
            newSpecializingPath = specializingPath.duplicate()
            self._logger.debug(""ALT Specialize of {0}/{1} with {2}"".format(
                i_child + 1, len(self.children), newSpecializingPath))

            childSpecializingPaths = child.specialize(newSpecializingPath)
            if len(childSpecializingPaths) == 0:
                self._logger.debug(""Path {0} on child {1} didn't succeed."".
                                   format(newSpecializingPath, child))
            else:
                self._logger.debug(""Path {0} on child {1} succeed."".format(
                    newSpecializingPath, child))
                for childSpecializingPath in childSpecializingPaths:
                    childSpecializingPath.addResult(
                        self,
                        childSpecializingPath.getDataAssignedToVariable(child))

                specializingPaths.extend(childSpecializingPaths)

        if len(specializingPaths) == 0:
            self._logger.debug(
                ""No children of {0} successfuly specialized"".format(self))

        # lets shuffle this ( :) ) >>> by default we only consider the first valid parsing path.
        random.shuffle(specializingPaths)
        return specializingPaths",len(childSpecializingPaths) == 0,not childSpecializingPaths
fuzzer,https://github.com/shellphish/fuzzer/tree/master/fuzzer/fuzzer.py,Fuzzer,_create_dict$488,"def _create_dict(self, dict_file):

        l.warning(""creating a dictionary of string references within binary \""%s\"""",
                self.binary_id)

        args = [sys.executable, self.create_dict_path]
        args += self.binary_path if self.is_multicb else [self.binary_path]

        with open(dict_file, ""wb"") as dfp:
            p = subprocess.Popen(args, stdout=dfp)
            retcode = p.wait()

        return retcode == 0 and os.path.getsize(dict_file)",retcode == 0,not retcode
Diamond,https://github.com/python-diamond/Diamond/tree/master/src/collectors/httpd/httpd.py,HttpdCollector,process_config$22,"def process_config(self):
        super(HttpdCollector, self).process_config()
        if 'url' in self.config:
            self.config['urls'].append(self.config['url'])

        self.urls = {}
        if isinstance(self.config['urls'], basestring):
            self.config['urls'] = self.config['urls'].split(',')

        for url in self.config['urls']:
            # Handle the case where there is a trailing comman on the urls list
            if len(url) == 0:
                continue
            if ' ' in url:
                parts = url.split(' ')
                self.urls[parts[0]] = parts[1]
            else:
                self.urls[''] = url",len(url) == 0,not url
pyro,https://github.com/pyro-ppl/pyro/tree/master/tests/integration_tests/test_tracegraph_elbo.py,NormalNormalNormalTests,do_elbo_test$157,"def do_elbo_test(
        self,
        repa1,
        repa2,
        n_steps,
        prec,
        lr,
        use_nn_baseline,
        use_decaying_avg_baseline,
    ):
        logger.info("" - - - - - DO NORMALNORMALNORMAL ELBO TEST - - - - - -"")
        logger.info(
            ""[reparameterized = %s, %s; nn_baseline = %s, decaying_baseline = %s]""
            % (repa1, repa2, use_nn_baseline, use_decaying_avg_baseline)
        )
        pyro.clear_param_store()
        Normal1 = dist.Normal if repa1 else fakes.NonreparameterizedNormal
        Normal2 = dist.Normal if repa2 else fakes.NonreparameterizedNormal

        if use_nn_baseline:

            class VanillaBaselineNN(nn.Module):
                def __init__(self, dim_input, dim_h):
                    super().__init__()
                    self.lin1 = nn.Linear(dim_input, dim_h)
                    self.lin2 = nn.Linear(dim_h, 2)
                    self.sigmoid = nn.Sigmoid()

                def forward(self, x):
                    h = self.sigmoid(self.lin1(x))
                    return self.lin2(h)

            loc_prime_baseline = pyro.module(
                ""loc_prime_baseline"", VanillaBaselineNN(2, 5)
            )
        else:
            loc_prime_baseline = None

        def model():
            with pyro.plate(""plate"", 2):
                loc_latent_prime = pyro.sample(
                    ""loc_latent_prime"", Normal1(self.loc0, torch.pow(self.lam0, -0.5))
                )
                loc_latent = pyro.sample(
                    ""loc_latent"", Normal2(loc_latent_prime, torch.pow(self.lam0, -0.5))
                )
                with pyro.plate(""data"", len(self.data)):
                    pyro.sample(
                        ""obs"",
                        dist.Normal(loc_latent, torch.pow(self.lam, -0.5)).expand_by(
                            self.data.shape[:1]
                        ),
                        obs=self.data,
                    )
            return loc_latent

        # note that the exact posterior is not mean field!
        def guide():
            loc_q = pyro.param(""loc_q"", self.analytic_loc_n.expand(2) + 0.334)
            log_sig_q = pyro.param(
                ""log_sig_q"", self.analytic_log_sig_n.expand(2) - 0.29
            )
            loc_q_prime = pyro.param(""loc_q_prime"", torch.tensor([-0.34, 0.52]))
            kappa_q = pyro.param(""kappa_q"", torch.tensor([0.74]))
            log_sig_q_prime = pyro.param(
                ""log_sig_q_prime"", -0.5 * torch.log(1.2 * self.lam0)
            )
            sig_q, sig_q_prime = torch.exp(log_sig_q), torch.exp(log_sig_q_prime)
            with pyro.plate(""plate"", 2):
                loc_latent = pyro.sample(
                    ""loc_latent"",
                    Normal2(loc_q, sig_q),
                    infer=dict(
                        baseline=dict(
                            use_decaying_avg_baseline=use_decaying_avg_baseline
                        )
                    ),
                )
                pyro.sample(
                    ""loc_latent_prime"",
                    Normal1(
                        kappa_q.expand_as(loc_latent) * loc_latent + loc_q_prime,
                        sig_q_prime,
                    ),
                    infer=dict(
                        baseline=dict(
                            nn_baseline=loc_prime_baseline,
                            nn_baseline_input=loc_latent,
                            use_decaying_avg_baseline=use_decaying_avg_baseline,
                        )
                    ),
                )
                with pyro.plate(""data"", len(self.data)):
                    pass

            return loc_latent

        adam = optim.Adam({""lr"": 0.0015, ""betas"": (0.97, 0.999)})
        svi = SVI(model, guide, adam, loss=TraceGraph_ELBO())

        for k in range(n_steps):
            svi.step()

            loc_error = param_mse(""loc_q"", self.analytic_loc_n)
            log_sig_error = param_mse(""log_sig_q"", self.analytic_log_sig_n)
            loc_prime_error = param_mse(""loc_q_prime"", 0.5 * self.loc0)
            kappa_error = param_mse(""kappa_q"", 0.5 * torch.ones(1))
            log_sig_prime_error = param_mse(
                ""log_sig_q_prime"", -0.5 * torch.log(2.0 * self.lam0)
            )

            if k % 500 == 0:
                logger.debug(""errors:  %.4f, %.4f"" % (loc_error, log_sig_error))
                logger.debug("", %.4f, %.4f"" % (loc_prime_error, log_sig_prime_error))
                logger.debug("", %.4f"" % kappa_error)

        assert_equal(0.0, loc_error, prec=prec)
        assert_equal(0.0, log_sig_error, prec=prec)
        assert_equal(0.0, loc_prime_error, prec=prec)
        assert_equal(0.0, log_sig_prime_error, prec=prec)
        assert_equal(0.0, kappa_error, prec=prec)",k % 500 == 0,not k % 500
cfn-lint,https://github.com/aws-cloudformation/cfn-lint/tree/master/src/cfnlint/rules/resources/properties/AtLeastOne.py,AtLeastOne,check$36,"def check(self, properties, atleastoneprops, path, cfn):
        """"""Check itself""""""
        matches = []

        for atleastoneprop in atleastoneprops:
            for (safe_properties, safe_path) in properties.items_safe(path):
                property_sets = cfn.get_object_without_conditions(
                    safe_properties, atleastoneprop
                )
                for property_set in property_sets:
                    count = 0
                    for prop in atleastoneprop:
                        if prop in property_set[""Object""]:
                            count += 1

                    if count == 0:
                        if property_set[""Scenario""] is None:
                            message = (
                                ""At least one of [{0}] should be specified for {1}""
                            )
                            matches.append(
                                RuleMatch(
                                    path,
                                    message.format(
                                        "", "".join(map(str, atleastoneprop)),
                                        ""/"".join(map(str, safe_path)),
                                    ),
                                )
                            )
                        else:
                            scenario_text = "" and "".join(
                                [
                                    f'when condition ""{k}"" is {v}'
                                    for (k, v) in property_set[""Scenario""].items()
                                ]
                            )
                            message = (
                                ""At least one of [{0}] should be specified {1} at {2}""
                            )
                            matches.append(
                                RuleMatch(
                                    path,
                                    message.format(
                                        "", "".join(map(str, atleastoneprop)),
                                        scenario_text,
                                        ""/"".join(map(str, safe_path)),
                                    ),
                                )
                            )

        return matches",count == 0,not count
dash,https://github.com/plotly/dash/tree/master/components/dash-core-components/tests/integration/loading/test_loading_component.py,,test_ldcp001_loading_component_initialization$6,"def test_ldcp001_loading_component_initialization(dash_dcc):
    lock = Lock()

    app = Dash(__name__)

    app.layout = html.Div(
        [dcc.Loading([html.Div(id=""div-1"")], className=""loading"")], id=""root""
    )

    @app.callback(Output(""div-1"", ""children""), [Input(""root"", ""n_clicks"")])
    def updateDiv(children):
        with lock:
            return ""content""

    with lock:
        dash_dcc.start_server(app)
        dash_dcc.find_element("".loading .dash-spinner"")
        # ensure inner component is also mounted
        dash_dcc.wait_for_text_to_equal(""#div-1"", """")

    dash_dcc.wait_for_text_to_equal(""#div-1"", ""content"")

    assert dash_dcc.get_logs() == []",dash_dcc.get_logs() == [],not dash_dcc.get_logs()
cryptography,https://github.com/pyca/cryptography/tree/master/src/cryptography/hazmat/primitives/serialization/pkcs7.py,PKCS7SignatureBuilder,sign$126,"def sign(
        self,
        encoding: serialization.Encoding,
        options: typing.Iterable[PKCS7Options],
        backend: typing.Any = None,
    ) -> bytes:
        if len(self._signers) == 0:
            raise ValueError(""Must have at least one signer"")
        if self._data is None:
            raise ValueError(""You must add data to sign"")
        options = list(options)
        if not all(isinstance(x, PKCS7Options) for x in options):
            raise ValueError(""options must be from the PKCS7Options enum"")
        if encoding not in (
            serialization.Encoding.PEM,
            serialization.Encoding.DER,
            serialization.Encoding.SMIME,
        ):
            raise ValueError(
                ""Must be PEM, DER, or SMIME from the Encoding enum""
            )

        # Text is a meaningless option unless it is accompanied by
        # DetachedSignature
        if (
            PKCS7Options.Text in options
            and PKCS7Options.DetachedSignature not in options
        ):
            raise ValueError(
                ""When passing the Text option you must also pass ""
                ""DetachedSignature""
            )

        if PKCS7Options.Text in options and encoding in (
            serialization.Encoding.DER,
            serialization.Encoding.PEM,
        ):
            raise ValueError(
                ""The Text option is only available for SMIME serialization""
            )

        # No attributes implies no capabilities so we'll error if you try to
        # pass both.
        if (
            PKCS7Options.NoAttributes in options
            and PKCS7Options.NoCapabilities in options
        ):
            raise ValueError(
                ""NoAttributes is a superset of NoCapabilities. Do not pass ""
                ""both values.""
            )

        return rust_pkcs7.sign_and_serialize(self, encoding, options)",len(self._signers) == 0,not self._signers
TelegramTUI,https://github.com/vtr0n/TelegramTUI/tree/master/telegramtui/src/npyscreen/wgtextbox.py,TextfieldBase,display_value$187,"def display_value(self, value):
        if value == None:
            return ''
        else:
            try:
                str_value = str(value)
            except UnicodeEncodeError:
                str_value = self.safe_string(value)
                return str_value
            except ReferenceError:                
                return "">*ERROR*ERROR*ERROR*<""
            return self.safe_string(str_value)",value == None,not value
hypothesis,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/tests/cover/test_simple_collections.py,,test_can_draw_empty_set_from_unsatisfiable_strategy$164,"def test_can_draw_empty_set_from_unsatisfiable_strategy():
    assert find_any(sets(integers().filter(lambda s: False))) == set()",find_any(sets(integers().filter(lambda s: False))) == set(),not find_any(sets(integers().filter(lambda s: False)))
PaddleSlim,https://github.com/PaddlePaddle/PaddleSlim/tree/master/demo/models/resnet.py,ResNet,net$29,"def net(self, input, class_dim=1000, conv1_name='conv1', fc_name=None):
        layers = self.layers
        prefix_name = self.prefix_name if self.prefix_name is '' else self.prefix_name + '_'
        supported_layers = [34, 50, 101, 152]
        assert layers in supported_layers, \
            ""supported layers are {} but input layer is {}"".format(supported_layers, layers)

        if layers == 34 or layers == 50:
            depth = [3, 4, 6, 3]
        elif layers == 101:
            depth = [3, 4, 23, 3]
        elif layers == 152:
            depth = [3, 8, 36, 3]
        num_filters = [64, 128, 256, 512]

        # TODO(wanghaoshuang@baidu.com):
        # fix name(""conv1"") conflict between student and teacher in distillation.
        conv = self.conv_bn_layer(
            input=input,
            num_filters=64,
            filter_size=7,
            stride=2,
            act='relu',
            name=prefix_name + conv1_name)
        conv = fluid.layers.pool2d(
            input=conv,
            pool_size=3,
            pool_stride=2,
            pool_padding=1,
            pool_type='max')

        if layers >= 50:
            for block in range(len(depth)):
                for i in range(depth[block]):
                    if layers in [101, 152] and block == 2:
                        if i == 0:
                            conv_name = ""res"" + str(block + 2) + ""a""
                        else:
                            conv_name = ""res"" + str(block + 2) + ""b"" + str(i)
                    else:
                        conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    conv_name = prefix_name + conv_name
                    conv = self.bottleneck_block(
                        input=conv,
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        name=conv_name)

            pool = fluid.layers.pool2d(
                input=conv, pool_size=7, pool_type='avg', global_pooling=True)
            stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)
            fc_name = fc_name if fc_name is None else prefix_name + fc_name
            out = paddle.static.nn.fc(
                pool,
                class_dim,
                activation='softmax',
                name=fc_name,
                weight_attr=paddle.ParamAttr(
                    initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))
        else:
            for block in range(len(depth)):
                for i in range(depth[block]):
                    conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    conv_name = prefix_name + conv_name
                    conv = self.basic_block(
                        input=conv,
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        is_first=block == i == 0,
                        name=conv_name)

            pool = fluid.layers.pool2d(
                input=conv, pool_type='avg', global_pooling=True)
            stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)
            fc_name = fc_name if fc_name is None else prefix_name + fc_name
            out = paddle.static.nn.fc(
                pool,
                class_dim,
                name=fc_name,
                weight_attr=paddle.ParamAttr(
                    initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))

        return out",i == 0,not i
PaddleSlim,https://github.com/PaddlePaddle/PaddleSlim/tree/master/demo/models/resnet.py,ResNet,net$29,"def net(self, input, class_dim=1000, conv1_name='conv1', fc_name=None):
        layers = self.layers
        prefix_name = self.prefix_name if self.prefix_name is '' else self.prefix_name + '_'
        supported_layers = [34, 50, 101, 152]
        assert layers in supported_layers, \
            ""supported layers are {} but input layer is {}"".format(supported_layers, layers)

        if layers == 34 or layers == 50:
            depth = [3, 4, 6, 3]
        elif layers == 101:
            depth = [3, 4, 23, 3]
        elif layers == 152:
            depth = [3, 8, 36, 3]
        num_filters = [64, 128, 256, 512]

        # TODO(wanghaoshuang@baidu.com):
        # fix name(""conv1"") conflict between student and teacher in distillation.
        conv = self.conv_bn_layer(
            input=input,
            num_filters=64,
            filter_size=7,
            stride=2,
            act='relu',
            name=prefix_name + conv1_name)
        conv = fluid.layers.pool2d(
            input=conv,
            pool_size=3,
            pool_stride=2,
            pool_padding=1,
            pool_type='max')

        if layers >= 50:
            for block in range(len(depth)):
                for i in range(depth[block]):
                    if layers in [101, 152] and block == 2:
                        if i == 0:
                            conv_name = ""res"" + str(block + 2) + ""a""
                        else:
                            conv_name = ""res"" + str(block + 2) + ""b"" + str(i)
                    else:
                        conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    conv_name = prefix_name + conv_name
                    conv = self.bottleneck_block(
                        input=conv,
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        name=conv_name)

            pool = fluid.layers.pool2d(
                input=conv, pool_size=7, pool_type='avg', global_pooling=True)
            stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)
            fc_name = fc_name if fc_name is None else prefix_name + fc_name
            out = paddle.static.nn.fc(
                pool,
                class_dim,
                activation='softmax',
                name=fc_name,
                weight_attr=paddle.ParamAttr(
                    initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))
        else:
            for block in range(len(depth)):
                for i in range(depth[block]):
                    conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    conv_name = prefix_name + conv_name
                    conv = self.basic_block(
                        input=conv,
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        is_first=block == i == 0,
                        name=conv_name)

            pool = fluid.layers.pool2d(
                input=conv, pool_type='avg', global_pooling=True)
            stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)
            fc_name = fc_name if fc_name is None else prefix_name + fc_name
            out = paddle.static.nn.fc(
                pool,
                class_dim,
                name=fc_name,
                weight_attr=paddle.ParamAttr(
                    initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))

        return out",i == 0,not i
PaddleSlim,https://github.com/PaddlePaddle/PaddleSlim/tree/master/demo/models/resnet.py,ResNet,net$29,"def net(self, input, class_dim=1000, conv1_name='conv1', fc_name=None):
        layers = self.layers
        prefix_name = self.prefix_name if self.prefix_name is '' else self.prefix_name + '_'
        supported_layers = [34, 50, 101, 152]
        assert layers in supported_layers, \
            ""supported layers are {} but input layer is {}"".format(supported_layers, layers)

        if layers == 34 or layers == 50:
            depth = [3, 4, 6, 3]
        elif layers == 101:
            depth = [3, 4, 23, 3]
        elif layers == 152:
            depth = [3, 8, 36, 3]
        num_filters = [64, 128, 256, 512]

        # TODO(wanghaoshuang@baidu.com):
        # fix name(""conv1"") conflict between student and teacher in distillation.
        conv = self.conv_bn_layer(
            input=input,
            num_filters=64,
            filter_size=7,
            stride=2,
            act='relu',
            name=prefix_name + conv1_name)
        conv = fluid.layers.pool2d(
            input=conv,
            pool_size=3,
            pool_stride=2,
            pool_padding=1,
            pool_type='max')

        if layers >= 50:
            for block in range(len(depth)):
                for i in range(depth[block]):
                    if layers in [101, 152] and block == 2:
                        if i == 0:
                            conv_name = ""res"" + str(block + 2) + ""a""
                        else:
                            conv_name = ""res"" + str(block + 2) + ""b"" + str(i)
                    else:
                        conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    conv_name = prefix_name + conv_name
                    conv = self.bottleneck_block(
                        input=conv,
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        name=conv_name)

            pool = fluid.layers.pool2d(
                input=conv, pool_size=7, pool_type='avg', global_pooling=True)
            stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)
            fc_name = fc_name if fc_name is None else prefix_name + fc_name
            out = paddle.static.nn.fc(
                pool,
                class_dim,
                activation='softmax',
                name=fc_name,
                weight_attr=paddle.ParamAttr(
                    initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))
        else:
            for block in range(len(depth)):
                for i in range(depth[block]):
                    conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    conv_name = prefix_name + conv_name
                    conv = self.basic_block(
                        input=conv,
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        is_first=block == i == 0,
                        name=conv_name)

            pool = fluid.layers.pool2d(
                input=conv, pool_type='avg', global_pooling=True)
            stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)
            fc_name = fc_name if fc_name is None else prefix_name + fc_name
            out = paddle.static.nn.fc(
                pool,
                class_dim,
                name=fc_name,
                weight_attr=paddle.ParamAttr(
                    initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))

        return out",block != 0,block
PaddleSlim,https://github.com/PaddlePaddle/PaddleSlim/tree/master/demo/models/resnet.py,ResNet,net$29,"def net(self, input, class_dim=1000, conv1_name='conv1', fc_name=None):
        layers = self.layers
        prefix_name = self.prefix_name if self.prefix_name is '' else self.prefix_name + '_'
        supported_layers = [34, 50, 101, 152]
        assert layers in supported_layers, \
            ""supported layers are {} but input layer is {}"".format(supported_layers, layers)

        if layers == 34 or layers == 50:
            depth = [3, 4, 6, 3]
        elif layers == 101:
            depth = [3, 4, 23, 3]
        elif layers == 152:
            depth = [3, 8, 36, 3]
        num_filters = [64, 128, 256, 512]

        # TODO(wanghaoshuang@baidu.com):
        # fix name(""conv1"") conflict between student and teacher in distillation.
        conv = self.conv_bn_layer(
            input=input,
            num_filters=64,
            filter_size=7,
            stride=2,
            act='relu',
            name=prefix_name + conv1_name)
        conv = fluid.layers.pool2d(
            input=conv,
            pool_size=3,
            pool_stride=2,
            pool_padding=1,
            pool_type='max')

        if layers >= 50:
            for block in range(len(depth)):
                for i in range(depth[block]):
                    if layers in [101, 152] and block == 2:
                        if i == 0:
                            conv_name = ""res"" + str(block + 2) + ""a""
                        else:
                            conv_name = ""res"" + str(block + 2) + ""b"" + str(i)
                    else:
                        conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    conv_name = prefix_name + conv_name
                    conv = self.bottleneck_block(
                        input=conv,
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        name=conv_name)

            pool = fluid.layers.pool2d(
                input=conv, pool_size=7, pool_type='avg', global_pooling=True)
            stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)
            fc_name = fc_name if fc_name is None else prefix_name + fc_name
            out = paddle.static.nn.fc(
                pool,
                class_dim,
                activation='softmax',
                name=fc_name,
                weight_attr=paddle.ParamAttr(
                    initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))
        else:
            for block in range(len(depth)):
                for i in range(depth[block]):
                    conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    conv_name = prefix_name + conv_name
                    conv = self.basic_block(
                        input=conv,
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        is_first=block == i == 0,
                        name=conv_name)

            pool = fluid.layers.pool2d(
                input=conv, pool_type='avg', global_pooling=True)
            stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)
            fc_name = fc_name if fc_name is None else prefix_name + fc_name
            out = paddle.static.nn.fc(
                pool,
                class_dim,
                name=fc_name,
                weight_attr=paddle.ParamAttr(
                    initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))

        return out",i == 0,not i
PaddleSlim,https://github.com/PaddlePaddle/PaddleSlim/tree/master/demo/models/resnet.py,ResNet,net$29,"def net(self, input, class_dim=1000, conv1_name='conv1', fc_name=None):
        layers = self.layers
        prefix_name = self.prefix_name if self.prefix_name is '' else self.prefix_name + '_'
        supported_layers = [34, 50, 101, 152]
        assert layers in supported_layers, \
            ""supported layers are {} but input layer is {}"".format(supported_layers, layers)

        if layers == 34 or layers == 50:
            depth = [3, 4, 6, 3]
        elif layers == 101:
            depth = [3, 4, 23, 3]
        elif layers == 152:
            depth = [3, 8, 36, 3]
        num_filters = [64, 128, 256, 512]

        # TODO(wanghaoshuang@baidu.com):
        # fix name(""conv1"") conflict between student and teacher in distillation.
        conv = self.conv_bn_layer(
            input=input,
            num_filters=64,
            filter_size=7,
            stride=2,
            act='relu',
            name=prefix_name + conv1_name)
        conv = fluid.layers.pool2d(
            input=conv,
            pool_size=3,
            pool_stride=2,
            pool_padding=1,
            pool_type='max')

        if layers >= 50:
            for block in range(len(depth)):
                for i in range(depth[block]):
                    if layers in [101, 152] and block == 2:
                        if i == 0:
                            conv_name = ""res"" + str(block + 2) + ""a""
                        else:
                            conv_name = ""res"" + str(block + 2) + ""b"" + str(i)
                    else:
                        conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    conv_name = prefix_name + conv_name
                    conv = self.bottleneck_block(
                        input=conv,
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        name=conv_name)

            pool = fluid.layers.pool2d(
                input=conv, pool_size=7, pool_type='avg', global_pooling=True)
            stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)
            fc_name = fc_name if fc_name is None else prefix_name + fc_name
            out = paddle.static.nn.fc(
                pool,
                class_dim,
                activation='softmax',
                name=fc_name,
                weight_attr=paddle.ParamAttr(
                    initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))
        else:
            for block in range(len(depth)):
                for i in range(depth[block]):
                    conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    conv_name = prefix_name + conv_name
                    conv = self.basic_block(
                        input=conv,
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        is_first=block == i == 0,
                        name=conv_name)

            pool = fluid.layers.pool2d(
                input=conv, pool_type='avg', global_pooling=True)
            stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)
            fc_name = fc_name if fc_name is None else prefix_name + fc_name
            out = paddle.static.nn.fc(
                pool,
                class_dim,
                name=fc_name,
                weight_attr=paddle.ParamAttr(
                    initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))

        return out",block != 0,block
Keras-YOLOv4,https://github.com/miemie2013/Keras-YOLOv4/tree/master//demo.py,,if_main_my$60,"if __name__ == '__main__':
    cfg = None
    if config_file == 0:
        cfg = YOLOv4_2x_Config()
    elif config_file == 1:
        cfg = PPYOLO_2x_Config()
    elif config_file == 2:
        cfg = PPYOLO_r18vd_Config()


    # 璇诲彇鐨勬ā鍨
    model_path = cfg.test_cfg['model_path']

    # 鏄鍚︾粰鍥剧墖鐢绘嗐
    draw_image = cfg.test_cfg['draw_image']
    draw_thresh = cfg.test_cfg['draw_thresh']

    all_classes = get_classes(cfg.classes_path)
    num_classes = len(all_classes)


    # 鍒涘缓妯″瀷
    Backbone = select_backbone(cfg.backbone_type)
    backbone = Backbone(**cfg.backbone)
    Head = select_head(cfg.head_type)
    cfg.head['drop_block'] = False   # 棰勬祴鏃跺叧闂璂ropBlock锛屼互鑾峰緱涓鑷寸殑鎺ㄧ悊缁撴灉銆
    head = Head(yolo_loss=None, nms_cfg=cfg.nms_cfg, **cfg.head)
    yolo = YOLO(backbone, head)

    x = keras.layers.Input(shape=(None, None, 3), name='x', dtype='float32')
    im_size = keras.layers.Input(shape=(2,), name='im_size', dtype='int32')
    outputs = yolo.get_outputs(x)
    preds = yolo.get_prediction(outputs, im_size)
    predict_model = keras.models.Model(inputs=[x, im_size], outputs=preds)
    predict_model.load_weights(model_path, by_name=True, skip_mismatch=True)
    predict_model.summary(line_length=130)

    _decode = Decode(predict_model, all_classes, use_gpu, cfg, for_test=True)

    if not os.path.exists('images/res/'): os.mkdir('images/res/')
    path_dir = os.listdir('images/test')

    # 璇绘暟鎹鐨勭嚎绋
    test_dic = {}
    thr = threading.Thread(target=read_test_data,
                           args=(path_dir,
                                 _decode,
                                 test_dic))
    thr.start()

    key_list = list(test_dic.keys())
    key_len = len(key_list)
    while key_len == 0:
        time.sleep(0.01)
        key_list = list(test_dic.keys())
        key_len = len(key_list)
    dic = test_dic['%.8d' % 0]
    image = dic['image']
    pimage = dic['pimage']
    im_size = dic['im_size']


    # warm up
    if use_gpu:
        for k in range(20):
            image, boxes, scores, classes = _decode.detect_image(image, pimage, im_size, draw_image=False)


    time_stat = deque(maxlen=20)
    start_time = time.time()
    end_time = time.time()
    num_imgs = len(path_dir)
    start = time.time()
    for k, filename in enumerate(path_dir):
        key_list = list(test_dic.keys())
        key_len = len(key_list)
        while key_len == 0:
            time.sleep(0.01)
            key_list = list(test_dic.keys())
            key_len = len(key_list)
        dic = test_dic.pop('%.8d' % k)
        image = dic['image']
        pimage = dic['pimage']
        im_size = dic['im_size']

        image, boxes, scores, classes = _decode.detect_image(image, pimage, im_size, draw_image, draw_thresh)

        # 浼拌″墿浣欐椂闂
        start_time = end_time
        end_time = time.time()
        time_stat.append(end_time - start_time)
        time_cost = np.mean(time_stat)
        eta_sec = (num_imgs - k) * time_cost
        eta = str(datetime.timedelta(seconds=int(eta_sec)))

        logger.info('Infer iter {}, num_imgs={}, eta={}.'.format(k, num_imgs, eta))
        if draw_image:
            t2 = threading.Thread(target=save_img, args=(filename, image))
            t2.start()
            logger.info(""Detection bbox results save in images/res/{}"".format(filename))
    cost = time.time() - start
    logger.info('total time: {0:.6f}s'.format(cost))
    logger.info('Speed: %.6fs per image,  %.1f FPS.'%((cost / num_imgs), (num_imgs / cost)))",config_file == 0,not config_file
Keras-YOLOv4,https://github.com/miemie2013/Keras-YOLOv4/tree/master//demo.py,,if_main_my$60,"if __name__ == '__main__':
    cfg = None
    if config_file == 0:
        cfg = YOLOv4_2x_Config()
    elif config_file == 1:
        cfg = PPYOLO_2x_Config()
    elif config_file == 2:
        cfg = PPYOLO_r18vd_Config()


    # 璇诲彇鐨勬ā鍨
    model_path = cfg.test_cfg['model_path']

    # 鏄鍚︾粰鍥剧墖鐢绘嗐
    draw_image = cfg.test_cfg['draw_image']
    draw_thresh = cfg.test_cfg['draw_thresh']

    all_classes = get_classes(cfg.classes_path)
    num_classes = len(all_classes)


    # 鍒涘缓妯″瀷
    Backbone = select_backbone(cfg.backbone_type)
    backbone = Backbone(**cfg.backbone)
    Head = select_head(cfg.head_type)
    cfg.head['drop_block'] = False   # 棰勬祴鏃跺叧闂璂ropBlock锛屼互鑾峰緱涓鑷寸殑鎺ㄧ悊缁撴灉銆
    head = Head(yolo_loss=None, nms_cfg=cfg.nms_cfg, **cfg.head)
    yolo = YOLO(backbone, head)

    x = keras.layers.Input(shape=(None, None, 3), name='x', dtype='float32')
    im_size = keras.layers.Input(shape=(2,), name='im_size', dtype='int32')
    outputs = yolo.get_outputs(x)
    preds = yolo.get_prediction(outputs, im_size)
    predict_model = keras.models.Model(inputs=[x, im_size], outputs=preds)
    predict_model.load_weights(model_path, by_name=True, skip_mismatch=True)
    predict_model.summary(line_length=130)

    _decode = Decode(predict_model, all_classes, use_gpu, cfg, for_test=True)

    if not os.path.exists('images/res/'): os.mkdir('images/res/')
    path_dir = os.listdir('images/test')

    # 璇绘暟鎹鐨勭嚎绋
    test_dic = {}
    thr = threading.Thread(target=read_test_data,
                           args=(path_dir,
                                 _decode,
                                 test_dic))
    thr.start()

    key_list = list(test_dic.keys())
    key_len = len(key_list)
    while key_len == 0:
        time.sleep(0.01)
        key_list = list(test_dic.keys())
        key_len = len(key_list)
    dic = test_dic['%.8d' % 0]
    image = dic['image']
    pimage = dic['pimage']
    im_size = dic['im_size']


    # warm up
    if use_gpu:
        for k in range(20):
            image, boxes, scores, classes = _decode.detect_image(image, pimage, im_size, draw_image=False)


    time_stat = deque(maxlen=20)
    start_time = time.time()
    end_time = time.time()
    num_imgs = len(path_dir)
    start = time.time()
    for k, filename in enumerate(path_dir):
        key_list = list(test_dic.keys())
        key_len = len(key_list)
        while key_len == 0:
            time.sleep(0.01)
            key_list = list(test_dic.keys())
            key_len = len(key_list)
        dic = test_dic.pop('%.8d' % k)
        image = dic['image']
        pimage = dic['pimage']
        im_size = dic['im_size']

        image, boxes, scores, classes = _decode.detect_image(image, pimage, im_size, draw_image, draw_thresh)

        # 浼拌″墿浣欐椂闂
        start_time = end_time
        end_time = time.time()
        time_stat.append(end_time - start_time)
        time_cost = np.mean(time_stat)
        eta_sec = (num_imgs - k) * time_cost
        eta = str(datetime.timedelta(seconds=int(eta_sec)))

        logger.info('Infer iter {}, num_imgs={}, eta={}.'.format(k, num_imgs, eta))
        if draw_image:
            t2 = threading.Thread(target=save_img, args=(filename, image))
            t2.start()
            logger.info(""Detection bbox results save in images/res/{}"".format(filename))
    cost = time.time() - start
    logger.info('total time: {0:.6f}s'.format(cost))
    logger.info('Speed: %.6fs per image,  %.1f FPS.'%((cost / num_imgs), (num_imgs / cost)))",key_len == 0,not key_len
Keras-YOLOv4,https://github.com/miemie2013/Keras-YOLOv4/tree/master//demo.py,,if_main_my$60,"if __name__ == '__main__':
    cfg = None
    if config_file == 0:
        cfg = YOLOv4_2x_Config()
    elif config_file == 1:
        cfg = PPYOLO_2x_Config()
    elif config_file == 2:
        cfg = PPYOLO_r18vd_Config()


    # 璇诲彇鐨勬ā鍨
    model_path = cfg.test_cfg['model_path']

    # 鏄鍚︾粰鍥剧墖鐢绘嗐
    draw_image = cfg.test_cfg['draw_image']
    draw_thresh = cfg.test_cfg['draw_thresh']

    all_classes = get_classes(cfg.classes_path)
    num_classes = len(all_classes)


    # 鍒涘缓妯″瀷
    Backbone = select_backbone(cfg.backbone_type)
    backbone = Backbone(**cfg.backbone)
    Head = select_head(cfg.head_type)
    cfg.head['drop_block'] = False   # 棰勬祴鏃跺叧闂璂ropBlock锛屼互鑾峰緱涓鑷寸殑鎺ㄧ悊缁撴灉銆
    head = Head(yolo_loss=None, nms_cfg=cfg.nms_cfg, **cfg.head)
    yolo = YOLO(backbone, head)

    x = keras.layers.Input(shape=(None, None, 3), name='x', dtype='float32')
    im_size = keras.layers.Input(shape=(2,), name='im_size', dtype='int32')
    outputs = yolo.get_outputs(x)
    preds = yolo.get_prediction(outputs, im_size)
    predict_model = keras.models.Model(inputs=[x, im_size], outputs=preds)
    predict_model.load_weights(model_path, by_name=True, skip_mismatch=True)
    predict_model.summary(line_length=130)

    _decode = Decode(predict_model, all_classes, use_gpu, cfg, for_test=True)

    if not os.path.exists('images/res/'): os.mkdir('images/res/')
    path_dir = os.listdir('images/test')

    # 璇绘暟鎹鐨勭嚎绋
    test_dic = {}
    thr = threading.Thread(target=read_test_data,
                           args=(path_dir,
                                 _decode,
                                 test_dic))
    thr.start()

    key_list = list(test_dic.keys())
    key_len = len(key_list)
    while key_len == 0:
        time.sleep(0.01)
        key_list = list(test_dic.keys())
        key_len = len(key_list)
    dic = test_dic['%.8d' % 0]
    image = dic['image']
    pimage = dic['pimage']
    im_size = dic['im_size']


    # warm up
    if use_gpu:
        for k in range(20):
            image, boxes, scores, classes = _decode.detect_image(image, pimage, im_size, draw_image=False)


    time_stat = deque(maxlen=20)
    start_time = time.time()
    end_time = time.time()
    num_imgs = len(path_dir)
    start = time.time()
    for k, filename in enumerate(path_dir):
        key_list = list(test_dic.keys())
        key_len = len(key_list)
        while key_len == 0:
            time.sleep(0.01)
            key_list = list(test_dic.keys())
            key_len = len(key_list)
        dic = test_dic.pop('%.8d' % k)
        image = dic['image']
        pimage = dic['pimage']
        im_size = dic['im_size']

        image, boxes, scores, classes = _decode.detect_image(image, pimage, im_size, draw_image, draw_thresh)

        # 浼拌″墿浣欐椂闂
        start_time = end_time
        end_time = time.time()
        time_stat.append(end_time - start_time)
        time_cost = np.mean(time_stat)
        eta_sec = (num_imgs - k) * time_cost
        eta = str(datetime.timedelta(seconds=int(eta_sec)))

        logger.info('Infer iter {}, num_imgs={}, eta={}.'.format(k, num_imgs, eta))
        if draw_image:
            t2 = threading.Thread(target=save_img, args=(filename, image))
            t2.start()
            logger.info(""Detection bbox results save in images/res/{}"".format(filename))
    cost = time.time() - start
    logger.info('total time: {0:.6f}s'.format(cost))
    logger.info('Speed: %.6fs per image,  %.1f FPS.'%((cost / num_imgs), (num_imgs / cost)))",key_len == 0,not key_len
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/pyramidnet_cifar.py,,get_pyramidnet_cifar$97,"def get_pyramidnet_cifar(classes,
                         blocks,
                         alpha,
                         bottleneck,
                         model_name=None,
                         pretrained=False,
                         ctx=cpu(),
                         root=os.path.join(""~"", "".mxnet"", ""models""),
                         **kwargs):
    """"""
    Create PyramidNet for CIFAR model with specific parameters.

    Parameters:
    ----------
    classes : int
        Number of classification classes.
    blocks : int
        Number of blocks.
    alpha : int
        PyramidNet's alpha value.
    bottleneck : bool
        Whether to use a bottleneck or simple block in units.
    model_name : str or None, default None
        Model name for loading pretrained model.
    pretrained : bool, default False
        Whether to load the pretrained weights for model.
    ctx : Context, default CPU
        The context in which to load the pretrained weights.
    root : str, default '~/.mxnet/models'
        Location for keeping the model parameters.
    """"""
    assert (classes in [10, 100])

    if bottleneck:
        assert ((blocks - 2) % 9 == 0)
        layers = [(blocks - 2) // 9] * 3
    else:
        assert ((blocks - 2) % 6 == 0)
        layers = [(blocks - 2) // 6] * 3
    init_block_channels = 16

    growth_add = float(alpha) / float(sum(layers))
    from functools import reduce
    channels = reduce(
        lambda xi, yi: xi + [[(i + 1) * growth_add + xi[-1][-1] for i in list(range(yi))]],
        layers,
        [[init_block_channels]])[1:]
    channels = [[int(round(cij)) for cij in ci] for ci in channels]

    if bottleneck:
        channels = [[cij * 4 for cij in ci] for ci in channels]

    net = CIFARPyramidNet(
        channels=channels,
        init_block_channels=init_block_channels,
        bottleneck=bottleneck,
        classes=classes,
        **kwargs)

    if pretrained:
        if (model_name is None) or (not model_name):
            raise ValueError(""Parameter `model_name` should be properly initialized for loading pretrained model."")
        from .model_store import get_model_file
        net.load_parameters(
            filename=get_model_file(
                model_name=model_name,
                local_model_store_dir_path=root),
            ctx=ctx)

    return net",(blocks - 2) % 9 == 0,not (blocks - 2) % 9
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/pyramidnet_cifar.py,,get_pyramidnet_cifar$97,"def get_pyramidnet_cifar(classes,
                         blocks,
                         alpha,
                         bottleneck,
                         model_name=None,
                         pretrained=False,
                         ctx=cpu(),
                         root=os.path.join(""~"", "".mxnet"", ""models""),
                         **kwargs):
    """"""
    Create PyramidNet for CIFAR model with specific parameters.

    Parameters:
    ----------
    classes : int
        Number of classification classes.
    blocks : int
        Number of blocks.
    alpha : int
        PyramidNet's alpha value.
    bottleneck : bool
        Whether to use a bottleneck or simple block in units.
    model_name : str or None, default None
        Model name for loading pretrained model.
    pretrained : bool, default False
        Whether to load the pretrained weights for model.
    ctx : Context, default CPU
        The context in which to load the pretrained weights.
    root : str, default '~/.mxnet/models'
        Location for keeping the model parameters.
    """"""
    assert (classes in [10, 100])

    if bottleneck:
        assert ((blocks - 2) % 9 == 0)
        layers = [(blocks - 2) // 9] * 3
    else:
        assert ((blocks - 2) % 6 == 0)
        layers = [(blocks - 2) // 6] * 3
    init_block_channels = 16

    growth_add = float(alpha) / float(sum(layers))
    from functools import reduce
    channels = reduce(
        lambda xi, yi: xi + [[(i + 1) * growth_add + xi[-1][-1] for i in list(range(yi))]],
        layers,
        [[init_block_channels]])[1:]
    channels = [[int(round(cij)) for cij in ci] for ci in channels]

    if bottleneck:
        channels = [[cij * 4 for cij in ci] for ci in channels]

    net = CIFARPyramidNet(
        channels=channels,
        init_block_channels=init_block_channels,
        bottleneck=bottleneck,
        classes=classes,
        **kwargs)

    if pretrained:
        if (model_name is None) or (not model_name):
            raise ValueError(""Parameter `model_name` should be properly initialized for loading pretrained model."")
        from .model_store import get_model_file
        net.load_parameters(
            filename=get_model_file(
                model_name=model_name,
                local_model_store_dir_path=root),
            ctx=ctx)

    return net",(blocks - 2) % 6 == 0,not (blocks - 2) % 6
DDParser,https://github.com/baidu/DDParser/tree/master/tools/representation/demo/ERNIE/reader/pretraining.py,ErnieDataReader,mixin_negtive_samples$178,"def mixin_negtive_samples(self, pos_sample_generator, buffer=1000):
        """""" 1. generate negtive samples by randomly group sentence_1 and sentence_2 of positive samples
            2. combine negtive samples and positive samples
            
            Args:
                pos_sample_generator: a generator producing a parsed positive sample, which is a list: [token_ids, sent_ids, pos_ids, 1]

            Returns:
                sample: one sample from shuffled positive samples and negtive samples
        """"""
        pos_samples = []
        num_total_miss = 0
        pos_sample_num = 0
        try:
            while True:
                while len(pos_samples) < buffer:
                    pos_sample = next(pos_sample_generator)
                    label = pos_sample[3]
                    assert label == 1, ""positive sample's label must be 1""
                    pos_samples.append(pos_sample)
                    pos_sample_num += 1

                neg_samples, miss_num = self.random_pair_neg_samples(pos_samples)
                num_total_miss += miss_num
                samples = pos_samples + neg_samples
                pos_samples = []
                np.random.shuffle(samples)
                for sample in samples:
                    yield sample
        except StopIteration:
            print(""stopiteration: reach end of file"")
            if len(pos_samples) == 1:
                yield pos_samples[0]
            elif len(pos_samples) == 0:
                yield None
            else:
                neg_samples, miss_num = self.random_pair_neg_samples(pos_samples)
                num_total_miss += miss_num
                samples = pos_samples + neg_samples
                pos_samples = []
                np.random.shuffle(samples)
                for sample in samples:
                    yield sample
            print(""miss_num:%d\tideal_total_sample_num:%d\tmiss_rate:%f"" %
                  (num_total_miss, pos_sample_num * 2, num_total_miss / (pos_sample_num * 2)))",len(pos_samples) == 0,not pos_samples
madmom,https://github.com/CPJKU/madmom/tree/master/madmom/evaluation/beats.py,,continuity$664,"def continuity(detections, annotations,
               phase_tolerance=CONTINUITY_PHASE_TOLERANCE,
               tempo_tolerance=CONTINUITY_TEMPO_TOLERANCE,
               offbeat=True, double=True, triple=True):
    """"""
    Calculate the cmlc, cmlt, amlc and amlt scores for the given detections and
    annotations.

    Parameters
    ----------
    detections : list or numpy array
        Detected beats.
    annotations : list or numpy array
        Annotated beats.
    phase_tolerance : float, optional
        Allowed phase tolerance.
    tempo_tolerance : float, optional
        Allowed tempo tolerance.
    offbeat : bool, optional
        Include offbeat variation.
    double  : bool, optional
        Include double and half tempo variations (and offbeat thereof).
    triple  : bool, optional
        Include triple and third tempo variations (and offbeats thereof).

    Returns
    -------
    cmlc : float
        Tracking accuracy, continuity at the correct metrical level required.
    cmlt : float
        Same as cmlc, continuity at the correct metrical level not required.
    amlc : float
        Same as cmlc, alternate metrical levels allowed.
    amlt : float
        Same as cmlt, alternate metrical levels allowed.

    See Also
    --------
    :func:`cml`

    """"""
    # neither detections nor annotations are given
    if len(detections) == 0 and len(annotations) == 0:
        return 1., 1., 1., 1.
    # either a single beat detections or annotations given, score 0
    if len(detections) <= 1 or len(annotations) <= 1:
        return 0., 0., 0., 0.
    # evaluate the correct tempo
    cmlc, cmlt = cml(detections, annotations, tempo_tolerance, phase_tolerance)
    amlc = cmlc
    amlt = cmlt
    # speed up calculation by skipping other metrical levels if the score is
    # higher than 0.5 already. We must have tested the correct metrical level
    # already, otherwise the cmlc score would be lower.
    if cmlc > 0.5:
        return cmlc, cmlt, amlc, amlt
    # create different variants of the annotations:
    # Note: double also includes half as does triple third, respectively
    sequences = variations(annotations, offbeat=offbeat, double=double,
                           half=double, triple=triple, third=triple)
    # evaluate these metrical variants
    for sequence in sequences:
        # if other metrical levels achieve higher accuracies, take these values
        try:
            # Note: catch the IntervalError here, because the beat variants
            #       could be too short for valid interval calculation;
            #       ok, since we already have valid values for amlc & amlt
            c, t = cml(detections, sequence, tempo_tolerance, phase_tolerance)
        except BeatIntervalError:
            c, t = np.nan, np.nan
        amlc = max(amlc, c)
        amlt = max(amlt, t)
    # return a tuple
    return cmlc, cmlt, amlc, amlt",len(detections) == 0,not detections
madmom,https://github.com/CPJKU/madmom/tree/master/madmom/evaluation/beats.py,,continuity$664,"def continuity(detections, annotations,
               phase_tolerance=CONTINUITY_PHASE_TOLERANCE,
               tempo_tolerance=CONTINUITY_TEMPO_TOLERANCE,
               offbeat=True, double=True, triple=True):
    """"""
    Calculate the cmlc, cmlt, amlc and amlt scores for the given detections and
    annotations.

    Parameters
    ----------
    detections : list or numpy array
        Detected beats.
    annotations : list or numpy array
        Annotated beats.
    phase_tolerance : float, optional
        Allowed phase tolerance.
    tempo_tolerance : float, optional
        Allowed tempo tolerance.
    offbeat : bool, optional
        Include offbeat variation.
    double  : bool, optional
        Include double and half tempo variations (and offbeat thereof).
    triple  : bool, optional
        Include triple and third tempo variations (and offbeats thereof).

    Returns
    -------
    cmlc : float
        Tracking accuracy, continuity at the correct metrical level required.
    cmlt : float
        Same as cmlc, continuity at the correct metrical level not required.
    amlc : float
        Same as cmlc, alternate metrical levels allowed.
    amlt : float
        Same as cmlt, alternate metrical levels allowed.

    See Also
    --------
    :func:`cml`

    """"""
    # neither detections nor annotations are given
    if len(detections) == 0 and len(annotations) == 0:
        return 1., 1., 1., 1.
    # either a single beat detections or annotations given, score 0
    if len(detections) <= 1 or len(annotations) <= 1:
        return 0., 0., 0., 0.
    # evaluate the correct tempo
    cmlc, cmlt = cml(detections, annotations, tempo_tolerance, phase_tolerance)
    amlc = cmlc
    amlt = cmlt
    # speed up calculation by skipping other metrical levels if the score is
    # higher than 0.5 already. We must have tested the correct metrical level
    # already, otherwise the cmlc score would be lower.
    if cmlc > 0.5:
        return cmlc, cmlt, amlc, amlt
    # create different variants of the annotations:
    # Note: double also includes half as does triple third, respectively
    sequences = variations(annotations, offbeat=offbeat, double=double,
                           half=double, triple=triple, third=triple)
    # evaluate these metrical variants
    for sequence in sequences:
        # if other metrical levels achieve higher accuracies, take these values
        try:
            # Note: catch the IntervalError here, because the beat variants
            #       could be too short for valid interval calculation;
            #       ok, since we already have valid values for amlc & amlt
            c, t = cml(detections, sequence, tempo_tolerance, phase_tolerance)
        except BeatIntervalError:
            c, t = np.nan, np.nan
        amlc = max(amlc, c)
        amlt = max(amlt, t)
    # return a tuple
    return cmlc, cmlt, amlc, amlt",len(annotations) == 0,not annotations
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/metrics/im2latex_images_match.py,,crop_image$91,"def crop_image(img, output_path, default_size=None):
    old_im = cv.imread(img, cv.IMREAD_GRAYSCALE)
    img_data = np.copy(old_im)
    nnz_inds = np.where(img_data != 255)
    if len(nnz_inds[0]) == 0:
        if not default_size:
            cv.imwrite(output_path, old_im)
            return False
        assert len(default_size) == 2, default_size
        x_min, y_min, x_max, y_max = 0, 0, default_size[0], default_size[1]
        old_im = old_im[y_min: y_max + 1, x_min, x_max + 1]
        cv.imwrite(output_path, old_im)
        return False
    y_min = np.min(nnz_inds[0])
    y_max = np.max(nnz_inds[0])
    x_min = np.min(nnz_inds[1])
    x_max = np.max(nnz_inds[1])

    old_im = old_im[y_min: y_max + 1, x_min: x_max + 1]
    cv.imwrite(output_path, old_im)
    return True",len(nnz_inds[0]) == 0,not nnz_inds[0]
Video-Downloader,https://github.com/EvilCult/Video-Downloader/tree/master/Module/acfunClass.py,ChaseAcfun,chaseUrl$27,"def chaseUrl (self) :
		result = {'stat': 1, 'msg': ''}
		videoID = self.__getVideoID(self.videoLink)

		if videoID :
			info = self.__getVideoInfo(videoID)
			if info != False:
				sourceInfo = self.__getSourceInfo(info)
				if sourceInfo != False:
					fileUrl = self.__getVideoFileUrl(sourceInfo)
					if fileUrl != False :
						listFile = self.__getFileList(fileUrl)
						if len(listFile) > 0:
							result['stat'] = 0
							result['msg'] = listFile
						else:
							result['msg'] = self.err.show(2)
					else :
						result['msg'] = self.err.show(4)
				else :
					result['msg'] = self.err.show(3)
			else :
				result['msg'] = self.err.show(3)
		else :
			result['msg'] = self.err.show(1)

		return result",info != False,info
Video-Downloader,https://github.com/EvilCult/Video-Downloader/tree/master/Module/acfunClass.py,ChaseAcfun,chaseUrl$27,"def chaseUrl (self) :
		result = {'stat': 1, 'msg': ''}
		videoID = self.__getVideoID(self.videoLink)

		if videoID :
			info = self.__getVideoInfo(videoID)
			if info != False:
				sourceInfo = self.__getSourceInfo(info)
				if sourceInfo != False:
					fileUrl = self.__getVideoFileUrl(sourceInfo)
					if fileUrl != False :
						listFile = self.__getFileList(fileUrl)
						if len(listFile) > 0:
							result['stat'] = 0
							result['msg'] = listFile
						else:
							result['msg'] = self.err.show(2)
					else :
						result['msg'] = self.err.show(4)
				else :
					result['msg'] = self.err.show(3)
			else :
				result['msg'] = self.err.show(3)
		else :
			result['msg'] = self.err.show(1)

		return result",sourceInfo != False,sourceInfo
Video-Downloader,https://github.com/EvilCult/Video-Downloader/tree/master/Module/acfunClass.py,ChaseAcfun,chaseUrl$27,"def chaseUrl (self) :
		result = {'stat': 1, 'msg': ''}
		videoID = self.__getVideoID(self.videoLink)

		if videoID :
			info = self.__getVideoInfo(videoID)
			if info != False:
				sourceInfo = self.__getSourceInfo(info)
				if sourceInfo != False:
					fileUrl = self.__getVideoFileUrl(sourceInfo)
					if fileUrl != False :
						listFile = self.__getFileList(fileUrl)
						if len(listFile) > 0:
							result['stat'] = 0
							result['msg'] = listFile
						else:
							result['msg'] = self.err.show(2)
					else :
						result['msg'] = self.err.show(4)
				else :
					result['msg'] = self.err.show(3)
			else :
				result['msg'] = self.err.show(3)
		else :
			result['msg'] = self.err.show(1)

		return result",fileUrl != False,fileUrl
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/hpo/ea/ga.py,GeneticAlgorithm,selection$89,"def selection(self):
        """"""Select pareto front individual.""""""
        ids = [id for id, value in self.scores.items() if ""score"" in value]
        if len(ids) == 0:
            return None
        rewards = [self.scores[id][""score""] for id in ids]
        indexes = get_pareto_index(np.array(rewards)).tolist()
        pareto = [id for i, id in enumerate(ids) if indexes[i]]

        if len(pareto) < 2:
            others = [id for id in self.scores if id not in pareto]
            pareto = pareto + random.sample(others, 2 - len(pareto))
        else:
            pareto = random.sample(pareto, 2)

        return [value[""config""] for id, value in self.scores.items() if id in pareto]",len(ids) == 0,not ids
torchdistill,https://github.com/yoshitomo-matsubara/torchdistill/tree/master/torchdistill/models/classification/resnet.py,,resnet$122,"def resnet(
        depth: int,
        num_classes: int,
        pretrained: bool,
        progress: bool,
        **kwargs: Any
) -> ResNet4Cifar:
    assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110, 1202'
    n = (depth - 2) // 6
    model = ResNet4Cifar(BasicBlock, [n, n, n], num_classes, **kwargs)
    model_key = 'cifar{}-resnet{}'.format(num_classes, depth)
    if pretrained and model_key in MODEL_URL_DICT:
        state_dict = torch.hub.load_state_dict_from_url(MODEL_URL_DICT[model_key], progress=progress)
        model.load_state_dict(state_dict)
    return model",(depth - 2) % 6 == 0,not (depth - 2) % 6
LibreTranslate,https://github.com/LibreTranslate/LibreTranslate/tree/master//wsgi.py,,app$3,"def app(*args, **kwargs):
    import sys
    sys.argv = ['--wsgi']
    for k in kwargs:
        ck = k.replace(""_"", ""-"")
        if isinstance(kwargs[k], bool) and kwargs[k]:
            sys.argv.append(""--"" + ck)
        else:
            sys.argv.append(""--"" + ck)
            sys.argv.append(kwargs[k])

    instance = main()

    if len(kwargs) == 0:
        return instance(*args, **kwargs)
    else:
        return instance",len(kwargs) == 0,not kwargs
tinytag,https://github.com/devsnd/tinytag/tree/master/tinytag/tests/test_cli.py,,test_save_image_bulk$60,"def test_save_image_bulk():
    temp_file = NamedTemporaryFile(suffix='.jpg')
    temp_file_no_ext = temp_file.name[:-4]
    assert file_size(temp_file.name) == 0
    run_cli('-i %s %s %s %s' % (temp_file.name, mp3_with_image, mp3_with_image, mp3_with_image))
    assert file_size(temp_file.name) == 0
    assert file_size(temp_file_no_ext + '00000.jpg') > 0
    assert file_size(temp_file_no_ext + '00001.jpg') > 0
    assert file_size(temp_file_no_ext + '00002.jpg') > 0",file_size(temp_file.name) == 0,not file_size(temp_file.name)
tinytag,https://github.com/devsnd/tinytag/tree/master/tinytag/tests/test_cli.py,,test_save_image_bulk$60,"def test_save_image_bulk():
    temp_file = NamedTemporaryFile(suffix='.jpg')
    temp_file_no_ext = temp_file.name[:-4]
    assert file_size(temp_file.name) == 0
    run_cli('-i %s %s %s %s' % (temp_file.name, mp3_with_image, mp3_with_image, mp3_with_image))
    assert file_size(temp_file.name) == 0
    assert file_size(temp_file_no_ext + '00000.jpg') > 0
    assert file_size(temp_file_no_ext + '00001.jpg') > 0
    assert file_size(temp_file_no_ext + '00002.jpg') > 0",file_size(temp_file.name) == 0,not file_size(temp_file.name)
ShivyC,https://github.com/ShivamSarodia/ShivyC/tree/master/shivyc/parser/declaration.py,,_find_pair_backward$329,"def _find_pair_backward(index,
                        open=token_kinds.open_paren,
                        close=token_kinds.close_paren,
                        mess=""mismatched parentheses in declaration""):
    """"""Find the opening parenthesis for the closing at given index.

    Same parameters as _find_pair_forward above.
    """"""
    depth = 0
    for i in range(index, -1, -1):
        if p.tokens[i].kind == close:
            depth += 1
        elif p.tokens[i].kind == open:
            depth -= 1

        if depth == 0:
            break
    else:
        # if loop did not break, no open paren was found
        raise_error(mess, index, ParserError.AT)
    return i",depth == 0,not depth
gyroflow,https://github.com/ElvinC/gyroflow/tree/master/gpmf/parse.py,,parse_dict$150,"def parse_dict(data):
    """"""Parse data into a dict recursively
    """"""

    elements = construct.GreedyRange(FOURCC).parse(data)
    new_dict = dict()

    for element in elements:
        if element.type == 0:

            if (element.key.decode('ascii') == ""STRM""):
                new_dict.setdefault(""STRM"", []).append(parse_dict(element.data))
            else:
                new_dict[element.key.decode('ascii')] = parse_dict(element.data)
            
        else: 
            try: 
                value = parse_value(element)
            except ValueError:
                value = element.data
            new_dict[element.key.decode('ascii')] = value

    return new_dict",element.type == 0,not element.type
fuxi,https://github.com/jeffzh3ng/fuxi/tree/master/fuxi/common/libs/ip_handler.py,IPint,_printPrefix$312,"def _printPrefix(self, want):
        """"""Prints Prefixlen/Netmask.

        Not really. In fact it is our universal Netmask/Prefixlen printer.
        This is considered an internal function.

        want == 0 / None        don't return anything    1.2.3.0
        want == 1               /prefix                  1.2.3.0/24
        want == 2               /netmask                 1.2.3.0/255.255.255.0
        want == 3               -lastip                  1.2.3.0-1.2.3.255
        """"""

        if (self._ipversion == 4 and self._prefixlen == 32) or \
                (self._ipversion == 6 and self._prefixlen == 128):
            if self.NoPrefixForSingleIp:
                want = 0
        if want == None:
            want = self.WantPrefixLen
            if want == None:
                want = 1
        if want:
            if want == 2:
                # this should work with IP and IPint
                netmask = self.netmask()
                if not isinstance(netmask, INT_TYPES):
                    netmask = netmask.int()
                return ""/%s"" % (intToIp(netmask, self._ipversion))
            elif want == 3:
                return ""-%s"" % (intToIp(self.ip + self.len() - 1, self._ipversion))
            else:
                # default
                return ""/%d"" % (self._prefixlen)
        else:
            return ''",want == None,not want
fuxi,https://github.com/jeffzh3ng/fuxi/tree/master/fuxi/common/libs/ip_handler.py,IPint,_printPrefix$312,"def _printPrefix(self, want):
        """"""Prints Prefixlen/Netmask.

        Not really. In fact it is our universal Netmask/Prefixlen printer.
        This is considered an internal function.

        want == 0 / None        don't return anything    1.2.3.0
        want == 1               /prefix                  1.2.3.0/24
        want == 2               /netmask                 1.2.3.0/255.255.255.0
        want == 3               -lastip                  1.2.3.0-1.2.3.255
        """"""

        if (self._ipversion == 4 and self._prefixlen == 32) or \
                (self._ipversion == 6 and self._prefixlen == 128):
            if self.NoPrefixForSingleIp:
                want = 0
        if want == None:
            want = self.WantPrefixLen
            if want == None:
                want = 1
        if want:
            if want == 2:
                # this should work with IP and IPint
                netmask = self.netmask()
                if not isinstance(netmask, INT_TYPES):
                    netmask = netmask.int()
                return ""/%s"" % (intToIp(netmask, self._ipversion))
            elif want == 3:
                return ""-%s"" % (intToIp(self.ip + self.len() - 1, self._ipversion))
            else:
                # default
                return ""/%d"" % (self._prefixlen)
        else:
            return ''",want == None,not want
scikit-fuzzy,https://github.com/scikit-fuzzy/scikit-fuzzy/tree/master/skfuzzy/image/arraypad.py,,_append_med$589,"def _append_med(arr, pad_amt, num, axis=-1):
    """"""
    Append `pad_amt` median values along `axis`.

    Parameters
    ----------
    arr : ndarray
        Input array of arbitrary shape.
    pad_amt : int
        Amount of padding to append.
    num : int
        Depth into `arr` along `axis` to calculate median.
        Range: [1, `arr.shape[axis]`] or None (entire axis)
    axis : int
        Axis along which to pad `arr`.

    Returns
    -------
    padarr : ndarray
        Output array, with `pad_amt` values appended along `axis`. The
        appended region is the median of the final `num` values along `axis`.

    """"""
    if pad_amt == 0:
        return arr

    # Equivalent to edge padding for single value, so do that instead
    if num == 1:
        return _append_edge(arr, pad_amt, axis)

    # Use entire array if `num` is too large
    if num is not None:
        if num >= arr.shape[axis]:
            num = None

    # Slice a chunk from the edge to calculate stats on
    end = arr.shape[axis] - 1
    if num is not None:
        med_slice = tuple(
            slice(None) if i != axis else slice(end, end - num, -1)
            for (i, x) in enumerate(arr.shape))
    else:
        med_slice = (slice(None),) * len(arr.shape)

    # Shape to restore singleton dimension after slicing
    pad_singleton = tuple(x if i != axis else 1
                          for (i, x) in enumerate(arr.shape))

    # Extract slice, calculate median, reshape to add singleton dimension back
    med_chunk = np.median(arr[med_slice], axis=axis).reshape(pad_singleton)
    _round_ifneeded(med_chunk, arr.dtype)

    # Concatenate `arr` with `med_chunk`, extended along `axis` by `pad_amt`
    return np.concatenate(
        (arr, med_chunk.repeat(pad_amt, axis).astype(arr.dtype)), axis=axis)",pad_amt == 0,not pad_amt
lbry-sdk,https://github.com/lbryio/lbry-sdk/tree/master/lbry/crypto/base58.py,Base58,encode$56,"def encode(cls, be_bytes):
        """"""Converts a big-endian bytearray into a base58 string.""""""
        value = bytes_to_int(be_bytes)

        txt = ''
        while value:
            value, mod = divmod(value, 58)
            txt += cls.chars[mod]

        for byte in be_bytes:
            if byte != 0:
                break
            txt += '1'

        return txt[::-1]",byte != 0,byte
pycm,https://github.com/sepandhaghighi/pycm/tree/master/pycm/pycm_util.py,,normalized_table_calc$196,"def normalized_table_calc(classes, table):
    """"""
    Return normalized confusion matrix.

    :param classes: classes list
    :type classes: list
    :param table: table
    :type table: dict
    :return: normalized table as dict
    """"""
    normalized_table = {}
    p = float(10**5)
    for key in classes:
        normalized_table[key] = {}
        div = sum(table[key].values())
        if div == 0:
            div = 1
        for item in classes:
            normalized_table[key][item] = custom_rounder(
                table[key][item] / div, p)
    return normalized_table",div == 0,not div
pychrome,https://github.com/fate0/pychrome/tree/master/tests/test_browser.py,,test_browser_list$38,"def test_browser_list():
    browser = pychrome.Browser()
    tabs = browser.list_tab()
    assert len(tabs) == 0",len(tabs) == 0,not tabs
CudaText,https://github.com/Alexey-T/CudaText/tree/master/app/cudatext.app/Contents/Resources/py/sys/requests/utils.py,,guess_json_utf$950,"def guess_json_utf(data):
    """"""
    :rtype: str
    """"""
    # JSON always starts with two ASCII characters, so detection is as
    # easy as counting the nulls and from their location and count
    # determine the encoding. Also detect a BOM, if present.
    sample = data[:4]
    if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):
        return ""utf-32""  # BOM included
    if sample[:3] == codecs.BOM_UTF8:
        return ""utf-8-sig""  # BOM included, MS style (discouraged)
    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):
        return ""utf-16""  # BOM included
    nullcount = sample.count(_null)
    if nullcount == 0:
        return ""utf-8""
    if nullcount == 2:
        if sample[::2] == _null2:  # 1st and 3rd are null
            return ""utf-16-be""
        if sample[1::2] == _null2:  # 2nd and 4th are null
            return ""utf-16-le""
        # Did not detect 2 valid UTF-16 ascii-range characters
    if nullcount == 3:
        if sample[:3] == _null3:
            return ""utf-32-be""
        if sample[1:] == _null3:
            return ""utf-32-le""
        # Did not detect a valid UTF-32 ascii-range character
    return None",nullcount == 0,not nullcount
audiolazy,https://github.com/danilobellini/audiolazy/tree/master/audiolazy/lazy_filters.py,ParallelFilter,__call__$1044,"def __call__(self, *args, **kwargs):
    if len(self) == 0:
      return Stream(kwargs[""zero""] if ""zero"" in kwargs else 0.
                    for _ in args[0])
    arg0 = thub(args[0], len(self))
    return reduce(operator.add, (filt(arg0, *args[1:], **kwargs)
                                 for filt in self.callables))",len(self) == 0,not self
DeepSpeed,https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/runtime/state_dict_factory.py,SDLoaderBase,get_merge_state_dicts$116,"def get_merge_state_dicts(self, mp_world_size, mp_rank):
        num_ckpt = len(self.ckpt_list)
        assert num_ckpt % mp_world_size == 0, 'Invalid checkpoints and world size for sd merge'

        num_to_merge = num_ckpt // mp_world_size
        ckpt_list = [
            self.ckpt_list[i] for i in range(num_to_merge * mp_rank,
                                             num_to_merge * (mp_rank + 1))
        ]

        logger.info(f""mp_rank: {mp_rank}, ckpt_list: {ckpt_list}"")
        sd_list = [
            self.checkpoint_engine.load(ckpt,
                                        map_location=lambda storage,
                                        loc: storage) for ckpt in ckpt_list
        ]
        return sd_list",num_ckpt % mp_world_size == 0,not num_ckpt % mp_world_size
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/generation_utils.py,GenerationMixin,group_beam_search$3203,"def group_beam_search(
        self,
        input_ids: torch.LongTensor,
        beam_scorer: BeamScorer,
        logits_processor: Optional[LogitsProcessorList] = None,
        stopping_criteria: Optional[StoppingCriteriaList] = None,
        max_length: Optional[int] = None,
        pad_token_id: Optional[int] = None,
        eos_token_id: Optional[int] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        output_scores: Optional[bool] = None,
        return_dict_in_generate: Optional[bool] = None,
        synced_gpus: Optional[bool] = False,
        **model_kwargs,
    ):
        r""""""
        Generates sequences of token ids for models with a language modeling head using **diverse beam search
        decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.

        Parameters:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                The sequence used as a prompt for the generation.
            beam_scorer (`BeamScorer`):
                An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and
                sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.
            logits_processor (`LogitsProcessorList`, *optional*):
                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]
                used to modify the prediction scores of the language modeling head applied at each generation step.
            stopping_criteria (`StoppingCriteriaList`, *optional*):
                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]
                used to tell if the generation loop should stop.
            max_length (`int`, *optional*, defaults to 20):
                **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated
                tokens. The maximum length of the sequence to be generated.
            pad_token_id (`int`, *optional*):
                The id of the *padding* token.
            eos_token_id (`int`, *optional*):
                The id of the *end-of-sequence* token.
            output_attentions (`bool`, *optional*, defaults to `False`):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more details.
            output_hidden_states (`bool`, *optional*, defaults to `False`):
                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
                for more details.
            output_scores (`bool`, *optional*, defaults to `False`):
                Whether or not to return the prediction scores. See `scores` under returned tensors for more details.
            return_dict_in_generate (`bool`, *optional*, defaults to `False`):
                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
            synced_gpus (`bool`, *optional*, defaults to `False`):
                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)

            model_kwargs:
                Additional model specific kwargs that will be forwarded to the `forward` function of the model. If
                model is an encoder-decoder model the kwargs should include `encoder_outputs`.

        Return:
            [`~generation_utils.BeamSearchDecoderOnlyOutput`], [`~generation_utils.BeamSearchEncoderDecoderOutput`] or
            `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a
            [`~generation_utils.BeamSearchDecoderOnlyOutput`] if [`~generation_utils.BeamSearchDecoderOnlyOutput`] if
            `model.config.is_encoder_decoder=False` and `return_dict_in_generate=True` or a
            [`~generation_utils.BeamSearchEncoderDecoderOutput`] if `model.config.is_encoder_decoder=True`.

        Examples:

        ```python
        >>> from transformers import (
        ...     AutoTokenizer,
        ...     AutoModelForSeq2SeqLM,
        ...     LogitsProcessorList,
        ...     MinLengthLogitsProcessor,
        ...     HammingDiversityLogitsProcessor,
        ...     BeamSearchScorer,
        ... )
        >>> import torch

        >>> tokenizer = AutoTokenizer.from_pretrained(""t5-base"")
        >>> model = AutoModelForSeq2SeqLM.from_pretrained(""t5-base"")

        >>> encoder_input_str = ""translate English to German: How old are you?""
        >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=""pt"").input_ids


        >>> # lets run diverse beam search using 6 beams
        >>> num_beams = 6
        >>> # define decoder start token ids
        >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
        >>> input_ids = input_ids * model.config.decoder_start_token_id

        >>> # add encoder_outputs to model keyword arguments
        >>> model_kwargs = {
        ...     ""encoder_outputs"": model.get_encoder()(
        ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
        ...     )
        ... }

        >>> # instantiate beam scorer
        >>> beam_scorer = BeamSearchScorer(
        ...     batch_size=1,
        ...     max_length=model.config.max_length,
        ...     num_beams=num_beams,
        ...     device=model.device,
        ...     num_beam_groups=3,
        ... )

        >>> # instantiate logits processors
        >>> logits_processor = LogitsProcessorList(
        ...     [
        ...         HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),
        ...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
        ...     ]
        ... )

        >>> outputs = model.group_beam_search(
        ...     input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
        ... )

        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
        ['Wie alt bist du?']
        ```""""""
        # init values
        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
        if max_length is not None:
            warnings.warn(
                ""`max_length` is deprecated in this function, use""
                "" `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead."",
                UserWarning,
            )
            stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)
        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id
        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id
        output_scores = output_scores if output_scores is not None else self.config.output_scores
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict_in_generate = (
            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate
        )

        batch_size = len(beam_scorer._beam_hyps)
        num_beams = beam_scorer.num_beams
        num_beam_groups = beam_scorer.num_beam_groups
        num_sub_beams = num_beams // num_beam_groups
        device = input_ids.device

        batch_beam_size, cur_len = input_ids.shape

        if return_dict_in_generate and output_scores:
            beam_indices = [tuple(() for _ in range(num_sub_beams * batch_size)) for _ in range(num_beam_groups)]
        else:
            beam_indices = None

        if num_beams * batch_size != batch_beam_size:
            raise ValueError(
                f""Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.""
            )

        # init attention / hidden states / scores tuples
        scores = () if (return_dict_in_generate and output_scores) else None
        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None
        cross_attentions = () if (return_dict_in_generate and output_attentions) else None
        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None

        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states
        if return_dict_in_generate and self.config.is_encoder_decoder:
            encoder_attentions = model_kwargs[""encoder_outputs""].get(""attentions"") if output_attentions else None
            encoder_hidden_states = (
                model_kwargs[""encoder_outputs""].get(""hidden_states"") if output_hidden_states else None
            )

        # initialise score of first beam of each group with 0 and the rest with -1e9. This ensures that the beams in
        # the same group don't produce same tokens everytime.
        beam_scores = torch.full((batch_size, num_beams), -1e9, dtype=torch.float, device=device)
        beam_scores[:, ::num_sub_beams] = 0
        beam_scores = beam_scores.view((batch_size * num_beams,))

        this_peer_finished = False  # used by synced_gpus only
        while True:
            if synced_gpus:
                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.
                # The following logic allows an early break if all peers finished generating their sequence
                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)
                # send 0.0 if we finished, 1.0 otherwise
                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)
                # did all peers finish? the reduced sum will be 0.0 then
                if this_peer_finished_flag.item() == 0.0:
                    break

            # predicted tokens in cur_len step
            current_tokens = torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)

            # indices which will form the beams in the next time step
            reordering_indices = torch.zeros(batch_size * num_beams, dtype=torch.long, device=device)

            # do one decoder step on all beams of all sentences in batch
            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
            outputs = self(
                **model_inputs,
                return_dict=True,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
            )

            if synced_gpus and this_peer_finished:
                cur_len = cur_len + 1
                continue  # don't waste resources running the code we don't need

            if output_scores:
                processed_score = torch.zeros_like(outputs.logits[:, -1, :])

            for beam_group_idx in range(num_beam_groups):
                group_start_idx = beam_group_idx * num_sub_beams
                group_end_idx = min(group_start_idx + num_sub_beams, num_beams)
                group_size = group_end_idx - group_start_idx

                # indices of beams of current group among all sentences in batch
                batch_group_indices = []

                for batch_idx in range(batch_size):
                    batch_group_indices.extend(
                        [batch_idx * num_beams + idx for idx in range(group_start_idx, group_end_idx)]
                    )
                group_input_ids = input_ids[batch_group_indices]

                # select outputs of beams of current group only
                next_token_logits = outputs.logits[batch_group_indices, -1, :]

                # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`
                # cannot be generated both before and after the `nn.functional.log_softmax` operation.
                next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)
                next_token_scores = nn.functional.log_softmax(
                    next_token_logits, dim=-1
                )  # (batch_size * group_size, vocab_size)
                vocab_size = next_token_scores.shape[-1]

                next_token_scores_processed = logits_processor(
                    group_input_ids, next_token_scores, current_tokens=current_tokens, beam_group_idx=beam_group_idx
                )
                next_token_scores = next_token_scores_processed + beam_scores[batch_group_indices].unsqueeze(-1)
                next_token_scores = next_token_scores.expand_as(next_token_scores_processed)

                if output_scores:
                    processed_score[batch_group_indices] = next_token_scores_processed

                # reshape for beam search
                next_token_scores = next_token_scores.view(batch_size, group_size * vocab_size)

                # Sample 2 next tokens for each beam (so we have some spare tokens and match output of beam search)
                next_token_scores, next_tokens = torch.topk(
                    next_token_scores, 2 * group_size, dim=1, largest=True, sorted=True
                )

                next_indices = torch_int_div(next_tokens, vocab_size)
                next_tokens = next_tokens % vocab_size

                # stateless
                process_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None
                beam_outputs = beam_scorer.process(
                    group_input_ids,
                    next_token_scores,
                    next_tokens,
                    next_indices,
                    pad_token_id=pad_token_id,
                    eos_token_id=eos_token_id,
                    beam_indices=process_beam_indices,
                )
                beam_scores[batch_group_indices] = beam_outputs[""next_beam_scores""]
                beam_next_tokens = beam_outputs[""next_beam_tokens""]
                beam_idx = beam_outputs[""next_beam_indices""]

                if return_dict_in_generate and output_scores:
                    beam_indices[beam_group_idx] = tuple(
                        beam_indices[beam_group_idx][beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices[0]))
                    )

                input_ids[batch_group_indices] = group_input_ids[beam_idx]
                group_input_ids = torch.cat([group_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)
                current_tokens[batch_group_indices] = group_input_ids[:, -1]

                # (beam_idx // group_size) -> batch_idx
                # (beam_idx % group_size) -> offset of idx inside the group
                reordering_indices[batch_group_indices] = (
                    num_beams * torch_int_div(beam_idx, group_size) + group_start_idx + (beam_idx % group_size)
                )

            # Store scores, attentions and hidden_states when required
            if return_dict_in_generate:
                if output_scores:
                    scores += (processed_score,)
                if output_attentions:
                    decoder_attentions += (
                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)
                    )
                    if self.config.is_encoder_decoder:
                        cross_attentions += (outputs.cross_attentions,)

                if output_hidden_states:
                    decoder_hidden_states += (
                        (outputs.decoder_hidden_states,)
                        if self.config.is_encoder_decoder
                        else (outputs.hidden_states,)
                    )

            input_ids = torch.cat([input_ids, current_tokens.unsqueeze(-1)], dim=-1)

            model_kwargs = self._update_model_kwargs_for_generation(
                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder
            )
            if model_kwargs[""past""] is not None:
                model_kwargs[""past""] = self._reorder_cache(model_kwargs[""past""], reordering_indices)

            # increase cur_len
            cur_len = cur_len + 1

            if beam_scorer.is_done or stopping_criteria(input_ids, scores):
                if not synced_gpus:
                    break
                else:
                    this_peer_finished = True

        final_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None
        sequence_outputs = beam_scorer.finalize(
            input_ids,
            beam_scores,
            next_tokens,
            next_indices,
            pad_token_id=pad_token_id,
            eos_token_id=eos_token_id,
            max_length=stopping_criteria.max_length,
            beam_indices=final_beam_indices,
        )

        if return_dict_in_generate:
            if not output_scores:
                sequence_outputs[""sequence_scores""] = None

            if self.config.is_encoder_decoder:
                return BeamSearchEncoderDecoderOutput(
                    sequences=sequence_outputs[""sequences""],
                    sequences_scores=sequence_outputs[""sequence_scores""],
                    scores=scores,
                    beam_indices=sequence_outputs[""beam_indices""],
                    encoder_attentions=encoder_attentions,
                    encoder_hidden_states=encoder_hidden_states,
                    decoder_attentions=decoder_attentions,
                    cross_attentions=cross_attentions,
                    decoder_hidden_states=decoder_hidden_states,
                )
            else:
                return BeamSearchDecoderOnlyOutput(
                    sequences=sequence_outputs[""sequences""],
                    sequences_scores=sequence_outputs[""sequence_scores""],
                    scores=scores,
                    beam_indices=sequence_outputs[""beam_indices""],
                    attentions=decoder_attentions,
                    hidden_states=decoder_hidden_states,
                )
        else:
            return sequence_outputs[""sequences""]",this_peer_finished_flag.item() == 0.0,not this_peer_finished_flag.item()
InvoiceNet,https://github.com/naiveHobo/InvoiceNet/tree/master/invoicenet/common/trainer.py,,train$29,"def train(model: Model,
          train_data: tf.data.Dataset,
          val_data: tf.data.Dataset,
          total_steps=50000,
          early_stop_steps=0):

    print_interval = 20
    no_improvement_steps = 0
    best = float(""inf"")

    train_iter = iter(train_data)
    val_iter = iter(val_data)

    start = time.time()
    for step in range(total_steps):
        try:
            train_loss = model.train_step(next(train_iter))
        except StopIteration:
            print(""Couldn't find any training data! Have you prepared your training data?"")
            print(""Terminating..."")
            break

        if not np.isfinite(train_loss):
            raise ValueError(""NaN loss"")

        if step % print_interval == 0:
            took = time.time() - start

            try:
                val_loss = model.val_step(next(val_iter))
            except StopIteration:
                print(""Couldn't find any validation data! Have you prepared your training data?"")
                print(""Terminating..."")
                break

            print(""[%d/%d | %.2f steps/s]: train loss: %.4f val loss: %.4f"" % (
                step, total_steps, (step + 1) / took, train_loss, val_loss))
            if not np.isfinite(val_loss):
                raise ValueError(""NaN loss"")
            if val_loss < best:
                no_improvement_steps = 0
                best = val_loss
                model.save(""best"")
            elif early_stop_steps > 0:
                no_improvement_steps += print_interval
                if no_improvement_steps >= early_stop_steps:
                    print(""Validation loss has not improved for {} steps, terminating!"".format(no_improvement_steps))
                    return",step % print_interval == 0,not step % print_interval
exbert,https://github.com/bhoov/exbert/tree/master/server/transformers/examples/run_generation.py,,main$150,"def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--model_type"",
        default=None,
        type=str,
        required=True,
        help=""Model type selected in the list: "" + "", "".join(MODEL_CLASSES.keys()),
    )
    parser.add_argument(
        ""--model_name_or_path"",
        default=None,
        type=str,
        required=True,
        help=""Path to pre-trained model or shortcut name selected in the list: "" + "", "".join(MODEL_CLASSES.keys()),
    )

    parser.add_argument(""--prompt"", type=str, default="""")
    parser.add_argument(""--length"", type=int, default=20)
    parser.add_argument(""--stop_token"", type=str, default=None, help=""Token at which text generation is stopped"")

    parser.add_argument(
        ""--temperature"",
        type=float,
        default=1.0,
        help=""temperature of 1.0 has no effect, lower tend toward greedy sampling"",
    )
    parser.add_argument(
        ""--repetition_penalty"", type=float, default=1.0, help=""primarily useful for CTRL model; in that case, use 1.2""
    )
    parser.add_argument(""--k"", type=int, default=0)
    parser.add_argument(""--p"", type=float, default=0.9)

    parser.add_argument(""--padding_text"", type=str, default="""", help=""Padding text for Transfo-XL and XLNet."")
    parser.add_argument(""--xlm_language"", type=str, default="""", help=""Optional language when used with the XLM model."")

    parser.add_argument(""--seed"", type=int, default=42, help=""random seed for initialization"")
    parser.add_argument(""--no_cuda"", action=""store_true"", help=""Avoid using CUDA when available"")
    parser.add_argument(""--num_return_sequences"", type=int, default=1, help=""The number of samples to generate."")
    args = parser.parse_args()

    args.device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")
    args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()

    set_seed(args)

    # Initialize the model and tokenizer
    try:
        args.model_type = args.model_type.lower()
        model_class, tokenizer_class = MODEL_CLASSES[args.model_type]
    except KeyError:
        raise KeyError(""the model {} you specified is not supported. You are welcome to add it and open a PR :)"")

    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)
    model = model_class.from_pretrained(args.model_name_or_path)
    model.to(args.device)

    args.length = adjust_length_to_model(args.length, max_sequence_length=model.config.max_position_embeddings)
    logger.info(args)

    prompt_text = args.prompt if args.prompt else input(""Model prompt >>> "")

    # Different models need different input formatting and/or extra arguments
    requires_preprocessing = args.model_type in PREPROCESSING_FUNCTIONS.keys()
    if requires_preprocessing:
        prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)
        preprocessed_prompt_text = prepare_input(args, model, tokenizer, prompt_text)
        encoded_prompt = tokenizer.encode(
            preprocessed_prompt_text, add_special_tokens=False, return_tensors=""pt"", add_space_before_punct_symbol=True
        )
    else:
        encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=""pt"")
    encoded_prompt = encoded_prompt.to(args.device)

    if encoded_prompt.size()[-1] == 0:
        input_ids = None
    else:
        input_ids = encoded_prompt

    output_sequences = model.generate(
        input_ids=input_ids,
        max_length=args.length + len(encoded_prompt[0]),
        temperature=args.temperature,
        top_k=args.k,
        top_p=args.p,
        repetition_penalty=args.repetition_penalty,
        do_sample=True,
        num_return_sequences=args.num_return_sequences,
    )

    # Remove the batch dimension when returning multiple sequences
    if len(output_sequences.shape) > 2:
        output_sequences.squeeze_()

    generated_sequences = []

    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):
        print(""=== GENERATED SEQUENCE {} ==="".format(generated_sequence_idx + 1))
        generated_sequence = generated_sequence.tolist()

        # Decode text
        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)

        # Remove all text after the stop token
        text = text[: text.find(args.stop_token) if args.stop_token else None]

        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing
        total_sequence = (
            prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]
        )

        generated_sequences.append(total_sequence)
        print(total_sequence)

    return generated_sequences",encoded_prompt.size()[-1] == 0,not encoded_prompt.size()[-1]
bindsnet,https://github.com/BindsNET/bindsnet/tree/master/bindsnet/analysis/dotTrace_plotter.py,,main$85,"def main():
    """"""
    File types:

    0) grid         - the 2D matrix observation
    1) reward       - list of rewards per iteration
    2) performance  - list of performance values
    """"""
    fileType = 0  # default to grid

    # By default, we'll search the examples directory, but tweak as needed.
    files = glob.glob(""../../examples/*/out/*csv"")

    if len(files) == 0:
        print(""Could not find any csv files. Exiting..."")
        sys.exit()

    plotAnotherFile = True

    while plotAnotherFile:
        print(""Select the file to generate grid plots from."")
        for i, f in enumerate(files):
            print(str(i), ""-"", f)

        # Select the intended file.
        sel = -1
        while sel < 0 or len(files) < sel:
            sel = int(input(""\nFile selection: ""))

        fileToPlot = files[sel]

        # Check file type
        if 0 < fileToPlot.find(""grid""):
            print(""\nFound 'grid' in name: assuming a grid file type."")
            fileType = 0
        elif 0 < fileToPlot.find(""rew""):
            print(""\nFound 'rew' in name: assuming a reward file type."")
            fileType = 1

        elif 0 < fileToPlot.find(""perf""):
            print(""\nFound 'perf' in name: assuming a performance file type."")
            fileType = 2
        else:
            print(""\nUnknown file type. Which type are we plotting?"")
            print(""\n0) grid\n1) reward\n2) performance"")
            fileType = -1
            while fileType < 0 or 2 < fileType:
                fileType = int(input(""\nFile type: ""))

        print(""\nPlotting: "", fileToPlot)
        data = np.genfromtxt(fileToPlot, delimiter="","")

        # Plot by file type
        if fileType == 0:
            plotGrids(data)
        elif fileType == 1:
            plotRewards(data, fileToPlot)
        elif fileType == 2:
            plotPerformance(data, fileToPlot)
        else:
            print(""ERROR: Unknown file type"")

        plotAnotherFile = str.lower(input(""Plot another file? (y/n): "")) == ""y""",len(files) == 0,not files
bindsnet,https://github.com/BindsNET/bindsnet/tree/master/bindsnet/analysis/dotTrace_plotter.py,,main$85,"def main():
    """"""
    File types:

    0) grid         - the 2D matrix observation
    1) reward       - list of rewards per iteration
    2) performance  - list of performance values
    """"""
    fileType = 0  # default to grid

    # By default, we'll search the examples directory, but tweak as needed.
    files = glob.glob(""../../examples/*/out/*csv"")

    if len(files) == 0:
        print(""Could not find any csv files. Exiting..."")
        sys.exit()

    plotAnotherFile = True

    while plotAnotherFile:
        print(""Select the file to generate grid plots from."")
        for i, f in enumerate(files):
            print(str(i), ""-"", f)

        # Select the intended file.
        sel = -1
        while sel < 0 or len(files) < sel:
            sel = int(input(""\nFile selection: ""))

        fileToPlot = files[sel]

        # Check file type
        if 0 < fileToPlot.find(""grid""):
            print(""\nFound 'grid' in name: assuming a grid file type."")
            fileType = 0
        elif 0 < fileToPlot.find(""rew""):
            print(""\nFound 'rew' in name: assuming a reward file type."")
            fileType = 1

        elif 0 < fileToPlot.find(""perf""):
            print(""\nFound 'perf' in name: assuming a performance file type."")
            fileType = 2
        else:
            print(""\nUnknown file type. Which type are we plotting?"")
            print(""\n0) grid\n1) reward\n2) performance"")
            fileType = -1
            while fileType < 0 or 2 < fileType:
                fileType = int(input(""\nFile type: ""))

        print(""\nPlotting: "", fileToPlot)
        data = np.genfromtxt(fileToPlot, delimiter="","")

        # Plot by file type
        if fileType == 0:
            plotGrids(data)
        elif fileType == 1:
            plotRewards(data, fileToPlot)
        elif fileType == 2:
            plotPerformance(data, fileToPlot)
        else:
            print(""ERROR: Unknown file type"")

        plotAnotherFile = str.lower(input(""Plot another file? (y/n): "")) == ""y""",fileType == 0,not fileType
buffalo,https://github.com/kakao/buffalo/tree/master/buffalo/parallel/base.py,ParBPRMF,topk_recommendation$142,"def topk_recommendation(self, keys, topk=10, pool=None, repr=False):
        """"""See the documentation of Parallel.""""""
        if self.algo.opt._nrz_P or self.algo.opt._nrz_Q:
            raise RuntimeError('Cannot make topk recommendation with normalized factors')
        # It is possible to skip make recommendation for not-existed keys.
        indexes = self.algo.get_index_pool(keys, group='user')
        keys = [k for k, i in zip(keys, indexes) if i is not None]
        indexes = np.array([i for i in indexes if i is not None], dtype=np.int32)
        if pool is not None:
            pool = self.algo.get_index_pool(pool, group='item')
            if len(pool) == 0:
                raise RuntimeError('pool is empty')
        else:
            # It assume that empty pool menas for all items
            pool = np.array([], dtype=np.int32)
        topks, scores = super()._topk_recommendation_bias(indexes, self.algo.P, self.algo.Q, self.algo.Qb, topk, pool)
        if repr:
            topks = [[self.algo._idmanager.itemids[t] for t in tt if t != -1] for tt in topks]
        return keys, topks, scores",len(pool) == 0,not pool
metrics,https://github.com/PyTorchLightning/metrics/tree/master/tests/bases/test_metric.py,,test_compute$125,"def test_compute():
    class A(DummyMetric):
        def update(self, x):
            self.x += x

        def compute(self):
            return self.x

    a = A()
    assert 0 == a.compute()
    assert 0 == a.x
    a.update(1)
    assert a._computed is None
    assert a.compute() == 1
    assert a._computed == 1
    a.update(2)
    assert a._computed is None
    assert a.compute() == 3
    assert a._computed == 3

    # called without update, should return cached value
    a._computed = 5
    assert a.compute() == 5",0 == a.compute(),not a.compute()
metrics,https://github.com/PyTorchLightning/metrics/tree/master/tests/bases/test_metric.py,,test_compute$125,"def test_compute():
    class A(DummyMetric):
        def update(self, x):
            self.x += x

        def compute(self):
            return self.x

    a = A()
    assert 0 == a.compute()
    assert 0 == a.x
    a.update(1)
    assert a._computed is None
    assert a.compute() == 1
    assert a._computed == 1
    a.update(2)
    assert a._computed is None
    assert a.compute() == 3
    assert a._computed == 3

    # called without update, should return cached value
    a._computed = 5
    assert a.compute() == 5",0 == a.x,not a.x
Flask-MonitoringDashboard,https://github.com/flask-dashboard/Flask-MonitoringDashboard/tree/master/tests/api/test_auth.py,,test_user_delete_normal_flow$41,"def test_user_delete_normal_flow(dashboard_user, another_user, session):
    response = dashboard_user.post(
        'dashboard/api/user/delete',
        data={'user_id': another_user.id},
    )
    assert response.status_code == 200
    assert response.data == b'OK'
    assert session.query(User).filter(User.username == another_user.username).count() == 0",session.query(User).filter(User.username == another_user.username).count() == 0,not session.query(User).filter(User.username == another_user.username).count()
PaddleX,https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex/ppdet/modeling/layers.py,AnchorGeneratorSSD,__init__$299,"def __init__(
            self,
            steps=[8, 16, 32, 64, 100, 300],
            aspect_ratios=[[2.], [2., 3.], [2., 3.], [2., 3.], [2.], [2.]],
            min_ratio=15,
            max_ratio=90,
            base_size=300,
            min_sizes=[30.0, 60.0, 111.0, 162.0, 213.0, 264.0],
            max_sizes=[60.0, 111.0, 162.0, 213.0, 264.0, 315.0],
            offset=0.5,
            flip=True,
            clip=False,
            min_max_aspect_ratios_order=False):
        self.steps = steps
        self.aspect_ratios = aspect_ratios
        self.min_ratio = min_ratio
        self.max_ratio = max_ratio
        self.base_size = base_size
        self.min_sizes = min_sizes
        self.max_sizes = max_sizes
        self.offset = offset
        self.flip = flip
        self.clip = clip
        self.min_max_aspect_ratios_order = min_max_aspect_ratios_order

        if self.min_sizes == [] and self.max_sizes == []:
            num_layer = len(aspect_ratios)
            step = int(
                math.floor(((self.max_ratio - self.min_ratio)) / (num_layer - 2
                                                                  )))
            for ratio in six.moves.range(self.min_ratio, self.max_ratio + 1,
                                         step):
                self.min_sizes.append(self.base_size * ratio / 100.)
                self.max_sizes.append(self.base_size * (ratio + step) / 100.)
            self.min_sizes = [self.base_size * .10] + self.min_sizes
            self.max_sizes = [self.base_size * .20] + self.max_sizes

        self.num_priors = []
        for aspect_ratio, min_size, max_size in zip(
                aspect_ratios, self.min_sizes, self.max_sizes):
            if isinstance(min_size, (list, tuple)):
                self.num_priors.append(
                    len(_to_list(min_size)) + len(_to_list(max_size)))
            else:
                self.num_priors.append((len(aspect_ratio) * 2 + 1) * len(
                    _to_list(min_size)) + len(_to_list(max_size)))",self.min_sizes == [],not self.min_sizes
PaddleX,https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex/ppdet/modeling/layers.py,AnchorGeneratorSSD,__init__$299,"def __init__(
            self,
            steps=[8, 16, 32, 64, 100, 300],
            aspect_ratios=[[2.], [2., 3.], [2., 3.], [2., 3.], [2.], [2.]],
            min_ratio=15,
            max_ratio=90,
            base_size=300,
            min_sizes=[30.0, 60.0, 111.0, 162.0, 213.0, 264.0],
            max_sizes=[60.0, 111.0, 162.0, 213.0, 264.0, 315.0],
            offset=0.5,
            flip=True,
            clip=False,
            min_max_aspect_ratios_order=False):
        self.steps = steps
        self.aspect_ratios = aspect_ratios
        self.min_ratio = min_ratio
        self.max_ratio = max_ratio
        self.base_size = base_size
        self.min_sizes = min_sizes
        self.max_sizes = max_sizes
        self.offset = offset
        self.flip = flip
        self.clip = clip
        self.min_max_aspect_ratios_order = min_max_aspect_ratios_order

        if self.min_sizes == [] and self.max_sizes == []:
            num_layer = len(aspect_ratios)
            step = int(
                math.floor(((self.max_ratio - self.min_ratio)) / (num_layer - 2
                                                                  )))
            for ratio in six.moves.range(self.min_ratio, self.max_ratio + 1,
                                         step):
                self.min_sizes.append(self.base_size * ratio / 100.)
                self.max_sizes.append(self.base_size * (ratio + step) / 100.)
            self.min_sizes = [self.base_size * .10] + self.min_sizes
            self.max_sizes = [self.base_size * .20] + self.max_sizes

        self.num_priors = []
        for aspect_ratio, min_size, max_size in zip(
                aspect_ratios, self.min_sizes, self.max_sizes):
            if isinstance(min_size, (list, tuple)):
                self.num_priors.append(
                    len(_to_list(min_size)) + len(_to_list(max_size)))
            else:
                self.num_priors.append((len(aspect_ratio) * 2 + 1) * len(
                    _to_list(min_size)) + len(_to_list(max_size)))",self.max_sizes == [],not self.max_sizes
napari,https://github.com/napari/napari/tree/master/napari/layers/tracks/tracks.py,Tracks,_extent_data$204,"def _extent_data(self) -> np.ndarray:
        """"""Extent of layer in data coordinates.

        Returns
        -------
        extent_data : array, shape (2, D)
        """"""
        if len(self.data) == 0:
            extrema = np.full((2, self.ndim), np.nan)
        else:
            maxs = np.max(self.data, axis=0)
            mins = np.min(self.data, axis=0)
            extrema = np.vstack([mins, maxs])
        return extrema[:, 1:]",len(self.data) == 0,not self.data
baserow,https://github.com/bram2w/baserow/tree/master/backend/tests/baserow/core/user/test_user_handler.py,,test_create_user_with_invitation$199,"def test_create_user_with_invitation(data_fixture):
    plugin_mock = MagicMock()
    with patch.dict(plugin_registry.registry, {""mock"": plugin_mock}):
        valid_password = ""thisIsAValidPassword""

        user_handler = UserHandler()
        core_handler = CoreHandler()

        invitation = data_fixture.create_group_invitation(email=""test0@test.nl"")
        signer = core_handler.get_group_invitation_signer()

        with pytest.raises(BadSignature):
            user_handler.create_user(
                ""Test1"",
                ""test0@test.nl"",
                valid_password,
                group_invitation_token=""INVALID"",
            )

        with pytest.raises(GroupInvitationDoesNotExist):
            user_handler.create_user(
                ""Test1"",
                ""test0@test.nl"",
                valid_password,
                group_invitation_token=signer.dumps(99999),
            )

        with pytest.raises(GroupInvitationEmailMismatch):
            user_handler.create_user(
                ""Test1"",
                ""test1@test.nl"",
                valid_password,
                group_invitation_token=signer.dumps(invitation.id),
            )

        data_fixture.update_settings(
            allow_new_signups=False, allow_signups_via_group_invitations=False
        )
        with pytest.raises(DisabledSignupError):
            user_handler.create_user(
                ""Test1"",
                ""test0@test.nl"",
                valid_password,
                group_invitation_token=signer.dumps(invitation.id),
            )

        data_fixture.update_settings(
            allow_new_signups=False, allow_signups_via_group_invitations=True
        )
        user = user_handler.create_user(
            ""Test1"",
            ""test0@test.nl"",
            valid_password,
            group_invitation_token=signer.dumps(invitation.id),
        )

        assert Group.objects.all().count() == 1
        assert Group.objects.all().first().id == invitation.group_id
        assert GroupUser.objects.all().count() == 2

        plugin_mock.user_created.assert_called_once()
        args = plugin_mock.user_created.call_args
        assert args[0][0] == user
        assert args[0][1].id == invitation.group_id
        assert args[0][2].email == invitation.email
        assert args[0][2].group_id == invitation.group_id

        # We do not expect any initial data to have been created.
        assert Database.objects.all().count() == 0
        assert Table.objects.all().count() == 0",Database.objects.all().count() == 0,not Database.objects.all().count()
baserow,https://github.com/bram2w/baserow/tree/master/backend/tests/baserow/core/user/test_user_handler.py,,test_create_user_with_invitation$199,"def test_create_user_with_invitation(data_fixture):
    plugin_mock = MagicMock()
    with patch.dict(plugin_registry.registry, {""mock"": plugin_mock}):
        valid_password = ""thisIsAValidPassword""

        user_handler = UserHandler()
        core_handler = CoreHandler()

        invitation = data_fixture.create_group_invitation(email=""test0@test.nl"")
        signer = core_handler.get_group_invitation_signer()

        with pytest.raises(BadSignature):
            user_handler.create_user(
                ""Test1"",
                ""test0@test.nl"",
                valid_password,
                group_invitation_token=""INVALID"",
            )

        with pytest.raises(GroupInvitationDoesNotExist):
            user_handler.create_user(
                ""Test1"",
                ""test0@test.nl"",
                valid_password,
                group_invitation_token=signer.dumps(99999),
            )

        with pytest.raises(GroupInvitationEmailMismatch):
            user_handler.create_user(
                ""Test1"",
                ""test1@test.nl"",
                valid_password,
                group_invitation_token=signer.dumps(invitation.id),
            )

        data_fixture.update_settings(
            allow_new_signups=False, allow_signups_via_group_invitations=False
        )
        with pytest.raises(DisabledSignupError):
            user_handler.create_user(
                ""Test1"",
                ""test0@test.nl"",
                valid_password,
                group_invitation_token=signer.dumps(invitation.id),
            )

        data_fixture.update_settings(
            allow_new_signups=False, allow_signups_via_group_invitations=True
        )
        user = user_handler.create_user(
            ""Test1"",
            ""test0@test.nl"",
            valid_password,
            group_invitation_token=signer.dumps(invitation.id),
        )

        assert Group.objects.all().count() == 1
        assert Group.objects.all().first().id == invitation.group_id
        assert GroupUser.objects.all().count() == 2

        plugin_mock.user_created.assert_called_once()
        args = plugin_mock.user_created.call_args
        assert args[0][0] == user
        assert args[0][1].id == invitation.group_id
        assert args[0][2].email == invitation.email
        assert args[0][2].group_id == invitation.group_id

        # We do not expect any initial data to have been created.
        assert Database.objects.all().count() == 0
        assert Table.objects.all().count() == 0",Table.objects.all().count() == 0,not Table.objects.all().count()
ezdxf,https://github.com/mozman/ezdxf/tree/master/tests/test_04_dxf_high_level_structs/test_411_acds_data.py,,test_empty_acds_section_will_not_be_exported$56,"def test_empty_acds_section_will_not_be_exported():
    doc = ezdxf.new(""R2013"")
    result = TagCollector.dxftags(doc.acdsdata)
    assert len(result) == 0",len(result) == 0,not result
gnss-ins-sim,https://github.com/Aceinna/gnss-ins-sim/tree/master/gnss_ins_sim/pathgen/pathgen.py,,gps_gen$596,"def gps_gen(ref_gps, gps_err, gps_type=0):
    '''
    Add error to true GPS data according to GPS receiver error parameters
    Args:
        ref_gps: If gps_type is 0, [Lat, Lon, Alt, vx, vy, vz], [rad, rad, m].
                 If gps_type is 1, [x, y, z, vx, vy, vz], [m, m, m].
                 ref_gps data are expressed in the navigation frame.
        gps_err: GPS reeceiver parameters.
            'stdp': RMS position error, [m, m, m].
            'stdv': RMS velocity error, [m/s, m/s, m/s].
        gps_type: GPS data type.
            0: default, position is in the form of [Lat, Lon, Alt], rad, m
            1: position is in the form of [x, y, z], m
    Returns:
        gps_mea: ref_gps with error.
    '''
    # total data count
    n = ref_gps.shape[0]
    pos_err = gps_err['stdp'].copy()
    # If position is in the form of LLA, convert gps_err['stdp'] to LLA error
    if gps_type == 0:   # GPS is in the form of LLA, stdp meter to rad
        earth_param = geoparams.geo_param(ref_gps[0, 0:3])
        pos_err[0] = pos_err[0] / earth_param[0]
        pos_err[1] = pos_err[1] / earth_param[1] / earth_param[4]
    ## simulate GPS error
    pos_noise = pos_err * np.random.randn(n, 3)
    vel_noise = gps_err['stdv'] * np.random.randn(n, 3)
    gps_mea = np.hstack([ref_gps[:, 0:3] + pos_noise,
                         ref_gps[:, 3:6] + vel_noise])
    return gps_mea",gps_type == 0,not gps_type
salt,https://github.com/saltstack/salt/tree/master/salt/client/ssh/ssh_py_shim.py,,main$278,"def main(argv):  # pylint: disable=W0613
    """"""
    Main program body
    """"""
    thin_path = os.path.join(OPTIONS.saltdir, THIN_ARCHIVE)
    if os.path.isfile(thin_path):
        if OPTIONS.checksum != get_hash(thin_path, OPTIONS.hashfunc):
            need_deployment()
        unpack_thin(thin_path)
        # Salt thin now is available to use
    else:
        if not sys.platform.startswith(""win""):
            scpstat = subprocess.Popen([""/bin/sh"", ""-c"", ""command -v scp""]).wait()
            if scpstat != 0:
                sys.exit(EX_SCP_NOT_FOUND)

        if os.path.exists(OPTIONS.saltdir) and not os.path.isdir(OPTIONS.saltdir):
            sys.stderr.write(
                'ERROR: salt path ""{0}"" exists but is not a directory\n'.format(
                    OPTIONS.saltdir
                )
            )
            sys.exit(EX_CANTCREAT)

        if not os.path.exists(OPTIONS.saltdir):
            need_deployment()

        code_checksum_path = os.path.normpath(
            os.path.join(OPTIONS.saltdir, ""code-checksum"")
        )
        if not os.path.exists(code_checksum_path) or not os.path.isfile(
            code_checksum_path
        ):
            sys.stderr.write(
                ""WARNING: Unable to locate current code checksum: {0}.\n"".format(
                    code_checksum_path
                )
            )
            need_deployment()
        with open(code_checksum_path, ""r"") as vpo:
            cur_code_cs = vpo.readline().strip()
        if cur_code_cs != OPTIONS.code_checksum:
            sys.stderr.write(
                ""WARNING: current code checksum {0} is different to {1}.\n"".format(
                    cur_code_cs, OPTIONS.code_checksum
                )
            )
            need_deployment()
        # Salt thin exists and is up-to-date - fall through and use it

    salt_call_path = os.path.join(OPTIONS.saltdir, ""salt-call"")
    if not os.path.isfile(salt_call_path):
        sys.stderr.write('ERROR: thin is missing ""{0}""\n'.format(salt_call_path))
        need_deployment()

    with open(os.path.join(OPTIONS.saltdir, ""minion""), ""w"") as config:
        config.write(OPTIONS.config + ""\n"")
    if OPTIONS.ext_mods:
        ext_path = os.path.join(OPTIONS.saltdir, EXT_ARCHIVE)
        if os.path.exists(ext_path):
            unpack_ext(ext_path)
        else:
            version_path = os.path.join(OPTIONS.saltdir, ""ext_version"")
            if not os.path.exists(version_path) or not os.path.isfile(version_path):
                need_ext()
            with open(version_path, ""r"") as vpo:
                cur_version = vpo.readline().strip()
            if cur_version != OPTIONS.ext_mods:
                need_ext()
    # Fix parameter passing issue
    if len(ARGS) == 1:
        argv_prepared = ARGS[0].split()
    else:
        argv_prepared = ARGS

    salt_argv = [
        get_executable(),
        salt_call_path,
        ""--retcode-passthrough"",
        ""--local"",
        ""--metadata"",
        ""--out"",
        ""json"",
        ""-l"",
        ""quiet"",
        ""-c"",
        OPTIONS.saltdir,
    ]

    try:
        if argv_prepared[-1].startswith(""--no-parse=""):
            salt_argv.append(argv_prepared.pop(-1))
    except (IndexError, TypeError):
        pass

    salt_argv.append(""--"")
    salt_argv.extend(argv_prepared)

    sys.stderr.write(""SALT_ARGV: {0}\n"".format(salt_argv))

    # Only emit the delimiter on *both* stdout and stderr when completely successful.
    # Yes, the flush() is necessary.
    sys.stdout.write(OPTIONS.delimiter + ""\n"")
    sys.stdout.flush()
    if not OPTIONS.tty:
        sys.stderr.write(OPTIONS.delimiter + ""\n"")
        sys.stderr.flush()
    if OPTIONS.cmd_umask is not None:
        old_umask = os.umask(OPTIONS.cmd_umask)  # pylint: disable=blacklisted-function
    if OPTIONS.tty:
        proc = subprocess.Popen(
            salt_argv, stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )
        # Returns bytes instead of string on python 3
        stdout, _ = proc.communicate()
        sys.stdout.write(
            stdout.decode(encoding=get_system_encoding(), errors=""replace"")
        )
        sys.stdout.flush()
        retcode = proc.returncode
        if OPTIONS.wipe:
            shutil.rmtree(OPTIONS.saltdir)
    elif OPTIONS.wipe:
        retcode = subprocess.call(salt_argv)
        shutil.rmtree(OPTIONS.saltdir)
    else:
        retcode = subprocess.call(salt_argv)
    if OPTIONS.cmd_umask is not None:
        os.umask(old_umask)  # pylint: disable=blacklisted-function
    return retcode",scpstat != 0,scpstat
eralchemy,https://github.com/Alexis-benoist/eralchemy/tree/master/eralchemy/helpers.py,,check_args$11,"def check_args(args):
    """"""Checks that the args are coherent.""""""
    check_args_has_attributes(args)
    if args.v:
        non_version_attrs = [v for k, v in args.__dict__.items() if k != 'v']
        print('non_version_attrs', non_version_attrs)
        if len([v for v in non_version_attrs if v is not None]) != 0:
            fail('Cannot show the version number with another command.')
        return
    if args.i is None:
        fail('Cannot draw ER diagram of no database.')
    if args.o is None:
        fail('Cannot draw ER diagram with no output file.')",len([v for v in non_version_attrs if v is not None]) != 0,[v for v in non_version_attrs if v is not None]
mmdetection3d,https://github.com/open-mmlab/mmdetection3d/tree/master/mmdet3d/models/necks/imvoxel_neck.py,IndoorImVoxelNeck,_make_layer$119,"def _make_layer(stride, n_channels, n_blocks):
        """"""Make a layer from several residual blocks.

        Args:
            stride (int): Stride of the first residual block.
            n_channels (int): Number of channels of the first residual block.
            n_blocks (int): Number of residual blocks.

        Returns:
            torch.nn.Module: With several residual blocks.
        """"""
        blocks = []
        for i in range(n_blocks):
            if i == 0 and stride != 1:
                blocks.append(ResModule(n_channels, n_channels * 2, stride))
                n_channels = n_channels * 2
            else:
                blocks.append(ResModule(n_channels, n_channels))
        return nn.Sequential(*blocks)",i == 0,not i
cms,https://github.com/amfoss/cms/tree/master/cms/io/rpc.py,RemoteServiceServer,run$325,"def run(self):
        """"""Start listening for requests, and go on forever.

        Read messages from the socket and issue greenlets to parse
        them, execute methods and send the response to the client.
        This method won't return as long as there's something to read,
        it's therefore advisable to spawn a greenlet to call it.

        """"""
        while True:
            try:
                data = self._read()
            except OSError:
                break

            if len(data) == 0:
                self.finalize(""Connection closed."")
                break

            gevent.spawn(self.process_data, data)",len(data) == 0,not data
JioNLP,https://github.com/dongrixinyu/JioNLP/tree/master/jionlp/textaug/swap_char_position.py,SwapCharPosition,_prepare$44,"def _prepare(self, swap_ratio=0.03, seed=1, scale=1.0):
        self.random = random
        self.seed = seed
        self.scale = scale
        if seed != 0:
            self.random.seed(seed)
        self.swap_ratio = swap_ratio",seed != 0,seed
lux,https://github.com/lux-org/lux/tree/master/tests_sql/test_sql_interestingness.py,,test_interestingness_0_1_1$40,"def test_interestingness_0_1_1(global_var):
    tbl = lux.LuxSQLTable()
    tbl.set_SQL_table(""cars"")

    tbl.set_intent(
        [
            lux.Clause(attribute=""origin"", filter_op=""="", value=""?""),
            lux.Clause(attribute=""milespergal""),
        ]
    )
    tbl._repr_html_()
    assert interestingness(tbl.recommendation[""Current Vis""][0], tbl) != None
    assert str(tbl.recommendation[""Current Vis""][0]._inferred_intent[2].value) == ""USA""
    tbl.clear_intent()","interestingness(tbl.recommendation['Current Vis'][0], tbl) != None","interestingness(tbl.recommendation['Current Vis'][0], tbl)"
mmf,https://github.com/facebookresearch/mmf/tree/master/mmf/datasets/processors/processors.py,VQAAnswerProcessor,__call__$590,"def __call__(self, item):
        """"""Takes in dict with answers or answers_tokens, and returns back
        a dict with answers (processed), ""answers_indices"" which point to
        indices of the answers if present and ""answers_scores"" which represent
        VQA style scores for the answers.

        Args:
            item (Dict): Dict containing answers or answers_tokens

        Returns:
            Dict: Processed answers, indices and scores.

        """"""
        tokens = []

        if not isinstance(item, dict):
            raise TypeError(""'item' passed to processor must be a dict"")

        if ""answer_tokens"" in item:
            tokens = item[""answer_tokens""]
        elif ""answers"" in item and item[""answers""] is not None:
            if self.preprocessor is None:
                raise AssertionError(
                    ""'preprocessor' must be defined if you ""
                    ""don't pass 'answer_tokens'""
                )

            tokens = [
                self.preprocessor({""text"": answer})[""text""]
                for answer in item[""answers""]
            ]
        else:
            raise AssertionError(
                ""'answers' or 'answer_tokens' must be passed""
                "" to answer processor in a dict""
            )

        if len(tokens) != 0:
            tokens = self._increase_to_ten(tokens)

        answers_indices = torch.zeros(self.DEFAULT_NUM_ANSWERS, dtype=torch.long)
        answers_indices.fill_(self.answer_vocab.get_unk_index())

        for idx, token in enumerate(tokens):
            answers_indices[idx] = self.answer_vocab.word2idx(token)

        answers_scores = self.compute_answers_scores(answers_indices)

        return {
            ""answers"": tokens,
            ""answers_indices"": answers_indices,
            ""answers_scores"": answers_scores,
        }",len(tokens) != 0,tokens
powerfulseal,https://github.com/powerfulseal/powerfulseal/tree/master/tests/metriccollectors/test_prometheus_collector.py,,test_add_filtered_to_empty_set_metric$276,"def test_add_filtered_to_empty_set_metric(prometheus_noop_scenario):
    before = REGISTRY.get_sample_value(FILTERED_TO_EMPTY_SET_METRIC_NAME)
    assert before == 0

    prometheus_noop_scenario.execute()

    after = REGISTRY.get_sample_value(FILTERED_TO_EMPTY_SET_METRIC_NAME)
    assert after == 1",before == 0,not before
integrations-core,https://github.com/DataDog/integrations-core/tree/master/kubelet/tests/test_common.py,,test_credentials_token_noverify$192,"def test_credentials_token_noverify():
    expected_headers = {'Authorization': 'Bearer mytoken'}
    creds = KubeletCredentials(
        {""verify_tls"": ""false"", ""ca_cert"": ""ca_cert"", ""client_crt"": ""ignore_me"", ""token"": ""mytoken""}
    )
    assert creds.verify() is False
    assert creds.cert_pair() is None
    assert creds.headers(""https://dummy"") == expected_headers
    # Make sure we don't leak the token over http
    assert creds.headers(""http://dummy"") is None

    instance = {'prometheus_url': 'https://dummy', 'namespace': 'foo'}
    scraper = OpenMetricsBaseCheck('prometheus', {}, [instance])
    scraper_config = scraper.create_scraper_configuration(instance)
    creds.configure_scraper(scraper_config)
    assert scraper_config['ssl_ca_cert'] is False
    assert scraper_config['ssl_cert'] is None
    assert scraper_config['ssl_private_key'] is None
    assert scraper_config['extra_headers'] == expected_headers

    # Make sure we don't leak the token over http
    scraper_config['prometheus_url'] = ""http://dummy""
    creds.configure_scraper(scraper_config)
    assert scraper_config['ssl_ca_cert'] is False
    assert scraper_config['ssl_cert'] is None
    assert scraper_config['ssl_private_key'] is None
    assert scraper_config['extra_headers'] == {}",scraper_config['extra_headers'] == {},not scraper_config['extra_headers']
commix,https://github.com/commixproject/commix/tree/master/src/core/shells/reverse_tcp.py,,configure_reverse_tcp$702,"def configure_reverse_tcp(separator):
  # Set up LHOST for the reverse TCP connection
  while True:
    sys.stdout.write(settings.REVERSE_TCP_SHELL)
    option = _input()
    if option.lower() == ""reverse_tcp"": 
      warn_msg = ""You are into the '"" + option.lower() + ""' mode.""
      print(settings.print_warning_msg(warn_msg))
      continue
    if option.lower() == ""?"": 
      menu.reverse_tcp_options()
      continue
    if option.lower() == ""quit"": 
      raise SystemExit()
    elif option.lower() == ""os_shell"" or option.lower() == ""back"": 
      settings.REVERSE_TCP = False   
      break 
    elif option.lower() == ""bind_tcp"":
      settings.BIND_TCP = True
      settings.REVERSE_TCP = False
      break 
    elif len(settings.LPORT) != 0 and len(settings.LHOST) != 0:
      break 
    elif option[0:4].lower() == ""set "":
      if option[4:10].lower() == ""lhost "":
        if check_lhost(option[10:]):
          if len(settings.LPORT) == 0:
            pass
          else:
            break
        else:
          continue
      elif option[4:10].lower() == ""rhost "":
        err_msg =  ""The '"" + option[4:9].upper() + ""' option, is not ""
        err_msg += ""usable for 'reverse_tcp' mode. Use 'LHOST' option.""
        print(settings.print_error_msg(err_msg))  
        continue  
      elif option[4:10].lower() == ""lport "":
        if check_lport(option[10:]):
          if len(settings.LHOST) == 0:
            pass
          else:
            break
        else:
          continue
      elif option[4:12].lower() == ""srvport "":
        check_srvport(option[12:])
      elif option[4:12].lower() == ""uripath "":
        check_uripath(option[12:])
      else:
        common.invalid_option(option)
        pass
    else:
      common.invalid_option(option)
      pass",len(settings.LPORT) != 0,settings.LPORT
commix,https://github.com/commixproject/commix/tree/master/src/core/shells/reverse_tcp.py,,configure_reverse_tcp$702,"def configure_reverse_tcp(separator):
  # Set up LHOST for the reverse TCP connection
  while True:
    sys.stdout.write(settings.REVERSE_TCP_SHELL)
    option = _input()
    if option.lower() == ""reverse_tcp"": 
      warn_msg = ""You are into the '"" + option.lower() + ""' mode.""
      print(settings.print_warning_msg(warn_msg))
      continue
    if option.lower() == ""?"": 
      menu.reverse_tcp_options()
      continue
    if option.lower() == ""quit"": 
      raise SystemExit()
    elif option.lower() == ""os_shell"" or option.lower() == ""back"": 
      settings.REVERSE_TCP = False   
      break 
    elif option.lower() == ""bind_tcp"":
      settings.BIND_TCP = True
      settings.REVERSE_TCP = False
      break 
    elif len(settings.LPORT) != 0 and len(settings.LHOST) != 0:
      break 
    elif option[0:4].lower() == ""set "":
      if option[4:10].lower() == ""lhost "":
        if check_lhost(option[10:]):
          if len(settings.LPORT) == 0:
            pass
          else:
            break
        else:
          continue
      elif option[4:10].lower() == ""rhost "":
        err_msg =  ""The '"" + option[4:9].upper() + ""' option, is not ""
        err_msg += ""usable for 'reverse_tcp' mode. Use 'LHOST' option.""
        print(settings.print_error_msg(err_msg))  
        continue  
      elif option[4:10].lower() == ""lport "":
        if check_lport(option[10:]):
          if len(settings.LHOST) == 0:
            pass
          else:
            break
        else:
          continue
      elif option[4:12].lower() == ""srvport "":
        check_srvport(option[12:])
      elif option[4:12].lower() == ""uripath "":
        check_uripath(option[12:])
      else:
        common.invalid_option(option)
        pass
    else:
      common.invalid_option(option)
      pass",len(settings.LHOST) != 0,settings.LHOST
commix,https://github.com/commixproject/commix/tree/master/src/core/shells/reverse_tcp.py,,configure_reverse_tcp$702,"def configure_reverse_tcp(separator):
  # Set up LHOST for the reverse TCP connection
  while True:
    sys.stdout.write(settings.REVERSE_TCP_SHELL)
    option = _input()
    if option.lower() == ""reverse_tcp"": 
      warn_msg = ""You are into the '"" + option.lower() + ""' mode.""
      print(settings.print_warning_msg(warn_msg))
      continue
    if option.lower() == ""?"": 
      menu.reverse_tcp_options()
      continue
    if option.lower() == ""quit"": 
      raise SystemExit()
    elif option.lower() == ""os_shell"" or option.lower() == ""back"": 
      settings.REVERSE_TCP = False   
      break 
    elif option.lower() == ""bind_tcp"":
      settings.BIND_TCP = True
      settings.REVERSE_TCP = False
      break 
    elif len(settings.LPORT) != 0 and len(settings.LHOST) != 0:
      break 
    elif option[0:4].lower() == ""set "":
      if option[4:10].lower() == ""lhost "":
        if check_lhost(option[10:]):
          if len(settings.LPORT) == 0:
            pass
          else:
            break
        else:
          continue
      elif option[4:10].lower() == ""rhost "":
        err_msg =  ""The '"" + option[4:9].upper() + ""' option, is not ""
        err_msg += ""usable for 'reverse_tcp' mode. Use 'LHOST' option.""
        print(settings.print_error_msg(err_msg))  
        continue  
      elif option[4:10].lower() == ""lport "":
        if check_lport(option[10:]):
          if len(settings.LHOST) == 0:
            pass
          else:
            break
        else:
          continue
      elif option[4:12].lower() == ""srvport "":
        check_srvport(option[12:])
      elif option[4:12].lower() == ""uripath "":
        check_uripath(option[12:])
      else:
        common.invalid_option(option)
        pass
    else:
      common.invalid_option(option)
      pass",len(settings.LPORT) == 0,not settings.LPORT
commix,https://github.com/commixproject/commix/tree/master/src/core/shells/reverse_tcp.py,,configure_reverse_tcp$702,"def configure_reverse_tcp(separator):
  # Set up LHOST for the reverse TCP connection
  while True:
    sys.stdout.write(settings.REVERSE_TCP_SHELL)
    option = _input()
    if option.lower() == ""reverse_tcp"": 
      warn_msg = ""You are into the '"" + option.lower() + ""' mode.""
      print(settings.print_warning_msg(warn_msg))
      continue
    if option.lower() == ""?"": 
      menu.reverse_tcp_options()
      continue
    if option.lower() == ""quit"": 
      raise SystemExit()
    elif option.lower() == ""os_shell"" or option.lower() == ""back"": 
      settings.REVERSE_TCP = False   
      break 
    elif option.lower() == ""bind_tcp"":
      settings.BIND_TCP = True
      settings.REVERSE_TCP = False
      break 
    elif len(settings.LPORT) != 0 and len(settings.LHOST) != 0:
      break 
    elif option[0:4].lower() == ""set "":
      if option[4:10].lower() == ""lhost "":
        if check_lhost(option[10:]):
          if len(settings.LPORT) == 0:
            pass
          else:
            break
        else:
          continue
      elif option[4:10].lower() == ""rhost "":
        err_msg =  ""The '"" + option[4:9].upper() + ""' option, is not ""
        err_msg += ""usable for 'reverse_tcp' mode. Use 'LHOST' option.""
        print(settings.print_error_msg(err_msg))  
        continue  
      elif option[4:10].lower() == ""lport "":
        if check_lport(option[10:]):
          if len(settings.LHOST) == 0:
            pass
          else:
            break
        else:
          continue
      elif option[4:12].lower() == ""srvport "":
        check_srvport(option[12:])
      elif option[4:12].lower() == ""uripath "":
        check_uripath(option[12:])
      else:
        common.invalid_option(option)
        pass
    else:
      common.invalid_option(option)
      pass",len(settings.LHOST) == 0,not settings.LHOST
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/blocks/layers.py,Embedding,_create_variables$405,"def _create_variables(self):
    if self.input_type.ndim != 0:
      raise TypeError('Embeddings take scalar inputs.')
    dtype = tf.as_dtype(self.input_type.dtype)
    if not dtype.is_integer: raise TypeError('Embeddings take integer inputs.')
    if dtype not in (tf.int32, tf.int64):  # only dtypes supported by tf.gather
      if np.iinfo(dtype.as_numpy_dtype).max > 2147483647:
         # pedantic future-proofing to handle hypothetical tf.uint64
        raise TypeError('cannot gather or upcast dtype %s' % dtype)
      self._cast = True
    else:
      self._cast = False
    self._weights = tf.get_variable(
        'weights', self._weights_shape, initializer=self._initializer,
        trainable=self._trainable)",self.input_type.ndim != 0,self.input_type.ndim
core,https://github.com/home-assistant/core/tree/master/homeassistant/util/dt.py,,find_next_time_expression_time$258,"def find_next_time_expression_time(
    now: dt.datetime,  # pylint: disable=redefined-outer-name
    seconds: list[int],
    minutes: list[int],
    hours: list[int],
) -> dt.datetime:
    """"""Find the next datetime from now for which the time expression matches.

    The algorithm looks at each time unit separately and tries to find the
    next one that matches for each. If any of them would roll over, all
    time units below that are reset to the first matching value.

    Timezones are also handled (the tzinfo of the now object is used),
    including daylight saving time.
    """"""
    if not seconds or not minutes or not hours:
        raise ValueError(""Cannot find a next time: Time expression never matches!"")

    while True:
        # Reset microseconds and fold; fold (for ambiguous DST times) will be handled later
        result = now.replace(microsecond=0, fold=0)

        # Match next second
        if (next_second := _lower_bound(seconds, result.second)) is None:
            # No second to match in this minute. Roll-over to next minute.
            next_second = seconds[0]
            result += dt.timedelta(minutes=1)

        result = result.replace(second=next_second)

        # Match next minute
        next_minute = _lower_bound(minutes, result.minute)
        if next_minute != result.minute:
            # We're in the next minute. Seconds needs to be reset.
            result = result.replace(second=seconds[0])

        if next_minute is None:
            # No minute to match in this hour. Roll-over to next hour.
            next_minute = minutes[0]
            result += dt.timedelta(hours=1)

        result = result.replace(minute=next_minute)

        # Match next hour
        next_hour = _lower_bound(hours, result.hour)
        if next_hour != result.hour:
            # We're in the next hour. Seconds+minutes needs to be reset.
            result = result.replace(second=seconds[0], minute=minutes[0])

        if next_hour is None:
            # No minute to match in this day. Roll-over to next day.
            next_hour = hours[0]
            result += dt.timedelta(days=1)

        result = result.replace(hour=next_hour)

        if result.tzinfo in (None, UTC):
            # Using UTC, no DST checking needed
            return result

        if not _datetime_exists(result):
            # When entering DST and clocks are turned forward.
            # There are wall clock times that don't ""exist"" (an hour is skipped).

            # -> trigger on the next time that 1. matches the pattern and 2. does exist
            # for example:
            #   on 2021.03.28 02:00:00 in CET timezone clocks are turned forward an hour
            #   with pattern ""02:30"", don't run on 28 mar (such a wall time does not exist on this day)
            #   instead run at 02:30 the next day

            # We solve this edge case by just iterating one second until the result exists
            # (max. 3600 operations, which should be fine for an edge case that happens once a year)
            now += dt.timedelta(seconds=1)
            continue

        now_is_ambiguous = _datetime_ambiguous(now)
        result_is_ambiguous = _datetime_ambiguous(result)

        # When leaving DST and clocks are turned backward.
        # Then there are wall clock times that are ambiguous i.e. exist with DST and without DST
        # The logic above does not take into account if a given pattern matches _twice_
        # in a day.
        # Example: on 2021.10.31 02:00:00 in CET timezone clocks are turned backward an hour

        if now_is_ambiguous and result_is_ambiguous:
            # `now` and `result` are both ambiguous, so the next match happens
            # _within_ the current fold.

            # Examples:
            #  1. 2021.10.31 02:00:00+02:00 with pattern 02:30 -> 2021.10.31 02:30:00+02:00
            #  2. 2021.10.31 02:00:00+01:00 with pattern 02:30 -> 2021.10.31 02:30:00+01:00
            return result.replace(fold=now.fold)

        if now_is_ambiguous and now.fold == 0 and not result_is_ambiguous:
            # `now` is in the first fold, but result is not ambiguous (meaning it no longer matches
            # within the fold).
            # -> Check if result matches in the next fold. If so, emit that match

            # Turn back the time by the DST offset, effectively run the algorithm on the first fold
            # If it matches on the first fold, that means it will also match on the second one.

            # Example: 2021.10.31 02:45:00+02:00 with pattern 02:30 -> 2021.10.31 02:30:00+01:00

            check_result = find_next_time_expression_time(
                now + _dst_offset_diff(now), seconds, minutes, hours
            )
            if _datetime_ambiguous(check_result):
                return check_result.replace(fold=1)

        return result",now.fold == 0,not now.fold
pytorch-image-models,https://github.com/rwightman/pytorch-image-models/tree/master/timm/models/layers/evo_norm.py,EvoNorm2dS2,__init__$307,"def __init__(
            self, num_features, groups=32, group_size=None,
            apply_act=True, act_layer=None, eps=1e-5, **_):
        super().__init__()
        act_layer = act_layer or nn.SiLU
        self.apply_act = apply_act  # apply activation (non-linearity)
        if act_layer is not None and apply_act:
            self.act = create_act_layer(act_layer)
        else:
            self.act = nn.Identity()
        if group_size:
            assert num_features % group_size == 0
            self.groups = num_features // group_size
        else:
            self.groups = groups
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        self.reset_parameters()",num_features % group_size == 0,not num_features % group_size
saleor,https://github.com/saleor/saleor/tree/master/saleor/checkout/tests/test_checkout_complete.py,,test_create_order_preauth_payment_creates_expected_events_anonymous_user$443,"def test_create_order_preauth_payment_creates_expected_events_anonymous_user(
    mock_notify,
    checkout_with_item,
    customer_user,
    shipping_method,
    payment_txn_preauth,
    channel_USD,
    site_settings,
):
    checkout = checkout_with_item
    checkout_user = None

    # Ensure not events are existing prior
    assert not OrderEvent.objects.exists()
    assert not CustomerEvent.objects.exists()

    # Prepare valid checkout
    checkout.user = checkout_user
    checkout.email = ""test@example.com""
    checkout.billing_address = customer_user.default_billing_address
    checkout.shipping_address = customer_user.default_shipping_address
    checkout.shipping_method = shipping_method
    checkout.payments.add(payment_txn_preauth)
    checkout.tracking_code = ""tracking_code""
    checkout.redirect_url = ""https://www.example.com""
    checkout.save()

    # Place checkout
    manager = get_plugins_manager()
    lines, _ = fetch_checkout_lines(checkout)
    checkout_info = fetch_checkout_info(checkout, lines, [], manager)
    order = _create_order(
        checkout_info=checkout_info,
        checkout_lines=lines,
        order_data=_prepare_order_data(
            manager=manager,
            checkout_info=checkout_info,
            lines=lines,
            discounts=[],
            prices_entered_with_tax=True,
        ),
        user=None,
        app=None,
        manager=manager,
    )
    flush_post_commit_hooks()

    (
        order_placed_event,
        payment_captured_event,
        order_confirmed_event,
    ) = order.events.all()  # type: OrderEvent

    # Ensure the correct order event was created
    # is the event the expected type
    assert order_placed_event.type == OrderEvents.PLACED
    # is the user anonymous/ the customer
    assert order_placed_event.user == checkout_user
    # is the associated backref order valid
    assert order_placed_event.order is order
    # ensure a date was set
    assert order_placed_event.date
    # should not have any additional parameters
    assert not order_placed_event.parameters

    # Ensure the correct order event was created
    # is the event the expected type
    assert payment_captured_event.type == OrderEvents.PAYMENT_AUTHORIZED
    # is the user anonymous/ the customer
    assert payment_captured_event.user == checkout_user
    # is the associated backref order valid
    assert payment_captured_event.order is order
    # ensure a date was set
    assert payment_captured_event.date
    # should not have any additional parameters
    assert ""amount"" in payment_captured_event.parameters.keys()
    assert ""payment_id"" in payment_captured_event.parameters.keys()
    assert ""payment_gateway"" in payment_captured_event.parameters.keys()

    expected_payload = {
        ""order"": get_default_order_payload(order, checkout.redirect_url),
        ""recipient_email"": order.get_customer_email(),
        **get_site_context_payload(site_settings.site),
    }
    # Ensure the correct order confirmed event was created
    # should be order confirmed event
    assert order_confirmed_event.type == OrderEvents.CONFIRMED
    # ensure the user is checkout user
    assert order_confirmed_event.user == checkout_user
    # ensure the order confirmed event is related to order
    assert order_confirmed_event.order is order
    # ensure a date was set
    assert order_confirmed_event.date
    # ensure the event parameters are empty
    assert order_confirmed_event.parameters == {}

    mock_notify.assert_called_once_with(
        NotifyEventType.ORDER_CONFIRMATION,
        expected_payload,
        channel_slug=channel_USD.slug,
    )

    # Check no event was created if the user was anonymous
    assert not CustomerEvent.objects.exists()",order_confirmed_event.parameters == {},not order_confirmed_event.parameters
betamax,https://github.com/betamaxpy/betamax/tree/master/tests/unit/test_configure.py,TestConfiguration,test_registers_pre_record_hooks$56,"def test_registers_pre_record_hooks(self):
        c = Configuration()
        assert Cassette.hooks['before_record'] == []
        c.before_record(callback=lambda: None)
        assert Cassette.hooks['before_record'] != []
        assert len(Cassette.hooks['before_record']) == 1
        assert callable(Cassette.hooks['before_record'][0])",Cassette.hooks['before_record'] == [],not Cassette.hooks['before_record']
betamax,https://github.com/betamaxpy/betamax/tree/master/tests/unit/test_configure.py,TestConfiguration,test_registers_pre_record_hooks$56,"def test_registers_pre_record_hooks(self):
        c = Configuration()
        assert Cassette.hooks['before_record'] == []
        c.before_record(callback=lambda: None)
        assert Cassette.hooks['before_record'] != []
        assert len(Cassette.hooks['before_record']) == 1
        assert callable(Cassette.hooks['before_record'][0])",Cassette.hooks['before_record'] != [],Cassette.hooks['before_record']
optuna,https://github.com/optuna/optuna/tree/master/optuna/samplers/_tpe/parzen_estimator.py,_ParzenEstimator,_calculate_categorical_params$364,"def _calculate_categorical_params(
        self, observations: np.ndarray, param_name: str
    ) -> np.ndarray:

        # TODO(kstoneriv3): This the bandwidth selection rule might not be optimal.
        observations = observations.astype(int)
        n_observations = self._n_observations
        consider_prior = self._parameters.consider_prior
        prior_weight = self._parameters.prior_weight
        distribution = self._search_space[param_name]
        assert isinstance(distribution, distributions.CategoricalDistribution)
        choices = distribution.choices

        if n_observations == 0:
            consider_prior = True

        if consider_prior:
            shape = (n_observations + 1, len(choices))
            assert prior_weight is not None
            value = prior_weight / (n_observations + 1)
        else:
            shape = (n_observations, len(choices))
            assert prior_weight is not None
            value = prior_weight / n_observations
        weights = np.full(shape, fill_value=value)
        weights[np.arange(n_observations), observations] += 1
        weights /= weights.sum(axis=1, keepdims=True)
        return weights",n_observations == 0,not n_observations
coding-problems,https://github.com/MTrajK/coding-problems/tree/master/Trees/find_second_largest_node.py,,traverse_tree$30,"def traverse_tree(node, arr):
    if node == None:
        return

    if arr[0].val < node.val:
        arr[1] = arr[0]
        arr[0] = node
    elif arr[1].val < node.val:
        arr[1] = node

    # search left
    traverse_tree(node.left, arr)
    # search right
    traverse_tree(node.right, arr)",node == None,not node
great_expectations,https://github.com/great-expectations/great_expectations/tree/master/great_expectations/validator/validation_graph.py,ValidationGraph,build_metric_dependency_graph$98,"def build_metric_dependency_graph(
        self,
        metric_configuration: MetricConfiguration,
        runtime_configuration: Optional[dict] = None,
    ) -> None:
        """"""
        Obtain domain and value keys for metrics and proceeds to add these metrics to the validation graph
        until all metrics have been added.

        Args:
            metric_configuration: Desired MetricConfiguration object to be resolved.
            runtime_configuration: Additional run-time settings (see ""Validator.DEFAULT_RUNTIME_CONFIGURATION"").
        """"""

        metric_impl_klass: MetricProvider
        metric_provider: Callable
        (
            metric_impl_klass,
            metric_provider,
        ) = self.set_metric_configuration_default_kwargs_if_absent(
            metric_configuration=metric_configuration
        )

        metric_dependencies = metric_impl_klass.get_evaluation_dependencies(
            metric=metric_configuration,
            execution_engine=self._execution_engine,
            runtime_configuration=runtime_configuration,
        )

        if len(metric_dependencies) == 0:
            self.add(
                MetricEdge(
                    left=metric_configuration,
                )
            )
        else:
            metric_configuration.metric_dependencies = metric_dependencies
            for metric_dependency in metric_dependencies.values():
                # TODO: <Alex>In the future, provide a more robust cycle detection mechanism.</Alex>
                if metric_dependency.id == metric_configuration.id:
                    logger.warning(
                        f""Metric {str(metric_configuration.id)} has created a circular dependency""
                    )
                    continue
                self.add(
                    MetricEdge(
                        left=metric_configuration,
                        right=metric_dependency,
                    )
                )
                self.build_metric_dependency_graph(
                    metric_configuration=metric_dependency,
                    runtime_configuration=runtime_configuration,
                )",len(metric_dependencies) == 0,not metric_dependencies
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/ClientServices.py,ServiceIPFS,PinDirectory$2971,"def PinDirectory( self, hashes, note ):
        
        job_key = ClientThreading.JobKey( pausable = True, cancellable = True )
        
        job_key.SetStatusTitle( 'creating ipfs directory on ' + self._name )
        
        HG.client_controller.pub( 'message', job_key )
        
        try:
            
            file_info = []
            
            hashes = sorted( hashes )
            
            for ( i, hash ) in enumerate( hashes ):
                
                ( i_paused, should_quit ) = job_key.WaitIfNeeded()
                
                if should_quit:
                    
                    job_key.SetVariable( 'popup_text_1', 'cancelled!' )
                    
                    return
                    
                
                job_key.SetVariable( 'popup_text_1', 'ensuring files are pinned: ' + HydrusData.ConvertValueRangeToPrettyString( i + 1, len( hashes ) ) )
                job_key.SetVariable( 'popup_gauge_1', ( i + 1, len( hashes ) ) )
                
                media_result = HG.client_controller.Read( 'media_result', hash )
                
                mime = media_result.GetMime()
                
                result = HG.client_controller.Read( 'service_filenames', self._service_key, { hash } )
                
                if len( result ) == 0:
                    
                    try:
                        
                        multihash = self.PinFile( hash, mime )
                        
                    except HydrusExceptions.DataMissing:
                        
                        HydrusData.ShowText( 'File {} could not be pinned!'.format( hash.hex() ) )
                        
                        continue
                        
                    
                else:
                    
                    ( multihash, ) = result
                    
                
                file_info.append( ( hash, mime, multihash ) )
                
            
            with self._lock:
                
                api_base_url = self._GetAPIBaseURL()
                
            
            url = api_base_url + 'object/new?arg=unixfs-dir'
            
            network_job = ClientNetworkingJobs.NetworkJobIPFS( url )
            
            HG.client_controller.network_engine.AddJob( network_job )
            
            network_job.WaitUntilDone()
            
            parsing_text = network_job.GetContentText()
            
            response_json = json.loads( parsing_text )
            
            for ( i, ( hash, mime, multihash ) ) in enumerate( file_info ):
                
                ( i_paused, should_quit ) = job_key.WaitIfNeeded()
                
                if should_quit:
                    
                    job_key.SetVariable( 'popup_text_1', 'cancelled!' )
                    
                    return
                    
                
                job_key.SetVariable( 'popup_text_1', 'creating directory: ' + HydrusData.ConvertValueRangeToPrettyString( i + 1, len( file_info ) ) )
                job_key.SetVariable( 'popup_gauge_1', ( i + 1, len( file_info ) ) )
                
                object_multihash = response_json[ 'Hash' ]
                
                filename = hash.hex() + HC.mime_ext_lookup[ mime ]
                
                url = api_base_url + 'object/patch/add-link?arg=' + object_multihash + '&arg=' + filename + '&arg=' + multihash
                
                network_job = ClientNetworkingJobs.NetworkJobIPFS( url )
                
                HG.client_controller.network_engine.AddJob( network_job )
                
                network_job.WaitUntilDone()
                
                parsing_text = network_job.GetContentText()
                
                response_json = json.loads( parsing_text )
                
            
            directory_multihash = response_json[ 'Hash' ]
            
            url = api_base_url + 'pin/add?arg=' + directory_multihash
            
            network_job = ClientNetworkingJobs.NetworkJobIPFS( url )
            
            HG.client_controller.network_engine.AddJob( network_job )
            
            network_job.WaitUntilDone()
            
            content_update_row = ( hashes, directory_multihash, note )
            
            content_updates = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_DIRECTORIES, HC.CONTENT_UPDATE_ADD, content_update_row ) ]
            
            HG.client_controller.WriteSynchronous( 'content_updates', { self._service_key : content_updates } )
            
            job_key.SetVariable( 'popup_text_1', 'done!' )
            
            with self._lock:
                
                text = self._multihash_prefix + directory_multihash
                
            
            job_key.SetVariable( 'popup_clipboard', ( 'copy multihash to clipboard', text ) )
            
            return directory_multihash
            
        except Exception as e:
            
            HydrusData.ShowException( e )
            
            job_key.SetErrorException( e )
            
            job_key.Cancel()
            
        finally:
            
            job_key.DeleteVariable( 'popup_gauge_1' )
            
            job_key.Finish()",len(result) == 0,not result
luminaire,https://github.com/zillow/luminaire/tree/master/luminaire/tests/test_exploration.py,TestDataExploration,test_data_change_detection$31,"def test_data_change_detection(self, change_test_data):

        de_obj = DataExploration()
        changepoint_output = de_obj._pelt_change_point_detection(change_test_data, 'raw', 21, 3 * 365)
        trendchange_output = de_obj._trend_changes(change_test_data, 'raw')

        assert isinstance(changepoint_output[1], list) and len(changepoint_output[1]) > 0
        assert isinstance(trendchange_output, list) and len(trendchange_output) == 0",len(trendchange_output) == 0,not trendchange_output
open-event-server,https://github.com/OpnTec/open-event-server/tree/master/tests/all/integration/api/helpers/order/test_validate_tickets.py,,test_validate_empty_tickets$12,"def test_validate_empty_tickets():
    assert validate_tickets([]) == []",validate_tickets([]) == [],not validate_tickets([])
CLUENER2020,https://github.com/CLUEbenchmark/CLUENER2020/tree/master/pytorch_version/models/transformers/tokenization_transfo_xl.py,TransfoXLTokenizer,count_file$108,"def count_file(self, path, verbose=False, add_eos=False):
        if verbose: logger.info('counting file {} ...'.format(path))
        assert os.path.exists(path)

        sents = []
        with open(path, 'r', encoding='utf-8') as f:
            for idx, line in enumerate(f):
                if verbose and idx > 0 and idx % 500000 == 0:
                    logger.info('    line {}'.format(idx))
                symbols = self.tokenize(line, add_eos=add_eos)
                self.counter.update(symbols)
                sents.append(symbols)

        return sents",idx % 500000 == 0,not idx % 500000
python_data_structures_and_algorithms,https://github.com/PegasusWang/python_data_structures_and_algorithms/tree/master/docs/04_闃熷垪/queue.py,,test_queue$144,"def test_queue():
    q = Queue()
    q.push(0)
    q.push(1)
    q.push(2)

    assert len(q) == 3

    assert q.pop() == 0
    assert q.pop() == 1
    assert q.pop() == 2

    import pytest    # pip install pytest
    with pytest.raises(EmptyError) as excinfo:   # 鎴戜滑鏉ユ祴璇曟槸鍚︾湡鐨勬姏鍑轰簡寮傚父
        q.pop()   # 缁х画璋冪敤浼氭姏鍑哄紓甯
    assert 'empty queue' == str(excinfo.value)",q.pop() == 0,not q.pop()
pyelftools,https://github.com/eliben/pyelftools/tree/master/elftools/elf/hash.py,ELFHashTable,get_symbol$42,"def get_symbol(self, name):
        """""" Look up a symbol from this hash table with the given name.
        """"""
        if self.params['nbuckets'] == 0:
            return None
        hval = self.elf_hash(name) % self.params['nbuckets']
        symndx = self.params['buckets'][hval]
        while symndx != 0:
            sym = self._symboltable.get_symbol(symndx)
            if sym.name == name:
                return sym
            symndx = self.params['chains'][symndx]
        return None",self.params['nbuckets'] == 0,not self.params['nbuckets']
pyelftools,https://github.com/eliben/pyelftools/tree/master/elftools/elf/hash.py,ELFHashTable,get_symbol$42,"def get_symbol(self, name):
        """""" Look up a symbol from this hash table with the given name.
        """"""
        if self.params['nbuckets'] == 0:
            return None
        hval = self.elf_hash(name) % self.params['nbuckets']
        symndx = self.params['buckets'][hval]
        while symndx != 0:
            sym = self._symboltable.get_symbol(symndx)
            if sym.name == name:
                return sym
            symndx = self.params['chains'][symndx]
        return None",symndx != 0,symndx
coding-problems,https://github.com/MTrajK/coding-problems/tree/master/Other/valid_parentheses.py,,is_valid$24,"def is_valid(string):
    closing = {
        '}': '{',
        ']': '[',
        ')': '('
    }
    stack = deque()

    for char in string:
        if char in closing:
            if len(stack) == 0:
                return False

            last = stack.pop()
            if last != closing[char]:
                return False
        else:
            stack.append(char)

    return True",len(stack) == 0,not stack
Sorcar,https://github.com/aachman98/Sorcar/tree/master//addon_updater_ops.py,addon_updater_update_target,poll$341,"def poll(cls, context):
		if updater.invalidupdater == True: return False
		return updater.update_ready != None and len(updater.tags)>0",updater.invalidupdater == True,updater.invalidupdater
Sorcar,https://github.com/aachman98/Sorcar/tree/master//addon_updater_ops.py,addon_updater_update_target,poll$341,"def poll(cls, context):
		if updater.invalidupdater == True: return False
		return updater.update_ready != None and len(updater.tags)>0",updater.update_ready != None,updater.update_ready
retopoflow,https://github.com/CGCookie/retopoflow/tree/master/retopoflow/rf/rf_target.py,RetopoFlow_Target,update_rot_object$912,"def update_rot_object(self):
        bbox = self.rftarget.get_selection_bbox()
        if bbox.min == None:
            #bbox = BBox.merge(src.get_bbox() for src in self.rfsources)
            bboxes = []
            for s in self.rfsources:
                verts = [matrix_vector_mult(s.obj.matrix_world, Vector((v[0], v[1], v[2], 1))) for v in s.obj.bound_box]
                verts = [(v[0]/v[3], v[1]/v[3], v[2]/v[3]) for v in verts]
                bboxes.append(BBox(from_coords=verts))
            bbox = BBox.merge(bboxes)
        # print('update_rot_object', bbox)
        diff = bbox.max - bbox.min
        rot_object = bpy.data.objects[options['rotate object']]
        rot_object.location = bbox.min + diff / 2
        rot_object.scale = diff / 2",bbox.min == None,not bbox.min
FALdetector,https://github.com/PeterWang512/FALdetector/tree/master/utils/tools.py,,resize_shorter_side$30,"def resize_shorter_side(img, min_length):
    """"""
    Resize the shorter side of img to min_length while
    preserving the aspect ratio.
    """"""
    ow, oh = img.size
    mult = 8
    if ow < oh:
        if ow == min_length and oh % mult == 0:
            return img, (ow, oh)
        w = min_length
        h = int(min_length * oh / ow)
    else:
        if oh == min_length and ow % mult == 0:
            return img, (ow, oh)
        h = min_length
        w = int(min_length * ow / oh)
    return img.resize((w, h), Image.BICUBIC), (w, h)",oh % mult == 0,not oh % mult
FALdetector,https://github.com/PeterWang512/FALdetector/tree/master/utils/tools.py,,resize_shorter_side$30,"def resize_shorter_side(img, min_length):
    """"""
    Resize the shorter side of img to min_length while
    preserving the aspect ratio.
    """"""
    ow, oh = img.size
    mult = 8
    if ow < oh:
        if ow == min_length and oh % mult == 0:
            return img, (ow, oh)
        w = min_length
        h = int(min_length * oh / ow)
    else:
        if oh == min_length and ow % mult == 0:
            return img, (ow, oh)
        h = min_length
        w = int(min_length * ow / oh)
    return img.resize((w, h), Image.BICUBIC), (w, h)",ow % mult == 0,not ow % mult
YOLOF,https://github.com/megvii-model/YOLOF/tree/master/cvpods/modeling/meta_arch/semantic_seg.py,SemSegFPNHead,layers$157,"def layers(self, features):
        for i, f in enumerate(self.in_features):
            if i == 0:
                x = self.scale_heads[i](features[f])
            else:
                x = x + self.scale_heads[i](features[f])
        x = self.predictor(x)
        return x",i == 0,not i
px,https://github.com/genotrance/px/tree/master//test.py,,quitTest$362,"def quitTest(cmd, port):
    if not checkPxStart(""localhost"", port):
        writeflush(""Px did not start\n"")
        return False

    writeflush(""cmd: "" + cmd + ""--quit\n"")
    ret, data = exec(cmd + ""--quit"", port)
    if ret != 0 or ""Quitting Px .. DONE"" not in data:
        writeflush(""Failed: Unable to --quit Px: %d\n%s\n"" % (ret, data + ""\n""))
        return False

    return True",ret != 0,ret
fedlearner,https://github.com/bytedance/fedlearner/tree/master/fedlearner/data_join/rsa_psi/rsa_psi_component.py,FollowerPsiRsaSigner,_rpc_sign_callback$649,"def _rpc_sign_callback(self, ctx, stub, rpc_future):
        try:
            response = rpc_future.result()
            if response.status.code != 0:
                raise RuntimeError(""Failed to call rpc for psi sign, ""\
                                   ""error code: {}, error message: {}"".format(
                                        response.status.code,
                                        response.status.error_message))
            ctx.trigger_rpc_finished()
            self._add_sign_stats(ctx.rpc_sign_duration(),
                                 ctx.rpc_pending_duration(),
                                 ctx.retry_cnt)
            self._revert_stub(stub, False)
            signed_blinded_hashed_ids = [bytes2int(item) for
                                         item in response.signed_ids]
            assert len(ctx.raw_id_batch) == len(signed_blinded_hashed_ids)
            self._callback_submitter.submit(self._deblind_signed_id_func,
                                            ctx, signed_blinded_hashed_ids)
            next_ctxs = []
            with self._lock:
                assert self._flying_rpc_num > 0
                self._flying_rpc_num -= 1
                req_num = self._flying_sign_rpc_threshold - self._flying_rpc_num
                if req_num > 0:
                    next_ctxs = self._pending_rpc_sign_ctx[:req_num]
                    self._pending_rpc_sign_ctx = \
                            self._pending_rpc_sign_ctx[req_num:]
            for nctx in next_ctxs:
                self._rpc_sign_func(nctx)
        except Exception as e: # pylint: disable=broad-except
            self._revert_stub(stub, True)
            begin_index = ctx.raw_id_batch.begin_index
            end_index = begin_index + len(ctx.raw_id_batch)
            logging.warning(""psi signer batch[%d, %d) sign ""\
                            ""failed for %d times, reson:%s. ""\
                            ""retry again"", begin_index,
                            end_index, ctx.retry_cnt, e)
            with self._lock:
                assert self._flying_rpc_num > 0
                self._flying_rpc_num -= 1
            ctx.trigger_retry()
            self._rpc_sign_func(ctx)",response.status.code != 0,response.status.code
warehouse,https://github.com/pypa/warehouse/tree/master/tests/unit/test_forms.py,TestForm,test_empty_form_no_errors$90,"def test_empty_form_no_errors(self):
        form = Form()
        assert form.errors == {}",form.errors == {},not form.errors
py-junos-eznc,https://github.com/Juniper/py-junos-eznc/tree/master/lib/jnpr/junos/factory/state_machine.py,StateMachine,parse_title_data$628,"def parse_title_data(self, event):
        """"""
        title description in Table helps to get the starting point for starting
        search

        Args:
            event: In case trigger want to pass some data

        Returns: None, Just add corresponding key, value to existing dict

        Let say we have CLI ""show xmchip 0 pt stats"" output as:

        WAN PT statistics (Index 0)
        ---------------------------

        PCT entries used by all WI-1 streams         : 0
        PCT entries used by all WI-0 streams         : 0
        PCT entries used by all LI streams           : 0
        CPT entries used by all multicast packets    : 0
        CPT entries used by all WI-1 streams         : 0
        CPT entries used by all WI-0 streams         : 0
        CPT entries used by all LI streams           : 0

        Fabric PT statistics (Index 1)
        ------------------------------

        PCT entries used by all FI streams           : 0
        PCT entries used by all WI (Unused) streams  : 0
        PCT entries used by all LI streams           : 0
        CPT entries used by all multicast packets    : 0
        CPT entries used by all FI streams           : 0
        CPT entries used by all WI (Unused) streams  : 0
        CPT entries used by all LI streams           : 0

        With below table/view to parse the data
        ---
        XMChipStatsTable:
          command: show xmchip 0 pt stats
          target: fpc1
          view: XMChipStatsView

        XMChipStatsView:
          fields:
            wan_pt_stats: _WANPTStatTable
            fabric_pt_stats: _FabricPTStatTable

        _WANPTStatTable:
          title: WAN PT statistics (Index 0)
          delimiter: "":""

        _FabricPTStatTable:
          title: Fabric PT statistics (Index 1)
          delimiter: "":""

        which returns:
        {'fabric_pt_stats': {'CPT entries used by all FI streams': 0,
                     'CPT entries used by all LI streams': 0,
                     'CPT entries used by all WI (Unused) streams': 0,
                     'CPT entries used by all multicast packets': 0,
                     'PCT entries used by all FI streams': 0,
                     'PCT entries used by all LI streams': 0,
                     'PCT entries used by all WI (Unused) streams': 0},
        'wan_pt_stats': {'CPT entries used by all LI streams': 0,
                     'CPT entries used by all WI-0 streams': 0,
                     'CPT entries used by all WI-1 streams': 0,
                     'CPT entries used by all multicast packets': 0,
                     'PCT entries used by all LI streams': 0,
                     'PCT entries used by all WI-0 streams': 0,
                     'PCT entries used by all WI-1 streams': 0}}
        """"""
        if self._view is not None and self._view.REGEX != {}:
            return self.regex_provided()
        # view have only fields, so contains only nested table
        if (
            self._view is not None
            and self._view.FIELDS
            and not self._view.COLUMNS
            and not self._view.REGEX
        ):
            return
        delimiter = self._table.DELIMITER or r""\s\s+""
        pre_space_delimit = self._get_pre_space_delimiter(self._lines[1])
        for line in self._lines[1:]:
            if re.match(pre_space_delimit + r""\s+"", line):
                break
            if line.startswith(pre_space_delimit):
                try:
                    items = re.split(delimiter, line.strip())
                    item_types = list(map(data_type, items))
                    key, value = convert_to_data_type(items)
                    if self._view is None:
                        self._data[key] = value
                    elif self._table.KEY_ITEMS is None:
                        self._data[self._view.FIELDS.get(key, key)] = value
                    elif key in self._table.KEY_ITEMS:
                        self._data[self._view.FIELDS.get(key, key)] = value
                except ValueError:
                    regex = (
                        r""(\d+)\s(.*)"" if item_types[0] == int else ""("" r"".*)\s(\d+)""
                    )
                    obj = re.search(regex, line)
                    if obj:
                        items = obj.groups()
                        key, value = convert_to_data_type(items)
                        self._data[key] = value
            # check if next line is blank or new title (delimiter test to fail)
            elif line.strip() == """" or len(re.split(delimiter, line.strip())) <= 1:
                break
        return self._data",self._view.REGEX != {},self._view.REGEX
scispacy,https://github.com/allenai/scispacy/tree/master/tests/test_abbreviation_detection.py,TestAbbreviationDetector,test_empty_span$168,"def test_empty_span(self):
        text = ""(19, 9, 4) Hadamard Designs and Their Residual Designs""
        doc = self.nlp(text)
        doc2 = self.detector(doc)
        assert len(doc2._.abbreviations) == 0",len(doc2._.abbreviations) == 0,not doc2._.abbreviations
Terminus,https://github.com/randy3k/Terminus/tree/master/terminus/ptty.py,TerminalScreen,erase_in_display$386,"def erase_in_display(self, how=0, *args, **kwargs):
        # dump the screen to history
        # check also https://github.com/selectel/pyte/pull/108

        if not self.alternate_buffer_mode and \
                (how == 2 or (how == 0 and self.cursor.x == 0 and self.cursor.y == 0)):
            self.push_lines_into_history()

        if how == 0:
            interval = range(self.cursor.y + 1, self.lines)
        elif how == 1:
            interval = range(self.cursor.y)
        elif how == 2 or how == 3:
            interval = range(self.lines)

        self.dirty.update(interval)
        for y in interval:
            line = self.buffer[y]
            for i, x in list(enumerate(line)):
                if i < self.columns:
                    line[x] = self.cursor.attrs
                else:
                    line.pop(x, None)

        if how == 0 or how == 1:
            self.erase_in_line(how)

        if how == 3:
            self.history.clear()
            self._clear_callback()",how == 0,not how
Terminus,https://github.com/randy3k/Terminus/tree/master/terminus/ptty.py,TerminalScreen,erase_in_display$386,"def erase_in_display(self, how=0, *args, **kwargs):
        # dump the screen to history
        # check also https://github.com/selectel/pyte/pull/108

        if not self.alternate_buffer_mode and \
                (how == 2 or (how == 0 and self.cursor.x == 0 and self.cursor.y == 0)):
            self.push_lines_into_history()

        if how == 0:
            interval = range(self.cursor.y + 1, self.lines)
        elif how == 1:
            interval = range(self.cursor.y)
        elif how == 2 or how == 3:
            interval = range(self.lines)

        self.dirty.update(interval)
        for y in interval:
            line = self.buffer[y]
            for i, x in list(enumerate(line)):
                if i < self.columns:
                    line[x] = self.cursor.attrs
                else:
                    line.pop(x, None)

        if how == 0 or how == 1:
            self.erase_in_line(how)

        if how == 3:
            self.history.clear()
            self._clear_callback()",how == 0,not how
Terminus,https://github.com/randy3k/Terminus/tree/master/terminus/ptty.py,TerminalScreen,erase_in_display$386,"def erase_in_display(self, how=0, *args, **kwargs):
        # dump the screen to history
        # check also https://github.com/selectel/pyte/pull/108

        if not self.alternate_buffer_mode and \
                (how == 2 or (how == 0 and self.cursor.x == 0 and self.cursor.y == 0)):
            self.push_lines_into_history()

        if how == 0:
            interval = range(self.cursor.y + 1, self.lines)
        elif how == 1:
            interval = range(self.cursor.y)
        elif how == 2 or how == 3:
            interval = range(self.lines)

        self.dirty.update(interval)
        for y in interval:
            line = self.buffer[y]
            for i, x in list(enumerate(line)):
                if i < self.columns:
                    line[x] = self.cursor.attrs
                else:
                    line.pop(x, None)

        if how == 0 or how == 1:
            self.erase_in_line(how)

        if how == 3:
            self.history.clear()
            self._clear_callback()",how == 0,not how
Terminus,https://github.com/randy3k/Terminus/tree/master/terminus/ptty.py,TerminalScreen,erase_in_display$386,"def erase_in_display(self, how=0, *args, **kwargs):
        # dump the screen to history
        # check also https://github.com/selectel/pyte/pull/108

        if not self.alternate_buffer_mode and \
                (how == 2 or (how == 0 and self.cursor.x == 0 and self.cursor.y == 0)):
            self.push_lines_into_history()

        if how == 0:
            interval = range(self.cursor.y + 1, self.lines)
        elif how == 1:
            interval = range(self.cursor.y)
        elif how == 2 or how == 3:
            interval = range(self.lines)

        self.dirty.update(interval)
        for y in interval:
            line = self.buffer[y]
            for i, x in list(enumerate(line)):
                if i < self.columns:
                    line[x] = self.cursor.attrs
                else:
                    line.pop(x, None)

        if how == 0 or how == 1:
            self.erase_in_line(how)

        if how == 3:
            self.history.clear()
            self._clear_callback()",self.cursor.x == 0,not self.cursor.x
Terminus,https://github.com/randy3k/Terminus/tree/master/terminus/ptty.py,TerminalScreen,erase_in_display$386,"def erase_in_display(self, how=0, *args, **kwargs):
        # dump the screen to history
        # check also https://github.com/selectel/pyte/pull/108

        if not self.alternate_buffer_mode and \
                (how == 2 or (how == 0 and self.cursor.x == 0 and self.cursor.y == 0)):
            self.push_lines_into_history()

        if how == 0:
            interval = range(self.cursor.y + 1, self.lines)
        elif how == 1:
            interval = range(self.cursor.y)
        elif how == 2 or how == 3:
            interval = range(self.lines)

        self.dirty.update(interval)
        for y in interval:
            line = self.buffer[y]
            for i, x in list(enumerate(line)):
                if i < self.columns:
                    line[x] = self.cursor.attrs
                else:
                    line.pop(x, None)

        if how == 0 or how == 1:
            self.erase_in_line(how)

        if how == 3:
            self.history.clear()
            self._clear_callback()",self.cursor.y == 0,not self.cursor.y
doit,https://github.com/pydoit/doit/tree/master/tests/test_control.py,TestTaskDispatcher_get_next_node,test_to_run$677,"def test_to_run(self):
        tasks = {'t1': Task('t1', None, task_dep=['t2']),
                 't2': Task('t2', None),
                 }
        td = TaskDispatcher(tasks, [], None)
        to_run = ['t2', 't1']
        td._gen_node(None, 't1') # t1 was already created
        got = td._get_next_node([], to_run)
        assert isinstance(got, ExecNode)
        assert 't2' == got.task.name
        assert [] == to_run",[] == to_run,not to_run
sharding,https://github.com/ethereum/sharding/tree/master/tests/contract/test_registry_management.py,,test_instant_release_notary$207,"def test_instant_release_notary(smc_handler):  # noqa: F811
    w3 = smc_handler.web3

    notary_0 = NotaryAccount(0)

    # Register notary 0
    smc_handler.register_notary(private_key=notary_0.private_key)
    mine(w3, 1)
    does_notary_exist = smc_handler.does_notary_exist(notary_0.checksum_address)
    assert does_notary_exist
    notary_pool_length = smc_handler.notary_pool_len()
    assert notary_pool_length == 1

    # Fast foward
    fast_forward(smc_handler, 1)

    # Deregister notary 0
    smc_handler.deregister_notary(private_key=notary_0.private_key)
    mine(w3, 1)
    notary_pool_length = smc_handler.notary_pool_len()
    assert notary_pool_length == 0

    # Instant release notary 0
    tx_hash = smc_handler.release_notary(private_key=notary_0.private_key)
    mine(w3, 1)
    # Check registry remain the same and the transaction consume all gas
    # and no logs has been emitted
    does_notary_exist = smc_handler.does_notary_exist(notary_0.checksum_address)
    assert does_notary_exist
    assert len(w3.eth.getTransactionReceipt(tx_hash)['logs']) == 0",notary_pool_length == 0,not notary_pool_length
sharding,https://github.com/ethereum/sharding/tree/master/tests/contract/test_registry_management.py,,test_instant_release_notary$207,"def test_instant_release_notary(smc_handler):  # noqa: F811
    w3 = smc_handler.web3

    notary_0 = NotaryAccount(0)

    # Register notary 0
    smc_handler.register_notary(private_key=notary_0.private_key)
    mine(w3, 1)
    does_notary_exist = smc_handler.does_notary_exist(notary_0.checksum_address)
    assert does_notary_exist
    notary_pool_length = smc_handler.notary_pool_len()
    assert notary_pool_length == 1

    # Fast foward
    fast_forward(smc_handler, 1)

    # Deregister notary 0
    smc_handler.deregister_notary(private_key=notary_0.private_key)
    mine(w3, 1)
    notary_pool_length = smc_handler.notary_pool_len()
    assert notary_pool_length == 0

    # Instant release notary 0
    tx_hash = smc_handler.release_notary(private_key=notary_0.private_key)
    mine(w3, 1)
    # Check registry remain the same and the transaction consume all gas
    # and no logs has been emitted
    does_notary_exist = smc_handler.does_notary_exist(notary_0.checksum_address)
    assert does_notary_exist
    assert len(w3.eth.getTransactionReceipt(tx_hash)['logs']) == 0",len(w3.eth.getTransactionReceipt(tx_hash)['logs']) == 0,not w3.eth.getTransactionReceipt(tx_hash)['logs']
yandex-music-api,https://github.com/MarshalX/yandex-music-api/tree/master/tests/test_licence_text_part.py,TestLicenceTextPart,test_de_list_none$15,"def test_de_list_none(self, client):
        assert LicenceTextPart.de_list({}, client) == []","LicenceTextPart.de_list({}, client) == []","not LicenceTextPart.de_list({}, client)"
strictyaml,https://github.com/crdoconnor/strictyaml/tree/master/strictyaml/ruamel/scanner.py,Scanner,scan_plain$1545,"def scan_plain(self):
        # type: () -> Any
        # See the specification for details.
        # We add an additional restriction for the flow context:
        #   plain scalars in the flow context cannot contain ',', ': '  and '?'.
        # We also keep track of the `allow_simple_key` flag here.
        # Indentation rules are loosed for the flow context.
        srp = self.reader.peek
        srf = self.reader.forward
        chunks = []  # type: List[Any]
        start_mark = self.reader.get_mark()
        end_mark = start_mark
        indent = self.indent + 1
        # We allow zero indentation for scalars, but then we need to check for
        # document separators at the beginning of the line.
        # if indent == 0:
        #     indent = 1
        spaces = []  # type: List[Any]
        while True:
            length = 0
            if srp() == ""#"":
                break
            while True:
                ch = srp(length)
                if ch == "":"" and srp(length + 1) not in _THE_END_SPACE_TAB:
                    pass
                elif ch == ""?"" and self.scanner_processing_version != (1, 1):
                    pass
                elif (
                    ch in _THE_END_SPACE_TAB
                    or (
                        not self.flow_level
                        and ch == "":""
                        and srp(length + 1) in _THE_END_SPACE_TAB
                    )
                    or (self.flow_level and ch in "",:?[]{}"")
                ):
                    break
                length += 1
            # It's not clear what we should do with ':' in the flow context.
            if (
                self.flow_level
                and ch == "":""
                and srp(length + 1) not in ""\0 \t\r\n\x85\u2028\u2029,[]{}""
            ):
                srf(length)
                raise ScannerError(
                    ""while scanning a plain scalar"",
                    start_mark,
                    ""found unexpected ':'"",
                    self.reader.get_mark(),
                    ""Please check ""
                    ""http://pyyaml.org/wiki/YAMLColonInFlowContext ""
                    ""for details."",
                )
            if length == 0:
                break
            self.allow_simple_key = False
            chunks.extend(spaces)
            chunks.append(self.reader.prefix(length))
            srf(length)
            end_mark = self.reader.get_mark()
            spaces = self.scan_plain_spaces(indent, start_mark)
            if (
                not spaces
                or srp() == ""#""
                or (not self.flow_level and self.reader.column < indent)
            ):
                break

        token = ScalarToken("""".join(chunks), True, start_mark, end_mark)
        if spaces and spaces[0] == ""\n"":
            # Create a comment token to preserve the trailing line breaks.
            comment = CommentToken("""".join(spaces) + ""\n"", start_mark, end_mark)
            token.add_post_comment(comment)
        return token",length == 0,not length
OctConv,https://github.com/facebookresearch/OctConv/tree/master/utils/gluon/utils/resnetv2.py,_ResNetV2,__init__$117,"def __init__(self, block, layers, groups=1, multiplier=1.,
                 ratio=(0., 0., 0., 0.),
                 num_out=(256, 512, 1024, 2048),
                 num_mid=( 64, 128,  256,  512),
                 classes=1000, use_se=False, down_pos=0,
                 norm_kwargs=None, last_gamma=False, deep_stem=False,
                 final_drop=0., use_global_stats=False,
                 name_prefix='', **kwargs):
        super(_ResNetV2, self).__init__(prefix=name_prefix)
        assert last_gamma == False, ""last_gamma should be False for ResNetV2""
        norm_kwargs = norm_kwargs if norm_kwargs is not None else {}
        if use_global_stats:
            norm_kwargs['use_global_stats'] = True
        # initialize residual networks
        k = multiplier
        self.use_se = use_se
        self.groups = groups
        self.down_pos=down_pos
        self.norm_kwargs = norm_kwargs

        with self.name_scope():
            self.conv1 = gluon.nn.HybridSequential()
            if not deep_stem:
                self.conv1.add(gluon.nn.Conv2D(channels=int(k*64), kernel_size=7, padding=3, strides=2,
                                         use_bias=False, prefix='conv1_'))
                self.conv1.add(gluon.nn.BatchNorm(prefix='bn1_',
                                         **({} if norm_kwargs is None else norm_kwargs)))
                self.conv1.add(gluon.nn.Activation('relu'))
            else:
                self.conv1.add(gluon.nn.Conv2D(channels=int(k*32), kernel_size=3, padding=1, strides=2,
                                         use_bias=False, prefix='stem_conv1_'))
                self.conv1.add(gluon.nn.BatchNorm(prefix='stem_bn1_',
                                         **({} if norm_kwargs is None else norm_kwargs)))
                self.conv1.add(gluon.nn.Activation('relu'))
                self.conv1.add(gluon.nn.Conv2D(channels=int(k*32), kernel_size=3, padding=1, strides=1,
                                         use_bias=False, prefix='stem_conv2_'))
                self.conv1.add(gluon.nn.BatchNorm(prefix='stem_bn2_',
                                         **({} if norm_kwargs is None else norm_kwargs)))
                self.conv1.add(gluon.nn.Activation('relu'))
                self.conv1.add(gluon.nn.Conv2D(channels=int(k*64), kernel_size=3, padding=1, strides=1,
                                         use_bias=False, prefix='stem_conv3_'))
                self.conv1.add(gluon.nn.BatchNorm(prefix='stem_bn3_',
                                         **({} if norm_kwargs is None else norm_kwargs)))
                self.conv1.add(gluon.nn.Activation('relu'))
            # ------------------------------------------------------------------
            self.maxpool = gluon.nn.MaxPool2D(pool_size=3, strides=2, padding=1)
            # ------------------------------------------------------------------
            # customized convolution starts from this line
            self.inplanes = (int(k*64), -1) # convert to proposed data format
            self._make_layer(1, block, layers[0], int(k*num_out[0]), num_mid[0], ratio[0])
            self._make_layer(2, block, layers[1], int(k*num_out[1]), num_mid[1], ratio[1], strides=2)
            self._make_layer(3, block, layers[2], int(k*num_out[2]), num_mid[2], ratio[2], strides=2)
            self._make_layer(4, block, layers[3], int(k*num_out[3]), num_mid[3], ratio[3], strides=2)
            # ------------------------------------------------------------------
            self.tail = gluon.nn.HybridSequential()
            self.tail.add(gluon.nn.BatchNorm(prefix='tail-bn_',
                                    **({} if norm_kwargs is None else norm_kwargs)))
            self.tail.add(gluon.nn.Activation('relu'))
            # ------------------------------------------------------------------
            self.avgpool = gluon.nn.GlobalAvgPool2D()
            self.drop = gluon.nn.Dropout(final_drop) if final_drop > 0. else lambda x: (x)
            self.classifer = gluon.nn.Conv2D(in_channels=int(k*num_out[3]), channels=classes,
                                       kernel_size=1, prefix='classifier_')
            self.flat = gluon.nn.Flatten()",last_gamma == False,not last_gamma
apprise,https://github.com/caronc/apprise/tree/master/test/test_plugin_matrix.py,,test_plugin_matrix_rooms$542,"def test_plugin_matrix_rooms(mock_post, mock_get):
    """"""
    NotifyMatrix() Room Testing

    """"""
    # Disable Throttling to speed testing
    plugins.NotifyBase.request_rate_per_sec = 0

    response_obj = {
        # Registration
        'access_token': 'abcd1234',
        'user_id': '@apprise:localhost',
        'home_server': 'localhost',

        # For joined_room response
        'joined_rooms': ['!abc123:localhost', '!def456:localhost'],

        # For room joining
        'room_id': '!abc123:localhost',
    }

    # Default configuration
    request = mock.Mock()
    request.status_code = requests.codes.ok
    request.content = dumps(response_obj)
    mock_post.return_value = request
    mock_get.return_value = request

    obj = plugins.NotifyMatrix()
    assert isinstance(obj, plugins.NotifyMatrix) is True
    assert obj.access_token is None

    # Can't get room listing if we're not connnected
    assert obj._room_join('#abc123') is None

    assert obj._register() is True
    assert obj.access_token is not None

    assert obj._room_join('!abc123') == response_obj['room_id']
    # Use cache to get same results
    assert len(obj._room_cache) == 1
    assert obj._room_join('!abc123') == response_obj['room_id']

    obj._room_cache = {}
    assert obj._room_join('!abc123:localhost') == response_obj['room_id']
    # Use cache to get same results
    assert len(obj._room_cache) == 1
    assert obj._room_join('!abc123:localhost') == response_obj['room_id']

    obj._room_cache = {}
    assert obj._room_join('abc123') == response_obj['room_id']
    # Use cache to get same results
    assert len(obj._room_cache) == 1
    assert obj._room_join('abc123') == response_obj['room_id']

    obj._room_cache = {}
    assert obj._room_join('abc123:localhost') == response_obj['room_id']
    # Use cache to get same results
    assert len(obj._room_cache) == 1
    assert obj._room_join('abc123:localhost') == response_obj['room_id']

    obj._room_cache = {}
    assert obj._room_join('#abc123:localhost') == response_obj['room_id']
    # Use cache to get same results
    assert len(obj._room_cache) == 1
    assert obj._room_join('#abc123:localhost') == response_obj['room_id']

    obj._room_cache = {}
    assert obj._room_join('%') is None
    assert obj._room_join(None) is None

    # 403 response; this will push for a room creation for alias based rooms
    # and these will fail
    request.status_code = 403
    obj._room_cache = {}
    assert obj._room_join('!abc123') is None
    obj._room_cache = {}
    assert obj._room_join('!abc123:localhost') is None
    obj._room_cache = {}
    assert obj._room_join('abc123') is None
    obj._room_cache = {}
    assert obj._room_join('abc123:localhost') is None
    obj._room_cache = {}
    assert obj._room_join('#abc123:localhost') is None

    # Room creation
    request.status_code = requests.codes.ok
    obj = plugins.NotifyMatrix()
    assert isinstance(obj, plugins.NotifyMatrix) is True
    assert obj.access_token is None

    # Can't get room listing if we're not connnected
    assert obj._room_create('#abc123') is None

    assert obj._register() is True
    assert obj.access_token is not None

    # You can't add room_id's, they must be aliases
    assert obj._room_create('!abc123') is None
    assert obj._room_create('!abc123:localhost') is None
    obj._room_cache = {}
    assert obj._room_create('abc123') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_create('abc123:localhost') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_create('#abc123:localhost') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_create('%') is None
    assert obj._room_create(None) is None

    # 403 response; this will push for a room creation for alias based rooms
    # and these will fail
    request.status_code = 403
    obj._room_cache = {}
    assert obj._room_create('abc123') is None
    obj._room_cache = {}
    assert obj._room_create('abc123:localhost') is None
    obj._room_cache = {}
    assert obj._room_create('#abc123:localhost') is None

    request.status_code = 403
    request.content = dumps({
        u'errcode': u'M_ROOM_IN_USE',
        u'error': u'Room alias already taken',
    })
    obj._room_cache = {}
    # This causes us to look up a channel ID if we get a ROOM_IN_USE response
    assert obj._room_create('#abc123:localhost') is None

    # Room detection
    request.status_code = requests.codes.ok
    request.content = dumps(response_obj)
    obj = plugins.NotifyMatrix()
    assert isinstance(obj, plugins.NotifyMatrix) is True
    assert obj.access_token is None

    # No rooms if we're not connected
    response = obj._joined_rooms()
    assert isinstance(response, list) is True
    assert len(response) == 0

    # register our account
    assert obj._register() is True
    assert obj.access_token is not None

    response = obj._joined_rooms()
    assert isinstance(response, list) is True
    assert len(response) == len(response_obj['joined_rooms'])
    for r in response:
        assert r in response_obj['joined_rooms']

    request.status_code = 403
    response = obj._joined_rooms()
    assert isinstance(response, list) is True
    assert len(response) == 0

    # Room id lookup
    request.status_code = requests.codes.ok
    obj = plugins.NotifyMatrix()
    assert isinstance(obj, plugins.NotifyMatrix) is True
    assert obj.access_token is None

    # Can't get room listing if we're not connnected
    assert obj._room_id('#abc123') is None

    assert obj._register() is True
    assert obj.access_token is not None

    # You can't add room_id's, they must be aliases
    assert obj._room_id('!abc123') is None
    assert obj._room_id('!abc123:localhost') is None
    obj._room_cache = {}
    assert obj._room_id('abc123') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_id('abc123:localhost') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_id('#abc123:localhost') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_id('%') is None
    assert obj._room_id(None) is None

    # If we can't look the code up, we return None
    request.status_code = 403
    obj._room_cache = {}
    assert obj._room_id('#abc123:localhost') is None",len(response) == 0,not response
apprise,https://github.com/caronc/apprise/tree/master/test/test_plugin_matrix.py,,test_plugin_matrix_rooms$542,"def test_plugin_matrix_rooms(mock_post, mock_get):
    """"""
    NotifyMatrix() Room Testing

    """"""
    # Disable Throttling to speed testing
    plugins.NotifyBase.request_rate_per_sec = 0

    response_obj = {
        # Registration
        'access_token': 'abcd1234',
        'user_id': '@apprise:localhost',
        'home_server': 'localhost',

        # For joined_room response
        'joined_rooms': ['!abc123:localhost', '!def456:localhost'],

        # For room joining
        'room_id': '!abc123:localhost',
    }

    # Default configuration
    request = mock.Mock()
    request.status_code = requests.codes.ok
    request.content = dumps(response_obj)
    mock_post.return_value = request
    mock_get.return_value = request

    obj = plugins.NotifyMatrix()
    assert isinstance(obj, plugins.NotifyMatrix) is True
    assert obj.access_token is None

    # Can't get room listing if we're not connnected
    assert obj._room_join('#abc123') is None

    assert obj._register() is True
    assert obj.access_token is not None

    assert obj._room_join('!abc123') == response_obj['room_id']
    # Use cache to get same results
    assert len(obj._room_cache) == 1
    assert obj._room_join('!abc123') == response_obj['room_id']

    obj._room_cache = {}
    assert obj._room_join('!abc123:localhost') == response_obj['room_id']
    # Use cache to get same results
    assert len(obj._room_cache) == 1
    assert obj._room_join('!abc123:localhost') == response_obj['room_id']

    obj._room_cache = {}
    assert obj._room_join('abc123') == response_obj['room_id']
    # Use cache to get same results
    assert len(obj._room_cache) == 1
    assert obj._room_join('abc123') == response_obj['room_id']

    obj._room_cache = {}
    assert obj._room_join('abc123:localhost') == response_obj['room_id']
    # Use cache to get same results
    assert len(obj._room_cache) == 1
    assert obj._room_join('abc123:localhost') == response_obj['room_id']

    obj._room_cache = {}
    assert obj._room_join('#abc123:localhost') == response_obj['room_id']
    # Use cache to get same results
    assert len(obj._room_cache) == 1
    assert obj._room_join('#abc123:localhost') == response_obj['room_id']

    obj._room_cache = {}
    assert obj._room_join('%') is None
    assert obj._room_join(None) is None

    # 403 response; this will push for a room creation for alias based rooms
    # and these will fail
    request.status_code = 403
    obj._room_cache = {}
    assert obj._room_join('!abc123') is None
    obj._room_cache = {}
    assert obj._room_join('!abc123:localhost') is None
    obj._room_cache = {}
    assert obj._room_join('abc123') is None
    obj._room_cache = {}
    assert obj._room_join('abc123:localhost') is None
    obj._room_cache = {}
    assert obj._room_join('#abc123:localhost') is None

    # Room creation
    request.status_code = requests.codes.ok
    obj = plugins.NotifyMatrix()
    assert isinstance(obj, plugins.NotifyMatrix) is True
    assert obj.access_token is None

    # Can't get room listing if we're not connnected
    assert obj._room_create('#abc123') is None

    assert obj._register() is True
    assert obj.access_token is not None

    # You can't add room_id's, they must be aliases
    assert obj._room_create('!abc123') is None
    assert obj._room_create('!abc123:localhost') is None
    obj._room_cache = {}
    assert obj._room_create('abc123') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_create('abc123:localhost') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_create('#abc123:localhost') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_create('%') is None
    assert obj._room_create(None) is None

    # 403 response; this will push for a room creation for alias based rooms
    # and these will fail
    request.status_code = 403
    obj._room_cache = {}
    assert obj._room_create('abc123') is None
    obj._room_cache = {}
    assert obj._room_create('abc123:localhost') is None
    obj._room_cache = {}
    assert obj._room_create('#abc123:localhost') is None

    request.status_code = 403
    request.content = dumps({
        u'errcode': u'M_ROOM_IN_USE',
        u'error': u'Room alias already taken',
    })
    obj._room_cache = {}
    # This causes us to look up a channel ID if we get a ROOM_IN_USE response
    assert obj._room_create('#abc123:localhost') is None

    # Room detection
    request.status_code = requests.codes.ok
    request.content = dumps(response_obj)
    obj = plugins.NotifyMatrix()
    assert isinstance(obj, plugins.NotifyMatrix) is True
    assert obj.access_token is None

    # No rooms if we're not connected
    response = obj._joined_rooms()
    assert isinstance(response, list) is True
    assert len(response) == 0

    # register our account
    assert obj._register() is True
    assert obj.access_token is not None

    response = obj._joined_rooms()
    assert isinstance(response, list) is True
    assert len(response) == len(response_obj['joined_rooms'])
    for r in response:
        assert r in response_obj['joined_rooms']

    request.status_code = 403
    response = obj._joined_rooms()
    assert isinstance(response, list) is True
    assert len(response) == 0

    # Room id lookup
    request.status_code = requests.codes.ok
    obj = plugins.NotifyMatrix()
    assert isinstance(obj, plugins.NotifyMatrix) is True
    assert obj.access_token is None

    # Can't get room listing if we're not connnected
    assert obj._room_id('#abc123') is None

    assert obj._register() is True
    assert obj.access_token is not None

    # You can't add room_id's, they must be aliases
    assert obj._room_id('!abc123') is None
    assert obj._room_id('!abc123:localhost') is None
    obj._room_cache = {}
    assert obj._room_id('abc123') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_id('abc123:localhost') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_id('#abc123:localhost') == response_obj['room_id']
    obj._room_cache = {}
    assert obj._room_id('%') is None
    assert obj._room_id(None) is None

    # If we can't look the code up, we return None
    request.status_code = 403
    obj._room_cache = {}
    assert obj._room_id('#abc123:localhost') is None",len(response) == 0,not response
passpie,https://github.com/marcwebbie/passpie/tree/master/tests/test_cli.py,CliAddTests,test_add_credentials_with_copy_copy_to_clipboard$185,"def test_add_credentials_with_copy_copy_to_clipboard(self, mocker, mock_config, irunner):
        mock_genpass = mocker.patch('passpie.cli.genpass', return_value='random')
        mock_copy = mocker.patch('passpie.cli.clipboard.copy')

        with mock_config() as cfg:
            pattern = cfg['genpass_pattern']
            result = irunner.invoke(cli.cli, ['add', ""fullname@name"", '--random', '--copy'])

            assert result.exit_code == 0
            assert mock_copy.called is True",result.exit_code == 0,not result.exit_code
gluoncv-torch,https://github.com/StacyYang/gluoncv-torch/tree/master/gluoncvth/models/wideresnet.py,WideResNet,__init__$118,"def __init__(self, structure, norm_act=ABN, classes=1000, dilation=False):
        """"""Wider ResNet with pre-activation (identity mapping) blocks
        This variant uses down-sampling by max-pooling in the first two blocks and by strided convolution in the others.
        Parameters
        ----------
        structure : list of int
            Number of residual blocks in each of the six modules of the network.
        norm_act : callable
            Function to create normalization / activation Module.
        classes : int
            If not `0` also include global average pooling and a fully-connected layer with `classes` outputs at the end
            of the network.
        dilation : bool
            If `True` apply dilation to the last three modules and change the down-sampling factor from 32 to 8.
        """"""
        super(WideResNet, self).__init__()
        self.structure = structure
        self.dilation = dilation

        if len(structure) != 6:
            raise ValueError(""Expected a structure with six values"")

        # Initial layers
        self.mod1 = nn.Sequential(OrderedDict([
            (""conv1"", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))
        ]))

        # Groups of residual blocks
        in_channels = 64
        channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048), (1024, 2048, 4096)]
        for mod_id, num in enumerate(structure):
            # Create blocks for module
            blocks = []
            for block_id in range(num):
                if not dilation:
                    dil = 1
                    stride = 2 if block_id == 0 and 2 <= mod_id <= 4 else 1
                else:
                    if mod_id == 3:
                        dil = 2
                    elif mod_id > 3:
                        dil = 4
                    else:
                        dil = 1
                    stride = 2 if block_id == 0 and mod_id == 2 else 1

                if mod_id == 4:
                    drop = partial(nn.Dropout2d, p=0.3)
                elif mod_id == 5:
                    drop = partial(nn.Dropout2d, p=0.5)
                else:
                    drop = None

                blocks.append((
                    ""block%d"" % (block_id + 1),
                    IdentityResidualBlock(in_channels, channels[mod_id], norm_act=norm_act, stride=stride, dilation=dil,
                                          dropout=drop)
                ))

                # Update channels and p_keep
                in_channels = channels[mod_id][-1]

            # Create module
            if mod_id < 2:
                self.add_module(""pool%d"" % (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))
            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))

        # Pooling and predictor
        self.bn_out = norm_act(in_channels)
        if classes != 0:
            self.classifier = nn.Sequential(OrderedDict([
                (""avg_pool"", GlobalAvgPool2d()),
                (""fc"", nn.Linear(in_channels, classes))
            ]))",classes != 0,classes
gluoncv-torch,https://github.com/StacyYang/gluoncv-torch/tree/master/gluoncvth/models/wideresnet.py,WideResNet,__init__$118,"def __init__(self, structure, norm_act=ABN, classes=1000, dilation=False):
        """"""Wider ResNet with pre-activation (identity mapping) blocks
        This variant uses down-sampling by max-pooling in the first two blocks and by strided convolution in the others.
        Parameters
        ----------
        structure : list of int
            Number of residual blocks in each of the six modules of the network.
        norm_act : callable
            Function to create normalization / activation Module.
        classes : int
            If not `0` also include global average pooling and a fully-connected layer with `classes` outputs at the end
            of the network.
        dilation : bool
            If `True` apply dilation to the last three modules and change the down-sampling factor from 32 to 8.
        """"""
        super(WideResNet, self).__init__()
        self.structure = structure
        self.dilation = dilation

        if len(structure) != 6:
            raise ValueError(""Expected a structure with six values"")

        # Initial layers
        self.mod1 = nn.Sequential(OrderedDict([
            (""conv1"", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))
        ]))

        # Groups of residual blocks
        in_channels = 64
        channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048), (1024, 2048, 4096)]
        for mod_id, num in enumerate(structure):
            # Create blocks for module
            blocks = []
            for block_id in range(num):
                if not dilation:
                    dil = 1
                    stride = 2 if block_id == 0 and 2 <= mod_id <= 4 else 1
                else:
                    if mod_id == 3:
                        dil = 2
                    elif mod_id > 3:
                        dil = 4
                    else:
                        dil = 1
                    stride = 2 if block_id == 0 and mod_id == 2 else 1

                if mod_id == 4:
                    drop = partial(nn.Dropout2d, p=0.3)
                elif mod_id == 5:
                    drop = partial(nn.Dropout2d, p=0.5)
                else:
                    drop = None

                blocks.append((
                    ""block%d"" % (block_id + 1),
                    IdentityResidualBlock(in_channels, channels[mod_id], norm_act=norm_act, stride=stride, dilation=dil,
                                          dropout=drop)
                ))

                # Update channels and p_keep
                in_channels = channels[mod_id][-1]

            # Create module
            if mod_id < 2:
                self.add_module(""pool%d"" % (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))
            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))

        # Pooling and predictor
        self.bn_out = norm_act(in_channels)
        if classes != 0:
            self.classifier = nn.Sequential(OrderedDict([
                (""avg_pool"", GlobalAvgPool2d()),
                (""fc"", nn.Linear(in_channels, classes))
            ]))",block_id == 0,not block_id
gluoncv-torch,https://github.com/StacyYang/gluoncv-torch/tree/master/gluoncvth/models/wideresnet.py,WideResNet,__init__$118,"def __init__(self, structure, norm_act=ABN, classes=1000, dilation=False):
        """"""Wider ResNet with pre-activation (identity mapping) blocks
        This variant uses down-sampling by max-pooling in the first two blocks and by strided convolution in the others.
        Parameters
        ----------
        structure : list of int
            Number of residual blocks in each of the six modules of the network.
        norm_act : callable
            Function to create normalization / activation Module.
        classes : int
            If not `0` also include global average pooling and a fully-connected layer with `classes` outputs at the end
            of the network.
        dilation : bool
            If `True` apply dilation to the last three modules and change the down-sampling factor from 32 to 8.
        """"""
        super(WideResNet, self).__init__()
        self.structure = structure
        self.dilation = dilation

        if len(structure) != 6:
            raise ValueError(""Expected a structure with six values"")

        # Initial layers
        self.mod1 = nn.Sequential(OrderedDict([
            (""conv1"", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))
        ]))

        # Groups of residual blocks
        in_channels = 64
        channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048), (1024, 2048, 4096)]
        for mod_id, num in enumerate(structure):
            # Create blocks for module
            blocks = []
            for block_id in range(num):
                if not dilation:
                    dil = 1
                    stride = 2 if block_id == 0 and 2 <= mod_id <= 4 else 1
                else:
                    if mod_id == 3:
                        dil = 2
                    elif mod_id > 3:
                        dil = 4
                    else:
                        dil = 1
                    stride = 2 if block_id == 0 and mod_id == 2 else 1

                if mod_id == 4:
                    drop = partial(nn.Dropout2d, p=0.3)
                elif mod_id == 5:
                    drop = partial(nn.Dropout2d, p=0.5)
                else:
                    drop = None

                blocks.append((
                    ""block%d"" % (block_id + 1),
                    IdentityResidualBlock(in_channels, channels[mod_id], norm_act=norm_act, stride=stride, dilation=dil,
                                          dropout=drop)
                ))

                # Update channels and p_keep
                in_channels = channels[mod_id][-1]

            # Create module
            if mod_id < 2:
                self.add_module(""pool%d"" % (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))
            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))

        # Pooling and predictor
        self.bn_out = norm_act(in_channels)
        if classes != 0:
            self.classifier = nn.Sequential(OrderedDict([
                (""avg_pool"", GlobalAvgPool2d()),
                (""fc"", nn.Linear(in_channels, classes))
            ]))",block_id == 0,not block_id
rewriting,https://github.com/davidbau/rewriting/tree/master/baselines/neural_best_buddies/util/util.py,,save_map_image$36,"def save_map_image(map_values, name, save_dir, level=0, binary_color=False):
    if level == 0:
        map_values = map_values
    else:
        scale_factor = int(math.pow(2,level-1))
        map_values = upsample_map(map_values, scale_factor)
    if binary_color==True:
        map_image = binary2color_image(map_values)
    else:
        map_image = map2image(map_values)
    save_image(map_image, os.path.join(save_dir, name + '.png'))",level == 0,not level
rewriting,https://github.com/davidbau/rewriting/tree/master/baselines/neural_best_buddies/util/util.py,,save_map_image$36,"def save_map_image(map_values, name, save_dir, level=0, binary_color=False):
    if level == 0:
        map_values = map_values
    else:
        scale_factor = int(math.pow(2,level-1))
        map_values = upsample_map(map_values, scale_factor)
    if binary_color==True:
        map_image = binary2color_image(map_values)
    else:
        map_image = map2image(map_values)
    save_image(map_image, os.path.join(save_dir, name + '.png'))",binary_color == True,binary_color
FoxDot,https://github.com/Qirky/FoxDot/tree/master/FoxDot/lib/Workspace/Simple/SimpleText.py,Text,get_current_block$20,"def get_current_block(self):
        """""" Returns the start and end rows of the current block """"""
        anchor, _ = self.get_insert()

        # Go forward
        row = anchor
        while True:
            text = self.GetLineText(row).strip()
            if len(text) == 0:
                end = row - 1
                break
            row += 1

        # Go backwards
        row = anchor
        while True:
            text = self.GetLineText(row).strip()
            if len(text) == 0:
                start = row + 1
                break
            row -= 1            
            
        return (start, end)",len(text) == 0,not text
FoxDot,https://github.com/Qirky/FoxDot/tree/master/FoxDot/lib/Workspace/Simple/SimpleText.py,Text,get_current_block$20,"def get_current_block(self):
        """""" Returns the start and end rows of the current block """"""
        anchor, _ = self.get_insert()

        # Go forward
        row = anchor
        while True:
            text = self.GetLineText(row).strip()
            if len(text) == 0:
                end = row - 1
                break
            row += 1

        # Go backwards
        row = anchor
        while True:
            text = self.GetLineText(row).strip()
            if len(text) == 0:
                start = row + 1
                break
            row -= 1            
            
        return (start, end)",len(text) == 0,not text
X2Paddle,https://github.com/PaddlePaddle/X2Paddle/tree/master/x2paddle/optimizer/pattern_matcher.py,PatternMatcher,get_subgraph$255,"def get_subgraph(pattern, graph, start_index):
            pattern_id2layers = pattern.get_global_layers()
            pattern_ids = list(pattern_id2layers.keys())
            pattern_layer_id = pattern_ids[0]
            subgraph_id2layers = dict()
            layer_id = list(graph.layers.keys())[start_index]
            graph_layers = graph.layers

            def update(layer_id, pattern_layer_id):
                layer = graph_layers[layer_id]
                pattern_layer = pattern_id2layers[pattern_layer_id]
                if layer.kernel != pattern_layer.kernel:
                    return False
                subgraph_id2layers[layer_id] = layer

            while len(subgraph_id2layers) != len(pattern_id2layers):
                out = update(layer_id, pattern_layer_id)
                if out == False:
                    return False
                else:
                    if len(subgraph_id2layers) == len(pattern_id2layers):
                        return subgraph_id2layers
                    else:
                        return False",out == False,not out
gluon-cv,https://github.com/dmlc/gluon-cv/tree/master/scripts/depth/trainer.py,Trainer,val$258,"def val(self):
        """"""Validate the model on a single minibatch
        """"""
        tbar = tqdm(self.val_loader)
        depth_metrics = {}
        abs_rel, sq_rel, rmse, rmse_log = 0, 0, 0, 0
        delta_1, delta_2, delta_3 = 0, 0, 0

        for metric in self.depth_metric_names:
            depth_metrics[metric] = 0
        for i, inputs in enumerate(tbar):
            outputs = self.process_batch(inputs, True)

            if ""depth_gt"" in inputs:
                self.compute_metrics(inputs, outputs, depth_metrics)

                # print evaluation results
                abs_rel = depth_metrics['de/abs_rel'] / (i + 1)
                sq_rel = depth_metrics['de/sq_rel'] / (i + 1)
                rmse = depth_metrics['de/rms'] / (i + 1)
                rmse_log = depth_metrics['de/log_rms'] / (i + 1)
                delta_1 = depth_metrics['da/a1'] / (i + 1)
                delta_2 = depth_metrics['da/a2'] / (i + 1)
                delta_3 = depth_metrics['da/a3'] / (i + 1)
                tbar.set_description(
                    'Epoch %d, validation '
                    'abs_REL: %.3f sq_REL: %.3f '
                    'RMSE: %.3f, RMSE_log: %.3f '
                    'Delta_1: %.3f Delta_2: %.3f Delta_2: %.3f' %
                    (self.epoch, abs_rel, sq_rel, rmse, rmse_log, delta_1, delta_2, delta_3))
            else:
                print(""Cannot find ground truth upon validation dataset!"")
                return
        self.logger.info(
            'Epoch %d, validation '
            'abs_REL: %.3f sq_REL: %.3f '
            'RMSE: %.3f, RMSE_log: %.3f '
            'Delta_1: %.3f Delta_2: %.3f Delta_2: %.3f' %
            (self.epoch, abs_rel, sq_rel, rmse, rmse_log, delta_1, delta_2, delta_3))

        mx.nd.waitall()
        if self.epoch % self.opt.save_frequency == 0:
            self.save_checkpoint(delta_1)

        if delta_1 > self.best_delta1:
            self.best_model = self.model
            self.best_delta1 = delta_1
            if self.use_pose_net:
                self.best_posenet = self.posenet",self.epoch % self.opt.save_frequency == 0,not self.epoch % self.opt.save_frequency
pandas,https://github.com/pandas-dev/pandas/tree/master/scripts/tests/test_validate_docstrings.py,TestMainFunction,test_no_exit_status_noerrors_for_validate_all$375,"def test_no_exit_status_noerrors_for_validate_all(self, monkeypatch):
        monkeypatch.setattr(
            validate_docstrings,
            ""validate_all"",
            lambda prefix, ignore_deprecated=False: {
                ""docstring1"": {""errors"": [], ""warnings"": [(""WN01"", ""warn desc"")]},
                ""docstring2"": {""errors"": []},
            },
        )
        exit_status = validate_docstrings.main(
            func_name=None,
            prefix=None,
            errors=[],
            output_format=""default"",
            ignore_deprecated=False,
        )
        assert exit_status == 0",exit_status == 0,not exit_status
django-oscar,https://github.com/django-oscar/django-oscar/tree/master/src/oscar/apps/offer/abstract_models.py,AbstractConditionalOffer,availability_restrictions$367,"def availability_restrictions(self):  # noqa (too complex (15))
        restrictions = []
        if self.is_suspended:
            restrictions.append({
                'description': _(""Offer is suspended""),
                'is_satisfied': False})

        if self.max_global_applications:
            remaining = self.max_global_applications - self.num_applications
            desc = _(""Limited to %(total)d uses (%(remainder)d remaining)"") \
                % {'total': self.max_global_applications,
                   'remainder': remaining}
            restrictions.append({'description': desc,
                                 'is_satisfied': remaining > 0})

        if self.max_user_applications:
            if self.max_user_applications == 1:
                desc = _(""Limited to 1 use per user"")
            else:
                desc = _(""Limited to %(total)d uses per user"") \
                    % {'total': self.max_user_applications}
            restrictions.append({'description': desc,
                                 'is_satisfied': True})

        if self.max_basket_applications:
            if self.max_user_applications == 1:
                desc = _(""Limited to 1 use per basket"")
            else:
                desc = _(""Limited to %(total)d uses per basket"") \
                    % {'total': self.max_basket_applications}
            restrictions.append({
                'description': desc,
                'is_satisfied': True})

        def hide_time_if_zero(dt):
            # Only show hours/minutes if they have been specified
            if dt.tzinfo:
                localtime = dt.astimezone(get_current_timezone())
            else:
                localtime = dt
            if localtime.hour == 0 and localtime.minute == 0:
                return date_filter(localtime, settings.DATE_FORMAT)
            return date_filter(localtime, settings.DATETIME_FORMAT)

        if self.start_datetime or self.end_datetime:
            today = now()
            if self.start_datetime and self.end_datetime:
                desc = _(""Available between %(start)s and %(end)s"") \
                    % {'start': hide_time_if_zero(self.start_datetime),
                       'end': hide_time_if_zero(self.end_datetime)}
                is_satisfied \
                    = self.start_datetime <= today <= self.end_datetime
            elif self.start_datetime:
                desc = _(""Available from %(start)s"") % {
                    'start': hide_time_if_zero(self.start_datetime)}
                is_satisfied = today >= self.start_datetime
            elif self.end_datetime:
                desc = _(""Available until %(end)s"") % {
                    'end': hide_time_if_zero(self.end_datetime)}
                is_satisfied = today <= self.end_datetime
            restrictions.append({
                'description': desc,
                'is_satisfied': is_satisfied})

        if self.max_discount:
            desc = _(""Limited to a cost of %(max)s"") % {
                'max': currency(self.max_discount)}
            restrictions.append({
                'description': desc,
                'is_satisfied': self.total_discount < self.max_discount})

        return restrictions",localtime.hour == 0,not localtime.hour
django-oscar,https://github.com/django-oscar/django-oscar/tree/master/src/oscar/apps/offer/abstract_models.py,AbstractConditionalOffer,availability_restrictions$367,"def availability_restrictions(self):  # noqa (too complex (15))
        restrictions = []
        if self.is_suspended:
            restrictions.append({
                'description': _(""Offer is suspended""),
                'is_satisfied': False})

        if self.max_global_applications:
            remaining = self.max_global_applications - self.num_applications
            desc = _(""Limited to %(total)d uses (%(remainder)d remaining)"") \
                % {'total': self.max_global_applications,
                   'remainder': remaining}
            restrictions.append({'description': desc,
                                 'is_satisfied': remaining > 0})

        if self.max_user_applications:
            if self.max_user_applications == 1:
                desc = _(""Limited to 1 use per user"")
            else:
                desc = _(""Limited to %(total)d uses per user"") \
                    % {'total': self.max_user_applications}
            restrictions.append({'description': desc,
                                 'is_satisfied': True})

        if self.max_basket_applications:
            if self.max_user_applications == 1:
                desc = _(""Limited to 1 use per basket"")
            else:
                desc = _(""Limited to %(total)d uses per basket"") \
                    % {'total': self.max_basket_applications}
            restrictions.append({
                'description': desc,
                'is_satisfied': True})

        def hide_time_if_zero(dt):
            # Only show hours/minutes if they have been specified
            if dt.tzinfo:
                localtime = dt.astimezone(get_current_timezone())
            else:
                localtime = dt
            if localtime.hour == 0 and localtime.minute == 0:
                return date_filter(localtime, settings.DATE_FORMAT)
            return date_filter(localtime, settings.DATETIME_FORMAT)

        if self.start_datetime or self.end_datetime:
            today = now()
            if self.start_datetime and self.end_datetime:
                desc = _(""Available between %(start)s and %(end)s"") \
                    % {'start': hide_time_if_zero(self.start_datetime),
                       'end': hide_time_if_zero(self.end_datetime)}
                is_satisfied \
                    = self.start_datetime <= today <= self.end_datetime
            elif self.start_datetime:
                desc = _(""Available from %(start)s"") % {
                    'start': hide_time_if_zero(self.start_datetime)}
                is_satisfied = today >= self.start_datetime
            elif self.end_datetime:
                desc = _(""Available until %(end)s"") % {
                    'end': hide_time_if_zero(self.end_datetime)}
                is_satisfied = today <= self.end_datetime
            restrictions.append({
                'description': desc,
                'is_satisfied': is_satisfied})

        if self.max_discount:
            desc = _(""Limited to a cost of %(max)s"") % {
                'max': currency(self.max_discount)}
            restrictions.append({
                'description': desc,
                'is_satisfied': self.total_discount < self.max_discount})

        return restrictions",localtime.minute == 0,not localtime.minute
PyVirtualDisplay,https://github.com/ponty/PyVirtualDisplay/tree/master/tests/test_core.py,,test_virt$13,"def test_virt():
    vd = Display()
    assert vd.return_code is None
    assert not vd.is_alive()
    vd.start()
    assert vd.return_code is None
    assert vd.is_alive()
    vd.stop()
    assert vd.return_code == 0
    assert not vd.is_alive()

    vd = Display().start().stop()
    assert vd.return_code == 0
    assert not vd.is_alive()",vd.return_code == 0,not vd.return_code
PyVirtualDisplay,https://github.com/ponty/PyVirtualDisplay/tree/master/tests/test_core.py,,test_virt$13,"def test_virt():
    vd = Display()
    assert vd.return_code is None
    assert not vd.is_alive()
    vd.start()
    assert vd.return_code is None
    assert vd.is_alive()
    vd.stop()
    assert vd.return_code == 0
    assert not vd.is_alive()

    vd = Display().start().stop()
    assert vd.return_code == 0
    assert not vd.is_alive()",vd.return_code == 0,not vd.return_code
ansible-modules-extras,https://github.com/ansible/ansible-modules-extras/tree/master/cloud/misc/ovirt.py,,get_vm$383,"def get_vm(conn, vmname):
    vm = conn.vms.get(name=vmname)
    if vm == None:
        name = ""empty""
    else:
        name = vm.get_name()
    return name",vm == None,not vm
transformers,https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation/utils.py,,init_gpu_params$51,"def init_gpu_params(params):
    """"""
    Handle single and multi-GPU / multi-node.
    """"""
    if params.n_gpu <= 0:
        params.local_rank = 0
        params.master_port = -1
        params.is_master = True
        params.multi_gpu = False
        return

    assert torch.cuda.is_available()

    logger.info(""Initializing GPUs"")
    if params.n_gpu > 1:
        assert params.local_rank != -1

        params.world_size = int(os.environ[""WORLD_SIZE""])
        params.n_gpu_per_node = int(os.environ[""N_GPU_NODE""])
        params.global_rank = int(os.environ[""RANK""])

        # number of nodes / node ID
        params.n_nodes = params.world_size // params.n_gpu_per_node
        params.node_id = params.global_rank // params.n_gpu_per_node
        params.multi_gpu = True

        assert params.n_nodes == int(os.environ[""N_NODES""])
        assert params.node_id == int(os.environ[""NODE_RANK""])

    # local job (single GPU)
    else:
        assert params.local_rank == -1

        params.n_nodes = 1
        params.node_id = 0
        params.local_rank = 0
        params.global_rank = 0
        params.world_size = 1
        params.n_gpu_per_node = 1
        params.multi_gpu = False

    # sanity checks
    assert params.n_nodes >= 1
    assert 0 <= params.node_id < params.n_nodes
    assert 0 <= params.local_rank <= params.global_rank < params.world_size
    assert params.world_size == params.n_nodes * params.n_gpu_per_node

    # define whether this is the master process / if we are in multi-node distributed mode
    params.is_master = params.node_id == 0 and params.local_rank == 0
    params.multi_node = params.n_nodes > 1

    # summary
    PREFIX = f""--- Global rank: {params.global_rank} - ""
    logger.info(PREFIX + ""Number of nodes: %i"" % params.n_nodes)
    logger.info(PREFIX + ""Node ID        : %i"" % params.node_id)
    logger.info(PREFIX + ""Local rank     : %i"" % params.local_rank)
    logger.info(PREFIX + ""World size     : %i"" % params.world_size)
    logger.info(PREFIX + ""GPUs per node  : %i"" % params.n_gpu_per_node)
    logger.info(PREFIX + ""Master         : %s"" % str(params.is_master))
    logger.info(PREFIX + ""Multi-node     : %s"" % str(params.multi_node))
    logger.info(PREFIX + ""Multi-GPU      : %s"" % str(params.multi_gpu))
    logger.info(PREFIX + ""Hostname       : %s"" % socket.gethostname())

    # set GPU device
    torch.cuda.set_device(params.local_rank)

    # initialize multi-GPU
    if params.multi_gpu:
        logger.info(""Initializing PyTorch distributed"")
        torch.distributed.init_process_group(
            init_method=""env://"",
            backend=""nccl"",
        )",params.node_id == 0,not params.node_id
transformers,https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation/utils.py,,init_gpu_params$51,"def init_gpu_params(params):
    """"""
    Handle single and multi-GPU / multi-node.
    """"""
    if params.n_gpu <= 0:
        params.local_rank = 0
        params.master_port = -1
        params.is_master = True
        params.multi_gpu = False
        return

    assert torch.cuda.is_available()

    logger.info(""Initializing GPUs"")
    if params.n_gpu > 1:
        assert params.local_rank != -1

        params.world_size = int(os.environ[""WORLD_SIZE""])
        params.n_gpu_per_node = int(os.environ[""N_GPU_NODE""])
        params.global_rank = int(os.environ[""RANK""])

        # number of nodes / node ID
        params.n_nodes = params.world_size // params.n_gpu_per_node
        params.node_id = params.global_rank // params.n_gpu_per_node
        params.multi_gpu = True

        assert params.n_nodes == int(os.environ[""N_NODES""])
        assert params.node_id == int(os.environ[""NODE_RANK""])

    # local job (single GPU)
    else:
        assert params.local_rank == -1

        params.n_nodes = 1
        params.node_id = 0
        params.local_rank = 0
        params.global_rank = 0
        params.world_size = 1
        params.n_gpu_per_node = 1
        params.multi_gpu = False

    # sanity checks
    assert params.n_nodes >= 1
    assert 0 <= params.node_id < params.n_nodes
    assert 0 <= params.local_rank <= params.global_rank < params.world_size
    assert params.world_size == params.n_nodes * params.n_gpu_per_node

    # define whether this is the master process / if we are in multi-node distributed mode
    params.is_master = params.node_id == 0 and params.local_rank == 0
    params.multi_node = params.n_nodes > 1

    # summary
    PREFIX = f""--- Global rank: {params.global_rank} - ""
    logger.info(PREFIX + ""Number of nodes: %i"" % params.n_nodes)
    logger.info(PREFIX + ""Node ID        : %i"" % params.node_id)
    logger.info(PREFIX + ""Local rank     : %i"" % params.local_rank)
    logger.info(PREFIX + ""World size     : %i"" % params.world_size)
    logger.info(PREFIX + ""GPUs per node  : %i"" % params.n_gpu_per_node)
    logger.info(PREFIX + ""Master         : %s"" % str(params.is_master))
    logger.info(PREFIX + ""Multi-node     : %s"" % str(params.multi_node))
    logger.info(PREFIX + ""Multi-GPU      : %s"" % str(params.multi_gpu))
    logger.info(PREFIX + ""Hostname       : %s"" % socket.gethostname())

    # set GPU device
    torch.cuda.set_device(params.local_rank)

    # initialize multi-GPU
    if params.multi_gpu:
        logger.info(""Initializing PyTorch distributed"")
        torch.distributed.init_process_group(
            init_method=""env://"",
            backend=""nccl"",
        )",params.local_rank == 0,not params.local_rank
abu,https://github.com/bbfamily/abu/tree/master/abupy/WidgetBu/ABuWGBRunBase.py,WidgetTimeModeMixin,on_time_mode_change$201,"def on_time_mode_change(self, change):
        """"""鍒囨崲浣跨敤骞存暟杩樻槸璧峰嬶紝缁撴潫鏃堕棿鍋氫负鍥炴祴鍙傛暟""""""
        if change['new'] == 0:
            self.run_years.disabled = False
            self.start.disabled = True
            self.end.disabled = True
        else:
            self.run_years.disabled = True
            self.start.disabled = False
            self.end.disabled = False",change['new'] == 0,not change['new']
opentracing-python,https://github.com/opentracing/opentracing-python/tree/master/opentracing/harness/scope_check.py,ScopeCompatibilityCheckMixin,test_activate$63,"def test_activate(self):
        def fn():
            scope_manager = self.scope_manager()
            span = mock.MagicMock(spec=Span)

            scope = scope_manager.activate(span, False)
            assert scope is not None
            assert scope_manager.active is scope

            scope.close()
            assert span.finish.call_count == 0
            assert scope_manager.active is None

        self.run_test(fn)",span.finish.call_count == 0,not span.finish.call_count
arcade,https://github.com/pythonarcade/arcade/tree/master/arcade/examples/array_backed_grid_sprites_1.py,MyGame,on_mouse_press$93,"def on_mouse_press(self, x, y, button, modifiers):
        """"""
        Called when the user presses a mouse button.
        """"""

        # Change the x/y screen coordinates to grid coordinates
        column = int(x // (WIDTH + MARGIN))
        row = int(y // (HEIGHT + MARGIN))

        print(f""Click coordinates: ({x}, {y}). Grid coordinates: ({row}, {column})"")

        # Make sure we are on-grid. It is possible to click in the upper right
        # corner in the margin and go to a grid location that doesn't exist
        if row < ROW_COUNT and column < COLUMN_COUNT:

            # Flip the location between 1 and 0.
            if self.grid[row][column] == 0:
                self.grid[row][column] = 1
            else:
                self.grid[row][column] = 0

        self.resync_grid_with_sprites()",self.grid[row][column] == 0,not self.grid[row][column]
texar,https://github.com/asyml/texar/tree/master/texar/tf/modules/memory/memory_network.py,MemNetRNNLike,_build$527,"def _build(self, memory=None, query=None, soft_memory=None, soft_query=None,
               mode=None, **kwargs):
        """"""Pass the :attr:`memory` and :attr:`query` through the memory network
        and return the :attr:`logits` after the final matrix.

        Only one of :attr:`memory` and :attr:`soft_memory` can be specified.
        They should not be specified at the same time.

        Args:
            memory (optional): Memory used in A/C operations. By default, it
                should be an integer tensor of shape
                `[batch_size, memory_size]`,
                containing the ids to embed if provided.
            query (optional): Query vectors as the intial input of the memory
                network.
                If you'd like to apply some transformation (e.g., embedding)
                on it before it's fed into the network, please set `use_B` to
                True and add `query_embed_fn` when constructing this instance.
                If `query_embed_fn` is set to
                :meth:`~texar.tf.modules.MemNetBase.get_default_embed_fn`,
                it should be of shape `[batch_size]`.
                If `use_B` is not set, it should be of shape
                `[batch_size, memory_dim]`.
            soft_memory (optional): Soft memory used in A/C operations. By
                default, it should be a tensor of shape
                `[batch_size, memory_size, raw_memory_dim]`,
                containing the weights used to mix the embedding vectors.
                If you'd like to apply a matrix multiplication on the memory,
                this option can also be used.
            soft_query (optional): Query vectors as the intial input of the
                memory network.
                If you'd like to apply some transformation (e.g., embedding)
                on it before it's fed into the network, please set `use_B` to
                True and add `query_embed_fn` when constructing this instance.
                Similar to :attr:`soft_memory`, if `query_embed_fn` is set to
                :meth:`~texar.tf.modules.MemNetBase.get_default_embed_fn`,
                then it must be of shape `[batch_size, raw_memory_dim]`.
                Ignored if `use_B` is not set.
            mode (optional): A tensor taking value in
                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including
                `TRAIN`, `EVAL`, and `PREDICT`. If `None`, dropout is
                controlled by :func:`texar.tf.global_mode`.
        """"""
        if self._B is not None:
            def _unsqueeze(x):
                return x if x is None else tf.expand_dims(x, 1)
            query = tf.squeeze(
                self._B(_unsqueeze(query), _unsqueeze(soft_query), mode=mode),
                1)
        self._u = [query]
        self._m = self._A(memory, soft_memory, mode=mode)
        self._c = self._C(memory, soft_memory, mode=mode)

        keep_prob = switch_dropout(1 - self.hparams.dropout_rate, mode=mode)
        if self.hparams.variational:
            with tf.variable_scope(""variational_dropout""):
                noise = tf.random_uniform(tf.shape(self._u[-1]))
                random_tensor = keep_prob + noise
                binary_tensor = tf.floor(random_tensor)

            def _variational_dropout(val):
                return tf.math.div(val, keep_prob) * binary_tensor

        for _ in range(self._n_hops):
            u_ = self._AC(self._u[-1], self._m, self._c)
            if self._relu_dim == 0:
                pass
            elif self._relu_dim == self._memory_dim:
                u_ = tf.nn.relu(u_)
            elif 0 < self._relu_dim < self._memory_dim:
                linear_part = u_[:, : self._memory_dim - self._relu_dim]
                relu_part = u_[:, self._memory_dim - self._relu_dim:]
                relued_part = tf.nn.relu(relu_part)
                u_ = tf.concat(axis=1, values=[linear_part, relued_part])
            else:
                raise ValueError(
                    ""relu_dim = {} is illegal"".format(self._relu_dim))
            if self.hparams.variational:
                u_ = _variational_dropout(u_)
            else:
                u_ = tf.nn.dropout(u_, keep_prob)
            self._u.append(u_)

        logits = self._W(self._u[-1])

        if not self._built:
            self._add_internal_trainable_variables()
            self._built = True

        return logits",self._relu_dim == 0,not self._relu_dim
congress-legislators,https://github.com/unitedstates/congress-legislators/tree/master/scripts/icpsr_ids.py,,run$15,"def run():

    # default to caching
    cache = utils.flags().get('cache', True)
    force = not cache


    only_bioguide = utils.flags().get('bioguide', None)
    congress = utils.flags().get('congress',None)


    data_files = []

    print(""Loading %s..."" % ""legislators-current.yaml"")
    legislators = load_data(""legislators-current.yaml"")
    data_files.append((legislators,""legislators-current.yaml""))
    print(""Loading %s..."" % ""legislators-historical.yaml"")
    legislators = load_data(""legislators-historical.yaml"")
    data_files.append((legislators,""legislators-historical.yaml""))

    # load member data from vote view
    if congress == None:
        raise Exception(""the --congress flag is required"")
    elif int(congress) < 10 and int(congress) > 0:
        url_senate = ""https://voteview.com/static/data/out/members/S00%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H00%s_members.csv"" % congress
    elif int(congress) < 100 and int(congress) >= 10:
        url_senate = ""https://voteview.com/static/data/out/members/S0%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H0%s_members.csv"" % congress
    elif int(congress) >= 100:
        url_senate = ""https://voteview.com/static/data/out/members/S%s_members.csv"" % congress
        url_house = ""https://voteview.com/static/data/out/members/H%s_members.csv"" % congress
    else:
        raise Exception(""no data for congress "" + congress)

    senate_destination = ""icpsr/source/senate_rollcall%s.txt"" % congress
    senate_data = utils.download(url_senate, senate_destination, force)

    house_destination = ""icpsr/source/house_rollcall%s.txt"" % congress
    house_data = utils.download(url_house, house_destination, force)

    error_log = csv.writer(open(""cache/errors/mismatch/mismatch_%s.csv"" % congress, ""w""))
    error_log.writerow([""error_type"",""matches"",""icpsr_name"",""icpsr_state"",""is_territory"",""old_id"",""new_id""])



    read_files = [(""sen"",senate_data),(""rep"",house_data)]
    print(""Running for congress "" + congress)
    for read_file_chamber,read_file_content in read_files:
        for data_file in data_files:
            for legislator in data_file[0]:
                num_matches = 0
                write_id = """"
                # this can't run unless we've already collected a bioguide for this person
                bioguide = legislator[""id""].get(""bioguide"", None)
                # if we've limited this to just one bioguide, skip over everyone else
                if only_bioguide and (bioguide != only_bioguide):
                    continue
                #if not in currently read chamber, skip
                chamber = legislator['terms'][len(legislator['terms'])-1]['type']
                if chamber != read_file_chamber:
                    continue

                #only run for selected congress
                latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms'])-1]['start'])))
                if chamber == ""sen"":
                    congresses = [latest_congress,latest_congress+1,latest_congress+2]
                else:
                    congresses =[latest_congress]

                if int(congress) not in congresses:
                    continue

                # pull data to match from yaml

                last_name = legislator['name']['last'].upper()
                state = utils.states[legislator['terms'][len(legislator['terms'])-1]['state']].upper()[:7].strip()

                # convert read_file_content str to file object, then parse as csv file
                content_as_file = StringIO(read_file_content)
                content_parsed = csv.reader(content_as_file, delimiter=',')

                # loop through congress members in read file, see if one matches the current legislator
                for icpsr_member in content_parsed:
                    # ensure unique match bassed of bioguide id
                    if bioguide == icpsr_member[10]:
                        num_matches += 1
                        write_id = int(icpsr_member[2])

                # skip if icpsr id is currently in data
                if ""icpsr"" in legislator[""id""]:
                    if write_id == legislator[""id""][""icpsr""] or write_id == """":
                        continue
                    elif write_id != legislator[""id""][""icpsr""] and write_id != """":
                        error_log.writerow([""Incorrect_ID"",""NA"",last_name[:8],state,""NA"",legislator[""id""][""icpsr""],write_id])
                        print(""ID updated for %s"" % last_name)

                if num_matches == 1:
                    legislator['id']['icpsr'] = int(write_id)
                else:
                    if state == 'GUAM' or state == 'PUERTO' or state == ""VIRGIN"" or state == ""DISTRIC"" or state == ""AMERICA"" or state == ""NORTHER"" or state == ""PHILIPP"":
                        print('error: non 1 match')
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name[:8],state,""Y"",""NA"",""NA""])
                    else:
                        print(str(num_matches) + "" matches found for ""+ last_name[:8] + "", "" + state + "" in congress "" + str(congress))
                        error_log.writerow([""Non_1_match_number"",str(num_matches),last_name,state,""N"",""NA"",""NA""])

            save_data(data_file[0], data_file[1])",congress == None,not congress
skidl,https://github.com/devbisme/skidl/tree/master/tests/test_net.py,,test_nets_1$12,"def test_nets_1():
    gnd = Net(""GND"")
    a = Net(""A"")
    b = Net(""B"")
    c = Net()
    p = Pin()
    assert len(default_circuit.get_nets()) == 0
    assert len(a) == 0
    assert len(b) == 0
    assert len(c) == 0
    a += p
    assert len(default_circuit.get_nets()) == 1
    assert len(a) == 1
    assert len(b) == 0
    assert len(c) == 0",len(default_circuit.get_nets()) == 0,not default_circuit.get_nets()
skidl,https://github.com/devbisme/skidl/tree/master/tests/test_net.py,,test_nets_1$12,"def test_nets_1():
    gnd = Net(""GND"")
    a = Net(""A"")
    b = Net(""B"")
    c = Net()
    p = Pin()
    assert len(default_circuit.get_nets()) == 0
    assert len(a) == 0
    assert len(b) == 0
    assert len(c) == 0
    a += p
    assert len(default_circuit.get_nets()) == 1
    assert len(a) == 1
    assert len(b) == 0
    assert len(c) == 0",len(a) == 0,not a
skidl,https://github.com/devbisme/skidl/tree/master/tests/test_net.py,,test_nets_1$12,"def test_nets_1():
    gnd = Net(""GND"")
    a = Net(""A"")
    b = Net(""B"")
    c = Net()
    p = Pin()
    assert len(default_circuit.get_nets()) == 0
    assert len(a) == 0
    assert len(b) == 0
    assert len(c) == 0
    a += p
    assert len(default_circuit.get_nets()) == 1
    assert len(a) == 1
    assert len(b) == 0
    assert len(c) == 0",len(b) == 0,not b
skidl,https://github.com/devbisme/skidl/tree/master/tests/test_net.py,,test_nets_1$12,"def test_nets_1():
    gnd = Net(""GND"")
    a = Net(""A"")
    b = Net(""B"")
    c = Net()
    p = Pin()
    assert len(default_circuit.get_nets()) == 0
    assert len(a) == 0
    assert len(b) == 0
    assert len(c) == 0
    a += p
    assert len(default_circuit.get_nets()) == 1
    assert len(a) == 1
    assert len(b) == 0
    assert len(c) == 0",len(c) == 0,not c
skidl,https://github.com/devbisme/skidl/tree/master/tests/test_net.py,,test_nets_1$12,"def test_nets_1():
    gnd = Net(""GND"")
    a = Net(""A"")
    b = Net(""B"")
    c = Net()
    p = Pin()
    assert len(default_circuit.get_nets()) == 0
    assert len(a) == 0
    assert len(b) == 0
    assert len(c) == 0
    a += p
    assert len(default_circuit.get_nets()) == 1
    assert len(a) == 1
    assert len(b) == 0
    assert len(c) == 0",len(b) == 0,not b
skidl,https://github.com/devbisme/skidl/tree/master/tests/test_net.py,,test_nets_1$12,"def test_nets_1():
    gnd = Net(""GND"")
    a = Net(""A"")
    b = Net(""B"")
    c = Net()
    p = Pin()
    assert len(default_circuit.get_nets()) == 0
    assert len(a) == 0
    assert len(b) == 0
    assert len(c) == 0
    a += p
    assert len(default_circuit.get_nets()) == 1
    assert len(a) == 1
    assert len(b) == 0
    assert len(c) == 0",len(c) == 0,not c
django-mysql,https://github.com/adamchainz/django-mysql/tree/master/tests/testapp/test_lookups.py,SoundexTests,test_sounds_like_lookup$41,"def test_sounds_like_lookup(self):
        principles = [""principle"", ""principal"", ""princpl""]
        created = {Author.objects.create(name=name) for name in principles}

        for name in principles:
            sounding = Author.objects.filter(name__sounds_like=name)
            assert set(sounding) == created

        sounding = Author.objects.filter(name__sounds_like="""")
        assert set(sounding) == set()

        sounding = Author.objects.filter(name__sounds_like=""nothing"")
        assert set(sounding) == set()",set(sounding) == set(),not set(sounding)
django-mysql,https://github.com/adamchainz/django-mysql/tree/master/tests/testapp/test_lookups.py,SoundexTests,test_sounds_like_lookup$41,"def test_sounds_like_lookup(self):
        principles = [""principle"", ""principal"", ""princpl""]
        created = {Author.objects.create(name=name) for name in principles}

        for name in principles:
            sounding = Author.objects.filter(name__sounds_like=name)
            assert set(sounding) == created

        sounding = Author.objects.filter(name__sounds_like="""")
        assert set(sounding) == set()

        sounding = Author.objects.filter(name__sounds_like=""nothing"")
        assert set(sounding) == set()",set(sounding) == set(),not set(sounding)
freqtrade,https://github.com/freqtrade/freqtrade/tree/master/tests/rpc/test_rpc_apiserver.py,,test_api_UvicornServer$297,"def test_api_UvicornServer(mocker):
    thread_mock = mocker.patch('freqtrade.rpc.api_server.uvicorn_threaded.threading.Thread')
    s = UvicornServer(uvicorn.Config(MagicMock(), port=8080, host='127.0.0.1'))
    assert thread_mock.call_count == 0

    s.install_signal_handlers()
    # Original implementation starts a thread - make sure that's not the case
    assert thread_mock.call_count == 0

    # Fake started to avoid sleeping forever
    s.started = True
    s.run_in_thread()
    assert thread_mock.call_count == 1

    s.cleanup()
    assert s.should_exit is True",thread_mock.call_count == 0,not thread_mock.call_count
freqtrade,https://github.com/freqtrade/freqtrade/tree/master/tests/rpc/test_rpc_apiserver.py,,test_api_UvicornServer$297,"def test_api_UvicornServer(mocker):
    thread_mock = mocker.patch('freqtrade.rpc.api_server.uvicorn_threaded.threading.Thread')
    s = UvicornServer(uvicorn.Config(MagicMock(), port=8080, host='127.0.0.1'))
    assert thread_mock.call_count == 0

    s.install_signal_handlers()
    # Original implementation starts a thread - make sure that's not the case
    assert thread_mock.call_count == 0

    # Fake started to avoid sleeping forever
    s.started = True
    s.run_in_thread()
    assert thread_mock.call_count == 1

    s.cleanup()
    assert s.should_exit is True",thread_mock.call_count == 0,not thread_mock.call_count
transformers,https://github.com/huggingface/transformers/tree/master/src/transformers/modelcard.py,,_insert_values_as_list$307,"def _insert_values_as_list(metadata, name, values):
    if values is None:
        return metadata
    if isinstance(values, str):
        values = [values]
    values = [v for v in values if v is not None]
    if len(values) == 0:
        return metadata
    metadata[name] = values
    return metadata",len(values) == 0,not values
flow,https://github.com/flow-project/flow/tree/master/flow/core/kernel/vehicle/traci.py,TraCIVehicle,_prev_edge_followers$911,"def _prev_edge_followers(self, veh_id, edge_dict, lane, num_edges):
        """"""Search for followers in the previous edge.

        Looks to the edges/junctions behind the vehicle's current edge for
        potential followers. This is currently done by only looking one
        edge/junction backwards.

        Returns
        -------
        tailway : float
            lane tailway for the specified lane
        follower : str
            lane follower for the specified lane
        """"""
        pos = self.get_position(veh_id)
        edge = self.get_edge(veh_id)

        tailway = 1000  # env.network.length
        follower = """"
        add_length = 0  # length increment in headway

        for _ in range(num_edges):
            # break if there are no edge/lane pairs behind the current one
            if len(self.master_kernel.network.prev_edge(edge, lane)) == 0:
                break

            edge, lane = self.master_kernel.network.prev_edge(edge, lane)[0]
            add_length += self.master_kernel.network.edge_length(edge)

            try:
                if len(edge_dict[edge][lane]) > 0:
                    tailway = pos - edge_dict[edge][lane][-1][1] + add_length \
                              - self.get_length(veh_id)
                    follower = edge_dict[edge][lane][-1][0]
            except KeyError:
                # current edge has no vehicles, so move on
                # print(traceback.format_exc())
                continue

            # stop if a lane follower is found
            if follower != """":
                break

        return tailway, follower","len(self.master_kernel.network.prev_edge(edge, lane)) == 0","not self.master_kernel.network.prev_edge(edge, lane)"
cobbler,https://github.com/cobbler/cobbler/tree/master/cobbler/remote.py,CobblerXMLRPCInterface,__paginate$635,"def __paginate(self, data, page=1, items_per_page=25, token=None):
        """"""
        Helper function to support returning parts of a selection, for example, for use in a web app where only a part
        of the results are to be presented on each screen.

        :param data: The data to paginate.
        :param page: The page to show.
        :param items_per_page: The number of items per page.
        :param token: The API-token obtained via the login() method.
        :return: The paginated items.
        """"""
        default_page = 1
        default_items_per_page = 25

        try:
            page = int(page)
            if page < 1:
                page = default_page
        except:
            page = default_page
        try:
            items_per_page = int(items_per_page)
            if items_per_page <= 0:
                items_per_page = default_items_per_page
        except:
            items_per_page = default_items_per_page

        num_items = len(data)
        num_pages = ((num_items - 1) // items_per_page) + 1
        if num_pages == 0:
            num_pages = 1
        if page > num_pages:
            page = num_pages
        start_item = (items_per_page * (page - 1))
        end_item = start_item + items_per_page
        if start_item > num_items:
            start_item = num_items - 1
        if end_item > num_items:
            end_item = num_items
        data = data[start_item:end_item]

        if page > 1:
            prev_page = page - 1
        else:
            prev_page = None
        if page < num_pages:
            next_page = page + 1
        else:
            next_page = None

        return (data, {
            'page': page,
            'prev_page': prev_page,
            'next_page': next_page,
            'pages': list(range(1, num_pages + 1)),
            'num_pages': num_pages,
            'num_items': num_items,
            'start_item': start_item,
            'end_item': end_item,
            'items_per_page': items_per_page,
            'items_per_page_list': [10, 20, 50, 100, 200, 500],
        })",num_pages == 0,not num_pages
typer,https://github.com/tiangolo/typer/tree/master/tests/test_tutorial/test_commands/test_help/test_tutorial001.py,,test_help$13,"def test_help():
    result = runner.invoke(app, [""--help""])
    assert result.exit_code == 0
    assert ""Awesome CLI user manager."" in result.output
    assert ""create"" in result.output
    assert ""Create a new user with USERNAME."" in result.output
    assert ""delete"" in result.output
    assert ""Delete a user with USERNAME."" in result.output
    assert ""delete-all"" in result.output
    assert ""Delete ALL users in the database."" in result.output
    assert ""init"" in result.output
    assert ""Initialize the users database."" in result.output",result.exit_code == 0,not result.exit_code
saleor,https://github.com/saleor/saleor/tree/master/saleor/plugins/webhook/tests/test_payment_webhook.py,,test_get_payment_gateways_filters_out_unsupported_currencies$354,"def test_get_payment_gateways_filters_out_unsupported_currencies(
    mock_send_request, payment_app, webhook_plugin
):
    plugin = webhook_plugin()
    mock_json_response = [
        {
            ""id"": ""credit-card"",
            ""name"": ""Credit Card"",
            ""currencies"": [""USD"", ""EUR""],
            ""config"": [],
        }
    ]
    mock_send_request.return_value = mock_json_response
    response_data = plugin.get_payment_gateways(""PLN"", None, None)
    assert response_data == []",response_data == [],not response_data
scikit-survival,https://github.com/sebp/scikit-survival/tree/master/tests/test_survival_svm.py,TestFastSurvivalSVM,test_fit_and_predict_regression_no_intercept$368,"def test_fit_and_predict_regression_no_intercept(make_whas500, optimizer_regression):
        whas500 = make_whas500(to_numeric=True)

        ssvm = FastSurvivalSVM(optimizer=optimizer_regression, rank_ratio=0.0,
                               max_iter=50, fit_intercept=False, random_state=0)
        ssvm.fit(whas500.x, whas500.y)

        assert not hasattr(ssvm, ""intercept_"")
        expected_coef = numpy.array([1.39989875, -1.16903161, -0.40195857, -0.05848903, -0.08421557, 4.11924729,
                                     0.25135451, 1.89067276, -0.25751401, -0.10213143, 1.56333622, 3.10136873,
                                     -2.23644848, -0.11620715])
        assert_array_almost_equal(expected_coef, ssvm.coef_)

        pred = ssvm.predict(whas500.x)
        rmse = numpy.sqrt(mean_squared_error(whas500.y['lenfol'], pred))
        assert round(abs(15838.510668936022 - rmse), 7) == 0","round(abs(15838.510668936022 - rmse), 7) == 0","not round(abs(15838.510668936022 - rmse), 7)"
JsFormat,https://github.com/jdavisclark/JsFormat/tree/master/libs/merge_utils.py,,_merge_code$34,"def _merge_code(view, edit, code, formatted):
    def ss(start, end):
        return view.substr(sublime.Region(start, end))

    dmp = diff_match_patch()
    diffs = dmp.diff_main(code, formatted)
    dmp.diff_cleanupEfficiency(diffs)
    i = 0
    dirty = False
    for k, s in diffs:
        l = len(s)
        if k == 0:
            # match
            l = len(s)
            if ss(i, i + l) != s:
                raise MergeException('mismatch', dirty)
            i += l
        else:
            dirty = True
            if k > 0:
                # insert
                view.insert(edit, i, s)
                i += l
            else:
                # delete
                if ss(i, i + l) != s:
                    raise MergeException('mismatch', dirty)
                view.erase(edit, sublime.Region(i, i + l))
    return dirty",k == 0,not k
trezor-firmware,https://github.com/trezor/trezor-firmware/tree/master/python/tests/test_protobuf_encoding.py,,test_load_uvarint$142,"def test_load_uvarint():
    assert load_uvarint(b""\x00"") == 0
    assert load_uvarint(b""\x01"") == 1
    assert load_uvarint(b""\xff\x01"") == 0xFF
    assert load_uvarint(b""\xc0\xc4\x07"") == 123456
    assert load_uvarint(b""\x80\x80\x80\x80\x00"") == 0",load_uvarint(b'\x00') == 0,not load_uvarint(b'\x00')
trezor-firmware,https://github.com/trezor/trezor-firmware/tree/master/python/tests/test_protobuf_encoding.py,,test_load_uvarint$142,"def test_load_uvarint():
    assert load_uvarint(b""\x00"") == 0
    assert load_uvarint(b""\x01"") == 1
    assert load_uvarint(b""\xff\x01"") == 0xFF
    assert load_uvarint(b""\xc0\xc4\x07"") == 123456
    assert load_uvarint(b""\x80\x80\x80\x80\x00"") == 0",load_uvarint(b'\x80\x80\x80\x80\x00') == 0,not load_uvarint(b'\x80\x80\x80\x80\x00')
fedlearner,https://github.com/bytedance/fedlearner/tree/master/fedlearner/data_join/common.py,,convert_dict_to_tf_example$172,"def convert_dict_to_tf_example(src_dict):
    assert isinstance(src_dict, dict)
    tf_feature = {}
    for key, feature in src_dict.items():
        if not isinstance(key, str):
            raise RuntimeError('the key {}({}) of dict must a '\
                               'string'.format(key, type(key)))
        basic_type = type(feature)
        # Due to all fields' value are type of str in csv format,
        # we try best to convert the digital string into numerical value.
        if basic_type == str and (
            (key in ALLOWED_FIELDS and ALLOWED_FIELDS[key].type != bytes) or
            key not in ALLOWED_FIELDS):
            if feature.lstrip('-').isdigit():
                feature = int(feature)
                basic_type = int
            else:
                try:
                    feature = float(feature)
                    basic_type = float
                except ValueError as e:
                    if key in ALLOWED_FIELDS:
                        raise ValueError(
                            '%s should be numerical instead of str'%key)
        if isinstance(type(feature), list):
            if len(feature) == 0:
                logging.debug('skip %s since feature is empty list', key)
                continue
            basic_type = feature[0]
            if not all(isinstance(x, basic_type) for x in feature):
                raise RuntimeError('type of elements in feature of key {} '\
                                   'is not the same'.format(key))
        if not isinstance(feature, _valid_basic_feature_type):
            raise RuntimeError(""feature type({}) of key {} is not support ""\
                               ""for tf Example"".format(basic_type, key))
        if basic_type == int:
            value = feature if isinstance(feature, list) else [feature]
            tf_feature[key] = tf.train.Feature(
                int64_list=tf.train.Int64List(value=value))
        elif basic_type == str:
            value = [feat.encode() for feat in feature] if \
                     isinstance(feature, list) else [feature.encode()]
            tf_feature[key] = tf.train.Feature(
                bytes_list=tf.train.BytesList(value=value))
        else:
            assert basic_type == float
            value = feature if isinstance(feature, list) else [feature]
            tf_feature[key] = tf.train.Feature(
                float_list=tf.train.FloatList(value=value))
    return tf.train.Example(features=tf.train.Features(feature=tf_feature))",len(feature) == 0,not feature
optuna,https://github.com/optuna/optuna/tree/master/tests/storages_tests/rdb_tests/test_models.py,TestTrialModel,test_count_past_trials$169,"def test_count_past_trials(session: Session) -> None:

        study_1 = StudyModel(study_id=1, study_name=""test-study-1"")
        study_2 = StudyModel(study_id=2, study_name=""test-study-2"")

        trial_1_1 = TrialModel(study_id=study_1.study_id, state=TrialState.COMPLETE)
        session.add(trial_1_1)
        session.commit()
        assert 0 == trial_1_1.count_past_trials(session)

        trial_1_2 = TrialModel(study_id=study_1.study_id, state=TrialState.RUNNING)
        session.add(trial_1_2)
        session.commit()
        assert 1 == trial_1_2.count_past_trials(session)

        trial_2_1 = TrialModel(study_id=study_2.study_id, state=TrialState.RUNNING)
        session.add(trial_2_1)
        session.commit()
        assert 0 == trial_2_1.count_past_trials(session)",0 == trial_1_1.count_past_trials(session),not trial_1_1.count_past_trials(session)
optuna,https://github.com/optuna/optuna/tree/master/tests/storages_tests/rdb_tests/test_models.py,TestTrialModel,test_count_past_trials$169,"def test_count_past_trials(session: Session) -> None:

        study_1 = StudyModel(study_id=1, study_name=""test-study-1"")
        study_2 = StudyModel(study_id=2, study_name=""test-study-2"")

        trial_1_1 = TrialModel(study_id=study_1.study_id, state=TrialState.COMPLETE)
        session.add(trial_1_1)
        session.commit()
        assert 0 == trial_1_1.count_past_trials(session)

        trial_1_2 = TrialModel(study_id=study_1.study_id, state=TrialState.RUNNING)
        session.add(trial_1_2)
        session.commit()
        assert 1 == trial_1_2.count_past_trials(session)

        trial_2_1 = TrialModel(study_id=study_2.study_id, state=TrialState.RUNNING)
        session.add(trial_2_1)
        session.commit()
        assert 0 == trial_2_1.count_past_trials(session)",0 == trial_2_1.count_past_trials(session),not trial_2_1.count_past_trials(session)
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/otConverters.py,AATLookupWithDataOffset,write$965,"def write(self, writer, font, tableDict, value, repeatIndex=None):
		# We do not work with OTTableWriter sub-writers because
		# the offsets in our AATLookup are relative to our data
		# table, for which we need to provide an offset value itself.
		# It might have been possible to somehow make a kludge for
		# performing this indirect offset computation directly inside
		# OTTableWriter. But this would have made the internal logic
		# of OTTableWriter even more complex than it already is,
		# so we decided to roll our own offset computation for the
		# contents of the AATLookup and associated data table.
		offsetByGlyph, offsetByData, dataLen = {}, {}, 0
		compiledData = []
		for glyph in sorted(value, key=font.getGlyphID):
			subWriter = OTTableWriter()
			value[glyph].compile(subWriter, font)
			data = subWriter.getAllData()
			offset = offsetByData.get(data, None)
			if offset == None:
				offset = dataLen
				dataLen = dataLen + len(data)
				offsetByData[data] = offset
				compiledData.append(data)
			offsetByGlyph[glyph] = offset
		# For calculating the offsets to our AATLookup and data table,
		# we can use the regular OTTableWriter infrastructure.
		lookupWriter = writer.getSubWriter(offsetSize=4)
		lookup = AATLookup('DataOffsets', None, None, UShort)
		lookup.write(lookupWriter, font, tableDict, offsetByGlyph, None)

		dataWriter = writer.getSubWriter(offsetSize=4)
		writer.writeSubTable(lookupWriter)
		writer.writeSubTable(dataWriter)
		for d in compiledData:
			dataWriter.writeData(d)",offset == None,not offset
albert_pytorch,https://github.com/lonePatient/albert_pytorch/tree/master/callback/progressbar.py,ProgressBar,__call__$16,"def __call__(self, step, info={}):
        now = time.time()
        current = step + 1
        recv_per = current / self.n_total
        bar = f'[{self.desc}] {current}/{self.n_total} ['
        if recv_per >= 1:
            recv_per = 1
        prog_width = int(self.width * recv_per)
        if prog_width > 0:
            bar += '=' * (prog_width - 1)
            if current< self.n_total:
                bar += "">""
            else:
                bar += '='
        bar += '.' * (self.width - prog_width)
        bar += ']'
        show_bar = f""\r{bar}""
        time_per_unit = (now - self.start_time) / current
        if current < self.n_total:
            eta = time_per_unit * (self.n_total - current)
            if eta > 3600:
                eta_format = ('%d:%02d:%02d' %
                              (eta // 3600, (eta % 3600) // 60, eta % 60))
            elif eta > 60:
                eta_format = '%d:%02d' % (eta // 60, eta % 60)
            else:
                eta_format = '%ds' % eta
            time_info = f' - ETA: {eta_format}'
        else:
            if time_per_unit >= 1:
                time_info = f' {time_per_unit:.1f}s/step'
            elif time_per_unit >= 1e-3:
                time_info = f' {time_per_unit * 1e3:.1f}ms/step'
            else:
                time_info = f' {time_per_unit * 1e6:.1f}us/step'

        show_bar += time_info
        if len(info) != 0:
            show_info = f'{show_bar} ' + \
                        ""-"".join([f' {key}: {value:.4f} ' for key, value in info.items()])
            print(show_info, end='')
        else:
            print(show_bar, end='')",len(info) != 0,info
deep-learning-for-image-processing,https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_object_detection/train_coco_dataset/train_utils/group_by_aspect_ratio.py,GroupedBatchSampler,__iter__$47,"def __iter__(self):
        buffer_per_group = defaultdict(list)
        samples_per_group = defaultdict(list)

        num_batches = 0
        for idx in self.sampler:
            group_id = self.group_ids[idx]
            buffer_per_group[group_id].append(idx)
            samples_per_group[group_id].append(idx)
            if len(buffer_per_group[group_id]) == self.batch_size:
                yield buffer_per_group[group_id]
                num_batches += 1
                del buffer_per_group[group_id]
            assert len(buffer_per_group[group_id]) < self.batch_size

        # now we have run out of elements that satisfy
        # the group criteria, let's return the remaining
        # elements so that the size of the sampler is
        # deterministic
        expected_num_batches = len(self)
        num_remaining = expected_num_batches - num_batches
        if num_remaining > 0:
            # for the remaining batches, take first the buffers with largest number
            # of elements
            for group_id, _ in sorted(buffer_per_group.items(),
                                      key=lambda x: len(x[1]), reverse=True):
                remaining = self.batch_size - len(buffer_per_group[group_id])
                samples_from_group_id = _repeat_to_at_least(samples_per_group[group_id], remaining)
                buffer_per_group[group_id].extend(samples_from_group_id[:remaining])
                assert len(buffer_per_group[group_id]) == self.batch_size
                yield buffer_per_group[group_id]
                num_remaining -= 1
                if num_remaining == 0:
                    break
        assert num_remaining == 0",num_remaining == 0,not num_remaining
deep-learning-for-image-processing,https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_object_detection/train_coco_dataset/train_utils/group_by_aspect_ratio.py,GroupedBatchSampler,__iter__$47,"def __iter__(self):
        buffer_per_group = defaultdict(list)
        samples_per_group = defaultdict(list)

        num_batches = 0
        for idx in self.sampler:
            group_id = self.group_ids[idx]
            buffer_per_group[group_id].append(idx)
            samples_per_group[group_id].append(idx)
            if len(buffer_per_group[group_id]) == self.batch_size:
                yield buffer_per_group[group_id]
                num_batches += 1
                del buffer_per_group[group_id]
            assert len(buffer_per_group[group_id]) < self.batch_size

        # now we have run out of elements that satisfy
        # the group criteria, let's return the remaining
        # elements so that the size of the sampler is
        # deterministic
        expected_num_batches = len(self)
        num_remaining = expected_num_batches - num_batches
        if num_remaining > 0:
            # for the remaining batches, take first the buffers with largest number
            # of elements
            for group_id, _ in sorted(buffer_per_group.items(),
                                      key=lambda x: len(x[1]), reverse=True):
                remaining = self.batch_size - len(buffer_per_group[group_id])
                samples_from_group_id = _repeat_to_at_least(samples_per_group[group_id], remaining)
                buffer_per_group[group_id].extend(samples_from_group_id[:remaining])
                assert len(buffer_per_group[group_id]) == self.batch_size
                yield buffer_per_group[group_id]
                num_remaining -= 1
                if num_remaining == 0:
                    break
        assert num_remaining == 0",num_remaining == 0,not num_remaining
alibi-detect,https://github.com/SeldonIO/alibi-detect/tree/master/alibi_detect/utils/pytorch/tests/test_distance_pt.py,,test_permed_lsdds$132,"def test_permed_lsdds(permed_lsdds_params):
    n, m, d, B, n_kcs = permed_lsdds_params

    kcs = torch.randn(n_kcs, d)
    x_ref = torch.randn(n, d)
    x_cur = 10 + 0.2*torch.randn(m, d)
    x_full = torch.cat([x_ref, x_cur], axis=0)
    sigma = torch.tensor((1.,))
    k_all_c = GaussianRBF(sigma)(x_full, kcs)
    H = GaussianRBF(np.sqrt(2.)*sigma)(kcs, kcs)

    perms = [torch.randperm(n+m) for _ in range(B)]
    x_perms = [perm[:n] for perm in perms]
    y_perms = [perm[n:] for perm in perms]

    lsdd_perms, H_lam_inv, lsdd_unpermed = permed_lsdds(
        k_all_c, x_perms, y_perms, H, return_unpermed=True
    )

    assert int((lsdd_perms > lsdd_unpermed).sum()) == 0
    assert H_lam_inv.shape == (n_kcs, n_kcs)",int((lsdd_perms > lsdd_unpermed).sum()) == 0,not int((lsdd_perms > lsdd_unpermed).sum())
pytorch_tabular,https://github.com/manujosephv/pytorch_tabular/tree/master/tests/test_mdn.py,,test_ssl$150,"def test_ssl(
    classification_data,
    continuous_cols,
    categorical_cols,
    continuous_feature_transform,
    normalize_continuous_features,
    num_gaussian,
    ssl_task,
    aug_task,
):
    (train, test, target) = classification_data
    if len(continuous_cols) + len(categorical_cols) == 0:
        assert True
    else:
        data_config = DataConfig(
            target=target,
            continuous_cols=continuous_cols,
            categorical_cols=categorical_cols,
            continuous_feature_transform=continuous_feature_transform,
            normalize_continuous_features=normalize_continuous_features,
        )
        model_config_params = dict(task=""ssl"", ssl_task=ssl_task, aug_task=aug_task)
        mdn_config = MixtureDensityHeadConfig(num_gaussian=num_gaussian)
        model_config_params[""mdn_config""] = mdn_config
        model_config = CategoryEmbeddingMDNConfig(**model_config_params)
        trainer_config = TrainerConfig(
            max_epochs=3,
            checkpoints=None,
            early_stopping=None,
            gpus=None,
            fast_dev_run=True,
        )
        optimizer_config = OptimizerConfig()
        with pytest.raises(AssertionError):
            tabular_model = TabularModel(
                data_config=data_config,
                model_config=model_config,
                optimizer_config=optimizer_config,
                trainer_config=trainer_config,
            )
            tabular_model.fit(train=train, test=test)",len(continuous_cols) + len(categorical_cols) == 0,not len(continuous_cols) + len(categorical_cols)
sympy,https://github.com/sympy/sympy/tree/master/sympy/strategies/tests/test_core.py,,test_exhaust$20,"def test_exhaust():
    sink = exhaust(posdec)
    assert sink(5) == 0
    assert sink(10) == 0",sink(5) == 0,not sink(5)
sympy,https://github.com/sympy/sympy/tree/master/sympy/strategies/tests/test_core.py,,test_exhaust$20,"def test_exhaust():
    sink = exhaust(posdec)
    assert sink(5) == 0
    assert sink(10) == 0",sink(10) == 0,not sink(10)
listenbrainz-server,https://github.com/metabrainz/listenbrainz-server/tree/master/listenbrainz/webserver/views/recommendations_cf_recording_api.py,,_process_recommendations$115,"def _process_recommendations(recommendations, count, artist_type, user_name, offset):
    """""" Process recommendations based on artist type.

        Args:
            recommendations: dict containing user recommendations.
            count (int): number of recommended recording mbids to return.
            artist_type (str): artist type i.e 'top', 'similar'
            user_name (str): musicbrainz id of the user.
            offset (int): number of entities to skip from the beginning

        Returns:
            - total_mbid_count (int): Total number of recommended mbids in the db for the user.
            - list of recommended mbids based on count and offset.

        Raises:
            APINoContent: if recommendations not found.
    """"""
    if artist_type == 'similar':
        data = getattr(recommendations, 'recording_mbid').dict()
        mbid_list = data['similar_artist']

    elif artist_type == 'top':
        data = getattr(recommendations, 'recording_mbid').dict()
        mbid_list = data['top_artist']

    total_mbid_count = len(mbid_list)

    if total_mbid_count == 0:
        err_msg = 'No recommendations for user {}, please try again later.'.format(user_name)
        raise APINoContent(err_msg, payload={'last_updated': int(getattr(recommendations, 'created').timestamp())})

    # For the purpose of experimenting with recommendations, we're allowing to fetch at most
    # 1K recommendations.
    count = min(count, 1000)

    return mbid_list[offset:offset+count], total_mbid_count",total_mbid_count == 0,not total_mbid_count
svtplay-dl,https://github.com/spaam/svtplay-dl/tree/master/lib/svtplay_dl/tests/test_dash.py,,test_parse_duration$62,"def test_parse_duration():
    assert parse_duration(""PT3459.520S"") == 3459.52
    assert parse_duration(""PT2.00S"") == 2.0
    assert parse_duration(""PT1H0M30.000S"") == 3630.0
    assert parse_duration(""P1Y1M1DT1H0M30.000S"") == 34218030.0
    assert parse_duration(""mMWroNG"") == 0",parse_duration('mMWroNG') == 0,not parse_duration('mMWroNG')
autograd,https://github.com/HIPS/autograd/tree/master/examples/lstm.py,,if_main_my$65,"if __name__ == '__main__':
    num_chars = 128

    # Learn to predict our own source code.
    text_filename = join(dirname(__file__), 'lstm.py')
    train_inputs = build_dataset(text_filename, sequence_length=30,
                                 alphabet_size=num_chars, max_lines=60)

    init_params = init_lstm_params(input_size=128, output_size=128,
                                   state_size=40, param_scale=0.01)

    def print_training_prediction(weights):
        print(""Training text                         Predicted text"")
        logprobs = np.asarray(lstm_predict(weights, train_inputs))
        for t in range(logprobs.shape[1]):
            training_text  = one_hot_to_string(train_inputs[:,t,:])
            predicted_text = one_hot_to_string(logprobs[:,t,:])
            print(training_text.replace('\n', ' ') + ""|"" +
                  predicted_text.replace('\n', ' '))

    def training_loss(params, iter):
        return -lstm_log_likelihood(params, train_inputs, train_inputs)

    def callback(weights, iter, gradient):
        if iter % 10 == 0:
            print(""Iteration"", iter, ""Train loss:"", training_loss(weights, 0))
            print_training_prediction(weights)

    # Build gradient of loss function using autograd.
    training_loss_grad = grad(training_loss)

    print(""Training LSTM..."")
    trained_params = adam(training_loss_grad, init_params, step_size=0.1,
                          num_iters=1000, callback=callback)

    print()
    print(""Generating text from LSTM..."")
    num_letters = 30
    for t in range(20):
        text = """"
        for i in range(num_letters):
            seqs = string_to_one_hot(text, num_chars)[:, np.newaxis, :]
            logprobs = lstm_predict(trained_params, seqs)[-1].ravel()
            text += chr(npr.choice(len(logprobs), p=np.exp(logprobs)))
        print(text)",iter % 10 == 0,not iter % 10
montydb,https://github.com/davidlatwe/montydb/tree/master/tests/test_engine/test_queries/test_queryop_comparsion_in.py,,test_qop_in_7$108,"def test_qop_in_7(monty_find, mongo_find):
    docs = [
        {""a"": [{""b"": 1}, {""b"": 2}]}
    ]
    spec = {""a.b"": {""$in"": [True]}}

    monty_c = monty_find(docs, spec)
    mongo_c = mongo_find(docs, spec)

    assert count_documents(mongo_c, spec) == 0
    assert count_documents(monty_c, spec) == count_documents(mongo_c, spec)","count_documents(mongo_c, spec) == 0","not count_documents(mongo_c, spec)"
cleanlab,https://github.com/cleanlab/cleanlab/tree/master/examples/cifar100/cifar100_train_crossval.py,,main_worker$182,"def main_worker(gpu, ngpus_per_node, args):
    global best_acc1
    args.gpu = gpu
    use_crossval = args.cvn > 0
    use_mask = args.dir_train_mask is not None
    cv_fold = args.cv
    cv_n_folds = args.cvn
    class_weights = None
    
    if use_crossval and use_mask:
        raise ValueError('Either args.cvn > 0 or dir-train-mask not None, but not both.')

    if args.gpu is not None:
        print(""Use GPU: {} for training"".format(args.gpu))

    if args.distributed:
        if args.dist_url == ""env://"" and args.rank == -1:
            args.rank = int(os.environ[""RANK""])
        if args.multiprocessing_distributed:
            # For multiprocessing distributed training, rank needs to be the
            # global rank among all the processes
            args.rank = args.rank * ngpus_per_node + gpu
        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                world_size=args.world_size, rank=args.rank)
    # create model
    if args.pretrained:
        print(""=> using pre-trained model '{}'"".format(args.arch))
        model = models.__dict__[args.arch](
            pretrained=True, num_classes=num_classes)
    else:
        print(""=> creating model '{}'"".format(args.arch))
        model = models.__dict__[args.arch](num_classes=num_classes)

    if args.distributed:
        # For multiprocessing distributed, DistributedDataParallel constructor
        # should always set the single device scope, otherwise,
        # DistributedDataParallel will use all available devices.
        if args.gpu is not None:
            torch.cuda.set_device(args.gpu)
            model.cuda(args.gpu)
            # When using a single GPU per process and per
            # DistributedDataParallel, we need to divide the batch size
            # ourselves based on the total number of GPUs we have
            args.batch_size = int(args.batch_size / ngpus_per_node)
            args.workers = int(args.workers / ngpus_per_node)
            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
        else:
            model.cuda()
            # DistributedDataParallel will divide and allocate batch_size to all
            # available GPUs if device_ids are not set
            model = torch.nn.parallel.DistributedDataParallel(model)
    elif args.gpu is not None:
        torch.cuda.set_device(args.gpu)
        model = model.cuda(args.gpu)
    else:
        # DataParallel will divide and allocate batch_size to all available GPUs
        if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):
            model.features = torch.nn.DataParallel(model.features)
            model.cuda()
        else:
            model = torch.nn.DataParallel(model).cuda()    
    
    # define optimizer
    optimizer = torch.optim.SGD(model.parameters(), args.lr,
                                momentum=args.momentum,
                                weight_decay=args.weight_decay)

    # optionally resume from a checkpoint
    if args.resume:
        if os.path.isfile(args.resume):
            print(""=> loading checkpoint '{}'"".format(args.resume))
            if args.gpu is None:
                checkpoint = torch.load(args.resume)
            else:
                # Map model to be loaded to specified single gpu.
                loc = 'cuda:{}'.format(args.gpu)
                checkpoint = torch.load(args.resume, map_location=loc)
            args.start_epoch = checkpoint['epoch']
            best_acc1 = checkpoint['best_acc1']
            if args.gpu is not None:
                # In case you load checkpoint from different GPU
                best_acc1 = best_acc1.to(args.gpu)
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            print(""=> loaded checkpoint '{}' (epoch {})""
                  .format(args.resume, checkpoint['epoch']))
        else:
            print(""=> no checkpoint found at '{}'"".format(args.resume))

    cudnn.benchmark = True

    # Data loading code
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'test')
    
    train_dataset = datasets.ImageFolder(
        traindir,
        transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(
                [0.507 , 0.487, 0.4417],
                [0.267, 0.256, 0.276],
            ),
        ]),
    )
    
    # if training labels are provided use those instead of dataset labels
    if args.train_labels is not None:
        with open(args.train_labels, 'r') as rf:
            train_labels_dict = json.load(rf)
        train_dataset.imgs = [(fn, train_labels_dict[fn]) for fn, _ in train_dataset.imgs]
        train_dataset.samples = train_dataset.imgs

    # If training only on a cross-validated portion & make val_set = train_holdout.
    if use_crossval:
        checkpoint_fn = ""model_{}__fold_{}__checkpoint.pth.tar"".format(args.arch, cv_fold)
        print('Computing fold indices. This takes 15 seconds.')
        # Prepare labels
        labels = [label for img, label in datasets.ImageFolder(traindir).imgs]
        # Split train into train and holdout for particular cv_fold.
        kf = StratifiedKFold(n_splits = cv_n_folds, shuffle = True, random_state = args.cv_seed)
        cv_train_idx, cv_holdout_idx = list(kf.split(range(len(labels)), labels))[cv_fold]
        # Seperate datasets        
        np.random.seed(args.cv_seed)
        holdout_dataset = copy.deepcopy(train_dataset)
        holdout_dataset.imgs = [train_dataset.imgs[i] for i in cv_holdout_idx]
        holdout_dataset.samples = holdout_dataset.imgs
        train_dataset.imgs = [train_dataset.imgs[i] for i in cv_train_idx]
        train_dataset.samples = train_dataset.imgs
        print('Train size:', len(cv_train_idx), len(train_dataset.imgs))
        print('Holdout size:', len(cv_holdout_idx), len(holdout_dataset.imgs))
    else:
        checkpoint_fn = ""model_{}__checkpoint.pth.tar"".format(args.arch)
        if use_mask:            
            checkpoint_fn = ""model_{}__masked__checkpoint.pth.tar"".format(args.arch)
            orig_class_counts = np.bincount(
                [lab for img, lab in datasets.ImageFolder(traindir).imgs])
            train_bool_mask = np.load(args.dir_train_mask)
            # Mask labels
            train_dataset.imgs = [img for i, img in enumerate(train_dataset.imgs) if train_bool_mask[i]]
            train_dataset.samples = train_dataset.imgs
            clean_class_counts = np.bincount(
                [lab for img, lab in train_dataset.imgs])
            print('Train size:', len(train_dataset.imgs))
            # Compute class weights to re-weight loss during training
            # Should use the confident joint to estimate the noise matrix then
            # class_weights = 1 / p(s=k, y=k) for each class k.
            # Here we approximate this with a simpler approach
            # class_weights = count(y=k) / count(s=k, y=k)
            class_weights = torch.Tensor(orig_class_counts / clean_class_counts)
    
    val_dataset = datasets.ImageFolder(
        valdir,
        transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(
                [0.507 , 0.487, 0.441],
                [0.267, 0.256, 0.276],
            ),
        ]),
    )


    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    else:
        train_sampler = None

    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),
        num_workers=args.workers, pin_memory=True, sampler=train_sampler,
    )

    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=args.batch_size, shuffle=False,
        num_workers=args.workers, pin_memory=True,
    )

    # define loss function (criterion)
    criterion = nn.CrossEntropyLoss(weight=class_weights).cuda(args.gpu)

    if args.evaluate:
        validate(val_loader, model, criterion, args)
        return

    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        adjust_learning_rate(optimizer, epoch, args)

        # train for one epoch
        train(train_loader, model, criterion, optimizer, epoch, args)

        # evaluate on validation set
        acc1 = validate(val_loader, model, criterion, args)

        # remember best acc@1, model, and save checkpoint
        is_best = acc1 > best_acc1
        best_acc1 = max(best_acc1, acc1)

        if not args.multiprocessing_distributed or (args.multiprocessing_distributed
                and args.rank % ngpus_per_node == 0):
            save_checkpoint(
                {
                    'epoch': epoch + 1,
                    'arch': args.arch,
                    'state_dict': model.state_dict(),
                    'best_acc1': best_acc1,
                    'optimizer' : optimizer.state_dict(),
                }, 
                is_best = is_best,                
                filename = checkpoint_fn,
                cv_fold = cv_fold,
                use_mask = use_mask,
            )
    if use_crossval:
        holdout_loader = torch.utils.data.DataLoader(
            holdout_dataset,
            batch_size=args.batch_size, shuffle=False,
            num_workers=args.workers, pin_memory=True,
        )
        print(""=> loading best model_{}__fold_{}_best.pth.tar"".format(args.arch, cv_fold))
        checkpoint = torch.load(""model_{}__fold_{}_best.pth.tar"".format(args.arch, cv_fold))
        model.load_state_dict(checkpoint['state_dict'])
        print(""Running forward pass on holdout set of size:"", len(holdout_dataset.imgs))
        probs = get_probs(holdout_loader, model, args)
        np.save('model_{}__fold_{}__probs.npy'.format(args.arch, cv_fold), probs)",args.rank % ngpus_per_node == 0,not args.rank % ngpus_per_node
programmingbitcoin,https://github.com/jimmysong/programmingbitcoin/tree/master/code-ch05/op.py,,encode_num$9,"def encode_num(num):
    if num == 0:
        return b''
    abs_num = abs(num)
    negative = num < 0
    result = bytearray()
    while abs_num:
        result.append(abs_num & 0xff)
        abs_num >>= 8
    # if the top bit is set,
    # for negative numbers we ensure that the top bit is set
    # for positive numbers we ensure that the top bit is not set
    if result[-1] & 0x80:
        if negative:
            result.append(0x80)
        else:
            result.append(0)
    elif negative:
        result[-1] |= 0x80
    return bytes(result)",num == 0,not num
DPR,https://github.com/facebookresearch/DPR/tree/master/dpr/data/tables.py,,read_nq_tables_jsonl$181,"def read_nq_tables_jsonl(path: str, out_file: str = None) -> Dict[str, Table]:
    tables_with_issues = 0
    single_row_tables = 0
    nested_tables = 0
    regular_tables = 0
    total_tables = 0
    total_rows = 0
    tables_dict = {}

    with jsonlines.open(path, mode=""r"") as jsonl_reader:
        for jline in jsonl_reader:
            tokens = jline[""tokens""]

            if ""( hide ) This section has multiple issues"" in "" "".join(tokens):
                tables_with_issues += 1
                continue
            mask = jline[""html_mask""]
            # _page_url = jline[""doc_url""]
            title = jline[""title""]
            p = NQTableParser(tokens, mask, title)
            tables = p.parse()

            nested_tables += len(tables[1:])

            for t in tables:
                total_tables += 1

                # calc amount of non empty rows
                non_empty_rows = sum([1 for r in t.body if r.cells and any([True for c in r.cells if c.value_tokens])])

                if non_empty_rows <= 1:
                    single_row_tables += 1
                else:
                    regular_tables += 1
                    total_rows += len(t.body)

                    if t.get_key() not in tables_dict:
                        tables_dict[t.get_key()] = t

            if len(tables_dict) % 1000 == 0:
                logger.info(""tables_dict %d"", len(tables_dict))

    logger.info(""regular tables %d"", regular_tables)
    logger.info(""tables_with_issues %d"", tables_with_issues)
    logger.info(""single_row_tables %d"", single_row_tables)
    logger.info(""nested_tables %d"", nested_tables)

    if out_file:
        convert_to_csv_for_lucene(tables_dict, out_file)
    return tables_dict",len(tables_dict) % 1000 == 0,not len(tables_dict) % 1000
blam,https://github.com/stuffmatic/blam/tree/master/src/blam.py,CameraCalibrationOperator,execute$1789,"def execute(self, context):
        '''Executes the operator.
        \param context The context in which the operator was executed.
        '''
        scn = bpy.context.scene
        singleVp = scn.calibration_type == 'one_vp'
        useHorizonSegment = scn.use_horizon_segment
        setBgImg = scn.set_cambg
        
        '''
        get the active camera
        '''
        cam = scn.camera     
        if not cam:
            self.report({'ERROR'}, ""No active camera."")
            return{'CANCELLED'}
        
        '''
        check settings
        '''
        if singleVp:
            upAxisIndex = ['x', 'y', 'z'].index(scn.up_axis)
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            
            if upAxisIndex == vp1AxisIndex:
                self.report({'ERROR'}, ""The up axis cannot be parallel to the axis of the line set."")
                return{'CANCELLED'}    
            vp2AxisIndex = (set([0, 1, 2]) ^ set([upAxisIndex, vp1AxisIndex])).pop()
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
        else:
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            vp2AxisIndex = ['x', 'y', 'z'].index(scn.vp2_axis)
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
            setBgImg = scn.set_cambg
            
            if vpAxisIndices[0] == vpAxisIndices[1]:
                self.report({'ERROR'}, ""The two line sets cannot be parallel to the same axis."")
                return{'CANCELLED'}
        
        '''
        gather lines for each vanishing point
        '''        
        activeSpace = bpy.context.area.spaces.active
        
        if not activeSpace.clip:
            self.report({'ERROR'}, ""There is no active movie clip."")
            return{'CANCELLED'}
        
        #check that we have the number of layers we need
        if not activeSpace.clip.grease_pencil:
            self.report({'ERROR'}, ""There is no grease pencil datablock."")
            return{'CANCELLED'}
        gpl = activeSpace.clip.grease_pencil.layers
        if len(gpl) == 0:
            self.report({'ERROR'}, ""There are no grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and not singleVp:
            self.report({'ERROR'}, ""Calibration using two vanishing points requires two grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and singleVp and useHorizonSegment:
            self.report({'ERROR'}, ""Single vanishing point calibration with a custom horizon line requires two grease pencil layers"")
            return{'CANCELLED'}
       
        vpLineSets = self.gatherGreasePencilSegments()
        
        #check that we have the expected number of line segment strokes
        if len(vpLineSets[0]) < 2:
            self.report({'ERROR'}, ""The first grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if not singleVp and len(vpLineSets[1]) < 2:
            self.report({'ERROR'}, ""The second grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if singleVp and useHorizonSegment and len(vpLineSets[1]) != 1:
            self.report({'ERROR'}, ""The second grease pencil layer must contain exactly one line segment stroke (the horizon line)."")
            return{'CANCELLED'}    
         
        '''
        get the principal point P in image plane coordinates
        TODO: get the value from the camera data panel, 
        currently always using the image center
        '''
        imageWidth = activeSpace.clip.size[0]
        imageHeight = activeSpace.clip.size[1]
        
        #principal point in image plane coordinates. 
        #in the middle of the image by default
        P = [0, 0]
        
        if singleVp:
            '''
            calibration using a single vanishing point
            '''
            imgAspect = imageWidth / float(imageHeight)
            
            #compute the horizon direction
            horizDir = normalize([1.0, 0.0]) #flat horizon by default
            if useHorizonSegment:
                xHorizDir = imgAspect * (vpLineSets[1][0][1][0] - vpLineSets[1][0][0][0])
                yHorizDir = vpLineSets[1][0][1][1] - vpLineSets[1][0][0][1]
                horizDir = normalize([-xHorizDir, -yHorizDir])
            #print(""horizDir"", horizDir)
            
            #compute the vanishing point location
            vp1 = self.computeIntersectionPointForLineSegments(vpLineSets[0])
            
            #get the current relative focal length
            fAbs = activeSpace.clip.tracking.camera.focal_length
            sensorWidth = activeSpace.clip.tracking.camera.sensor_width
            
            f = fAbs / sensorWidth * imgAspect
            #print(""fAbs"", fAbs, ""f rel"", f)
            Fu = self.relImgCoords2ImgPlaneCoords(vp1, imageWidth, imageHeight)
            Fv = self.computeSecondVanishingPoint(Fu, f, P, horizDir)
        else:
            '''
            calibration using two vanishing points
            '''
            if scn.optical_center_type == 'camdata':
                #get the principal point location from camera data
                P = [x for x in  activeSpace.clip.tracking.camera.principal]
                #print(""camera data optical center"", P[:])
                P[0] /= imageWidth
                P[1] /= imageHeight
                #print(""normlz. optical center"", P[:])
                P = self.relImgCoords2ImgPlaneCoords(P, imageWidth, imageHeight)
            elif scn.optical_center_type == 'compute':
                if len(vpLineSets) < 3:
                    self.report({'ERROR'}, ""A third grease pencil layer is needed to compute the optical center."")
                    return{'CANCELLED'}
                #compute the principal point using a vanishing point from a third gp layer.
                #this computation does not rely on the order of the line sets
                vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(len(vpLineSets))]
                vps = [self.relImgCoords2ImgPlaneCoords(vps[i], imageWidth, imageHeight) for i in range(len(vps))]
                P = self.computeTriangleOrthocenter(vps)
            else:
                #assume optical center in image midpoint
                pass
            
            #compute the two vanishing points
            vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(2)]    
        
            #order vanishing points along the image x axis
            if vps[1][0] < vps[0][0]:
                vps.reverse()
                vpLineSets.reverse()
                vpAxisIndices.reverse()            
        
            '''
            compute focal length
            '''
            Fu = self.relImgCoords2ImgPlaneCoords(vps[0], imageWidth, imageHeight)
            Fv = self.relImgCoords2ImgPlaneCoords(vps[1], imageWidth, imageHeight)
            
            f = self.computeFocalLength(Fu, Fv, P)
            
            if f == None:
                self.report({'ERROR'}, ""Failed to compute focal length. Invalid vanishing point constellation."")
                return{'CANCELLED'}
        
        '''
        compute camera orientation
        '''
        print(Fu, Fv, f)
        #initial orientation based on the vanishing points and focal length
        M = self.computeCameraRotationMatrix(Fu, Fv, f, P)
        
        #sanity check: M should be a pure rotation matrix, 
        #so its determinant should be 1
        eps = 0.00001
        if 1.0 - M.determinant() < -eps or 1.0 - M.determinant() > eps:
            self.report({'ERROR'}, ""Non unit rotation matrix determinant: "" + str(M.determinant()))    
            #return{'CANCELLED'} 
        
        #align the camera to the coordinate axes as specified
        M = self.alignCoordinateAxes(M, vpAxisIndices[0], vpAxisIndices[1])
        #apply the transform to the camera
        cam.matrix_world = M
        
        '''
        move the camera an arbitrary distance away from the ground plane
        TODO: focus on the origin or something
        '''
        cam.location = (0, 0, 2)
    
        #compute an absolute focal length in mm based 
        #on the current camera settings
        #TODO: make sure this works for all combinations of
        #image dimensions and camera sensor settings
        if imageWidth >= imageHeight:
            fMm = cam.data.sensor_height * f
        else:
            fMm = cam.data.sensor_width * f
        cam.data.lens = fMm
        self.report({'INFO'}, ""Camera focal length set to "" + str(fMm))
        
        #move principal point of the blender camera
        r = imageWidth / float(imageHeight)
        cam.data.shift_x = -1 * P[0] / r
        cam.data.shift_y = -1 * P[1] / r
        
        '''
        set the camera background image
        '''
        bpy.context.scene.render.resolution_x = imageWidth
        bpy.context.scene.render.resolution_y = imageHeight
        
        if setBgImg:
            bpy.ops.clip.set_viewport_background()
            
        return{'FINISHED'}",len(gpl) == 0,not gpl
blam,https://github.com/stuffmatic/blam/tree/master/src/blam.py,CameraCalibrationOperator,execute$1789,"def execute(self, context):
        '''Executes the operator.
        \param context The context in which the operator was executed.
        '''
        scn = bpy.context.scene
        singleVp = scn.calibration_type == 'one_vp'
        useHorizonSegment = scn.use_horizon_segment
        setBgImg = scn.set_cambg
        
        '''
        get the active camera
        '''
        cam = scn.camera     
        if not cam:
            self.report({'ERROR'}, ""No active camera."")
            return{'CANCELLED'}
        
        '''
        check settings
        '''
        if singleVp:
            upAxisIndex = ['x', 'y', 'z'].index(scn.up_axis)
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            
            if upAxisIndex == vp1AxisIndex:
                self.report({'ERROR'}, ""The up axis cannot be parallel to the axis of the line set."")
                return{'CANCELLED'}    
            vp2AxisIndex = (set([0, 1, 2]) ^ set([upAxisIndex, vp1AxisIndex])).pop()
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
        else:
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            vp2AxisIndex = ['x', 'y', 'z'].index(scn.vp2_axis)
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
            setBgImg = scn.set_cambg
            
            if vpAxisIndices[0] == vpAxisIndices[1]:
                self.report({'ERROR'}, ""The two line sets cannot be parallel to the same axis."")
                return{'CANCELLED'}
        
        '''
        gather lines for each vanishing point
        '''        
        activeSpace = bpy.context.area.spaces.active
        
        if not activeSpace.clip:
            self.report({'ERROR'}, ""There is no active movie clip."")
            return{'CANCELLED'}
        
        #check that we have the number of layers we need
        if not activeSpace.clip.grease_pencil:
            self.report({'ERROR'}, ""There is no grease pencil datablock."")
            return{'CANCELLED'}
        gpl = activeSpace.clip.grease_pencil.layers
        if len(gpl) == 0:
            self.report({'ERROR'}, ""There are no grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and not singleVp:
            self.report({'ERROR'}, ""Calibration using two vanishing points requires two grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and singleVp and useHorizonSegment:
            self.report({'ERROR'}, ""Single vanishing point calibration with a custom horizon line requires two grease pencil layers"")
            return{'CANCELLED'}
       
        vpLineSets = self.gatherGreasePencilSegments()
        
        #check that we have the expected number of line segment strokes
        if len(vpLineSets[0]) < 2:
            self.report({'ERROR'}, ""The first grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if not singleVp and len(vpLineSets[1]) < 2:
            self.report({'ERROR'}, ""The second grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if singleVp and useHorizonSegment and len(vpLineSets[1]) != 1:
            self.report({'ERROR'}, ""The second grease pencil layer must contain exactly one line segment stroke (the horizon line)."")
            return{'CANCELLED'}    
         
        '''
        get the principal point P in image plane coordinates
        TODO: get the value from the camera data panel, 
        currently always using the image center
        '''
        imageWidth = activeSpace.clip.size[0]
        imageHeight = activeSpace.clip.size[1]
        
        #principal point in image plane coordinates. 
        #in the middle of the image by default
        P = [0, 0]
        
        if singleVp:
            '''
            calibration using a single vanishing point
            '''
            imgAspect = imageWidth / float(imageHeight)
            
            #compute the horizon direction
            horizDir = normalize([1.0, 0.0]) #flat horizon by default
            if useHorizonSegment:
                xHorizDir = imgAspect * (vpLineSets[1][0][1][0] - vpLineSets[1][0][0][0])
                yHorizDir = vpLineSets[1][0][1][1] - vpLineSets[1][0][0][1]
                horizDir = normalize([-xHorizDir, -yHorizDir])
            #print(""horizDir"", horizDir)
            
            #compute the vanishing point location
            vp1 = self.computeIntersectionPointForLineSegments(vpLineSets[0])
            
            #get the current relative focal length
            fAbs = activeSpace.clip.tracking.camera.focal_length
            sensorWidth = activeSpace.clip.tracking.camera.sensor_width
            
            f = fAbs / sensorWidth * imgAspect
            #print(""fAbs"", fAbs, ""f rel"", f)
            Fu = self.relImgCoords2ImgPlaneCoords(vp1, imageWidth, imageHeight)
            Fv = self.computeSecondVanishingPoint(Fu, f, P, horizDir)
        else:
            '''
            calibration using two vanishing points
            '''
            if scn.optical_center_type == 'camdata':
                #get the principal point location from camera data
                P = [x for x in  activeSpace.clip.tracking.camera.principal]
                #print(""camera data optical center"", P[:])
                P[0] /= imageWidth
                P[1] /= imageHeight
                #print(""normlz. optical center"", P[:])
                P = self.relImgCoords2ImgPlaneCoords(P, imageWidth, imageHeight)
            elif scn.optical_center_type == 'compute':
                if len(vpLineSets) < 3:
                    self.report({'ERROR'}, ""A third grease pencil layer is needed to compute the optical center."")
                    return{'CANCELLED'}
                #compute the principal point using a vanishing point from a third gp layer.
                #this computation does not rely on the order of the line sets
                vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(len(vpLineSets))]
                vps = [self.relImgCoords2ImgPlaneCoords(vps[i], imageWidth, imageHeight) for i in range(len(vps))]
                P = self.computeTriangleOrthocenter(vps)
            else:
                #assume optical center in image midpoint
                pass
            
            #compute the two vanishing points
            vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(2)]    
        
            #order vanishing points along the image x axis
            if vps[1][0] < vps[0][0]:
                vps.reverse()
                vpLineSets.reverse()
                vpAxisIndices.reverse()            
        
            '''
            compute focal length
            '''
            Fu = self.relImgCoords2ImgPlaneCoords(vps[0], imageWidth, imageHeight)
            Fv = self.relImgCoords2ImgPlaneCoords(vps[1], imageWidth, imageHeight)
            
            f = self.computeFocalLength(Fu, Fv, P)
            
            if f == None:
                self.report({'ERROR'}, ""Failed to compute focal length. Invalid vanishing point constellation."")
                return{'CANCELLED'}
        
        '''
        compute camera orientation
        '''
        print(Fu, Fv, f)
        #initial orientation based on the vanishing points and focal length
        M = self.computeCameraRotationMatrix(Fu, Fv, f, P)
        
        #sanity check: M should be a pure rotation matrix, 
        #so its determinant should be 1
        eps = 0.00001
        if 1.0 - M.determinant() < -eps or 1.0 - M.determinant() > eps:
            self.report({'ERROR'}, ""Non unit rotation matrix determinant: "" + str(M.determinant()))    
            #return{'CANCELLED'} 
        
        #align the camera to the coordinate axes as specified
        M = self.alignCoordinateAxes(M, vpAxisIndices[0], vpAxisIndices[1])
        #apply the transform to the camera
        cam.matrix_world = M
        
        '''
        move the camera an arbitrary distance away from the ground plane
        TODO: focus on the origin or something
        '''
        cam.location = (0, 0, 2)
    
        #compute an absolute focal length in mm based 
        #on the current camera settings
        #TODO: make sure this works for all combinations of
        #image dimensions and camera sensor settings
        if imageWidth >= imageHeight:
            fMm = cam.data.sensor_height * f
        else:
            fMm = cam.data.sensor_width * f
        cam.data.lens = fMm
        self.report({'INFO'}, ""Camera focal length set to "" + str(fMm))
        
        #move principal point of the blender camera
        r = imageWidth / float(imageHeight)
        cam.data.shift_x = -1 * P[0] / r
        cam.data.shift_y = -1 * P[1] / r
        
        '''
        set the camera background image
        '''
        bpy.context.scene.render.resolution_x = imageWidth
        bpy.context.scene.render.resolution_y = imageHeight
        
        if setBgImg:
            bpy.ops.clip.set_viewport_background()
            
        return{'FINISHED'}",f == None,not f
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,MdeReferenceOrganizeCommand,run$519,"def run(self, edit):
        """"""Run command callback.""""""
        view = self.view

        # reorder
        markers = getMarkers(view)
        marker_order = sorted(
            markers.keys(), key=lambda marker: min(markers[marker].regions, key=lambda reg: reg.a).a
        )
        marker_order = dict(zip(marker_order, range(0, len(marker_order))))

        refs = getReferences(view)
        flatrefs = []
        flatfns = []
        sel = view.sel()
        sel.clear()
        for name in refs:
            for link_reg in refs[name].regions:
                line_reg = view.full_line(link_reg)
                if name[0] == ""^"":
                    flatfns.append((name, view.substr(line_reg).strip(""\n"")))
                else:
                    flatrefs.append((name, view.substr(line_reg).strip(""\n"")))
                sel.add(line_reg)

        flatfns.sort(key=operator.itemgetter(0))
        flatrefs.sort(
            key=lambda x: marker_order[x[0].lower()] if x[0].lower() in marker_order else 9999
        )

        view.run_command(""left_delete"")
        if view.size() >= 2 and view.substr(sublime.Region(view.size() - 2, view.size())) == ""\n\n"":
            view.erase(edit, sublime.Region(view.size() - 1, view.size()))
        for fn_tuple in flatfns:
            view.insert(edit, view.size(), fn_tuple[1])
            view.insert(edit, view.size(), ""\n"")
        view.insert(edit, view.size(), ""\n"")

        for ref_tuple in flatrefs:
            view.insert(edit, view.size(), ref_tuple[1])
            view.insert(edit, view.size(), ""\n"")

        # delete duplicate / report conflict
        sel.clear()
        refs = getReferences(view)
        conflicts = {}
        unique_links = {}
        output = """"

        for name in refs:
            if name[0] == ""^"":
                continue
            n_links = len(refs[name].regions)
            if n_links > 1:
                for ref in refs[name].regions:
                    link_begin = findScopeFrom(view, ref.end(), ref_link_scope_name)
                    link = view.substr(getCurrentScopeRegion(view, link_begin))
                    if name in unique_links:
                        if link == unique_links[name]:
                            output += ""%s has duplicate value of %s\n"" % (refs[name].label, link)
                            sel.add(view.full_line(ref.begin()))
                        elif name in conflicts:
                            conflicts[name].append(link)
                        else:
                            conflicts[name] = [link]
                    else:
                        unique_links[name] = link

        # view.run_command(""left_delete"")

        for name in conflicts:
            output += ""%s has conflict values: %s with %s\n"" % (
                refs[name].label,
                unique_links[name],
                "", "".join(conflicts[name]),
            )

        # report missing
        refs = getReferences(view)
        lower_refs = [ref.lower() for ref in refs]
        missings = []
        for ref in refs:
            if ref not in marker_order:
                missings.append(refs[ref].label)
        if len(missings) > 0:
            if len(missings) > 1:
                noun, verb = ""Definitions"", ""have""
            else:
                noun, verb = ""Definition"", ""has""

            output += ""Error: %s %s %s no reference\n"" % (
                noun,
                repr(missings),
                verb,
            )

        missings = []
        for marker in markers:
            if marker not in lower_refs:
                missings.append(markers[marker].label)
        if len(missings) > 0:
            if len(missings) > 1:
                noun, verb = ""References"", ""have""
            else:
                noun, verb = ""Reference"", ""has""

            output += ""Error: %s %s %s no definition\n"" % (
                noun,
                repr(missings),
                verb,
            )

        # sel.clear()
        if len(output) == 0:
            output = ""All references are well defined :)\n""

        output += ""===================\n""

        def get_times_string(n):
            if n == 0:
                return ""0 time""
            elif n == 1:
                return ""1 time""
            else:
                return ""%i times"" % n

        output += ""\n"".join(
            (
                ""[%s] is referenced %s""
                % (markers[m].label, get_times_string(len(markers[m].regions)))
            )
            for m in markers
        )

        window = view.window()
        output_panel = window.create_output_panel(""mde"")
        output_panel.run_command(""erase_view"")
        output_panel.run_command(""append"", {""characters"": output})
        window.run_command(""show_panel"", {""panel"": ""output.mde""})",len(output) == 0,not output
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,MdeReferenceOrganizeCommand,run$519,"def run(self, edit):
        """"""Run command callback.""""""
        view = self.view

        # reorder
        markers = getMarkers(view)
        marker_order = sorted(
            markers.keys(), key=lambda marker: min(markers[marker].regions, key=lambda reg: reg.a).a
        )
        marker_order = dict(zip(marker_order, range(0, len(marker_order))))

        refs = getReferences(view)
        flatrefs = []
        flatfns = []
        sel = view.sel()
        sel.clear()
        for name in refs:
            for link_reg in refs[name].regions:
                line_reg = view.full_line(link_reg)
                if name[0] == ""^"":
                    flatfns.append((name, view.substr(line_reg).strip(""\n"")))
                else:
                    flatrefs.append((name, view.substr(line_reg).strip(""\n"")))
                sel.add(line_reg)

        flatfns.sort(key=operator.itemgetter(0))
        flatrefs.sort(
            key=lambda x: marker_order[x[0].lower()] if x[0].lower() in marker_order else 9999
        )

        view.run_command(""left_delete"")
        if view.size() >= 2 and view.substr(sublime.Region(view.size() - 2, view.size())) == ""\n\n"":
            view.erase(edit, sublime.Region(view.size() - 1, view.size()))
        for fn_tuple in flatfns:
            view.insert(edit, view.size(), fn_tuple[1])
            view.insert(edit, view.size(), ""\n"")
        view.insert(edit, view.size(), ""\n"")

        for ref_tuple in flatrefs:
            view.insert(edit, view.size(), ref_tuple[1])
            view.insert(edit, view.size(), ""\n"")

        # delete duplicate / report conflict
        sel.clear()
        refs = getReferences(view)
        conflicts = {}
        unique_links = {}
        output = """"

        for name in refs:
            if name[0] == ""^"":
                continue
            n_links = len(refs[name].regions)
            if n_links > 1:
                for ref in refs[name].regions:
                    link_begin = findScopeFrom(view, ref.end(), ref_link_scope_name)
                    link = view.substr(getCurrentScopeRegion(view, link_begin))
                    if name in unique_links:
                        if link == unique_links[name]:
                            output += ""%s has duplicate value of %s\n"" % (refs[name].label, link)
                            sel.add(view.full_line(ref.begin()))
                        elif name in conflicts:
                            conflicts[name].append(link)
                        else:
                            conflicts[name] = [link]
                    else:
                        unique_links[name] = link

        # view.run_command(""left_delete"")

        for name in conflicts:
            output += ""%s has conflict values: %s with %s\n"" % (
                refs[name].label,
                unique_links[name],
                "", "".join(conflicts[name]),
            )

        # report missing
        refs = getReferences(view)
        lower_refs = [ref.lower() for ref in refs]
        missings = []
        for ref in refs:
            if ref not in marker_order:
                missings.append(refs[ref].label)
        if len(missings) > 0:
            if len(missings) > 1:
                noun, verb = ""Definitions"", ""have""
            else:
                noun, verb = ""Definition"", ""has""

            output += ""Error: %s %s %s no reference\n"" % (
                noun,
                repr(missings),
                verb,
            )

        missings = []
        for marker in markers:
            if marker not in lower_refs:
                missings.append(markers[marker].label)
        if len(missings) > 0:
            if len(missings) > 1:
                noun, verb = ""References"", ""have""
            else:
                noun, verb = ""Reference"", ""has""

            output += ""Error: %s %s %s no definition\n"" % (
                noun,
                repr(missings),
                verb,
            )

        # sel.clear()
        if len(output) == 0:
            output = ""All references are well defined :)\n""

        output += ""===================\n""

        def get_times_string(n):
            if n == 0:
                return ""0 time""
            elif n == 1:
                return ""1 time""
            else:
                return ""%i times"" % n

        output += ""\n"".join(
            (
                ""[%s] is referenced %s""
                % (markers[m].label, get_times_string(len(markers[m].regions)))
            )
            for m in markers
        )

        window = view.window()
        output_panel = window.create_output_panel(""mde"")
        output_panel.run_command(""erase_view"")
        output_panel.run_command(""append"", {""characters"": output})
        window.run_command(""show_panel"", {""panel"": ""output.mde""})",n == 0,not n
ezdxf,https://github.com/mozman/ezdxf/tree/master/tests/test_02_dxf_graphics/test_228_mesh.py,,test_mesh_dxf_attribs$115,"def test_mesh_dxf_attribs(mesh):
    assert 2 == mesh.dxf.version
    assert 0 == mesh.dxf.blend_crease
    assert 3 == mesh.dxf.subdivision_levels",0 == mesh.dxf.blend_crease,not mesh.dxf.blend_crease
astropy,https://github.com/astropy/astropy/tree/master/astropy/io/fits/tests/test_header.py,TestHeaderFunctions,test_hierarch_create_and_update$762,"def test_hierarch_create_and_update(self):
        """"""
        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/158

        Tests several additional use cases for working with HIERARCH cards.
        """"""

        msg = ""a HIERARCH card will be created""

        header = fits.Header()
        with pytest.warns(VerifyWarning) as w:
            header.update({""HIERARCH BLAH BLAH"": ""TESTA""})
            assert len(w) == 0
            assert ""BLAH BLAH"" in header
            assert header[""BLAH BLAH""] == ""TESTA""

            header.update({""HIERARCH BLAH BLAH"": ""TESTB""})
            assert len(w) == 0
            assert header[""BLAH BLAH""], ""TESTB""

            # Update without explicitly stating 'HIERARCH':
            header.update({""BLAH BLAH"": ""TESTC""})
            assert len(w) == 1
            assert len(header) == 1
            assert header[""BLAH BLAH""], ""TESTC""

            # Test case-insensitivity
            header.update({""HIERARCH blah blah"": ""TESTD""})
            assert len(w) == 1
            assert len(header) == 1
            assert header[""blah blah""], ""TESTD""

            header.update({""blah blah"": ""TESTE""})
            assert len(w) == 2
            assert len(header) == 1
            assert header[""blah blah""], ""TESTE""

            # Create a HIERARCH card > 8 characters without explicitly stating
            # 'HIERARCH'
            header.update({""BLAH BLAH BLAH"": ""TESTA""})
            assert len(w) == 3
            assert msg in str(w[0].message)

            header.update({""HIERARCH BLAH BLAH BLAH"": ""TESTB""})
            assert len(w) == 3
            assert header[""BLAH BLAH BLAH""], ""TESTB""

            # Update without explicitly stating 'HIERARCH':
            header.update({""BLAH BLAH BLAH"": ""TESTC""})
            assert len(w) == 4
            assert header[""BLAH BLAH BLAH""], ""TESTC""

            # Test case-insensitivity
            header.update({""HIERARCH blah blah blah"": ""TESTD""})
            assert len(w) == 4
            assert header[""blah blah blah""], ""TESTD""

            header.update({""blah blah blah"": ""TESTE""})
            assert len(w) == 5
            assert header[""blah blah blah""], ""TESTE""",len(w) == 0,not w
astropy,https://github.com/astropy/astropy/tree/master/astropy/io/fits/tests/test_header.py,TestHeaderFunctions,test_hierarch_create_and_update$762,"def test_hierarch_create_and_update(self):
        """"""
        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/158

        Tests several additional use cases for working with HIERARCH cards.
        """"""

        msg = ""a HIERARCH card will be created""

        header = fits.Header()
        with pytest.warns(VerifyWarning) as w:
            header.update({""HIERARCH BLAH BLAH"": ""TESTA""})
            assert len(w) == 0
            assert ""BLAH BLAH"" in header
            assert header[""BLAH BLAH""] == ""TESTA""

            header.update({""HIERARCH BLAH BLAH"": ""TESTB""})
            assert len(w) == 0
            assert header[""BLAH BLAH""], ""TESTB""

            # Update without explicitly stating 'HIERARCH':
            header.update({""BLAH BLAH"": ""TESTC""})
            assert len(w) == 1
            assert len(header) == 1
            assert header[""BLAH BLAH""], ""TESTC""

            # Test case-insensitivity
            header.update({""HIERARCH blah blah"": ""TESTD""})
            assert len(w) == 1
            assert len(header) == 1
            assert header[""blah blah""], ""TESTD""

            header.update({""blah blah"": ""TESTE""})
            assert len(w) == 2
            assert len(header) == 1
            assert header[""blah blah""], ""TESTE""

            # Create a HIERARCH card > 8 characters without explicitly stating
            # 'HIERARCH'
            header.update({""BLAH BLAH BLAH"": ""TESTA""})
            assert len(w) == 3
            assert msg in str(w[0].message)

            header.update({""HIERARCH BLAH BLAH BLAH"": ""TESTB""})
            assert len(w) == 3
            assert header[""BLAH BLAH BLAH""], ""TESTB""

            # Update without explicitly stating 'HIERARCH':
            header.update({""BLAH BLAH BLAH"": ""TESTC""})
            assert len(w) == 4
            assert header[""BLAH BLAH BLAH""], ""TESTC""

            # Test case-insensitivity
            header.update({""HIERARCH blah blah blah"": ""TESTD""})
            assert len(w) == 4
            assert header[""blah blah blah""], ""TESTD""

            header.update({""blah blah blah"": ""TESTE""})
            assert len(w) == 5
            assert header[""blah blah blah""], ""TESTE""",len(w) == 0,not w
SReC,https://github.com/caoscott/SReC/tree/master/src/network.py,Bits,add_bits$103,"def add_bits(self, other: ""Bits"") -> ""Bits"":
        keys = other.get_keys()
        assert keys == self.get_keys() or len(self.get_keys()) == 0, (
            f""{self.get_keys()} != {keys}"")

        for key in keys:
            self.key_to_bits[key] += other.get_bits(key)
            self.key_to_sizes[key] += other.get_size(key)
            # Don't do anything with self.key_to_probs at the moment.
        return self",len(self.get_keys()) == 0,not self.get_keys()
python_video_stab,https://github.com/AdamSpannbauer/python_video_stab/tree/master/vidstab/VidStab.py,VidStab,_process_first_frame$189,"def _process_first_frame(self, array=None):
        # read first frame
        _, _, _ = self.frame_queue.read_frame(array=array, pop_ind=False)

        if array is None and len(self.frame_queue.frames) == 0:
            raise ValueError('First frame is None. Check if input file/stream is correct.')

        # convert to gray scale
        prev_frame = self.frame_queue.frames[-1]
        prev_frame_gray = prev_frame.gray_image
        prev_frame_gray = self._resize_frame(prev_frame_gray)

        # detect keypoints
        prev_kps = self.kp_detector.detect(prev_frame_gray)
        # noinspection PyArgumentList
        self.prev_kps = np.array([kp.pt for kp in prev_kps], dtype='float32').reshape(-1, 1, 2)

        self.prev_gray = prev_frame_gray[:]",len(self.frame_queue.frames) == 0,not self.frame_queue.frames
sparrow-wifi,https://github.com/ghostop14/sparrow-wifi/tree/master//sparrowbluetooth.py,SparrowBluetooth,ubertoothSpecanRunning$1019,"def ubertoothSpecanRunning():
        result = subprocess.run(['pgrep', '-f','ubertooth-specan'], stdout=subprocess.PIPE,stderr=subprocess.DEVNULL)

        # it's a grep, so if the pattern exists, pgrep returns 0, else it returns 1 (or something greater than 0 as an err_not_found
        if result.returncode == 0:
            return True
        else:
            return False",result.returncode == 0,not result.returncode
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/scripts/processing/apply_subword.py,,main$111,"def main(args):
    start = time.time()
    if args.model == 'spm':
        assert args.model_path is not None, 'Must specify --model_path when using the ""spm"" model.'
        tokenizer_model = tokenizers.create('spm',
                                            model_path=args.model_path,
                                            vocab=args.vocab_path)
    elif args.model == 'subword_nmt':
        assert args.model_path is not None,\
            'Must specify --model_path when using the ""subword_nmt"" model.'
        assert args.vocab_path is not None, \
            'Must specify --vocab_path when using the ""subword_nmt"" model.'
        tokenizer_model = tokenizers.create('subword_nmt',
                                            model_path=args.model_path,
                                            vocab=args.vocab_path,
                                            bpe_dropout=args.bpe_dropout)
    elif args.model == 'yttm':
        assert args.model_path is not None,\
            'Must specify --model_path when using the ""subword_nmt"" model.'
        args.bpe_dropout = 0.0 if not args.bpe_dropout else args.bpe_dropout
        tokenizer_model = tokenizers.create('yttm',
                                            model_path=args.model_path,
                                            vocab=args.vocab_path,
                                            bpe_dropout=args.bpe_dropout,
                                            n_threads=1)
    elif args.model == 'hf_bytebpe' or 'hf_bpe' or 'hf_wordpiece':
        if is_new_version_model_file(args.model_path):
            assert args.model_path is not None, \
                'Must specify --model_path when using the ""{}"" model.'.format(args.model)
            assert args.vocab_path is not None, \
                'Must specify --vocab_path when using the ""{}"" model.'.format(args.model)
            tokenizer_model = tokenizers.create('hf_tokenizer',
                                                model_path=args.model_path,
                                                vocab=args.vocab_path)
        else:
            if args.model == 'hf_bytebpe':
                tokenizer_model = tokenizers.create('hf_bytebpe',
                                                    merges_file=args.model_path,
                                                    vocab_file=args.vocab_path,
                                                    dropout=args.bpe_dropout,
                                                    lowercase=args.lowercase)
            elif args.model == 'hf_wordpiece':
                tokenizer_model = tokenizers.create('hf_wordpiece',
                                                    vocab_file=args.vocab_path,
                                                    lowercase=args.lowercase,
                                                    strip_accents=args.strip_accents)
            elif args.model == 'hf_bpe':
                tokenizer_model = tokenizers.create('hf_bpe',
                                                    merges_file=args.model_path,
                                                    vocab_file=args.vocab_path,
                                                    dropout=args.bpe_dropout,
                                                    lowercase=args.lowercase)
    else:
        raise NotImplementedError
    print('Applying ""{}"" to ""{}"" and save to ""{}""'
          .format(tokenizer_model.__class__.__name__,
                  ', '.join(args.corpus),
                  args.save_path))
    output_type = {'subword': str, 'id': int}[args.output_type]
    applyer = ParallelCorpusApplyer(args.corpus, tokenizer_model, output_type)
    with open(args.save_path, 'w', encoding='utf-8', newline='\n') as fo:
        with Pool(args.num_process) as pool:
            sentence_count = token_count = unk_count = 0
            for i, (tokenized_sentences, sentence_num, token_num, unk_num) in \
                enumerate(pool.imap(applyer.process_chunk, applyer.chunk_iter())):
                fo.write('\n'.join(tokenized_sentences))
                fo.write('\n')
                sentence_count += sentence_num
                token_count += token_num
                unk_count += unk_num
                if (i + 1) % 100 == 0:
                    print('Chunk {} , #Lines processed: {}'
                          .format(i + 1, sentence_count))
    end = time.time()
    print('Done, #Lines processed {}, Avg tokens of sentences {:.1f},'
          'Unknown rate {:.1f}%, Time spent {}'
          .format(sentence_count, token_count / sentence_count,
                  unk_count * 100 / token_count, end - start))",(i + 1) % 100 == 0,not (i + 1) % 100
hachoir,https://github.com/vstinner/hachoir/tree/master/hachoir/parser/container/mp4.py,SampleSizeTable,createFields$952,"def createFields(self):
        yield UInt8(self, ""version"")
        yield NullBits(self, ""flags"", 24)
        yield UInt32(self, ""uniform_size"", description=""Uniform size of each sample (0 if non-uniform)"")
        yield UInt32(self, ""count"", description=""Number of samples"")
        if self['uniform_size'].value == 0:
            for i in range(self['count'].value):
                yield UInt32(self, ""sample_size[]"")",self['uniform_size'].value == 0,not self['uniform_size'].value
ansible,https://github.com/ansible/ansible/tree/master/lib/ansible/modules/service.py,LinuxService,get_service_tools$465,"def get_service_tools(self):

        paths = ['/sbin', '/usr/sbin', '/bin', '/usr/bin']
        binaries = ['service', 'chkconfig', 'update-rc.d', 'rc-service', 'rc-update', 'initctl', 'systemctl', 'start', 'stop', 'restart', 'insserv']
        initpaths = ['/etc/init.d']
        location = dict()

        for binary in binaries:
            location[binary] = self.module.get_bin_path(binary, opt_dirs=paths)

        for initdir in initpaths:
            initscript = ""%s/%s"" % (initdir, self.name)
            if os.path.isfile(initscript):
                self.svc_initscript = initscript

        def check_systemd():

            # tools must be installed
            if location.get('systemctl', False):

                # this should show if systemd is the boot init system
                # these mirror systemd's own sd_boot test http://www.freedesktop.org/software/systemd/man/sd_booted.html
                for canary in [""/run/systemd/system/"", ""/dev/.run/systemd/"", ""/dev/.systemd/""]:
                    if os.path.exists(canary):
                        return True

                # If all else fails, check if init is the systemd command, using comm as cmdline could be symlink
                try:
                    f = open('/proc/1/comm', 'r')
                except IOError:
                    # If comm doesn't exist, old kernel, no systemd
                    return False

                for line in f:
                    if 'systemd' in line:
                        return True

            return False

        # Locate a tool to enable/disable a service
        if check_systemd():
            # service is managed by systemd
            self.__systemd_unit = self.name
            self.svc_cmd = location['systemctl']
            self.enable_cmd = location['systemctl']

        elif location.get('initctl', False) and os.path.exists(""/etc/init/%s.conf"" % self.name):
            # service is managed by upstart
            self.enable_cmd = location['initctl']
            # set the upstart version based on the output of 'initctl version'
            self.upstart_version = LooseVersion('0.0.0')
            try:
                version_re = re.compile(r'\(upstart (.*)\)')
                rc, stdout, stderr = self.module.run_command('%s version' % location['initctl'])
                if rc == 0:
                    res = version_re.search(stdout)
                    if res:
                        self.upstart_version = LooseVersion(res.groups()[0])
            except Exception:
                pass  # we'll use the default of 0.0.0

            self.svc_cmd = location['initctl']

        elif location.get('rc-service', False):
            # service is managed by OpenRC
            self.svc_cmd = location['rc-service']
            self.enable_cmd = location['rc-update']
            return  # already have service start/stop tool too!

        elif self.svc_initscript:
            # service is managed by with SysV init scripts
            if location.get('update-rc.d', False):
                # and uses update-rc.d
                self.enable_cmd = location['update-rc.d']
            elif location.get('insserv', None):
                # and uses insserv
                self.enable_cmd = location['insserv']
            elif location.get('chkconfig', False):
                # and uses chkconfig
                self.enable_cmd = location['chkconfig']

        if self.enable_cmd is None:
            fail_if_missing(self.module, False, self.name, msg='host')

        # If no service control tool selected yet, try to see if 'service' is available
        if self.svc_cmd is None and location.get('service', False):
            self.svc_cmd = location['service']

        # couldn't find anything yet
        if self.svc_cmd is None and not self.svc_initscript:
            self.module.fail_json(msg='cannot find \'service\' binary or init script for service,  possible typo in service name?, aborting')

        if location.get('initctl', False):
            self.svc_initctl = location['initctl']",rc == 0,not rc
apex,https://github.com/NVIDIA/apex/tree/master/apex/transformer/pipeline_parallel/utils.py,,_reconfigure_microbatch_calculator$71,"def _reconfigure_microbatch_calculator(
        rank: int,
        rampup_batch_size: Optional[List[int]],
        global_batch_size: int,
        micro_batch_size: int,
        data_parallel_size: int,
) -> None:
    if torch.distributed.get_rank() == 0:
        import warnings
        warnings.warn(""This function is only for unittest"")
    global _GLOBAL_NUM_MICROBATCHES_CALCULATOR

    _GLOBAL_NUM_MICROBATCHES_CALCULATOR = build_num_microbatches_calculator(
        rank, rampup_batch_size, global_batch_size, micro_batch_size, data_parallel_size)",torch.distributed.get_rank() == 0,not torch.distributed.get_rank()
chainer,https://github.com/chainer/chainer/tree/master/examples/text_classification/text_datasets.py,,read_other_dataset$136,"def read_other_dataset(path, shrink=1, char_based=False):
    dataset = []
    with io.open(path, encoding='utf-8', errors='ignore') as f:
        for i, l in enumerate(f):
            if i % shrink != 0 or not len(l.strip()) >= 3:
                continue
            label, text = l.strip().split(None, 1)
            label = int(label)
            tokens = split_text(normalize_text(text), char_based)
            dataset.append((tokens, label))
    return dataset",i % shrink != 0,i % shrink
PaperTTY,https://github.com/joukos/PaperTTY/tree/master/papertty/drivers/drivers_full.py,EPD2in7,init$171,"def init(self, **kwargs):
        if self.epd_init() != 0:
            return -1
        # EPD hardware init start
        self.reset()
        self.send_command(self.POWER_SETTING)
        self.send_data(0x03)  # VDS_EN, VDG_EN
        self.send_data(0x00)  # VCOM_HV, VGHL_LV[1], VGHL_LV[0]
        self.send_data(0x2b)  # VDH
        self.send_data(0x2b)  # VDL
        self.send_data(0x09)  # VDHR
        self.send_command(self.BOOSTER_SOFT_START)
        self.send_data(0x07)
        self.send_data(0x07)
        self.send_data(0x17)
        # Power optimization
        self.send_command(0xF8)
        self.send_data(0x60)
        self.send_data(0xA5)
        # Power optimization
        self.send_command(0xF8)
        self.send_data(0x89)
        self.send_data(0xA5)
        # Power optimization
        self.send_command(0xF8)
        self.send_data(0x90)
        self.send_data(0x00)
        # Power optimization
        self.send_command(0xF8)
        self.send_data(0x93)
        self.send_data(0x2A)
        # Power optimization
        self.send_command(0xF8)
        self.send_data(0xA0)
        self.send_data(0xA5)
        # Power optimization
        self.send_command(0xF8)
        self.send_data(0xA1)
        self.send_data(0x00)
        # Power optimization
        self.send_command(0xF8)
        self.send_data(0x73)
        self.send_data(0x41)
        self.send_command(self.PARTIAL_DISPLAY_REFRESH)
        self.send_data(0x00)
        self.send_command(self.POWER_ON)
        self.wait_until_idle()

        self.send_command(self.PANEL_SETTING)
        self.send_data(0xAF)  # KW-BF   KWR-AF    BWROTP 0f
        self.send_command(self.PLL_CONTROL)
        self.send_data(0x3A)  # 3A 100HZ   29 150Hz 39 200HZ    31 171HZ
        self.send_command(self.VCM_DC_SETTING_REGISTER)
        self.send_data(0x12)
        self.delay_ms(2)
        self.set_lut()
        # EPD hardware init end
        return 0",self.epd_init() != 0,self.epd_init()
SSDTTime,https://github.com/corpnewt/SSDTTime/tree/master/Scripts/dsdt.py,DSDT,get_devices$340,"def get_devices(self,search=None,types=(""Device ("",""Scope (""),strip_comments=False):
        # Returns a list of tuples organized as (Device/Scope,d_s_index,matched_index)
        if search == None:
            return []
        last_device = None
        device_index = 0
        devices = []
        for index,line in enumerate(self.dsdt_lines):
            if self.is_hex(line):
                continue
            line = self.get_line(line) if strip_comments else line
            if any ((x for x in types if x in line)):
                # Got a last_device match
                last_device = line
                device_index = index
            if search in line:
                # Got a search hit - add it
                devices.append((last_device,device_index,index))
        return devices",search == None,not search
variety,https://github.com/varietywalls/variety/tree/master/variety/VarietyWindow.py,VarietyWindow,set_wallpaper$1766,"def set_wallpaper(self, img, auto_changed=False):
        logger.info(lambda: ""Calling set_wallpaper with "" + img)
        if img == self.current and not self.is_current_refreshable():
            return
        if os.access(img, os.R_OK):
            at_front = self.position == 0
            self.used = self.used[self.position :]
            if len(self.used) == 0 or self.used[0] != img:
                self.used.insert(0, img)
                self.refresh_thumbs_history(img, at_front)
            self.position = 0
            if len(self.used) > 1000:
                self.used = self.used[:1000]

            self._remove_from_unseen(img)

            self.auto_changed = auto_changed
            self.last_change_time = time.time()
            self.set_wp_throttled(img)

            # Unsplash API requires that we call their download endpoint
            # when setting the wallpaper, not when queueing it:
            meta = Util.read_metadata(img)
            if meta and ""sourceType"" in meta:
                for image_source in Options.IMAGE_SOURCES:
                    if image_source.get_source_type() == meta[""sourceType""]:

                        def _do_hook():
                            image_source.on_image_set_as_wallpaper(img, meta)

                        threading.Timer(0, _do_hook).start()
        else:
            logger.warning(lambda: ""set_wallpaper called with unaccessible image "" + img)",len(self.used) == 0,not self.used
mlrun,https://github.com/mlrun/mlrun/tree/master/tests/api/api/test_projects.py,,_list_project_names_and_assert$1224,"def _list_project_names_and_assert(
    client: TestClient, expected_names: typing.List[str], params: typing.Dict = None
):
    params = params or {}
    params[""format""] = mlrun.api.schemas.ProjectsFormat.name_only
    # list - names only - filter by state
    response = client.get(
        ""projects"",
        params=params,
    )
    assert (
        deepdiff.DeepDiff(
            expected_names,
            response.json()[""projects""],
            ignore_order=True,
        )
        == {}
    )","deepdiff.DeepDiff(expected_names, response.json()['projects'], ignore_order=True) == {}","not deepdiff.DeepDiff(expected_names, response.json()['projects'], ignore_order=True)"
pandas-ta,https://github.com/twopirllc/pandas-ta/tree/master/pandas_ta/trend/short_run.py,,short_run$7,"def short_run(fast, slow, length=None, offset=None, **kwargs):
    """"""Indicator: Short Run""""""
    # Validate Arguments
    length = int(length) if length and length > 0 else 2
    fast = verify_series(fast, length)
    slow = verify_series(slow, length)
    offset = get_offset(offset)

    if fast is None or slow is None: return

    # Calculate Result
    pt = decreasing(fast, length) & increasing(slow, length)  # potential top or top
    bd = decreasing(fast, length) & decreasing(slow, length)  # fast and slow are decreasing
    short_run = pt | bd

    # Offset
    if offset != 0:
        short_run = short_run.shift(offset)

    # Handle fills
    if ""fillna"" in kwargs:
        short_run.fillna(kwargs[""fillna""], inplace=True)
    if ""fill_method"" in kwargs:
        short_run.fillna(method=kwargs[""fill_method""], inplace=True)

    # Name and Categorize it
    short_run.name = f""SR_{length}""
    short_run.category = ""trend""

    return short_run",offset != 0,offset
robotics-rl-srl,https://github.com/araffin/robotics-rl-srl/tree/master/real_robots/omnirobot_utils/marker_finder.py,MakerFinder,intersection$73,"def intersection(self, l1, l2):
        """"""
        TODO
        :param l1:
        :param l2:
        :return:
        """"""
        vx = l1[0]
        vy = l1[1]
        ux = l2[0]
        uy = l2[1]
        wx = l2[2]-l1[2]
        wy = l2[3]-l1[3]

        tmp = vx * uy - vy * ux
        if tmp == 0:
            tmp = 1

        s = (vy * wx - vx * wy) / tmp
        px = l2[2]+s*ux
        py = l2[3]+s*uy

        return px, py",tmp == 0,not tmp
fbchat,https://github.com/fbchat-dev/fbchat/tree/master/tests/test_util.py,,test_datetime_to_seconds$217,"def test_datetime_to_seconds():
    assert datetime_to_seconds(DT_0) == 0
    assert datetime_to_seconds(DT) == 1542333064  # Rounded
    datetime_to_seconds(DT_NO_TIMEZONE)",datetime_to_seconds(DT_0) == 0,not datetime_to_seconds(DT_0)
hachoir,https://github.com/vstinner/hachoir/tree/master/hachoir/parser/misc/mapsforge_map.py,IntVbe,__init__$53,"def __init__(self, parent, name, description=None):
        Field.__init__(self, parent, name, description=description)

        value = 0
        size = 0
        shift = 0
        while True:
            byteValue = self._parent.stream.readBytes(
                self.absolute_address + (size * 8), 1)[0]

            haveMoreData = (byteValue & 0x80)
            if size == 0:
                isNegative = (byteValue & 0x40)
                value = (byteValue & 0x3f)
                shift += 6
            else:
                value = value | ((byteValue & 0x7f) << shift)
                shift += 7
            size += 1
            assert size < 100, ""IntVBE is too large""

            if not(haveMoreData):
                break

        if isNegative:
            value *= -1

        self._size = size * 8
        self.createValue = lambda: value",size == 0,not size
mmaction2,https://github.com/open-mmlab/mmaction2/tree/master/tests/test_models/test_neck.py,,test_tpn$11,"def test_tpn():
    """"""Test TPN backbone.""""""

    tpn_cfg = dict(
        in_channels=(1024, 2048),
        out_channels=1024,
        spatial_modulation_cfg=dict(
            in_channels=(1024, 2048), out_channels=2048),
        temporal_modulation_cfg=dict(downsample_scales=(8, 8)),
        upsample_cfg=dict(scale_factor=(1, 1, 1)),
        downsample_cfg=dict(downsample_scale=(1, 1, 1)),
        level_fusion_cfg=dict(
            in_channels=(1024, 1024),
            mid_channels=(1024, 1024),
            out_channels=2048,
            downsample_scales=((1, 1, 1), (1, 1, 1))),
        aux_head_cfg=dict(out_channels=400, loss_weight=0.5))

    with pytest.raises(AssertionError):
        tpn_cfg_ = copy.deepcopy(tpn_cfg)
        tpn_cfg_['in_channels'] = list(tpn_cfg_['in_channels'])
        TPN(**tpn_cfg_)

    with pytest.raises(AssertionError):
        tpn_cfg_ = copy.deepcopy(tpn_cfg)
        tpn_cfg_['out_channels'] = float(tpn_cfg_['out_channels'])
        TPN(**tpn_cfg_)

    with pytest.raises(AssertionError):
        tpn_cfg_ = copy.deepcopy(tpn_cfg)
        tpn_cfg_['downsample_cfg']['downsample_position'] = 'unsupport'
        TPN(**tpn_cfg_)

    for k in tpn_cfg:
        if not k.endswith('_cfg'):
            continue
        tpn_cfg_ = copy.deepcopy(tpn_cfg)
        tpn_cfg_[k] = list()
        with pytest.raises(AssertionError):
            TPN(**tpn_cfg_)

    with pytest.raises(ValueError):
        tpn_cfg_ = copy.deepcopy(tpn_cfg)
        tpn_cfg_['flow_type'] = 'unsupport'
        TPN(**tpn_cfg_)

    target_shape = (32, 1)
    target = generate_backbone_demo_inputs(target_shape).long().squeeze()
    x0_shape = (32, 1024, 1, 4, 4)
    x1_shape = (32, 2048, 1, 2, 2)
    x0 = generate_backbone_demo_inputs(x0_shape)
    x1 = generate_backbone_demo_inputs(x1_shape)
    x = [x0, x1]

    # ResNetTPN with 'cascade' flow_type
    tpn_cfg_ = copy.deepcopy(tpn_cfg)
    tpn_cascade = TPN(**tpn_cfg_)
    feat, loss_aux = tpn_cascade(x, target)
    assert feat.shape == torch.Size([32, 2048, 1, 2, 2])
    assert len(loss_aux) == 1

    # ResNetTPN with 'parallel' flow_type
    tpn_cfg_ = copy.deepcopy(tpn_cfg)
    tpn_parallel = TPN(flow_type='parallel', **tpn_cfg_)
    feat, loss_aux = tpn_parallel(x, target)
    assert feat.shape == torch.Size([32, 2048, 1, 2, 2])
    assert len(loss_aux) == 1

    # ResNetTPN with 'cascade' flow_type and target is None
    feat, loss_aux = tpn_cascade(x, None)
    assert feat.shape == torch.Size([32, 2048, 1, 2, 2])
    assert len(loss_aux) == 0

    # ResNetTPN with 'parallel' flow_type and target is None
    feat, loss_aux = tpn_parallel(x, None)
    assert feat.shape == torch.Size([32, 2048, 1, 2, 2])
    assert len(loss_aux) == 0",len(loss_aux) == 0,not loss_aux
mmaction2,https://github.com/open-mmlab/mmaction2/tree/master/tests/test_models/test_neck.py,,test_tpn$11,"def test_tpn():
    """"""Test TPN backbone.""""""

    tpn_cfg = dict(
        in_channels=(1024, 2048),
        out_channels=1024,
        spatial_modulation_cfg=dict(
            in_channels=(1024, 2048), out_channels=2048),
        temporal_modulation_cfg=dict(downsample_scales=(8, 8)),
        upsample_cfg=dict(scale_factor=(1, 1, 1)),
        downsample_cfg=dict(downsample_scale=(1, 1, 1)),
        level_fusion_cfg=dict(
            in_channels=(1024, 1024),
            mid_channels=(1024, 1024),
            out_channels=2048,
            downsample_scales=((1, 1, 1), (1, 1, 1))),
        aux_head_cfg=dict(out_channels=400, loss_weight=0.5))

    with pytest.raises(AssertionError):
        tpn_cfg_ = copy.deepcopy(tpn_cfg)
        tpn_cfg_['in_channels'] = list(tpn_cfg_['in_channels'])
        TPN(**tpn_cfg_)

    with pytest.raises(AssertionError):
        tpn_cfg_ = copy.deepcopy(tpn_cfg)
        tpn_cfg_['out_channels'] = float(tpn_cfg_['out_channels'])
        TPN(**tpn_cfg_)

    with pytest.raises(AssertionError):
        tpn_cfg_ = copy.deepcopy(tpn_cfg)
        tpn_cfg_['downsample_cfg']['downsample_position'] = 'unsupport'
        TPN(**tpn_cfg_)

    for k in tpn_cfg:
        if not k.endswith('_cfg'):
            continue
        tpn_cfg_ = copy.deepcopy(tpn_cfg)
        tpn_cfg_[k] = list()
        with pytest.raises(AssertionError):
            TPN(**tpn_cfg_)

    with pytest.raises(ValueError):
        tpn_cfg_ = copy.deepcopy(tpn_cfg)
        tpn_cfg_['flow_type'] = 'unsupport'
        TPN(**tpn_cfg_)

    target_shape = (32, 1)
    target = generate_backbone_demo_inputs(target_shape).long().squeeze()
    x0_shape = (32, 1024, 1, 4, 4)
    x1_shape = (32, 2048, 1, 2, 2)
    x0 = generate_backbone_demo_inputs(x0_shape)
    x1 = generate_backbone_demo_inputs(x1_shape)
    x = [x0, x1]

    # ResNetTPN with 'cascade' flow_type
    tpn_cfg_ = copy.deepcopy(tpn_cfg)
    tpn_cascade = TPN(**tpn_cfg_)
    feat, loss_aux = tpn_cascade(x, target)
    assert feat.shape == torch.Size([32, 2048, 1, 2, 2])
    assert len(loss_aux) == 1

    # ResNetTPN with 'parallel' flow_type
    tpn_cfg_ = copy.deepcopy(tpn_cfg)
    tpn_parallel = TPN(flow_type='parallel', **tpn_cfg_)
    feat, loss_aux = tpn_parallel(x, target)
    assert feat.shape == torch.Size([32, 2048, 1, 2, 2])
    assert len(loss_aux) == 1

    # ResNetTPN with 'cascade' flow_type and target is None
    feat, loss_aux = tpn_cascade(x, None)
    assert feat.shape == torch.Size([32, 2048, 1, 2, 2])
    assert len(loss_aux) == 0

    # ResNetTPN with 'parallel' flow_type and target is None
    feat, loss_aux = tpn_parallel(x, None)
    assert feat.shape == torch.Size([32, 2048, 1, 2, 2])
    assert len(loss_aux) == 0",len(loss_aux) == 0,not loss_aux
ByteTrack,https://github.com/ifzhang/ByteTrack/tree/master/tutorials/trades/mot_online/matching.py,,fuse_motion$185,"def fuse_motion(kf, cost_matrix, tracks, detections, only_position=False, lambda_=0.98):
    if cost_matrix.size == 0:
        return cost_matrix
    gating_dim = 2 if only_position else 4
    gating_threshold = chi2inv95[gating_dim]
    measurements = np.asarray([det.to_xyah() for det in detections])
    for row, track in enumerate(tracks):
        gating_distance = kf.gating_distance(
            track.mean, track.covariance, measurements, only_position, metric='maha')
        cost_matrix[row, gating_distance > gating_threshold] = np.inf
        cost_matrix[row] = lambda_ * cost_matrix[row] + (1 - lambda_) * gating_distance
    return cost_matrix",cost_matrix.size == 0,not cost_matrix.size
kafka-tools,https://github.com/linkedin/kafka-tools/tree/master/tests/tools/client/test_offsets.py,OffsetTests,test_send_set_offset_request$76,"def test_send_set_offset_request(self):
        offsets = TopicOffsets(self.client.cluster.topics['topic1'])
        offsets.partitions[0] = 2342
        offsets.partitions[1] = 8793

        self.client._send_group_aware_request = MagicMock()
        self.client._send_group_aware_request.return_value = 'responseobject'
        val = self.client._send_set_offset_request('testgroup', [offsets])
        assert val == 'responseobject'

        self.client._send_group_aware_request.assert_called_once()
        assert self.client._send_group_aware_request.call_args[0][0] == 'testgroup'
        req = self.client._send_group_aware_request.call_args[0][1]
        assert isinstance(req, OffsetCommitV2Request)
        assert req['group_id'] == 'testgroup'
        assert req['group_generation_id'] == -1
        assert req['member_id'] == ''
        assert req['retention_time'] == -1
        assert len(req['topics']) == 1
        assert req['topics'][0]['topic'] == 'topic1'
        assert len(req['topics'][0]['partitions']) == 2
        assert req['topics'][0]['partitions'][0]['partition'] == 0
        assert req['topics'][0]['partitions'][0]['offset'] == 2342
        assert req['topics'][0]['partitions'][0]['metadata'] is None
        assert req['topics'][0]['partitions'][1]['partition'] == 1
        assert req['topics'][0]['partitions'][1]['offset'] == 8793
        assert req['topics'][0]['partitions'][1]['metadata'] is None",req['topics'][0]['partitions'][0]['partition'] == 0,not req['topics'][0]['partitions'][0]['partition']
gluon-ts,https://github.com/awslabs/gluon-ts/tree/master/src/gluonts/model/san/_network.py,SelfAttentionNetwork,__init__$89,"def __init__(
        self,
        context_length: int,
        prediction_length: int,
        d_hidden: int,
        m_ffn: int,
        n_head: int,
        n_layers: int,
        n_output: int,
        cardinalities: List[int],
        kernel_sizes: Optional[List[int]],
        dist_enc: Optional[str],
        pre_ln: bool,
        dropout: float,
        temperature: float,
        normalizer_eps: float = 1e-5,
        **kwargs,
    ):
        super().__init__(**kwargs)
        if kernel_sizes is None or len(kernel_sizes) == 0:
            self.kernel_sizes = (1,)
        else:
            self.kernel_sizes = kernel_sizes
        self.context_length = context_length
        self.prediction_length = prediction_length
        self.d_hidden = d_hidden
        assert (n_output % 2 == 1) and (n_output <= 9)
        self.quantiles = sum(
            [[i / 10, 1.0 - i / 10] for i in range(1, (n_output + 1) // 2)],
            [0.5],
        )
        self.normalizer_eps = normalizer_eps

        with self.name_scope():
            self._blocks = []
            for layer in range(n_layers):
                block = SelfAttentionBlock(
                    d_hidden=self.d_hidden,
                    m_ffn=m_ffn,
                    kernel_sizes=self.kernel_sizes,
                    n_head=n_head,
                    dist_enc=dist_enc,
                    pre_ln=pre_ln,
                    dropout=dropout,
                    temperature=temperature,
                )
                self.register_child(block=block, name=f""block_{layer+1}"")
                self._blocks.append(block)

            self.target_proj = nn.Dense(
                units=self.d_hidden,
                in_units=1,
                use_bias=True,
                flatten=False,
                weight_initializer=init.Xavier(),
                prefix=""target_proj_"",
            )
            self.covar_proj = nn.Dense(
                units=self.d_hidden,
                use_bias=True,
                flatten=False,
                weight_initializer=init.Xavier(),
                prefix=""covar_proj_"",
            )
            if cardinalities:
                self.embedder = FeatureEmbedder(
                    cardinalities=cardinalities,
                    embedding_dims=[self.d_hidden] * len(cardinalities),
                    prefix=""embedder_"",
                )
            self.output = QuantileOutput(quantiles=self.quantiles)
            self.output_proj = self.output.get_quantile_proj()
            self.loss = self.output.get_loss()",len(kernel_sizes) == 0,not kernel_sizes
tf-encrypted,https://github.com/tf-encrypted/tf-encrypted/tree/master/tf_encrypted/protocol/aby3/aby3.py,,_sort_private$5708,"def _sort_private(
    prot: ABY3,
    x: ABY3PrivateTensor,
    axis: int,
    acc: bool = True,
) -> ABY3PrivateTensor:

    with tf.name_scope(""sort""):

        def bitonic_index(n, stage, sub_stage):
            assert sub_stage <= stage, ""The i-th stage can have at most i+1 sub stages.""
            # In bitonic sorting network, the 0-th sub stage in each stage has a different pattern from
            # other sub stages.
            if sub_stage == 0:
                a = np.arange(n)
                b = np.split(a, n / (2**stage))
                left = np.concatenate(b[0::2])
                right = np.concatenate([np.flip(x) for x in b[1::2]])
                return (left, right)
            else:
                a = np.arange(n)
                b = np.split(a, n / (2 ** (stage - sub_stage)))
                left = np.concatenate(b[0::2])
                right = np.concatenate(b[1::2])
                return (left, right)

        # def bitonic_sort(x):
        # n = int(x.shape[0])
        # n_stages = ceil(log2(n))
        # for stage in range(n_stages):
        # print(""building stage: "", stage)
        # for sub_stage in range(stage + 1):
        # left_idx, right_idx = bitonic_index(n, stage, sub_stage)
        # left = prot.gather(x, left_idx)
        # right = prot.gather(x, right_idx)
        # left, right = prot.cmp_swap(left, right)
        # z0 = prot.scatter_nd(
        # np.expand_dims(left_idx, axis=1),
        # left,
        # x.shape)
        # z1 = prot.scatter_nd(
        # np.expand_dims(right_idx, axis=1),
        # right,
        # x.shape)
        # x = z0 + z1
        # return x

        # return bitonic_sort(x)

        def build_bitonic_index(n):
            indices = []
            n_stages = ceil(log2(n))
            for stage in range(n_stages):
                for sub_stage in range(stage + 1):
                    left_idx, right_idx = bitonic_index(n, stage, sub_stage)
                    indices.append(np.stack([left_idx, right_idx]))
            return np.stack(indices)

        axis = axis % len(x.shape)
        if axis < 0:
            axis += len(x.shape)
        if axis != 0:
            x = prot.transpose(
                x,
                perm=[axis]
                + list(range(0, axis))
                + list(range(axis + 1, len(x.shape))),
            )

        unpadded_n = int(x.shape[0])
        n = next_power_of_two(unpadded_n)
        if n > unpadded_n:
            # We can only handle numbers of bit length k-2 for comparison
            max_bound = (1 << (x.backing_dtype.nbits - 2)) - 1
            pad = prot.define_constant(
                np.ones([n - unpadded_n] + x.shape[1:]) * max_bound, apply_scaling=False
            )
            pad = pad.to_private(x.share_type)
            pad.is_scaled = x.is_scaled
            x = prot.concat([x, pad], axis=0)

        n_stages = ceil(log2(n))
        n_sub_stages = int((1 + n_stages) * n_stages / 2)

        indices = build_bitonic_index(n)
        indices = prot.define_constant(indices, apply_scaling=False)

        def cond(i, x):
            return i < n_sub_stages

        def body(i, x):
            left_idx = indices[i][0]
            right_idx = indices[i][1]

            left = prot.gather(x, left_idx, axis=0)
            right = prot.gather(x, right_idx, axis=0)
            min_, max_ = prot.cmp_swap(left, right)

            if acc:
                min_idx, max_idx = (left_idx, right_idx)
            else:
                min_idx, max_idx = (right_idx, left_idx)

            z0 = prot.scatter_nd(prot.expand_dims(min_idx, axis=1), min_, x.shape)
            z1 = prot.scatter_nd(prot.expand_dims(max_idx, axis=1), max_, x.shape)
            x = z0 + z1

            return (i + 1, x)

        _, x = prot.while_loop(cond, body, [tf.constant(0), x])
        if n > unpadded_n:
            if acc:
                x = x[:unpadded_n]
            else:
                x = x[(n - unpadded_n) :]
        if axis != 0:
            x = prot.transpose(
                x,
                perm=list(range(1, axis + 1))
                + [0]
                + list(range(axis + 1, len(x.shape))),
            )
        return x",axis != 0,axis
tf-encrypted,https://github.com/tf-encrypted/tf-encrypted/tree/master/tf_encrypted/protocol/aby3/aby3.py,,_sort_private$5708,"def _sort_private(
    prot: ABY3,
    x: ABY3PrivateTensor,
    axis: int,
    acc: bool = True,
) -> ABY3PrivateTensor:

    with tf.name_scope(""sort""):

        def bitonic_index(n, stage, sub_stage):
            assert sub_stage <= stage, ""The i-th stage can have at most i+1 sub stages.""
            # In bitonic sorting network, the 0-th sub stage in each stage has a different pattern from
            # other sub stages.
            if sub_stage == 0:
                a = np.arange(n)
                b = np.split(a, n / (2**stage))
                left = np.concatenate(b[0::2])
                right = np.concatenate([np.flip(x) for x in b[1::2]])
                return (left, right)
            else:
                a = np.arange(n)
                b = np.split(a, n / (2 ** (stage - sub_stage)))
                left = np.concatenate(b[0::2])
                right = np.concatenate(b[1::2])
                return (left, right)

        # def bitonic_sort(x):
        # n = int(x.shape[0])
        # n_stages = ceil(log2(n))
        # for stage in range(n_stages):
        # print(""building stage: "", stage)
        # for sub_stage in range(stage + 1):
        # left_idx, right_idx = bitonic_index(n, stage, sub_stage)
        # left = prot.gather(x, left_idx)
        # right = prot.gather(x, right_idx)
        # left, right = prot.cmp_swap(left, right)
        # z0 = prot.scatter_nd(
        # np.expand_dims(left_idx, axis=1),
        # left,
        # x.shape)
        # z1 = prot.scatter_nd(
        # np.expand_dims(right_idx, axis=1),
        # right,
        # x.shape)
        # x = z0 + z1
        # return x

        # return bitonic_sort(x)

        def build_bitonic_index(n):
            indices = []
            n_stages = ceil(log2(n))
            for stage in range(n_stages):
                for sub_stage in range(stage + 1):
                    left_idx, right_idx = bitonic_index(n, stage, sub_stage)
                    indices.append(np.stack([left_idx, right_idx]))
            return np.stack(indices)

        axis = axis % len(x.shape)
        if axis < 0:
            axis += len(x.shape)
        if axis != 0:
            x = prot.transpose(
                x,
                perm=[axis]
                + list(range(0, axis))
                + list(range(axis + 1, len(x.shape))),
            )

        unpadded_n = int(x.shape[0])
        n = next_power_of_two(unpadded_n)
        if n > unpadded_n:
            # We can only handle numbers of bit length k-2 for comparison
            max_bound = (1 << (x.backing_dtype.nbits - 2)) - 1
            pad = prot.define_constant(
                np.ones([n - unpadded_n] + x.shape[1:]) * max_bound, apply_scaling=False
            )
            pad = pad.to_private(x.share_type)
            pad.is_scaled = x.is_scaled
            x = prot.concat([x, pad], axis=0)

        n_stages = ceil(log2(n))
        n_sub_stages = int((1 + n_stages) * n_stages / 2)

        indices = build_bitonic_index(n)
        indices = prot.define_constant(indices, apply_scaling=False)

        def cond(i, x):
            return i < n_sub_stages

        def body(i, x):
            left_idx = indices[i][0]
            right_idx = indices[i][1]

            left = prot.gather(x, left_idx, axis=0)
            right = prot.gather(x, right_idx, axis=0)
            min_, max_ = prot.cmp_swap(left, right)

            if acc:
                min_idx, max_idx = (left_idx, right_idx)
            else:
                min_idx, max_idx = (right_idx, left_idx)

            z0 = prot.scatter_nd(prot.expand_dims(min_idx, axis=1), min_, x.shape)
            z1 = prot.scatter_nd(prot.expand_dims(max_idx, axis=1), max_, x.shape)
            x = z0 + z1

            return (i + 1, x)

        _, x = prot.while_loop(cond, body, [tf.constant(0), x])
        if n > unpadded_n:
            if acc:
                x = x[:unpadded_n]
            else:
                x = x[(n - unpadded_n) :]
        if axis != 0:
            x = prot.transpose(
                x,
                perm=list(range(1, axis + 1))
                + [0]
                + list(range(axis + 1, len(x.shape))),
            )
        return x",axis != 0,axis
tf-encrypted,https://github.com/tf-encrypted/tf-encrypted/tree/master/tf_encrypted/protocol/aby3/aby3.py,,_sort_private$5708,"def _sort_private(
    prot: ABY3,
    x: ABY3PrivateTensor,
    axis: int,
    acc: bool = True,
) -> ABY3PrivateTensor:

    with tf.name_scope(""sort""):

        def bitonic_index(n, stage, sub_stage):
            assert sub_stage <= stage, ""The i-th stage can have at most i+1 sub stages.""
            # In bitonic sorting network, the 0-th sub stage in each stage has a different pattern from
            # other sub stages.
            if sub_stage == 0:
                a = np.arange(n)
                b = np.split(a, n / (2**stage))
                left = np.concatenate(b[0::2])
                right = np.concatenate([np.flip(x) for x in b[1::2]])
                return (left, right)
            else:
                a = np.arange(n)
                b = np.split(a, n / (2 ** (stage - sub_stage)))
                left = np.concatenate(b[0::2])
                right = np.concatenate(b[1::2])
                return (left, right)

        # def bitonic_sort(x):
        # n = int(x.shape[0])
        # n_stages = ceil(log2(n))
        # for stage in range(n_stages):
        # print(""building stage: "", stage)
        # for sub_stage in range(stage + 1):
        # left_idx, right_idx = bitonic_index(n, stage, sub_stage)
        # left = prot.gather(x, left_idx)
        # right = prot.gather(x, right_idx)
        # left, right = prot.cmp_swap(left, right)
        # z0 = prot.scatter_nd(
        # np.expand_dims(left_idx, axis=1),
        # left,
        # x.shape)
        # z1 = prot.scatter_nd(
        # np.expand_dims(right_idx, axis=1),
        # right,
        # x.shape)
        # x = z0 + z1
        # return x

        # return bitonic_sort(x)

        def build_bitonic_index(n):
            indices = []
            n_stages = ceil(log2(n))
            for stage in range(n_stages):
                for sub_stage in range(stage + 1):
                    left_idx, right_idx = bitonic_index(n, stage, sub_stage)
                    indices.append(np.stack([left_idx, right_idx]))
            return np.stack(indices)

        axis = axis % len(x.shape)
        if axis < 0:
            axis += len(x.shape)
        if axis != 0:
            x = prot.transpose(
                x,
                perm=[axis]
                + list(range(0, axis))
                + list(range(axis + 1, len(x.shape))),
            )

        unpadded_n = int(x.shape[0])
        n = next_power_of_two(unpadded_n)
        if n > unpadded_n:
            # We can only handle numbers of bit length k-2 for comparison
            max_bound = (1 << (x.backing_dtype.nbits - 2)) - 1
            pad = prot.define_constant(
                np.ones([n - unpadded_n] + x.shape[1:]) * max_bound, apply_scaling=False
            )
            pad = pad.to_private(x.share_type)
            pad.is_scaled = x.is_scaled
            x = prot.concat([x, pad], axis=0)

        n_stages = ceil(log2(n))
        n_sub_stages = int((1 + n_stages) * n_stages / 2)

        indices = build_bitonic_index(n)
        indices = prot.define_constant(indices, apply_scaling=False)

        def cond(i, x):
            return i < n_sub_stages

        def body(i, x):
            left_idx = indices[i][0]
            right_idx = indices[i][1]

            left = prot.gather(x, left_idx, axis=0)
            right = prot.gather(x, right_idx, axis=0)
            min_, max_ = prot.cmp_swap(left, right)

            if acc:
                min_idx, max_idx = (left_idx, right_idx)
            else:
                min_idx, max_idx = (right_idx, left_idx)

            z0 = prot.scatter_nd(prot.expand_dims(min_idx, axis=1), min_, x.shape)
            z1 = prot.scatter_nd(prot.expand_dims(max_idx, axis=1), max_, x.shape)
            x = z0 + z1

            return (i + 1, x)

        _, x = prot.while_loop(cond, body, [tf.constant(0), x])
        if n > unpadded_n:
            if acc:
                x = x[:unpadded_n]
            else:
                x = x[(n - unpadded_n) :]
        if axis != 0:
            x = prot.transpose(
                x,
                perm=list(range(1, axis + 1))
                + [0]
                + list(range(axis + 1, len(x.shape))),
            )
        return x",sub_stage == 0,not sub_stage
armory,https://github.com/armory3d/armory/tree/master/blender/arm/proxy.py,,traverse$44,"def traverse(obj, is_parent=False):
    if obj == None or obj.library == None or obj.proxy != None:
        return

    # Make proxy for all linked children
    for c in obj.children:
        traverse(c)

    override = bpy.context.copy()
    override['object'] = obj
    bpy.context.view_layer.objects.active = obj
    bpy.ops.object.proxy_make(override)

    # Reparent created proxies
    for c in obj.children:
        if c.proxy != None:
            c.parent = bpy.context.view_layer.objects.active
            c.matrix_parent_inverse = bpy.context.view_layer.objects.active.matrix_world.inverted()

    active = bpy.context.view_layer.objects.active
    sync_modifiers(active)
    # No transform sync for parent
    if is_parent:
        active.arm_proxy_sync_loc = False
        active.arm_proxy_sync_rot = False
        active.arm_proxy_sync_scale = False",obj == None,not obj
armory,https://github.com/armory3d/armory/tree/master/blender/arm/proxy.py,,traverse$44,"def traverse(obj, is_parent=False):
    if obj == None or obj.library == None or obj.proxy != None:
        return

    # Make proxy for all linked children
    for c in obj.children:
        traverse(c)

    override = bpy.context.copy()
    override['object'] = obj
    bpy.context.view_layer.objects.active = obj
    bpy.ops.object.proxy_make(override)

    # Reparent created proxies
    for c in obj.children:
        if c.proxy != None:
            c.parent = bpy.context.view_layer.objects.active
            c.matrix_parent_inverse = bpy.context.view_layer.objects.active.matrix_world.inverted()

    active = bpy.context.view_layer.objects.active
    sync_modifiers(active)
    # No transform sync for parent
    if is_parent:
        active.arm_proxy_sync_loc = False
        active.arm_proxy_sync_rot = False
        active.arm_proxy_sync_scale = False",obj.library == None,not obj.library
armory,https://github.com/armory3d/armory/tree/master/blender/arm/proxy.py,,traverse$44,"def traverse(obj, is_parent=False):
    if obj == None or obj.library == None or obj.proxy != None:
        return

    # Make proxy for all linked children
    for c in obj.children:
        traverse(c)

    override = bpy.context.copy()
    override['object'] = obj
    bpy.context.view_layer.objects.active = obj
    bpy.ops.object.proxy_make(override)

    # Reparent created proxies
    for c in obj.children:
        if c.proxy != None:
            c.parent = bpy.context.view_layer.objects.active
            c.matrix_parent_inverse = bpy.context.view_layer.objects.active.matrix_world.inverted()

    active = bpy.context.view_layer.objects.active
    sync_modifiers(active)
    # No transform sync for parent
    if is_parent:
        active.arm_proxy_sync_loc = False
        active.arm_proxy_sync_rot = False
        active.arm_proxy_sync_scale = False",obj.proxy != None,obj.proxy
armory,https://github.com/armory3d/armory/tree/master/blender/arm/proxy.py,,traverse$44,"def traverse(obj, is_parent=False):
    if obj == None or obj.library == None or obj.proxy != None:
        return

    # Make proxy for all linked children
    for c in obj.children:
        traverse(c)

    override = bpy.context.copy()
    override['object'] = obj
    bpy.context.view_layer.objects.active = obj
    bpy.ops.object.proxy_make(override)

    # Reparent created proxies
    for c in obj.children:
        if c.proxy != None:
            c.parent = bpy.context.view_layer.objects.active
            c.matrix_parent_inverse = bpy.context.view_layer.objects.active.matrix_world.inverted()

    active = bpy.context.view_layer.objects.active
    sync_modifiers(active)
    # No transform sync for parent
    if is_parent:
        active.arm_proxy_sync_loc = False
        active.arm_proxy_sync_rot = False
        active.arm_proxy_sync_scale = False",c.proxy != None,c.proxy
OneNet,https://github.com/PeizeSun/OneNet/tree/master/projects/OneSeg/oneseg/util/misc.py,,accuracy$432,"def accuracy(output, target, topk=(1,)):
    """"""Computes the precision@k for the specified values of k""""""
    if target.numel() == 0:
        return [torch.zeros([], device=output.device)]
    maxk = max(topk)
    batch_size = target.size(0)

    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))

    res = []
    for k in topk:
        correct_k = correct[:k].view(-1).float().sum(0)
        res.append(correct_k.mul_(100.0 / batch_size))
    return res",target.numel() == 0,not target.numel()
self-critical.pytorch,https://github.com/ruotianluo/self-critical.pytorch/tree/master/captioning/models/BertCapModel.py,BertCapModel,core$92,"def core(self, it, fc_feats_ph, att_feats_ph, memory, state, mask):
        """"""
        state = [ys.unsqueeze(0)]
        """"""
        if len(state) == 0:
            ys = it.unsqueeze(1)
        else:
            ys = torch.cat([state[0][0], it.unsqueeze(1)], dim=1)
        out = self.model.decode(memory, mask, 
                               ys, 
                               subsequent_mask(ys.size(1))
                                        .to(memory.device))
        return out[:, -1], [ys.unsqueeze(0)]",len(state) == 0,not state
dask-ml,https://github.com/dask/dask-ml/tree/master/tests/model_selection/dask_searchcv/test_model_selection_sklearn.py,,test_return_train_score_warn$217,"def test_return_train_score_warn():
    # Test that warnings are raised. Will be removed in sklearn 0.21
    X = np.arange(100).reshape(10, 10)
    y = np.array([0] * 5 + [1] * 5)
    X = (X - X.mean(0)) / X.std(0)  # help convergence
    grid = {""C"": [0.1, 0.5]}

    for val in [True, False]:
        est = dcv.GridSearchCV(
            LinearSVC(random_state=0, tol=0.5), grid, return_train_score=val
        )
        with pytest.warns(None) as warns:
            results = est.fit(X, y).cv_results_
        assert not warns
        assert type(results) is dict

    est = dcv.GridSearchCV(LinearSVC(random_state=0), grid)
    with pytest.warns(None) as warns:
        results = est.fit(X, y).cv_results_
    assert not warns

    train_keys = {
        ""split0_train_score"",
        ""split1_train_score"",
        ""split2_train_score"",
        ""mean_train_score"",
        ""std_train_score"",
    }

    include_train_score = SK_VERSION <= packaging.version.parse(""0.21.dev0"")

    if include_train_score:
        assert all(x in results for x in train_keys)
    else:
        result = train_keys & set(results)
        assert result == {}

    for key in results:
        if key in train_keys:
            with pytest.warns(FutureWarning):
                results[key]
        else:
            with pytest.warns(None) as warns:
                results[key]
            assert not warns",result == {},not result
sparrow-wifi,https://github.com/ghostop14/sparrow-wifi/tree/master//sparrow-wifi.py,mainWindow,checkForBluetooth$630,"def checkForBluetooth(self):
        self.hasBluetooth = False
        self.hasUbertooth = False
        self.hasRemoteBluetooth = False
        self.hasRemoteUbertooth = False
        
        numBtAdapters = len(SparrowBluetooth.getBluetoothInterfaces())
        if numBtAdapters > 0:
            self.hasBluetooth = True
        
        if SparrowBluetooth.getNumUbertoothDevices() > 0:
            #SparrowBluetooth.ubertoothStopSpecan()
            errcode, errmsg = SparrowBluetooth.hasUbertoothTools()
            # errcode, errmsg = SparrowBluetooth.ubertoothOnline()
            if errcode == 0:
                self.hasUbertooth = True
        
        if self.hasBluetooth or self.hasUbertooth:
            self.bluetooth = SparrowBluetooth()
        else:
            self.bluetooth = None",errcode == 0,not errcode
urh,https://github.com/jopohl/urh/tree/master/src/urh/ainterpretation/AutoInterpretation.py,,merge_plateau_lengths$276,"def merge_plateau_lengths(plateau_lengths, tolerance=None) -> list:
    if tolerance is None:
        tolerance = estimate_tolerance_from_plateau_lengths(plateau_lengths)

    if tolerance == 0 or tolerance is None:
        return plateau_lengths

    return c_auto_interpretation.merge_plateaus(plateau_lengths, tolerance, max_count=10000)",tolerance == 0,not tolerance
edx-platform,https://github.com/edx/edx-platform/tree/master/lms/djangoapps/course_wiki/utils.py,,article_course_wiki_root_slug$76,"def article_course_wiki_root_slug(article):
    """"""
    We assume the second level ancestor is the course wiki root. Examples:
    / returns None
    /Phy101 returns 'Phy101'
    /Phy101/Mechanics returns 'Phy101'
    /Chem101/Metals/Iron returns 'Chem101'

    Note that someone can create an article /random-article/sub-article on the
    wiki. In this case this function will return 'some-random-article' even
    if no course with course number 'some-random-article' exists.
    """"""

    try:
        urlpath = article.urlpath_set.get()
    except ObjectDoesNotExist:
        return None

    # Ancestors of /Phy101/Mechanics/Acceleration/ is a list of URLPaths
    # ['Root', 'Phy101', 'Mechanics']
    ancestors = urlpath.cached_ancestors

    course_wiki_root_urlpath = None

    if len(ancestors) == 0:  # It is the wiki root article.
        course_wiki_root_urlpath = None
    elif len(ancestors) == 1:  # It is a course wiki root article.
        course_wiki_root_urlpath = urlpath
    else:  # It is an article inside a course wiki.
        course_wiki_root_urlpath = ancestors[1]

    if course_wiki_root_urlpath is not None:
        return course_wiki_root_urlpath.slug

    return None",len(ancestors) == 0,not ancestors
YOLOF,https://github.com/megvii-model/YOLOF/tree/master/cvpods/checkpoint/utils.py,,_group_to_str$98,"def _group_to_str(group: list):
    """"""
    Format a group of parameter name suffixes into a loggable string.
    Args:
        group (list[str]): list of parameter name suffixes.
    Returns:
        str: formated string.
    """"""
    if len(group) == 0:
        return """"

    if len(group) == 1:
        return ""."" + group[0]

    return "".{"" + "", "".join(group) + ""}""",len(group) == 0,not group
PaddleViT,https://github.com/BR-IDL/PaddleViT/tree/master/image_classification/CrossViT/main_multi_gpu.py,,train$60,"def train(dataloader,
          model,
          optimizer,
          criterion,
          epoch,
          total_epochs,
          total_batches,
          debug_steps=100,
          accum_iter=1,
          model_ema=None,
          mixup_fn=None,
          amp_grad_scaler=None,
          local_logger=None,
          master_logger=None):
    """"""Training for one epoch
    Args:
        dataloader: paddle.io.DataLoader, dataloader instance
        model: nn.Layer, a ViT model
        optimizer: nn.optimizer
        criterion: nn.XXLoss
        epoch: int, current epoch
        total_epochs: int, total num of epochs
        total_batches: int, total num of batches for one epoch
        debug_steps: int, num of iters to log info, default: 100
        accum_iter: int, num of iters for accumulating gradients, default: 1
        model_ema: ModelEma, model moving average instance
        mixup_fn: Mixup, mixup instance, default: None
        amp_grad_scaler: GradScaler, if not None pass the GradScaler and enable AMP, default: None
        local_logger: logger for local process/gpu, default: None
        master_logger: logger for main process, default: None
    Returns:
        train_loss_meter.avg: float, average loss on current process/gpu
        train_acc_meter.avg: float, average acc@1 on current process/gpu
        master_loss_meter.avg: float, average loss on all processes/gpus
        master_acc_meter.avg: float, average acc@1 on all processes/gpus
        train_time: float, training time
    """"""
    time_st = time.time()
    train_loss_meter = AverageMeter()
    train_acc_meter = AverageMeter()
    master_loss_meter = AverageMeter()
    master_acc_meter = AverageMeter()

    model.train()
    optimizer.clear_grad()

    for batch_id, data in enumerate(dataloader):
        # get data
        images = data[0]
        label = data[1]
        label_orig = label.clone()
        batch_size = images.shape[0]

        if mixup_fn is not None:
            images, label = mixup_fn(images, label_orig)

        # forward
        with paddle.amp.auto_cast(amp_grad_scaler is not None):
            output = model(images)
            loss = criterion(output, label)

        loss_value = loss.item()
        if not math.isfinite(loss_value):
            print(""Loss is {}, stopping training"".format(loss_value))
            sys.exit(1)

        loss = loss / accum_iter

        # backward and step
        if amp_grad_scaler is None: # fp32
            loss.backward()
            if ((batch_id + 1) % accum_iter == 0) or (batch_id + 1 == len(dataloader)):
                optimizer.step()
                optimizer.clear_grad()
        else: # amp
            scaled_loss = amp_grad_scaler.scale(loss)
            scaled_loss.backward()
            if ((batch_id + 1) % accum_iter == 0) or (batch_id + 1 == len(dataloader)):
                # amp for param group reference: https://github.com/PaddlePaddle/Paddle/issues/37188
                amp_grad_scaler.step(optimizer)
                amp_grad_scaler.update()
                optimizer.clear_grad()

        if model_ema is not None and paddle.distributed.get_rank() == 0:
            model_ema.update(model)

        # average of output and kd_output, same as eval mode
        pred = paddle.nn.functional.softmax(output)
        acc = paddle.metric.accuracy(pred,
            label_orig if mixup_fn else label_orig.unsqueeze(1)).item()

        # sync from other gpus for overall loss and acc
        master_loss = all_reduce_mean(loss_value)
        master_acc = all_reduce_mean(acc)
        master_batch_size = all_reduce_mean(batch_size)

        master_loss_meter.update(master_loss, master_batch_size)
        master_acc_meter.update(master_acc, master_batch_size)
        train_loss_meter.update(loss_value, batch_size)
        train_acc_meter.update(acc, batch_size)

        if batch_id % debug_steps == 0 or batch_id + 1 == len(dataloader):
            general_message = (f""Epoch[{epoch:03d}/{total_epochs:03d}], ""
                               f""Step[{batch_id:04d}/{total_batches:04d}], ""
                               f""Lr: {optimizer.get_lr():04f}, "")
            local_message = (general_message +
                             f""Loss: {loss_value:.4f} ({train_loss_meter.avg:.4f}), ""
                             f""Avg Acc: {train_acc_meter.avg:.4f}"")
            master_message = (general_message +
                              f""Loss: {master_loss:.4f} ({master_loss_meter.avg:.4f}), ""
                              f""Avg Acc: {master_acc_meter.avg:.4f}"")
            write_log(local_logger, master_logger, local_message, master_message)

    paddle.distributed.barrier()
    train_time = time.time() - time_st
    return (train_loss_meter.avg,
            train_acc_meter.avg,
            master_loss_meter.avg,
            master_acc_meter.avg,
            train_time)",paddle.distributed.get_rank() == 0,not paddle.distributed.get_rank()
PaddleViT,https://github.com/BR-IDL/PaddleViT/tree/master/image_classification/CrossViT/main_multi_gpu.py,,train$60,"def train(dataloader,
          model,
          optimizer,
          criterion,
          epoch,
          total_epochs,
          total_batches,
          debug_steps=100,
          accum_iter=1,
          model_ema=None,
          mixup_fn=None,
          amp_grad_scaler=None,
          local_logger=None,
          master_logger=None):
    """"""Training for one epoch
    Args:
        dataloader: paddle.io.DataLoader, dataloader instance
        model: nn.Layer, a ViT model
        optimizer: nn.optimizer
        criterion: nn.XXLoss
        epoch: int, current epoch
        total_epochs: int, total num of epochs
        total_batches: int, total num of batches for one epoch
        debug_steps: int, num of iters to log info, default: 100
        accum_iter: int, num of iters for accumulating gradients, default: 1
        model_ema: ModelEma, model moving average instance
        mixup_fn: Mixup, mixup instance, default: None
        amp_grad_scaler: GradScaler, if not None pass the GradScaler and enable AMP, default: None
        local_logger: logger for local process/gpu, default: None
        master_logger: logger for main process, default: None
    Returns:
        train_loss_meter.avg: float, average loss on current process/gpu
        train_acc_meter.avg: float, average acc@1 on current process/gpu
        master_loss_meter.avg: float, average loss on all processes/gpus
        master_acc_meter.avg: float, average acc@1 on all processes/gpus
        train_time: float, training time
    """"""
    time_st = time.time()
    train_loss_meter = AverageMeter()
    train_acc_meter = AverageMeter()
    master_loss_meter = AverageMeter()
    master_acc_meter = AverageMeter()

    model.train()
    optimizer.clear_grad()

    for batch_id, data in enumerate(dataloader):
        # get data
        images = data[0]
        label = data[1]
        label_orig = label.clone()
        batch_size = images.shape[0]

        if mixup_fn is not None:
            images, label = mixup_fn(images, label_orig)

        # forward
        with paddle.amp.auto_cast(amp_grad_scaler is not None):
            output = model(images)
            loss = criterion(output, label)

        loss_value = loss.item()
        if not math.isfinite(loss_value):
            print(""Loss is {}, stopping training"".format(loss_value))
            sys.exit(1)

        loss = loss / accum_iter

        # backward and step
        if amp_grad_scaler is None: # fp32
            loss.backward()
            if ((batch_id + 1) % accum_iter == 0) or (batch_id + 1 == len(dataloader)):
                optimizer.step()
                optimizer.clear_grad()
        else: # amp
            scaled_loss = amp_grad_scaler.scale(loss)
            scaled_loss.backward()
            if ((batch_id + 1) % accum_iter == 0) or (batch_id + 1 == len(dataloader)):
                # amp for param group reference: https://github.com/PaddlePaddle/Paddle/issues/37188
                amp_grad_scaler.step(optimizer)
                amp_grad_scaler.update()
                optimizer.clear_grad()

        if model_ema is not None and paddle.distributed.get_rank() == 0:
            model_ema.update(model)

        # average of output and kd_output, same as eval mode
        pred = paddle.nn.functional.softmax(output)
        acc = paddle.metric.accuracy(pred,
            label_orig if mixup_fn else label_orig.unsqueeze(1)).item()

        # sync from other gpus for overall loss and acc
        master_loss = all_reduce_mean(loss_value)
        master_acc = all_reduce_mean(acc)
        master_batch_size = all_reduce_mean(batch_size)

        master_loss_meter.update(master_loss, master_batch_size)
        master_acc_meter.update(master_acc, master_batch_size)
        train_loss_meter.update(loss_value, batch_size)
        train_acc_meter.update(acc, batch_size)

        if batch_id % debug_steps == 0 or batch_id + 1 == len(dataloader):
            general_message = (f""Epoch[{epoch:03d}/{total_epochs:03d}], ""
                               f""Step[{batch_id:04d}/{total_batches:04d}], ""
                               f""Lr: {optimizer.get_lr():04f}, "")
            local_message = (general_message +
                             f""Loss: {loss_value:.4f} ({train_loss_meter.avg:.4f}), ""
                             f""Avg Acc: {train_acc_meter.avg:.4f}"")
            master_message = (general_message +
                              f""Loss: {master_loss:.4f} ({master_loss_meter.avg:.4f}), ""
                              f""Avg Acc: {master_acc_meter.avg:.4f}"")
            write_log(local_logger, master_logger, local_message, master_message)

    paddle.distributed.barrier()
    train_time = time.time() - time_st
    return (train_loss_meter.avg,
            train_acc_meter.avg,
            master_loss_meter.avg,
            master_acc_meter.avg,
            train_time)",batch_id % debug_steps == 0,not batch_id % debug_steps
PaddleViT,https://github.com/BR-IDL/PaddleViT/tree/master/image_classification/CrossViT/main_multi_gpu.py,,train$60,"def train(dataloader,
          model,
          optimizer,
          criterion,
          epoch,
          total_epochs,
          total_batches,
          debug_steps=100,
          accum_iter=1,
          model_ema=None,
          mixup_fn=None,
          amp_grad_scaler=None,
          local_logger=None,
          master_logger=None):
    """"""Training for one epoch
    Args:
        dataloader: paddle.io.DataLoader, dataloader instance
        model: nn.Layer, a ViT model
        optimizer: nn.optimizer
        criterion: nn.XXLoss
        epoch: int, current epoch
        total_epochs: int, total num of epochs
        total_batches: int, total num of batches for one epoch
        debug_steps: int, num of iters to log info, default: 100
        accum_iter: int, num of iters for accumulating gradients, default: 1
        model_ema: ModelEma, model moving average instance
        mixup_fn: Mixup, mixup instance, default: None
        amp_grad_scaler: GradScaler, if not None pass the GradScaler and enable AMP, default: None
        local_logger: logger for local process/gpu, default: None
        master_logger: logger for main process, default: None
    Returns:
        train_loss_meter.avg: float, average loss on current process/gpu
        train_acc_meter.avg: float, average acc@1 on current process/gpu
        master_loss_meter.avg: float, average loss on all processes/gpus
        master_acc_meter.avg: float, average acc@1 on all processes/gpus
        train_time: float, training time
    """"""
    time_st = time.time()
    train_loss_meter = AverageMeter()
    train_acc_meter = AverageMeter()
    master_loss_meter = AverageMeter()
    master_acc_meter = AverageMeter()

    model.train()
    optimizer.clear_grad()

    for batch_id, data in enumerate(dataloader):
        # get data
        images = data[0]
        label = data[1]
        label_orig = label.clone()
        batch_size = images.shape[0]

        if mixup_fn is not None:
            images, label = mixup_fn(images, label_orig)

        # forward
        with paddle.amp.auto_cast(amp_grad_scaler is not None):
            output = model(images)
            loss = criterion(output, label)

        loss_value = loss.item()
        if not math.isfinite(loss_value):
            print(""Loss is {}, stopping training"".format(loss_value))
            sys.exit(1)

        loss = loss / accum_iter

        # backward and step
        if amp_grad_scaler is None: # fp32
            loss.backward()
            if ((batch_id + 1) % accum_iter == 0) or (batch_id + 1 == len(dataloader)):
                optimizer.step()
                optimizer.clear_grad()
        else: # amp
            scaled_loss = amp_grad_scaler.scale(loss)
            scaled_loss.backward()
            if ((batch_id + 1) % accum_iter == 0) or (batch_id + 1 == len(dataloader)):
                # amp for param group reference: https://github.com/PaddlePaddle/Paddle/issues/37188
                amp_grad_scaler.step(optimizer)
                amp_grad_scaler.update()
                optimizer.clear_grad()

        if model_ema is not None and paddle.distributed.get_rank() == 0:
            model_ema.update(model)

        # average of output and kd_output, same as eval mode
        pred = paddle.nn.functional.softmax(output)
        acc = paddle.metric.accuracy(pred,
            label_orig if mixup_fn else label_orig.unsqueeze(1)).item()

        # sync from other gpus for overall loss and acc
        master_loss = all_reduce_mean(loss_value)
        master_acc = all_reduce_mean(acc)
        master_batch_size = all_reduce_mean(batch_size)

        master_loss_meter.update(master_loss, master_batch_size)
        master_acc_meter.update(master_acc, master_batch_size)
        train_loss_meter.update(loss_value, batch_size)
        train_acc_meter.update(acc, batch_size)

        if batch_id % debug_steps == 0 or batch_id + 1 == len(dataloader):
            general_message = (f""Epoch[{epoch:03d}/{total_epochs:03d}], ""
                               f""Step[{batch_id:04d}/{total_batches:04d}], ""
                               f""Lr: {optimizer.get_lr():04f}, "")
            local_message = (general_message +
                             f""Loss: {loss_value:.4f} ({train_loss_meter.avg:.4f}), ""
                             f""Avg Acc: {train_acc_meter.avg:.4f}"")
            master_message = (general_message +
                              f""Loss: {master_loss:.4f} ({master_loss_meter.avg:.4f}), ""
                              f""Avg Acc: {master_acc_meter.avg:.4f}"")
            write_log(local_logger, master_logger, local_message, master_message)

    paddle.distributed.barrier()
    train_time = time.time() - time_st
    return (train_loss_meter.avg,
            train_acc_meter.avg,
            master_loss_meter.avg,
            master_acc_meter.avg,
            train_time)",(batch_id + 1) % accum_iter == 0,not (batch_id + 1) % accum_iter
PaddleViT,https://github.com/BR-IDL/PaddleViT/tree/master/image_classification/CrossViT/main_multi_gpu.py,,train$60,"def train(dataloader,
          model,
          optimizer,
          criterion,
          epoch,
          total_epochs,
          total_batches,
          debug_steps=100,
          accum_iter=1,
          model_ema=None,
          mixup_fn=None,
          amp_grad_scaler=None,
          local_logger=None,
          master_logger=None):
    """"""Training for one epoch
    Args:
        dataloader: paddle.io.DataLoader, dataloader instance
        model: nn.Layer, a ViT model
        optimizer: nn.optimizer
        criterion: nn.XXLoss
        epoch: int, current epoch
        total_epochs: int, total num of epochs
        total_batches: int, total num of batches for one epoch
        debug_steps: int, num of iters to log info, default: 100
        accum_iter: int, num of iters for accumulating gradients, default: 1
        model_ema: ModelEma, model moving average instance
        mixup_fn: Mixup, mixup instance, default: None
        amp_grad_scaler: GradScaler, if not None pass the GradScaler and enable AMP, default: None
        local_logger: logger for local process/gpu, default: None
        master_logger: logger for main process, default: None
    Returns:
        train_loss_meter.avg: float, average loss on current process/gpu
        train_acc_meter.avg: float, average acc@1 on current process/gpu
        master_loss_meter.avg: float, average loss on all processes/gpus
        master_acc_meter.avg: float, average acc@1 on all processes/gpus
        train_time: float, training time
    """"""
    time_st = time.time()
    train_loss_meter = AverageMeter()
    train_acc_meter = AverageMeter()
    master_loss_meter = AverageMeter()
    master_acc_meter = AverageMeter()

    model.train()
    optimizer.clear_grad()

    for batch_id, data in enumerate(dataloader):
        # get data
        images = data[0]
        label = data[1]
        label_orig = label.clone()
        batch_size = images.shape[0]

        if mixup_fn is not None:
            images, label = mixup_fn(images, label_orig)

        # forward
        with paddle.amp.auto_cast(amp_grad_scaler is not None):
            output = model(images)
            loss = criterion(output, label)

        loss_value = loss.item()
        if not math.isfinite(loss_value):
            print(""Loss is {}, stopping training"".format(loss_value))
            sys.exit(1)

        loss = loss / accum_iter

        # backward and step
        if amp_grad_scaler is None: # fp32
            loss.backward()
            if ((batch_id + 1) % accum_iter == 0) or (batch_id + 1 == len(dataloader)):
                optimizer.step()
                optimizer.clear_grad()
        else: # amp
            scaled_loss = amp_grad_scaler.scale(loss)
            scaled_loss.backward()
            if ((batch_id + 1) % accum_iter == 0) or (batch_id + 1 == len(dataloader)):
                # amp for param group reference: https://github.com/PaddlePaddle/Paddle/issues/37188
                amp_grad_scaler.step(optimizer)
                amp_grad_scaler.update()
                optimizer.clear_grad()

        if model_ema is not None and paddle.distributed.get_rank() == 0:
            model_ema.update(model)

        # average of output and kd_output, same as eval mode
        pred = paddle.nn.functional.softmax(output)
        acc = paddle.metric.accuracy(pred,
            label_orig if mixup_fn else label_orig.unsqueeze(1)).item()

        # sync from other gpus for overall loss and acc
        master_loss = all_reduce_mean(loss_value)
        master_acc = all_reduce_mean(acc)
        master_batch_size = all_reduce_mean(batch_size)

        master_loss_meter.update(master_loss, master_batch_size)
        master_acc_meter.update(master_acc, master_batch_size)
        train_loss_meter.update(loss_value, batch_size)
        train_acc_meter.update(acc, batch_size)

        if batch_id % debug_steps == 0 or batch_id + 1 == len(dataloader):
            general_message = (f""Epoch[{epoch:03d}/{total_epochs:03d}], ""
                               f""Step[{batch_id:04d}/{total_batches:04d}], ""
                               f""Lr: {optimizer.get_lr():04f}, "")
            local_message = (general_message +
                             f""Loss: {loss_value:.4f} ({train_loss_meter.avg:.4f}), ""
                             f""Avg Acc: {train_acc_meter.avg:.4f}"")
            master_message = (general_message +
                              f""Loss: {master_loss:.4f} ({master_loss_meter.avg:.4f}), ""
                              f""Avg Acc: {master_acc_meter.avg:.4f}"")
            write_log(local_logger, master_logger, local_message, master_message)

    paddle.distributed.barrier()
    train_time = time.time() - time_st
    return (train_loss_meter.avg,
            train_acc_meter.avg,
            master_loss_meter.avg,
            master_acc_meter.avg,
            train_time)",(batch_id + 1) % accum_iter == 0,not (batch_id + 1) % accum_iter
centerNet-deep-sort,https://github.com/kimyoon-young/centerNet-deep-sort/tree/master/tools/cocoeval.py,COCOeval,summarize$422,"def summarize(self):
        '''
        Compute and display summary metrics for evaluation results.
        Note this functin can *only* be applied on the default parameter setting
        '''
        def _summarize( ap=1, iouThr=None, areaRng='all', maxDets=100 ):
            p = self.params
            iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'
            titleStr = 'Average Precision' if ap == 1 else 'Average Recall'
            typeStr = '(AP)' if ap==1 else '(AR)'
            iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) \
                if iouThr is None else '{:0.2f}'.format(iouThr)

            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]
            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]
            if ap == 1:
                # dimension of precision: [TxRxKxAxM]
                s = self.eval['precision']
                # IoU
                if iouThr is not None:
                    t = np.where(iouThr == p.iouThrs)[0]
                    s = s[t]
                s = s[:,:,:,aind,mind]
            else:
                # dimension of recall: [TxKxAxM]
                s = self.eval['recall']
                if iouThr is not None:
                    t = np.where(iouThr == p.iouThrs)[0]
                    s = s[t]
                s = s[:,:,aind,mind]
            if len(s[s>-1])==0:
                mean_s = -1
            else:
                mean_s = np.mean(s[s>-1])

                #cacluate AP(average precision) for each category
                num_classes = 80
                avg_ap = 0.0
                if ap == 1:
                    for i in range(0, num_classes):
                        print('category : {0} : {1}'.format(i,np.mean(s[:,:,i,:])))
                        avg_ap +=np.mean(s[:,:,i,:])
                    print('(all categories) mAP : {}'.format(avg_ap / num_classes))

            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))
            return mean_s
        def _summarizeDets():
            stats = np.zeros((12,))
            stats[0] = _summarize(1)
            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])
            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])
            stats[3] = _summarize(1, areaRng='small', maxDets=self.params.maxDets[2])
            stats[4] = _summarize(1, areaRng='medium', maxDets=self.params.maxDets[2])
            stats[5] = _summarize(1, areaRng='large', maxDets=self.params.maxDets[2])
            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])
            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])
            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])
            stats[9] = _summarize(0, areaRng='small', maxDets=self.params.maxDets[2])
            stats[10] = _summarize(0, areaRng='medium', maxDets=self.params.maxDets[2])
            stats[11] = _summarize(0, areaRng='large', maxDets=self.params.maxDets[2])
            return stats
        def _summarizeKps():
            stats = np.zeros((10,))
            stats[0] = _summarize(1, maxDets=20)
            stats[1] = _summarize(1, maxDets=20, iouThr=.5)
            stats[2] = _summarize(1, maxDets=20, iouThr=.75)
            stats[3] = _summarize(1, maxDets=20, areaRng='medium')
            stats[4] = _summarize(1, maxDets=20, areaRng='large')
            stats[5] = _summarize(0, maxDets=20)
            stats[6] = _summarize(0, maxDets=20, iouThr=.5)
            stats[7] = _summarize(0, maxDets=20, iouThr=.75)
            stats[8] = _summarize(0, maxDets=20, areaRng='medium')
            stats[9] = _summarize(0, maxDets=20, areaRng='large')
            return stats
        if not self.eval:
            raise Exception('Please run accumulate() first')
        iouType = self.params.iouType
        if iouType == 'segm' or iouType == 'bbox':
            summarize = _summarizeDets
        elif iouType == 'keypoints':
            summarize = _summarizeKps
        self.stats = summarize()",len(s[s > -1]) == 0,not s[s > -1]
ansible-modules-core,https://github.com/ansible/ansible-modules-core/tree/master/files/replace.py,,write_changes$104,"def write_changes(module,contents,dest):

    tmpfd, tmpfile = tempfile.mkstemp()
    f = os.fdopen(tmpfd,'wb')
    f.write(contents)
    f.close()

    validate = module.params.get('validate', None)
    valid = not validate
    if validate:
        if ""%s"" not in validate:
            module.fail_json(msg=""validate must contain %%s: %s"" % (validate))
        (rc, out, err) = module.run_command(validate % tmpfile)
        valid = rc == 0
        if rc != 0:
            module.fail_json(msg='failed to validate: '
                                 'rc:%s error:%s' % (rc,err))
    if valid:
        module.atomic_move(tmpfile, dest, unsafe_writes=module.params['unsafe_writes'])",rc != 0,rc
tetris_game,https://github.com/LoveDaisy/tetris_game/tree/master//tetris_game.py,,drawSquare$140,"def drawSquare(painter, x, y, val, s):
    colorTable = [0x000000, 0xCC6666, 0x66CC66, 0x6666CC,
                  0xCCCC66, 0xCC66CC, 0x66CCCC, 0xDAAA00]

    if val == 0:
        return

    color = QColor(colorTable[val])
    painter.fillRect(x + 1, y + 1, s - 2, s - 2, color)

    painter.setPen(color.lighter())
    painter.drawLine(x, y + s - 1, x, y)
    painter.drawLine(x, y, x + s - 1, y)

    painter.setPen(color.darker())
    painter.drawLine(x + 1, y + s - 1, x + s - 1, y + s - 1)
    painter.drawLine(x + s - 1, y + s - 1, x + s - 1, y + 1)",val == 0,not val
sentry,https://github.com/getsentry/sentry/tree/master/tests/sentry/integrations/slack/test_notify_action.py,SlackNotifyActionTest,test_disabled_org_integration$353,"def test_disabled_org_integration(self):
        org = self.create_organization(owner=self.user)
        OrganizationIntegration.objects.create(organization=org, integration=self.integration)
        OrganizationIntegration.objects.filter(
            integration=self.integration, organization=self.event.project.organization
        ).update(status=ObjectStatus.DISABLED)
        event = self.get_event()

        rule = self.get_rule(data={""workspace"": self.integration.id, ""channel"": ""#my-channel""})

        results = list(rule.after(event=event, state=self.get_state()))
        assert len(results) == 0",len(results) == 0,not results
pynguin,https://github.com/se2p/pynguin/tree/master/tests/ga/test_testcasechromosome.py,,test_mutation_delete_skipping$219,"def test_mutation_delete_skipping():
    test_case = dtc.DefaultTestCase()
    chromosome = tcc.TestCaseChromosome(test_case)
    with mock.patch.object(chromosome, ""_delete_statement"") as delete_mock:
        delete_mock.return_value = True
        with mock.patch.object(chromosome, ""get_last_mutatable_statement"") as mut_mock:
            mut_mock.return_value = 3
            assert not chromosome._mutation_delete()
            assert delete_mock.call_count == 0",delete_mock.call_count == 0,not delete_mock.call_count
lc-all-solutions,https://github.com/csujedihy/lc-all-solutions/tree/master/440.k-th-smallest-in-lexicographical-order/k-th-smallest-in-lexicographical-order.py,Solution,_findKthNumber$3,"def _findKthNumber(self, n, k):
    """"""
    :type n: int
    :type k: int
    :rtype: int
    """"""

    def dfs(cur, n):
      if self.k == 0:
        return cur
      self.k -= 1
      if cur == 0:
        for i in range(1, 10):
          if i > n:
            break
          ret = dfs(i, n)
          if ret:
            return ret
      else:
        for i in range(0, 10):
          if cur * 10 + i > n:
            break
          ret = dfs(cur * 10 + i, n)
          if ret:
            return ret

    self.k = k
    return dfs(0, n)",self.k == 0,not self.k
lc-all-solutions,https://github.com/csujedihy/lc-all-solutions/tree/master/440.k-th-smallest-in-lexicographical-order/k-th-smallest-in-lexicographical-order.py,Solution,_findKthNumber$3,"def _findKthNumber(self, n, k):
    """"""
    :type n: int
    :type k: int
    :rtype: int
    """"""

    def dfs(cur, n):
      if self.k == 0:
        return cur
      self.k -= 1
      if cur == 0:
        for i in range(1, 10):
          if i > n:
            break
          ret = dfs(i, n)
          if ret:
            return ret
      else:
        for i in range(0, 10):
          if cur * 10 + i > n:
            break
          ret = dfs(cur * 10 + i, n)
          if ret:
            return ret

    self.k = k
    return dfs(0, n)",cur == 0,not cur
sentry,https://github.com/getsentry/sentry/tree/master/tests/sentry/api/endpoints/test_release_deploys.py,ReleaseDeploysCreateTest,test_with_invalid_project_slug$149,"def test_with_invalid_project_slug(self):
        bar_project = self.create_project(organization=self.org, name=""bar"")
        release = Release.objects.create(organization_id=self.org.id, version=""1"", total_deploys=0)
        release.add_project(self.project)

        url = reverse(
            ""sentry-api-0-organization-release-deploys"",
            kwargs={
                ""organization_slug"": self.org.slug,
                ""version"": release.version,
            },
        )

        response = self.client.post(
            url,
            data={
                ""name"": ""foo"",
                ""environment"": ""production"",
                ""url"": ""https://www.example.com"",
                ""projects"": [bar_project.slug],
            },
        )
        assert response.status_code == 400, response.content
        assert response.data[""detail""][""code""] == ""parameter-validation-error""
        assert ""Invalid projects"" in response.data[""detail""][""message""]
        assert 0 == Deploy.objects.count()",0 == Deploy.objects.count(),not Deploy.objects.count()
luminoth,https://github.com/tryolabs/luminoth/tree/master/luminoth/tools/dataset/readers/object_detection/pascalvoc.py,PascalVOCReader,iterate$74,"def iterate(self):
        for image_id in self._get_record_names():
            if self._stop_iteration():
                # Finish iteration.
                return

            if self._should_skip(image_id):
                continue

            try:
                annotation_path = self._get_image_annotation(image_id)
                image_path = self._get_image_path(image_id)

                # Read both the image and the annotation into memory.
                annotation = read_xml(annotation_path)
                image = read_image(image_path)
            except tf.errors.NotFoundError:
                tf.logging.debug(
                    'Error reading image or annotation for ""{}"".'.format(
                        image_id))
                self.errors += 1
                continue

            gt_boxes = []

            for b in annotation['object']:
                try:
                    label_id = self.classes.index(b['name'])
                except ValueError:
                    continue

                gt_boxes.append({
                    'label': label_id,
                    'xmin': b['bndbox']['xmin'],
                    'ymin': b['bndbox']['ymin'],
                    'xmax': b['bndbox']['xmax'],
                    'ymax': b['bndbox']['ymax'],
                })

            if len(gt_boxes) == 0:
                continue

            record = {
                'width': annotation['size']['width'],
                'height': annotation['size']['height'],
                'depth': annotation['size']['depth'],
                'filename': annotation['filename'],
                'image_raw': image,
                'gt_boxes': gt_boxes,
            }
            self._will_add_record(record)
            self.yielded_records += 1

            yield record",len(gt_boxes) == 0,not gt_boxes
typer,https://github.com/tiangolo/typer/tree/master/tests/test_tutorial/test_commands/test_context/test_tutorial002.py,,test_create$13,"def test_create():
    result = runner.invoke(app, [""create"", ""Camila""])
    assert result.exit_code == 0
    assert ""Initializing database"" in result.output
    assert ""Creating user: Camila"" in result.output",result.exit_code == 0,not result.exit_code
Sublime-JSHint,https://github.com/victorporof/Sublime-JSHint/tree/master//JSHint.py,JshintCommand,file_unsupported$69,"def file_unsupported(self):
    file_path = self.view.file_name()
    view_settings = self.view.settings()
    has_js_or_html_extension = file_path != None and bool(re.search(r'\.(jsm?|html?)$', file_path))
    has_js_or_html_syntax = bool(re.search(r'JavaScript|HTML', view_settings.get(""syntax""), re.I))
    has_json_syntax = bool(re.search(""JSON"", view_settings.get(""syntax""), re.I))
    return has_json_syntax or (not has_js_or_html_extension and not has_js_or_html_syntax)",file_path != None,file_path
Sprytile,https://github.com/Sprytile/Sprytile/tree/master//addon_updater.py,Singleton_updater,stage_repository$636,"def stage_repository(self, url):

        local = os.path.join(self._updater_path, ""update_staging"")
        error = None

        # make/clear the staging folder
        # ensure the folder is always ""clean""
        if self._verbose: print(""Preparing staging folder for download:\n"", local)
        if os.path.isdir(local) == True:
            try:
                shutil.rmtree(local)
                os.makedirs(local)
            except:
                error = ""failed to remove existing staging directory""
        else:
            try:
                os.makedirs(local)
            except:
                error = ""failed to create staging directory""

        if error != None:
            if self._verbose: print(""Error: Aborting update, "" + error)
            self._error = ""Update aborted, staging path error""
            self._error_msg = ""Error: {}"".format(error)
            return False

        if self._backup_current == True:
            self.create_backup()
        if self._verbose: print(""Now retrieving the new source zip"")

        self._source_zip = os.path.join(local, ""source.zip"")

        if self._verbose: print(""Starting download update zip"")
        try:
            request = urllib.request.Request(url)

            # setup private token if appropriate
            if self._engine.token != None:
                if self._engine.name == ""gitlab"":
                    request.add_header('PRIVATE-TOKEN', self._engine.token)
                else:
                    if self._verbose: print(""Tokens not setup for selected engine yet"")
            self.urlretrieve(urllib.request.urlopen(request), self._source_zip)
            # add additional checks on file size being non-zero
            if self._verbose: print(""Successfully downloaded update zip"")
            return True
        except Exception as e:
            self._error = ""Error retrieving download, bad link?""
            self._error_msg = ""Error: {}"".format(e)
            if self._verbose:
                print(""Error retrieving download, bad link?"")
                print(""Error: {}"".format(e))
            return False",os.path.isdir(local) == True,os.path.isdir(local)
Sprytile,https://github.com/Sprytile/Sprytile/tree/master//addon_updater.py,Singleton_updater,stage_repository$636,"def stage_repository(self, url):

        local = os.path.join(self._updater_path, ""update_staging"")
        error = None

        # make/clear the staging folder
        # ensure the folder is always ""clean""
        if self._verbose: print(""Preparing staging folder for download:\n"", local)
        if os.path.isdir(local) == True:
            try:
                shutil.rmtree(local)
                os.makedirs(local)
            except:
                error = ""failed to remove existing staging directory""
        else:
            try:
                os.makedirs(local)
            except:
                error = ""failed to create staging directory""

        if error != None:
            if self._verbose: print(""Error: Aborting update, "" + error)
            self._error = ""Update aborted, staging path error""
            self._error_msg = ""Error: {}"".format(error)
            return False

        if self._backup_current == True:
            self.create_backup()
        if self._verbose: print(""Now retrieving the new source zip"")

        self._source_zip = os.path.join(local, ""source.zip"")

        if self._verbose: print(""Starting download update zip"")
        try:
            request = urllib.request.Request(url)

            # setup private token if appropriate
            if self._engine.token != None:
                if self._engine.name == ""gitlab"":
                    request.add_header('PRIVATE-TOKEN', self._engine.token)
                else:
                    if self._verbose: print(""Tokens not setup for selected engine yet"")
            self.urlretrieve(urllib.request.urlopen(request), self._source_zip)
            # add additional checks on file size being non-zero
            if self._verbose: print(""Successfully downloaded update zip"")
            return True
        except Exception as e:
            self._error = ""Error retrieving download, bad link?""
            self._error_msg = ""Error: {}"".format(e)
            if self._verbose:
                print(""Error retrieving download, bad link?"")
                print(""Error: {}"".format(e))
            return False",error != None,error
Sprytile,https://github.com/Sprytile/Sprytile/tree/master//addon_updater.py,Singleton_updater,stage_repository$636,"def stage_repository(self, url):

        local = os.path.join(self._updater_path, ""update_staging"")
        error = None

        # make/clear the staging folder
        # ensure the folder is always ""clean""
        if self._verbose: print(""Preparing staging folder for download:\n"", local)
        if os.path.isdir(local) == True:
            try:
                shutil.rmtree(local)
                os.makedirs(local)
            except:
                error = ""failed to remove existing staging directory""
        else:
            try:
                os.makedirs(local)
            except:
                error = ""failed to create staging directory""

        if error != None:
            if self._verbose: print(""Error: Aborting update, "" + error)
            self._error = ""Update aborted, staging path error""
            self._error_msg = ""Error: {}"".format(error)
            return False

        if self._backup_current == True:
            self.create_backup()
        if self._verbose: print(""Now retrieving the new source zip"")

        self._source_zip = os.path.join(local, ""source.zip"")

        if self._verbose: print(""Starting download update zip"")
        try:
            request = urllib.request.Request(url)

            # setup private token if appropriate
            if self._engine.token != None:
                if self._engine.name == ""gitlab"":
                    request.add_header('PRIVATE-TOKEN', self._engine.token)
                else:
                    if self._verbose: print(""Tokens not setup for selected engine yet"")
            self.urlretrieve(urllib.request.urlopen(request), self._source_zip)
            # add additional checks on file size being non-zero
            if self._verbose: print(""Successfully downloaded update zip"")
            return True
        except Exception as e:
            self._error = ""Error retrieving download, bad link?""
            self._error_msg = ""Error: {}"".format(e)
            if self._verbose:
                print(""Error retrieving download, bad link?"")
                print(""Error: {}"".format(e))
            return False",self._backup_current == True,self._backup_current
Sprytile,https://github.com/Sprytile/Sprytile/tree/master//addon_updater.py,Singleton_updater,stage_repository$636,"def stage_repository(self, url):

        local = os.path.join(self._updater_path, ""update_staging"")
        error = None

        # make/clear the staging folder
        # ensure the folder is always ""clean""
        if self._verbose: print(""Preparing staging folder for download:\n"", local)
        if os.path.isdir(local) == True:
            try:
                shutil.rmtree(local)
                os.makedirs(local)
            except:
                error = ""failed to remove existing staging directory""
        else:
            try:
                os.makedirs(local)
            except:
                error = ""failed to create staging directory""

        if error != None:
            if self._verbose: print(""Error: Aborting update, "" + error)
            self._error = ""Update aborted, staging path error""
            self._error_msg = ""Error: {}"".format(error)
            return False

        if self._backup_current == True:
            self.create_backup()
        if self._verbose: print(""Now retrieving the new source zip"")

        self._source_zip = os.path.join(local, ""source.zip"")

        if self._verbose: print(""Starting download update zip"")
        try:
            request = urllib.request.Request(url)

            # setup private token if appropriate
            if self._engine.token != None:
                if self._engine.name == ""gitlab"":
                    request.add_header('PRIVATE-TOKEN', self._engine.token)
                else:
                    if self._verbose: print(""Tokens not setup for selected engine yet"")
            self.urlretrieve(urllib.request.urlopen(request), self._source_zip)
            # add additional checks on file size being non-zero
            if self._verbose: print(""Successfully downloaded update zip"")
            return True
        except Exception as e:
            self._error = ""Error retrieving download, bad link?""
            self._error_msg = ""Error: {}"".format(e)
            if self._verbose:
                print(""Error retrieving download, bad link?"")
                print(""Error: {}"".format(e))
            return False",self._engine.token != None,self._engine.token
GetTogether,https://github.com/GetTogetherComm/GetTogether/tree/master/resume/tests.py,ResumeTests,test_redirect_to_view$21,"def test_redirect_to_view(self):

        assert len(self.request._resume_points) == 0

        no_resume_point = self.request._resume_points.pop()
        assert no_resume_point is None

        response = resume_or_redirect(self.request, ""test-view"")
        assert response.status_code == 302
        assert response.url == ""/test/view""",len(self.request._resume_points) == 0,not self.request._resume_points
fedlearner,https://github.com/bytedance/fedlearner/tree/master/fedlearner/data_join/rsa_psi/rsa_psi_component.py,PsiRsaSigner,_make_inner_generator$265,"def _make_inner_generator(self, next_index):
        assert next_index is not None
        max_flying_sign_batch = self._max_flying_sign_batch
        raw_id_batches, next_index = self._consum_raw_id_batch(
                next_index, max_flying_sign_batch
            )
        flying_batch_num = len(raw_id_batches)
        signed_batch_futures = self._promise_signed_batches(raw_id_batches)
        wait4batch = False
        while len(signed_batch_futures) > 0:
            if signed_batch_futures[0].done() or wait4batch or \
                    len(signed_batch_futures) >= max_flying_sign_batch:
                signed_batch = signed_batch_futures[0].result()
                self._yield_batch_num += 1
                if self._yield_batch_num % 32 == 0:
                    slow_sign_batch_num, sign_batch_num, avg_duration, \
                        avg_retry_cnt, avg_pending_duration, \
                        slow_avg_duration, slow_avg_retry_cnt, \
                        slow_avg_pending_duration = self._get_sign_stats()
                    logging.warning(""%d/%d batch sign cost more than %d ""\
                                    ""second, avg duration: %f for each batch,""\
                                    ""avg retry cnt: %f. avg pending duration ""\
                                    ""%f, slow avg duration %f, slow avg retry""\
                                    "" cnt %f, slow avg pending duration %f"",
                                    slow_sign_batch_num, sign_batch_num,
                                    self._slow_sign_threshold,
                                    avg_duration, avg_retry_cnt,
                                    avg_pending_duration, slow_avg_duration,
                                    slow_avg_retry_cnt,
                                    slow_avg_pending_duration)
                yield signed_batch, False
                signed_batch_futures = signed_batch_futures[1:]
            required_num = max_flying_sign_batch - len(signed_batch_futures)
            raw_id_batches, next_index = \
                    self._consum_raw_id_batch(next_index, required_num)
            wait4batch = len(raw_id_batches) == 0
            signed_batch_futures += self._promise_signed_batches(raw_id_batches)
        yield self._make_item_batch(next_index), True",self._yield_batch_num % 32 == 0,not self._yield_batch_num % 32
LinOTP,https://github.com/LinOTP/LinOTP/tree/master/linotp/provider/smsprovider/DeviceSMSProvider.py,DeviceSMSProvider,_submitMessage$62,"def _submitMessage(self, phone, message):
        """"""
        submitMessage()
        - send out a message to a phone

        """"""
        if ""CONFIGFILE"" not in self.config:
            log.error(""[submitMessage] No config key CONFIGFILE found!"")
            return False

        # NOTE 1: The LinOTP service account need rw-access to /dev/ttyXXX
        # NOTE 2: we need gnokii 0.6.29 or higher, since 0.6.28 will crash with
        # a bug
        args = [
            ""gnokii"",
            ""--config"",
            self.config.get(""CONFIGFILE""),
            ""--sendsms"",
            phone,
        ]

        if ""SMSC"" in self.config:
            args.append(""--smsc"")
            args.append(self.config.get(""SMSC""))

        log.info(""[submitMessage] sending SMS : %s"", "" "".join(args))
        proc = subprocess.Popen(
            args,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            close_fds=True,
        )

        (smsout, smserr) = proc.communicate(message)

        if proc.returncode == 0:
            log.debug(""[submitMessage] output: %s"", smsout)
            return True

        log.error(""[submitMessage] output: %s"", smsout)
        log.error(
            ""[submitMessage] SMS sending failed, return code: %s"",
            proc.returncode,
        )

        return False",proc.returncode == 0,not proc.returncode
Gooey,https://github.com/chriskiehl/Gooey/tree/master/gooey/python_bindings/argparse_to_json.py,,action_to_json$435,"def action_to_json(action, widget, options):
    dropdown_types = {'Listbox', 'Dropdown', 'Counter'}
    if action.required:
        # Text fields get a default check that user input is present
        # and not just spaces, dropdown types get a simplified
        # is-it-present style check
        validator = ('user_input and not user_input.isspace()'
                     if widget not in dropdown_types
                     else 'user_input')
        error_msg = 'This field is required'
    else:
        # not required; do nothing;
        validator = 'True'
        error_msg = ''

    base = merge(item_default, {
        'validator': {
            'type': 'ExpressionValidator',
            'test': validator,
            'message': error_msg
        },
    })

    if (options.get(action.dest) or {}).get('initial_value') != None:
        value = options[action.dest]['initial_value']
        options[action.dest]['initial_value'] = handle_initial_values(action, widget, value)
    default = handle_initial_values(action, widget, action.default)
    if default == argparse.SUPPRESS:
        default = None



    final_options = merge(base, options.get(action.dest) or {})
    validate_gooey_options(action, widget, final_options)

    return {
        'id': action.dest,
        'type': widget,
        'cli_type': choose_cli_type(action),
        'required': action.required,
        'data': {
            'display_name': action.metavar or action.dest,
            'help': (action.help or '').replace('%%', '%'),
            'required': action.required,
            'nargs': action.nargs or '',
            'commands': action.option_strings,
            'choices': list(map(str, action.choices)) if action.choices else [],
            'default': default,
            'dest': action.dest,
        },
        'options': final_options
    }",(options.get(action.dest) or {}).get('initial_value') != None,(options.get(action.dest) or {}).get('initial_value')
dulwich,https://github.com/dulwich/dulwich/tree/master/dulwich/porcelain.py,,describe$1843,"def describe(repo):
    """"""Describe the repository version.

    Args:
      projdir: git repository root
    Returns: a string description of the current git revision

    Examples: ""gabcdefh"", ""v0.1"" or ""v0.1-5-gabcdefh"".
    """"""
    # Get the repository
    with open_repo_closing(repo) as r:
        # Get a list of all tags
        refs = r.get_refs()
        tags = {}
        for key, value in refs.items():
            key = key.decode()
            obj = r.get_object(value)
            if u""tags"" not in key:
                continue

            _, tag = key.rsplit(u""/"", 1)

            try:
                commit = obj.object
            except AttributeError:
                continue
            else:
                commit = r.get_object(commit[1])
            tags[tag] = [
                datetime.datetime(*time.gmtime(commit.commit_time)[:6]),
                commit.id.decode(""ascii""),
            ]

        sorted_tags = sorted(tags.items(), key=lambda tag: tag[1][0], reverse=True)

        # If there are no tags, return the current commit
        if len(sorted_tags) == 0:
            return ""g{}"".format(r[r.head()].id.decode(""ascii"")[:7])

        # We're now 0 commits from the top
        commit_count = 0

        # Get the latest commit
        latest_commit = r[r.head()]

        # Walk through all commits
        walker = r.get_walker()
        for entry in walker:
            # Check if tag
            commit_id = entry.commit.id.decode(""ascii"")
            for tag in sorted_tags:
                tag_name = tag[0]
                tag_commit = tag[1][1]
                if commit_id == tag_commit:
                    if commit_count == 0:
                        return tag_name
                    else:
                        return ""{}-{}-g{}"".format(
                            tag_name,
                            commit_count,
                            latest_commit.id.decode(""ascii"")[:7],
                        )

            commit_count += 1

        # Return plain commit if no parent tag can be found
        return ""g{}"".format(latest_commit.id.decode(""ascii"")[:7])",len(sorted_tags) == 0,not sorted_tags
dulwich,https://github.com/dulwich/dulwich/tree/master/dulwich/porcelain.py,,describe$1843,"def describe(repo):
    """"""Describe the repository version.

    Args:
      projdir: git repository root
    Returns: a string description of the current git revision

    Examples: ""gabcdefh"", ""v0.1"" or ""v0.1-5-gabcdefh"".
    """"""
    # Get the repository
    with open_repo_closing(repo) as r:
        # Get a list of all tags
        refs = r.get_refs()
        tags = {}
        for key, value in refs.items():
            key = key.decode()
            obj = r.get_object(value)
            if u""tags"" not in key:
                continue

            _, tag = key.rsplit(u""/"", 1)

            try:
                commit = obj.object
            except AttributeError:
                continue
            else:
                commit = r.get_object(commit[1])
            tags[tag] = [
                datetime.datetime(*time.gmtime(commit.commit_time)[:6]),
                commit.id.decode(""ascii""),
            ]

        sorted_tags = sorted(tags.items(), key=lambda tag: tag[1][0], reverse=True)

        # If there are no tags, return the current commit
        if len(sorted_tags) == 0:
            return ""g{}"".format(r[r.head()].id.decode(""ascii"")[:7])

        # We're now 0 commits from the top
        commit_count = 0

        # Get the latest commit
        latest_commit = r[r.head()]

        # Walk through all commits
        walker = r.get_walker()
        for entry in walker:
            # Check if tag
            commit_id = entry.commit.id.decode(""ascii"")
            for tag in sorted_tags:
                tag_name = tag[0]
                tag_commit = tag[1][1]
                if commit_id == tag_commit:
                    if commit_count == 0:
                        return tag_name
                    else:
                        return ""{}-{}-g{}"".format(
                            tag_name,
                            commit_count,
                            latest_commit.id.decode(""ascii"")[:7],
                        )

            commit_count += 1

        # Return plain commit if no parent tag can be found
        return ""g{}"".format(latest_commit.id.decode(""ascii"")[:7])",commit_count == 0,not commit_count
LightNet,https://github.com/linksense/LightNet/tree/master/scripts/yellowfin.py,YFOptimizer,update_hyper_param$463,"def update_hyper_param(self):
        for group in self._optimizer.param_groups:
            group['momentum'] = self._mu_t
            # group['momentum'] = max(self._mu, self._mu_t)
            if self._force_non_inc_step == False:
                group['lr'] = self._lr_t * self._lr_factor
                # a loose clamping to prevent catastrophically large move. If the move
                # is too large, we set lr to 0 and only use the momentum to move
                if self._adapt_clip and (group['lr'] * np.sqrt(
                        self._global_state['grad_norm_squared']) >= self._catastrophic_move_thresh):
                    group['lr'] = self._catastrophic_move_thresh / np.sqrt(
                        self._global_state['grad_norm_squared'] + eps)
                    if self._verbose:
                        logging.warning(""clip catastropic move!"")
            elif self._iter > self._curv_win_width:
                # force to guarantee lr * grad_norm not increasing dramatically.
                # Not necessary for basic use. Please refer to the comments
                # in YFOptimizer.__init__ for more details
                self.lr_grad_norm_avg()
                debias_factor = self.zero_debias_factor()
                group['lr'] = min(self._lr * self._lr_factor,
                                  2.0 * self._global_state[""lr_grad_norm_avg_min""] \
                                  / (np.sqrt(
                                      np.exp(self._global_state['grad_norm_squared_avg_log'] / debias_factor)) + eps))
        return",self._force_non_inc_step == False,not self._force_non_inc_step
myscan,https://github.com/amcai/myscan/tree/master/myscan/pocs/perfolder/citrix/poc_citrix-cve-2020-8982-unauth-fileread_2020.py,POC,verify$23,"def verify(self):
        # 鏍规嵁config.py 閰嶇疆鐨勬繁搴︼紝闄愬畾涓涓嬬洰褰曟繁搴
        if self.url.count(""/"") > int(scan_set.get(""max_dir"", 2)) + 2:
            return

        req = {
            ""method"": ""GET"",
            ""url"": self.url + ""XmlPeek.aspx?dt=\\\\..\\\\..\\\\..\\\\..\\\\..\\\\..\\\\Windows\\\\win.ini&x=/validate.ashx?requri"",
            ""timeout"": 10,
            ""allow_redirects"": False,
            ""verify"": False,
        }
        r = request(**req)
        words = [b""bit app support"", b""fonts"", b""extensions""]
        if r != None and r.status_code == 200 and all([x in r.content for x in words]):
            parser_ = response_parser(r)
            self.result.append({
                ""name"": self.name,
                ""url"": self.url,
                ""level"": self.level,  # 0:Low  1:Medium 2:High
                ""detail"": {
                    ""vulmsg"": self.vulmsg,
                    ""request"": parser_.getrequestraw(),
                    ""response"": parser_.getresponseraw()
                }
            })",r != None,r
DialoGPT,https://github.com/microsoft/DialoGPT/tree/master/pycocoevalcap/cider/cider_scorer.py,CiderScorer,sim$133,"def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):
            '''
            Compute the cosine similarity of two vectors.
            :param vec_hyp: array of dictionary for vector corresponding to hypothesis
            :param vec_ref: array of dictionary for vector corresponding to reference
            :param norm_hyp: array of float for vector corresponding to hypothesis
            :param norm_ref: array of float for vector corresponding to reference
            :param length_hyp: int containing length of hypothesis
            :param length_ref: int containing length of reference
            :return: array of score for each n-grams cosine similarity
            '''
            delta = float(length_hyp - length_ref)
            # measure consine similarity
            val = np.array([0.0 for _ in range(self.n)])
            for n in range(self.n):
                # ngram
                for (ngram,count) in vec_hyp[n].items():
                    # vrama91 : added clipping
                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]

                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):
                    val[n] /= (norm_hyp[n]*norm_ref[n])

                assert(not math.isnan(val[n]))
                # vrama91: added a length based gaussian penalty
                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))
            return val",norm_hyp[n] != 0,norm_hyp[n]
DialoGPT,https://github.com/microsoft/DialoGPT/tree/master/pycocoevalcap/cider/cider_scorer.py,CiderScorer,sim$133,"def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):
            '''
            Compute the cosine similarity of two vectors.
            :param vec_hyp: array of dictionary for vector corresponding to hypothesis
            :param vec_ref: array of dictionary for vector corresponding to reference
            :param norm_hyp: array of float for vector corresponding to hypothesis
            :param norm_ref: array of float for vector corresponding to reference
            :param length_hyp: int containing length of hypothesis
            :param length_ref: int containing length of reference
            :return: array of score for each n-grams cosine similarity
            '''
            delta = float(length_hyp - length_ref)
            # measure consine similarity
            val = np.array([0.0 for _ in range(self.n)])
            for n in range(self.n):
                # ngram
                for (ngram,count) in vec_hyp[n].items():
                    # vrama91 : added clipping
                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]

                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):
                    val[n] /= (norm_hyp[n]*norm_ref[n])

                assert(not math.isnan(val[n]))
                # vrama91: added a length based gaussian penalty
                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))
            return val",norm_ref[n] != 0,norm_ref[n]
Multi-Camera-Live-Object-Tracking,https://github.com/LeonLok/Multi-Camera-Live-Object-Tracking/tree/master/traffic_counting/deep_sort/linear_assignment.py,,matching_cascade$78,"def matching_cascade(
        distance_metric, max_distance, cascade_depth, tracks, detections,
        track_indices=None, detection_indices=None):
    """"""Run matching cascade.

    Parameters
    ----------
    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray
        The distance metric is given a list of tracks and detections as well as
        a list of N track indices and M detection indices. The metric should
        return the NxM dimensional cost matrix, where element (i, j) is the
        association cost between the i-th track in the given track indices and
        the j-th detection in the given detection indices.
    max_distance : float
        Gating threshold. Associations with cost larger than this value are
        disregarded.
    cascade_depth: int
        The cascade depth, should be se to the maximum track age.
    tracks : List[track.Track]
        A list of predicted tracks at the current time step.
    detections : List[detection.Detection]
        A list of detections at the current time step.
    track_indices : Optional[List[int]]
        List of track indices that maps rows in `cost_matrix` to tracks in
        `tracks` (see description above). Defaults to all tracks.
    detection_indices : Optional[List[int]]
        List of detection indices that maps columns in `cost_matrix` to
        detections in `detections` (see description above). Defaults to all
        detections.

    Returns
    -------
    (List[(int, int)], List[int], List[int])
        Returns a tuple with the following three entries:
        * A list of matched track and detection indices.
        * A list of unmatched track indices.
        * A list of unmatched detection indices.

    """"""
    if track_indices is None:
        track_indices = list(range(len(tracks)))
    if detection_indices is None:
        detection_indices = list(range(len(detections)))

    unmatched_detections = detection_indices
    matches = []
    for level in range(cascade_depth):
        if len(unmatched_detections) == 0:  # No detections left
            break

        track_indices_l = [
            k for k in track_indices
            if tracks[k].time_since_update == 1 + level
        ]
        if len(track_indices_l) == 0:  # Nothing to match at this level
            continue

        matches_l, _, unmatched_detections = \
            min_cost_matching(
                distance_metric, max_distance, tracks, detections,
                track_indices_l, unmatched_detections)
        matches += matches_l
    unmatched_tracks = list(set(track_indices) - set(k for k, _ in matches))
    return matches, unmatched_tracks, unmatched_detections",len(unmatched_detections) == 0,not unmatched_detections
Multi-Camera-Live-Object-Tracking,https://github.com/LeonLok/Multi-Camera-Live-Object-Tracking/tree/master/traffic_counting/deep_sort/linear_assignment.py,,matching_cascade$78,"def matching_cascade(
        distance_metric, max_distance, cascade_depth, tracks, detections,
        track_indices=None, detection_indices=None):
    """"""Run matching cascade.

    Parameters
    ----------
    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray
        The distance metric is given a list of tracks and detections as well as
        a list of N track indices and M detection indices. The metric should
        return the NxM dimensional cost matrix, where element (i, j) is the
        association cost between the i-th track in the given track indices and
        the j-th detection in the given detection indices.
    max_distance : float
        Gating threshold. Associations with cost larger than this value are
        disregarded.
    cascade_depth: int
        The cascade depth, should be se to the maximum track age.
    tracks : List[track.Track]
        A list of predicted tracks at the current time step.
    detections : List[detection.Detection]
        A list of detections at the current time step.
    track_indices : Optional[List[int]]
        List of track indices that maps rows in `cost_matrix` to tracks in
        `tracks` (see description above). Defaults to all tracks.
    detection_indices : Optional[List[int]]
        List of detection indices that maps columns in `cost_matrix` to
        detections in `detections` (see description above). Defaults to all
        detections.

    Returns
    -------
    (List[(int, int)], List[int], List[int])
        Returns a tuple with the following three entries:
        * A list of matched track and detection indices.
        * A list of unmatched track indices.
        * A list of unmatched detection indices.

    """"""
    if track_indices is None:
        track_indices = list(range(len(tracks)))
    if detection_indices is None:
        detection_indices = list(range(len(detections)))

    unmatched_detections = detection_indices
    matches = []
    for level in range(cascade_depth):
        if len(unmatched_detections) == 0:  # No detections left
            break

        track_indices_l = [
            k for k in track_indices
            if tracks[k].time_since_update == 1 + level
        ]
        if len(track_indices_l) == 0:  # Nothing to match at this level
            continue

        matches_l, _, unmatched_detections = \
            min_cost_matching(
                distance_metric, max_distance, tracks, detections,
                track_indices_l, unmatched_detections)
        matches += matches_l
    unmatched_tracks = list(set(track_indices) - set(k for k, _ in matches))
    return matches, unmatched_tracks, unmatched_detections",len(track_indices_l) == 0,not track_indices_l
quay,https://github.com/quay/quay/tree/master/image/docker/schema2/test/test_config.py,,test_valid_config$106,"def test_valid_config():
    config = DockerSchema2Config(Bytes.for_string_or_unicode(CONFIG_BYTES))
    history = list(config.history)
    assert len(history) == 4

    assert not history[0].is_empty
    assert history[1].is_empty

    assert history[0].created_datetime.year == 2018
    assert history[1].command == '/bin/sh -c #(nop)  CMD [""sh""]'
    assert history[2].command == ""sh""

    for index, history_entry in enumerate(history):
        v1_compat = config.build_v1_compatibility(
            history_entry, ""somev1id"", ""someparentid"", index == 3
        )
        assert v1_compat[""id""] == ""somev1id""
        assert v1_compat[""parent""] == ""someparentid""

        if index == 3:
            assert v1_compat[""container_config""] == config._parsed[""container_config""]
        else:
            assert ""Hostname"" not in v1_compat[""container_config""]
            assert v1_compat[""container_config""][""Cmd""] == [history_entry.command]

    assert config.labels == {}",config.labels == {},not config.labels
django-tables2,https://github.com/jieter/django-tables2/tree/master/django_tables2/utils.py,Sequence,expand$25,"def expand(self, columns):
        """"""
        Expands the ``'...'`` item in the sequence into the appropriate column
        names that should be placed there.

        arguments:
            columns (list): list of column names.
        returns:
            The current instance.

        raises:
            `ValueError` if the sequence is invalid for the columns.
        """"""
        ellipses = self.count(""..."")
        if ellipses > 1:
            raise ValueError(""'...' must be used at most once in a sequence."")
        elif ellipses == 0:
            self.append(""..."")

        # everything looks good, let's expand the ""..."" item
        columns = list(columns)  # take a copy and exhaust the generator
        head = []
        tail = []
        target = head  # start by adding things to the head
        for name in self:
            if name == ""..."":
                # now we'll start adding elements to the tail
                target = tail
                continue
            target.append(name)
            if name in columns:
                columns.pop(columns.index(name))
        self[:] = chain(head, columns, tail)

        return self",ellipses == 0,not ellipses
xonsh,https://github.com/xonsh/xonsh/tree/master/xonsh/completers/commands.py,,complete_skipper$44,"def complete_skipper(command_context: CommandContext):
    """"""
    Skip over several tokens (e.g., sudo) and complete based on the rest of the command.
    """"""

    # Contextual completers don't need us to skip tokens since they get the correct completion context -
    # meaning we only need to skip commands like ``sudo``.
    skip_part_num = 0
    # all the args before the current argument
    for arg in command_context.args[: command_context.arg_index]:
        if arg.value not in SKIP_TOKENS:
            break
        skip_part_num += 1

    if skip_part_num == 0:
        return None

    skipped_command_context = command_context._replace(
        args=command_context.args[skip_part_num:],
        arg_index=command_context.arg_index - skip_part_num,
    )

    if skipped_command_context.arg_index == 0:
        # completing the command after a SKIP_TOKEN
        return complete_command(skipped_command_context)

    completer: Completer = XSH.shell.shell.completer  # type: ignore
    return completer.complete_from_context(CompletionContext(skipped_command_context))",skip_part_num == 0,not skip_part_num
xonsh,https://github.com/xonsh/xonsh/tree/master/xonsh/completers/commands.py,,complete_skipper$44,"def complete_skipper(command_context: CommandContext):
    """"""
    Skip over several tokens (e.g., sudo) and complete based on the rest of the command.
    """"""

    # Contextual completers don't need us to skip tokens since they get the correct completion context -
    # meaning we only need to skip commands like ``sudo``.
    skip_part_num = 0
    # all the args before the current argument
    for arg in command_context.args[: command_context.arg_index]:
        if arg.value not in SKIP_TOKENS:
            break
        skip_part_num += 1

    if skip_part_num == 0:
        return None

    skipped_command_context = command_context._replace(
        args=command_context.args[skip_part_num:],
        arg_index=command_context.arg_index - skip_part_num,
    )

    if skipped_command_context.arg_index == 0:
        # completing the command after a SKIP_TOKEN
        return complete_command(skipped_command_context)

    completer: Completer = XSH.shell.shell.completer  # type: ignore
    return completer.complete_from_context(CompletionContext(skipped_command_context))",skipped_command_context.arg_index == 0,not skipped_command_context.arg_index
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/generation_flax_utils.py,FlaxGenerationMixin,unflatten_beam_dim$718,"def unflatten_beam_dim(tensor, batch_size, num_beams):
            """"""Unflattens the first, flat batch*beam dimension of a non-scalar array.""""""
            # ignore scalars (e.g. cache index)
            if tensor.ndim == 0:
                return tensor
            return tensor.reshape((batch_size, num_beams) + tensor.shape[1:])",tensor.ndim == 0,not tensor.ndim
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_discovery_v1.py,TestModel_DocumentAccepted,test_document_accepted_serialization$7737,"def test_document_accepted_serialization(self):
        """"""
        Test serialization/deserialization for DocumentAccepted
        """"""

        # Construct dict forms of any model objects needed in order to build this model.

        notice_model = {} # Notice
        notice_model['notice_id'] = 'testString'
        notice_model['created'] = ""2019-01-01T12:00:00Z""
        notice_model['document_id'] = 'testString'
        notice_model['query_id'] = 'testString'
        notice_model['severity'] = 'warning'
        notice_model['step'] = 'testString'
        notice_model['description'] = 'testString'

        # Construct a json representation of a DocumentAccepted model
        document_accepted_model_json = {}
        document_accepted_model_json['document_id'] = 'testString'
        document_accepted_model_json['status'] = 'processing'
        document_accepted_model_json['notices'] = [notice_model]

        # Construct a model instance of DocumentAccepted by calling from_dict on the json representation
        document_accepted_model = DocumentAccepted.from_dict(document_accepted_model_json)
        assert document_accepted_model != False

        # Construct a model instance of DocumentAccepted by calling from_dict on the json representation
        document_accepted_model_dict = DocumentAccepted.from_dict(document_accepted_model_json).__dict__
        document_accepted_model2 = DocumentAccepted(**document_accepted_model_dict)

        # Verify the model instances are equivalent
        assert document_accepted_model == document_accepted_model2

        # Convert model instance back to dict and verify no loss of data
        document_accepted_model_json2 = document_accepted_model.to_dict()
        assert document_accepted_model_json2 == document_accepted_model_json",document_accepted_model != False,document_accepted_model
transformers,https://github.com/huggingface/transformers/tree/master/src/transformers/models/distilbert/modeling_distilbert.py,MultiHeadSelfAttention,__init__$138,"def __init__(self, config: PretrainedConfig):
        super().__init__()

        self.n_heads = config.n_heads
        self.dim = config.dim
        self.dropout = nn.Dropout(p=config.attention_dropout)

        # Have an even number of multi heads that divide the dimensions
        if self.dim % self.n_heads != 0:
            # Raise value errors for even multi-head attention nodes
            raise ValueError(f""self.n_heads: {self.n_heads} must divide self.dim: {self.dim} evenly"")

        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)
        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)
        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)
        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)

        self.pruned_heads: Set[int] = set()",self.dim % self.n_heads != 0,self.dim % self.n_heads
numpy,https://github.com/numpy/numpy/tree/master/numpy/array_api/tests/test_array_object.py,,test_python_scalar_construtors$262,"def test_python_scalar_construtors():
    b = asarray(False)
    i = asarray(0)
    f = asarray(0.0)

    assert bool(b) == False
    assert int(i) == 0
    assert float(f) == 0.0
    assert operator.index(i) == 0

    # bool/int/float should only be allowed on 0-D arrays.
    assert_raises(TypeError, lambda: bool(asarray([False])))
    assert_raises(TypeError, lambda: int(asarray([0])))
    assert_raises(TypeError, lambda: float(asarray([0.0])))
    assert_raises(TypeError, lambda: operator.index(asarray([0])))

    # bool/int/float should only be allowed on arrays of the corresponding
    # dtype
    assert_raises(ValueError, lambda: bool(i))
    assert_raises(ValueError, lambda: bool(f))

    assert_raises(ValueError, lambda: int(b))
    assert_raises(ValueError, lambda: int(f))

    assert_raises(ValueError, lambda: float(b))
    assert_raises(ValueError, lambda: float(i))

    assert_raises(TypeError, lambda: operator.index(b))
    assert_raises(TypeError, lambda: operator.index(f))",bool(b) == False,not b
numpy,https://github.com/numpy/numpy/tree/master/numpy/array_api/tests/test_array_object.py,,test_python_scalar_construtors$262,"def test_python_scalar_construtors():
    b = asarray(False)
    i = asarray(0)
    f = asarray(0.0)

    assert bool(b) == False
    assert int(i) == 0
    assert float(f) == 0.0
    assert operator.index(i) == 0

    # bool/int/float should only be allowed on 0-D arrays.
    assert_raises(TypeError, lambda: bool(asarray([False])))
    assert_raises(TypeError, lambda: int(asarray([0])))
    assert_raises(TypeError, lambda: float(asarray([0.0])))
    assert_raises(TypeError, lambda: operator.index(asarray([0])))

    # bool/int/float should only be allowed on arrays of the corresponding
    # dtype
    assert_raises(ValueError, lambda: bool(i))
    assert_raises(ValueError, lambda: bool(f))

    assert_raises(ValueError, lambda: int(b))
    assert_raises(ValueError, lambda: int(f))

    assert_raises(ValueError, lambda: float(b))
    assert_raises(ValueError, lambda: float(i))

    assert_raises(TypeError, lambda: operator.index(b))
    assert_raises(TypeError, lambda: operator.index(f))",int(i) == 0,not int(i)
numpy,https://github.com/numpy/numpy/tree/master/numpy/array_api/tests/test_array_object.py,,test_python_scalar_construtors$262,"def test_python_scalar_construtors():
    b = asarray(False)
    i = asarray(0)
    f = asarray(0.0)

    assert bool(b) == False
    assert int(i) == 0
    assert float(f) == 0.0
    assert operator.index(i) == 0

    # bool/int/float should only be allowed on 0-D arrays.
    assert_raises(TypeError, lambda: bool(asarray([False])))
    assert_raises(TypeError, lambda: int(asarray([0])))
    assert_raises(TypeError, lambda: float(asarray([0.0])))
    assert_raises(TypeError, lambda: operator.index(asarray([0])))

    # bool/int/float should only be allowed on arrays of the corresponding
    # dtype
    assert_raises(ValueError, lambda: bool(i))
    assert_raises(ValueError, lambda: bool(f))

    assert_raises(ValueError, lambda: int(b))
    assert_raises(ValueError, lambda: int(f))

    assert_raises(ValueError, lambda: float(b))
    assert_raises(ValueError, lambda: float(i))

    assert_raises(TypeError, lambda: operator.index(b))
    assert_raises(TypeError, lambda: operator.index(f))",float(f) == 0.0,not float(f)
numpy,https://github.com/numpy/numpy/tree/master/numpy/array_api/tests/test_array_object.py,,test_python_scalar_construtors$262,"def test_python_scalar_construtors():
    b = asarray(False)
    i = asarray(0)
    f = asarray(0.0)

    assert bool(b) == False
    assert int(i) == 0
    assert float(f) == 0.0
    assert operator.index(i) == 0

    # bool/int/float should only be allowed on 0-D arrays.
    assert_raises(TypeError, lambda: bool(asarray([False])))
    assert_raises(TypeError, lambda: int(asarray([0])))
    assert_raises(TypeError, lambda: float(asarray([0.0])))
    assert_raises(TypeError, lambda: operator.index(asarray([0])))

    # bool/int/float should only be allowed on arrays of the corresponding
    # dtype
    assert_raises(ValueError, lambda: bool(i))
    assert_raises(ValueError, lambda: bool(f))

    assert_raises(ValueError, lambda: int(b))
    assert_raises(ValueError, lambda: int(f))

    assert_raises(ValueError, lambda: float(b))
    assert_raises(ValueError, lambda: float(i))

    assert_raises(TypeError, lambda: operator.index(b))
    assert_raises(TypeError, lambda: operator.index(f))",operator.index(i) == 0,not operator.index(i)
OctoBot,https://github.com/Drakkar-Software/OctoBot/tree/master/octobot/community/community_manager.py,CommunityManager,_get_real_portfolio_value$206,"def _get_real_portfolio_value(self):
        if self.has_real_trader:
            total_value = 0
            for exchange_manager in self.exchange_managers:
                current_value = trading_api.get_current_portfolio_value(exchange_manager)
                # current_value might be 0 if no trades have been made / canceled => use origin value
                if current_value == 0:
                    current_value = trading_api.get_origin_portfolio_value(exchange_manager)
                total_value += current_value
            return float(total_value)
        else:
            return 0",current_value == 0,not current_value
pretix,https://github.com/pretix/pretix/tree/master/src/tests/api/test_events.py,,test_event_list_filter$186,"def test_event_list_filter(token_client, organizer, event):
    resp = token_client.get('/api/v1/organizers/{}/events/?attr[type]=Conference'.format(organizer.slug))
    assert resp.status_code == 200
    assert resp.data['count'] == 1

    resp = token_client.get('/api/v1/organizers/{}/events/?attr[type]='.format(organizer.slug))
    assert resp.status_code == 200
    assert resp.data['count'] == 0",resp.data['count'] == 0,not resp.data['count']
albert_zh,https://github.com/brightmart/albert_zh/tree/master//run_classifier_sp_google.py,XnliProcessor,get_dev_examples$241,"def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    lines = self._read_tsv(os.path.join(data_dir, ""xnli.dev.tsv""))
    examples = []
    for (i, line) in enumerate(lines):
      if i == 0:
        continue
      guid = ""dev-%d"" % (i)
      language = tokenization.convert_to_unicode(line[0])
      if language != tokenization.convert_to_unicode(self.language):
        continue
      text_a = tokenization.convert_to_unicode(line[6])
      text_b = tokenization.convert_to_unicode(line[7])
      label = tokenization.convert_to_unicode(line[1])
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    return examples",i == 0,not i
topydo,https://github.com/topydo/topydo/tree/master/topydo/lib/printers/PrettyPrinter.py,PrettyPrinter,print_groups$89,"def print_groups(self, p_groups):
        result = []
        first = True

        def print_header(p_key):
            """""" Prints a header for the given key. """"""
            if not first:
                result.append('')

            key_string = "", "".join(p_key)
            result.append(key_string)
            result.append(""="" * len(key_string))

        for key, todos in p_groups.items():
            if key != ():
                # don't print a header for the case that no valid grouping
                # could be made (e.g. an invalid group expression)
                print_header(key)

            first = False
            result += self.print_list(todos)

        return [TopydoString(s) for s in result]",key != (),key
open_lth,https://github.com/facebookresearch/open_lth/tree/master/lottery/runner.py,LotteryRunner,_prune_level$122,"def _prune_level(self, level: int):
        new_location = self.desc.run_path(self.replicate, level)
        if Mask.exists(new_location): return

        if level == 0:
            Mask.ones_like(models.registry.get(self.desc.model_hparams)).save(new_location)
        else:
            old_location = self.desc.run_path(self.replicate, level-1)
            model = models.registry.load(old_location, self.desc.train_end_step,
                                         self.desc.model_hparams, self.desc.train_outputs)
            pruning.registry.get(self.desc.pruning_hparams)(model, Mask.load(old_location)).save(new_location)",level == 0,not level
specter-desktop,https://github.com/cryptoadvance/specter-desktop/tree/master/src/cryptoadvance/specter/wallet.py,Wallet,check_unused$604,"def check_unused(self):
        """"""Check current receive address is unused and get new if needed""""""
        addr = self.address
        try:
            while self.rpc.getreceivedbyaddress(addr, 0) != 0:
                addr = self.getnewaddress()
        except Exception as e:
            logger.error(f""Failed to check for address reuse: {e}"")","self.rpc.getreceivedbyaddress(addr, 0) != 0","self.rpc.getreceivedbyaddress(addr, 0)"
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_speech_to_text_v1.py,TestModel_AudioMetrics,test_audio_metrics_serialization$3752,"def test_audio_metrics_serialization(self):
        """"""
        Test serialization/deserialization for AudioMetrics
        """"""

        # Construct dict forms of any model objects needed in order to build this model.

        audio_metrics_histogram_bin_model = {} # AudioMetricsHistogramBin
        audio_metrics_histogram_bin_model['begin'] = 72.5
        audio_metrics_histogram_bin_model['end'] = 72.5
        audio_metrics_histogram_bin_model['count'] = 38

        audio_metrics_details_model = {} # AudioMetricsDetails
        audio_metrics_details_model['final'] = True
        audio_metrics_details_model['end_time'] = 72.5
        audio_metrics_details_model['signal_to_noise_ratio'] = 72.5
        audio_metrics_details_model['speech_ratio'] = 72.5
        audio_metrics_details_model['high_frequency_loss'] = 72.5
        audio_metrics_details_model['direct_current_offset'] = [audio_metrics_histogram_bin_model]
        audio_metrics_details_model['clipping_rate'] = [audio_metrics_histogram_bin_model]
        audio_metrics_details_model['speech_level'] = [audio_metrics_histogram_bin_model]
        audio_metrics_details_model['non_speech_level'] = [audio_metrics_histogram_bin_model]

        # Construct a json representation of a AudioMetrics model
        audio_metrics_model_json = {}
        audio_metrics_model_json['sampling_interval'] = 72.5
        audio_metrics_model_json['accumulated'] = audio_metrics_details_model

        # Construct a model instance of AudioMetrics by calling from_dict on the json representation
        audio_metrics_model = AudioMetrics.from_dict(audio_metrics_model_json)
        assert audio_metrics_model != False

        # Construct a model instance of AudioMetrics by calling from_dict on the json representation
        audio_metrics_model_dict = AudioMetrics.from_dict(audio_metrics_model_json).__dict__
        audio_metrics_model2 = AudioMetrics(**audio_metrics_model_dict)

        # Verify the model instances are equivalent
        assert audio_metrics_model == audio_metrics_model2

        # Convert model instance back to dict and verify no loss of data
        audio_metrics_model_json2 = audio_metrics_model.to_dict()
        assert audio_metrics_model_json2 == audio_metrics_model_json",audio_metrics_model != False,audio_metrics_model
routersploit,https://github.com/threat9/routersploit/tree/master/routersploit/modules/exploits/routers/tplink/wdr842nd_wdr842n_configure_disclosure.py,Exploit,decrypt_authKey$26,"def decrypt_authKey(self, authKey):
        matrix = [[0 for _ in xrange(15)] for _ in range(15)]
        passwdLen = 0
        strDe = ""RDpbLfCPsJZ7fiv""
        dic = ""yLwVl0zKqws7LgKPRQ84Mdt708T1qQ3Ha7xv3H7NyU84p21BriUWBU43odz3iP4rBL3cD"" \
              ""02KZciXTysVXiV8ngg6vL48rPJyAUw0HurW20xqxv9aYb4M9wK1Ae0wlro510qXeU07kV5"" \
              ""7fQMc8L6aLgMLwygtc0F10a0Dg70TOoouyFhdysuRMO51yY5ZlOZZLEal1h0t9YQW0Ko7oB"" \
              ""wmCAHoic4HYbUyVeU3sfQ1xtXcPcf1aT303wAQhv66qzW""

        passwd = ''
        for crIndex in xrange(0, 15):
            passwdList = ''
            strComp_authkey = authKey[crIndex]
            codeCr = ord(strDe[crIndex])
            for index in xrange(32, 127):
                strtmp = chr(index)
                codeCl = ord(strtmp[0])
                strDic = dic[(codeCl ^ codeCr) % 255]
                if strComp_authkey == strDic:
                    passwdList += strtmp
            matrix[crIndex] = passwdList

        for i in xrange(0, 15):
            if len(matrix[i]) == 0:
                passwdLen = i
                break
            elif i == 14:
                passwdLen = 15
        for i in xrange(0, passwdLen):
            passwd += matrix[i] + '\n'
        return passwd",len(matrix[i]) == 0,not matrix[i]
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_spectral_norm_op.py,,spectral_norm$24,"def spectral_norm(weight, u, v, dim, power_iters, eps):
    shape = weight.shape
    weight_mat = weight.copy()
    h = shape[dim]
    w = np.prod(shape) // h
    if dim != 0:
        perm = [dim] + [d for d in range(len(shape)) if d != dim]
        weight_mat = weight_mat.transpose(perm)
    weight_mat = weight_mat.reshape((h, w))

    u = u.reshape((h, 1))
    v = v.reshape((w, 1))
    for i in range(power_iters):
        v = np.matmul(weight_mat.T, u)
        v_norm = np.sqrt((v * v).sum())
        v = v / (v_norm + eps)
        u = np.matmul(weight_mat, v)
        u_norm = np.sqrt((u * u).sum())
        u = u / (u_norm + eps)

    sigma = (u * np.matmul(weight_mat, v)).sum()
    return weight / sigma",dim != 0,dim
Final_word_Similarity,https://github.com/yaleimeng/Final_word_Similarity/tree/master/hownet/howNet.py,How_Similarity,calcGlossarySim$213,"def calcGlossarySim(self, w1, w2):
        '''      璁＄畻璇嶆眹琛ㄤ腑涓や釜璇嶇殑鐩镐技搴        '''
        if w1 == None or w2 == None: return 0.0

        if w1.solid != w2.solid: return 0.0

        sim1 = self.calcSememeSimFirst(w1, w2)
        sim2 = self.calcSememeSimOther(w1, w2)
        sim3 = self.calcSememeSimRelation(w1, w2)
        sim4 = self.calcSememeSimSymbol(w1, w2)

        sim = self.BETA[0] * sim1 + self.BETA[1] * sim1 * sim2 + self.BETA[2] * sim1 * sim2 * sim3 + (
                self.BETA[3] * sim1 * sim2 * sim3 * sim4)

        return sim",w1 == None,not w1
Final_word_Similarity,https://github.com/yaleimeng/Final_word_Similarity/tree/master/hownet/howNet.py,How_Similarity,calcGlossarySim$213,"def calcGlossarySim(self, w1, w2):
        '''      璁＄畻璇嶆眹琛ㄤ腑涓や釜璇嶇殑鐩镐技搴        '''
        if w1 == None or w2 == None: return 0.0

        if w1.solid != w2.solid: return 0.0

        sim1 = self.calcSememeSimFirst(w1, w2)
        sim2 = self.calcSememeSimOther(w1, w2)
        sim3 = self.calcSememeSimRelation(w1, w2)
        sim4 = self.calcSememeSimSymbol(w1, w2)

        sim = self.BETA[0] * sim1 + self.BETA[1] * sim1 * sim2 + self.BETA[2] * sim1 * sim2 * sim3 + (
                self.BETA[3] * sim1 * sim2 * sim3 * sim4)

        return sim",w2 == None,not w2
pyld,https://github.com/digitalbazaar/pyld/tree/master/lib/pyld/jsonld.py,JsonLdProcessor,_filter_subject$4291,"def _filter_subject(self, state, subject, frame, flags):
        """"""
        Returns True if the given subject matches the given frame.

        Matches either based on explicit type inclusion where the node has any
        type listed in the frame. If the frame has empty types defined matches
        nodes not having a @type. If the frame has a type of {} defined matches
        nodes having any type defined.

        Otherwise, does duck typing, where the node must have all of the
        properties defined in the frame.

        :param state: the current framing state.
        :param subject: the subject to check.
        :param frame: the frame to check.
        :param flags: the frame flags.

        :return: True if the subject matches, False if not.
        """"""
        # check ducktype
        wildcard = True
        matches_some = False
        for k, v in sorted(frame.items()):
            match_this = False
            node_values = JsonLdProcessor.get_values(subject, k)
            is_empty = len(v) == 0

            if k == '@id':
                # if @id is not a wildcard and is not empty, then match
                # or not on specific value
                if len(frame['@id']) == 0 or _is_empty_object(frame['@id'][0]):
                    match_this = True
                elif frame['@id']:
                    match_this = node_values[0] in frame['@id']
                if not flags['requireAll']:
                    return match_this
            elif k == '@type':
                wildcard = False
                if is_empty:
                    if len(node_values) > 0:
                        # don't match on no @type
                        return False
                    match_this = True
                elif len(frame['@type']) == 1 and _is_empty_object(frame['@type'][0]):
                    match_this = len(node_values) > 0
                else:
                    # match on a specific @type
                    for tv in frame['@type']:
                        if _is_object(tv) and '@default' in tv:
                            # match on default object
                            match_this = True
                        elif not match_this:
                            match_this = tv in node_values
                    if not flags['requireAll']:
                        return match_this
            elif _is_keyword(k):
                continue
            else:
                # force a copy of this frame entry so it can be manipulated
                this_frame = JsonLdProcessor.get_values(frame, k)
                this_frame = this_frame[0] if this_frame else None
                has_default = False
                if this_frame:
                    self._validate_frame([this_frame])
                    has_default = '@default' in this_frame

                # no longer a wildcard pattern if frame has any non-keyword
                # properties
                wildcard = False

                # skip, but allow match if node has no value for property, and
                # frame has a default value
                if not node_values and has_default:
                    continue

                # if frame value is empty, don't match if subject has any value
                if node_values and is_empty:
                    return False

                if this_frame is None:
                    # node does not match if values is not empty and the value of
                    # property in frame is match none.
                    if node_values:
                        return False
                    match_this = True
                else:
                    if _is_list(this_frame):
                        list_value = this_frame['@list'][0] if this_frame['@list'] else None
                        if _is_list(node_values[0] if node_values else None):
                            node_list_values = node_values[0]['@list']
                            if _is_value(list_value):
                                match_this = any(
                                    self._value_match(list_value, lv) for lv in node_list_values)
                            elif _is_subject(list_value) or _is_subject_reference(list_value):
                                match_this = any(
                                    self._node_match(state, list_value, lv, flags) for lv in node_list_values)
                    elif _is_value(this_frame):
                        # match on any matching value
                        match_this = any(self._value_match(this_frame, nv) for nv in node_values)
                    elif _is_subject_reference(this_frame):
                        match_this = any(
                            self._node_match(state, this_frame, nv, flags) for nv in node_values)
                    elif _is_object(this_frame):
                        match_this = len(node_values) > 0
                    else:
                        match_this = False

            # all non-defaulted values must match if requireAll is set
            if not match_this and flags['requireAll']:
                return False

            matches_some = matches_some or match_this

        # return true if wildcard or subject matches some properties
        return wildcard or matches_some",len(frame['@id']) == 0,not frame['@id']
gandissect,https://github.com/CSAILVision/gandissect/tree/master/netdissect/aceoptimize.py,,evaluate_ablation$769,"def evaluate_ablation(args, model, segmenter, eval_sample, classnum, layer,
        ordering):
    total_bincount = 0
    data_size = 0
    progress = default_progress()
    for l in model.ablation:
        model.ablation[l] = None
    feature_units = model.feature_shape[args.layer][1]
    feature_shape = model.feature_shape[args.layer][2:]
    repeats = len(ordering)
    total_scores = torch.zeros(repeats + 1)
    for i, batch in enumerate(progress(torch.utils.data.DataLoader(
                TensorDataset(eval_sample),
                batch_size=args.inference_batch_size, num_workers=10,
                pin_memory=True),
                desc=""Evaluate interventions"")):
        tensor_image = model(zbatch)
        segmented_image = segmenter.segment_batch(tensor_image,
                    downsample=2)
        mask = (segmented_image == classnum).max(1)[0]
        downsampled_seg = torch.nn.functional.adaptive_avg_pool2d(
                mask.float()[:,None,:,:], feature_shape)[:,0,:,:]
        total_scores[0] += downsampled_seg.sum().cpu()
        # Now we need to do an intervention for every location
        # that had a nonzero downsampled_seg, if any.
        interventions_needed = downsampled_seg.nonzero()
        location_count = len(interventions_needed)
        if location_count == 0:
            continue
        interventions_needed = interventions_needed.repeat(repeats, 1)
        inter_z = batch[0][interventions_needed[:,0]].to(device)
        inter_chan = torch.zeros(repeats, location_count, feature_units,
                device=device)
        for j, u in enumerate(ordering):
            inter_chan[j:, :, u] = 1
        inter_chan = inter_chan.view(len(inter_z), feature_units)
        inter_loc = interventions_needed[:,1:]
        scores = torch.zeros(len(inter_z))
        batch_size = len(batch[0])
        for j in range(0, len(inter_z), batch_size):
            ibz = inter_z[j:j+batch_size]
            ibl = inter_loc[j:j+batch_size].t()
            imask = torch.zeros((len(ibz),) + feature_shape, device=ibz.device)
            imask[(torch.arange(len(ibz)),) + tuple(ibl)] = 1
            ibc = inter_chan[j:j+batch_size]
            model.edit_layer(args.layer, ablation=(
                    imask.float()[:,None,:,:] * ibc[:,:,None,None]))
            _, seg, _, _, _ = (
                recovery.recover_im_seg_bc_and_features(
                    [ibz], model))
            mask = (seg == classnum).max(1)[0]
            downsampled_iseg = torch.nn.functional.adaptive_avg_pool2d(
                    mask.float()[:,None,:,:], feature_shape)[:,0,:,:]
            scores[j:j+batch_size] = downsampled_iseg[
                    (torch.arange(len(ibz)),) + tuple(ibl)]
        scores = scores.view(repeats, location_count).sum(1)
        total_scores[1:] += scores
    return total_scores",location_count == 0,not location_count
DIG,https://github.com/divelab/DIG/tree/master/dig/ggraph/method/GraphAF/model/graphflow_con_rl.py,GraphFlowModel_con_rl,initialize_masks$1331,"def initialize_masks(self, max_node_unroll=38, max_edge_unroll=12):
        """"""
        Args:
            max node unroll: maximal number of nodes in molecules to be generated (default: 38)
            max edge unroll: maximal number of edges to predict for each generated nodes (default: 12, calculated from zink250K data)
        Returns:
            node_masks: node mask for each step
            adj_masks: adjacency mask for each step
            is_node_update_mask: 1 indicate this step is for updating node features
            flow_core_edge_mask: get the distributions we want to model in adjacency matrix
        """"""
        num_masks = int(max_node_unroll + (max_edge_unroll - 1) * max_edge_unroll / 2 + (max_node_unroll - max_edge_unroll) * (max_edge_unroll))
        num_mask_edge = int(num_masks - max_node_unroll)

        node_masks1 = torch.zeros([max_node_unroll, max_node_unroll]).byte()
        adj_masks1 = torch.zeros([max_node_unroll, max_node_unroll, max_node_unroll]).byte()
        node_masks2 = torch.zeros([num_mask_edge, max_node_unroll]).byte()
        adj_masks2 = torch.zeros([num_mask_edge, max_node_unroll, max_node_unroll]).byte()        
        link_prediction_index = torch.zeros([num_mask_edge, 2]).long()
        flow_core_edge_masks = torch.zeros([max_node_unroll, max_node_unroll]).byte()

        cnt = 0
        cnt_node = 0
        cnt_edge = 0
        for i in range(max_node_unroll):
            node_masks1[cnt_node][:i] = 1
            adj_masks1[cnt_node][:i, :i] = 1
            cnt += 1
            cnt_node += 1

            edge_total = 0
            if i < max_edge_unroll:
                start = 0
                edge_total = i
            else:
                start = i - max_edge_unroll
                edge_total = max_edge_unroll
            for j in range(edge_total):
                if j == 0:
                    node_masks2[cnt_edge][:i+1] = 1
                    adj_masks2[cnt_edge] = adj_masks1[cnt_node-1].clone()
                    adj_masks2[cnt_edge][i,i] = 1
                else:
                    node_masks2[cnt_edge][:i+1] = 1
                    adj_masks2[cnt_edge] = adj_masks2[cnt_edge-1].clone()
                    adj_masks2[cnt_edge][i, start + j -1] = 1
                    adj_masks2[cnt_edge][start + j -1, i] = 1
                cnt += 1
                cnt_edge += 1
        assert cnt == num_masks, 'masks cnt wrong'
        assert cnt_node == max_node_unroll, 'node masks cnt wrong'
        assert cnt_edge == num_mask_edge, 'edge masks cnt wrong'

    
        cnt = 0
        for i in range(max_node_unroll):
            if i < max_edge_unroll:
                start = 0
                edge_total = i
            else:
                start = i - max_edge_unroll
                edge_total = max_edge_unroll
        
            for j in range(edge_total):
                link_prediction_index[cnt][0] = start + j
                link_prediction_index[cnt][1] = i
                cnt += 1
        assert cnt == num_mask_edge, 'edge mask initialize fail'


        for i in range(max_node_unroll):
            if i == 0:
                continue
            if i < max_edge_unroll:
                start = 0
                end = i
            else:
                start = i - max_edge_unroll
                end = i 
            flow_core_edge_masks[i][start:end] = 1

        node_masks = torch.cat((node_masks1, node_masks2), dim=0)
        adj_masks = torch.cat((adj_masks1, adj_masks2), dim=0)

        node_masks = nn.Parameter(node_masks, requires_grad=False)
        adj_masks = nn.Parameter(adj_masks, requires_grad=False)
        link_prediction_index = nn.Parameter(link_prediction_index, requires_grad=False)
        flow_core_edge_masks = nn.Parameter(flow_core_edge_masks, requires_grad=False)
        
        return node_masks, adj_masks, link_prediction_index, flow_core_edge_masks",i == 0,not i
DIG,https://github.com/divelab/DIG/tree/master/dig/ggraph/method/GraphAF/model/graphflow_con_rl.py,GraphFlowModel_con_rl,initialize_masks$1331,"def initialize_masks(self, max_node_unroll=38, max_edge_unroll=12):
        """"""
        Args:
            max node unroll: maximal number of nodes in molecules to be generated (default: 38)
            max edge unroll: maximal number of edges to predict for each generated nodes (default: 12, calculated from zink250K data)
        Returns:
            node_masks: node mask for each step
            adj_masks: adjacency mask for each step
            is_node_update_mask: 1 indicate this step is for updating node features
            flow_core_edge_mask: get the distributions we want to model in adjacency matrix
        """"""
        num_masks = int(max_node_unroll + (max_edge_unroll - 1) * max_edge_unroll / 2 + (max_node_unroll - max_edge_unroll) * (max_edge_unroll))
        num_mask_edge = int(num_masks - max_node_unroll)

        node_masks1 = torch.zeros([max_node_unroll, max_node_unroll]).byte()
        adj_masks1 = torch.zeros([max_node_unroll, max_node_unroll, max_node_unroll]).byte()
        node_masks2 = torch.zeros([num_mask_edge, max_node_unroll]).byte()
        adj_masks2 = torch.zeros([num_mask_edge, max_node_unroll, max_node_unroll]).byte()        
        link_prediction_index = torch.zeros([num_mask_edge, 2]).long()
        flow_core_edge_masks = torch.zeros([max_node_unroll, max_node_unroll]).byte()

        cnt = 0
        cnt_node = 0
        cnt_edge = 0
        for i in range(max_node_unroll):
            node_masks1[cnt_node][:i] = 1
            adj_masks1[cnt_node][:i, :i] = 1
            cnt += 1
            cnt_node += 1

            edge_total = 0
            if i < max_edge_unroll:
                start = 0
                edge_total = i
            else:
                start = i - max_edge_unroll
                edge_total = max_edge_unroll
            for j in range(edge_total):
                if j == 0:
                    node_masks2[cnt_edge][:i+1] = 1
                    adj_masks2[cnt_edge] = adj_masks1[cnt_node-1].clone()
                    adj_masks2[cnt_edge][i,i] = 1
                else:
                    node_masks2[cnt_edge][:i+1] = 1
                    adj_masks2[cnt_edge] = adj_masks2[cnt_edge-1].clone()
                    adj_masks2[cnt_edge][i, start + j -1] = 1
                    adj_masks2[cnt_edge][start + j -1, i] = 1
                cnt += 1
                cnt_edge += 1
        assert cnt == num_masks, 'masks cnt wrong'
        assert cnt_node == max_node_unroll, 'node masks cnt wrong'
        assert cnt_edge == num_mask_edge, 'edge masks cnt wrong'

    
        cnt = 0
        for i in range(max_node_unroll):
            if i < max_edge_unroll:
                start = 0
                edge_total = i
            else:
                start = i - max_edge_unroll
                edge_total = max_edge_unroll
        
            for j in range(edge_total):
                link_prediction_index[cnt][0] = start + j
                link_prediction_index[cnt][1] = i
                cnt += 1
        assert cnt == num_mask_edge, 'edge mask initialize fail'


        for i in range(max_node_unroll):
            if i == 0:
                continue
            if i < max_edge_unroll:
                start = 0
                end = i
            else:
                start = i - max_edge_unroll
                end = i 
            flow_core_edge_masks[i][start:end] = 1

        node_masks = torch.cat((node_masks1, node_masks2), dim=0)
        adj_masks = torch.cat((adj_masks1, adj_masks2), dim=0)

        node_masks = nn.Parameter(node_masks, requires_grad=False)
        adj_masks = nn.Parameter(adj_masks, requires_grad=False)
        link_prediction_index = nn.Parameter(link_prediction_index, requires_grad=False)
        flow_core_edge_masks = nn.Parameter(flow_core_edge_masks, requires_grad=False)
        
        return node_masks, adj_masks, link_prediction_index, flow_core_edge_masks",j == 0,not j
ansible,https://github.com/ansible/ansible/tree/master/test/units/playbook/test_included_file.py,,test_process_include_results$108,"def test_process_include_results(mock_iterator, mock_variable_manager):
    hostname = ""testhost1""
    hostname2 = ""testhost2""

    parent_task_ds = {'debug': 'msg=foo'}
    parent_task = Task.load(parent_task_ds)
    parent_task._play = None

    task_ds = {'include': 'include_test.yml'}
    loaded_task = TaskInclude.load(task_ds, task_include=parent_task)

    return_data = {'include': 'include_test.yml'}
    # The task in the TaskResult has to be a TaskInclude so it has a .static attr
    result1 = task_result.TaskResult(host=hostname, task=loaded_task, return_data=return_data)
    result2 = task_result.TaskResult(host=hostname2, task=loaded_task, return_data=return_data)
    results = [result1, result2]

    fake_loader = DictDataLoader({'include_test.yml': """"})

    res = IncludedFile.process_include_results(results, mock_iterator, fake_loader, mock_variable_manager)
    assert isinstance(res, list)
    assert len(res) == 1
    assert res[0]._filename == os.path.join(os.getcwd(), 'include_test.yml')
    assert res[0]._hosts == ['testhost1', 'testhost2']
    assert res[0]._args == {}
    assert res[0]._vars == {}",res[0]._args == {},not res[0]._args
ansible,https://github.com/ansible/ansible/tree/master/test/units/playbook/test_included_file.py,,test_process_include_results$108,"def test_process_include_results(mock_iterator, mock_variable_manager):
    hostname = ""testhost1""
    hostname2 = ""testhost2""

    parent_task_ds = {'debug': 'msg=foo'}
    parent_task = Task.load(parent_task_ds)
    parent_task._play = None

    task_ds = {'include': 'include_test.yml'}
    loaded_task = TaskInclude.load(task_ds, task_include=parent_task)

    return_data = {'include': 'include_test.yml'}
    # The task in the TaskResult has to be a TaskInclude so it has a .static attr
    result1 = task_result.TaskResult(host=hostname, task=loaded_task, return_data=return_data)
    result2 = task_result.TaskResult(host=hostname2, task=loaded_task, return_data=return_data)
    results = [result1, result2]

    fake_loader = DictDataLoader({'include_test.yml': """"})

    res = IncludedFile.process_include_results(results, mock_iterator, fake_loader, mock_variable_manager)
    assert isinstance(res, list)
    assert len(res) == 1
    assert res[0]._filename == os.path.join(os.getcwd(), 'include_test.yml')
    assert res[0]._hosts == ['testhost1', 'testhost2']
    assert res[0]._args == {}
    assert res[0]._vars == {}",res[0]._vars == {},not res[0]._vars
pynguin,https://github.com/se2p/pynguin/tree/master/tests/ga/test_chromosomefactory.py,,test_suite_factory_get_chromosome$17,"def test_suite_factory_get_chromosome():
    test_case_chromosome_factory = MagicMock(tccf.TestCaseChromosomeFactory)
    factory = tscf.TestSuiteChromosomeFactory(
        test_case_chromosome_factory, OrderedSet(), OrderedSet()
    )
    config.configuration.search_algorithm.min_initial_tests = 5
    config.configuration.search_algorithm.max_initial_tests = 5
    chromosome = factory.get_chromosome()
    assert (
        config.configuration.search_algorithm.min_initial_tests
        <= test_case_chromosome_factory.get_chromosome.call_count
        <= config.configuration.search_algorithm.max_initial_tests
    )
    assert isinstance(chromosome, tsc.TestSuiteChromosome)
    assert chromosome.get_fitness_functions() == []",chromosome.get_fitness_functions() == [],not chromosome.get_fitness_functions()
yt-dlc,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/downloader/f4m.py,F4mFD,real_download$319,"def real_download(self, filename, info_dict):
        man_url = info_dict['url']
        requested_bitrate = info_dict.get('tbr')
        self.to_screen('[%s] Downloading f4m manifest' % self.FD_NAME)

        urlh = self.ydl.urlopen(self._prepare_url(info_dict, man_url))
        man_url = urlh.geturl()
        # Some manifests may be malformed, e.g. prosiebensat1 generated manifests
        # (see https://github.com/ytdl-org/youtube-dl/issues/6215#issuecomment-121704244
        # and https://github.com/ytdl-org/youtube-dl/issues/7823)
        manifest = fix_xml_ampersands(urlh.read().decode('utf-8', 'ignore')).strip()

        doc = compat_etree_fromstring(manifest)
        formats = [(int(f.attrib.get('bitrate', -1)), f)
                   for f in self._get_unencrypted_media(doc)]
        if requested_bitrate is None or len(formats) == 1:
            # get the best format
            formats = sorted(formats, key=lambda f: f[0])
            rate, media = formats[-1]
        else:
            rate, media = list(filter(
                lambda f: int(f[0]) == requested_bitrate, formats))[0]

        # Prefer baseURL for relative URLs as per 11.2 of F4M 3.0 spec.
        man_base_url = get_base_url(doc) or man_url

        base_url = compat_urlparse.urljoin(man_base_url, media.attrib['url'])
        bootstrap_node = doc.find(_add_ns('bootstrapInfo'))
        boot_info, bootstrap_url = self._parse_bootstrap_node(
            bootstrap_node, man_base_url)
        live = boot_info['live']
        metadata_node = media.find(_add_ns('metadata'))
        if metadata_node is not None:
            metadata = compat_b64decode(metadata_node.text)
        else:
            metadata = None

        fragments_list = build_fragments_list(boot_info)
        test = self.params.get('test', False)
        if test:
            # We only download the first fragment
            fragments_list = fragments_list[:1]
        total_frags = len(fragments_list)
        # For some akamai manifests we'll need to add a query to the fragment url
        akamai_pv = xpath_text(doc, _add_ns('pv-2.0'))

        ctx = {
            'filename': filename,
            'total_frags': total_frags,
            'live': live,
        }

        self._prepare_frag_download(ctx)

        dest_stream = ctx['dest_stream']

        if ctx['complete_frags_downloaded_bytes'] == 0:
            write_flv_header(dest_stream)
            if not live:
                write_metadata_tag(dest_stream, metadata)

        base_url_parsed = compat_urllib_parse_urlparse(base_url)

        self._start_frag_download(ctx)

        frag_index = 0
        while fragments_list:
            seg_i, frag_i = fragments_list.pop(0)
            frag_index += 1
            if frag_index <= ctx['fragment_index']:
                continue
            name = 'Seg%d-Frag%d' % (seg_i, frag_i)
            query = []
            if base_url_parsed.query:
                query.append(base_url_parsed.query)
            if akamai_pv:
                query.append(akamai_pv.strip(';'))
            if info_dict.get('extra_param_to_segment_url'):
                query.append(info_dict['extra_param_to_segment_url'])
            url_parsed = base_url_parsed._replace(path=base_url_parsed.path + name, query='&'.join(query))
            try:
                success, down_data = self._download_fragment(ctx, url_parsed.geturl(), info_dict)
                if not success:
                    return False
                reader = FlvReader(down_data)
                while True:
                    try:
                        _, box_type, box_data = reader.read_box_info()
                    except DataTruncatedError:
                        if test:
                            # In tests, segments may be truncated, and thus
                            # FlvReader may not be able to parse the whole
                            # chunk. If so, write the segment as is
                            # See https://github.com/ytdl-org/youtube-dl/issues/9214
                            dest_stream.write(down_data)
                            break
                        raise
                    if box_type == b'mdat':
                        self._append_fragment(ctx, box_data)
                        break
            except (compat_urllib_error.HTTPError, ) as err:
                if live and (err.code == 404 or err.code == 410):
                    # We didn't keep up with the live window. Continue
                    # with the next available fragment.
                    msg = 'Fragment %d unavailable' % frag_i
                    self.report_warning(msg)
                    fragments_list = []
                else:
                    raise

            if not fragments_list and not test and live and bootstrap_url:
                fragments_list = self._update_live_fragments(bootstrap_url, frag_i)
                total_frags += len(fragments_list)
                if fragments_list and (fragments_list[0][1] > frag_i + 1):
                    msg = 'Missed %d fragments' % (fragments_list[0][1] - (frag_i + 1))
                    self.report_warning(msg)

        self._finish_frag_download(ctx)

        return True",ctx['complete_frags_downloaded_bytes'] == 0,not ctx['complete_frags_downloaded_bytes']
briefcase,https://github.com/beeware/briefcase/tree/master/tests/integrations/java/test_JDK__verify.py,,test_jdk_download_failure$403,"def test_jdk_download_failure(mock_tools, tmp_path):
    """"""If an error occurs downloading the JDK, an error is raised.""""""
    # Mock Linux as the host
    mock_tools.host_os = ""Linux""

    # Mock a failure on download
    mock_tools.download.file.side_effect = NetworkFailure(""mock"")

    # Invoking verify_jdk causes a network failure.
    with pytest.raises(NetworkFailure, match=""Unable to mock""):
        JDK.verify(mock_tools)

    # That download was attempted
    mock_tools.download.file.assert_called_with(
        url=""https://github.com/AdoptOpenJDK/openjdk8-binaries/releases/download/""
        ""jdk8u242-b08/OpenJDK8U-jdk_x64_linux_hotspot_8u242b08.tar.gz"",
        download_path=tmp_path / ""tools"",
        role=""Java 8 JDK"",
    )
    # No attempt was made to unpack the archive
    assert mock_tools.shutil.unpack_archive.call_count == 0",mock_tools.shutil.unpack_archive.call_count == 0,not mock_tools.shutil.unpack_archive.call_count
Transfer-Learning-Library,https://github.com/thuml/Transfer-Learning-Library/tree/master/examples/domain_adaptation/image_classification/mcc.py,,train$136,"def train(train_source_iter: ForeverDataIterator, train_target_iter: ForeverDataIterator,
          model: ImageClassifier, mcc: MinimumClassConfusionLoss, optimizer: SGD,
          lr_scheduler: LambdaLR, epoch: int, args: argparse.Namespace):
    batch_time = AverageMeter('Time', ':3.1f')
    data_time = AverageMeter('Data', ':3.1f')
    losses = AverageMeter('Loss', ':3.2f')
    trans_losses = AverageMeter('Trans Loss', ':3.2f')
    cls_accs = AverageMeter('Cls Acc', ':3.1f')

    progress = ProgressMeter(
        args.iters_per_epoch,
        [batch_time, data_time, losses, trans_losses, cls_accs],
        prefix=""Epoch: [{}]"".format(epoch))

    # switch to train mode
    model.train()

    end = time.time()
    for i in range(args.iters_per_epoch):
        x_s, labels_s = next(train_source_iter)[:2]
        x_t, = next(train_target_iter)[:1]

        x_s = x_s.to(device)
        x_t = x_t.to(device)
        labels_s = labels_s.to(device)

        # measure data loading time
        data_time.update(time.time() - end)

        # compute output
        x = torch.cat((x_s, x_t), dim=0)
        y, f = model(x)
        y_s, y_t = y.chunk(2, dim=0)

        cls_loss = F.cross_entropy(y_s, labels_s)
        transfer_loss = mcc(y_t)
        loss = cls_loss + transfer_loss * args.trade_off

        cls_acc = accuracy(y_s, labels_s)[0]

        losses.update(loss.item(), x_s.size(0))
        cls_accs.update(cls_acc.item(), x_s.size(0))
        trans_losses.update(transfer_loss.item(), x_s.size(0))

        # compute gradient and do SGD step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        if i % args.print_freq == 0:
            progress.display(i)",i % args.print_freq == 0,not i % args.print_freq
ML-GCN,https://github.com/Megvii-Nanjing/ML-GCN/tree/master//voc.py,,read_object_labels_csv$82,"def read_object_labels_csv(file, header=True):
    images = []
    num_categories = 0
    print('[dataset] read', file)
    with open(file, 'r') as f:
        reader = csv.reader(f)
        rownum = 0
        for row in reader:
            if header and rownum == 0:
                header = row
            else:
                if num_categories == 0:
                    num_categories = len(row) - 1
                name = row[0]
                labels = (np.asarray(row[1:num_categories + 1])).astype(np.float32)
                labels = torch.from_numpy(labels)
                item = (name, labels)
                images.append(item)
            rownum += 1
    return images",rownum == 0,not rownum
ML-GCN,https://github.com/Megvii-Nanjing/ML-GCN/tree/master//voc.py,,read_object_labels_csv$82,"def read_object_labels_csv(file, header=True):
    images = []
    num_categories = 0
    print('[dataset] read', file)
    with open(file, 'r') as f:
        reader = csv.reader(f)
        rownum = 0
        for row in reader:
            if header and rownum == 0:
                header = row
            else:
                if num_categories == 0:
                    num_categories = len(row) - 1
                name = row[0]
                labels = (np.asarray(row[1:num_categories + 1])).astype(np.float32)
                labels = torch.from_numpy(labels)
                item = (name, labels)
                images.append(item)
            rownum += 1
    return images",num_categories == 0,not num_categories
briefcase,https://github.com/beeware/briefcase/tree/master/tests/test_cmdline.py,,test_version_only$66,"def test_version_only(capsys):
    """"""``briefcase -V`` returns current version.""""""
    with pytest.raises(SystemExit) as excinfo:
        cmdline.parse_cmdline(""-V"".split())

    # Normal exit due to displaying help
    assert excinfo.value.code == 0
    # Version is displayed.
    output = capsys.readouterr().out
    assert output == f""{__version__}\n""",excinfo.value.code == 0,not excinfo.value.code
wtforms,https://github.com/wtforms/wtforms/tree/master/tests/validators/test_data_required.py,,test_data_required_clobber$28,"def test_data_required_clobber(dummy_form, dummy_field):
    """"""
    Data required should clobber the errors
    """"""
    validator = data_required()
    dummy_field.data = """"
    dummy_field.errors = [""Invalid Integer Value""]
    assert len(dummy_field.errors) == 1
    with pytest.raises(StopValidation):
        validator(dummy_form, dummy_field)
        assert len(dummy_field.errors) == 0",len(dummy_field.errors) == 0,not dummy_field.errors
breathe,https://github.com/michaeljones/breathe/tree/master/breathe/renderer/sphinxrenderer.py,SphinxRenderer,visit_union$1010,"def visit_union(self, node) -> List[Node]:
        # Read in the corresponding xml file and process
        file_data = self.compound_parser.parse(node.refid)
        nodeDef = file_data.compounddef

        self.context = cast(RenderContext, self.context)
        parent_context = self.context.create_child_context(file_data)
        new_context = parent_context.create_child_context(nodeDef)

        with WithContext(self, new_context):
            names = self.get_qualification()
            if self.nesting_level == 0:
                names.extend(nodeDef.compoundname.split(""::""))
            else:
                names.append(nodeDef.compoundname.split(""::"")[-1])
            declaration = self.join_nested_name(names)

            def content(contentnode):
                if nodeDef.includes:
                    for include in nodeDef.includes:
                        contentnode.extend(
                            self.render(include, new_context.create_child_context(include))
                        )
                rendered_data = self.render(file_data, parent_context)
                contentnode.extend(rendered_data)

            nodes = self.handle_declaration(nodeDef, declaration, content_callback=content)
        return nodes",self.nesting_level == 0,not self.nesting_level
addons-server,https://github.com/mozilla/addons-server/tree/master/src/olympia/addons/tests/test_tasks.py,,test_update_addon_weekly_downloads$233,"def test_update_addon_weekly_downloads():
    addon = addon_factory(weekly_downloads=0)
    count = 123
    data = [(addon.addonguid.hashed_guid, count)]
    assert addon.weekly_downloads == 0

    update_addon_weekly_downloads(data)
    addon.refresh_from_db()

    assert addon.weekly_downloads == count",addon.weekly_downloads == 0,not addon.weekly_downloads
zao-,https://github.com/qiucheng025/zao-/tree/master/plugins/extract/pipeline.py,Extractor,launch_detector$211,"def launch_detector(self):
        """""" Launch the face detector """"""
        logger.debug(""Launching Detector"")
        kwargs = {""in_queue"": self.queues[""extract_detect_in""],
                  ""out_queue"": self.queues[""extract_align_in""]}
        mp_func = PoolProcess if self.detector.parent_is_pool else SpawnProcess
        process = mp_func(self.detector.run, **kwargs)

        event = process.event if hasattr(process, ""event"") else None
        error = process.error if hasattr(process, ""error"") else None
        process.start()
        self.processes.append(process)

        if event is None:
            logger.debug(""Launched Detector"")
            return

        for mins in reversed(range(5)):
            for seconds in range(60):
                event.wait(seconds)
                if event.is_set():
                    break
                if error and error.is_set():
                    break
            if event.is_set():
                break
            if mins == 0 or (error and error.is_set()):
                raise ValueError(""Error initializing Detector"")
            logger.info(""Waiting for Detector... Time out in %s minutes"", mins)

        logger.debug(""Launched Detector"")",mins == 0,not mins
django-mysql,https://github.com/adamchainz/django-mysql/tree/master/tests/testapp/test_locks.py,LockTests,test_held_with_prefix$195,"def test_held_with_prefix(self):
        if not self.supports_lock_info:
            self.skipTest(
                ""Only MariaDB 10.0.7+ has the metadata_lock_info plugin on ""
                ""which held_with_prefix relies""
            )

        assert Lock.held_with_prefix("""") == {}
        assert Lock.held_with_prefix(""mylock"") == {}

        with Lock(""mylock-alpha"") as lock:
            assert Lock.held_with_prefix("""") == {
                ""mylock-alpha"": lock.holding_connection_id()
            }
            assert Lock.held_with_prefix(""mylock"") == {
                ""mylock-alpha"": lock.holding_connection_id()
            }
            assert Lock.held_with_prefix(""mylock-beta"") == {}

        assert Lock.held_with_prefix("""") == {}
        assert Lock.held_with_prefix(""mylock"") == {}",Lock.held_with_prefix('') == {},not Lock.held_with_prefix('')
django-mysql,https://github.com/adamchainz/django-mysql/tree/master/tests/testapp/test_locks.py,LockTests,test_held_with_prefix$195,"def test_held_with_prefix(self):
        if not self.supports_lock_info:
            self.skipTest(
                ""Only MariaDB 10.0.7+ has the metadata_lock_info plugin on ""
                ""which held_with_prefix relies""
            )

        assert Lock.held_with_prefix("""") == {}
        assert Lock.held_with_prefix(""mylock"") == {}

        with Lock(""mylock-alpha"") as lock:
            assert Lock.held_with_prefix("""") == {
                ""mylock-alpha"": lock.holding_connection_id()
            }
            assert Lock.held_with_prefix(""mylock"") == {
                ""mylock-alpha"": lock.holding_connection_id()
            }
            assert Lock.held_with_prefix(""mylock-beta"") == {}

        assert Lock.held_with_prefix("""") == {}
        assert Lock.held_with_prefix(""mylock"") == {}",Lock.held_with_prefix('mylock') == {},not Lock.held_with_prefix('mylock')
django-mysql,https://github.com/adamchainz/django-mysql/tree/master/tests/testapp/test_locks.py,LockTests,test_held_with_prefix$195,"def test_held_with_prefix(self):
        if not self.supports_lock_info:
            self.skipTest(
                ""Only MariaDB 10.0.7+ has the metadata_lock_info plugin on ""
                ""which held_with_prefix relies""
            )

        assert Lock.held_with_prefix("""") == {}
        assert Lock.held_with_prefix(""mylock"") == {}

        with Lock(""mylock-alpha"") as lock:
            assert Lock.held_with_prefix("""") == {
                ""mylock-alpha"": lock.holding_connection_id()
            }
            assert Lock.held_with_prefix(""mylock"") == {
                ""mylock-alpha"": lock.holding_connection_id()
            }
            assert Lock.held_with_prefix(""mylock-beta"") == {}

        assert Lock.held_with_prefix("""") == {}
        assert Lock.held_with_prefix(""mylock"") == {}",Lock.held_with_prefix('') == {},not Lock.held_with_prefix('')
django-mysql,https://github.com/adamchainz/django-mysql/tree/master/tests/testapp/test_locks.py,LockTests,test_held_with_prefix$195,"def test_held_with_prefix(self):
        if not self.supports_lock_info:
            self.skipTest(
                ""Only MariaDB 10.0.7+ has the metadata_lock_info plugin on ""
                ""which held_with_prefix relies""
            )

        assert Lock.held_with_prefix("""") == {}
        assert Lock.held_with_prefix(""mylock"") == {}

        with Lock(""mylock-alpha"") as lock:
            assert Lock.held_with_prefix("""") == {
                ""mylock-alpha"": lock.holding_connection_id()
            }
            assert Lock.held_with_prefix(""mylock"") == {
                ""mylock-alpha"": lock.holding_connection_id()
            }
            assert Lock.held_with_prefix(""mylock-beta"") == {}

        assert Lock.held_with_prefix("""") == {}
        assert Lock.held_with_prefix(""mylock"") == {}",Lock.held_with_prefix('mylock') == {},not Lock.held_with_prefix('mylock')
django-mysql,https://github.com/adamchainz/django-mysql/tree/master/tests/testapp/test_locks.py,LockTests,test_held_with_prefix$195,"def test_held_with_prefix(self):
        if not self.supports_lock_info:
            self.skipTest(
                ""Only MariaDB 10.0.7+ has the metadata_lock_info plugin on ""
                ""which held_with_prefix relies""
            )

        assert Lock.held_with_prefix("""") == {}
        assert Lock.held_with_prefix(""mylock"") == {}

        with Lock(""mylock-alpha"") as lock:
            assert Lock.held_with_prefix("""") == {
                ""mylock-alpha"": lock.holding_connection_id()
            }
            assert Lock.held_with_prefix(""mylock"") == {
                ""mylock-alpha"": lock.holding_connection_id()
            }
            assert Lock.held_with_prefix(""mylock-beta"") == {}

        assert Lock.held_with_prefix("""") == {}
        assert Lock.held_with_prefix(""mylock"") == {}",Lock.held_with_prefix('mylock-beta') == {},not Lock.held_with_prefix('mylock-beta')
mmcv,https://github.com/open-mmlab/mmcv/tree/master/mmcv/image/geometric.py,,bbox_clip$342,"def bbox_clip(bboxes, img_shape):
    """"""Clip bboxes to fit the image shape.

    Args:
        bboxes (ndarray): Shape (..., 4*k)
        img_shape (tuple[int]): (height, width) of the image.

    Returns:
        ndarray: Clipped bboxes.
    """"""
    assert bboxes.shape[-1] % 4 == 0
    cmin = np.empty(bboxes.shape[-1], dtype=bboxes.dtype)
    cmin[0::2] = img_shape[1] - 1
    cmin[1::2] = img_shape[0] - 1
    clipped_bboxes = np.maximum(np.minimum(bboxes, cmin), 0)
    return clipped_bboxes",bboxes.shape[-1] % 4 == 0,not bboxes.shape[-1] % 4
ICCV2019-LearningToPaint,https://github.com/megvii-research/ICCV2019-LearningToPaint/tree/master/baseline/DRL/ddpg.py,DDPG,evaluate$96,"def evaluate(self, state, action, target=False):
        T = state[:, 6 : 7]
        gt = state[:, 3 : 6].float() / 255
        canvas0 = state[:, :3].float() / 255
        canvas1 = decode(action, canvas0)
        gan_reward = cal_reward(canvas1, gt) - cal_reward(canvas0, gt)
        # L2_reward = ((canvas0 - gt) ** 2).mean(1).mean(1).mean(1) - ((canvas1 - gt) ** 2).mean(1).mean(1).mean(1)        
        coord_ = coord.expand(state.shape[0], 2, 128, 128)
        merged_state = torch.cat([canvas0, canvas1, gt, (T + 1).float() / self.max_step, coord_], 1)
        # canvas0 is not necessarily added
        if target:
            Q = self.critic_target(merged_state)
            return (Q + gan_reward), gan_reward
        else:
            Q = self.critic(merged_state)
            if self.log % 20 == 0:
                self.writer.add_scalar('train/expect_reward', Q.mean(), self.log)
                self.writer.add_scalar('train/gan_reward', gan_reward.mean(), self.log)
            return (Q + gan_reward), gan_reward",self.log % 20 == 0,not self.log % 20
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/tests/test_detection_metrics.py,TestBoxMatch,test_multi_label$113,"def test_multi_label(self):
        gt = make_representation(""1 0 0 5 5; 0 9 9 10 10"", is_ground_truth=True)
        pred = make_representation(""1 1 0 0 5 5; 0.8 0 7 7 8 8"")
        overlap_evaluator = IOU({})

        tp, fp = bbox_match(gt, pred, 1, overlap_evaluator)[:2]
        assert tp.shape[0] == 1
        assert tp[0] == 1
        assert fp[0] == 0

        tp, fp = bbox_match(gt, pred, 0, overlap_evaluator)[:2]
        assert tp.shape[0] == 1
        assert tp[0] == 0
        assert fp[0] == 1",fp[0] == 0,not fp[0]
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/tests/test_detection_metrics.py,TestBoxMatch,test_multi_label$113,"def test_multi_label(self):
        gt = make_representation(""1 0 0 5 5; 0 9 9 10 10"", is_ground_truth=True)
        pred = make_representation(""1 1 0 0 5 5; 0.8 0 7 7 8 8"")
        overlap_evaluator = IOU({})

        tp, fp = bbox_match(gt, pred, 1, overlap_evaluator)[:2]
        assert tp.shape[0] == 1
        assert tp[0] == 1
        assert fp[0] == 0

        tp, fp = bbox_match(gt, pred, 0, overlap_evaluator)[:2]
        assert tp.shape[0] == 1
        assert tp[0] == 0
        assert fp[0] == 1",tp[0] == 0,not tp[0]
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster/dagster_tests/cli_tests/command_tests/test_execute_command.py,,test_multiproc$351,"def test_multiproc():
    with instance_for_test():
        runner = CliRunner()
        add_result = runner_job_execute(
            runner,
            [
                ""-f"",
                file_relative_path(__file__, ""../../general_tests/test_repository.py""),
                ""-a"",
                ""dagster_test_repository"",
                ""--config"",
                file_relative_path(__file__, ""../../environments/adder_job.yaml""),
                ""-j"",
                ""multi_job"",  # job name
            ],
        )
        assert add_result.exit_code == 0

        assert ""RUN_SUCCESS"" in add_result.output

        add_result = runner_job_execute(
            runner,
            [
                ""-f"",
                file_relative_path(__file__, ""test_cli_commands.py""),
                ""-a"",
                ""multiproc"",
            ],
        )
        assert add_result.exit_code == 0

        assert ""RUN_SUCCESS"" in add_result.output",add_result.exit_code == 0,not add_result.exit_code
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster/dagster_tests/cli_tests/command_tests/test_execute_command.py,,test_multiproc$351,"def test_multiproc():
    with instance_for_test():
        runner = CliRunner()
        add_result = runner_job_execute(
            runner,
            [
                ""-f"",
                file_relative_path(__file__, ""../../general_tests/test_repository.py""),
                ""-a"",
                ""dagster_test_repository"",
                ""--config"",
                file_relative_path(__file__, ""../../environments/adder_job.yaml""),
                ""-j"",
                ""multi_job"",  # job name
            ],
        )
        assert add_result.exit_code == 0

        assert ""RUN_SUCCESS"" in add_result.output

        add_result = runner_job_execute(
            runner,
            [
                ""-f"",
                file_relative_path(__file__, ""test_cli_commands.py""),
                ""-a"",
                ""multiproc"",
            ],
        )
        assert add_result.exit_code == 0

        assert ""RUN_SUCCESS"" in add_result.output",add_result.exit_code == 0,not add_result.exit_code
fgmk,https://github.com/ericoporto/fgmk/tree/master/fgmk/game_init.py,,regenerateInit$65,"def regenerateInit():
    initFileJsonTree = getInitFile()
    needupdate = False

    levelJsonTree = regenerateLevelList(initFileJsonTree)
    if(levelJsonTree == None):
        #regress to before json tree
        levelJsonTree = initFileJsonTree
    else:
        needupdate = True

    musicJsonTree = regenerateMusicList(levelJsonTree)
    if(musicJsonTree == None):
        #regress to before json tree
        musicJsonTree = levelJsonTree
    else:
        needupdate = True

    soundJsonTree = regenerateSoundList(musicJsonTree)
    if(soundJsonTree == None):
        #regress to before json tree
        soundJsonTree = musicJsonTree
    else:
        needupdate = True

    pictureJsonTree = regeneratePictureList(soundJsonTree)
    if(pictureJsonTree == None):
        #regress to before json tree
        pictureJsonTree = soundJsonTree
    else:
        needupdate = True

    animJsonTree = regenerateAnimationList(pictureJsonTree)
    if(animJsonTree == None):
        #regress to before json tree
        animJsonTree = pictureJsonTree
    else:
        needupdate = True

    charasetJsonTree = regenerateCharasetFileList(animJsonTree)
    if(charasetJsonTree == None):
        #regress to before json tree
        charasetJsonTree = animJsonTree
    else:
        needupdate = True

    if(needupdate):
        gamefolder = os.path.join(current_project.settings[""gamefolder""])
        saveInitFile(gamefolder, charasetJsonTree)
        return True

    return False",levelJsonTree == None,not levelJsonTree
fgmk,https://github.com/ericoporto/fgmk/tree/master/fgmk/game_init.py,,regenerateInit$65,"def regenerateInit():
    initFileJsonTree = getInitFile()
    needupdate = False

    levelJsonTree = regenerateLevelList(initFileJsonTree)
    if(levelJsonTree == None):
        #regress to before json tree
        levelJsonTree = initFileJsonTree
    else:
        needupdate = True

    musicJsonTree = regenerateMusicList(levelJsonTree)
    if(musicJsonTree == None):
        #regress to before json tree
        musicJsonTree = levelJsonTree
    else:
        needupdate = True

    soundJsonTree = regenerateSoundList(musicJsonTree)
    if(soundJsonTree == None):
        #regress to before json tree
        soundJsonTree = musicJsonTree
    else:
        needupdate = True

    pictureJsonTree = regeneratePictureList(soundJsonTree)
    if(pictureJsonTree == None):
        #regress to before json tree
        pictureJsonTree = soundJsonTree
    else:
        needupdate = True

    animJsonTree = regenerateAnimationList(pictureJsonTree)
    if(animJsonTree == None):
        #regress to before json tree
        animJsonTree = pictureJsonTree
    else:
        needupdate = True

    charasetJsonTree = regenerateCharasetFileList(animJsonTree)
    if(charasetJsonTree == None):
        #regress to before json tree
        charasetJsonTree = animJsonTree
    else:
        needupdate = True

    if(needupdate):
        gamefolder = os.path.join(current_project.settings[""gamefolder""])
        saveInitFile(gamefolder, charasetJsonTree)
        return True

    return False",musicJsonTree == None,not musicJsonTree
fgmk,https://github.com/ericoporto/fgmk/tree/master/fgmk/game_init.py,,regenerateInit$65,"def regenerateInit():
    initFileJsonTree = getInitFile()
    needupdate = False

    levelJsonTree = regenerateLevelList(initFileJsonTree)
    if(levelJsonTree == None):
        #regress to before json tree
        levelJsonTree = initFileJsonTree
    else:
        needupdate = True

    musicJsonTree = regenerateMusicList(levelJsonTree)
    if(musicJsonTree == None):
        #regress to before json tree
        musicJsonTree = levelJsonTree
    else:
        needupdate = True

    soundJsonTree = regenerateSoundList(musicJsonTree)
    if(soundJsonTree == None):
        #regress to before json tree
        soundJsonTree = musicJsonTree
    else:
        needupdate = True

    pictureJsonTree = regeneratePictureList(soundJsonTree)
    if(pictureJsonTree == None):
        #regress to before json tree
        pictureJsonTree = soundJsonTree
    else:
        needupdate = True

    animJsonTree = regenerateAnimationList(pictureJsonTree)
    if(animJsonTree == None):
        #regress to before json tree
        animJsonTree = pictureJsonTree
    else:
        needupdate = True

    charasetJsonTree = regenerateCharasetFileList(animJsonTree)
    if(charasetJsonTree == None):
        #regress to before json tree
        charasetJsonTree = animJsonTree
    else:
        needupdate = True

    if(needupdate):
        gamefolder = os.path.join(current_project.settings[""gamefolder""])
        saveInitFile(gamefolder, charasetJsonTree)
        return True

    return False",soundJsonTree == None,not soundJsonTree
fgmk,https://github.com/ericoporto/fgmk/tree/master/fgmk/game_init.py,,regenerateInit$65,"def regenerateInit():
    initFileJsonTree = getInitFile()
    needupdate = False

    levelJsonTree = regenerateLevelList(initFileJsonTree)
    if(levelJsonTree == None):
        #regress to before json tree
        levelJsonTree = initFileJsonTree
    else:
        needupdate = True

    musicJsonTree = regenerateMusicList(levelJsonTree)
    if(musicJsonTree == None):
        #regress to before json tree
        musicJsonTree = levelJsonTree
    else:
        needupdate = True

    soundJsonTree = regenerateSoundList(musicJsonTree)
    if(soundJsonTree == None):
        #regress to before json tree
        soundJsonTree = musicJsonTree
    else:
        needupdate = True

    pictureJsonTree = regeneratePictureList(soundJsonTree)
    if(pictureJsonTree == None):
        #regress to before json tree
        pictureJsonTree = soundJsonTree
    else:
        needupdate = True

    animJsonTree = regenerateAnimationList(pictureJsonTree)
    if(animJsonTree == None):
        #regress to before json tree
        animJsonTree = pictureJsonTree
    else:
        needupdate = True

    charasetJsonTree = regenerateCharasetFileList(animJsonTree)
    if(charasetJsonTree == None):
        #regress to before json tree
        charasetJsonTree = animJsonTree
    else:
        needupdate = True

    if(needupdate):
        gamefolder = os.path.join(current_project.settings[""gamefolder""])
        saveInitFile(gamefolder, charasetJsonTree)
        return True

    return False",pictureJsonTree == None,not pictureJsonTree
fgmk,https://github.com/ericoporto/fgmk/tree/master/fgmk/game_init.py,,regenerateInit$65,"def regenerateInit():
    initFileJsonTree = getInitFile()
    needupdate = False

    levelJsonTree = regenerateLevelList(initFileJsonTree)
    if(levelJsonTree == None):
        #regress to before json tree
        levelJsonTree = initFileJsonTree
    else:
        needupdate = True

    musicJsonTree = regenerateMusicList(levelJsonTree)
    if(musicJsonTree == None):
        #regress to before json tree
        musicJsonTree = levelJsonTree
    else:
        needupdate = True

    soundJsonTree = regenerateSoundList(musicJsonTree)
    if(soundJsonTree == None):
        #regress to before json tree
        soundJsonTree = musicJsonTree
    else:
        needupdate = True

    pictureJsonTree = regeneratePictureList(soundJsonTree)
    if(pictureJsonTree == None):
        #regress to before json tree
        pictureJsonTree = soundJsonTree
    else:
        needupdate = True

    animJsonTree = regenerateAnimationList(pictureJsonTree)
    if(animJsonTree == None):
        #regress to before json tree
        animJsonTree = pictureJsonTree
    else:
        needupdate = True

    charasetJsonTree = regenerateCharasetFileList(animJsonTree)
    if(charasetJsonTree == None):
        #regress to before json tree
        charasetJsonTree = animJsonTree
    else:
        needupdate = True

    if(needupdate):
        gamefolder = os.path.join(current_project.settings[""gamefolder""])
        saveInitFile(gamefolder, charasetJsonTree)
        return True

    return False",animJsonTree == None,not animJsonTree
fgmk,https://github.com/ericoporto/fgmk/tree/master/fgmk/game_init.py,,regenerateInit$65,"def regenerateInit():
    initFileJsonTree = getInitFile()
    needupdate = False

    levelJsonTree = regenerateLevelList(initFileJsonTree)
    if(levelJsonTree == None):
        #regress to before json tree
        levelJsonTree = initFileJsonTree
    else:
        needupdate = True

    musicJsonTree = regenerateMusicList(levelJsonTree)
    if(musicJsonTree == None):
        #regress to before json tree
        musicJsonTree = levelJsonTree
    else:
        needupdate = True

    soundJsonTree = regenerateSoundList(musicJsonTree)
    if(soundJsonTree == None):
        #regress to before json tree
        soundJsonTree = musicJsonTree
    else:
        needupdate = True

    pictureJsonTree = regeneratePictureList(soundJsonTree)
    if(pictureJsonTree == None):
        #regress to before json tree
        pictureJsonTree = soundJsonTree
    else:
        needupdate = True

    animJsonTree = regenerateAnimationList(pictureJsonTree)
    if(animJsonTree == None):
        #regress to before json tree
        animJsonTree = pictureJsonTree
    else:
        needupdate = True

    charasetJsonTree = regenerateCharasetFileList(animJsonTree)
    if(charasetJsonTree == None):
        #regress to before json tree
        charasetJsonTree = animJsonTree
    else:
        needupdate = True

    if(needupdate):
        gamefolder = os.path.join(current_project.settings[""gamefolder""])
        saveInitFile(gamefolder, charasetJsonTree)
        return True

    return False",charasetJsonTree == None,not charasetJsonTree
fairscale,https://github.com/facebookresearch/fairscale/tree/master/benchmarks/experimental/experimental_async_approaches.py,,train$600,"def train(lm_dataloader, model, criterion, optimizer, vocab_size, args):
    model.train()
    from functools import reduce
    import operator

    num_params = reduce(operator.add, (reduce(operator.mul, x.size()) for x in model.parameters()))
    if model.group:
        total = torch.Tensor([num_params])
        if torch.cuda.is_available():
            total = total.cuda()
        torch.distributed.all_reduce(total, group=model.group)
        logging.info(
            f""training model, #prams = {num_params}, group: {model.group.rank()}, grank:""
            f"" {torch.distributed.get_rank()}, sizes {model.group.size()}""
        )
        torch.distributed.barrier()
        if model.group.rank() == 0:
            logging.info(f""total #prams = {total.item()}"")
    else:
        logging.info(f""training model, #prams = {num_params}"")
    vocab_size = 10000  # FIXME
    total_loss = 0.0
    start_time = time.time()
    word_counter = 0

    optimizer = optimizer(model, args)
    transform_and_log = AsyncDelegate(vocab_size)
    model.interleave(
        lm_dataloader, criterion, optimizer, transform_and_log, args.min_update_interval, args.spectrain or args.xpipe
    )
    if model.group.rank() == model.group.size() - 1:
        print(""Done with an epoch"")",model.group.rank() == 0,not model.group.rank()
XlsxWriter,https://github.com/jmcnamara/XlsxWriter/tree/master/xlsxwriter/worksheet.py,Worksheet,set_row$1818,"def set_row(self, row, height=None, cell_format=None, options=None):
        """"""
        Set the width, and other properties of a row.

        Args:
            row:         Row number (zero-indexed).
            height:      Row height. (optional).
            cell_format: Row cell_format. (optional).
            options:     Dict of options such as hidden, level and collapsed.

        Returns:
            0:  Success.
            -1: Row number is out of worksheet bounds.

        """"""
        if options is None:
            options = {}

        # Use minimum col in _check_dimensions().
        if self.dim_colmin is not None:
            min_col = self.dim_colmin
        else:
            min_col = 0

        # Check that row is valid.
        if self._check_dimensions(row, min_col):
            return -1

        if height is None:
            height = self.default_row_height

        # Set optional row values.
        hidden = options.get('hidden', False)
        collapsed = options.get('collapsed', False)
        level = options.get('level', 0)

        # If the height is 0 the row is hidden and the height is the default.
        if height == 0:
            hidden = 1
            height = self.default_row_height

        # Set the limits for the outline levels (0 <= x <= 7).
        if level < 0:
            level = 0
        if level > 7:
            level = 7

        if level > self.outline_row_level:
            self.outline_row_level = level

        # Store the row properties.
        self.set_rows[row] = [height, cell_format, hidden, level, collapsed]

        # Store the row change to allow optimizations.
        self.row_size_changed = True

        # Store the row sizes for use when calculating image vertices.
        self.row_sizes[row] = [height, hidden]

        return 0",height == 0,not height
urh,https://github.com/jopohl/urh/tree/master/tests/test_send_recv_dialog_gui.py,TestSendRecvDialog,test_continuous_send_dialog$216,"def test_continuous_send_dialog(self):
        self.add_signal_to_form(""esaver.complex16s"")
        self.__add_first_signal_to_generator()

        port = util.get_free_port()

        gframe = self.form.generator_tab_controller # type: GeneratorTabController
        for msg in gframe.table_model.protocol.messages:
            msg.pause = 5000

        expected = IQArray(None, np.float32, gframe.total_modulated_samples)
        expected = gframe.modulate_data(expected)
        current_index = Value(""L"", 0)
        buffer = Array(""f"", 4 * len(expected))
        ready = Value(""i"", 0)

        process = Process(target=receive, args=(port, current_index, len(expected), buffer, ready))
        process.daemon = True
        process.start()
        n = 0
        while ready.value == 0 and n < 50:  # ensure server is up
            time.sleep(0.1)
            n += 1

        self.assertTrue(ready.value)

        ContinuousModulator.BUFFER_SIZE_MB = 10

        continuous_send_dialog = self.__get_continuous_send_dialog()
        continuous_send_dialog.device.set_client_port(port)
        continuous_send_dialog.device_settings_widget.ui.spinBoxNRepeat.setValue(2)
        continuous_send_dialog.ui.btnStart.click()
        process.join(10)

        # CI sometimes swallows a sample
        self.assertGreaterEqual(current_index.value, len(expected) - 1)

        buffer = np.frombuffer(buffer.get_obj(), dtype=np.float32)
        buffer = buffer.reshape((len(buffer) // 2, 2))

        for i in range(len(expected)):
            self.assertEqual(buffer[i, 0], expected[i, 0], msg=str(i))
            self.assertEqual(buffer[i, 1], expected[i, 1], msg=str(i))

        continuous_send_dialog.ui.btnStop.click()
        continuous_send_dialog.ui.btnClear.click()
        QTest.qWait(1)

        continuous_send_dialog.close()",ready.value == 0,not ready.value
Invertible-Image-Rescaling,https://github.com/pkuxmq/Invertible-Image-Rescaling/tree/master/codes/models/IRNp_model.py,IRNpModel,optimize_parameters$155,"def optimize_parameters(self, step):
        # G
        for p in self.netD.parameters():
            p.requires_grad = False

        self.optimizer_G.zero_grad()

        self.input = self.real_H
        self.output = self.netG(x=self.input)

        loss = 0
        zshape = self.output[:, 3:, :, :].shape

        LR = self.Quantization(self.output[:, :3, :, :])

        gaussian_scale = self.train_opt['gaussian_scale'] if self.train_opt['gaussian_scale'] != None else 1
        y_ = torch.cat((LR, gaussian_scale * self.gaussian_batch(zshape)), dim=1)

        self.fake_H = self.netG(x=y_, rev=True)

        if step % self.D_update_ratio == 0 and step > self.D_init_iters:
            l_forw_fit = self.loss_forward(self.output, self.ref_L)
            l_back_rec, l_back_fea, l_back_gan = self.loss_backward(self.real_H, self.fake_H)

            loss += l_forw_fit + l_back_rec + l_back_fea + l_back_gan

            loss.backward()

            # gradient clipping
            if self.train_opt['gradient_clipping']:
                nn.utils.clip_grad_norm_(self.netG.parameters(), self.train_opt['gradient_clipping'])

            self.optimizer_G.step()

        # D
        for p in self.netD.parameters():
            p.requires_grad = True

        self.optimizer_D.zero_grad()
        l_d_total = 0
        pred_d_real = self.netD(self.real_H)
        pred_d_fake = self.netD(self.fake_H.detach())
        if self.opt['train']['gan_type'] == 'gan':
            l_d_real = self.cri_gan(pred_d_real, True)
            l_d_fake = self.cri_gan(pred_d_fake, False)
            l_d_total = l_d_real + l_d_fake
        elif self.opt['train']['gan_type'] == 'ragan':
            l_d_real = self.cri_gan(pred_d_real - torch.mean(pred_d_fake), True)
            l_d_fake = self.cri_gan(pred_d_fake - torch.mean(pred_d_real), False)
            l_d_total = (l_d_real + l_d_fake) / 2

        l_d_total.backward()
        self.optimizer_D.step()

        # set log
        if step % self.D_update_ratio == 0 and step > self.D_init_iters:
            self.log_dict['l_forw_fit'] = l_forw_fit.item()
            self.log_dict['l_back_rec'] = l_back_rec.item()
            self.log_dict['l_back_fea'] = l_back_fea.item()
            self.log_dict['l_back_gan'] = l_back_gan.item()
        self.log_dict['l_d'] = l_d_total.item()",step % self.D_update_ratio == 0,not step % self.D_update_ratio
Invertible-Image-Rescaling,https://github.com/pkuxmq/Invertible-Image-Rescaling/tree/master/codes/models/IRNp_model.py,IRNpModel,optimize_parameters$155,"def optimize_parameters(self, step):
        # G
        for p in self.netD.parameters():
            p.requires_grad = False

        self.optimizer_G.zero_grad()

        self.input = self.real_H
        self.output = self.netG(x=self.input)

        loss = 0
        zshape = self.output[:, 3:, :, :].shape

        LR = self.Quantization(self.output[:, :3, :, :])

        gaussian_scale = self.train_opt['gaussian_scale'] if self.train_opt['gaussian_scale'] != None else 1
        y_ = torch.cat((LR, gaussian_scale * self.gaussian_batch(zshape)), dim=1)

        self.fake_H = self.netG(x=y_, rev=True)

        if step % self.D_update_ratio == 0 and step > self.D_init_iters:
            l_forw_fit = self.loss_forward(self.output, self.ref_L)
            l_back_rec, l_back_fea, l_back_gan = self.loss_backward(self.real_H, self.fake_H)

            loss += l_forw_fit + l_back_rec + l_back_fea + l_back_gan

            loss.backward()

            # gradient clipping
            if self.train_opt['gradient_clipping']:
                nn.utils.clip_grad_norm_(self.netG.parameters(), self.train_opt['gradient_clipping'])

            self.optimizer_G.step()

        # D
        for p in self.netD.parameters():
            p.requires_grad = True

        self.optimizer_D.zero_grad()
        l_d_total = 0
        pred_d_real = self.netD(self.real_H)
        pred_d_fake = self.netD(self.fake_H.detach())
        if self.opt['train']['gan_type'] == 'gan':
            l_d_real = self.cri_gan(pred_d_real, True)
            l_d_fake = self.cri_gan(pred_d_fake, False)
            l_d_total = l_d_real + l_d_fake
        elif self.opt['train']['gan_type'] == 'ragan':
            l_d_real = self.cri_gan(pred_d_real - torch.mean(pred_d_fake), True)
            l_d_fake = self.cri_gan(pred_d_fake - torch.mean(pred_d_real), False)
            l_d_total = (l_d_real + l_d_fake) / 2

        l_d_total.backward()
        self.optimizer_D.step()

        # set log
        if step % self.D_update_ratio == 0 and step > self.D_init_iters:
            self.log_dict['l_forw_fit'] = l_forw_fit.item()
            self.log_dict['l_back_rec'] = l_back_rec.item()
            self.log_dict['l_back_fea'] = l_back_fea.item()
            self.log_dict['l_back_gan'] = l_back_gan.item()
        self.log_dict['l_d'] = l_d_total.item()",step % self.D_update_ratio == 0,not step % self.D_update_ratio
awx,https://github.com/ansible/awx/tree/master/awx/main/tests/unit/models/test_survey_models.py,SurveyVariableValidation,test_survey_answers_as_string$22,"def test_survey_answers_as_string(self, job_template_factory):
        objects = job_template_factory('job-template-with-survey', survey=[{'variable': 'var1', 'type': 'text'}], persisted=False)
        jt = objects.job_template
        user_extra_vars = json.dumps({'var1': 'asdf'})
        accepted, ignored, errors = jt._accept_or_ignore_job_kwargs(extra_vars=user_extra_vars)
        assert ignored.get('extra_vars', {}) == {}, [str(element) for element in errors]
        assert 'var1' in accepted['extra_vars']","ignored.get('extra_vars', {}) == {}","not ignored.get('extra_vars', {})"
Bluetooth_Headset_Battery_Level,https://github.com/TheWeirdDev/Bluetooth_Headset_Battery_Level/tree/master//bluetooth_battery.py,BatteryStateQuerier,_perform_query$95,"def _perform_query(self) -> int:
        """"""
        Will try to get and print the battery level of supported devices
        """"""
        result = None
        sock = RFCOMMSocket()
        sock.connect(self._bt_settings)
        # Iterate received packets until there is no more or a result was found
        for line in sock:
            if b""BRSF"" in line:
                sock.send(b""+BRSF: 1024"")
                sock.send(b""OK"")
            elif b""CIND="" in line:
                sock.send(b""+CIND: (\""battchg\"",(0-5))"")
                sock.send(b""OK"")
            elif b""CIND?"" in line:
                sock.send(b""+CIND: 5"")
                sock.send(b""OK"")
            elif b""BIND=?"" in line:
                # Announce that we support the battery level HF indicator
                # https://www.bluetooth.com/specifications/assigned-numbers/hands-free-profile/
                sock.send(b""+BIND: (2)"")
                sock.send(b""OK"")
            elif b""BIND?"" in line:
                # Enable battery level HF indicator
                sock.send(b""+BIND: 2,1"")
                sock.send(b""OK"")
            elif b""XAPL="" in line:
                sock.send(b""+XAPL=iPhone,7"")
                sock.send(b""OK"")
            elif b""IPHONEACCEV"" in line:
                parts = line.strip().split(b',')[1:]
                if len(parts) > 1 and (len(parts) % 2) == 0:
                    parts = iter(parts)
                    params = dict(zip(parts, parts))
                    if b'1' in params:
                        result = (int(params[b'1']) + 1) * 10
                        break
            elif b""BIEV="" in line:
                params = line.strip().split(b""="")[1].split(b"","")
                if params[0] == b""2"":
                    result = int(params[1])
                    break
            elif b""XEVENT=BATTERY"" in line:
                params = line.strip().split(b""="")[1].split(b"","")
                result = int(params[1]) / int(params[2]) * 100
                break
            else:
                sock.send(b""OK"")
        sock.close()
        # Check whether the result was found, otherwise raise an Error
        if result is None:
            raise BatteryQueryError(""Could not query the battery state."")
        return result",len(parts) % 2 == 0,not len(parts) % 2
MONAI,https://github.com/Project-MONAI/MONAI/tree/master/monai/networks/layers/convutils.py,,polyval$136,"def polyval(coef, x) -> torch.Tensor:
    """"""
    Evaluates the polynomial defined by `coef` at `x`.

    For a 1D sequence of coef (length n), evaluate::

        y = coef[n-1] + x * (coef[n-2] + ... + x * (coef[1] + x * coef[0]))

    Args:
        coef: a sequence of floats representing the coefficients of the polynomial
        x: float or a sequence of floats representing the variable of the polynomial

    Returns:
        1D torch tensor
    """"""
    device = x.device if isinstance(x, torch.Tensor) else None
    coef = torch.as_tensor(coef, dtype=torch.float, device=device)
    if coef.ndim == 0 or (len(coef) < 1):
        return torch.zeros(x.shape)
    x = torch.as_tensor(x, dtype=torch.float, device=device)
    ans = coef[0]
    for c in coef[1:]:
        ans = ans * x + c
    return ans",coef.ndim == 0,not coef.ndim
textflint,https://github.com/textflint/textflint/tree/master/test/common/utils/test_list_op.py,TestListOp,test_insert_after_index$240,"def test_insert_after_index(self):
        self.assertEqual(
            [1, 0, 2, 3, 4], insert_after_index([1, 2, 3, 4], 0, [0]))
        self.assertEqual(
            [1, 0, 2, 3, 4], insert_after_index([1, 2, 3, 4], 0, 0))
        self.assertEqual(
            [1, 2, 3, 4, 0, 1, 2],
            insert_after_index([1, 2, 3, 4], 3, [0, 1, 2]))
        self.assertEqual(
            [1, 2, 3, 4, 5], insert_after_index([1, 2, 3, 4], 3, [5]))

        # Test successive replacement of individual elements
        # TODO merge
        num = random.randint(200, 1000)
        n = random.randint(20, 99)
        pos = random.sample(list(range(1, num - 1)), n)
        pos += [0, num]
        n += 2
        pos.sort()

        if n % 2 == 0:
            pos = pos[:-1]
            n -= 1
            pos[n - 1] = num
        test_pos = [[pos[i * 2 + 1], pos[i * 2 + 2]] for i in range(int(n / 2))]
        test_list = []
        origin = list(range(num))

        for i in range(int(n / 2)):
            test_list += origin[pos[2 * i]:pos[2 * i + 1]]
        random.shuffle(test_pos)
        for i in range(len(test_pos) - 1):
            test_list = insert_after_index(
                test_list,
                test_list.index(test_pos[i][0] - 1),
                list(range(test_pos[i][0], test_pos[i][1])))
        self.assertEqual(
            origin,
            insert_after_index(
                test_list,
                test_list.index(test_pos[len(test_pos) - 1][0] - 1),
                list(range(test_pos[len(test_pos) - 1][0],
                           test_pos[len(test_pos) - 1][1]))))",n % 2 == 0,not n % 2
haxor-news,https://github.com/donnemartin/haxor-news/tree/master/tests/test_hacker_news_cli.py,HackerNewsCliTest,test_ask$41,"def test_ask(self, mock_hn_call):
        result = self.runner.invoke(self.hacker_news_cli.cli, ['ask'])
        mock_hn_call.assert_called_with(self.limit)
        assert result.exit_code == 0",result.exit_code == 0,not result.exit_code
enterprise_gateway,https://github.com/jupyter/enterprise_gateway/tree/master/enterprise_gateway/services/processproxies/yarn.py,YarnClusterProcessProxy,send_signal$281,"def send_signal(self, signum: int) -> bool | None:
        """"""Currently only support 0 as poll and other as kill.

        :param signum
        :return:
        """"""
        if signum == 0:
            return self.poll()
        elif signum == signal.SIGKILL:
            return self.kill()
        else:
            # Yarn api doesn't support the equivalent to interrupts, so take our chances
            # via a remote signal.  Note that this condition cannot check against the
            # signum value because altternate interrupt signals might be in play.
            return super().send_signal(signum)",signum == 0,not signum
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/frontend/paddlepaddle.py,,convert_interpolate$725,"def convert_interpolate(g, op, block):
    """"""Operator converter for interpolate.""""""

    def get_interpolate_mode(op):
        """"""Get parameters for interpolation methods.""""""

        interp_method = op.attr(""interp_method"")
        align_corners = op.attr(""align_corners"")
        align_mode = op.attr(""align_mode"")

        rounding_method = """"
        if interp_method == ""nearest"":
            interp_method = ""nearest_neighbor""
            coordinate_transformation_mode = ""asymmetric""
            rounding_method = ""floor""
        elif interp_method == ""bilinear"":
            interp_method = ""linear""
            if not align_corners and align_mode == 0:
                coordinate_transformation_mode = ""half_pixel""
            else:
                if align_corners:
                    coordinate_transformation_mode = ""align_corners""
                else:
                    coordinate_transformation_mode = ""asymmetric""
        elif interp_method == ""bicubic"":
            interp_method = ""cubic""
            if align_corners:
                coordinate_transformation_mode = ""align_corners""
            else:
                coordinate_transformation_mode = ""half_pixel""
        else:
            msg = ""interp_method {} is not supported for PaddlePaddle's interpolate""
            raise tvm.error.OpAttributeInvalid(msg.format(interp_method))
        return rounding_method, interp_method, coordinate_transformation_mode

    layout = op.attr(""data_layout"")
    out_h = op.attr(""out_h"")
    out_w = op.attr(""out_w"")
    scale = op.attr(""scale"")
    if not isinstance(scale, (list, tuple)):
        scale = [scale, scale]

    x = g.get_node(op.input(""X"")[0])
    x_shape = infer_shape(x)
    assert len(x_shape) == 4, ""Only 4D input tensor is supported for PaddlePaddle's interpolate""
    input_out_size = op.input(""OutSize"")
    input_size_tensor = op.input(""SizeTensor"")
    input_scale = op.input(""Scale"")
    rounding_method, interp_method, coordinate_transformation_mode = get_interpolate_mode(op)

    if input_size_tensor:
        # if out_size is a list of tensor
        out_size = list()
        for name in input_size_tensor:
            size = g.get_node(name)
            if len(infer_shape(size)) == 0:
                size = _op.reshape(size, [-1])
            out_size.append(size)
        out_size = _op.concatenate(out_size, axis=0)
        out_size, infered = try_infer_value(out_size, parameters=g.get_params())
        if infered:
            out_size = out_size.tolist()
    elif input_scale:
        # if out_size is not defined, but scale is defined
        input_scale = g.get_node(input_scale[0])
        input_shape = shape_of(x).astype(""float32"")
        if layout.startswith(""NC""):
            out_size = _op.strided_slice(input_shape, begin=[2], end=[4]) * input_scale
        else:
            out_size = _op.strided_slice(input_shape, begin=[1], end=[3]) * input_scale
        out_size = out_size.astype(""int32"")
        out_size, infered = try_infer_value(out_size, parameters=g.get_params())
        if infered:
            out_size = out_size.tolist()
    elif scale and scale[0] > 0 and scale[1] > 0:
        # use attribute scale
        input_shape = shape_of(x).astype(""float32"")
        input_scale = _expr.const(np.array([scale[0], scale[1]]).astype(""float32""))
        if layout.startswith(""NC""):
            out_size = _op.strided_slice(input_shape, begin=[2], end=[4]) * input_scale
        else:
            out_size = _op.strided_slice(input_shape, begin=[1], end=[3]) * input_scale
        out_size = out_size.astype(""int32"")
        out_size, infered = try_infer_value(out_size, parameters=g.get_params())
        if infered:
            out_size = out_size.tolist()
    elif input_out_size:
        # if out_size is a tensor
        out_size = g.get_node(input_out_size[0])
        out_size, infered = try_infer_value(out_size, parameters=g.get_params())
        if infered:
            out_size = out_size.tolist()
    else:
        # if out_size is a constant value
        out_size = [out_h, out_w]

    out = _op.image.resize2d(
        x,
        size=out_size,
        layout=layout,
        method=interp_method,
        coordinate_transformation_mode=coordinate_transformation_mode,
        rounding_method=rounding_method,
        cubic_alpha=-0.75,
    )
    g.add_node(op.output(""Out"")[0], out)",len(infer_shape(size)) == 0,not infer_shape(size)
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/frontend/paddlepaddle.py,,convert_interpolate$725,"def convert_interpolate(g, op, block):
    """"""Operator converter for interpolate.""""""

    def get_interpolate_mode(op):
        """"""Get parameters for interpolation methods.""""""

        interp_method = op.attr(""interp_method"")
        align_corners = op.attr(""align_corners"")
        align_mode = op.attr(""align_mode"")

        rounding_method = """"
        if interp_method == ""nearest"":
            interp_method = ""nearest_neighbor""
            coordinate_transformation_mode = ""asymmetric""
            rounding_method = ""floor""
        elif interp_method == ""bilinear"":
            interp_method = ""linear""
            if not align_corners and align_mode == 0:
                coordinate_transformation_mode = ""half_pixel""
            else:
                if align_corners:
                    coordinate_transformation_mode = ""align_corners""
                else:
                    coordinate_transformation_mode = ""asymmetric""
        elif interp_method == ""bicubic"":
            interp_method = ""cubic""
            if align_corners:
                coordinate_transformation_mode = ""align_corners""
            else:
                coordinate_transformation_mode = ""half_pixel""
        else:
            msg = ""interp_method {} is not supported for PaddlePaddle's interpolate""
            raise tvm.error.OpAttributeInvalid(msg.format(interp_method))
        return rounding_method, interp_method, coordinate_transformation_mode

    layout = op.attr(""data_layout"")
    out_h = op.attr(""out_h"")
    out_w = op.attr(""out_w"")
    scale = op.attr(""scale"")
    if not isinstance(scale, (list, tuple)):
        scale = [scale, scale]

    x = g.get_node(op.input(""X"")[0])
    x_shape = infer_shape(x)
    assert len(x_shape) == 4, ""Only 4D input tensor is supported for PaddlePaddle's interpolate""
    input_out_size = op.input(""OutSize"")
    input_size_tensor = op.input(""SizeTensor"")
    input_scale = op.input(""Scale"")
    rounding_method, interp_method, coordinate_transformation_mode = get_interpolate_mode(op)

    if input_size_tensor:
        # if out_size is a list of tensor
        out_size = list()
        for name in input_size_tensor:
            size = g.get_node(name)
            if len(infer_shape(size)) == 0:
                size = _op.reshape(size, [-1])
            out_size.append(size)
        out_size = _op.concatenate(out_size, axis=0)
        out_size, infered = try_infer_value(out_size, parameters=g.get_params())
        if infered:
            out_size = out_size.tolist()
    elif input_scale:
        # if out_size is not defined, but scale is defined
        input_scale = g.get_node(input_scale[0])
        input_shape = shape_of(x).astype(""float32"")
        if layout.startswith(""NC""):
            out_size = _op.strided_slice(input_shape, begin=[2], end=[4]) * input_scale
        else:
            out_size = _op.strided_slice(input_shape, begin=[1], end=[3]) * input_scale
        out_size = out_size.astype(""int32"")
        out_size, infered = try_infer_value(out_size, parameters=g.get_params())
        if infered:
            out_size = out_size.tolist()
    elif scale and scale[0] > 0 and scale[1] > 0:
        # use attribute scale
        input_shape = shape_of(x).astype(""float32"")
        input_scale = _expr.const(np.array([scale[0], scale[1]]).astype(""float32""))
        if layout.startswith(""NC""):
            out_size = _op.strided_slice(input_shape, begin=[2], end=[4]) * input_scale
        else:
            out_size = _op.strided_slice(input_shape, begin=[1], end=[3]) * input_scale
        out_size = out_size.astype(""int32"")
        out_size, infered = try_infer_value(out_size, parameters=g.get_params())
        if infered:
            out_size = out_size.tolist()
    elif input_out_size:
        # if out_size is a tensor
        out_size = g.get_node(input_out_size[0])
        out_size, infered = try_infer_value(out_size, parameters=g.get_params())
        if infered:
            out_size = out_size.tolist()
    else:
        # if out_size is a constant value
        out_size = [out_h, out_w]

    out = _op.image.resize2d(
        x,
        size=out_size,
        layout=layout,
        method=interp_method,
        coordinate_transformation_mode=coordinate_transformation_mode,
        rounding_method=rounding_method,
        cubic_alpha=-0.75,
    )
    g.add_node(op.output(""Out"")[0], out)",align_mode == 0,not align_mode
textdistance,https://github.com/life4/textdistance/tree/master/textdistance/algorithms/sequence_based.py,RatcliffObershelp,_find$151,"def _find(self, *sequences):
        subseq = LCSStr()(*sequences)
        length = len(subseq)
        if length == 0:
            return 0
        before = [s[:s.find(subseq)] for s in sequences]
        after = [s[s.find(subseq) + length:] for s in sequences]
        return self._find(*before) + length + self._find(*after)",length == 0,not length
model-optimization,https://github.com/tensorflow/model-optimization/tree/master/tensorflow_model_optimization/python/core/clustering/keras/cluster_distributed_test.py,ClusterDistributedTest,testAssociationValuesPerReplica$88,"def testAssociationValuesPerReplica(self, distribution):
    """"""Verifies that associations of weights are updated per replica.""""""
    assert tf.distribute.get_replica_context() is not None
    with distribution.scope():
      assert tf.distribute.get_replica_context() is None
      input_shape = (1, 2)
      output_shape = (2, 8)
      l = cluster_wrapper.ClusterWeights(
          keras.layers.Dense(8, input_shape=input_shape),
          number_of_clusters=2,
          cluster_centroids_init=CentroidInitialization.LINEAR
      )
      l.build(input_shape)

      clusterable_weights = l.layer.get_clusterable_weights()
      self.assertLen(clusterable_weights, 1)
      weights_name = clusterable_weights[0][0]
      self.assertEqual(weights_name, 'kernel')
      centroids1 = l.cluster_centroids[weights_name]

      mean_weight = tf.reduce_mean(l.layer.kernel)
      min_weight = tf.reduce_min(l.layer.kernel)
      max_weight = tf.reduce_max(l.layer.kernel)
      max_dist = max_weight - min_weight

      def assert_all_cluster_indices(per_replica, indices_val):
        if indices_val == 1:
          val_tensor = tf.dtypes.cast(
              tf.ones(shape=output_shape), per_replica[0].dtype)
        if indices_val == 0:
          val_tensor = tf.dtypes.cast(
              tf.zeros(shape=output_shape), per_replica[0].dtype)
        for i in range(0, len(per_replica)):
          all_equal = tf.reduce_all(tf.equal(per_replica[i], val_tensor))
          self.assertTrue(all_equal)

      def update_fn(v, val):
        return v.assign(val)

      initial_val = tf.Variable([mean_weight, mean_weight + 2.0 * max_dist],
                                aggregation=tf.VariableAggregation.MEAN)

      centroids1 = distribution.extended.update(
          centroids1, update_fn, args=(initial_val,))
      l.call(tf.ones(shape=input_shape))

      clst_indices = l.pulling_indices[weights_name]
      per_replica = distribution.experimental_local_results(clst_indices)
      assert_all_cluster_indices(per_replica, 0)

      second_val = tf.Variable([mean_weight - 2.0 * max_dist, mean_weight],
                               aggregation=tf.VariableAggregation.MEAN)
      centroids2 = l.cluster_centroids[weights_name]
      centroids2 = distribution.extended.update(
          centroids2, update_fn, args=(second_val,))
      l.call(tf.ones(shape=input_shape))

      clst_indices = l.pulling_indices[weights_name]
      per_replica = distribution.experimental_local_results(clst_indices)
      assert_all_cluster_indices(per_replica, 1)",indices_val == 0,not indices_val
PaddleX,https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/evaluate/detection.py,DetEvaluator,cal_ap$595,"def cal_ap(self):
        '''璁＄畻鍚勭被AP銆
        '''
        self.aps = dict()
        for id, ap in enumerate(self.APs):
            if id == 0:
                continue
            self.aps[self.cid2cname[id]] = ap
        return self.aps",id == 0,not id
saleor,https://github.com/saleor/saleor/tree/master/saleor/graphql/warehouse/tests/test_warehouse.py,,test_query_warehouses_with_filters_name$417,"def test_query_warehouses_with_filters_name(
    staff_api_client, permission_manage_products, warehouse
):
    variables_exists = {""filters"": {""search"": ""warehouse""}}
    response = staff_api_client.post_graphql(
        QUERY_WAREHOUSES_WITH_FILTERS,
        variables=variables_exists,
        permissions=[permission_manage_products],
    )
    content = get_graphql_content(response)
    warehouse_id = graphene.Node.to_global_id(""Warehouse"", warehouse.id)
    content_warehouse = content[""data""][""warehouses""][""edges""][0][""node""]
    assert content_warehouse[""id""] == warehouse_id
    variables_does_not_exists = {""filters"": {""search"": ""Absolutelywrong name""}}
    response1 = staff_api_client.post_graphql(
        QUERY_WAREHOUSES_WITH_FILTERS, variables=variables_does_not_exists
    )
    content1 = get_graphql_content(response1)
    total_count = content1[""data""][""warehouses""][""totalCount""]
    assert total_count == 0",total_count == 0,not total_count
pydicom,https://github.com/pydicom/pydicom/tree/master/pydicom/tests/test_encoders_pydicom.py,TestRLEEncodeFrame,test_big_endian_arr$439,"def test_big_endian_arr(self):
        """"""Test using a big endian array works.""""""
        ds = dcmread(EXPL_16_3_1F)
        ref = ds.pixel_array
        assert ds.BitsAllocated == 16
        assert ds.SamplesPerPixel == 3
        assert ds.PixelRepresentation == 0

        arr = ref.newbyteorder('>')
        assert id(arr) != id(ref)
        assert arr.dtype == '>u2'
        encoded = rle_encode_frame(arr)
        decoded = _rle_decode_frame(
            encoded, ds.Rows, ds.Columns, ds.SamplesPerPixel, ds.BitsAllocated
        )
        ds.PlanarConfiguration = 1
        arr = np.frombuffer(decoded, '<u2')
        arr = reshape_pixel_array(ds, arr)

        assert np.array_equal(ref, arr)",ds.PixelRepresentation == 0,not ds.PixelRepresentation
seirsplus,https://github.com/ryansmcgee/seirsplus/tree/master/build/lib/seirsplus/models.py,SEIRSNetworkModel,run$1311,"def run(self, T, checkpoints=None, print_interval=10, verbose='t'):
        if(T>0):
            self.tmax += T
        else:
            return False

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # Pre-process checkpoint values:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        if(checkpoints):
            numCheckpoints = len(checkpoints['t'])
            for chkpt_param, chkpt_values in checkpoints.items():
                assert(isinstance(chkpt_values, (list, numpy.ndarray)) and len(chkpt_values)==numCheckpoints), ""Expecting a list of values with length equal to number of checkpoint times (""+str(numCheckpoints)+"") for each checkpoint parameter.""
            checkpointIdx  = numpy.searchsorted(checkpoints['t'], self.t) # Finds 1st index in list greater than given val
            if(checkpointIdx >= numCheckpoints):
                # We are out of checkpoints, stop checking them:
                checkpoints = None 
            else:
                checkpointTime = checkpoints['t'][checkpointIdx]

        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        # Run the simulation loop:
        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        print_reset = True
        running     = True
        while running:

            running = self.run_iteration()

            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            # Handle checkpoints if applicable:
            if(checkpoints):
                if(self.t >= checkpointTime):
                    if(verbose is not False):
                        print(""[Checkpoint: Updating parameters]"")
                    # A checkpoint has been reached, update param values:
                    for param in list(self.parameters.keys()):
                        if(param in list(checkpoints.keys())):
                            self.parameters.update({param: checkpoints[param][checkpointIdx]})
                    # Update parameter data structures and scenario flags:
                    self.update_parameters()
                    # Update the next checkpoint time:
                    checkpointIdx  = numpy.searchsorted(checkpoints['t'], self.t) # Finds 1st index in list greater than given val
                    if(checkpointIdx >= numCheckpoints):
                        # We are out of checkpoints, stop checking them:
                        checkpoints = None 
                    else:
                        checkpointTime = checkpoints['t'][checkpointIdx]
            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

            if(print_interval):
                if(print_reset and (int(self.t) % print_interval == 0)):
                    if(verbose==""t""):
                        print(""t = %.2f"" % self.t)
                    if(verbose==True):
                        print(""t = %.2f"" % self.t)
                        print(""\t S      = "" + str(self.numS[self.tidx]))
                        print(""\t E      = "" + str(self.numE[self.tidx]))
                        print(""\t I  = "" + str(self.numI[self.tidx]))
                        print(""\t R      = "" + str(self.numR[self.tidx]))
                        print(""\t F      = "" + str(self.numF[self.tidx]))
                        print(""\t Q_E    = "" + str(self.numQ_E[self.tidx]))
                        print(""\t Q_I  = "" + str(self.numQ_I[self.tidx]))
                    print_reset = False
                elif(not print_reset and (int(self.t) % 10 != 0)):
                    print_reset = True

        return True",int(self.t) % print_interval == 0,not int(self.t) % print_interval
seirsplus,https://github.com/ryansmcgee/seirsplus/tree/master/build/lib/seirsplus/models.py,SEIRSNetworkModel,run$1311,"def run(self, T, checkpoints=None, print_interval=10, verbose='t'):
        if(T>0):
            self.tmax += T
        else:
            return False

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # Pre-process checkpoint values:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        if(checkpoints):
            numCheckpoints = len(checkpoints['t'])
            for chkpt_param, chkpt_values in checkpoints.items():
                assert(isinstance(chkpt_values, (list, numpy.ndarray)) and len(chkpt_values)==numCheckpoints), ""Expecting a list of values with length equal to number of checkpoint times (""+str(numCheckpoints)+"") for each checkpoint parameter.""
            checkpointIdx  = numpy.searchsorted(checkpoints['t'], self.t) # Finds 1st index in list greater than given val
            if(checkpointIdx >= numCheckpoints):
                # We are out of checkpoints, stop checking them:
                checkpoints = None 
            else:
                checkpointTime = checkpoints['t'][checkpointIdx]

        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        # Run the simulation loop:
        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        print_reset = True
        running     = True
        while running:

            running = self.run_iteration()

            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            # Handle checkpoints if applicable:
            if(checkpoints):
                if(self.t >= checkpointTime):
                    if(verbose is not False):
                        print(""[Checkpoint: Updating parameters]"")
                    # A checkpoint has been reached, update param values:
                    for param in list(self.parameters.keys()):
                        if(param in list(checkpoints.keys())):
                            self.parameters.update({param: checkpoints[param][checkpointIdx]})
                    # Update parameter data structures and scenario flags:
                    self.update_parameters()
                    # Update the next checkpoint time:
                    checkpointIdx  = numpy.searchsorted(checkpoints['t'], self.t) # Finds 1st index in list greater than given val
                    if(checkpointIdx >= numCheckpoints):
                        # We are out of checkpoints, stop checking them:
                        checkpoints = None 
                    else:
                        checkpointTime = checkpoints['t'][checkpointIdx]
            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

            if(print_interval):
                if(print_reset and (int(self.t) % print_interval == 0)):
                    if(verbose==""t""):
                        print(""t = %.2f"" % self.t)
                    if(verbose==True):
                        print(""t = %.2f"" % self.t)
                        print(""\t S      = "" + str(self.numS[self.tidx]))
                        print(""\t E      = "" + str(self.numE[self.tidx]))
                        print(""\t I  = "" + str(self.numI[self.tidx]))
                        print(""\t R      = "" + str(self.numR[self.tidx]))
                        print(""\t F      = "" + str(self.numF[self.tidx]))
                        print(""\t Q_E    = "" + str(self.numQ_E[self.tidx]))
                        print(""\t Q_I  = "" + str(self.numQ_I[self.tidx]))
                    print_reset = False
                elif(not print_reset and (int(self.t) % 10 != 0)):
                    print_reset = True

        return True",verbose == True,verbose
seirsplus,https://github.com/ryansmcgee/seirsplus/tree/master/build/lib/seirsplus/models.py,SEIRSNetworkModel,run$1311,"def run(self, T, checkpoints=None, print_interval=10, verbose='t'):
        if(T>0):
            self.tmax += T
        else:
            return False

        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # Pre-process checkpoint values:
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        if(checkpoints):
            numCheckpoints = len(checkpoints['t'])
            for chkpt_param, chkpt_values in checkpoints.items():
                assert(isinstance(chkpt_values, (list, numpy.ndarray)) and len(chkpt_values)==numCheckpoints), ""Expecting a list of values with length equal to number of checkpoint times (""+str(numCheckpoints)+"") for each checkpoint parameter.""
            checkpointIdx  = numpy.searchsorted(checkpoints['t'], self.t) # Finds 1st index in list greater than given val
            if(checkpointIdx >= numCheckpoints):
                # We are out of checkpoints, stop checking them:
                checkpoints = None 
            else:
                checkpointTime = checkpoints['t'][checkpointIdx]

        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        # Run the simulation loop:
        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        print_reset = True
        running     = True
        while running:

            running = self.run_iteration()

            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            # Handle checkpoints if applicable:
            if(checkpoints):
                if(self.t >= checkpointTime):
                    if(verbose is not False):
                        print(""[Checkpoint: Updating parameters]"")
                    # A checkpoint has been reached, update param values:
                    for param in list(self.parameters.keys()):
                        if(param in list(checkpoints.keys())):
                            self.parameters.update({param: checkpoints[param][checkpointIdx]})
                    # Update parameter data structures and scenario flags:
                    self.update_parameters()
                    # Update the next checkpoint time:
                    checkpointIdx  = numpy.searchsorted(checkpoints['t'], self.t) # Finds 1st index in list greater than given val
                    if(checkpointIdx >= numCheckpoints):
                        # We are out of checkpoints, stop checking them:
                        checkpoints = None 
                    else:
                        checkpointTime = checkpoints['t'][checkpointIdx]
            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

            if(print_interval):
                if(print_reset and (int(self.t) % print_interval == 0)):
                    if(verbose==""t""):
                        print(""t = %.2f"" % self.t)
                    if(verbose==True):
                        print(""t = %.2f"" % self.t)
                        print(""\t S      = "" + str(self.numS[self.tidx]))
                        print(""\t E      = "" + str(self.numE[self.tidx]))
                        print(""\t I  = "" + str(self.numI[self.tidx]))
                        print(""\t R      = "" + str(self.numR[self.tidx]))
                        print(""\t F      = "" + str(self.numF[self.tidx]))
                        print(""\t Q_E    = "" + str(self.numQ_E[self.tidx]))
                        print(""\t Q_I  = "" + str(self.numQ_I[self.tidx]))
                    print_reset = False
                elif(not print_reset and (int(self.t) % 10 != 0)):
                    print_reset = True

        return True",int(self.t) % 10 != 0,int(self.t) % 10
ansible-modules-extras,https://github.com/ansible/ansible-modules-extras/tree/master/storage/netapp/netapp_e_storagepool.py,NetAppESeriesStoragePool,get_expansion_candidate_drives$642,"def get_expansion_candidate_drives(self):
        # sanity checks; don't call this if we can't/don't need to expand
        if not self.needs_expansion:
            self.module.fail_json(msg=""can't get expansion candidates when pool doesn't need expansion"")

        self.debug(""fetching expansion candidate drives..."")
        try:
            (rc, resp) = request(
                self.api_url + ""/storage-systems/%s/storage-pools/%s/expand"" % (self.ssid,
                                                                                self.pool_detail['id']),
                method='GET', url_username=self.api_usr, url_password=self.api_pwd, validate_certs=self.validate_certs,
                timeout=120)
        except:
            err = get_exception()
            pool_id = self.pool_detail['id']
            self.module.exit_json(
                msg=""Failed to fetch candidate drives for storage pool. Pool id [%s]. Array id [%s].  Error[%s]."" % (
                    pool_id, self.ssid, str(err)))

        current_drive_count = len(self.sp_drives)
        current_capacity_bytes = int(self.pool_detail['totalRaidedSpace'])  # TODO: is this the right attribute to use?

        if self.criteria_min_usable_capacity:
            requested_capacity_bytes = self.criteria_min_usable_capacity * self._size_unit_map[self.criteria_size_unit]
        else:
            requested_capacity_bytes = current_capacity_bytes

        if self.criteria_drive_count:
            minimum_disks_to_add = max((self.criteria_drive_count - current_drive_count), 1)
        else:
            minimum_disks_to_add = 1

        minimum_bytes_to_add = max(requested_capacity_bytes - current_capacity_bytes, 0)

        # FUTURE: allow more control over expansion candidate selection?
        # loop over candidate disk sets and add until we've met both criteria

        added_drive_count = 0
        added_capacity_bytes = 0

        drives_to_add = set()

        for s in resp:
            # don't trust the API not to give us duplicate drives across candidate sets, especially in multi-drive sets
            candidate_drives = s['drives']
            if len(drives_to_add.intersection(candidate_drives)) != 0:
                # duplicate, skip
                continue
            drives_to_add.update(candidate_drives)
            added_drive_count += len(candidate_drives)
            added_capacity_bytes += int(s['usableCapacity'])

            if added_drive_count >= minimum_disks_to_add and added_capacity_bytes >= minimum_bytes_to_add:
                break

        if (added_drive_count < minimum_disks_to_add) or (added_capacity_bytes < minimum_bytes_to_add):
            self.module.fail_json(
                msg=""unable to find at least %s drives to add that would add at least %s bytes of capacity"" % (
                    minimum_disks_to_add, minimum_bytes_to_add))

        return list(drives_to_add)",len(drives_to_add.intersection(candidate_drives)) != 0,drives_to_add.intersection(candidate_drives)
transformers,https://github.com/huggingface/transformers/tree/master/src/transformers/models/ibert/quant_modules.py,,get_percentile_min_max$530,"def get_percentile_min_max(input, lower_percentile, upper_percentile, output_tensor=False):
    """"""
    Calculate the percentile max and min values in a given tensor

    Args:
        input (`torch.Tensor`):
            The target tensor to calculate percentile max and min.
        lower_percentile (`float`):
            If 0.1, means we return the value of the smallest 0.1% value in the tensor as percentile min.
        upper_percentile (`float`):
            If 99.9, means we return the value of the largest 0.1% value in the tensor as percentile max.
        output_tensor (`bool`, *optional*, defaults to `False`):
            If True, this function returns tensors, otherwise it returns values.

    Returns:
        `Tuple(torch.Tensor, torch.Tensor)`: Percentile min and max value of *input*
    """"""
    input_length = input.shape[0]

    lower_index = round(input_length * (1 - lower_percentile * 0.01))
    upper_index = round(input_length * upper_percentile * 0.01)

    upper_bound = torch.kthvalue(input, k=upper_index).values

    if lower_percentile == 0:
        lower_bound = upper_bound * 0
        # lower_index += 1
    else:
        lower_bound = -torch.kthvalue(-input, k=lower_index).values

    if not output_tensor:
        lower_bound = lower_bound.item()
        upper_bound = upper_bound.item()
    return lower_bound, upper_bound",lower_percentile == 0,not lower_percentile
sympy,https://github.com/sympy/sympy/tree/master/sympy/printing/latex.py,,multiline_latex$3015,"def multiline_latex(lhs, rhs, terms_per_line=1, environment=""align*"", use_dots=False, **settings):
    r""""""
    This function generates a LaTeX equation with a multiline right-hand side
    in an ``align*``, ``eqnarray`` or ``IEEEeqnarray`` environment.

    Parameters
    ==========

    lhs : Expr
        Left-hand side of equation

    rhs : Expr
        Right-hand side of equation

    terms_per_line : integer, optional
        Number of terms per line to print. Default is 1.

    environment : ""string"", optional
        Which LaTeX wnvironment to use for the output. Options are ""align*""
        (default), ""eqnarray"", and ""IEEEeqnarray"".

    use_dots : boolean, optional
        If ``True``, ``\\dots`` is added to the end of each line. Default is ``False``.

    Examples
    ========

    >>> from sympy import multiline_latex, symbols, sin, cos, exp, log, I
    >>> x, y, alpha = symbols('x y alpha')
    >>> expr = sin(alpha*y) + exp(I*alpha) - cos(log(y))
    >>> print(multiline_latex(x, expr))
    \begin{align*}
    x = & e^{i \alpha} \\
    & + \sin{\left(\alpha y \right)} \\
    & - \cos{\left(\log{\left(y \right)} \right)}
    \end{align*}

    Using at most two terms per line:
    >>> print(multiline_latex(x, expr, 2))
    \begin{align*}
    x = & e^{i \alpha} + \sin{\left(\alpha y \right)} \\
    & - \cos{\left(\log{\left(y \right)} \right)}
    \end{align*}

    Using ``eqnarray`` and dots:
    >>> print(multiline_latex(x, expr, terms_per_line=2, environment=""eqnarray"", use_dots=True))
    \begin{eqnarray}
    x & = & e^{i \alpha} + \sin{\left(\alpha y \right)} \dots\nonumber\\
    & & - \cos{\left(\log{\left(y \right)} \right)}
    \end{eqnarray}

    Using ``IEEEeqnarray``:
    >>> print(multiline_latex(x, expr, environment=""IEEEeqnarray""))
    \begin{IEEEeqnarray}{rCl}
    x & = & e^{i \alpha} \nonumber\\
    & & + \sin{\left(\alpha y \right)} \nonumber\\
    & & - \cos{\left(\log{\left(y \right)} \right)}
    \end{IEEEeqnarray}

    Notes
    =====

    All optional parameters from ``latex`` can also be used.

    """"""

    # Based on code from https://github.com/sympy/sympy/issues/3001
    l = LatexPrinter(**settings)
    if environment == ""eqnarray"":
        result = r'\begin{eqnarray}' + '\n'
        first_term = '& = &'
        nonumber = r'\nonumber'
        end_term = '\n\\end{eqnarray}'
        doubleet = True
    elif environment == ""IEEEeqnarray"":
        result = r'\begin{IEEEeqnarray}{rCl}' + '\n'
        first_term = '& = &'
        nonumber = r'\nonumber'
        end_term = '\n\\end{IEEEeqnarray}'
        doubleet = True
    elif environment == ""align*"":
        result = r'\begin{align*}' + '\n'
        first_term = '= &'
        nonumber = ''
        end_term =  '\n\\end{align*}'
        doubleet = False
    else:
        raise ValueError(""Unknown environment: {}"".format(environment))
    dots = ''
    if use_dots:
        dots=r'\dots'
    terms = rhs.as_ordered_terms()
    n_terms = len(terms)
    term_count = 1
    for i in range(n_terms):
        term = terms[i]
        term_start = ''
        term_end = ''
        sign = '+'
        if term_count > terms_per_line:
            if doubleet:
                term_start = '& & '
            else:
                term_start = '& '
            term_count = 1
        if term_count == terms_per_line:
            # End of line
            if i < n_terms-1:
                # There are terms remaining
                term_end = dots + nonumber + r'\\' + '\n'
            else:
                term_end = ''

        if term.as_ordered_factors()[0] == -1:
            term = -1*term
            sign = r'-'
        if i == 0: # beginning
            if sign == '+':
                sign = ''
            result += r'{:s} {:s}{:s} {:s} {:s}'.format(l.doprint(lhs),
                        first_term, sign, l.doprint(term), term_end)
        else:
            result += r'{:s}{:s} {:s} {:s}'.format(term_start, sign,
                        l.doprint(term), term_end)
        term_count += 1
    result += end_term
    return result",i == 0,not i
horovod,https://github.com/horovod/horovod/tree/master/examples/tensorflow2/tensorflow2_mnist.py,,main$23,"def main():
    # Horovod: initialize Horovod.
    hvd.init()

    # Horovod: pin GPU to be used to process local rank (one GPU per process)
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    if gpus:
        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')

    (mnist_images, mnist_labels), _ = \
        tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())

    dataset = tf.data.Dataset.from_tensor_slices(
        (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),
                 tf.cast(mnist_labels, tf.int64))
    )
    dataset = dataset.repeat().shuffle(10000).batch(128)

    mnist_model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),
        tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Dropout(0.25),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    loss = tf.losses.SparseCategoricalCrossentropy()

    # Horovod: adjust learning rate based on number of GPUs.
    opt = tf.optimizers.Adam(0.001 * hvd.size())

    checkpoint_dir = './checkpoints'
    checkpoint = tf.train.Checkpoint(model=mnist_model, optimizer=opt)

    @tf.function
    def training_step(images, labels, first_batch):
        with tf.GradientTape() as tape:
            probs = mnist_model(images, training=True)
            loss_value = loss(labels, probs)

        # Horovod: add Horovod Distributed GradientTape.
        tape = hvd.DistributedGradientTape(tape)

        grads = tape.gradient(loss_value, mnist_model.trainable_variables)
        opt.apply_gradients(zip(grads, mnist_model.trainable_variables))

        # Horovod: broadcast initial variable states from rank 0 to all other processes.
        # This is necessary to ensure consistent initialization of all workers when
        # training is started with random weights or restored from a checkpoint.
        #
        # Note: broadcast should be done after the first gradient step to ensure optimizer
        # initialization.
        if first_batch:
            hvd.broadcast_variables(mnist_model.variables, root_rank=0)
            hvd.broadcast_variables(opt.variables(), root_rank=0)

        return loss_value

    # Horovod: adjust number of steps based on number of GPUs.
    for batch, (images, labels) in enumerate(dataset.take(10000 // hvd.size())):
        loss_value = training_step(images, labels, batch == 0)

        if batch % 10 == 0 and hvd.rank() == 0:
            print('Step #%d\tLoss: %.6f' % (batch, loss_value))

    # Horovod: save checkpoints only on worker 0 to prevent other workers from
    # corrupting it.
    if hvd.rank() == 0:
        checkpoint.save(checkpoint_dir)",hvd.rank() == 0,not hvd.rank()
horovod,https://github.com/horovod/horovod/tree/master/examples/tensorflow2/tensorflow2_mnist.py,,main$23,"def main():
    # Horovod: initialize Horovod.
    hvd.init()

    # Horovod: pin GPU to be used to process local rank (one GPU per process)
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    if gpus:
        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')

    (mnist_images, mnist_labels), _ = \
        tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())

    dataset = tf.data.Dataset.from_tensor_slices(
        (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),
                 tf.cast(mnist_labels, tf.int64))
    )
    dataset = dataset.repeat().shuffle(10000).batch(128)

    mnist_model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),
        tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Dropout(0.25),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    loss = tf.losses.SparseCategoricalCrossentropy()

    # Horovod: adjust learning rate based on number of GPUs.
    opt = tf.optimizers.Adam(0.001 * hvd.size())

    checkpoint_dir = './checkpoints'
    checkpoint = tf.train.Checkpoint(model=mnist_model, optimizer=opt)

    @tf.function
    def training_step(images, labels, first_batch):
        with tf.GradientTape() as tape:
            probs = mnist_model(images, training=True)
            loss_value = loss(labels, probs)

        # Horovod: add Horovod Distributed GradientTape.
        tape = hvd.DistributedGradientTape(tape)

        grads = tape.gradient(loss_value, mnist_model.trainable_variables)
        opt.apply_gradients(zip(grads, mnist_model.trainable_variables))

        # Horovod: broadcast initial variable states from rank 0 to all other processes.
        # This is necessary to ensure consistent initialization of all workers when
        # training is started with random weights or restored from a checkpoint.
        #
        # Note: broadcast should be done after the first gradient step to ensure optimizer
        # initialization.
        if first_batch:
            hvd.broadcast_variables(mnist_model.variables, root_rank=0)
            hvd.broadcast_variables(opt.variables(), root_rank=0)

        return loss_value

    # Horovod: adjust number of steps based on number of GPUs.
    for batch, (images, labels) in enumerate(dataset.take(10000 // hvd.size())):
        loss_value = training_step(images, labels, batch == 0)

        if batch % 10 == 0 and hvd.rank() == 0:
            print('Step #%d\tLoss: %.6f' % (batch, loss_value))

    # Horovod: save checkpoints only on worker 0 to prevent other workers from
    # corrupting it.
    if hvd.rank() == 0:
        checkpoint.save(checkpoint_dir)",batch % 10 == 0,not batch % 10
horovod,https://github.com/horovod/horovod/tree/master/examples/tensorflow2/tensorflow2_mnist.py,,main$23,"def main():
    # Horovod: initialize Horovod.
    hvd.init()

    # Horovod: pin GPU to be used to process local rank (one GPU per process)
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    if gpus:
        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')

    (mnist_images, mnist_labels), _ = \
        tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())

    dataset = tf.data.Dataset.from_tensor_slices(
        (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),
                 tf.cast(mnist_labels, tf.int64))
    )
    dataset = dataset.repeat().shuffle(10000).batch(128)

    mnist_model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),
        tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Dropout(0.25),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    loss = tf.losses.SparseCategoricalCrossentropy()

    # Horovod: adjust learning rate based on number of GPUs.
    opt = tf.optimizers.Adam(0.001 * hvd.size())

    checkpoint_dir = './checkpoints'
    checkpoint = tf.train.Checkpoint(model=mnist_model, optimizer=opt)

    @tf.function
    def training_step(images, labels, first_batch):
        with tf.GradientTape() as tape:
            probs = mnist_model(images, training=True)
            loss_value = loss(labels, probs)

        # Horovod: add Horovod Distributed GradientTape.
        tape = hvd.DistributedGradientTape(tape)

        grads = tape.gradient(loss_value, mnist_model.trainable_variables)
        opt.apply_gradients(zip(grads, mnist_model.trainable_variables))

        # Horovod: broadcast initial variable states from rank 0 to all other processes.
        # This is necessary to ensure consistent initialization of all workers when
        # training is started with random weights or restored from a checkpoint.
        #
        # Note: broadcast should be done after the first gradient step to ensure optimizer
        # initialization.
        if first_batch:
            hvd.broadcast_variables(mnist_model.variables, root_rank=0)
            hvd.broadcast_variables(opt.variables(), root_rank=0)

        return loss_value

    # Horovod: adjust number of steps based on number of GPUs.
    for batch, (images, labels) in enumerate(dataset.take(10000 // hvd.size())):
        loss_value = training_step(images, labels, batch == 0)

        if batch % 10 == 0 and hvd.rank() == 0:
            print('Step #%d\tLoss: %.6f' % (batch, loss_value))

    # Horovod: save checkpoints only on worker 0 to prevent other workers from
    # corrupting it.
    if hvd.rank() == 0:
        checkpoint.save(checkpoint_dir)",hvd.rank() == 0,not hvd.rank()
CellProfiler,https://github.com/CellProfiler/CellProfiler/tree/master/tests/modules/test_identifydeadworms.py,,test_find_adjacent_by_distance_one$269,"def test_find_adjacent_by_distance_one():
    workspace, module = make_workspace(numpy.zeros((20, 10), bool))
    assert isinstance(module, cellprofiler.modules.identifydeadworms.IdentifyDeadWorms)

    first, second = module.find_adjacent_by_distance(
        numpy.zeros(1), numpy.zeros(1), numpy.zeros(1)
    )
    assert len(first) == 1
    assert first[0] == 0
    assert len(second) == 1
    assert second[0] == 0",first[0] == 0,not first[0]
CellProfiler,https://github.com/CellProfiler/CellProfiler/tree/master/tests/modules/test_identifydeadworms.py,,test_find_adjacent_by_distance_one$269,"def test_find_adjacent_by_distance_one():
    workspace, module = make_workspace(numpy.zeros((20, 10), bool))
    assert isinstance(module, cellprofiler.modules.identifydeadworms.IdentifyDeadWorms)

    first, second = module.find_adjacent_by_distance(
        numpy.zeros(1), numpy.zeros(1), numpy.zeros(1)
    )
    assert len(first) == 1
    assert first[0] == 0
    assert len(second) == 1
    assert second[0] == 0",second[0] == 0,not second[0]
tvnamer,https://github.com/dbr/tvnamer/tree/master/tvnamer/main.py,,tvnamer$358,"def tvnamer(paths):
    # type: (List[str]) -> None
    """"""Main tvnamer function, takes an array of paths, does stuff.
    """"""

    print(""#"" * 20)
    print(""# Starting tvnamer"")

    episodes_found = []

    for cfile in find_files(paths):
        parser = FileParser(cfile)
        try:
            episode = parser.parse()
        except InvalidFilename as e:
            warn(""Invalid filename: %s"" % e)
        else:
            if (
                episode.seriesname is None
                and Config[""force_name""] is None
                and Config[""series_id""] is None
            ):
                warn(
                    ""Parsed filename did not contain series name (and --name or --series-id not specified), skipping: %s""
                    % cfile
                )

            else:
                episodes_found.append(episode)

    if len(episodes_found) == 0:
        raise NoValidFilesFoundError()

    print(
        ""# Found %d episode"" % len(episodes_found) + (""s"" * (len(episodes_found) > 1))
    )

    # Sort episodes by series name, season and episode number
    episodes_found.sort(key=lambda x: x.sortable_info())

    # episode sort order
    if Config[""order""] == ""dvd"":
        dvdorder = True
    else:
        dvdorder = False

    if Config[""tvdb_api_key""] is not None:
        LOG.debug(""Using custom API key from config"")
        api_key = Config[""tvdb_api_key""]
    else:
        LOG.debug(""Using tvnamer default API key"")
        api_key = TVNAMER_API_KEY

    if os.getenv(""TVNAMER_TEST_MODE"", ""0"") == ""1"":
        from .test_cache import get_test_cache_session
        cache = get_test_cache_session()
    else:
        cache = True

    tvdb_instance = tvdb_api.Tvdb(
        interactive=not Config[""select_first""],
        search_all_languages=Config[""search_all_languages""],
        language=Config[""language""],
        dvdorder=dvdorder,
        cache=cache,
        apikey=api_key,
    )

    for episode in episodes_found:
        process_file(tvdb_instance, episode)
        print("""")

    print(""#"" * 20)
    print(""# Done"")",len(episodes_found) == 0,not episodes_found
pybossa,https://github.com/Scifabric/pybossa/tree/master/test/test_view/test_project_passwords.py,TestProjectPassword,test_normal_auth_used_if_no_password_protected$214,"def test_normal_auth_used_if_no_password_protected(self, fake_authorizer):
        """"""Test if a project is password protected, that is the only authorization
        required for it to be seen""""""
        project = ProjectFactory.create()
        TaskFactory.create(project=project)

        self.app.get('/project/%s' % project.short_name, follow_redirects=True)

        assert fake_authorizer.called == True",fake_authorizer.called == True,fake_authorizer.called
understand_facenet,https://github.com/boyliwensheng/understand_facenet/tree/master/understand_facenet/align/detect_face.py,,layer$16,"def layer(op):
    """"""Decorator for composable network layers.""""""

    def layer_decorated(self, *args, **kwargs):
        # Automatically set a name if not provided.
        name = kwargs.setdefault('name', self.get_unique_name(op.__name__))
        # Figure out the layer inputs.
        if len(self.terminals) == 0:
            raise RuntimeError('No input variables found for layer %s.' % name)
        elif len(self.terminals) == 1:
            layer_input = self.terminals[0]
        else:
            layer_input = list(self.terminals)
        # Perform the operation and get the output.
        layer_output = op(self, layer_input, *args, **kwargs)
        # Add to layer LUT.
        self.layers[name] = layer_output
        # This output is now the input for the next layer.
        self.feed(layer_output)
        # Return self for chained calls.
        return self

    return layer_decorated",len(self.terminals) == 0,not self.terminals
zao-,https://github.com/qiucheng025/zao-/tree/master/scripts/convert.py,Predict,queue_out_frames$634,"def queue_out_frames(self, batch, swapped_faces):
        """""" Compile the batch back to original frames and put to out_queue """"""
        logger.trace(""Queueing out batch. Batchsize: %s"", len(batch))
        pointer = 0
        for item in batch:
            num_faces = len(item[""detected_faces""])
            if num_faces == 0:
                item[""swapped_faces""] = np.array(list())
            else:
                item[""swapped_faces""] = swapped_faces[pointer:pointer + num_faces]

            logger.trace(""Putting to queue. ('%s', detected_faces: %s, swapped_faces: %s)"",
                         item[""filename""], len(item[""detected_faces""]),
                         item[""swapped_faces""].shape[0])
            pointer += num_faces
        self.out_queue.put(batch)
        logger.trace(""Queued out batch. Batchsize: %s"", len(batch))",num_faces == 0,not num_faces
real-url,https://github.com/wbt5/real-url/tree/master/danmu/danmaku/tars/__tup.py,TarsUniPacket,get$82,"def get(self, vtype, name):
        if (name in self.__buffer) == False:
            raise Exception(""UniAttribute not found key:%s,type:%s"" %
                            (name, vtype.__tars_class__))

        t = self.__buffer[name]
        if (vtype.__tars_class__ in t) == False:
            raise Exception(""UniAttribute not found type:"" +
                            vtype.__tars_class__)

        o = TarsInputStream(t[vtype.__tars_class__])
        return o.read(vtype, 0, True)",(name in self.__buffer) == False,not name in self.__buffer
real-url,https://github.com/wbt5/real-url/tree/master/danmu/danmaku/tars/__tup.py,TarsUniPacket,get$82,"def get(self, vtype, name):
        if (name in self.__buffer) == False:
            raise Exception(""UniAttribute not found key:%s,type:%s"" %
                            (name, vtype.__tars_class__))

        t = self.__buffer[name]
        if (vtype.__tars_class__ in t) == False:
            raise Exception(""UniAttribute not found type:"" +
                            vtype.__tars_class__)

        o = TarsInputStream(t[vtype.__tars_class__])
        return o.read(vtype, 0, True)",(vtype.__tars_class__ in t) == False,not vtype.__tars_class__ in t
opytimizer,https://github.com/gugarosa/opytimizer/tree/master/tests/opytimizer/optimizers/population/test_hho.py,,test_hho_calculate_initial_coefficients$9,"def test_hho_calculate_initial_coefficients():
    new_hho = hho.HHO()

    E, J = new_hho._calculate_initial_coefficients(1, 10)

    assert E[0] != 0
    assert J[0] != 0",E[0] != 0,E[0]
opytimizer,https://github.com/gugarosa/opytimizer/tree/master/tests/opytimizer/optimizers/population/test_hho.py,,test_hho_calculate_initial_coefficients$9,"def test_hho_calculate_initial_coefficients():
    new_hho = hho.HHO()

    E, J = new_hho._calculate_initial_coefficients(1, 10)

    assert E[0] != 0
    assert J[0] != 0",J[0] != 0,J[0]
mythril,https://github.com/ConsenSys/mythril/tree/master/mythril/laser/ethereum/transaction/transaction_models.py,ContractCreationTransaction,end$265,"def end(self, global_state: GlobalState, return_data=None, revert=False):
        """"""

        :param global_state:
        :param return_data:
        :param revert:
        """"""

        if return_data is None or return_data.size == 0:
            self.return_data = None
            raise TransactionEndSignal(global_state, revert=revert)

        global_state.environment.active_account.code.assign_bytecode(
            tuple(return_data.return_data)
        )
        return_data = str(hex(global_state.environment.active_account.address.value))
        self.return_data = ReturnData(return_data, len(return_data) // 2)
        assert global_state.environment.active_account.code.instruction_list != []

        raise TransactionEndSignal(global_state, revert=revert)",global_state.environment.active_account.code.instruction_list != [],global_state.environment.active_account.code.instruction_list
mythril,https://github.com/ConsenSys/mythril/tree/master/mythril/laser/ethereum/transaction/transaction_models.py,ContractCreationTransaction,end$265,"def end(self, global_state: GlobalState, return_data=None, revert=False):
        """"""

        :param global_state:
        :param return_data:
        :param revert:
        """"""

        if return_data is None or return_data.size == 0:
            self.return_data = None
            raise TransactionEndSignal(global_state, revert=revert)

        global_state.environment.active_account.code.assign_bytecode(
            tuple(return_data.return_data)
        )
        return_data = str(hex(global_state.environment.active_account.address.value))
        self.return_data = ReturnData(return_data, len(return_data) // 2)
        assert global_state.environment.active_account.code.instruction_list != []

        raise TransactionEndSignal(global_state, revert=revert)",return_data.size == 0,not return_data.size
mbrl-lib,https://github.com/facebookresearch/mbrl-lib/tree/master/tests/core/test_replay_buffer.py,,test_sac_buffer_batched_add$118,"def test_sac_buffer_batched_add():
    def compare_batch_to_buffer_slice(
        start_idx, batch_size, obs, act, next_obs, reward, done
    ):
        for i in range(batch_size):
            buffer_idx = (start_idx + i) % buffer.capacity
            np.testing.assert_array_equal(buffer.obses[buffer_idx], obs[i])
            np.testing.assert_array_equal(buffer.actions[buffer_idx], act[i])
            np.testing.assert_array_equal(buffer.next_obses[buffer_idx], next_obs[i])
            np.testing.assert_array_equal(buffer.rewards[buffer_idx], reward[i])
            np.testing.assert_array_equal(
                buffer.not_dones[buffer_idx], np.logical_not(done[i])
            )
            np.testing.assert_array_equal(buffer.not_dones_no_max[buffer_idx], done[i])

    buffer = sac_buffer.ReplayBuffer((2,), (1,), 20, torch.device(""cpu""))

    # Test adding less than capacity
    batch_size_ = 10
    obs_, act_, next_obs_, reward_, done_ = _create_batch(batch_size_)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == batch_size_
    assert not buffer.full
    compare_batch_to_buffer_slice(0, batch_size_, obs_, act_, next_obs_, reward_, done_)

    # Test adding up to capacity
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == 0
    assert buffer.full
    compare_batch_to_buffer_slice(
        batch_size_, batch_size_, obs_, act_, next_obs_, reward_, done_
    )  # new additions
    compare_batch_to_buffer_slice(
        0, batch_size_, obs_, act_, next_obs_, reward_, done_
    )  # Check that nothing changed here

    # Test adding beyond capacity
    start = 4
    buffer = sac_buffer.ReplayBuffer((2,), (1,), 20, torch.device(""cpu""))
    # first add a few elements to set buffer.idx != 0
    obs_, act_, next_obs_, reward_, done_ = _create_batch(start, mult=3)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    # now add a batch larger than capacity
    batch_size_ = 27
    obs_, act_, next_obs_, reward_, done_ = _create_batch(batch_size_, mult=7)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == 11
    assert buffer.full
    # The last 11 observations loop around and overwrite the first 11
    compare_batch_to_buffer_slice(
        0, 11, obs_[16:], act_[16:], next_obs_[16:], reward_[16:], done_[16:]
    )
    # Now check that the last 9 observations are correct
    compare_batch_to_buffer_slice(
        11, 9, obs_[7:16], act_[7:16], next_obs_[7:16], reward_[7:16], done_[7:16]
    )",buffer.idx == 0,not buffer.idx
pre-commit,https://github.com/pre-commit/pre-commit/tree/master/tests/commands/install_uninstall_test.py,,test_hook_types_configured_nonsense$46,"def test_hook_types_configured_nonsense(tmpdir):
    cfg = tmpdir.join('t.cfg')
    cfg.write('default_install_hook_types: []\nrepos: []\n')

    # hopefully the user doesn't do this, but the code allows it!
    assert _hook_types(str(cfg), None) == []","_hook_types(str(cfg), None) == []","not _hook_types(str(cfg), None)"
CV-Backbones,https://github.com/huawei-noah/CV-Backbones/tree/master/legonet_pytorch/vgg.py,lego_vgg16,_make_layers$29,"def _make_layers(self, cfg):
        layers = []
        in_channels = 3
        for i, x in enumerate(cfg):
            if i == 0:
                layers += [nn.Conv2d(in_channels, x, 3, padding = 1),
                    nn.BatchNorm2d(x),
                    nn.ReLU(inplace=True)]
                in_channels = x
                continue
            if x == 'M':
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                layers += [LegoConv2d(in_channels, x, 3, self.n_split, self.n_lego),
                           nn.BatchNorm2d(x),
                           nn.ReLU(inplace=True)]
                in_channels = x
        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]
        return nn.Sequential(*layers)",i == 0,not i
mongoengine,https://github.com/MongoEngine/mongoengine/tree/master/tests/document/test_delta.py,TestDelta,test_nested_nested_fields_db_field_set__gets_mark_as_changed_and_cleaned$796,"def test_nested_nested_fields_db_field_set__gets_mark_as_changed_and_cleaned(self):
        class EmbeddedDoc(EmbeddedDocument):
            name = StringField(db_field=""db_name"")

        class MyDoc(Document):
            embed = EmbeddedDocumentField(EmbeddedDoc, db_field=""db_embed"")
            name = StringField(db_field=""db_name"")

        MyDoc.drop_collection()

        MyDoc(name=""testcase1"", embed=EmbeddedDoc(name=""foo"")).save()

        mydoc = MyDoc.objects.first()
        mydoc.embed.name = ""foo1""

        assert mydoc.embed._get_changed_fields() == [""db_name""]
        assert mydoc._get_changed_fields() == [""db_embed.db_name""]

        mydoc = MyDoc.objects.first()
        embed = EmbeddedDoc(name=""foo2"")
        embed.name = ""bar""
        mydoc.embed = embed

        assert embed._get_changed_fields() == [""db_name""]
        assert mydoc._get_changed_fields() == [""db_embed""]

        mydoc._clear_changed_fields()
        assert mydoc._get_changed_fields() == []",mydoc._get_changed_fields() == [],not mydoc._get_changed_fields()
edx-platform,https://github.com/edx/edx-platform/tree/master/cms/djangoapps/contentstore/views/tests/test_exam_settings_view.py,TestExamSettingsView,test_exam_settings_alert_not_shown$169,"def test_exam_settings_alert_not_shown(self, page_handler):
        """"""
        Alert should not be visible if no proctored exam setting error exists
        """"""
        # course_handler raise 404 for old mongo course
        if self.course.id.deprecated and page_handler == 'course_handler':
            raise SkipTest(""course_handler raise 404 for old mongo course"")
        url = reverse_course_url(page_handler, self.course.id)
        resp = self.client.get(url, HTTP_ACCEPT='text/html')
        parsed_html = lxml.html.fromstring(resp.content)
        alert_nodes = parsed_html.find_class('exam-settings-alert')
        assert len(alert_nodes) == 0",len(alert_nodes) == 0,not alert_nodes
SiCKRAGE,https://github.com/SiCKRAGE/SiCKRAGE/tree/master/sickrage/core/search.py,,pick_best_result$145,"def pick_best_result(results, season_pack=False):
    """"""
    Find the best result out of a list of search results for a show

    :param results: list of result objects
    :param series_id: Show ID we check for
    :return: best result object
    """"""

    results = results if isinstance(results, list) else [results]

    sickrage.app.log.debug(""Picking the best result out of "" + str([x.name for x in results]))

    best_result = None

    # find the best result for the current episode
    for cur_result in results:
        show_obj = find_show(cur_result.series_id, cur_result.series_provider_id)

        # build the black And white list
        if show_obj.is_anime:
            if not show_obj.release_groups.is_valid(cur_result):
                continue

        sickrage.app.log.info(""Quality of "" + cur_result.name + "" is "" + cur_result.quality.display_name)

        any_qualities, best_qualities = Quality.split_quality(show_obj.quality)
        if cur_result.quality not in any_qualities + best_qualities:
            sickrage.app.log.debug(cur_result.name + "" is a quality we know we don't want, rejecting it"")
            continue

        # check if seeders meet out minimum requirements, disgard result if it does not
        if cur_result.provider.custom_settings.get('minseed', 0) and cur_result.seeders not in (-1, None):
            if int(cur_result.seeders) < min(cur_result.provider.custom_settings.get('minseed', 0), 1):
                sickrage.app.log.info(""Discarding torrent because it doesn't meet the minimum seeders: {}. Seeders:  ""
                                      ""{}"".format(cur_result.name, cur_result.seeders))
                continue

        # check if leechers meet out minimum requirements, disgard result if it does not
        if cur_result.provider.custom_settings.get('minleech', 0) and cur_result.leechers not in (-1, None):
            if int(cur_result.leechers) < min(cur_result.provider.custom_settings.get('minleech', 0), 0):
                sickrage.app.log.info(""Discarding torrent because it doesn't meet the minimum leechers: {}. Leechers:  ""
                                      ""{}"".format(cur_result.name, cur_result.leechers))
                continue

        if show_obj.rls_ignore_words and show_names.contains_at_least_one_word(cur_result.name, show_obj.rls_ignore_words):
            sickrage.app.log.info(""Ignoring "" + cur_result.name + "" based on ignored words filter: "" + show_obj.rls_ignore_words)
            continue

        if show_obj.rls_require_words and not show_names.contains_at_least_one_word(cur_result.name, show_obj.rls_require_words):
            sickrage.app.log.info(""Ignoring "" + cur_result.name + "" based on required words filter: "" + show_obj.rls_require_words)
            continue

        if not show_names.filter_bad_releases(cur_result.name, parse=False):
            sickrage.app.log.info(""Ignoring "" + cur_result.name + "" because its not a valid scene release that we want"")
            continue

        if hasattr(cur_result, 'size'):
            if FailedHistory.has_failed(cur_result.name, cur_result.size, cur_result.provider.name):
                sickrage.app.log.info(cur_result.name + "" has previously failed, rejecting it"")
                continue

            # quality definition video file size constraints check
            try:
                if cur_result.size:
                    quality_size_min = sickrage.app.config.quality_sizes[cur_result.quality.name].min_size
                    quality_size_max = sickrage.app.config.quality_sizes[cur_result.quality.name].max_size

                    if quality_size_min != 0 and quality_size_max != 0:
                        if season_pack and not len(cur_result.episodes):
                            episode_count = len([x for x in show_obj.episodes if x.season == cur_result.season])
                            file_size = float(cur_result.size / episode_count / 1000000)
                        else:
                            file_size = float(cur_result.size / len(cur_result.episodes) / 1000000)

                        if quality_size_min > file_size > quality_size_max:
                            raise Exception(""Ignoring "" + cur_result.name + "" with size {}"".format(file_size))
            except Exception as e:
                sickrage.app.log.info(e)
                continue

        # verify result content
        # if not cur_result.provider.private:
        #     if cur_result.provider_type in [""nzb"", ""torrent""] and not cur_result.provider.get_content(cur_result.url):
        #         if not sickrage.app.config.general.download_unverified_magnet_link and cur_result.url.startswith('magnet'):
        #             sickrage.app.log.info(""Ignoring {} because we are unable to verify the download url"".format(cur_result.name))
        #             continue

        if not best_result:
            best_result = cur_result
        elif cur_result.quality in best_qualities and (
                best_result.quality < cur_result.quality or best_result.quality not in best_qualities):
            best_result = cur_result
        elif cur_result.quality in any_qualities and best_result.quality not in best_qualities and best_result.quality < cur_result.quality:
            best_result = cur_result
        elif best_result.quality == cur_result.quality:
            if ""proper"" in cur_result.name.lower() or ""repack"" in cur_result.name.lower():
                best_result = cur_result
            elif ""internal"" in best_result.name.lower() and ""internal"" not in cur_result.name.lower():
                best_result = cur_result
            elif ""xvid"" in best_result.name.lower() and ""x264"" in cur_result.name.lower():
                sickrage.app.log.info(""Preferring "" + cur_result.name + "" (x264 over xvid)"")
                best_result = cur_result

    if best_result:
        sickrage.app.log.debug(""Picked "" + best_result.name + "" as the best"")
    else:
        sickrage.app.log.debug(""No result picked."")

    return best_result",quality_size_min != 0,quality_size_min
SiCKRAGE,https://github.com/SiCKRAGE/SiCKRAGE/tree/master/sickrage/core/search.py,,pick_best_result$145,"def pick_best_result(results, season_pack=False):
    """"""
    Find the best result out of a list of search results for a show

    :param results: list of result objects
    :param series_id: Show ID we check for
    :return: best result object
    """"""

    results = results if isinstance(results, list) else [results]

    sickrage.app.log.debug(""Picking the best result out of "" + str([x.name for x in results]))

    best_result = None

    # find the best result for the current episode
    for cur_result in results:
        show_obj = find_show(cur_result.series_id, cur_result.series_provider_id)

        # build the black And white list
        if show_obj.is_anime:
            if not show_obj.release_groups.is_valid(cur_result):
                continue

        sickrage.app.log.info(""Quality of "" + cur_result.name + "" is "" + cur_result.quality.display_name)

        any_qualities, best_qualities = Quality.split_quality(show_obj.quality)
        if cur_result.quality not in any_qualities + best_qualities:
            sickrage.app.log.debug(cur_result.name + "" is a quality we know we don't want, rejecting it"")
            continue

        # check if seeders meet out minimum requirements, disgard result if it does not
        if cur_result.provider.custom_settings.get('minseed', 0) and cur_result.seeders not in (-1, None):
            if int(cur_result.seeders) < min(cur_result.provider.custom_settings.get('minseed', 0), 1):
                sickrage.app.log.info(""Discarding torrent because it doesn't meet the minimum seeders: {}. Seeders:  ""
                                      ""{}"".format(cur_result.name, cur_result.seeders))
                continue

        # check if leechers meet out minimum requirements, disgard result if it does not
        if cur_result.provider.custom_settings.get('minleech', 0) and cur_result.leechers not in (-1, None):
            if int(cur_result.leechers) < min(cur_result.provider.custom_settings.get('minleech', 0), 0):
                sickrage.app.log.info(""Discarding torrent because it doesn't meet the minimum leechers: {}. Leechers:  ""
                                      ""{}"".format(cur_result.name, cur_result.leechers))
                continue

        if show_obj.rls_ignore_words and show_names.contains_at_least_one_word(cur_result.name, show_obj.rls_ignore_words):
            sickrage.app.log.info(""Ignoring "" + cur_result.name + "" based on ignored words filter: "" + show_obj.rls_ignore_words)
            continue

        if show_obj.rls_require_words and not show_names.contains_at_least_one_word(cur_result.name, show_obj.rls_require_words):
            sickrage.app.log.info(""Ignoring "" + cur_result.name + "" based on required words filter: "" + show_obj.rls_require_words)
            continue

        if not show_names.filter_bad_releases(cur_result.name, parse=False):
            sickrage.app.log.info(""Ignoring "" + cur_result.name + "" because its not a valid scene release that we want"")
            continue

        if hasattr(cur_result, 'size'):
            if FailedHistory.has_failed(cur_result.name, cur_result.size, cur_result.provider.name):
                sickrage.app.log.info(cur_result.name + "" has previously failed, rejecting it"")
                continue

            # quality definition video file size constraints check
            try:
                if cur_result.size:
                    quality_size_min = sickrage.app.config.quality_sizes[cur_result.quality.name].min_size
                    quality_size_max = sickrage.app.config.quality_sizes[cur_result.quality.name].max_size

                    if quality_size_min != 0 and quality_size_max != 0:
                        if season_pack and not len(cur_result.episodes):
                            episode_count = len([x for x in show_obj.episodes if x.season == cur_result.season])
                            file_size = float(cur_result.size / episode_count / 1000000)
                        else:
                            file_size = float(cur_result.size / len(cur_result.episodes) / 1000000)

                        if quality_size_min > file_size > quality_size_max:
                            raise Exception(""Ignoring "" + cur_result.name + "" with size {}"".format(file_size))
            except Exception as e:
                sickrage.app.log.info(e)
                continue

        # verify result content
        # if not cur_result.provider.private:
        #     if cur_result.provider_type in [""nzb"", ""torrent""] and not cur_result.provider.get_content(cur_result.url):
        #         if not sickrage.app.config.general.download_unverified_magnet_link and cur_result.url.startswith('magnet'):
        #             sickrage.app.log.info(""Ignoring {} because we are unable to verify the download url"".format(cur_result.name))
        #             continue

        if not best_result:
            best_result = cur_result
        elif cur_result.quality in best_qualities and (
                best_result.quality < cur_result.quality or best_result.quality not in best_qualities):
            best_result = cur_result
        elif cur_result.quality in any_qualities and best_result.quality not in best_qualities and best_result.quality < cur_result.quality:
            best_result = cur_result
        elif best_result.quality == cur_result.quality:
            if ""proper"" in cur_result.name.lower() or ""repack"" in cur_result.name.lower():
                best_result = cur_result
            elif ""internal"" in best_result.name.lower() and ""internal"" not in cur_result.name.lower():
                best_result = cur_result
            elif ""xvid"" in best_result.name.lower() and ""x264"" in cur_result.name.lower():
                sickrage.app.log.info(""Preferring "" + cur_result.name + "" (x264 over xvid)"")
                best_result = cur_result

    if best_result:
        sickrage.app.log.debug(""Picked "" + best_result.name + "" as the best"")
    else:
        sickrage.app.log.debug(""No result picked."")

    return best_result",quality_size_max != 0,quality_size_max
MultiQC,https://github.com/ewels/MultiQC/tree/master/multiqc/modules/hicup/hicup.py,MultiqcModule,parse_hicup_logs$67,"def parse_hicup_logs(self, f):
        """"""Parse a HiCUP summary report""""""
        if not f[""fn""].endswith("".txt""):
            return None
        header = []
        lines = f[""f""].splitlines()
        for l in lines:
            s = l.split(""\t"")
            if len(header) == 0:
                if s[0] != ""File"":
                    return None
                header = s[1:]
            else:
                s_name = self.clean_s_name(s[0], f)
                if s_name.startswith(""HiCUP_output/""):
                    s_name = s_name[13:]
                parsed_data = {}
                for idx, num in enumerate(s[1:]):
                    try:
                        parsed_data[header[idx]] = float(num)
                    except:
                        parsed_data[header[idx]] = num
                parsed_data[""Duplicate_Read_Pairs""] = (
                    parsed_data[""Valid_Pairs""] - parsed_data[""Deduplication_Read_Pairs_Uniques""]
                )
                if s_name in self.hicup_data:
                    log.debug(""Duplicate sample name found! Overwriting: {}"".format(s_name))
                self.add_data_source(f, s_name)
                self.hicup_data[s_name] = parsed_data",len(header) == 0,not header
quodlibet,https://github.com/quodlibet/quodlibet/tree/master/quodlibet/ext/songsmenu/tapbpm.py,TapBpmPanel,count_tap$112,"def count_tap(self, now):
        now = now / 1000.
        # reset?
        if now - self.last > self.pause:
            self.clicks = 0
            self.bpm = 0.0

            self.last_times = []
            self.last_bpms = []
            self.last_floating_bpms = []
            self.last_floating_squares = []
            self.bpms_sum = 0.0
            self.squares_sum = 0.0
            self.average_count = 0
            self.min = 0
            self.max = 0
            self.floating_bpm = 0.0
            self.floating_square = 0.0
        elif now > self.last:
            # Use previous 5 values to average BPM
            bpms = []
            bpms.append(60.0 / (now - self.last))
            # and four out of the list
            for i in iter(range(1, 5)):
                if len(self.last_times) <= i:
                    break
                bpms.append((i + 1) * 60.0 / (now - self.last_times[-i]))

            self.bpm = sum(bpms) / len(bpms)

            # Exponentially weighted floating average
            weight = (1.0 / self.clicks) ** .5
            if weight < self.min_weight:
                weight = self.min_weight
            self.floating_bpm = \
                self.floating_bpm * (1.0 - weight) \
                + self.bpm * weight
            self.floating_square = \
                self.floating_square * (1.0 - weight) \
                + self.bpm * self.bpm * weight

            if self.bpm < self.min or self.average_count == 0:
                self.min = self.bpm
            if self.bpm > self.max or self.average_count == 0:
                self.max = self.bpm
            self.bpms_sum += self.bpm
            self.squares_sum += self.bpm * self.bpm
            self.average_count += 1

            # Update history
            self.last_times = self.last_times[-(self.keep - 1):] + [self.last]
            self.last_bpms = self.last_bpms[-(self.keep - 1):] + [self.bpm]
            self.last_floating_bpms = \
                self.last_floating_bpms[-(self.keep - 1):] \
                + [self.floating_bpm]
            self.last_floating_squares = \
                self.last_floating_squares[-(self.keep - 1):] \
                + [self.floating_square]

        self.last = now
        self.clicks += 1",self.average_count == 0,not self.average_count
quodlibet,https://github.com/quodlibet/quodlibet/tree/master/quodlibet/ext/songsmenu/tapbpm.py,TapBpmPanel,count_tap$112,"def count_tap(self, now):
        now = now / 1000.
        # reset?
        if now - self.last > self.pause:
            self.clicks = 0
            self.bpm = 0.0

            self.last_times = []
            self.last_bpms = []
            self.last_floating_bpms = []
            self.last_floating_squares = []
            self.bpms_sum = 0.0
            self.squares_sum = 0.0
            self.average_count = 0
            self.min = 0
            self.max = 0
            self.floating_bpm = 0.0
            self.floating_square = 0.0
        elif now > self.last:
            # Use previous 5 values to average BPM
            bpms = []
            bpms.append(60.0 / (now - self.last))
            # and four out of the list
            for i in iter(range(1, 5)):
                if len(self.last_times) <= i:
                    break
                bpms.append((i + 1) * 60.0 / (now - self.last_times[-i]))

            self.bpm = sum(bpms) / len(bpms)

            # Exponentially weighted floating average
            weight = (1.0 / self.clicks) ** .5
            if weight < self.min_weight:
                weight = self.min_weight
            self.floating_bpm = \
                self.floating_bpm * (1.0 - weight) \
                + self.bpm * weight
            self.floating_square = \
                self.floating_square * (1.0 - weight) \
                + self.bpm * self.bpm * weight

            if self.bpm < self.min or self.average_count == 0:
                self.min = self.bpm
            if self.bpm > self.max or self.average_count == 0:
                self.max = self.bpm
            self.bpms_sum += self.bpm
            self.squares_sum += self.bpm * self.bpm
            self.average_count += 1

            # Update history
            self.last_times = self.last_times[-(self.keep - 1):] + [self.last]
            self.last_bpms = self.last_bpms[-(self.keep - 1):] + [self.bpm]
            self.last_floating_bpms = \
                self.last_floating_bpms[-(self.keep - 1):] \
                + [self.floating_bpm]
            self.last_floating_squares = \
                self.last_floating_squares[-(self.keep - 1):] \
                + [self.floating_square]

        self.last = now
        self.clicks += 1",self.average_count == 0,not self.average_count
fairseq,https://github.com/pytorch/fairseq/tree/master/fairseq/models/transformer/transformer_encoder.py,TransformerEncoderBase,reorder_encoder_out$269,"def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
        """"""
        Reorder encoder output according to *new_order*.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            *encoder_out* rearranged according to *new_order*
        """"""
        if len(encoder_out[""encoder_out""]) == 0:
            new_encoder_out = []
        else:
            new_encoder_out = [encoder_out[""encoder_out""][0].index_select(1, new_order)]
        if len(encoder_out[""encoder_padding_mask""]) == 0:
            new_encoder_padding_mask = []
        else:
            new_encoder_padding_mask = [
                encoder_out[""encoder_padding_mask""][0].index_select(0, new_order)
            ]
        if len(encoder_out[""encoder_embedding""]) == 0:
            new_encoder_embedding = []
        else:
            new_encoder_embedding = [
                encoder_out[""encoder_embedding""][0].index_select(0, new_order)
            ]

        if len(encoder_out[""src_tokens""]) == 0:
            src_tokens = []
        else:
            src_tokens = [(encoder_out[""src_tokens""][0]).index_select(0, new_order)]

        if len(encoder_out[""src_lengths""]) == 0:
            src_lengths = []
        else:
            src_lengths = [(encoder_out[""src_lengths""][0]).index_select(0, new_order)]

        encoder_states = encoder_out[""encoder_states""]
        if len(encoder_states) > 0:
            for idx, state in enumerate(encoder_states):
                encoder_states[idx] = state.index_select(1, new_order)

        return {
            ""encoder_out"": new_encoder_out,  # T x B x C
            ""encoder_padding_mask"": new_encoder_padding_mask,  # B x T
            ""encoder_embedding"": new_encoder_embedding,  # B x T x C
            ""encoder_states"": encoder_states,  # List[T x B x C]
            ""src_tokens"": src_tokens,  # B x T
            ""src_lengths"": src_lengths,  # B x 1
        }",len(encoder_out['encoder_out']) == 0,not encoder_out['encoder_out']
fairseq,https://github.com/pytorch/fairseq/tree/master/fairseq/models/transformer/transformer_encoder.py,TransformerEncoderBase,reorder_encoder_out$269,"def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
        """"""
        Reorder encoder output according to *new_order*.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            *encoder_out* rearranged according to *new_order*
        """"""
        if len(encoder_out[""encoder_out""]) == 0:
            new_encoder_out = []
        else:
            new_encoder_out = [encoder_out[""encoder_out""][0].index_select(1, new_order)]
        if len(encoder_out[""encoder_padding_mask""]) == 0:
            new_encoder_padding_mask = []
        else:
            new_encoder_padding_mask = [
                encoder_out[""encoder_padding_mask""][0].index_select(0, new_order)
            ]
        if len(encoder_out[""encoder_embedding""]) == 0:
            new_encoder_embedding = []
        else:
            new_encoder_embedding = [
                encoder_out[""encoder_embedding""][0].index_select(0, new_order)
            ]

        if len(encoder_out[""src_tokens""]) == 0:
            src_tokens = []
        else:
            src_tokens = [(encoder_out[""src_tokens""][0]).index_select(0, new_order)]

        if len(encoder_out[""src_lengths""]) == 0:
            src_lengths = []
        else:
            src_lengths = [(encoder_out[""src_lengths""][0]).index_select(0, new_order)]

        encoder_states = encoder_out[""encoder_states""]
        if len(encoder_states) > 0:
            for idx, state in enumerate(encoder_states):
                encoder_states[idx] = state.index_select(1, new_order)

        return {
            ""encoder_out"": new_encoder_out,  # T x B x C
            ""encoder_padding_mask"": new_encoder_padding_mask,  # B x T
            ""encoder_embedding"": new_encoder_embedding,  # B x T x C
            ""encoder_states"": encoder_states,  # List[T x B x C]
            ""src_tokens"": src_tokens,  # B x T
            ""src_lengths"": src_lengths,  # B x 1
        }",len(encoder_out['encoder_padding_mask']) == 0,not encoder_out['encoder_padding_mask']
fairseq,https://github.com/pytorch/fairseq/tree/master/fairseq/models/transformer/transformer_encoder.py,TransformerEncoderBase,reorder_encoder_out$269,"def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
        """"""
        Reorder encoder output according to *new_order*.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            *encoder_out* rearranged according to *new_order*
        """"""
        if len(encoder_out[""encoder_out""]) == 0:
            new_encoder_out = []
        else:
            new_encoder_out = [encoder_out[""encoder_out""][0].index_select(1, new_order)]
        if len(encoder_out[""encoder_padding_mask""]) == 0:
            new_encoder_padding_mask = []
        else:
            new_encoder_padding_mask = [
                encoder_out[""encoder_padding_mask""][0].index_select(0, new_order)
            ]
        if len(encoder_out[""encoder_embedding""]) == 0:
            new_encoder_embedding = []
        else:
            new_encoder_embedding = [
                encoder_out[""encoder_embedding""][0].index_select(0, new_order)
            ]

        if len(encoder_out[""src_tokens""]) == 0:
            src_tokens = []
        else:
            src_tokens = [(encoder_out[""src_tokens""][0]).index_select(0, new_order)]

        if len(encoder_out[""src_lengths""]) == 0:
            src_lengths = []
        else:
            src_lengths = [(encoder_out[""src_lengths""][0]).index_select(0, new_order)]

        encoder_states = encoder_out[""encoder_states""]
        if len(encoder_states) > 0:
            for idx, state in enumerate(encoder_states):
                encoder_states[idx] = state.index_select(1, new_order)

        return {
            ""encoder_out"": new_encoder_out,  # T x B x C
            ""encoder_padding_mask"": new_encoder_padding_mask,  # B x T
            ""encoder_embedding"": new_encoder_embedding,  # B x T x C
            ""encoder_states"": encoder_states,  # List[T x B x C]
            ""src_tokens"": src_tokens,  # B x T
            ""src_lengths"": src_lengths,  # B x 1
        }",len(encoder_out['encoder_embedding']) == 0,not encoder_out['encoder_embedding']
fairseq,https://github.com/pytorch/fairseq/tree/master/fairseq/models/transformer/transformer_encoder.py,TransformerEncoderBase,reorder_encoder_out$269,"def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
        """"""
        Reorder encoder output according to *new_order*.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            *encoder_out* rearranged according to *new_order*
        """"""
        if len(encoder_out[""encoder_out""]) == 0:
            new_encoder_out = []
        else:
            new_encoder_out = [encoder_out[""encoder_out""][0].index_select(1, new_order)]
        if len(encoder_out[""encoder_padding_mask""]) == 0:
            new_encoder_padding_mask = []
        else:
            new_encoder_padding_mask = [
                encoder_out[""encoder_padding_mask""][0].index_select(0, new_order)
            ]
        if len(encoder_out[""encoder_embedding""]) == 0:
            new_encoder_embedding = []
        else:
            new_encoder_embedding = [
                encoder_out[""encoder_embedding""][0].index_select(0, new_order)
            ]

        if len(encoder_out[""src_tokens""]) == 0:
            src_tokens = []
        else:
            src_tokens = [(encoder_out[""src_tokens""][0]).index_select(0, new_order)]

        if len(encoder_out[""src_lengths""]) == 0:
            src_lengths = []
        else:
            src_lengths = [(encoder_out[""src_lengths""][0]).index_select(0, new_order)]

        encoder_states = encoder_out[""encoder_states""]
        if len(encoder_states) > 0:
            for idx, state in enumerate(encoder_states):
                encoder_states[idx] = state.index_select(1, new_order)

        return {
            ""encoder_out"": new_encoder_out,  # T x B x C
            ""encoder_padding_mask"": new_encoder_padding_mask,  # B x T
            ""encoder_embedding"": new_encoder_embedding,  # B x T x C
            ""encoder_states"": encoder_states,  # List[T x B x C]
            ""src_tokens"": src_tokens,  # B x T
            ""src_lengths"": src_lengths,  # B x 1
        }",len(encoder_out['src_tokens']) == 0,not encoder_out['src_tokens']
fairseq,https://github.com/pytorch/fairseq/tree/master/fairseq/models/transformer/transformer_encoder.py,TransformerEncoderBase,reorder_encoder_out$269,"def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
        """"""
        Reorder encoder output according to *new_order*.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            *encoder_out* rearranged according to *new_order*
        """"""
        if len(encoder_out[""encoder_out""]) == 0:
            new_encoder_out = []
        else:
            new_encoder_out = [encoder_out[""encoder_out""][0].index_select(1, new_order)]
        if len(encoder_out[""encoder_padding_mask""]) == 0:
            new_encoder_padding_mask = []
        else:
            new_encoder_padding_mask = [
                encoder_out[""encoder_padding_mask""][0].index_select(0, new_order)
            ]
        if len(encoder_out[""encoder_embedding""]) == 0:
            new_encoder_embedding = []
        else:
            new_encoder_embedding = [
                encoder_out[""encoder_embedding""][0].index_select(0, new_order)
            ]

        if len(encoder_out[""src_tokens""]) == 0:
            src_tokens = []
        else:
            src_tokens = [(encoder_out[""src_tokens""][0]).index_select(0, new_order)]

        if len(encoder_out[""src_lengths""]) == 0:
            src_lengths = []
        else:
            src_lengths = [(encoder_out[""src_lengths""][0]).index_select(0, new_order)]

        encoder_states = encoder_out[""encoder_states""]
        if len(encoder_states) > 0:
            for idx, state in enumerate(encoder_states):
                encoder_states[idx] = state.index_select(1, new_order)

        return {
            ""encoder_out"": new_encoder_out,  # T x B x C
            ""encoder_padding_mask"": new_encoder_padding_mask,  # B x T
            ""encoder_embedding"": new_encoder_embedding,  # B x T x C
            ""encoder_states"": encoder_states,  # List[T x B x C]
            ""src_tokens"": src_tokens,  # B x T
            ""src_lengths"": src_lengths,  # B x 1
        }",len(encoder_out['src_lengths']) == 0,not encoder_out['src_lengths']
ezdxf,https://github.com/mozman/ezdxf/tree/master/tests/test_01_dxf_entities/test_105_xdata.py,,test_empty_xdata$58,"def test_empty_xdata():
    xdata = XData()
    assert len(xdata) == 0
    tagwriter = TagWriter()
    xdata.export_dxf(tagwriter)
    assert len(tagwriter.tags) == 0",len(xdata) == 0,not xdata
ezdxf,https://github.com/mozman/ezdxf/tree/master/tests/test_01_dxf_entities/test_105_xdata.py,,test_empty_xdata$58,"def test_empty_xdata():
    xdata = XData()
    assert len(xdata) == 0
    tagwriter = TagWriter()
    xdata.export_dxf(tagwriter)
    assert len(tagwriter.tags) == 0",len(tagwriter.tags) == 0,not tagwriter.tags
py2neo,https://github.com/py2neo-org/py2neo/tree/master/test/unit/test_data.py,IntersectionTestCase,test_node_intersection_different$909,"def test_node_intersection_different(self):
        s = alice & bob
        assert len(s.nodes) == 0
        assert len(s.relationships) == 0",len(s.nodes) == 0,not s.nodes
py2neo,https://github.com/py2neo-org/py2neo/tree/master/test/unit/test_data.py,IntersectionTestCase,test_node_intersection_different$909,"def test_node_intersection_different(self):
        s = alice & bob
        assert len(s.nodes) == 0
        assert len(s.relationships) == 0",len(s.relationships) == 0,not s.relationships
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",color != None,color
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",color == None,not color
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",all != None,all
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",all == None,not all
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",detailed != None,detailed
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",detailed == None,not detailed
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",cfg_only != None,cfg_only
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",cfg_only == None,not cfg_only
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",count_of_findings != None,count_of_findings
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",count_of_findings == None,not count_of_findings
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",multiprocessing != None,multiprocessing
Ropper,https://github.com/sashs/Ropper/tree/master/ropper/service.py,Options,__checkOptions$174,"def __checkOptions(self, options):
        if not isinstance(options, dict):
            raise TypeError('options has to be an instance of dict')

        inst_count = options.get('inst_count')
        if inst_count and not isinstance(inst_count, (int)):
            raise TypeError('inst_count has to be an instance of int')
        elif not inst_count:
            options['inst_count'] = 6
        elif inst_count < 1:
            raise AttributeError('inst_count has to be bigger than 0')

        color = options.get('color')
        if color != None and not isinstance(color, bool):
            raise TypeError('color has to be an instance of bool')
        elif color == None:
            options['color'] = False

        badbytes = options.get('badbytes')
        if badbytes and not isinstance(badbytes, str):
            raise TypeError('badbytes has to be an instance of str')
        elif badbytes and len(badbytes) % 2 == 1:
            raise AttributeError('length of badbytes has to be even')
        elif badbytes and not isHex('0x'+badbytes):
            raise AttributeError('badbytes has to consist of 0-9 a-f A-F')
        elif not badbytes:
            options['badbytes'] = ''

        all = options.get('all')
        if all != None and not isinstance(all, bool):
            raise TypeError('all has to be an instance of bool')
        elif all == None:
            options['all'] = False

        gtype = options.get('type')
        if gtype and not isinstance(gtype, str):
            raise TypeError('type has to be an instance of str')
        elif gtype and gtype not in ['rop', 'jop', 'sys', 'all']:
            raise AttributeError('type has to be a ""rop"", ""jop"", ""sys"" or ""all""')
        elif not gtype:
            options['type'] = 'all'

        detailed = options.get('detailed')
        if detailed != None and not isinstance(detailed, bool):
            raise TypeError('detailed has to be an instance of bool')
        elif detailed == None:
            options['detailed'] = False

        cfg_only = options.get('cfg_only')
        if cfg_only != None and not isinstance(cfg_only, bool):
            raise TypeError('cfg_only has to be an instance of bool')
        elif cfg_only == None:
            options['cfg_only'] = False

        count_of_findings = options.get('count_of_findings')
        if count_of_findings != None and not isinstance(count_of_findings, int):
            raise TypeError('cfg_only has to be an instance of int')
        elif count_of_findings == None:
            options['count_of_findings'] = 5

        multiprocessing = options.get('multiprocessing')
        if multiprocessing != None and not isinstance(multiprocessing, bool):
            raise TypeError('count_of_processes has to be an instance of bool')
        elif multiprocessing and sys.platform.startswith('win'):
            raise AttributeError('multiprocessing cannot be used on windows.')
        elif multiprocessing == None:
            options['multiprocessing'] = not sys.platform.startswith('win')",multiprocessing == None,not multiprocessing
Kunlun-M,https://github.com/LoRexxar/Kunlun-M/tree/master/core/core_engine/php/engine.py,,init_match_rule$14,"def init_match_rule(data):
    """"""
    澶勭悊鏂扮敓鎴愯勫垯鍒濆嬪寲姝ｅ垯鍖归厤
    :param data: 
    :return: 
    """"""

    try:
        object = data[0]
        match = """"

        if isinstance(object, php.Method) or isinstance(object, php.Function):
            function_params = object.params
            function_name = object.name
            param = data[1]
            index = 0
            for function_param in function_params:
                if function_param.name == param.name:
                    break
                index += 1

            # curl_setopt\s*\(.*,\s*CURLOPT_URL\s*,(.*)\)
            match = ""(?:\A|\s|\\b)"" + function_name + ""\s*\(""
            for i in range(len(function_params)):
                if i != 0:
                    match += "",""

                    if function_params[i].default is not None:
                        match += ""?""

                if i == index:
                    match += ""([^,\)]*)""
                else:
                    match += ""[^,\)]*""

            match += ""\)""

            # 鍘婚櫎瀹氫箟鍑芥暟
            match2 = ""function\s+"" + function_name
            vul_function = function_name
            origin_func_name = data[2]

        elif isinstance(object, php.Class):
            class_params = data[2]
            class_name = object.name
            param = data[1]
            index = 0

            for class_param in class_params:
                if class_param.name == param.name:
                    break
                index += 1

            # $A = new a($x, $y);
            match = ""new\s*"" + class_name + ""\s*\(""

            for i in range(len(class_params)):
                if i != 0:
                    match += "",""

                    if class_params[i].default is not None:
                        match += ""?""

                if i == index:
                    match += ""([^,\)]*)""
                else:
                    match += ""[^,\)]*""

            match += ""\)""

            # 鍘婚櫎瀹氫箟绫伙紝绫诲畾涔夊拰璋冪敤鏂瑰紡涓嶄竴鏍凤紝浣嗘槸涓轰簡涓嶅奖鍝嶇粨鏋勶紝渚濈劧璧嬪
            match2 = ""class\s+"" + class_name + ""\s*{""
            vul_function = class_name
            origin_func_name = data[3]

    except:
        logger.error('[New Rule] Error to unpack function param, Something error')
        traceback.print_exc()
        match = None
        match2 = None
        index = 0
        vul_function = None
        origin_func_name = ""None""

    return match, match2, vul_function, index, origin_func_name",i != 0,i
Kunlun-M,https://github.com/LoRexxar/Kunlun-M/tree/master/core/core_engine/php/engine.py,,init_match_rule$14,"def init_match_rule(data):
    """"""
    澶勭悊鏂扮敓鎴愯勫垯鍒濆嬪寲姝ｅ垯鍖归厤
    :param data: 
    :return: 
    """"""

    try:
        object = data[0]
        match = """"

        if isinstance(object, php.Method) or isinstance(object, php.Function):
            function_params = object.params
            function_name = object.name
            param = data[1]
            index = 0
            for function_param in function_params:
                if function_param.name == param.name:
                    break
                index += 1

            # curl_setopt\s*\(.*,\s*CURLOPT_URL\s*,(.*)\)
            match = ""(?:\A|\s|\\b)"" + function_name + ""\s*\(""
            for i in range(len(function_params)):
                if i != 0:
                    match += "",""

                    if function_params[i].default is not None:
                        match += ""?""

                if i == index:
                    match += ""([^,\)]*)""
                else:
                    match += ""[^,\)]*""

            match += ""\)""

            # 鍘婚櫎瀹氫箟鍑芥暟
            match2 = ""function\s+"" + function_name
            vul_function = function_name
            origin_func_name = data[2]

        elif isinstance(object, php.Class):
            class_params = data[2]
            class_name = object.name
            param = data[1]
            index = 0

            for class_param in class_params:
                if class_param.name == param.name:
                    break
                index += 1

            # $A = new a($x, $y);
            match = ""new\s*"" + class_name + ""\s*\(""

            for i in range(len(class_params)):
                if i != 0:
                    match += "",""

                    if class_params[i].default is not None:
                        match += ""?""

                if i == index:
                    match += ""([^,\)]*)""
                else:
                    match += ""[^,\)]*""

            match += ""\)""

            # 鍘婚櫎瀹氫箟绫伙紝绫诲畾涔夊拰璋冪敤鏂瑰紡涓嶄竴鏍凤紝浣嗘槸涓轰簡涓嶅奖鍝嶇粨鏋勶紝渚濈劧璧嬪
            match2 = ""class\s+"" + class_name + ""\s*{""
            vul_function = class_name
            origin_func_name = data[3]

    except:
        logger.error('[New Rule] Error to unpack function param, Something error')
        traceback.print_exc()
        match = None
        match2 = None
        index = 0
        vul_function = None
        origin_func_name = ""None""

    return match, match2, vul_function, index, origin_func_name",i != 0,i
caffe-tensorflow,https://github.com/ethereon/caffe-tensorflow/tree/master/kaffe/tensorflow/network.py,,layer_decorated$10,"def layer_decorated(self, *args, **kwargs):
        # Automatically set a name if not provided.
        name = kwargs.setdefault('name', self.get_unique_name(op.__name__))
        # Figure out the layer inputs.
        if len(self.terminals) == 0:
            raise RuntimeError('No input variables found for layer %s.' % name)
        elif len(self.terminals) == 1:
            layer_input = self.terminals[0]
        else:
            layer_input = list(self.terminals)
        # Perform the operation and get the output.
        layer_output = op(self, layer_input, *args, **kwargs)
        # Add to layer LUT.
        self.layers[name] = layer_output
        # This output is now the input for the next layer.
        self.feed(layer_output)
        # Return self for chained calls.
        return self",len(self.terminals) == 0,not self.terminals
salt,https://github.com/saltstack/salt/tree/master/salt/modules/dracr.py,,get_general$1378,"def get_general(cfg_sec, cfg_var, host=None, admin_username=None, admin_password=None):
    ret = __execute_ret(
        ""getconfig -g {} -o {}"".format(cfg_sec, cfg_var),
        host=host,
        admin_username=admin_username,
        admin_password=admin_password,
    )

    if ret[""retcode""] == 0:
        return ret[""stdout""]
    else:
        return ret",ret['retcode'] == 0,not ret['retcode']
RTA,https://github.com/endgameinc/RTA/tree/master/red_ttp/common.py,,find_remote_host$242,"def find_remote_host():
    log(""Searching for remote Windows hosts"")
    _, stdout = execute(""net view"", hide_log=True)
    hosts = re.findall(r""\\\\([\w\d\._-]+)"", stdout)

    # _, current_user = execute(""whoami"", hide_log=True)
    pending = {}

    log(""Discovery %d possible hosts"" % len(hosts))
    for name in hosts[:MAX_HOSTS]:
        name = name.lower()
        if name.split('.')[0] == HOSTNAME.split('.')[0]:
            continue

        # log(""Checking if %s has remote admin permissions to %s"" % (current_user, name))
        dev_null = open(os.devnull, ""w"")
        p = subprocess.Popen('sc.exe \\\\%s query' % name,
                             stdout=dev_null,
                             stderr=dev_null,
                             stdin=subprocess.PIPE)
        pending[name] = p

    if len(pending) > 0:
        # See which ones return first with a success code, and use that host
        for _ in xrange(20):
            for hostname, pending_process in sorted(pending.items()):
                if pending_process.poll() is None:
                    pending_process.stdin.write(os.linesep)
                if pending_process.returncode == 0:
                    # Now need to get the IP address
                    ip = get_ipv4_address(hostname)
                    if ip is not None:
                        log('Using remote host %s (%s)' % (ip, hostname))
                        return ip
                    pending.pop(hostname)
            time.sleep(0.5)

    log(""Unable to find a remote host to pivot to. Using local host %s"" % HOSTNAME, log_type=""!"")
    return LOCAL_IP",pending_process.returncode == 0,not pending_process.returncode
LightAutoML,https://github.com/sberbank-ai-lab/LightAutoML/tree/master/lightautoml/addons/uplift/metalearners.py,XLearner,__init__$657,"def __init__(
        self,
        outcome_learners: Optional[Sequence[AutoML]] = None,
        effect_learners: Optional[Sequence[AutoML]] = None,
        propensity_learner: Optional[AutoML] = None,
        base_task: Optional[Task] = None,
        timeout: Optional[int] = None,
        cpu_limit: int = 4,
        gpu_ids: Optional[str] = ""all"",
    ):
        if (outcome_learners is None or len(outcome_learners) == 0) and base_task is None:
            raise RuntimeError('Must specify any of learners or ""base_task""')

        if outcome_learners is not None and len(outcome_learners) > 0:
            base_task = self._get_task(outcome_learners[0])
            super().__init__(self._get_task(outcome_learners[0]))

        super().__init__(base_task, timeout, cpu_limit, gpu_ids)

        self.learners: Dict[str, Union[Dict[str, AutoML], AutoML]] = {
            ""outcome"": {},
            ""effect"": {},
        }
        if propensity_learner is None:
            self.learners[""propensity""] = self._get_default_learner(Task(""binary""))
        else:
            self.learners[""propensity""] = propensity_learner

        if outcome_learners is None or len(outcome_learners) == 0:
            self.learners[""outcome""][""control""] = self._get_default_learner(self.base_task)
            self.learners[""outcome""][""treatment""] = self._get_default_learner(self.base_task)
        elif len(outcome_learners) == 1:
            self.learners[""outcome""][""control""] = outcome_learners[0]
            self.learners[""outcome""][""treatment""] = copy.deepcopy(outcome_learners[0])
        elif len(outcome_learners) == 2:
            self.learners[""outcome""][""control""] = outcome_learners[0]
            self.learners[""outcome""][""treatment""] = outcome_learners[1]
        else:
            raise RuntimeError('The number of ""outcome_learners"" must be 0/1/2')

        if effect_learners is None or len(effect_learners) == 0:
            self.learners[""effect""][""control""] = self._get_default_learner(Task(""reg""))
            self.learners[""effect""][""treatment""] = self._get_default_learner(Task(""reg""))
        elif len(effect_learners) == 1:
            self.learners[""effect""][""control""] = effect_learners[0]
            self.learners[""effect""][""treatment""] = copy.deepcopy(effect_learners[0])
        elif len(effect_learners) == 2:
            self.learners[""effect""][""control""] = effect_learners[0]
            self.learners[""effect""][""treatment""] = effect_learners[1]
        else:
            raise RuntimeError('The number of ""effect_learners"" must be 0/1/2')",len(outcome_learners) == 0,not outcome_learners
LightAutoML,https://github.com/sberbank-ai-lab/LightAutoML/tree/master/lightautoml/addons/uplift/metalearners.py,XLearner,__init__$657,"def __init__(
        self,
        outcome_learners: Optional[Sequence[AutoML]] = None,
        effect_learners: Optional[Sequence[AutoML]] = None,
        propensity_learner: Optional[AutoML] = None,
        base_task: Optional[Task] = None,
        timeout: Optional[int] = None,
        cpu_limit: int = 4,
        gpu_ids: Optional[str] = ""all"",
    ):
        if (outcome_learners is None or len(outcome_learners) == 0) and base_task is None:
            raise RuntimeError('Must specify any of learners or ""base_task""')

        if outcome_learners is not None and len(outcome_learners) > 0:
            base_task = self._get_task(outcome_learners[0])
            super().__init__(self._get_task(outcome_learners[0]))

        super().__init__(base_task, timeout, cpu_limit, gpu_ids)

        self.learners: Dict[str, Union[Dict[str, AutoML], AutoML]] = {
            ""outcome"": {},
            ""effect"": {},
        }
        if propensity_learner is None:
            self.learners[""propensity""] = self._get_default_learner(Task(""binary""))
        else:
            self.learners[""propensity""] = propensity_learner

        if outcome_learners is None or len(outcome_learners) == 0:
            self.learners[""outcome""][""control""] = self._get_default_learner(self.base_task)
            self.learners[""outcome""][""treatment""] = self._get_default_learner(self.base_task)
        elif len(outcome_learners) == 1:
            self.learners[""outcome""][""control""] = outcome_learners[0]
            self.learners[""outcome""][""treatment""] = copy.deepcopy(outcome_learners[0])
        elif len(outcome_learners) == 2:
            self.learners[""outcome""][""control""] = outcome_learners[0]
            self.learners[""outcome""][""treatment""] = outcome_learners[1]
        else:
            raise RuntimeError('The number of ""outcome_learners"" must be 0/1/2')

        if effect_learners is None or len(effect_learners) == 0:
            self.learners[""effect""][""control""] = self._get_default_learner(Task(""reg""))
            self.learners[""effect""][""treatment""] = self._get_default_learner(Task(""reg""))
        elif len(effect_learners) == 1:
            self.learners[""effect""][""control""] = effect_learners[0]
            self.learners[""effect""][""treatment""] = copy.deepcopy(effect_learners[0])
        elif len(effect_learners) == 2:
            self.learners[""effect""][""control""] = effect_learners[0]
            self.learners[""effect""][""treatment""] = effect_learners[1]
        else:
            raise RuntimeError('The number of ""effect_learners"" must be 0/1/2')",len(effect_learners) == 0,not effect_learners
LightAutoML,https://github.com/sberbank-ai-lab/LightAutoML/tree/master/lightautoml/addons/uplift/metalearners.py,XLearner,__init__$657,"def __init__(
        self,
        outcome_learners: Optional[Sequence[AutoML]] = None,
        effect_learners: Optional[Sequence[AutoML]] = None,
        propensity_learner: Optional[AutoML] = None,
        base_task: Optional[Task] = None,
        timeout: Optional[int] = None,
        cpu_limit: int = 4,
        gpu_ids: Optional[str] = ""all"",
    ):
        if (outcome_learners is None or len(outcome_learners) == 0) and base_task is None:
            raise RuntimeError('Must specify any of learners or ""base_task""')

        if outcome_learners is not None and len(outcome_learners) > 0:
            base_task = self._get_task(outcome_learners[0])
            super().__init__(self._get_task(outcome_learners[0]))

        super().__init__(base_task, timeout, cpu_limit, gpu_ids)

        self.learners: Dict[str, Union[Dict[str, AutoML], AutoML]] = {
            ""outcome"": {},
            ""effect"": {},
        }
        if propensity_learner is None:
            self.learners[""propensity""] = self._get_default_learner(Task(""binary""))
        else:
            self.learners[""propensity""] = propensity_learner

        if outcome_learners is None or len(outcome_learners) == 0:
            self.learners[""outcome""][""control""] = self._get_default_learner(self.base_task)
            self.learners[""outcome""][""treatment""] = self._get_default_learner(self.base_task)
        elif len(outcome_learners) == 1:
            self.learners[""outcome""][""control""] = outcome_learners[0]
            self.learners[""outcome""][""treatment""] = copy.deepcopy(outcome_learners[0])
        elif len(outcome_learners) == 2:
            self.learners[""outcome""][""control""] = outcome_learners[0]
            self.learners[""outcome""][""treatment""] = outcome_learners[1]
        else:
            raise RuntimeError('The number of ""outcome_learners"" must be 0/1/2')

        if effect_learners is None or len(effect_learners) == 0:
            self.learners[""effect""][""control""] = self._get_default_learner(Task(""reg""))
            self.learners[""effect""][""treatment""] = self._get_default_learner(Task(""reg""))
        elif len(effect_learners) == 1:
            self.learners[""effect""][""control""] = effect_learners[0]
            self.learners[""effect""][""treatment""] = copy.deepcopy(effect_learners[0])
        elif len(effect_learners) == 2:
            self.learners[""effect""][""control""] = effect_learners[0]
            self.learners[""effect""][""treatment""] = effect_learners[1]
        else:
            raise RuntimeError('The number of ""effect_learners"" must be 0/1/2')",len(outcome_learners) == 0,not outcome_learners
pennylane,https://github.com/PennyLaneAI/pennylane/tree/master/tests/ops/qubit/test_observables.py,TestProjector,test_projector_diagonalization$435,"def test_projector_diagonalization(self, basis_state, tol):
        """"""Test that the projector has an empty list of diagonalizing gates.""""""
        num_wires = len(basis_state)
        diag_gates = qml.Projector(basis_state, wires=range(num_wires)).diagonalizing_gates()
        assert diag_gates == []

        diag_gates_static = qml.Projector.compute_diagonalizing_gates(
            basis_state, wires=range(num_wires)
        )
        assert diag_gates_static == []",diag_gates == [],not diag_gates
pennylane,https://github.com/PennyLaneAI/pennylane/tree/master/tests/ops/qubit/test_observables.py,TestProjector,test_projector_diagonalization$435,"def test_projector_diagonalization(self, basis_state, tol):
        """"""Test that the projector has an empty list of diagonalizing gates.""""""
        num_wires = len(basis_state)
        diag_gates = qml.Projector(basis_state, wires=range(num_wires)).diagonalizing_gates()
        assert diag_gates == []

        diag_gates_static = qml.Projector.compute_diagonalizing_gates(
            basis_state, wires=range(num_wires)
        )
        assert diag_gates_static == []",diag_gates_static == [],not diag_gates_static
scipy,https://github.com/scipy/scipy/tree/master/scipy/signal/_filter_design.py,,zpk2sos$1280,"def zpk2sos(z, p, k, pairing=None, *, analog=False):
    """"""Return second-order sections from zeros, poles, and gain of a system

    Parameters
    ----------
    z : array_like
        Zeros of the transfer function.
    p : array_like
        Poles of the transfer function.
    k : float
        System gain.
    pairing : {None, 'nearest', 'keep_odd', 'minimal'}, optional
        The method to use to combine pairs of poles and zeros into sections.
        If analog is False and pairing is None, pairing is set to 'nearest';
        if analog is True, pairing must be 'minimal', and is set to that if
        it is None.
    analog : bool, optional
        If True, system is analog, otherwise discrete.

        .. versionadded:: 1.8.0

    Returns
    -------
    sos : ndarray
        Array of second-order filter coefficients, with shape
        ``(n_sections, 6)``. See `sosfilt` for the SOS filter format
        specification.

    See Also
    --------
    sosfilt

    Notes
    -----
    The algorithm used to convert ZPK to SOS format is designed to
    minimize errors due to numerical precision issues. The pairing
    algorithm attempts to minimize the peak gain of each biquadratic
    section. This is done by pairing poles with the nearest zeros, starting
    with the poles closest to the unit circle for discrete-time systems, and
    poles closest to the imaginary axis for continuous-time systems.

    ``pairing='minimal'`` outputs may not be suitable for `sosfilt`,
    and ``analog=True`` outputs will never be suitable for `sosfilt`.

    *Algorithms*

    The steps in the ``pairing='nearest'``, ``pairing='keep_odd'``,
    and ``pairing='minimal'`` algorithms are mostly shared. The
    ``'nearest'`` algorithm attempts to minimize the peak gain, while
    ``'keep_odd'`` minimizes peak gain under the constraint that
    odd-order systems should retain one section as first order.
    ``'minimal'`` is similar to ``'keep_odd'``, but no additional
    poles or zeros are introduced

    The algorithm steps are as follows:

    As a pre-processing step for ``pairing='nearest'``,
    ``pairing='keep_odd'``, add poles or zeros to the origin as
    necessary to obtain the same number of poles and zeros for
    pairing.  If ``pairing == 'nearest'`` and there are an odd number
    of poles, add an additional pole and a zero at the origin.

    The following steps are then iterated over until no more poles or
    zeros remain:

    1. Take the (next remaining) pole (complex or real) closest to the
       unit circle (or imaginary axis, for ``analog=True``) to
       begin a new filter section.

    2. If the pole is real and there are no other remaining real poles [#]_,
       add the closest real zero to the section and leave it as a first
       order section. Note that after this step we are guaranteed to be
       left with an even number of real poles, complex poles, real zeros,
       and complex zeros for subsequent pairing iterations.

    3. Else:

        1. If the pole is complex and the zero is the only remaining real
           zero*, then pair the pole with the *next* closest zero
           (guaranteed to be complex). This is necessary to ensure that
           there will be a real zero remaining to eventually create a
           first-order section (thus keeping the odd order).

        2. Else pair the pole with the closest remaining zero (complex or
           real).

        3. Proceed to complete the second-order section by adding another
           pole and zero to the current pole and zero in the section:

            1. If the current pole and zero are both complex, add their
               conjugates.

            2. Else if the pole is complex and the zero is real, add the
               conjugate pole and the next closest real zero.

            3. Else if the pole is real and the zero is complex, add the
               conjugate zero and the real pole closest to those zeros.

            4. Else (we must have a real pole and real zero) add the next
               real pole closest to the unit circle, and then add the real
               zero closest to that pole.

    .. [#] This conditional can only be met for specific odd-order inputs
           with the ``pairing = 'keep_odd'`` or ``'minimal'`` methods.

    .. versionadded:: 0.16.0

    Examples
    --------

    Design a 6th order low-pass elliptic digital filter for a system with a
    sampling rate of 8000 Hz that has a pass-band corner frequency of
    1000 Hz. The ripple in the pass-band should not exceed 0.087 dB, and
    the attenuation in the stop-band should be at least 90 dB.

    In the following call to `ellip`, we could use ``output='sos'``,
    but for this example, we'll use ``output='zpk'``, and then convert
    to SOS format with `zpk2sos`:

    >>> from scipy import signal
    >>> z, p, k = signal.ellip(6, 0.087, 90, 1000/(0.5*8000), output='zpk')

    Now convert to SOS format.

    >>> sos = signal.zpk2sos(z, p, k)

    The coefficients of the numerators of the sections:

    >>> sos[:, :3]
    array([[ 0.0014154 ,  0.00248707,  0.0014154 ],
           [ 1.        ,  0.72965193,  1.        ],
           [ 1.        ,  0.17594966,  1.        ]])

    The symmetry in the coefficients occurs because all the zeros are on the
    unit circle.

    The coefficients of the denominators of the sections:

    >>> sos[:, 3:]
    array([[ 1.        , -1.32543251,  0.46989499],
           [ 1.        , -1.26117915,  0.6262586 ],
           [ 1.        , -1.25707217,  0.86199667]])

    The next example shows the effect of the `pairing` option.  We have a
    system with three poles and three zeros, so the SOS array will have
    shape (2, 6). The means there is, in effect, an extra pole and an extra
    zero at the origin in the SOS representation.

    >>> z1 = np.array([-1, -0.5-0.5j, -0.5+0.5j])
    >>> p1 = np.array([0.75, 0.8+0.1j, 0.8-0.1j])

    With ``pairing='nearest'`` (the default), we obtain

    >>> signal.zpk2sos(z1, p1, 1)
    array([[ 1.  ,  1.  ,  0.5 ,  1.  , -0.75,  0.  ],
           [ 1.  ,  1.  ,  0.  ,  1.  , -1.6 ,  0.65]])

    The first section has the zeros {-0.5-0.05j, -0.5+0.5j} and the poles
    {0, 0.75}, and the second section has the zeros {-1, 0} and poles
    {0.8+0.1j, 0.8-0.1j}. Note that the extra pole and zero at the origin
    have been assigned to different sections.

    With ``pairing='keep_odd'``, we obtain:

    >>> signal.zpk2sos(z1, p1, 1, pairing='keep_odd')
    array([[ 1.  ,  1.  ,  0.  ,  1.  , -0.75,  0.  ],
           [ 1.  ,  1.  ,  0.5 ,  1.  , -1.6 ,  0.65]])

    The extra pole and zero at the origin are in the same section.
    The first section is, in effect, a first-order section.

    With ``pairing='minimal'``, the first-order section doesn't have
    the extra pole and zero at the origin:

    >>> signal.zpk2sos(z1, p1, 1, pairing='minimal')
    array([[ 0.  ,  1.  ,  1.  ,  0.  ,  1.  , -0.75],
           [ 1.  ,  1.  ,  0.5 ,  1.  , -1.6 ,  0.65]])

    """"""
    # TODO in the near future:
    # 1. Add SOS capability to `filtfilt`, `freqz`, etc. somehow (#3259).
    # 2. Make `decimate` use `sosfilt` instead of `lfilter`.
    # 3. Make sosfilt automatically simplify sections to first order
    #    when possible. Note this might make `sosfiltfilt` a bit harder (ICs).
    # 4. Further optimizations of the section ordering / pole-zero pairing.
    # See the wiki for other potential issues.

    if pairing is None:
        pairing = 'minimal' if analog else 'nearest'

    valid_pairings = ['nearest', 'keep_odd', 'minimal']
    if pairing not in valid_pairings:
        raise ValueError('pairing must be one of %s, not %s'
                         % (valid_pairings, pairing))

    if analog and pairing != 'minimal':
        raise ValueError('for analog zpk2sos conversion, '
                         'pairing must be ""minimal""')

    if len(z) == len(p) == 0:
        if not analog:
            return np.array([[k, 0., 0., 1., 0., 0.]])
        else:
            return np.array([[0., 0., k, 0., 0., 1.]])

    if pairing != 'minimal':
        # ensure we have the same number of poles and zeros, and make copies
        p = np.concatenate((p, np.zeros(max(len(z) - len(p), 0))))
        z = np.concatenate((z, np.zeros(max(len(p) - len(z), 0))))
        n_sections = (max(len(p), len(z)) + 1) // 2

        if len(p) % 2 == 1 and pairing == 'nearest':
            p = np.concatenate((p, [0.]))
            z = np.concatenate((z, [0.]))
        assert len(p) == len(z)
    else:
        if len(p) < len(z):
            raise ValueError('for analog zpk2sos conversion, '
                             'must have len(p)>=len(z)')

        n_sections = (len(p) + 1) // 2

    # Ensure we have complex conjugate pairs
    # (note that _cplxreal only gives us one element of each complex pair):
    z = np.concatenate(_cplxreal(z))
    p = np.concatenate(_cplxreal(p))
    if not np.isreal(k):
        raise ValueError('k must be real')
    k = k.real

    if not analog:
        # digital: ""worst"" is the closest to the unit circle
        def idx_worst(p):
            return np.argmin(np.abs(1 - np.abs(p)))
    else:
        # analog: ""worst"" is the closest to the imaginary axis
        def idx_worst(p):
            return np.argmin(np.abs(np.real(p)))

    sos = np.zeros((n_sections, 6))

    # Construct the system, reversing order so the ""worst"" are last
    for si in range(n_sections-1, -1, -1):
        # Select the next ""worst"" pole
        p1_idx = idx_worst(p)
        p1 = p[p1_idx]
        p = np.delete(p, p1_idx)

        # Pair that pole with a zero

        if np.isreal(p1) and np.isreal(p).sum() == 0:
            # Special case (1): last remaining real pole
            if pairing != 'minimal':
                z1_idx = _nearest_real_complex_idx(z, p1, 'real')
                z1 = z[z1_idx]
                z = np.delete(z, z1_idx)
                sos[si] = _single_zpksos([z1, 0], [p1, 0], 1)
            elif len(z) > 0:
                z1_idx = _nearest_real_complex_idx(z, p1, 'real')
                z1 = z[z1_idx]
                z = np.delete(z, z1_idx)
                sos[si] = _single_zpksos([z1], [p1], 1)
            else:
                sos[si] = _single_zpksos([], [p1], 1)

        elif (len(p) + 1 == len(z)
              and not np.isreal(p1)
              and np.isreal(p).sum() == 1
              and np.isreal(z).sum() == 1):

            # Special case (2): there's one real pole and one real zero
            # left, and an equal number of poles and zeros to pair up.
            # We *must* pair with a complex zero

            z1_idx = _nearest_real_complex_idx(z, p1, 'complex')
            z1 = z[z1_idx]
            z = np.delete(z, z1_idx)
            sos[si] = _single_zpksos([z1, z1.conj()], [p1, p1.conj()], 1)

        else:
            if np.isreal(p1):
                prealidx = np.flatnonzero(np.isreal(p))
                p2_idx = prealidx[idx_worst(p[prealidx])]
                p2 = p[p2_idx]
                p = np.delete(p, p2_idx)
            else:
                p2 = p1.conj()

            # find closest zero
            if len(z) > 0:
                z1_idx = _nearest_real_complex_idx(z, p1, 'any')
                z1 = z[z1_idx]
                z = np.delete(z, z1_idx)

                if not np.isreal(z1):
                    sos[si] = _single_zpksos([z1, z1.conj()], [p1, p2], 1)
                else:
                    if len(z) > 0:
                        z2_idx = _nearest_real_complex_idx(z, p1, 'real')
                        z2 = z[z2_idx]
                        assert np.isreal(z2)
                        z = np.delete(z, z2_idx)
                        sos[si] = _single_zpksos([z1, z2], [p1, p2], 1)
                    else:
                        sos[si] = _single_zpksos([z1], [p1, p2], 1)
            else:
                # no more zeros
                sos[si] = _single_zpksos([], [p1, p2], 1)

    assert len(p) == len(z) == 0  # we've consumed all poles and zeros
    del p, z

    # put gain in first sos
    sos[0][:3] *= k
    return sos",np.isreal(p).sum() == 0,not np.isreal(p).sum()
R-Drop,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/mobilebert/modeling_tf_mobilebert.py,TFMobileBertSelfAttention,__init__$209,"def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError(
                f""The hidden size ({config.hidden_size}) is not a multiple of the number of attention ""
                f""heads ({config.num_attention_heads}""
            )

        self.num_attention_heads = config.num_attention_heads
        self.output_attentions = config.output_attentions
        assert config.hidden_size % config.num_attention_heads == 0
        self.attention_head_size = int(config.true_hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=""query""
        )
        self.key = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=""key""
        )
        self.value = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=""value""
        )

        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)",config.hidden_size % config.num_attention_heads != 0,config.hidden_size % config.num_attention_heads
R-Drop,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/mobilebert/modeling_tf_mobilebert.py,TFMobileBertSelfAttention,__init__$209,"def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError(
                f""The hidden size ({config.hidden_size}) is not a multiple of the number of attention ""
                f""heads ({config.num_attention_heads}""
            )

        self.num_attention_heads = config.num_attention_heads
        self.output_attentions = config.output_attentions
        assert config.hidden_size % config.num_attention_heads == 0
        self.attention_head_size = int(config.true_hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=""query""
        )
        self.key = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=""key""
        )
        self.value = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=""value""
        )

        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)",config.hidden_size % config.num_attention_heads == 0,not config.hidden_size % config.num_attention_heads
interfacegan,https://github.com/genforce/interfacegan/tree/master/models/pggan_tf_official/train.py,,train_progressive_gan$133,"def train_progressive_gan(
    G_smoothing             = 0.999,        # Exponential running average of generator weights.
    D_repeats               = 1,            # How many times the discriminator is trained per G iteration.
    minibatch_repeats       = 4,            # Number of minibatches to run before adjusting training parameters.
    reset_opt_for_new_lod   = True,         # Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?
    total_kimg              = 15000,        # Total length of the training, measured in thousands of real images.
    mirror_augment          = False,        # Enable mirror augment?
    drange_net              = [-1,1],       # Dynamic range used when feeding image data to the networks.
    image_snapshot_ticks    = 1,            # How often to export image snapshots?
    network_snapshot_ticks  = 10,           # How often to export network snapshots?
    save_tf_graph           = False,        # Include full TensorFlow computation graph in the tfevents file?
    save_weight_histograms  = False,        # Include weight histograms in the tfevents file?
    resume_run_id           = None,         # Run ID or network pkl to resume training from, None = start from scratch.
    resume_snapshot         = None,         # Snapshot index to resume training from, None = autodetect.
    resume_kimg             = 0.0,          # Assumed training progress at the beginning. Affects reporting and training schedule.
    resume_time             = 0.0):         # Assumed wallclock time at the beginning. Affects reporting.

    maintenance_start_time = time.time()
    training_set = dataset.load_dataset(data_dir=config.data_dir, verbose=True, **config.dataset)

    # Construct networks.
    with tf.device('/gpu:0'):
        if resume_run_id is not None:
            network_pkl = misc.locate_network_pkl(resume_run_id, resume_snapshot)
            print('Loading networks from ""%s""...' % network_pkl)
            G, D, Gs = misc.load_pkl(network_pkl)
        else:
            print('Constructing networks...')
            G = tfutil.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)
            D = tfutil.Network('D', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.D)
            Gs = G.clone('Gs')
        Gs_update_op = Gs.setup_as_moving_average_of(G, beta=G_smoothing)
    G.print_layers(); D.print_layers()

    print('Building TensorFlow graph...')
    with tf.name_scope('Inputs'):
        lod_in          = tf.placeholder(tf.float32, name='lod_in', shape=[])
        lrate_in        = tf.placeholder(tf.float32, name='lrate_in', shape=[])
        minibatch_in    = tf.placeholder(tf.int32, name='minibatch_in', shape=[])
        minibatch_split = minibatch_in // config.num_gpus
        reals, labels   = training_set.get_minibatch_tf()
        reals_split     = tf.split(reals, config.num_gpus)
        labels_split    = tf.split(labels, config.num_gpus)
    G_opt = tfutil.Optimizer(name='TrainG', learning_rate=lrate_in, **config.G_opt)
    D_opt = tfutil.Optimizer(name='TrainD', learning_rate=lrate_in, **config.D_opt)
    for gpu in range(config.num_gpus):
        with tf.name_scope('GPU%d' % gpu), tf.device('/gpu:%d' % gpu):
            G_gpu = G if gpu == 0 else G.clone(G.name + '_shadow')
            D_gpu = D if gpu == 0 else D.clone(D.name + '_shadow')
            lod_assign_ops = [tf.assign(G_gpu.find_var('lod'), lod_in), tf.assign(D_gpu.find_var('lod'), lod_in)]
            reals_gpu = process_reals(reals_split[gpu], lod_in, mirror_augment, training_set.dynamic_range, drange_net)
            labels_gpu = labels_split[gpu]
            with tf.name_scope('G_loss'), tf.control_dependencies(lod_assign_ops):
                G_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=G_opt, training_set=training_set, minibatch_size=minibatch_split, **config.G_loss)
            with tf.name_scope('D_loss'), tf.control_dependencies(lod_assign_ops):
                D_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=D_opt, training_set=training_set, minibatch_size=minibatch_split, reals=reals_gpu, labels=labels_gpu, **config.D_loss)
            G_opt.register_gradients(tf.reduce_mean(G_loss), G_gpu.trainables)
            D_opt.register_gradients(tf.reduce_mean(D_loss), D_gpu.trainables)
    G_train_op = G_opt.apply_updates()
    D_train_op = D_opt.apply_updates()

    print('Setting up snapshot image grid...')
    grid_size, grid_reals, grid_labels, grid_latents = setup_snapshot_image_grid(G, training_set, **config.grid)
    sched = TrainingSchedule(total_kimg * 1000, training_set, **config.sched)
    grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)

    print('Setting up result dir...')
    result_subdir = misc.create_result_subdir(config.result_dir, config.desc)
    misc.save_image_grid(grid_reals, os.path.join(result_subdir, 'reals.png'), drange=training_set.dynamic_range, grid_size=grid_size)
    misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % 0), drange=drange_net, grid_size=grid_size)
    summary_log = tf.summary.FileWriter(result_subdir)
    if save_tf_graph:
        summary_log.add_graph(tf.get_default_graph())
    if save_weight_histograms:
        G.setup_weight_histograms(); D.setup_weight_histograms()

    print('Training...')
    cur_nimg = int(resume_kimg * 1000)
    cur_tick = 0
    tick_start_nimg = cur_nimg
    tick_start_time = time.time()
    train_start_time = tick_start_time - resume_time
    prev_lod = -1.0
    while cur_nimg < total_kimg * 1000:

        # Choose training parameters and configure training ops.
        sched = TrainingSchedule(cur_nimg, training_set, **config.sched)
        training_set.configure(sched.minibatch, sched.lod)
        if reset_opt_for_new_lod:
            if np.floor(sched.lod) != np.floor(prev_lod) or np.ceil(sched.lod) != np.ceil(prev_lod):
                G_opt.reset_optimizer_state(); D_opt.reset_optimizer_state()
        prev_lod = sched.lod

        # Run training ops.
        for repeat in range(minibatch_repeats):
            for _ in range(D_repeats):
                tfutil.run([D_train_op, Gs_update_op], {lod_in: sched.lod, lrate_in: sched.D_lrate, minibatch_in: sched.minibatch})
                cur_nimg += sched.minibatch
            tfutil.run([G_train_op], {lod_in: sched.lod, lrate_in: sched.G_lrate, minibatch_in: sched.minibatch})

        # Perform maintenance tasks once per tick.
        done = (cur_nimg >= total_kimg * 1000)
        if cur_nimg >= tick_start_nimg + sched.tick_kimg * 1000 or done:
            cur_tick += 1
            cur_time = time.time()
            tick_kimg = (cur_nimg - tick_start_nimg) / 1000.0
            tick_start_nimg = cur_nimg
            tick_time = cur_time - tick_start_time
            total_time = cur_time - train_start_time
            maintenance_time = tick_start_time - maintenance_start_time
            maintenance_start_time = cur_time

            # Report progress.
            print('tick %-5d kimg %-8.1f lod %-5.2f minibatch %-4d time %-12s sec/tick %-7.1f sec/kimg %-7.2f maintenance %.1f' % (
                tfutil.autosummary('Progress/tick', cur_tick),
                tfutil.autosummary('Progress/kimg', cur_nimg / 1000.0),
                tfutil.autosummary('Progress/lod', sched.lod),
                tfutil.autosummary('Progress/minibatch', sched.minibatch),
                misc.format_time(tfutil.autosummary('Timing/total_sec', total_time)),
                tfutil.autosummary('Timing/sec_per_tick', tick_time),
                tfutil.autosummary('Timing/sec_per_kimg', tick_time / tick_kimg),
                tfutil.autosummary('Timing/maintenance_sec', maintenance_time)))
            tfutil.autosummary('Timing/total_hours', total_time / (60.0 * 60.0))
            tfutil.autosummary('Timing/total_days', total_time / (24.0 * 60.0 * 60.0))
            tfutil.save_summaries(summary_log, cur_nimg)

            # Save snapshots.
            if cur_tick % image_snapshot_ticks == 0 or done:
                grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)
                misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % (cur_nimg // 1000)), drange=drange_net, grid_size=grid_size)
            if cur_tick % network_snapshot_ticks == 0 or done:
                misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-snapshot-%06d.pkl' % (cur_nimg // 1000)))

            # Record start time of the next tick.
            tick_start_time = time.time()

    # Write final results.
    misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-final.pkl'))
    summary_log.close()
    open(os.path.join(result_subdir, '_training-done.txt'), 'wt').close()",cur_tick % image_snapshot_ticks == 0,not cur_tick % image_snapshot_ticks
interfacegan,https://github.com/genforce/interfacegan/tree/master/models/pggan_tf_official/train.py,,train_progressive_gan$133,"def train_progressive_gan(
    G_smoothing             = 0.999,        # Exponential running average of generator weights.
    D_repeats               = 1,            # How many times the discriminator is trained per G iteration.
    minibatch_repeats       = 4,            # Number of minibatches to run before adjusting training parameters.
    reset_opt_for_new_lod   = True,         # Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?
    total_kimg              = 15000,        # Total length of the training, measured in thousands of real images.
    mirror_augment          = False,        # Enable mirror augment?
    drange_net              = [-1,1],       # Dynamic range used when feeding image data to the networks.
    image_snapshot_ticks    = 1,            # How often to export image snapshots?
    network_snapshot_ticks  = 10,           # How often to export network snapshots?
    save_tf_graph           = False,        # Include full TensorFlow computation graph in the tfevents file?
    save_weight_histograms  = False,        # Include weight histograms in the tfevents file?
    resume_run_id           = None,         # Run ID or network pkl to resume training from, None = start from scratch.
    resume_snapshot         = None,         # Snapshot index to resume training from, None = autodetect.
    resume_kimg             = 0.0,          # Assumed training progress at the beginning. Affects reporting and training schedule.
    resume_time             = 0.0):         # Assumed wallclock time at the beginning. Affects reporting.

    maintenance_start_time = time.time()
    training_set = dataset.load_dataset(data_dir=config.data_dir, verbose=True, **config.dataset)

    # Construct networks.
    with tf.device('/gpu:0'):
        if resume_run_id is not None:
            network_pkl = misc.locate_network_pkl(resume_run_id, resume_snapshot)
            print('Loading networks from ""%s""...' % network_pkl)
            G, D, Gs = misc.load_pkl(network_pkl)
        else:
            print('Constructing networks...')
            G = tfutil.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)
            D = tfutil.Network('D', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.D)
            Gs = G.clone('Gs')
        Gs_update_op = Gs.setup_as_moving_average_of(G, beta=G_smoothing)
    G.print_layers(); D.print_layers()

    print('Building TensorFlow graph...')
    with tf.name_scope('Inputs'):
        lod_in          = tf.placeholder(tf.float32, name='lod_in', shape=[])
        lrate_in        = tf.placeholder(tf.float32, name='lrate_in', shape=[])
        minibatch_in    = tf.placeholder(tf.int32, name='minibatch_in', shape=[])
        minibatch_split = minibatch_in // config.num_gpus
        reals, labels   = training_set.get_minibatch_tf()
        reals_split     = tf.split(reals, config.num_gpus)
        labels_split    = tf.split(labels, config.num_gpus)
    G_opt = tfutil.Optimizer(name='TrainG', learning_rate=lrate_in, **config.G_opt)
    D_opt = tfutil.Optimizer(name='TrainD', learning_rate=lrate_in, **config.D_opt)
    for gpu in range(config.num_gpus):
        with tf.name_scope('GPU%d' % gpu), tf.device('/gpu:%d' % gpu):
            G_gpu = G if gpu == 0 else G.clone(G.name + '_shadow')
            D_gpu = D if gpu == 0 else D.clone(D.name + '_shadow')
            lod_assign_ops = [tf.assign(G_gpu.find_var('lod'), lod_in), tf.assign(D_gpu.find_var('lod'), lod_in)]
            reals_gpu = process_reals(reals_split[gpu], lod_in, mirror_augment, training_set.dynamic_range, drange_net)
            labels_gpu = labels_split[gpu]
            with tf.name_scope('G_loss'), tf.control_dependencies(lod_assign_ops):
                G_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=G_opt, training_set=training_set, minibatch_size=minibatch_split, **config.G_loss)
            with tf.name_scope('D_loss'), tf.control_dependencies(lod_assign_ops):
                D_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=D_opt, training_set=training_set, minibatch_size=minibatch_split, reals=reals_gpu, labels=labels_gpu, **config.D_loss)
            G_opt.register_gradients(tf.reduce_mean(G_loss), G_gpu.trainables)
            D_opt.register_gradients(tf.reduce_mean(D_loss), D_gpu.trainables)
    G_train_op = G_opt.apply_updates()
    D_train_op = D_opt.apply_updates()

    print('Setting up snapshot image grid...')
    grid_size, grid_reals, grid_labels, grid_latents = setup_snapshot_image_grid(G, training_set, **config.grid)
    sched = TrainingSchedule(total_kimg * 1000, training_set, **config.sched)
    grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)

    print('Setting up result dir...')
    result_subdir = misc.create_result_subdir(config.result_dir, config.desc)
    misc.save_image_grid(grid_reals, os.path.join(result_subdir, 'reals.png'), drange=training_set.dynamic_range, grid_size=grid_size)
    misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % 0), drange=drange_net, grid_size=grid_size)
    summary_log = tf.summary.FileWriter(result_subdir)
    if save_tf_graph:
        summary_log.add_graph(tf.get_default_graph())
    if save_weight_histograms:
        G.setup_weight_histograms(); D.setup_weight_histograms()

    print('Training...')
    cur_nimg = int(resume_kimg * 1000)
    cur_tick = 0
    tick_start_nimg = cur_nimg
    tick_start_time = time.time()
    train_start_time = tick_start_time - resume_time
    prev_lod = -1.0
    while cur_nimg < total_kimg * 1000:

        # Choose training parameters and configure training ops.
        sched = TrainingSchedule(cur_nimg, training_set, **config.sched)
        training_set.configure(sched.minibatch, sched.lod)
        if reset_opt_for_new_lod:
            if np.floor(sched.lod) != np.floor(prev_lod) or np.ceil(sched.lod) != np.ceil(prev_lod):
                G_opt.reset_optimizer_state(); D_opt.reset_optimizer_state()
        prev_lod = sched.lod

        # Run training ops.
        for repeat in range(minibatch_repeats):
            for _ in range(D_repeats):
                tfutil.run([D_train_op, Gs_update_op], {lod_in: sched.lod, lrate_in: sched.D_lrate, minibatch_in: sched.minibatch})
                cur_nimg += sched.minibatch
            tfutil.run([G_train_op], {lod_in: sched.lod, lrate_in: sched.G_lrate, minibatch_in: sched.minibatch})

        # Perform maintenance tasks once per tick.
        done = (cur_nimg >= total_kimg * 1000)
        if cur_nimg >= tick_start_nimg + sched.tick_kimg * 1000 or done:
            cur_tick += 1
            cur_time = time.time()
            tick_kimg = (cur_nimg - tick_start_nimg) / 1000.0
            tick_start_nimg = cur_nimg
            tick_time = cur_time - tick_start_time
            total_time = cur_time - train_start_time
            maintenance_time = tick_start_time - maintenance_start_time
            maintenance_start_time = cur_time

            # Report progress.
            print('tick %-5d kimg %-8.1f lod %-5.2f minibatch %-4d time %-12s sec/tick %-7.1f sec/kimg %-7.2f maintenance %.1f' % (
                tfutil.autosummary('Progress/tick', cur_tick),
                tfutil.autosummary('Progress/kimg', cur_nimg / 1000.0),
                tfutil.autosummary('Progress/lod', sched.lod),
                tfutil.autosummary('Progress/minibatch', sched.minibatch),
                misc.format_time(tfutil.autosummary('Timing/total_sec', total_time)),
                tfutil.autosummary('Timing/sec_per_tick', tick_time),
                tfutil.autosummary('Timing/sec_per_kimg', tick_time / tick_kimg),
                tfutil.autosummary('Timing/maintenance_sec', maintenance_time)))
            tfutil.autosummary('Timing/total_hours', total_time / (60.0 * 60.0))
            tfutil.autosummary('Timing/total_days', total_time / (24.0 * 60.0 * 60.0))
            tfutil.save_summaries(summary_log, cur_nimg)

            # Save snapshots.
            if cur_tick % image_snapshot_ticks == 0 or done:
                grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)
                misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % (cur_nimg // 1000)), drange=drange_net, grid_size=grid_size)
            if cur_tick % network_snapshot_ticks == 0 or done:
                misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-snapshot-%06d.pkl' % (cur_nimg // 1000)))

            # Record start time of the next tick.
            tick_start_time = time.time()

    # Write final results.
    misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-final.pkl'))
    summary_log.close()
    open(os.path.join(result_subdir, '_training-done.txt'), 'wt').close()",cur_tick % network_snapshot_ticks == 0,not cur_tick % network_snapshot_ticks
molecule,https://github.com/ansible-community/molecule/tree/master/src/molecule/test/unit/command/test_base.py,,test_command_completion$295,"def test_command_completion(shell: str) -> None:
    env = os.environ.copy()
    env[""_MOLECULE_COMPLETE""] = f""{shell}_source""

    if ""bash"" in shell:
        bash_version = util.run_command([""bash"", ""--version""]).stdout.split()[3][0:3]

    result = util.run_command([""molecule""], env=env)

    if ""bash"" in shell and (float(bash_version) < 4.4):
        assert result.returncode == 1
        assert ""Found config file"" not in result.stdout
    else:
        assert result.returncode == 0
        assert ""Found config file"" not in result.stdout",result.returncode == 0,not result.returncode
micropython-lib,https://github.com/micropython/micropython-lib/tree/master/micropython/umqtt.simple/umqtt/simple.py,MQTTClient,connect$65,"def connect(self, clean_session=True):
        self.sock = socket.socket()
        addr = socket.getaddrinfo(self.server, self.port)[0][-1]
        self.sock.connect(addr)
        if self.ssl:
            import ussl

            self.sock = ussl.wrap_socket(self.sock, **self.ssl_params)
        premsg = bytearray(b""\x10\0\0\0\0\0"")
        msg = bytearray(b""\x04MQTT\x04\x02\0\0"")

        sz = 10 + 2 + len(self.client_id)
        msg[6] = clean_session << 1
        if self.user is not None:
            sz += 2 + len(self.user) + 2 + len(self.pswd)
            msg[6] |= 0xC0
        if self.keepalive:
            assert self.keepalive < 65536
            msg[7] |= self.keepalive >> 8
            msg[8] |= self.keepalive & 0x00FF
        if self.lw_topic:
            sz += 2 + len(self.lw_topic) + 2 + len(self.lw_msg)
            msg[6] |= 0x4 | (self.lw_qos & 0x1) << 3 | (self.lw_qos & 0x2) << 3
            msg[6] |= self.lw_retain << 5

        i = 1
        while sz > 0x7F:
            premsg[i] = (sz & 0x7F) | 0x80
            sz >>= 7
            i += 1
        premsg[i] = sz

        self.sock.write(premsg, i + 2)
        self.sock.write(msg)
        # print(hex(len(msg)), hexlify(msg, "":""))
        self._send_str(self.client_id)
        if self.lw_topic:
            self._send_str(self.lw_topic)
            self._send_str(self.lw_msg)
        if self.user is not None:
            self._send_str(self.user)
            self._send_str(self.pswd)
        resp = self.sock.read(4)
        assert resp[0] == 0x20 and resp[1] == 0x02
        if resp[3] != 0:
            raise MQTTException(resp[3])
        return resp[2] & 1",resp[3] != 0,resp[3]
maml_rl,https://github.com/cbfinn/maml_rl/tree/master/rllab/algos/cma_es_lib.py,CMAEvolutionStrategy,feedForResume$4163,"def feedForResume(self, X, function_values):
        """"""Given all ""previous"" candidate solutions and their respective
        function values, the state of a `CMAEvolutionStrategy` object
        can be reconstructed from this history. This is the purpose of
        function `feedForResume`.

        Arguments
        ---------
            `X`
              (all) solution points in chronological order, phenotypic
              representation. The number of points must be a multiple
              of popsize.
            `function_values`
              respective objective function values

        Details
        -------
        `feedForResume` can be called repeatedly with only parts of
        the history. The part must have the length of a multiple
        of the population size.
        `feedForResume` feeds the history in popsize-chunks into `tell`.
        The state of the random number generator might not be
        reconstructed, but this would be only relevant for the future.

        Example
        -------
        ::

            import cma

            # prepare
            (x0, sigma0) = ... # initial values from previous trial
            X = ... # list of generated solutions from a previous trial
            f = ... # respective list of f-values

            # resume
            es = cma.CMAEvolutionStrategy(x0, sigma0)
            es.feedForResume(X, f)

            # continue with func as objective function
            while not es.stop():
               X = es.ask()
               es.tell(X, [func(x) for x in X])

        Credits to Dirk Bueche and Fabrice Marchal for the feeding idea.

        :See: class `CMAEvolutionStrategy` for a simple dump/load to resume

        """"""
        if self.countiter > 0:
            _print_warning('feed should generally be used with a new object instance')
        if len(X) != len(function_values):
            raise _Error('number of solutions ' + str(len(X)) +
                ' and number function values ' +
                str(len(function_values)) + ' must not differ')
        popsize = self.sp.popsize
        if (len(X) % popsize) != 0:
            raise _Error('number of solutions ' + str(len(X)) +
                    ' must be a multiple of popsize (lambda) ' +
                    str(popsize))
        for i in rglen((X) / popsize):
            # feed in chunks of size popsize
            self.ask()  # a fake ask, mainly for a conditioned calling of updateBD
                        # and secondary to get possibly the same random state
            self.tell(X[i * popsize:(i + 1) * popsize], function_values[i * popsize:(i + 1) * popsize])",len(X) % popsize != 0,len(X) % popsize
barlowtwins,https://github.com/facebookresearch/barlowtwins/tree/master//evaluate.py,,main_worker$66,"def main_worker(gpu, args):
    args.rank += gpu
    torch.distributed.init_process_group(
        backend='nccl', init_method=args.dist_url,
        world_size=args.world_size, rank=args.rank)

    if args.rank == 0:
        args.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        stats_file = open(args.checkpoint_dir / 'stats.txt', 'a', buffering=1)
        print(' '.join(sys.argv))
        print(' '.join(sys.argv), file=stats_file)

    torch.cuda.set_device(gpu)
    torch.backends.cudnn.benchmark = True

    model = models.resnet50().cuda(gpu)
    state_dict = torch.load(args.pretrained, map_location='cpu')
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    assert missing_keys == ['fc.weight', 'fc.bias'] and unexpected_keys == []
    model.fc.weight.data.normal_(mean=0.0, std=0.01)
    model.fc.bias.data.zero_()
    if args.weights == 'freeze':
        model.requires_grad_(False)
        model.fc.requires_grad_(True)
    classifier_parameters, model_parameters = [], []
    for name, param in model.named_parameters():
        if name in {'fc.weight', 'fc.bias'}:
            classifier_parameters.append(param)
        else:
            model_parameters.append(param)
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])

    criterion = nn.CrossEntropyLoss().cuda(gpu)

    param_groups = [dict(params=classifier_parameters, lr=args.lr_classifier)]
    if args.weights == 'finetune':
        param_groups.append(dict(params=model_parameters, lr=args.lr_backbone))
    optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)

    # automatically resume from checkpoint if it exists
    if (args.checkpoint_dir / 'checkpoint.pth').is_file():
        ckpt = torch.load(args.checkpoint_dir / 'checkpoint.pth',
                          map_location='cpu')
        start_epoch = ckpt['epoch']
        best_acc = ckpt['best_acc']
        model.load_state_dict(ckpt['model'])
        optimizer.load_state_dict(ckpt['optimizer'])
        scheduler.load_state_dict(ckpt['scheduler'])
    else:
        start_epoch = 0
        best_acc = argparse.Namespace(top1=0, top5=0)

    # Data loading code
    traindir = args.data / 'train'
    valdir = args.data / 'val'
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ]))
    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            normalize,
        ]))

    if args.train_percent in {1, 10}:
        train_dataset.samples = []
        for fname in args.train_files:
            fname = fname.decode().strip()
            cls = fname.split('_')[0]
            train_dataset.samples.append(
                (traindir / cls / fname, train_dataset.class_to_idx[cls]))

    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    kwargs = dict(batch_size=args.batch_size // args.world_size, num_workers=args.workers, pin_memory=True)
    train_loader = torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, **kwargs)
    val_loader = torch.utils.data.DataLoader(val_dataset, **kwargs)

    start_time = time.time()
    for epoch in range(start_epoch, args.epochs):
        # train
        if args.weights == 'finetune':
            model.train()
        elif args.weights == 'freeze':
            model.eval()
        else:
            assert False
        train_sampler.set_epoch(epoch)
        for step, (images, target) in enumerate(train_loader, start=epoch * len(train_loader)):
            output = model(images.cuda(gpu, non_blocking=True))
            loss = criterion(output, target.cuda(gpu, non_blocking=True))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if step % args.print_freq == 0:
                torch.distributed.reduce(loss.div_(args.world_size), 0)
                if args.rank == 0:
                    pg = optimizer.param_groups
                    lr_classifier = pg[0]['lr']
                    lr_backbone = pg[1]['lr'] if len(pg) == 2 else 0
                    stats = dict(epoch=epoch, step=step, lr_backbone=lr_backbone,
                                 lr_classifier=lr_classifier, loss=loss.item(),
                                 time=int(time.time() - start_time))
                    print(json.dumps(stats))
                    print(json.dumps(stats), file=stats_file)

        # evaluate
        model.eval()
        if args.rank == 0:
            top1 = AverageMeter('Acc@1')
            top5 = AverageMeter('Acc@5')
            with torch.no_grad():
                for images, target in val_loader:
                    output = model(images.cuda(gpu, non_blocking=True))
                    acc1, acc5 = accuracy(output, target.cuda(gpu, non_blocking=True), topk=(1, 5))
                    top1.update(acc1[0].item(), images.size(0))
                    top5.update(acc5[0].item(), images.size(0))
            best_acc.top1 = max(best_acc.top1, top1.avg)
            best_acc.top5 = max(best_acc.top5, top5.avg)
            stats = dict(epoch=epoch, acc1=top1.avg, acc5=top5.avg, best_acc1=best_acc.top1, best_acc5=best_acc.top5)
            print(json.dumps(stats))
            print(json.dumps(stats), file=stats_file)

        # sanity check
        if args.weights == 'freeze':
            reference_state_dict = torch.load(args.pretrained, map_location='cpu')
            model_state_dict = model.module.state_dict()
            for k in reference_state_dict:
                assert torch.equal(model_state_dict[k].cpu(), reference_state_dict[k]), k

        scheduler.step()
        if args.rank == 0:
            state = dict(
                epoch=epoch + 1, best_acc=best_acc, model=model.state_dict(),
                optimizer=optimizer.state_dict(), scheduler=scheduler.state_dict())
            torch.save(state, args.checkpoint_dir / 'checkpoint.pth')",args.rank == 0,not args.rank
barlowtwins,https://github.com/facebookresearch/barlowtwins/tree/master//evaluate.py,,main_worker$66,"def main_worker(gpu, args):
    args.rank += gpu
    torch.distributed.init_process_group(
        backend='nccl', init_method=args.dist_url,
        world_size=args.world_size, rank=args.rank)

    if args.rank == 0:
        args.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        stats_file = open(args.checkpoint_dir / 'stats.txt', 'a', buffering=1)
        print(' '.join(sys.argv))
        print(' '.join(sys.argv), file=stats_file)

    torch.cuda.set_device(gpu)
    torch.backends.cudnn.benchmark = True

    model = models.resnet50().cuda(gpu)
    state_dict = torch.load(args.pretrained, map_location='cpu')
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    assert missing_keys == ['fc.weight', 'fc.bias'] and unexpected_keys == []
    model.fc.weight.data.normal_(mean=0.0, std=0.01)
    model.fc.bias.data.zero_()
    if args.weights == 'freeze':
        model.requires_grad_(False)
        model.fc.requires_grad_(True)
    classifier_parameters, model_parameters = [], []
    for name, param in model.named_parameters():
        if name in {'fc.weight', 'fc.bias'}:
            classifier_parameters.append(param)
        else:
            model_parameters.append(param)
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])

    criterion = nn.CrossEntropyLoss().cuda(gpu)

    param_groups = [dict(params=classifier_parameters, lr=args.lr_classifier)]
    if args.weights == 'finetune':
        param_groups.append(dict(params=model_parameters, lr=args.lr_backbone))
    optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)

    # automatically resume from checkpoint if it exists
    if (args.checkpoint_dir / 'checkpoint.pth').is_file():
        ckpt = torch.load(args.checkpoint_dir / 'checkpoint.pth',
                          map_location='cpu')
        start_epoch = ckpt['epoch']
        best_acc = ckpt['best_acc']
        model.load_state_dict(ckpt['model'])
        optimizer.load_state_dict(ckpt['optimizer'])
        scheduler.load_state_dict(ckpt['scheduler'])
    else:
        start_epoch = 0
        best_acc = argparse.Namespace(top1=0, top5=0)

    # Data loading code
    traindir = args.data / 'train'
    valdir = args.data / 'val'
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ]))
    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            normalize,
        ]))

    if args.train_percent in {1, 10}:
        train_dataset.samples = []
        for fname in args.train_files:
            fname = fname.decode().strip()
            cls = fname.split('_')[0]
            train_dataset.samples.append(
                (traindir / cls / fname, train_dataset.class_to_idx[cls]))

    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    kwargs = dict(batch_size=args.batch_size // args.world_size, num_workers=args.workers, pin_memory=True)
    train_loader = torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, **kwargs)
    val_loader = torch.utils.data.DataLoader(val_dataset, **kwargs)

    start_time = time.time()
    for epoch in range(start_epoch, args.epochs):
        # train
        if args.weights == 'finetune':
            model.train()
        elif args.weights == 'freeze':
            model.eval()
        else:
            assert False
        train_sampler.set_epoch(epoch)
        for step, (images, target) in enumerate(train_loader, start=epoch * len(train_loader)):
            output = model(images.cuda(gpu, non_blocking=True))
            loss = criterion(output, target.cuda(gpu, non_blocking=True))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if step % args.print_freq == 0:
                torch.distributed.reduce(loss.div_(args.world_size), 0)
                if args.rank == 0:
                    pg = optimizer.param_groups
                    lr_classifier = pg[0]['lr']
                    lr_backbone = pg[1]['lr'] if len(pg) == 2 else 0
                    stats = dict(epoch=epoch, step=step, lr_backbone=lr_backbone,
                                 lr_classifier=lr_classifier, loss=loss.item(),
                                 time=int(time.time() - start_time))
                    print(json.dumps(stats))
                    print(json.dumps(stats), file=stats_file)

        # evaluate
        model.eval()
        if args.rank == 0:
            top1 = AverageMeter('Acc@1')
            top5 = AverageMeter('Acc@5')
            with torch.no_grad():
                for images, target in val_loader:
                    output = model(images.cuda(gpu, non_blocking=True))
                    acc1, acc5 = accuracy(output, target.cuda(gpu, non_blocking=True), topk=(1, 5))
                    top1.update(acc1[0].item(), images.size(0))
                    top5.update(acc5[0].item(), images.size(0))
            best_acc.top1 = max(best_acc.top1, top1.avg)
            best_acc.top5 = max(best_acc.top5, top5.avg)
            stats = dict(epoch=epoch, acc1=top1.avg, acc5=top5.avg, best_acc1=best_acc.top1, best_acc5=best_acc.top5)
            print(json.dumps(stats))
            print(json.dumps(stats), file=stats_file)

        # sanity check
        if args.weights == 'freeze':
            reference_state_dict = torch.load(args.pretrained, map_location='cpu')
            model_state_dict = model.module.state_dict()
            for k in reference_state_dict:
                assert torch.equal(model_state_dict[k].cpu(), reference_state_dict[k]), k

        scheduler.step()
        if args.rank == 0:
            state = dict(
                epoch=epoch + 1, best_acc=best_acc, model=model.state_dict(),
                optimizer=optimizer.state_dict(), scheduler=scheduler.state_dict())
            torch.save(state, args.checkpoint_dir / 'checkpoint.pth')",unexpected_keys == [],not unexpected_keys
barlowtwins,https://github.com/facebookresearch/barlowtwins/tree/master//evaluate.py,,main_worker$66,"def main_worker(gpu, args):
    args.rank += gpu
    torch.distributed.init_process_group(
        backend='nccl', init_method=args.dist_url,
        world_size=args.world_size, rank=args.rank)

    if args.rank == 0:
        args.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        stats_file = open(args.checkpoint_dir / 'stats.txt', 'a', buffering=1)
        print(' '.join(sys.argv))
        print(' '.join(sys.argv), file=stats_file)

    torch.cuda.set_device(gpu)
    torch.backends.cudnn.benchmark = True

    model = models.resnet50().cuda(gpu)
    state_dict = torch.load(args.pretrained, map_location='cpu')
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    assert missing_keys == ['fc.weight', 'fc.bias'] and unexpected_keys == []
    model.fc.weight.data.normal_(mean=0.0, std=0.01)
    model.fc.bias.data.zero_()
    if args.weights == 'freeze':
        model.requires_grad_(False)
        model.fc.requires_grad_(True)
    classifier_parameters, model_parameters = [], []
    for name, param in model.named_parameters():
        if name in {'fc.weight', 'fc.bias'}:
            classifier_parameters.append(param)
        else:
            model_parameters.append(param)
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])

    criterion = nn.CrossEntropyLoss().cuda(gpu)

    param_groups = [dict(params=classifier_parameters, lr=args.lr_classifier)]
    if args.weights == 'finetune':
        param_groups.append(dict(params=model_parameters, lr=args.lr_backbone))
    optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)

    # automatically resume from checkpoint if it exists
    if (args.checkpoint_dir / 'checkpoint.pth').is_file():
        ckpt = torch.load(args.checkpoint_dir / 'checkpoint.pth',
                          map_location='cpu')
        start_epoch = ckpt['epoch']
        best_acc = ckpt['best_acc']
        model.load_state_dict(ckpt['model'])
        optimizer.load_state_dict(ckpt['optimizer'])
        scheduler.load_state_dict(ckpt['scheduler'])
    else:
        start_epoch = 0
        best_acc = argparse.Namespace(top1=0, top5=0)

    # Data loading code
    traindir = args.data / 'train'
    valdir = args.data / 'val'
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ]))
    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            normalize,
        ]))

    if args.train_percent in {1, 10}:
        train_dataset.samples = []
        for fname in args.train_files:
            fname = fname.decode().strip()
            cls = fname.split('_')[0]
            train_dataset.samples.append(
                (traindir / cls / fname, train_dataset.class_to_idx[cls]))

    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    kwargs = dict(batch_size=args.batch_size // args.world_size, num_workers=args.workers, pin_memory=True)
    train_loader = torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, **kwargs)
    val_loader = torch.utils.data.DataLoader(val_dataset, **kwargs)

    start_time = time.time()
    for epoch in range(start_epoch, args.epochs):
        # train
        if args.weights == 'finetune':
            model.train()
        elif args.weights == 'freeze':
            model.eval()
        else:
            assert False
        train_sampler.set_epoch(epoch)
        for step, (images, target) in enumerate(train_loader, start=epoch * len(train_loader)):
            output = model(images.cuda(gpu, non_blocking=True))
            loss = criterion(output, target.cuda(gpu, non_blocking=True))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if step % args.print_freq == 0:
                torch.distributed.reduce(loss.div_(args.world_size), 0)
                if args.rank == 0:
                    pg = optimizer.param_groups
                    lr_classifier = pg[0]['lr']
                    lr_backbone = pg[1]['lr'] if len(pg) == 2 else 0
                    stats = dict(epoch=epoch, step=step, lr_backbone=lr_backbone,
                                 lr_classifier=lr_classifier, loss=loss.item(),
                                 time=int(time.time() - start_time))
                    print(json.dumps(stats))
                    print(json.dumps(stats), file=stats_file)

        # evaluate
        model.eval()
        if args.rank == 0:
            top1 = AverageMeter('Acc@1')
            top5 = AverageMeter('Acc@5')
            with torch.no_grad():
                for images, target in val_loader:
                    output = model(images.cuda(gpu, non_blocking=True))
                    acc1, acc5 = accuracy(output, target.cuda(gpu, non_blocking=True), topk=(1, 5))
                    top1.update(acc1[0].item(), images.size(0))
                    top5.update(acc5[0].item(), images.size(0))
            best_acc.top1 = max(best_acc.top1, top1.avg)
            best_acc.top5 = max(best_acc.top5, top5.avg)
            stats = dict(epoch=epoch, acc1=top1.avg, acc5=top5.avg, best_acc1=best_acc.top1, best_acc5=best_acc.top5)
            print(json.dumps(stats))
            print(json.dumps(stats), file=stats_file)

        # sanity check
        if args.weights == 'freeze':
            reference_state_dict = torch.load(args.pretrained, map_location='cpu')
            model_state_dict = model.module.state_dict()
            for k in reference_state_dict:
                assert torch.equal(model_state_dict[k].cpu(), reference_state_dict[k]), k

        scheduler.step()
        if args.rank == 0:
            state = dict(
                epoch=epoch + 1, best_acc=best_acc, model=model.state_dict(),
                optimizer=optimizer.state_dict(), scheduler=scheduler.state_dict())
            torch.save(state, args.checkpoint_dir / 'checkpoint.pth')",args.rank == 0,not args.rank
barlowtwins,https://github.com/facebookresearch/barlowtwins/tree/master//evaluate.py,,main_worker$66,"def main_worker(gpu, args):
    args.rank += gpu
    torch.distributed.init_process_group(
        backend='nccl', init_method=args.dist_url,
        world_size=args.world_size, rank=args.rank)

    if args.rank == 0:
        args.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        stats_file = open(args.checkpoint_dir / 'stats.txt', 'a', buffering=1)
        print(' '.join(sys.argv))
        print(' '.join(sys.argv), file=stats_file)

    torch.cuda.set_device(gpu)
    torch.backends.cudnn.benchmark = True

    model = models.resnet50().cuda(gpu)
    state_dict = torch.load(args.pretrained, map_location='cpu')
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    assert missing_keys == ['fc.weight', 'fc.bias'] and unexpected_keys == []
    model.fc.weight.data.normal_(mean=0.0, std=0.01)
    model.fc.bias.data.zero_()
    if args.weights == 'freeze':
        model.requires_grad_(False)
        model.fc.requires_grad_(True)
    classifier_parameters, model_parameters = [], []
    for name, param in model.named_parameters():
        if name in {'fc.weight', 'fc.bias'}:
            classifier_parameters.append(param)
        else:
            model_parameters.append(param)
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])

    criterion = nn.CrossEntropyLoss().cuda(gpu)

    param_groups = [dict(params=classifier_parameters, lr=args.lr_classifier)]
    if args.weights == 'finetune':
        param_groups.append(dict(params=model_parameters, lr=args.lr_backbone))
    optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)

    # automatically resume from checkpoint if it exists
    if (args.checkpoint_dir / 'checkpoint.pth').is_file():
        ckpt = torch.load(args.checkpoint_dir / 'checkpoint.pth',
                          map_location='cpu')
        start_epoch = ckpt['epoch']
        best_acc = ckpt['best_acc']
        model.load_state_dict(ckpt['model'])
        optimizer.load_state_dict(ckpt['optimizer'])
        scheduler.load_state_dict(ckpt['scheduler'])
    else:
        start_epoch = 0
        best_acc = argparse.Namespace(top1=0, top5=0)

    # Data loading code
    traindir = args.data / 'train'
    valdir = args.data / 'val'
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ]))
    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            normalize,
        ]))

    if args.train_percent in {1, 10}:
        train_dataset.samples = []
        for fname in args.train_files:
            fname = fname.decode().strip()
            cls = fname.split('_')[0]
            train_dataset.samples.append(
                (traindir / cls / fname, train_dataset.class_to_idx[cls]))

    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    kwargs = dict(batch_size=args.batch_size // args.world_size, num_workers=args.workers, pin_memory=True)
    train_loader = torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, **kwargs)
    val_loader = torch.utils.data.DataLoader(val_dataset, **kwargs)

    start_time = time.time()
    for epoch in range(start_epoch, args.epochs):
        # train
        if args.weights == 'finetune':
            model.train()
        elif args.weights == 'freeze':
            model.eval()
        else:
            assert False
        train_sampler.set_epoch(epoch)
        for step, (images, target) in enumerate(train_loader, start=epoch * len(train_loader)):
            output = model(images.cuda(gpu, non_blocking=True))
            loss = criterion(output, target.cuda(gpu, non_blocking=True))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if step % args.print_freq == 0:
                torch.distributed.reduce(loss.div_(args.world_size), 0)
                if args.rank == 0:
                    pg = optimizer.param_groups
                    lr_classifier = pg[0]['lr']
                    lr_backbone = pg[1]['lr'] if len(pg) == 2 else 0
                    stats = dict(epoch=epoch, step=step, lr_backbone=lr_backbone,
                                 lr_classifier=lr_classifier, loss=loss.item(),
                                 time=int(time.time() - start_time))
                    print(json.dumps(stats))
                    print(json.dumps(stats), file=stats_file)

        # evaluate
        model.eval()
        if args.rank == 0:
            top1 = AverageMeter('Acc@1')
            top5 = AverageMeter('Acc@5')
            with torch.no_grad():
                for images, target in val_loader:
                    output = model(images.cuda(gpu, non_blocking=True))
                    acc1, acc5 = accuracy(output, target.cuda(gpu, non_blocking=True), topk=(1, 5))
                    top1.update(acc1[0].item(), images.size(0))
                    top5.update(acc5[0].item(), images.size(0))
            best_acc.top1 = max(best_acc.top1, top1.avg)
            best_acc.top5 = max(best_acc.top5, top5.avg)
            stats = dict(epoch=epoch, acc1=top1.avg, acc5=top5.avg, best_acc1=best_acc.top1, best_acc5=best_acc.top5)
            print(json.dumps(stats))
            print(json.dumps(stats), file=stats_file)

        # sanity check
        if args.weights == 'freeze':
            reference_state_dict = torch.load(args.pretrained, map_location='cpu')
            model_state_dict = model.module.state_dict()
            for k in reference_state_dict:
                assert torch.equal(model_state_dict[k].cpu(), reference_state_dict[k]), k

        scheduler.step()
        if args.rank == 0:
            state = dict(
                epoch=epoch + 1, best_acc=best_acc, model=model.state_dict(),
                optimizer=optimizer.state_dict(), scheduler=scheduler.state_dict())
            torch.save(state, args.checkpoint_dir / 'checkpoint.pth')",args.rank == 0,not args.rank
barlowtwins,https://github.com/facebookresearch/barlowtwins/tree/master//evaluate.py,,main_worker$66,"def main_worker(gpu, args):
    args.rank += gpu
    torch.distributed.init_process_group(
        backend='nccl', init_method=args.dist_url,
        world_size=args.world_size, rank=args.rank)

    if args.rank == 0:
        args.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        stats_file = open(args.checkpoint_dir / 'stats.txt', 'a', buffering=1)
        print(' '.join(sys.argv))
        print(' '.join(sys.argv), file=stats_file)

    torch.cuda.set_device(gpu)
    torch.backends.cudnn.benchmark = True

    model = models.resnet50().cuda(gpu)
    state_dict = torch.load(args.pretrained, map_location='cpu')
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    assert missing_keys == ['fc.weight', 'fc.bias'] and unexpected_keys == []
    model.fc.weight.data.normal_(mean=0.0, std=0.01)
    model.fc.bias.data.zero_()
    if args.weights == 'freeze':
        model.requires_grad_(False)
        model.fc.requires_grad_(True)
    classifier_parameters, model_parameters = [], []
    for name, param in model.named_parameters():
        if name in {'fc.weight', 'fc.bias'}:
            classifier_parameters.append(param)
        else:
            model_parameters.append(param)
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])

    criterion = nn.CrossEntropyLoss().cuda(gpu)

    param_groups = [dict(params=classifier_parameters, lr=args.lr_classifier)]
    if args.weights == 'finetune':
        param_groups.append(dict(params=model_parameters, lr=args.lr_backbone))
    optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)

    # automatically resume from checkpoint if it exists
    if (args.checkpoint_dir / 'checkpoint.pth').is_file():
        ckpt = torch.load(args.checkpoint_dir / 'checkpoint.pth',
                          map_location='cpu')
        start_epoch = ckpt['epoch']
        best_acc = ckpt['best_acc']
        model.load_state_dict(ckpt['model'])
        optimizer.load_state_dict(ckpt['optimizer'])
        scheduler.load_state_dict(ckpt['scheduler'])
    else:
        start_epoch = 0
        best_acc = argparse.Namespace(top1=0, top5=0)

    # Data loading code
    traindir = args.data / 'train'
    valdir = args.data / 'val'
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ]))
    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            normalize,
        ]))

    if args.train_percent in {1, 10}:
        train_dataset.samples = []
        for fname in args.train_files:
            fname = fname.decode().strip()
            cls = fname.split('_')[0]
            train_dataset.samples.append(
                (traindir / cls / fname, train_dataset.class_to_idx[cls]))

    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    kwargs = dict(batch_size=args.batch_size // args.world_size, num_workers=args.workers, pin_memory=True)
    train_loader = torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, **kwargs)
    val_loader = torch.utils.data.DataLoader(val_dataset, **kwargs)

    start_time = time.time()
    for epoch in range(start_epoch, args.epochs):
        # train
        if args.weights == 'finetune':
            model.train()
        elif args.weights == 'freeze':
            model.eval()
        else:
            assert False
        train_sampler.set_epoch(epoch)
        for step, (images, target) in enumerate(train_loader, start=epoch * len(train_loader)):
            output = model(images.cuda(gpu, non_blocking=True))
            loss = criterion(output, target.cuda(gpu, non_blocking=True))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if step % args.print_freq == 0:
                torch.distributed.reduce(loss.div_(args.world_size), 0)
                if args.rank == 0:
                    pg = optimizer.param_groups
                    lr_classifier = pg[0]['lr']
                    lr_backbone = pg[1]['lr'] if len(pg) == 2 else 0
                    stats = dict(epoch=epoch, step=step, lr_backbone=lr_backbone,
                                 lr_classifier=lr_classifier, loss=loss.item(),
                                 time=int(time.time() - start_time))
                    print(json.dumps(stats))
                    print(json.dumps(stats), file=stats_file)

        # evaluate
        model.eval()
        if args.rank == 0:
            top1 = AverageMeter('Acc@1')
            top5 = AverageMeter('Acc@5')
            with torch.no_grad():
                for images, target in val_loader:
                    output = model(images.cuda(gpu, non_blocking=True))
                    acc1, acc5 = accuracy(output, target.cuda(gpu, non_blocking=True), topk=(1, 5))
                    top1.update(acc1[0].item(), images.size(0))
                    top5.update(acc5[0].item(), images.size(0))
            best_acc.top1 = max(best_acc.top1, top1.avg)
            best_acc.top5 = max(best_acc.top5, top5.avg)
            stats = dict(epoch=epoch, acc1=top1.avg, acc5=top5.avg, best_acc1=best_acc.top1, best_acc5=best_acc.top5)
            print(json.dumps(stats))
            print(json.dumps(stats), file=stats_file)

        # sanity check
        if args.weights == 'freeze':
            reference_state_dict = torch.load(args.pretrained, map_location='cpu')
            model_state_dict = model.module.state_dict()
            for k in reference_state_dict:
                assert torch.equal(model_state_dict[k].cpu(), reference_state_dict[k]), k

        scheduler.step()
        if args.rank == 0:
            state = dict(
                epoch=epoch + 1, best_acc=best_acc, model=model.state_dict(),
                optimizer=optimizer.state_dict(), scheduler=scheduler.state_dict())
            torch.save(state, args.checkpoint_dir / 'checkpoint.pth')",step % args.print_freq == 0,not step % args.print_freq
barlowtwins,https://github.com/facebookresearch/barlowtwins/tree/master//evaluate.py,,main_worker$66,"def main_worker(gpu, args):
    args.rank += gpu
    torch.distributed.init_process_group(
        backend='nccl', init_method=args.dist_url,
        world_size=args.world_size, rank=args.rank)

    if args.rank == 0:
        args.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        stats_file = open(args.checkpoint_dir / 'stats.txt', 'a', buffering=1)
        print(' '.join(sys.argv))
        print(' '.join(sys.argv), file=stats_file)

    torch.cuda.set_device(gpu)
    torch.backends.cudnn.benchmark = True

    model = models.resnet50().cuda(gpu)
    state_dict = torch.load(args.pretrained, map_location='cpu')
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    assert missing_keys == ['fc.weight', 'fc.bias'] and unexpected_keys == []
    model.fc.weight.data.normal_(mean=0.0, std=0.01)
    model.fc.bias.data.zero_()
    if args.weights == 'freeze':
        model.requires_grad_(False)
        model.fc.requires_grad_(True)
    classifier_parameters, model_parameters = [], []
    for name, param in model.named_parameters():
        if name in {'fc.weight', 'fc.bias'}:
            classifier_parameters.append(param)
        else:
            model_parameters.append(param)
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])

    criterion = nn.CrossEntropyLoss().cuda(gpu)

    param_groups = [dict(params=classifier_parameters, lr=args.lr_classifier)]
    if args.weights == 'finetune':
        param_groups.append(dict(params=model_parameters, lr=args.lr_backbone))
    optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)

    # automatically resume from checkpoint if it exists
    if (args.checkpoint_dir / 'checkpoint.pth').is_file():
        ckpt = torch.load(args.checkpoint_dir / 'checkpoint.pth',
                          map_location='cpu')
        start_epoch = ckpt['epoch']
        best_acc = ckpt['best_acc']
        model.load_state_dict(ckpt['model'])
        optimizer.load_state_dict(ckpt['optimizer'])
        scheduler.load_state_dict(ckpt['scheduler'])
    else:
        start_epoch = 0
        best_acc = argparse.Namespace(top1=0, top5=0)

    # Data loading code
    traindir = args.data / 'train'
    valdir = args.data / 'val'
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ]))
    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            normalize,
        ]))

    if args.train_percent in {1, 10}:
        train_dataset.samples = []
        for fname in args.train_files:
            fname = fname.decode().strip()
            cls = fname.split('_')[0]
            train_dataset.samples.append(
                (traindir / cls / fname, train_dataset.class_to_idx[cls]))

    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    kwargs = dict(batch_size=args.batch_size // args.world_size, num_workers=args.workers, pin_memory=True)
    train_loader = torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, **kwargs)
    val_loader = torch.utils.data.DataLoader(val_dataset, **kwargs)

    start_time = time.time()
    for epoch in range(start_epoch, args.epochs):
        # train
        if args.weights == 'finetune':
            model.train()
        elif args.weights == 'freeze':
            model.eval()
        else:
            assert False
        train_sampler.set_epoch(epoch)
        for step, (images, target) in enumerate(train_loader, start=epoch * len(train_loader)):
            output = model(images.cuda(gpu, non_blocking=True))
            loss = criterion(output, target.cuda(gpu, non_blocking=True))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if step % args.print_freq == 0:
                torch.distributed.reduce(loss.div_(args.world_size), 0)
                if args.rank == 0:
                    pg = optimizer.param_groups
                    lr_classifier = pg[0]['lr']
                    lr_backbone = pg[1]['lr'] if len(pg) == 2 else 0
                    stats = dict(epoch=epoch, step=step, lr_backbone=lr_backbone,
                                 lr_classifier=lr_classifier, loss=loss.item(),
                                 time=int(time.time() - start_time))
                    print(json.dumps(stats))
                    print(json.dumps(stats), file=stats_file)

        # evaluate
        model.eval()
        if args.rank == 0:
            top1 = AverageMeter('Acc@1')
            top5 = AverageMeter('Acc@5')
            with torch.no_grad():
                for images, target in val_loader:
                    output = model(images.cuda(gpu, non_blocking=True))
                    acc1, acc5 = accuracy(output, target.cuda(gpu, non_blocking=True), topk=(1, 5))
                    top1.update(acc1[0].item(), images.size(0))
                    top5.update(acc5[0].item(), images.size(0))
            best_acc.top1 = max(best_acc.top1, top1.avg)
            best_acc.top5 = max(best_acc.top5, top5.avg)
            stats = dict(epoch=epoch, acc1=top1.avg, acc5=top5.avg, best_acc1=best_acc.top1, best_acc5=best_acc.top5)
            print(json.dumps(stats))
            print(json.dumps(stats), file=stats_file)

        # sanity check
        if args.weights == 'freeze':
            reference_state_dict = torch.load(args.pretrained, map_location='cpu')
            model_state_dict = model.module.state_dict()
            for k in reference_state_dict:
                assert torch.equal(model_state_dict[k].cpu(), reference_state_dict[k]), k

        scheduler.step()
        if args.rank == 0:
            state = dict(
                epoch=epoch + 1, best_acc=best_acc, model=model.state_dict(),
                optimizer=optimizer.state_dict(), scheduler=scheduler.state_dict())
            torch.save(state, args.checkpoint_dir / 'checkpoint.pth')",args.rank == 0,not args.rank
strawberryfields,https://github.com/XanaduAI/strawberryfields/tree/master/strawberryfields/backends/states.py,BaseBosonicState,mean_photon$1777,"def mean_photon(self, mode, **kwargs):
        """"""Returns the mean photon number of a particular mode.

        Args:
            mode (int): specifies the mode

        Returns:
            tuple: the mean photon number and variance

        Raises:
            ValueError: if the mean or the variance is complex
        """"""
        weights, mus, covs = self.reduced_bosonic([mode])
        cov_trace = np.matrix.trace(covs, axis1=1, axis2=2)
        mean_dots = np.einsum(
            ""...j,...j"",
            mus,
            mus,
        )
        mean = np.sum(weights * (cov_trace + mean_dots)) / (2 * self._hbar) - 0.5

        cov_sq_trace = np.matrix.trace(covs @ covs, axis1=1, axis2=2)
        mean_cov_dots = np.einsum(
            ""...j,...jk,...k"",
            mus,
            covs,
            mus,
        )
        var = np.sum(weights * (cov_sq_trace + 2 * mean_cov_dots)) / (2 * self._hbar**2) - 0.25
        var += np.sum(weights * ((cov_trace + mean_dots) / (2 * self._hbar) - 0.5) ** 2)
        var -= mean**2
        mean = np.real_if_close(mean)
        var = np.real_if_close(var)

        if mean.imag != 0 or var.imag != 0:
            raise ValueError(""Mean or variance of photon number is complex."")

        return mean, var",mean.imag != 0,mean.imag
strawberryfields,https://github.com/XanaduAI/strawberryfields/tree/master/strawberryfields/backends/states.py,BaseBosonicState,mean_photon$1777,"def mean_photon(self, mode, **kwargs):
        """"""Returns the mean photon number of a particular mode.

        Args:
            mode (int): specifies the mode

        Returns:
            tuple: the mean photon number and variance

        Raises:
            ValueError: if the mean or the variance is complex
        """"""
        weights, mus, covs = self.reduced_bosonic([mode])
        cov_trace = np.matrix.trace(covs, axis1=1, axis2=2)
        mean_dots = np.einsum(
            ""...j,...j"",
            mus,
            mus,
        )
        mean = np.sum(weights * (cov_trace + mean_dots)) / (2 * self._hbar) - 0.5

        cov_sq_trace = np.matrix.trace(covs @ covs, axis1=1, axis2=2)
        mean_cov_dots = np.einsum(
            ""...j,...jk,...k"",
            mus,
            covs,
            mus,
        )
        var = np.sum(weights * (cov_sq_trace + 2 * mean_cov_dots)) / (2 * self._hbar**2) - 0.25
        var += np.sum(weights * ((cov_trace + mean_dots) / (2 * self._hbar) - 0.5) ** 2)
        var -= mean**2
        mean = np.real_if_close(mean)
        var = np.real_if_close(var)

        if mean.imag != 0 or var.imag != 0:
            raise ValueError(""Mean or variance of photon number is complex."")

        return mean, var",var.imag != 0,var.imag
CellProfiler,https://github.com/CellProfiler/CellProfiler/tree/master/tests/modules/test_identifysecondaryobjects.py,,test_relationships_zero$1249,"def test_relationships_zero():
    workspace, module = make_workspace(
        numpy.zeros((10, 10)), numpy.zeros((10, 10), int)
    )
    assert isinstance(
        module, cellprofiler.modules.identifysecondaryobjects.IdentifySecondaryObjects
    )
    module.run(workspace)
    m = workspace.measurements
    assert isinstance(m,cellprofiler_core.measurement.Measurements)
    result = m.get_relationships(
        module.module_num,
        cellprofiler.modules.identifysecondaryobjects.R_PARENT,
        module.x_name.value,
        module.y_name.value,
    )
    assert len(result) == 0",len(result) == 0,not result
explainerdashboard,https://github.com/oegedijk/explainerdashboard/tree/master/tests/integration_tests/test_dashboards.py,,test_xgboost_multiclass_dashboard$77,"def test_xgboost_multiclass_dashboard(dash_duo, precalculated_xgb_multiclass_explainer):
    db = ExplainerDashboard(precalculated_xgb_multiclass_explainer, title=""testing"", responsive=False)
    html = db.to_html()
    assert html.startswith('\n<!DOCTYPE html>\n<html'), ""failed to generate dashboard to_html""

    dash_duo.start_server(db.app)
    dash_duo.wait_for_text_to_equal(""h1"", ""testing"", timeout=30)
    assert dash_duo.get_logs() == [], ""browser console should contain no error""",dash_duo.get_logs() == [],not dash_duo.get_logs()
coding-interview-gym,https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/358_Rearrange_String_k_Distance_Apart.py,Solution,rearrangeString$5,"def rearrangeString(self, s, k):
        """"""
        :type s: str
        :type k: int
        :rtype: str
        """"""
        if k == 0:
            return s
        result, priorityQueue = """", []
        charFrequencies = Counter(s)
        for key, value in charFrequencies.items():
            heapq.heappush(priorityQueue, (-value, key))
        while priorityQueue:
            tempCharHolder, currentDistance = [], 0
            while currentDistance < k:
                if priorityQueue:
                    currentDistance += 1
                    currentCharFrequency, currentChar = heapq.heappop(priorityQueue)
                    result += currentChar
                    if currentCharFrequency != -1:
                        tempCharHolder.append((currentCharFrequency + 1, currentChar))
                else:
                    if tempCharHolder:
                        return """"
                    else:
                        return result
            for item in tempCharHolder:
                heapq.heappush(priorityQueue, item)
        return result",k == 0,not k
R-Drop,https://github.com/dropreg/R-Drop/tree/master/fairseq_src/fairseq/search.py,PrefixConstrainedBeamSearch,step$169,"def step(
        self,
        step: int,
        lprobs: Tensor,
        scores: Tensor,
        prev_output_tokens: Tensor,
        original_batch_idxs: Tensor,
    ):
        bsz, beam_size, vocab_size = lprobs.size()

        lprobs += self.apply_mask(
            lprobs.view(bsz * beam_size, 1, vocab_size),
            prev_output_tokens,
            original_batch_idxs,
        ).view(bsz, beam_size, vocab_size)

        if step == 0:
            # at the first step all hypotheses are equally likely, so use
            # only the first beam
            lprobs = lprobs[:, ::beam_size, :].contiguous()
        else:
            # make probs contain cumulative scores for each hypothesis
            assert scores is not None
            lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)

        top_prediction = torch.topk(
            lprobs.view(bsz, -1),
            k=min(
                # Take the best beam_size predictions. We'll choose the first
                # beam_size of these which don't predict eos to continue with.
                beam_size,
                lprobs.view(bsz, -1).size(1) - 1,  # -1 so we never select pad
            ),
        )
        scores_buf = top_prediction[0]
        indices_buf = top_prediction[1]
        beams_buf = indices_buf // vocab_size
        indices_buf = indices_buf.fmod(vocab_size)
        return scores_buf, indices_buf, beams_buf",step == 0,not step
sympy,https://github.com/sympy/sympy/tree/master/sympy/physics/vector/tests/test_vector.py,,test_Vector$14,"def test_Vector():
    assert A.x != A.y
    assert A.y != A.z
    assert A.z != A.x

    assert A.x + 0 == A.x

    v1 = x*A.x + y*A.y + z*A.z
    v2 = x**2*A.x + y**2*A.y + z**2*A.z
    v3 = v1 + v2
    v4 = v1 - v2

    assert isinstance(v1, Vector)
    assert dot(v1, A.x) == x
    assert dot(v1, A.y) == y
    assert dot(v1, A.z) == z

    assert isinstance(v2, Vector)
    assert dot(v2, A.x) == x**2
    assert dot(v2, A.y) == y**2
    assert dot(v2, A.z) == z**2

    assert isinstance(v3, Vector)
    # We probably shouldn't be using simplify in dot...
    assert dot(v3, A.x) == x**2 + x
    assert dot(v3, A.y) == y**2 + y
    assert dot(v3, A.z) == z**2 + z

    assert isinstance(v4, Vector)
    # We probably shouldn't be using simplify in dot...
    assert dot(v4, A.x) == x - x**2
    assert dot(v4, A.y) == y - y**2
    assert dot(v4, A.z) == z - z**2

    assert v1.to_matrix(A) == Matrix([[x], [y], [z]])
    q = symbols('q')
    B = A.orientnew('B', 'Axis', (q, A.x))
    assert v1.to_matrix(B) == Matrix([[x],
                                      [ y * cos(q) + z * sin(q)],
                                      [-y * sin(q) + z * cos(q)]])

    #Test the separate method
    B = ReferenceFrame('B')
    v5 = x*A.x + y*A.y + z*B.z
    assert Vector(0).separate() == {}
    assert v1.separate() == {A: v1}
    assert v5.separate() == {A: x*A.x + y*A.y, B: z*B.z}

    #Test the free_symbols property
    v6 = x*A.x + y*A.y + z*A.z
    assert v6.free_symbols(A) == {x,y,z}

    raises(TypeError, lambda: v3.applyfunc(v1))",Vector(0).separate() == {},not Vector(0).separate()
deepdrive,https://github.com/deepdrive/deepdrive/tree/master/tests/integration_tests/reset.py,,main$4,"def main():
    # TODO: Add some asserts and get working on Jenkins
    env = sim.start(is_sync=True)
    forward = sim.action(throttle=1, steering=0, brake=0)
    done = False
    i = 0
    while 1:
        i += 1
        observation, reward, done, info = env.step(forward)
        if i % 10 == 0:
            env.reset()",i % 10 == 0,not i % 10
barman,https://github.com/EnterpriseDB/barman/tree/master/barman/cloud.py,,copyfileobj_pad_truncate$76,"def copyfileobj_pad_truncate(src, dst, length=None):
    """"""
    Copy length bytes from fileobj src to fileobj dst.
    If length is None, copy the entire content.
    This method is used by the TarFileIgnoringTruncate.addfile().
    """"""
    if length == 0:
        return

    if length is None:
        shutil.copyfileobj(src, dst, BUFSIZE)
        return

    blocks, remainder = divmod(length, BUFSIZE)
    for _ in range(blocks):
        buf = src.read(BUFSIZE)
        dst.write(buf)
        if len(buf) < BUFSIZE:
            # End of file reached
            # The file must have  been truncated, so pad with zeroes
            dst.write(tarfile.NUL * (BUFSIZE - len(buf)))

    if remainder != 0:
        buf = src.read(remainder)
        dst.write(buf)
        if len(buf) < remainder:
            # End of file reached
            # The file must have  been truncated, so pad with zeroes
            dst.write(tarfile.NUL * (remainder - len(buf)))",length == 0,not length
barman,https://github.com/EnterpriseDB/barman/tree/master/barman/cloud.py,,copyfileobj_pad_truncate$76,"def copyfileobj_pad_truncate(src, dst, length=None):
    """"""
    Copy length bytes from fileobj src to fileobj dst.
    If length is None, copy the entire content.
    This method is used by the TarFileIgnoringTruncate.addfile().
    """"""
    if length == 0:
        return

    if length is None:
        shutil.copyfileobj(src, dst, BUFSIZE)
        return

    blocks, remainder = divmod(length, BUFSIZE)
    for _ in range(blocks):
        buf = src.read(BUFSIZE)
        dst.write(buf)
        if len(buf) < BUFSIZE:
            # End of file reached
            # The file must have  been truncated, so pad with zeroes
            dst.write(tarfile.NUL * (BUFSIZE - len(buf)))

    if remainder != 0:
        buf = src.read(remainder)
        dst.write(buf)
        if len(buf) < remainder:
            # End of file reached
            # The file must have  been truncated, so pad with zeroes
            dst.write(tarfile.NUL * (remainder - len(buf)))",remainder != 0,remainder
imagededup,https://github.com/idealo/imagededup/tree/master/tests/test_hashing.py,,test_find_duplicates_clearing$615,"def test_find_duplicates_clearing():
    phasher = PHash()
    duplicate_dict = phasher.find_duplicates(
        image_dir=PATH_IMAGE_DIR,
        max_distance_threshold=10,
        scores=True,
        search_method='brute_force_cython',
    )

    duplicate_dict = phasher.find_duplicates(
        image_dir=PATH_IMAGE_DIR,
        max_distance_threshold=10,
        scores=True,
        search_method='brute_force_cython',
    )

    assert isinstance(duplicate_dict, dict)
    duplicates = list(duplicate_dict.values())
    assert isinstance(duplicates[0], list)
    assert duplicate_dict['ukbench09268.jpg'] == []
    assert duplicate_dict['ukbench00120.jpg'] == [('ukbench00120_resize.jpg', 0)]",duplicate_dict['ukbench09268.jpg'] == [],not duplicate_dict['ukbench09268.jpg']
dcc,https://github.com/amimo/dcc/tree/master/androguard/core/bytecodes/dvm.py,ParameterAnnotation,get_obj$926,"def get_obj(self):
        if self.annotations_off != 0:
            self.annotations_off = self.CM.get_obj_by_offset(
                self.annotations_off).get_off()

        return pack(""=I"", self.method_idx) + pack(""=I"", self.annotations_off)",self.annotations_off != 0,self.annotations_off
fairscale,https://github.com/facebookresearch/fairscale/tree/master/fairscale/nn/pipe/rpc.py,PipeRPCWrapper,__init__$94,"def __init__(self, *args: Any, **kwargs: Any):
        super().__init__()
        self.group = cast(ProcessGroup, kwargs.get(""group"")) or get_pipeline_parallel_group()
        assert self.group.rank() == 0
        self.lock = Lock()

        if True:
            assert (
                self.group == get_pipeline_parallel_group()
            ), ""Can't pickle groups, so group must be `get_pipeline_parallel_group()`""
            kwargs[""group""] = None
        else:
            kwargs[""group""] = self.group

        kwargs[""input_device""] = torch.device(""cuda"", torch.cuda.current_device())

        self.model = AsyncPipe(*args, **kwargs)
        self.worker_map = kwargs[""worker_map""]
        self._foreach_worker(self._register_remote_model, args=(args, kwargs))
        self.model.cuda()",self.group.rank() == 0,not self.group.rank()
espresso,https://github.com/freewym/espresso/tree/master/examples/multilingual/data_scripts/remove_valid_test_in_train.py,,if_main_my$199,"if __name__ == ""__main__"":
    #######
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--from-folder',  
        required=True,
        type=str)
    parser.add_argument(
        '--to-folder',  
        required=True,
        type=str)
    parser.add_argument(
        '--directions',  
        default=None,
        type=str)


    args = parser.parse_args()    
    raw_data = args.from_folder
    to_folder = args.to_folder
    os.makedirs(to_folder, exist_ok=True)

    if args.directions:
        directions = args.directions.split(',')
    else:
        raw_files = itertools.chain(
            glob.glob(f'{raw_data}/train*'),
            glob.glob(f'{raw_data}/valid*'),
            glob.glob(f'{raw_data}/test*'),
        )
        directions = [os.path.split(file_path)[-1].split('.')[1] for file_path in raw_files]
    print('working on directions: ', directions)

    ##########
    


    all_test_data, test_data = get_all_test_data(raw_data, directions, 'test')
    print('==loaded test data==')
    all_valid_data, valid_data = get_all_test_data(raw_data, directions, 'valid')
    print('==loaded valid data==')
    all_valid_test_data =  merge_valid_test_messup(all_test_data, all_valid_data)
    mess_up_train, data_sizes = check_train_all(raw_data, directions, all_valid_test_data)
    print('training messing up with valid, test data:', len(mess_up_train))
    data_situation = train_size_if_remove_in_otherset(data_sizes, mess_up_train)
    df = pd.DataFrame(data_situation, columns=['direction', 'train_size_after_remove', 'orig_size', 'num_to_remove', 'remove_percent'])
    df.sort_values('remove_percent', ascending=False)
    df.to_csv(f'{raw_data}/clean_summary.tsv', sep='\t')
    print(f'projected data clean summary in: {raw_data}/clean_summary.tsv')    

    # correct the dataset:
    all_test_pairs, mess_up_test_train_pairs = get_messed_up_test_pairs('test', directions)
    all_valid_pairs, mess_up_valid_train_pairs = get_messed_up_test_pairs('valid', directions)

    all_messed_pairs = set(mess_up_test_train_pairs.keys()).union(set(mess_up_valid_train_pairs.keys()))    
    corrected_directions = set()

    real_data_situation = []
    for direction in directions:
        org_size, new_size = remove_messed_up_sentences(raw_data, direction, mess_up_train, all_messed_pairs, corrected_directions)
        if org_size == 0:
            print(f""{direction} has size 0"")
            continue
        real_data_situation.append(
            (direction, new_size, org_size, org_size - new_size, (org_size - new_size) / org_size * 100)
        )
    print('corrected directions: ', corrected_directions)
    df = pd.DataFrame(real_data_situation, columns=['direction', 'train_size_after_remove', 'orig_size', 'num_to_remove', 'remove_percent'])
    df.sort_values('remove_percent', ascending=False)
    df.to_csv(f'{raw_data}/actual_clean_summary.tsv', sep='\t')
    print(f'actual data clean summary (which can be different from the projected one because of duplications) in: {raw_data}/actual_clean_summary.tsv')        

    import shutil
    for direction in directions:
        src_lang, tgt_lang = direction.split('-')
        for split in ['train', 'valid', 'test']:
            # copying valid, test and uncorrected train
            if direction in corrected_directions and split == 'train':
                continue
            tgt = f""{raw_data}/{split}.{direction}.{tgt_lang}""
            src = f""{raw_data}/{split}.{direction}.{src_lang}""
            if not (os.path.exists(src) and os.path.exists(tgt)):
                continue
            corrected_tgt = f""{to_folder}/{split}.{direction}.{tgt_lang}""
            corrected_src = f""{to_folder}/{split}.{direction}.{src_lang}""
            print(f'copying {src} to {corrected_src}')
            shutil.copyfile(src, corrected_src)
            print(f'copying {tgt} to {corrected_tgt}')
            shutil.copyfile(tgt, corrected_tgt)   

    print('completed')",org_size == 0,not org_size
gradient-checkpointing,https://github.com/cybertronai/gradient-checkpointing/tree/master/test/linearize.py,,linearize$414,"def linearize(targets=None, modify_graph=True):
  """"""Obtain a single valid execution order which approximately minimizes
  peak memory usage.

  TODO: deprecate/hide modify_graph arg
  Args:
    targets: specifies list of computation Tensor or op targets or a single
        target.
        skipped. If None, all nodes are considered targets.
    modify_graph: if True, will add control dependencies to force this order

  Returns:
    Number of control dependencies that were added if modify_graph=True,
    otherwise returns list of ops in this order.
  """"""

  graph, targets = _process_targets(targets)
  parent_graph = reversed_graph(graph)

  toposort(copy_graph(graph))   # check for cycles (raises exception)

  # The algorithm works by keeping an ""active"" set nodes that have no
  # unscheduled children, hence are ready for execution.  At each iteration,
  # schedule all nodes ready for for execution with least memory-hungry
  # nodes first and repeat to convergence.
  last_node = None

  # count of unscheduled children for each node
  unscheduled = OrderedDict()
  active = []
  for node in graph:
    unscheduled[node] = len(graph[node])
    if unscheduled[node] == 0:
      active.append(node)

  assert len(active)>0, ""List of targets contains a cycle""

  for node in active:
    assert unscheduled[node] == 0
  
  control_edges_added = 0
  order = []
  wave_number = 0
  while active:
#    print(""wave %d, %d""%(wave_number, len(active)))
    wave_number+=1
    new_active = []
    for node in memsorted(active):
      assert unscheduled[node] == 0

      order.append(node)
      if DEBUG:
        print(""Executing "", node.name)

      if last_node:
        if modify_graph:
          control_edges_added += _run_after(last_node, node)
      last_node = node
        

      # this node is scheduled, so update unscheduled counts of parents
      for parent in parent_graph[node]:
        assert unscheduled[parent] > 0
        unscheduled[parent] -= 1
        if unscheduled[parent] == 0:
          new_active.append(parent)
#          print(""Adding %s to active"" % (parent,))

    active = new_active  # end while

  if modify_graph:
    return control_edges_added
  else:
    result = list(reversed(order))
    return result",unscheduled[node] == 0,not unscheduled[node]
gradient-checkpointing,https://github.com/cybertronai/gradient-checkpointing/tree/master/test/linearize.py,,linearize$414,"def linearize(targets=None, modify_graph=True):
  """"""Obtain a single valid execution order which approximately minimizes
  peak memory usage.

  TODO: deprecate/hide modify_graph arg
  Args:
    targets: specifies list of computation Tensor or op targets or a single
        target.
        skipped. If None, all nodes are considered targets.
    modify_graph: if True, will add control dependencies to force this order

  Returns:
    Number of control dependencies that were added if modify_graph=True,
    otherwise returns list of ops in this order.
  """"""

  graph, targets = _process_targets(targets)
  parent_graph = reversed_graph(graph)

  toposort(copy_graph(graph))   # check for cycles (raises exception)

  # The algorithm works by keeping an ""active"" set nodes that have no
  # unscheduled children, hence are ready for execution.  At each iteration,
  # schedule all nodes ready for for execution with least memory-hungry
  # nodes first and repeat to convergence.
  last_node = None

  # count of unscheduled children for each node
  unscheduled = OrderedDict()
  active = []
  for node in graph:
    unscheduled[node] = len(graph[node])
    if unscheduled[node] == 0:
      active.append(node)

  assert len(active)>0, ""List of targets contains a cycle""

  for node in active:
    assert unscheduled[node] == 0
  
  control_edges_added = 0
  order = []
  wave_number = 0
  while active:
#    print(""wave %d, %d""%(wave_number, len(active)))
    wave_number+=1
    new_active = []
    for node in memsorted(active):
      assert unscheduled[node] == 0

      order.append(node)
      if DEBUG:
        print(""Executing "", node.name)

      if last_node:
        if modify_graph:
          control_edges_added += _run_after(last_node, node)
      last_node = node
        

      # this node is scheduled, so update unscheduled counts of parents
      for parent in parent_graph[node]:
        assert unscheduled[parent] > 0
        unscheduled[parent] -= 1
        if unscheduled[parent] == 0:
          new_active.append(parent)
#          print(""Adding %s to active"" % (parent,))

    active = new_active  # end while

  if modify_graph:
    return control_edges_added
  else:
    result = list(reversed(order))
    return result",unscheduled[node] == 0,not unscheduled[node]
gradient-checkpointing,https://github.com/cybertronai/gradient-checkpointing/tree/master/test/linearize.py,,linearize$414,"def linearize(targets=None, modify_graph=True):
  """"""Obtain a single valid execution order which approximately minimizes
  peak memory usage.

  TODO: deprecate/hide modify_graph arg
  Args:
    targets: specifies list of computation Tensor or op targets or a single
        target.
        skipped. If None, all nodes are considered targets.
    modify_graph: if True, will add control dependencies to force this order

  Returns:
    Number of control dependencies that were added if modify_graph=True,
    otherwise returns list of ops in this order.
  """"""

  graph, targets = _process_targets(targets)
  parent_graph = reversed_graph(graph)

  toposort(copy_graph(graph))   # check for cycles (raises exception)

  # The algorithm works by keeping an ""active"" set nodes that have no
  # unscheduled children, hence are ready for execution.  At each iteration,
  # schedule all nodes ready for for execution with least memory-hungry
  # nodes first and repeat to convergence.
  last_node = None

  # count of unscheduled children for each node
  unscheduled = OrderedDict()
  active = []
  for node in graph:
    unscheduled[node] = len(graph[node])
    if unscheduled[node] == 0:
      active.append(node)

  assert len(active)>0, ""List of targets contains a cycle""

  for node in active:
    assert unscheduled[node] == 0
  
  control_edges_added = 0
  order = []
  wave_number = 0
  while active:
#    print(""wave %d, %d""%(wave_number, len(active)))
    wave_number+=1
    new_active = []
    for node in memsorted(active):
      assert unscheduled[node] == 0

      order.append(node)
      if DEBUG:
        print(""Executing "", node.name)

      if last_node:
        if modify_graph:
          control_edges_added += _run_after(last_node, node)
      last_node = node
        

      # this node is scheduled, so update unscheduled counts of parents
      for parent in parent_graph[node]:
        assert unscheduled[parent] > 0
        unscheduled[parent] -= 1
        if unscheduled[parent] == 0:
          new_active.append(parent)
#          print(""Adding %s to active"" % (parent,))

    active = new_active  # end while

  if modify_graph:
    return control_edges_added
  else:
    result = list(reversed(order))
    return result",unscheduled[node] == 0,not unscheduled[node]
gradient-checkpointing,https://github.com/cybertronai/gradient-checkpointing/tree/master/test/linearize.py,,linearize$414,"def linearize(targets=None, modify_graph=True):
  """"""Obtain a single valid execution order which approximately minimizes
  peak memory usage.

  TODO: deprecate/hide modify_graph arg
  Args:
    targets: specifies list of computation Tensor or op targets or a single
        target.
        skipped. If None, all nodes are considered targets.
    modify_graph: if True, will add control dependencies to force this order

  Returns:
    Number of control dependencies that were added if modify_graph=True,
    otherwise returns list of ops in this order.
  """"""

  graph, targets = _process_targets(targets)
  parent_graph = reversed_graph(graph)

  toposort(copy_graph(graph))   # check for cycles (raises exception)

  # The algorithm works by keeping an ""active"" set nodes that have no
  # unscheduled children, hence are ready for execution.  At each iteration,
  # schedule all nodes ready for for execution with least memory-hungry
  # nodes first and repeat to convergence.
  last_node = None

  # count of unscheduled children for each node
  unscheduled = OrderedDict()
  active = []
  for node in graph:
    unscheduled[node] = len(graph[node])
    if unscheduled[node] == 0:
      active.append(node)

  assert len(active)>0, ""List of targets contains a cycle""

  for node in active:
    assert unscheduled[node] == 0
  
  control_edges_added = 0
  order = []
  wave_number = 0
  while active:
#    print(""wave %d, %d""%(wave_number, len(active)))
    wave_number+=1
    new_active = []
    for node in memsorted(active):
      assert unscheduled[node] == 0

      order.append(node)
      if DEBUG:
        print(""Executing "", node.name)

      if last_node:
        if modify_graph:
          control_edges_added += _run_after(last_node, node)
      last_node = node
        

      # this node is scheduled, so update unscheduled counts of parents
      for parent in parent_graph[node]:
        assert unscheduled[parent] > 0
        unscheduled[parent] -= 1
        if unscheduled[parent] == 0:
          new_active.append(parent)
#          print(""Adding %s to active"" % (parent,))

    active = new_active  # end while

  if modify_graph:
    return control_edges_added
  else:
    result = list(reversed(order))
    return result",unscheduled[parent] == 0,not unscheduled[parent]
pyopencl,https://github.com/inducer/pyopencl/tree/master/test/test_algorithm.py,,test_astype$194,"def test_astype(ctx_factory):
    context = ctx_factory()
    queue = cl.CommandQueue(context)

    from pyopencl.clrandom import rand as clrand

    if not has_double_support(context.devices[0]):
        from pytest import skip
        skip(""double precision not supported on %s"" % context.devices[0])

    a_gpu = clrand(queue, (2000,), dtype=np.float32)

    a = a_gpu.get().astype(np.float64)
    a2 = a_gpu.astype(np.float64).get()

    assert a2.dtype == np.float64
    assert la.norm(a - a2) == 0, (a, a2)

    a_gpu = clrand(queue, (2000,), dtype=np.float64)

    a = a_gpu.get().astype(np.float32)
    a2 = a_gpu.astype(np.float32).get()

    assert a2.dtype == np.float32
    assert la.norm(a - a2) / la.norm(a) < 1e-7",la.norm(a - a2) == 0,not la.norm(a - a2)
safaribooks,https://github.com/lorenzodifuccia/safaribooks/tree/master//safaribooks.py,SafariBooks,get_book_chapters$550,"def get_book_chapters(self, page=1):
        response = self.requests_provider(urljoin(self.api_url, ""chapter/?page=%s"" % page))
        if response == 0:
            self.display.exit(""API: unable to retrieve book chapters."")

        response = response.json()

        if not isinstance(response, dict) or len(response.keys()) == 1:
            self.display.exit(self.display.api_error(response))

        if ""results"" not in response or not len(response[""results""]):
            self.display.exit(""API: unable to retrieve book chapters."")

        if response[""count""] > sys.getrecursionlimit():
            sys.setrecursionlimit(response[""count""])

        result = []
        result.extend([c for c in response[""results""] if ""cover"" in c[""filename""] or ""cover"" in c[""title""]])
        for c in result:
            del response[""results""][response[""results""].index(c)]

        result += response[""results""]
        return result + (self.get_book_chapters(page + 1) if response[""next""] else [])",response == 0,not response
Dain-App,https://github.com/BurguerJohn/Dain-App/tree/master//my_design.py,My_Ui_Dialog,CreateRenderData$649,"def CreateRenderData(self):
		if self.selectFiles == """":
			self.MessageBox(""Error"", ""Select a file before hiting Render."")
			return []
		if self.selectedOutFolder == """":
			self.MessageBox(""Error"", ""Select a output folder before hiting Render."")
			return []

		warnings.filterwarnings(""ignore"")

		renderList = []
		for i in range(0, len(self.selectFiles)):

			selectFile = self.selectFiles[i]
			ftp = self.fpsInput.text()
			path = os.path.dirname(selectFile)
			name = os.path.basename(selectFile)
			name_no_ext = name.split(""."")[0]

			if self.inputType != 3:
				outFolder = self.selectedOutFolder +""/""+name_no_ext+""/""
			else:
				outFolder = self.selectedOutFolder

			resize = self.useResize.isChecked()
			resizeWidth = 0
			if resize:
				resizeWidth = int(self.widthValue.text())

			useWatermark = 0

			interAlgo = self.intAlgo.currentIndex()
			interpolCode = self.framerateConf
			interpolMethod =  self.interpolMethod.currentIndex()
			alphaOpt = self.alphaOpt.currentIndex()
			audioVersion = int(self.audioVersion.isChecked())

			to60 = int(self.to60.isChecked())
			to60Smooth = int(self.to60C1.isChecked())
			to60Sharp = int(self.to60C2.isChecked())

			exportExt = self.exportType.currentIndex()
			animMethod = self.animMethod.currentIndex()

			splitFrames = int(self.useSplit.isChecked())
			splitSizeX = 0
			splitSizeY = 0
			splitPad = 0

			cleanInterpol = int(self.cleanInterpol.isChecked())
			fpsLimit = int(self.fpsLimit.text())
			checkScenes = int(self.dontInterpolateScenes.isChecked())

			debugKeepDuplicates = int(self.debugKeepDuplicates.isChecked())
			onlyInterpolateMissing = int(self.onlyInterpolateMissing.isChecked())

			flowPath = ""./""+ self.model_path + self.flowModel.currentText()

			device = int(self.deviceList.currentIndex())
		


			checkScenes = int(self.dontInterpolateScenes.isChecked())
			checkScenes = int(self.dontInterpolateScenes.isChecked())

			sf = self.smoothFlow.text()
			ff = self.flowForce.text()
			if sf == """":
				sf = 0
			if ff == """":
				ff = 20
			flowForce = int(ff)
			SmoothFlow = int(sf)

			sceneSenvility =  int(self.lineEdit_2.text())

			useHalf = int(self.useHalf.isChecked())


			if splitFrames == 1:
				splitSizeX = int(self.splitSizeX.text())
				splitSizeY = int(self.splitSizeY.text())
				splitPad = int(self.splitPad.text())

				if splitSizeX == 0 or splitSizeY == 0:
					self.MessageBox(""Error"", ""Division value should not be zero. If you do not want divisions in a axis, use the value 1."")
					return []

			
			batchSize =  int(self.batchSize.text())

			ext = ""mp4""
			if exportExt == 0:
				ext = ""mp4""
			elif exportExt == 1:
				ext = ""webm""
			elif exportExt == 2:
				ext = ""gif""
			elif exportExt == 3:
				ext = ""apng""

			if len(self.selectFiles) > 1:
				ftp = self.GetFPS(self.selectFiles[i])

			if self.inputType != 3:
				if float(ftp) == 0:
					self.MessageBox(""Error"", ""Input FPS cannot be zero."")
					return []


			myRenderData = RenderData.RenderData()
			myRenderData.inputType = self.inputType
			myRenderData.video = selectFile
			myRenderData.outStr = outFolder+""/""+name_no_ext+"".""+ext
			myRenderData.fps = float(ftp)
			myRenderData.palette = int(self.limitPalette.isChecked())
			myRenderData.resc = int(resize)
			myRenderData.maxResc = resizeWidth
			myRenderData.loop = int(self.perfectLoop.isChecked())
			myRenderData.uploadBar = self.UpdateLoading
			myRenderData.useWatermark = useWatermark
			myRenderData.interpolationAlgorithm = interAlgo
			myRenderData.framerateConf = interpolCode
			myRenderData.use60 = to60
			myRenderData.use60C1 = to60Smooth
			myRenderData.use60C2 = to60Sharp
			myRenderData.interpolationMethod = interpolMethod
			myRenderData.exportPng = 0
			myRenderData.useAnimationMethod = animMethod
			myRenderData.splitFrames = splitFrames
			myRenderData.splitSizeX = splitSizeX
			myRenderData.splitSizeY = splitSizeY
			myRenderData.splitPad = splitPad
			myRenderData.alphaMethod = alphaOpt
			myRenderData.checkSceneChanges = checkScenes
			myRenderData.sceneChangeSensibility = sceneSenvility
			myRenderData.model = flowPath
			myRenderData.sel_process = device

			myRenderData.use60RealFps = fpsLimit
			myRenderData.inputMethod = self.GetInputType()
			myRenderData.outFolder = outFolder
			myRenderData.cleanInterpol = cleanInterpol
			myRenderData.audioVersion = audioVersion
			myRenderData.onlyRenderMissing = onlyInterpolateMissing
			myRenderData.debugKeepDuplicates = debugKeepDuplicates
			myRenderData.use_half = useHalf
			myRenderData.fastMode = int(self.fastMode.isChecked())

			myRenderData.version = RenderData.GetVersion()
			myRenderData.quiet = 0

			myRenderData.pixelBgColor = (255,0,127)

			myRenderData.crf = int(self.crfVal.text())
			myRenderData.pngcompress = int(self.pngCompress.text())

			myRenderData.pixelUpscaleDowscaleBefore = self.pixelUpscaleDowscaleBefore.currentIndex() + 1
			myRenderData.pixelDownscaleUpscaleAfter = self.pixelDownscaleUpscaleAfter.currentIndex() + 1
			myRenderData.pixelUpscaleAfter = self.pixelUpscaleAfter.currentIndex() + 1

			myRenderData.mute_ffmpeg = 1 - int(self.ffmpegPrint.isChecked())
			myRenderData.cleanCudaCache = 1 - int(self.dontCleanCache.isChecked())
			myRenderData.useBenchmark = int(self.doBenchmark.isChecked())


			myRenderData.batch_size = batchSize

			myRenderData.flowForce = flowForce
			myRenderData.SmoothFlow = SmoothFlow

			renderList.append(myRenderData)
		return renderList",exportExt == 0,not exportExt
Dain-App,https://github.com/BurguerJohn/Dain-App/tree/master//my_design.py,My_Ui_Dialog,CreateRenderData$649,"def CreateRenderData(self):
		if self.selectFiles == """":
			self.MessageBox(""Error"", ""Select a file before hiting Render."")
			return []
		if self.selectedOutFolder == """":
			self.MessageBox(""Error"", ""Select a output folder before hiting Render."")
			return []

		warnings.filterwarnings(""ignore"")

		renderList = []
		for i in range(0, len(self.selectFiles)):

			selectFile = self.selectFiles[i]
			ftp = self.fpsInput.text()
			path = os.path.dirname(selectFile)
			name = os.path.basename(selectFile)
			name_no_ext = name.split(""."")[0]

			if self.inputType != 3:
				outFolder = self.selectedOutFolder +""/""+name_no_ext+""/""
			else:
				outFolder = self.selectedOutFolder

			resize = self.useResize.isChecked()
			resizeWidth = 0
			if resize:
				resizeWidth = int(self.widthValue.text())

			useWatermark = 0

			interAlgo = self.intAlgo.currentIndex()
			interpolCode = self.framerateConf
			interpolMethod =  self.interpolMethod.currentIndex()
			alphaOpt = self.alphaOpt.currentIndex()
			audioVersion = int(self.audioVersion.isChecked())

			to60 = int(self.to60.isChecked())
			to60Smooth = int(self.to60C1.isChecked())
			to60Sharp = int(self.to60C2.isChecked())

			exportExt = self.exportType.currentIndex()
			animMethod = self.animMethod.currentIndex()

			splitFrames = int(self.useSplit.isChecked())
			splitSizeX = 0
			splitSizeY = 0
			splitPad = 0

			cleanInterpol = int(self.cleanInterpol.isChecked())
			fpsLimit = int(self.fpsLimit.text())
			checkScenes = int(self.dontInterpolateScenes.isChecked())

			debugKeepDuplicates = int(self.debugKeepDuplicates.isChecked())
			onlyInterpolateMissing = int(self.onlyInterpolateMissing.isChecked())

			flowPath = ""./""+ self.model_path + self.flowModel.currentText()

			device = int(self.deviceList.currentIndex())
		


			checkScenes = int(self.dontInterpolateScenes.isChecked())
			checkScenes = int(self.dontInterpolateScenes.isChecked())

			sf = self.smoothFlow.text()
			ff = self.flowForce.text()
			if sf == """":
				sf = 0
			if ff == """":
				ff = 20
			flowForce = int(ff)
			SmoothFlow = int(sf)

			sceneSenvility =  int(self.lineEdit_2.text())

			useHalf = int(self.useHalf.isChecked())


			if splitFrames == 1:
				splitSizeX = int(self.splitSizeX.text())
				splitSizeY = int(self.splitSizeY.text())
				splitPad = int(self.splitPad.text())

				if splitSizeX == 0 or splitSizeY == 0:
					self.MessageBox(""Error"", ""Division value should not be zero. If you do not want divisions in a axis, use the value 1."")
					return []

			
			batchSize =  int(self.batchSize.text())

			ext = ""mp4""
			if exportExt == 0:
				ext = ""mp4""
			elif exportExt == 1:
				ext = ""webm""
			elif exportExt == 2:
				ext = ""gif""
			elif exportExt == 3:
				ext = ""apng""

			if len(self.selectFiles) > 1:
				ftp = self.GetFPS(self.selectFiles[i])

			if self.inputType != 3:
				if float(ftp) == 0:
					self.MessageBox(""Error"", ""Input FPS cannot be zero."")
					return []


			myRenderData = RenderData.RenderData()
			myRenderData.inputType = self.inputType
			myRenderData.video = selectFile
			myRenderData.outStr = outFolder+""/""+name_no_ext+"".""+ext
			myRenderData.fps = float(ftp)
			myRenderData.palette = int(self.limitPalette.isChecked())
			myRenderData.resc = int(resize)
			myRenderData.maxResc = resizeWidth
			myRenderData.loop = int(self.perfectLoop.isChecked())
			myRenderData.uploadBar = self.UpdateLoading
			myRenderData.useWatermark = useWatermark
			myRenderData.interpolationAlgorithm = interAlgo
			myRenderData.framerateConf = interpolCode
			myRenderData.use60 = to60
			myRenderData.use60C1 = to60Smooth
			myRenderData.use60C2 = to60Sharp
			myRenderData.interpolationMethod = interpolMethod
			myRenderData.exportPng = 0
			myRenderData.useAnimationMethod = animMethod
			myRenderData.splitFrames = splitFrames
			myRenderData.splitSizeX = splitSizeX
			myRenderData.splitSizeY = splitSizeY
			myRenderData.splitPad = splitPad
			myRenderData.alphaMethod = alphaOpt
			myRenderData.checkSceneChanges = checkScenes
			myRenderData.sceneChangeSensibility = sceneSenvility
			myRenderData.model = flowPath
			myRenderData.sel_process = device

			myRenderData.use60RealFps = fpsLimit
			myRenderData.inputMethod = self.GetInputType()
			myRenderData.outFolder = outFolder
			myRenderData.cleanInterpol = cleanInterpol
			myRenderData.audioVersion = audioVersion
			myRenderData.onlyRenderMissing = onlyInterpolateMissing
			myRenderData.debugKeepDuplicates = debugKeepDuplicates
			myRenderData.use_half = useHalf
			myRenderData.fastMode = int(self.fastMode.isChecked())

			myRenderData.version = RenderData.GetVersion()
			myRenderData.quiet = 0

			myRenderData.pixelBgColor = (255,0,127)

			myRenderData.crf = int(self.crfVal.text())
			myRenderData.pngcompress = int(self.pngCompress.text())

			myRenderData.pixelUpscaleDowscaleBefore = self.pixelUpscaleDowscaleBefore.currentIndex() + 1
			myRenderData.pixelDownscaleUpscaleAfter = self.pixelDownscaleUpscaleAfter.currentIndex() + 1
			myRenderData.pixelUpscaleAfter = self.pixelUpscaleAfter.currentIndex() + 1

			myRenderData.mute_ffmpeg = 1 - int(self.ffmpegPrint.isChecked())
			myRenderData.cleanCudaCache = 1 - int(self.dontCleanCache.isChecked())
			myRenderData.useBenchmark = int(self.doBenchmark.isChecked())


			myRenderData.batch_size = batchSize

			myRenderData.flowForce = flowForce
			myRenderData.SmoothFlow = SmoothFlow

			renderList.append(myRenderData)
		return renderList",float(ftp) == 0,not float(ftp)
Dain-App,https://github.com/BurguerJohn/Dain-App/tree/master//my_design.py,My_Ui_Dialog,CreateRenderData$649,"def CreateRenderData(self):
		if self.selectFiles == """":
			self.MessageBox(""Error"", ""Select a file before hiting Render."")
			return []
		if self.selectedOutFolder == """":
			self.MessageBox(""Error"", ""Select a output folder before hiting Render."")
			return []

		warnings.filterwarnings(""ignore"")

		renderList = []
		for i in range(0, len(self.selectFiles)):

			selectFile = self.selectFiles[i]
			ftp = self.fpsInput.text()
			path = os.path.dirname(selectFile)
			name = os.path.basename(selectFile)
			name_no_ext = name.split(""."")[0]

			if self.inputType != 3:
				outFolder = self.selectedOutFolder +""/""+name_no_ext+""/""
			else:
				outFolder = self.selectedOutFolder

			resize = self.useResize.isChecked()
			resizeWidth = 0
			if resize:
				resizeWidth = int(self.widthValue.text())

			useWatermark = 0

			interAlgo = self.intAlgo.currentIndex()
			interpolCode = self.framerateConf
			interpolMethod =  self.interpolMethod.currentIndex()
			alphaOpt = self.alphaOpt.currentIndex()
			audioVersion = int(self.audioVersion.isChecked())

			to60 = int(self.to60.isChecked())
			to60Smooth = int(self.to60C1.isChecked())
			to60Sharp = int(self.to60C2.isChecked())

			exportExt = self.exportType.currentIndex()
			animMethod = self.animMethod.currentIndex()

			splitFrames = int(self.useSplit.isChecked())
			splitSizeX = 0
			splitSizeY = 0
			splitPad = 0

			cleanInterpol = int(self.cleanInterpol.isChecked())
			fpsLimit = int(self.fpsLimit.text())
			checkScenes = int(self.dontInterpolateScenes.isChecked())

			debugKeepDuplicates = int(self.debugKeepDuplicates.isChecked())
			onlyInterpolateMissing = int(self.onlyInterpolateMissing.isChecked())

			flowPath = ""./""+ self.model_path + self.flowModel.currentText()

			device = int(self.deviceList.currentIndex())
		


			checkScenes = int(self.dontInterpolateScenes.isChecked())
			checkScenes = int(self.dontInterpolateScenes.isChecked())

			sf = self.smoothFlow.text()
			ff = self.flowForce.text()
			if sf == """":
				sf = 0
			if ff == """":
				ff = 20
			flowForce = int(ff)
			SmoothFlow = int(sf)

			sceneSenvility =  int(self.lineEdit_2.text())

			useHalf = int(self.useHalf.isChecked())


			if splitFrames == 1:
				splitSizeX = int(self.splitSizeX.text())
				splitSizeY = int(self.splitSizeY.text())
				splitPad = int(self.splitPad.text())

				if splitSizeX == 0 or splitSizeY == 0:
					self.MessageBox(""Error"", ""Division value should not be zero. If you do not want divisions in a axis, use the value 1."")
					return []

			
			batchSize =  int(self.batchSize.text())

			ext = ""mp4""
			if exportExt == 0:
				ext = ""mp4""
			elif exportExt == 1:
				ext = ""webm""
			elif exportExt == 2:
				ext = ""gif""
			elif exportExt == 3:
				ext = ""apng""

			if len(self.selectFiles) > 1:
				ftp = self.GetFPS(self.selectFiles[i])

			if self.inputType != 3:
				if float(ftp) == 0:
					self.MessageBox(""Error"", ""Input FPS cannot be zero."")
					return []


			myRenderData = RenderData.RenderData()
			myRenderData.inputType = self.inputType
			myRenderData.video = selectFile
			myRenderData.outStr = outFolder+""/""+name_no_ext+"".""+ext
			myRenderData.fps = float(ftp)
			myRenderData.palette = int(self.limitPalette.isChecked())
			myRenderData.resc = int(resize)
			myRenderData.maxResc = resizeWidth
			myRenderData.loop = int(self.perfectLoop.isChecked())
			myRenderData.uploadBar = self.UpdateLoading
			myRenderData.useWatermark = useWatermark
			myRenderData.interpolationAlgorithm = interAlgo
			myRenderData.framerateConf = interpolCode
			myRenderData.use60 = to60
			myRenderData.use60C1 = to60Smooth
			myRenderData.use60C2 = to60Sharp
			myRenderData.interpolationMethod = interpolMethod
			myRenderData.exportPng = 0
			myRenderData.useAnimationMethod = animMethod
			myRenderData.splitFrames = splitFrames
			myRenderData.splitSizeX = splitSizeX
			myRenderData.splitSizeY = splitSizeY
			myRenderData.splitPad = splitPad
			myRenderData.alphaMethod = alphaOpt
			myRenderData.checkSceneChanges = checkScenes
			myRenderData.sceneChangeSensibility = sceneSenvility
			myRenderData.model = flowPath
			myRenderData.sel_process = device

			myRenderData.use60RealFps = fpsLimit
			myRenderData.inputMethod = self.GetInputType()
			myRenderData.outFolder = outFolder
			myRenderData.cleanInterpol = cleanInterpol
			myRenderData.audioVersion = audioVersion
			myRenderData.onlyRenderMissing = onlyInterpolateMissing
			myRenderData.debugKeepDuplicates = debugKeepDuplicates
			myRenderData.use_half = useHalf
			myRenderData.fastMode = int(self.fastMode.isChecked())

			myRenderData.version = RenderData.GetVersion()
			myRenderData.quiet = 0

			myRenderData.pixelBgColor = (255,0,127)

			myRenderData.crf = int(self.crfVal.text())
			myRenderData.pngcompress = int(self.pngCompress.text())

			myRenderData.pixelUpscaleDowscaleBefore = self.pixelUpscaleDowscaleBefore.currentIndex() + 1
			myRenderData.pixelDownscaleUpscaleAfter = self.pixelDownscaleUpscaleAfter.currentIndex() + 1
			myRenderData.pixelUpscaleAfter = self.pixelUpscaleAfter.currentIndex() + 1

			myRenderData.mute_ffmpeg = 1 - int(self.ffmpegPrint.isChecked())
			myRenderData.cleanCudaCache = 1 - int(self.dontCleanCache.isChecked())
			myRenderData.useBenchmark = int(self.doBenchmark.isChecked())


			myRenderData.batch_size = batchSize

			myRenderData.flowForce = flowForce
			myRenderData.SmoothFlow = SmoothFlow

			renderList.append(myRenderData)
		return renderList",splitSizeX == 0,not splitSizeX
Dain-App,https://github.com/BurguerJohn/Dain-App/tree/master//my_design.py,My_Ui_Dialog,CreateRenderData$649,"def CreateRenderData(self):
		if self.selectFiles == """":
			self.MessageBox(""Error"", ""Select a file before hiting Render."")
			return []
		if self.selectedOutFolder == """":
			self.MessageBox(""Error"", ""Select a output folder before hiting Render."")
			return []

		warnings.filterwarnings(""ignore"")

		renderList = []
		for i in range(0, len(self.selectFiles)):

			selectFile = self.selectFiles[i]
			ftp = self.fpsInput.text()
			path = os.path.dirname(selectFile)
			name = os.path.basename(selectFile)
			name_no_ext = name.split(""."")[0]

			if self.inputType != 3:
				outFolder = self.selectedOutFolder +""/""+name_no_ext+""/""
			else:
				outFolder = self.selectedOutFolder

			resize = self.useResize.isChecked()
			resizeWidth = 0
			if resize:
				resizeWidth = int(self.widthValue.text())

			useWatermark = 0

			interAlgo = self.intAlgo.currentIndex()
			interpolCode = self.framerateConf
			interpolMethod =  self.interpolMethod.currentIndex()
			alphaOpt = self.alphaOpt.currentIndex()
			audioVersion = int(self.audioVersion.isChecked())

			to60 = int(self.to60.isChecked())
			to60Smooth = int(self.to60C1.isChecked())
			to60Sharp = int(self.to60C2.isChecked())

			exportExt = self.exportType.currentIndex()
			animMethod = self.animMethod.currentIndex()

			splitFrames = int(self.useSplit.isChecked())
			splitSizeX = 0
			splitSizeY = 0
			splitPad = 0

			cleanInterpol = int(self.cleanInterpol.isChecked())
			fpsLimit = int(self.fpsLimit.text())
			checkScenes = int(self.dontInterpolateScenes.isChecked())

			debugKeepDuplicates = int(self.debugKeepDuplicates.isChecked())
			onlyInterpolateMissing = int(self.onlyInterpolateMissing.isChecked())

			flowPath = ""./""+ self.model_path + self.flowModel.currentText()

			device = int(self.deviceList.currentIndex())
		


			checkScenes = int(self.dontInterpolateScenes.isChecked())
			checkScenes = int(self.dontInterpolateScenes.isChecked())

			sf = self.smoothFlow.text()
			ff = self.flowForce.text()
			if sf == """":
				sf = 0
			if ff == """":
				ff = 20
			flowForce = int(ff)
			SmoothFlow = int(sf)

			sceneSenvility =  int(self.lineEdit_2.text())

			useHalf = int(self.useHalf.isChecked())


			if splitFrames == 1:
				splitSizeX = int(self.splitSizeX.text())
				splitSizeY = int(self.splitSizeY.text())
				splitPad = int(self.splitPad.text())

				if splitSizeX == 0 or splitSizeY == 0:
					self.MessageBox(""Error"", ""Division value should not be zero. If you do not want divisions in a axis, use the value 1."")
					return []

			
			batchSize =  int(self.batchSize.text())

			ext = ""mp4""
			if exportExt == 0:
				ext = ""mp4""
			elif exportExt == 1:
				ext = ""webm""
			elif exportExt == 2:
				ext = ""gif""
			elif exportExt == 3:
				ext = ""apng""

			if len(self.selectFiles) > 1:
				ftp = self.GetFPS(self.selectFiles[i])

			if self.inputType != 3:
				if float(ftp) == 0:
					self.MessageBox(""Error"", ""Input FPS cannot be zero."")
					return []


			myRenderData = RenderData.RenderData()
			myRenderData.inputType = self.inputType
			myRenderData.video = selectFile
			myRenderData.outStr = outFolder+""/""+name_no_ext+"".""+ext
			myRenderData.fps = float(ftp)
			myRenderData.palette = int(self.limitPalette.isChecked())
			myRenderData.resc = int(resize)
			myRenderData.maxResc = resizeWidth
			myRenderData.loop = int(self.perfectLoop.isChecked())
			myRenderData.uploadBar = self.UpdateLoading
			myRenderData.useWatermark = useWatermark
			myRenderData.interpolationAlgorithm = interAlgo
			myRenderData.framerateConf = interpolCode
			myRenderData.use60 = to60
			myRenderData.use60C1 = to60Smooth
			myRenderData.use60C2 = to60Sharp
			myRenderData.interpolationMethod = interpolMethod
			myRenderData.exportPng = 0
			myRenderData.useAnimationMethod = animMethod
			myRenderData.splitFrames = splitFrames
			myRenderData.splitSizeX = splitSizeX
			myRenderData.splitSizeY = splitSizeY
			myRenderData.splitPad = splitPad
			myRenderData.alphaMethod = alphaOpt
			myRenderData.checkSceneChanges = checkScenes
			myRenderData.sceneChangeSensibility = sceneSenvility
			myRenderData.model = flowPath
			myRenderData.sel_process = device

			myRenderData.use60RealFps = fpsLimit
			myRenderData.inputMethod = self.GetInputType()
			myRenderData.outFolder = outFolder
			myRenderData.cleanInterpol = cleanInterpol
			myRenderData.audioVersion = audioVersion
			myRenderData.onlyRenderMissing = onlyInterpolateMissing
			myRenderData.debugKeepDuplicates = debugKeepDuplicates
			myRenderData.use_half = useHalf
			myRenderData.fastMode = int(self.fastMode.isChecked())

			myRenderData.version = RenderData.GetVersion()
			myRenderData.quiet = 0

			myRenderData.pixelBgColor = (255,0,127)

			myRenderData.crf = int(self.crfVal.text())
			myRenderData.pngcompress = int(self.pngCompress.text())

			myRenderData.pixelUpscaleDowscaleBefore = self.pixelUpscaleDowscaleBefore.currentIndex() + 1
			myRenderData.pixelDownscaleUpscaleAfter = self.pixelDownscaleUpscaleAfter.currentIndex() + 1
			myRenderData.pixelUpscaleAfter = self.pixelUpscaleAfter.currentIndex() + 1

			myRenderData.mute_ffmpeg = 1 - int(self.ffmpegPrint.isChecked())
			myRenderData.cleanCudaCache = 1 - int(self.dontCleanCache.isChecked())
			myRenderData.useBenchmark = int(self.doBenchmark.isChecked())


			myRenderData.batch_size = batchSize

			myRenderData.flowForce = flowForce
			myRenderData.SmoothFlow = SmoothFlow

			renderList.append(myRenderData)
		return renderList",splitSizeY == 0,not splitSizeY
python-telegram-bot,https://github.com/python-telegram-bot/python-telegram-bot/tree/master/tests/test_messageautodeletetimerchanged.py,TestMessageAutoDeleteTimerChanged,test_de_json$31,"def test_de_json(self):
        json_dict = {""message_auto_delete_time"": self.message_auto_delete_time}
        madtc = MessageAutoDeleteTimerChanged.de_json(json_dict, None)
        assert madtc.api_kwargs == {}

        assert madtc.message_auto_delete_time == self.message_auto_delete_time",madtc.api_kwargs == {},not madtc.api_kwargs
core,https://github.com/home-assistant/core/tree/master/tests/components/usb/test_init.py,,test_get_serial_by_id_no_dir$703,"def test_get_serial_by_id_no_dir():
    """"""Test serial by id conversion if there's no /dev/serial/by-id.""""""
    p1 = patch(""os.path.isdir"", MagicMock(return_value=False))
    p2 = patch(""os.scandir"")
    with p1 as is_dir_mock, p2 as scan_mock:
        res = usb.get_serial_by_id(sentinel.path)
        assert res is sentinel.path
        assert is_dir_mock.call_count == 1
        assert scan_mock.call_count == 0",scan_mock.call_count == 0,not scan_mock.call_count
brownie,https://github.com/eth-brownie/brownie/tree/master/tests/project/main/test_main_project.py,,test_close$86,"def test_close(project, newproject):
    assert len(project.get_loaded_projects()) == 0
    newproject.load()
    assert len(project.get_loaded_projects()) == 1
    newproject.close()
    assert len(project.get_loaded_projects()) == 0
    newproject.close(False)
    with pytest.raises(ProjectNotFound):
        newproject.close()",len(project.get_loaded_projects()) == 0,not project.get_loaded_projects()
brownie,https://github.com/eth-brownie/brownie/tree/master/tests/project/main/test_main_project.py,,test_close$86,"def test_close(project, newproject):
    assert len(project.get_loaded_projects()) == 0
    newproject.load()
    assert len(project.get_loaded_projects()) == 1
    newproject.close()
    assert len(project.get_loaded_projects()) == 0
    newproject.close(False)
    with pytest.raises(ProjectNotFound):
        newproject.close()",len(project.get_loaded_projects()) == 0,not project.get_loaded_projects()
GraphGAN,https://github.com/hwwang55/GraphGAN/tree/master/src/GraphGAN/graph_gan.py,GraphGAN,train$122,"def train(self):
        # restore the model from the latest checkpoint if exists
        checkpoint = tf.train.get_checkpoint_state(config.model_log)
        if checkpoint and checkpoint.model_checkpoint_path and config.load_model:
            print(""loading the checkpoint: %s"" % checkpoint.model_checkpoint_path)
            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)

        self.write_embeddings_to_file()
        self.evaluation(self)

        print(""start training..."")
        for epoch in range(config.n_epochs):
            print(""epoch %d"" % epoch)

            # save the model
            if epoch > 0 and epoch % config.save_steps == 0:
                self.saver.save(self.sess, config.model_log + ""model.checkpoint"")

            # D-steps
            center_nodes = []
            neighbor_nodes = []
            labels = []
            for d_epoch in range(config.n_epochs_dis):
                # generate new nodes for the discriminator for every dis_interval iterations
                if d_epoch % config.dis_interval == 0:
                    center_nodes, neighbor_nodes, labels = self.prepare_data_for_d()
                # training
                train_size = len(center_nodes)
                start_list = list(range(0, train_size, config.batch_size_dis))
                np.random.shuffle(start_list)
                for start in start_list:
                    end = start + config.batch_size_dis
                    self.sess.run(self.discriminator.d_updates,
                                  feed_dict={self.discriminator.node_id: np.array(center_nodes[start:end]),
                                             self.discriminator.node_neighbor_id: np.array(neighbor_nodes[start:end]),
                                             self.discriminator.label: np.array(labels[start:end])})

            # G-steps
            node_1 = []
            node_2 = []
            reward = []
            for g_epoch in range(config.n_epochs_gen):
                if g_epoch % config.gen_interval == 0:
                    node_1, node_2, reward = self.prepare_data_for_g()

                # training
                train_size = len(node_1)
                start_list = list(range(0, train_size, config.batch_size_gen))
                np.random.shuffle(start_list)
                for start in start_list:
                    end = start + config.batch_size_gen
                    self.sess.run(self.generator.g_updates,
                                  feed_dict={self.generator.node_id: np.array(node_1[start:end]),
                                             self.generator.node_neighbor_id: np.array(node_2[start:end]),
                                             self.generator.reward: np.array(reward[start:end])})

            self.write_embeddings_to_file()
            self.evaluation(self)
        print(""training completes"")",epoch % config.save_steps == 0,not epoch % config.save_steps
GraphGAN,https://github.com/hwwang55/GraphGAN/tree/master/src/GraphGAN/graph_gan.py,GraphGAN,train$122,"def train(self):
        # restore the model from the latest checkpoint if exists
        checkpoint = tf.train.get_checkpoint_state(config.model_log)
        if checkpoint and checkpoint.model_checkpoint_path and config.load_model:
            print(""loading the checkpoint: %s"" % checkpoint.model_checkpoint_path)
            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)

        self.write_embeddings_to_file()
        self.evaluation(self)

        print(""start training..."")
        for epoch in range(config.n_epochs):
            print(""epoch %d"" % epoch)

            # save the model
            if epoch > 0 and epoch % config.save_steps == 0:
                self.saver.save(self.sess, config.model_log + ""model.checkpoint"")

            # D-steps
            center_nodes = []
            neighbor_nodes = []
            labels = []
            for d_epoch in range(config.n_epochs_dis):
                # generate new nodes for the discriminator for every dis_interval iterations
                if d_epoch % config.dis_interval == 0:
                    center_nodes, neighbor_nodes, labels = self.prepare_data_for_d()
                # training
                train_size = len(center_nodes)
                start_list = list(range(0, train_size, config.batch_size_dis))
                np.random.shuffle(start_list)
                for start in start_list:
                    end = start + config.batch_size_dis
                    self.sess.run(self.discriminator.d_updates,
                                  feed_dict={self.discriminator.node_id: np.array(center_nodes[start:end]),
                                             self.discriminator.node_neighbor_id: np.array(neighbor_nodes[start:end]),
                                             self.discriminator.label: np.array(labels[start:end])})

            # G-steps
            node_1 = []
            node_2 = []
            reward = []
            for g_epoch in range(config.n_epochs_gen):
                if g_epoch % config.gen_interval == 0:
                    node_1, node_2, reward = self.prepare_data_for_g()

                # training
                train_size = len(node_1)
                start_list = list(range(0, train_size, config.batch_size_gen))
                np.random.shuffle(start_list)
                for start in start_list:
                    end = start + config.batch_size_gen
                    self.sess.run(self.generator.g_updates,
                                  feed_dict={self.generator.node_id: np.array(node_1[start:end]),
                                             self.generator.node_neighbor_id: np.array(node_2[start:end]),
                                             self.generator.reward: np.array(reward[start:end])})

            self.write_embeddings_to_file()
            self.evaluation(self)
        print(""training completes"")",d_epoch % config.dis_interval == 0,not d_epoch % config.dis_interval
GraphGAN,https://github.com/hwwang55/GraphGAN/tree/master/src/GraphGAN/graph_gan.py,GraphGAN,train$122,"def train(self):
        # restore the model from the latest checkpoint if exists
        checkpoint = tf.train.get_checkpoint_state(config.model_log)
        if checkpoint and checkpoint.model_checkpoint_path and config.load_model:
            print(""loading the checkpoint: %s"" % checkpoint.model_checkpoint_path)
            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)

        self.write_embeddings_to_file()
        self.evaluation(self)

        print(""start training..."")
        for epoch in range(config.n_epochs):
            print(""epoch %d"" % epoch)

            # save the model
            if epoch > 0 and epoch % config.save_steps == 0:
                self.saver.save(self.sess, config.model_log + ""model.checkpoint"")

            # D-steps
            center_nodes = []
            neighbor_nodes = []
            labels = []
            for d_epoch in range(config.n_epochs_dis):
                # generate new nodes for the discriminator for every dis_interval iterations
                if d_epoch % config.dis_interval == 0:
                    center_nodes, neighbor_nodes, labels = self.prepare_data_for_d()
                # training
                train_size = len(center_nodes)
                start_list = list(range(0, train_size, config.batch_size_dis))
                np.random.shuffle(start_list)
                for start in start_list:
                    end = start + config.batch_size_dis
                    self.sess.run(self.discriminator.d_updates,
                                  feed_dict={self.discriminator.node_id: np.array(center_nodes[start:end]),
                                             self.discriminator.node_neighbor_id: np.array(neighbor_nodes[start:end]),
                                             self.discriminator.label: np.array(labels[start:end])})

            # G-steps
            node_1 = []
            node_2 = []
            reward = []
            for g_epoch in range(config.n_epochs_gen):
                if g_epoch % config.gen_interval == 0:
                    node_1, node_2, reward = self.prepare_data_for_g()

                # training
                train_size = len(node_1)
                start_list = list(range(0, train_size, config.batch_size_gen))
                np.random.shuffle(start_list)
                for start in start_list:
                    end = start + config.batch_size_gen
                    self.sess.run(self.generator.g_updates,
                                  feed_dict={self.generator.node_id: np.array(node_1[start:end]),
                                             self.generator.node_neighbor_id: np.array(node_2[start:end]),
                                             self.generator.reward: np.array(reward[start:end])})

            self.write_embeddings_to_file()
            self.evaluation(self)
        print(""training completes"")",g_epoch % config.gen_interval == 0,not g_epoch % config.gen_interval
mkosi,https://github.com/systemd/mkosi/tree/master/tests/test_config_parser.py,,test_verb_shell_cli_args7$921,"def test_verb_shell_cli_args7(tmpdir: Path) -> None:
    with change_cwd(tmpdir):
        cmdline_ref = [""-i"", ""summary""]
        args = mkosi.parse_args(cmdline_ref)
        assert args[""default""].verb == Verb.summary
        assert args[""default""].incremental == True",args['default'].incremental == True,args['default'].incremental
sympy,https://github.com/sympy/sympy/tree/master/sympy/physics/mechanics/models.py,,n_link_pendulum_on_cart$112,"def n_link_pendulum_on_cart(n=1, cart_force=True, joint_torques=False):
    r""""""Returns the system containing the symbolic first order equations of
    motion for a 2D n-link pendulum on a sliding cart under the influence of
    gravity.

    ::

                  |
         o    y   v
          \ 0 ^   g
           \  |
          --\-|----
          |  \|   |
      F-> |   o --|---> x
          |       |
          ---------
           o     o

    Parameters
    ==========

    n : integer
        The number of links in the pendulum.
    cart_force : boolean, default=True
        If true an external specified lateral force is applied to the cart.
    joint_torques : boolean, default=False
        If true joint torques will be added as specified inputs at each
        joint.

    Returns
    =======

    kane : sympy.physics.mechanics.kane.KanesMethod
        A KanesMethod object.

    Notes
    =====

    The degrees of freedom of the system are n + 1, i.e. one for each
    pendulum link and one for the lateral motion of the cart.

    M x' = F, where x = [u0, ..., un+1, q0, ..., qn+1]

    The joint angles are all defined relative to the ground where the x axis
    defines the ground line and the y axis points up. The joint torques are
    applied between each adjacent link and the between the cart and the
    lower link where a positive torque corresponds to positive angle.

    """"""
    if n <= 0:
        raise ValueError('The number of links must be a positive integer.')

    q = me.dynamicsymbols('q:{}'.format(n + 1))
    u = me.dynamicsymbols('u:{}'.format(n + 1))

    if joint_torques is True:
        T = me.dynamicsymbols('T1:{}'.format(n + 1))

    m = sm.symbols('m:{}'.format(n + 1))
    l = sm.symbols('l:{}'.format(n))
    g, t = sm.symbols('g t')

    I = me.ReferenceFrame('I')
    O = me.Point('O')
    O.set_vel(I, 0)

    P0 = me.Point('P0')
    P0.set_pos(O, q[0] * I.x)
    P0.set_vel(I, u[0] * I.x)
    Pa0 = me.Particle('Pa0', P0, m[0])

    frames = [I]
    points = [P0]
    particles = [Pa0]
    forces = [(P0, -m[0] * g * I.y)]
    kindiffs = [q[0].diff(t) - u[0]]

    if cart_force is True or joint_torques is True:
        specified = []
    else:
        specified = None

    for i in range(n):
        Bi = I.orientnew('B{}'.format(i), 'Axis', [q[i + 1], I.z])
        Bi.set_ang_vel(I, u[i + 1] * I.z)
        frames.append(Bi)

        Pi = points[-1].locatenew('P{}'.format(i + 1), l[i] * Bi.y)
        Pi.v2pt_theory(points[-1], I, Bi)
        points.append(Pi)

        Pai = me.Particle('Pa' + str(i + 1), Pi, m[i + 1])
        particles.append(Pai)

        forces.append((Pi, -m[i + 1] * g * I.y))

        if joint_torques is True:

            specified.append(T[i])

            if i == 0:
                forces.append((I, -T[i] * I.z))

            if i == n - 1:
                forces.append((Bi, T[i] * I.z))
            else:
                forces.append((Bi, T[i] * I.z - T[i + 1] * I.z))

        kindiffs.append(q[i + 1].diff(t) - u[i + 1])

    if cart_force is True:
        F = me.dynamicsymbols('F')
        forces.append((P0, F * I.x))
        specified.append(F)

    kane = me.KanesMethod(I, q_ind=q, u_ind=u, kd_eqs=kindiffs)
    kane.kanes_equations(particles, forces)

    return kane",i == 0,not i
sdc,https://github.com/IntelPython/sdc/tree/master/sdc/tests/test_rolling.py,TestRolling,test_df_rolling_apply_mean_no_unboxing$909,"def test_df_rolling_apply_mean_no_unboxing(self):
        def test_impl(window, min_periods):
            def func(x):
                if len(x) == 0:
                    return np.nan
                return x.mean()

            df = pd.DataFrame({
                'A': [0, 1, 2, 3, 4],
                'B': [1., -1., 0., 0.1, -0.1],
                'C': [1., np.inf, np.inf, -1., 0.],
                'D': [np.nan, np.inf, np.inf, np.nan, np.nan],
            })
            return df.rolling(window, min_periods).apply(func)

        hpat_func = self.jit(test_impl)
        for window in range(0, 8, 2):
            for min_periods in range(0, window + 1, 2):
                with self.subTest(window=window, min_periods=min_periods):
                    jit_result = hpat_func(window, min_periods)
                    ref_result = test_impl(window, min_periods)
                    pd.testing.assert_frame_equal(jit_result, ref_result)",len(x) == 0,not x
chainer,https://github.com/chainer/chainer/tree/master/tests/chainer_tests/testing_tests/test_condition.py,MockUnitTest,probabilistic_case$51,"def probabilistic_case(self):
        MockUnitTest.probabilistic_case_counter += 1
        if MockUnitTest.probabilistic_case_counter % 2 == 0:
            MockUnitTest.probabilistic_case_success_counter += 1
            self.assertTrue(True)
        else:
            MockUnitTest.probabilistic_case_failure_counter += 1
            self.fail()",MockUnitTest.probabilistic_case_counter % 2 == 0,not MockUnitTest.probabilistic_case_counter % 2
transformers,https://github.com/huggingface/transformers/tree/master/src/transformers/models/openai/tokenization_openai.py,BasicTokenizer,_clean_text$191,"def _clean_text(self, text):
        """"""Performs invalid character removal and whitespace cleanup on text.""""""
        output = []
        for char in text:
            cp = ord(char)
            if cp == 0 or cp == 0xFFFD or _is_control(char):
                continue
            if _is_whitespace(char):
                output.append("" "")
            else:
                output.append(char)
        return """".join(output)",cp == 0,not cp
pulsar,https://github.com/quantmind/pulsar/tree/master/examples/philosophers/manage.py,,pickup_fork$95,"def pickup_fork(request, fork_right):
    self = request.actor.app
    num_philosophers = self.cfg.workers
    fork_left = fork_right - 1
    if fork_left == 0:
        fork_left = num_philosophers
    for fork in (fork_right, fork_left):
        if fork not in self.not_available_forks:
            # Fork is available, send it to the philosopher
            self.not_available_forks.add(fork)
            return fork",fork_left == 0,not fork_left
GCNet,https://github.com/xvjiarui/GCNet/tree/master/mmdet/models/detectors/grid_rcnn.py,GridRCNN,simple_test$175,"def simple_test(self, img, img_meta, proposals=None, rescale=False):
        """"""Test without augmentation.""""""
        assert self.with_bbox, ""Bbox head must be implemented.""

        x = self.extract_feat(img)

        proposal_list = self.simple_test_rpn(
            x, img_meta, self.test_cfg.rpn) if proposals is None else proposals

        det_bboxes, det_labels = self.simple_test_bboxes(
            x, img_meta, proposal_list, self.test_cfg.rcnn, rescale=False)

        # pack rois into bboxes
        grid_rois = bbox2roi([det_bboxes[:, :4]])
        grid_feats = self.grid_roi_extractor(
            x[:len(self.grid_roi_extractor.featmap_strides)], grid_rois)
        if grid_rois.shape[0] != 0:
            self.grid_head.test_mode = True
            grid_pred = self.grid_head(grid_feats)
            det_bboxes = self.grid_head.get_bboxes(det_bboxes,
                                                   grid_pred['fused'],
                                                   img_meta)
            if rescale:
                det_bboxes[:, :4] /= img_meta[0]['scale_factor']
        else:
            det_bboxes = torch.Tensor([])

        bbox_results = bbox2result(det_bboxes, det_labels,
                                   self.bbox_head.num_classes)

        return bbox_results",grid_rois.shape[0] != 0,grid_rois.shape[0]
d2l-en,https://github.com/d2l-ai/d2l-en/tree/master/d2l/mxnet.py,Trainer,__init__$270,"def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):
        self.save_hyperparameters()
        assert num_gpus == 0, 'No GPU support yet'",num_gpus == 0,not num_gpus
SeleniumLibrary,https://github.com/robotframework/SeleniumLibrary/tree/master/src/SeleniumLibrary/keywords/tableelement.py,TableElementKeywords,get_table_cell$26,"def get_table_cell(
        self,
        locator: Union[WebElement, None, str],
        row: int,
        column: int,
        loglevel: str = ""TRACE"",
    ) -> str:
        """"""Returns contents of a table cell.

        The table is located using the ``locator`` argument and its cell
        found using ``row`` and ``column``. See the `Locating elements`
        section for details about the locator syntax.

        Both row and column indexes start from 1, and header and footer
        rows are included in the count. It is possible to refer to rows
        and columns from the end by using negative indexes so that -1
        is the last row/column, -2 is the second last, and so on.

        All ``<th>`` and ``<td>`` elements anywhere in the table are
        considered to be cells.

        See `Page Should Contain` for an explanation about the ``loglevel``
        argument.
        """"""
        if row == 0 or column == 0:
            raise ValueError(
                ""Both row and column must be non-zero, ""
                f""got row {row} and column {column}.""
            )
        try:
            cell = self._get_cell(locator, row, column)
        except AssertionError:
            self.log_source(loglevel)
            raise
        return cell.text",row == 0,not row
SeleniumLibrary,https://github.com/robotframework/SeleniumLibrary/tree/master/src/SeleniumLibrary/keywords/tableelement.py,TableElementKeywords,get_table_cell$26,"def get_table_cell(
        self,
        locator: Union[WebElement, None, str],
        row: int,
        column: int,
        loglevel: str = ""TRACE"",
    ) -> str:
        """"""Returns contents of a table cell.

        The table is located using the ``locator`` argument and its cell
        found using ``row`` and ``column``. See the `Locating elements`
        section for details about the locator syntax.

        Both row and column indexes start from 1, and header and footer
        rows are included in the count. It is possible to refer to rows
        and columns from the end by using negative indexes so that -1
        is the last row/column, -2 is the second last, and so on.

        All ``<th>`` and ``<td>`` elements anywhere in the table are
        considered to be cells.

        See `Page Should Contain` for an explanation about the ``loglevel``
        argument.
        """"""
        if row == 0 or column == 0:
            raise ValueError(
                ""Both row and column must be non-zero, ""
                f""got row {row} and column {column}.""
            )
        try:
            cell = self._get_cell(locator, row, column)
        except AssertionError:
            self.log_source(loglevel)
            raise
        return cell.text",column == 0,not column
salt,https://github.com/saltstack/salt/tree/master/tests/pytests/integration/cli/test_salt_call.py,,test_return$311,"def test_return(salt_call_cli, salt_run_cli):
    command = ""echo returnTOmaster""
    ret = salt_call_cli.run(""cmd.run"", command)
    assert ret.exitcode == 0
    assert ret.json == ""returnTOmaster""

    ret = salt_run_cli.run(""jobs.list_jobs"")
    assert ret.exitcode == 0
    jid = target = None
    for jid, details in ret.json.items():
        if command in details[""Arguments""]:
            target = details[""Target""]
            break

    ret = salt_run_cli.run(""jobs.lookup_jid"", jid, _timeout=60)
    assert ret.exitcode == 0
    assert target in ret.json
    assert ret.json[target] == ""returnTOmaster""",ret.exitcode == 0,not ret.exitcode
salt,https://github.com/saltstack/salt/tree/master/tests/pytests/integration/cli/test_salt_call.py,,test_return$311,"def test_return(salt_call_cli, salt_run_cli):
    command = ""echo returnTOmaster""
    ret = salt_call_cli.run(""cmd.run"", command)
    assert ret.exitcode == 0
    assert ret.json == ""returnTOmaster""

    ret = salt_run_cli.run(""jobs.list_jobs"")
    assert ret.exitcode == 0
    jid = target = None
    for jid, details in ret.json.items():
        if command in details[""Arguments""]:
            target = details[""Target""]
            break

    ret = salt_run_cli.run(""jobs.lookup_jid"", jid, _timeout=60)
    assert ret.exitcode == 0
    assert target in ret.json
    assert ret.json[target] == ""returnTOmaster""",ret.exitcode == 0,not ret.exitcode
salt,https://github.com/saltstack/salt/tree/master/tests/pytests/integration/cli/test_salt_call.py,,test_return$311,"def test_return(salt_call_cli, salt_run_cli):
    command = ""echo returnTOmaster""
    ret = salt_call_cli.run(""cmd.run"", command)
    assert ret.exitcode == 0
    assert ret.json == ""returnTOmaster""

    ret = salt_run_cli.run(""jobs.list_jobs"")
    assert ret.exitcode == 0
    jid = target = None
    for jid, details in ret.json.items():
        if command in details[""Arguments""]:
            target = details[""Target""]
            break

    ret = salt_run_cli.run(""jobs.lookup_jid"", jid, _timeout=60)
    assert ret.exitcode == 0
    assert target in ret.json
    assert ret.json[target] == ""returnTOmaster""",ret.exitcode == 0,not ret.exitcode
mmdetection,https://github.com/open-mmlab/mmdetection/tree/master/tests/test_models/test_loss.py,,test_iou_type_loss_zeros_weight$19,"def test_iou_type_loss_zeros_weight(loss_class):
    pred = torch.rand((10, 4))
    target = torch.rand((10, 4))
    weight = torch.zeros(10)

    loss = loss_class()(pred, target, weight)
    assert loss == 0.",loss == 0.0,not loss
great_expectations,https://github.com/great-expectations/great_expectations/tree/master/tests/data_context/test_data_context.py,,test_compile_evaluation_parameter_dependencies$354,"def test_compile_evaluation_parameter_dependencies(
    data_context_parameterized_expectation_suite: DataContext,
):
    assert (
        data_context_parameterized_expectation_suite._evaluation_parameter_dependencies
        == {}
    )
    data_context_parameterized_expectation_suite._compile_evaluation_parameter_dependencies()
    assert (
        data_context_parameterized_expectation_suite._evaluation_parameter_dependencies
        == {
            ""source_diabetes_data.default"": [
                {
                    ""metric_kwargs_id"": {
                        ""column=patient_nbr"": [
                            ""expect_column_unique_value_count_to_be_between.result.observed_value""
                        ]
                    }
                }
            ],
            ""source_patient_data.default"": [
                ""expect_table_row_count_to_equal.result.observed_value""
            ],
        }
    )",data_context_parameterized_expectation_suite._evaluation_parameter_dependencies == {},not data_context_parameterized_expectation_suite._evaluation_parameter_dependencies
Sorcar,https://github.com/aachman98/Sorcar/tree/master//addon_updater.py,Singleton_updater,json$291,"def json(self):
		if self._json == {}:
			self.set_updater_json()
		return self._json",self._json == {},not self._json
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/chainer_/chainercv2/models/irevnet.py,IRevUnit,__init__$228,"def __init__(self,
                 in_channels,
                 out_channels,
                 stride,
                 preactivate):
        super(IRevUnit, self).__init__()
        if not preactivate:
            in_channels = in_channels // 2

        padding = 2 * (out_channels - in_channels)
        self.do_padding = (padding != 0) and (stride == 1)
        self.do_downscale = (stride != 1)

        with self.init_scope():
            if self.do_padding:
                self.pad = IRevInjectivePad(padding)
            self.bottleneck = IRevBottleneck(
                in_channels=in_channels,
                out_channels=out_channels,
                stride=stride,
                preactivate=preactivate)
            if self.do_downscale:
                self.psi = IRevDownscale(stride)",padding != 0,padding
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/downloader/common.py,FileDownloader,calc_eta$101,"def calc_eta(start, now, total, current):
        if total is None:
            return None
        if now is None:
            now = time.time()
        dif = now - start
        if current == 0 or dif < 0.001:  # One millisecond
            return None
        rate = float(current) / dif
        return int((float(total) - float(current)) / rate)",current == 0,not current
pyWhat,https://github.com/bee-san/pyWhat/tree/master/tests/test_click.py,,test_file_fixture_visa$174,"def test_file_fixture_visa():
    runner = CliRunner()
    result = runner.invoke(main, [""-db"", ""fixtures/file""])
    assert result.exit_code == 0
    assert re.findall(""Visa"", str(result.output))",result.exit_code == 0,not result.exit_code
flake8,https://github.com/PyCQA/flake8/tree/master/tests/integration/test_main.py,,test_cli_config_option_respected$310,"def test_cli_config_option_respected(tmp_path):
    """"""Test --config is used.""""""
    config = tmp_path / ""flake8.ini""
    config.write_text(
        """"""\
[flake8]
ignore = F401
""""""
    )

    py_file = tmp_path / ""t.py""
    py_file.write_text(""import os\n"")

    assert cli.main([""--config"", str(config), str(py_file)]) == 0","cli.main(['--config', str(config), str(py_file)]) == 0","not cli.main(['--config', str(config), str(py_file)])"
myscan,https://github.com/amcai/myscan/tree/master/myscan/pocs/perfolder/coldfusion/poc_coldfusion-cve-2010-2861-lfi_2010.py,POC,verify$21,"def verify(self):
        # 鏍规嵁config.py 閰嶇疆鐨勬繁搴︼紝闄愬畾涓涓嬬洰褰曟繁搴
        if self.url.count(""/"") > int(scan_set.get(""max_dir"", 2)) + 2:
            return
        req = {
            ""method"": ""GET"",
            ""url"": self.url + ""CFIDE/administrator/enter.cfm?locale=../../../../../../../lib/password.properties%00en"",
            ""headers"": self.dictdata.get(""request"").get(""headers""),  # 涓昏佷繚鐣檆ookie绛塰eaders
            ""timeout"": 10,
            ""verify"": False,
        }
        r = request(**req)
        if r != None and r.status_code == 200 and b""rdspassword="" in r.content and b""encrypted="" in r.content:
            parser_ = response_parser(r)
            self.result.append({
                ""name"": self.name,
                ""url"": parser_.geturl(),
                ""level"": self.level,  # 0:Low  1:Medium 2:High
                ""detail"": {
                    ""vulmsg"": self.vulmsg,
                    ""request"": parser_.getrequestraw(),
                    ""response"": parser_.getresponseraw()
                }
            })",r != None,r
dpark,https://github.com/douban/dpark/tree/master/dpark/task.py,BucketDumper,_get_tmp$312,"def _get_tmp(self, reduce_id, is_final, size):
        # each reduce has one tmp
        # each tmp may be opened and appended multi times

        i = reduce_id
        tmp_paths = self.tmp_paths[i]
        if tmp_paths:
            tmp_path = tmp_paths[0]
        else:
            if is_final and self.num_dump == 0:
                tmp_path = ShuffleWorkDir.alloc_tmp(datasize=size)
            else:
                tmp_path = ShuffleWorkDir.alloc_tmp(mem_first=False)
            tmp_paths.append(tmp_path)

        return tmp_path",self.num_dump == 0,not self.num_dump
clusterfuzz,https://github.com/google/clusterfuzz/tree/master/src/appengine/handlers/testcase_detail/show.py,,highlight_common_stack_frames$99,"def highlight_common_stack_frames(crash_stacktrace):
  """"""Highlights common stack frames between first two stacks.""""""
  crash_stacks = [[]]
  highlighted_crash_stacktrace_lines = []
  old_frame_no = 0
  stack_index = 0
  stack_trace_line_format = '^ *#([0-9]+) *0x[0-9a-f]+ (.*)'

  for line in crash_stacktrace.splitlines():
    # Stacktrace separator prefix.
    if stack_index and line.startswith('+-'):
      break

    match = re.match(stack_trace_line_format, line)
    if match:
      frame_no = int(match.group(1))

      # This means we encountered another stack like free or alloc stack.
      if old_frame_no > frame_no:
        stack_index += 1
        crash_stacks.append([])

      crash_stacks[stack_index].append(match.group(2))
      old_frame_no = frame_no

  # If we have just one crash stack and no other stack,
  # then nothing to highlight.
  if stack_index == 0:
    return crash_stacktrace

  # Compare stack frames between first two stacks.
  match_index = -1
  start_index_crash_stack_1 = len(crash_stacks[0]) - 1
  start_index_crash_stack_2 = len(crash_stacks[1]) - 1
  while True:
    if (crash_stacks[0][start_index_crash_stack_1] !=
        crash_stacks[1][start_index_crash_stack_2]):
      break

    match_index = [start_index_crash_stack_1, start_index_crash_stack_2]

    if not start_index_crash_stack_1:
      break
    if not start_index_crash_stack_2:
      break

    start_index_crash_stack_1 -= 1
    start_index_crash_stack_2 -= 1

  # No match found, nothing to highlight.
  if match_index == -1:
    return crash_stacktrace

  old_frame_no = 0
  stack_index = 0
  frame_index = -1
  for line in crash_stacktrace.splitlines():
    match = re.match(stack_trace_line_format, line)
    if match:
      frame_no = int(match.group(1))

      # This means we encountered another stack like free or alloc stack.
      if old_frame_no > frame_no:
        stack_index += 1
        frame_index = -1

      frame_index += 1
      old_frame_no = frame_no

      # We only care about highlighting the first two stacks.
      if stack_index <= 1 and frame_index >= match_index[stack_index]:
        line = '<b>%s</b>' % line

    highlighted_crash_stacktrace_lines.append(line)

  return '\n'.join(highlighted_crash_stacktrace_lines)",stack_index == 0,not stack_index
MONAI,https://github.com/Project-MONAI/MONAI/tree/master/monai/transforms/croppad/dictionary.py,RandSpatialCropd,__call__$580,"def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:
        d = dict(data)
        first_key: Union[Hashable, List] = self.first_key(d)
        if first_key == []:
            return d

        self.randomize(d[first_key].shape[1:])  # type: ignore
        if self._size is None:
            raise RuntimeError(""self._size not specified."")
        for key in self.key_iterator(d):
            if self.random_center:
                self.push_transform(d, key, {""slices"": [(i.start, i.stop) for i in self._slices[1:]]})  # type: ignore
                d[key] = d[key][self._slices]
            else:
                self.push_transform(d, key)
                cropper = CenterSpatialCrop(self._size)
                d[key] = cropper(d[key])
        return d",first_key == [],not first_key
DetectoRS,https://github.com/joe-siyuan-qiao/DetectoRS/tree/master/tools/test_robustness.py,,multi_gpu_test$112,"def multi_gpu_test(model, data_loader, tmpdir=None):
    model.eval()
    results = []
    dataset = data_loader.dataset
    rank, world_size = get_dist_info()
    if rank == 0:
        prog_bar = mmcv.ProgressBar(len(dataset))
    for i, data in enumerate(data_loader):
        with torch.no_grad():
            result = model(return_loss=False, rescale=True, **data)
        results.append(result)

        if rank == 0:
            batch_size = data['img'][0].size(0)
            for _ in range(batch_size * world_size):
                prog_bar.update()

    # collect results from all ranks
    results = collect_results(results, len(dataset), tmpdir)

    return results",rank == 0,not rank
DetectoRS,https://github.com/joe-siyuan-qiao/DetectoRS/tree/master/tools/test_robustness.py,,multi_gpu_test$112,"def multi_gpu_test(model, data_loader, tmpdir=None):
    model.eval()
    results = []
    dataset = data_loader.dataset
    rank, world_size = get_dist_info()
    if rank == 0:
        prog_bar = mmcv.ProgressBar(len(dataset))
    for i, data in enumerate(data_loader):
        with torch.no_grad():
            result = model(return_loss=False, rescale=True, **data)
        results.append(result)

        if rank == 0:
            batch_size = data['img'][0].size(0)
            for _ in range(batch_size * world_size):
                prog_bar.update()

    # collect results from all ranks
    results = collect_results(results, len(dataset), tmpdir)

    return results",rank == 0,not rank
zvt,https://github.com/zvtvz/zvt/tree/master/tests/api/test_trading.py,,test_000778_manager_trading$29,"def test_000778_manager_trading():
    result: List[ManagerTrading] = ManagerTrading.query_data(session=session, provider='eastmoney',
                                                             return_type='domain',
                                                             codes=['000778'],
                                                             end_timestamp='2018-09-30',
                                                             start_timestamp='2017-09-30',
                                                             order=ManagerTrading.holding.desc())
    assert len(result) == 1
    assert result[0].trading_person == '宸╁浗骞'
    assert result[0].volume == 8400
    assert result[0].price == None
    assert result[0].holding == 18700
    assert result[0].trading_way == '澧炴寔'
    assert result[0].manager_position == '鑱屽伐鐩戜簨'
    assert result[0].manager == '宸╁浗骞'
    assert result[0].relationship_with_manager == '鏈浜'",result[0].price == None,not result[0].price
scvi-tools,https://github.com/YosefLab/scvi-tools/tree/master/scvi/nn/_base_components.py,FCLayers,inject_into_layer$107,"def inject_into_layer(self, layer_num) -> bool:
        """"""Helper to determine if covariates should be injected.""""""
        user_cond = layer_num == 0 or (layer_num > 0 and self.inject_covariates)
        return user_cond",layer_num == 0,not layer_num
PaddleViT,https://github.com/BR-IDL/PaddleViT/tree/master/image_classification/Focal_Transformer/main_multi_gpu.py,,validate$183,"def validate(dataloader,
             model,
             criterion,
             total_batches,
             debug_steps=100,
             local_logger=None,
             master_logger=None):
    """"""Validation for the whole dataset
    Args:
        dataloader: paddle.io.DataLoader, dataloader instance
        model: nn.Layer, a ViT model
        total_batches: int, total num of batches for one epoch
        debug_steps: int, num of iters to log info, default: 100
        local_logger: logger for local process/gpu, default: None
        master_logger: logger for main process, default: None
    Returns:
        val_loss_meter.avg: float, average loss on current process/gpu
        val_acc1_meter.avg: float, average top1 accuracy on current processes/gpus
        val_acc5_meter.avg: float, average top5 accuracy on current processes/gpus
        master_loss_meter.avg: float, average loss on all processes/gpus
        master_acc1_meter.avg: float, average top1 accuracy on all processes/gpus
        master_acc5_meter.avg: float, average top5 accuracy on all processes/gpus
        val_time: float, validation time
    """"""
    model.eval()
    val_loss_meter = AverageMeter()
    val_acc1_meter = AverageMeter()
    val_acc5_meter = AverageMeter()
    master_loss_meter = AverageMeter()
    master_acc1_meter = AverageMeter()
    master_acc5_meter = AverageMeter()

    time_st = time.time()

    for batch_id, data in enumerate(dataloader):
        # get data
        images = data[0]
        label = data[1]
        batch_size = images.shape[0]

        output = model(images)
        loss = criterion(output, label)
        loss_value = loss.item()

        pred = paddle.nn.functional.softmax(output)
        acc1 = paddle.metric.accuracy(pred, label.unsqueeze(1)).item()
        acc5 = paddle.metric.accuracy(pred, label.unsqueeze(1), k=5).item()

        # sync from other gpus for overall loss and acc
        master_loss = all_reduce_mean(loss_value)
        master_acc1 = all_reduce_mean(acc1)
        master_acc5 = all_reduce_mean(acc5)
        master_batch_size = all_reduce_mean(batch_size)

        master_loss_meter.update(master_loss, master_batch_size)
        master_acc1_meter.update(master_acc1, master_batch_size)
        master_acc5_meter.update(master_acc5, master_batch_size)
        val_loss_meter.update(loss_value, batch_size)
        val_acc1_meter.update(acc1, batch_size)
        val_acc5_meter.update(acc5, batch_size)

        if batch_id % debug_steps == 0:
            local_message = (f""Step[{batch_id:04d}/{total_batches:04d}], ""
                             f""Avg Loss: {val_loss_meter.avg:.4f}, ""
                             f""Avg Acc@1: {val_acc1_meter.avg:.4f}, ""
                             f""Avg Acc@5: {val_acc5_meter.avg:.4f}"")
            master_message = (f""Step[{batch_id:04d}/{total_batches:04d}], ""
                              f""Avg Loss: {master_loss_meter.avg:.4f}, ""
                              f""Avg Acc@1: {master_acc1_meter.avg:.4f}, ""
                              f""Avg Acc@5: {master_acc5_meter.avg:.4f}"")
            write_log(local_logger, master_logger, local_message, master_message)
    paddle.distributed.barrier()
    val_time = time.time() - time_st
    return (val_loss_meter.avg,
            val_acc1_meter.avg,
            val_acc5_meter.avg,
            master_loss_meter.avg,
            master_acc1_meter.avg,
            master_acc5_meter.avg,
            val_time)",batch_id % debug_steps == 0,not batch_id % debug_steps
swift,https://github.com/openstack/swift/tree/master/test/unit/obj/test_reconstructor.py,TestGlobalSetupObjectReconstructor,part_1$240,"def part_1(set):
                if set == 0:
                    # one local and all of another
                    if obj_num == 0:
                        return local_id
                    else:
                        return (local_id + 2) % 3
                else:
                    # just the local node
                    return local_id",set == 0,not set
swift,https://github.com/openstack/swift/tree/master/test/unit/obj/test_reconstructor.py,TestGlobalSetupObjectReconstructor,part_1$240,"def part_1(set):
                if set == 0:
                    # one local and all of another
                    if obj_num == 0:
                        return local_id
                    else:
                        return (local_id + 2) % 3
                else:
                    # just the local node
                    return local_id",obj_num == 0,not obj_num
mmdetection-mini,https://github.com/hhaAndroid/mmdetection-mini/tree/master/mmdet/det_core/post_processing/bbox_nms.py,,multiclass_nms$5,"def multiclass_nms(multi_bboxes,
                   multi_scores,
                   score_thr,
                   nms_cfg,
                   max_num=-1,
                   score_factors=None):
    """"""NMS for multi-class bboxes.

    Args:
        multi_bboxes (Tensor): shape (n, #class*4) or (n, 4)
        multi_scores (Tensor): shape (n, #class), where the last column
            contains scores of the background class, but this will be ignored.
        score_thr (float): bbox threshold, bboxes with scores lower than it
            will not be considered.
        nms_thr (float): NMS IoU threshold
        max_num (int): if there are more than max_num bboxes after NMS,
            only top max_num will be kept.
        score_factors (Tensor): The factors multiplied to scores before
            applying NMS

    Returns:
        tuple: (bboxes, labels), tensors of shape (k, 5) and (k, 1). Labels \
            are 0-based.
    """"""
    num_classes = multi_scores.size(1) - 1
    # exclude background category
    if multi_bboxes.shape[1] > 4:
        bboxes = multi_bboxes.view(multi_scores.size(0), -1, 4)
    else:
        bboxes = multi_bboxes[:, None].expand(
            multi_scores.size(0), num_classes, 4)
    scores = multi_scores[:, :-1]

    # filter out boxes with low scores
    valid_mask = scores > score_thr

    # We use masked_select for ONNX exporting purpose,
    # which is equivalent to bboxes = bboxes[valid_mask]
    # (TODO): as ONNX does not support repeat now,
    # we have to use this ugly code
    bboxes = torch.masked_select(
        bboxes,
        torch.stack((valid_mask, valid_mask, valid_mask, valid_mask),
                    -1)).view(-1, 4)
    if score_factors is not None:
        scores = scores * score_factors[:, None]
    scores = torch.masked_select(scores, valid_mask)
    labels = valid_mask.nonzero(as_tuple=False)[:, 1]

    if bboxes.numel() == 0:
        bboxes = multi_bboxes.new_zeros((0, 5))
        labels = multi_bboxes.new_zeros((0, ), dtype=torch.long)

        if torch.onnx.is_in_onnx_export():
            raise RuntimeError('[ONNX Error] Can not record NMS '
                               'as it has not been executed this time')
        return bboxes, labels

    dets, keep = batched_nms(bboxes, scores, labels, nms_cfg)

    if max_num > 0:
        dets = dets[:max_num]
        keep = keep[:max_num]

    return dets, labels[keep]",bboxes.numel() == 0,not bboxes.numel()
magenta,https://github.com/magenta/magenta/tree/master/magenta/models/music_vae/data.py,LegacyEventListOneHotConverter,__init__$376,"def __init__(self, event_list_fn, event_extractor_fn,
               legacy_encoder_decoder, add_end_token=False, slice_bars=None,
               slice_steps=None, steps_per_quarter=None, steps_per_second=None,
               quarters_per_bar=4, pad_to_total_time=False,
               max_tensors_per_notesequence=None,
               presplit_on_time_changes=True, chord_encoding=None,
               condition_on_key=False, dedupe_event_lists=True):
    if (steps_per_quarter, steps_per_second).count(None) != 1:
      raise ValueError(
          'Exactly one of `steps_per_quarter` and `steps_per_second` should be '
          'provided.')
    if (slice_bars, slice_steps).count(None) == 0:
      raise ValueError(
          'At most one of `slice_bars` and `slice_steps` should be provided.')
    self._event_list_fn = event_list_fn
    self._event_extractor_fn = event_extractor_fn
    self._legacy_encoder_decoder = legacy_encoder_decoder
    self._chord_encoding = chord_encoding
    self._condition_on_key = condition_on_key
    self._steps_per_quarter = steps_per_quarter
    if steps_per_quarter:
      self._steps_per_bar = steps_per_quarter * quarters_per_bar
    self._steps_per_second = steps_per_second
    if slice_bars:
      self._slice_steps = self._steps_per_bar * slice_bars
    else:
      self._slice_steps = slice_steps
    self._pad_to_total_time = pad_to_total_time
    self._dedupe_event_lists = dedupe_event_lists

    depth = legacy_encoder_decoder.num_classes + add_end_token
    control_depth = (
        chord_encoding.num_classes if chord_encoding is not None else 0
    ) + (
        12 if condition_on_key else 0
    )
    super(LegacyEventListOneHotConverter, self).__init__(
        input_depth=depth,
        input_dtype=np.bool,
        output_depth=depth,
        output_dtype=np.bool,
        control_depth=control_depth,
        control_dtype=np.bool,
        end_token=legacy_encoder_decoder.num_classes if add_end_token else None,
        presplit_on_time_changes=presplit_on_time_changes,
        max_tensors_per_notesequence=max_tensors_per_notesequence)","(slice_bars, slice_steps).count(None) == 0","not (slice_bars, slice_steps).count(None)"
gandissect,https://github.com/CSAILVision/gandissect/tree/master/netdissect/runningstats.py,RunningQuantile,_expand$318,"def _expand(self):
        cap = self._next_capacity()
        if cap > 0:
            # First, make a new layer of the proper capacity.
            self.data.insert(0, torch.zeros(self.depth, cap,
                dtype=self.dtype, device=self.device))
            self.firstfree.insert(0, 0)
        else:
            # Unless we're so big we are just subsampling.
            assert self.firstfree[0] == 0
            self.samplerate *= 0.5
        for index in range(1, len(self.data)):
            # Scan for existing data that needs to be moved down a level.
            amount = self.firstfree[index]
            if amount == 0:
                continue
            position = self.firstfree[index-1]
            # Move data down if it would leave enough empty space there
            # This is the key invariant: enough empty space to fit half
            # of the previous level's buffer size (rounding up)
            if self.data[index-1].shape[1] - (amount + position) >= (
                    -(-self.data[index-2].shape[1] // 2) if (index-1) else 1):
                self.data[index-1][:,position:position + amount] = (
                        self.data[index][:,:amount])
                self.firstfree[index-1] += amount
                self.firstfree[index] = 0
            else:
                # Scrunch the data if it would not.
                data = self.data[index][:,:amount]
                data = data.sort()[0]
                if index == 1:
                    self._update_extremes(data[:,0], data[:,-1])
                offset = self._randbit()
                scrunched = data[:,offset::2]
                self.data[index][:,:scrunched.shape[1]] = scrunched
                self.firstfree[index] = scrunched.shape[1]
        return cap > 0",self.firstfree[0] == 0,not self.firstfree[0]
gandissect,https://github.com/CSAILVision/gandissect/tree/master/netdissect/runningstats.py,RunningQuantile,_expand$318,"def _expand(self):
        cap = self._next_capacity()
        if cap > 0:
            # First, make a new layer of the proper capacity.
            self.data.insert(0, torch.zeros(self.depth, cap,
                dtype=self.dtype, device=self.device))
            self.firstfree.insert(0, 0)
        else:
            # Unless we're so big we are just subsampling.
            assert self.firstfree[0] == 0
            self.samplerate *= 0.5
        for index in range(1, len(self.data)):
            # Scan for existing data that needs to be moved down a level.
            amount = self.firstfree[index]
            if amount == 0:
                continue
            position = self.firstfree[index-1]
            # Move data down if it would leave enough empty space there
            # This is the key invariant: enough empty space to fit half
            # of the previous level's buffer size (rounding up)
            if self.data[index-1].shape[1] - (amount + position) >= (
                    -(-self.data[index-2].shape[1] // 2) if (index-1) else 1):
                self.data[index-1][:,position:position + amount] = (
                        self.data[index][:,:amount])
                self.firstfree[index-1] += amount
                self.firstfree[index] = 0
            else:
                # Scrunch the data if it would not.
                data = self.data[index][:,:amount]
                data = data.sort()[0]
                if index == 1:
                    self._update_extremes(data[:,0], data[:,-1])
                offset = self._randbit()
                scrunched = data[:,offset::2]
                self.data[index][:,:scrunched.shape[1]] = scrunched
                self.firstfree[index] = scrunched.shape[1]
        return cap > 0",amount == 0,not amount
novelWriter,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True

    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile(""Hello"", nwItemClass.NOVEL,     ""a508bb932959c"")
    cHandle = theProject.newFile(""Jane"",  nwItemClass.CHARACTER, ""afb3043c7b2b3"")

    assert theIndex.getNovelData("""", """") is None
    assert theIndex.getNovelData(""a508bb932959c"", """") is None

    assert theIndex.scanText(cHandle, (
        ""# Jane Smith\n""
        ""@tag: Jane\n""
    ))
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n""
    ))

    # The novel structure should contain the pointer to the novel file header
    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    # Check that excluded files can be skipped
    theProject.projTree[nHandle].setExported(False)

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)

    assert theKeys == []

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == []

    # The novel file should have the correct counts
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 62  # Characters in text and title only
    assert wC == 12  # Words in text and title only
    assert pC == 2   # Paragraphs in text only

    # getReferences
    # =============

    # Look up an ivalid handle
    theRefs = theIndex.getReferences(""Not a handle"")
    assert theRefs[""@pov""] == []
    assert theRefs[""@char""] == []

    # The novel file should now refer to Jane as @pov and @char
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs[""@pov""] == [""Jane""]
    assert theRefs[""@char""] == [""Jane""]

    # getBackReferenceList
    # ====================

    # None handle should return an empty dict
    assert theIndex.getBackReferenceList(None) == {}

    # The character file should have a record of the reference from the novel file
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: ""T000001""}

    # getTagSource
    # ============

    assert theIndex.getTagSource(""Jane"") == (cHandle, 2, ""T000001"")
    assert theIndex.getTagSource(""John"") == (None, 0, ""T000000"")

    # getCounts
    # =========
    # For whole text and sections

    # Get section counts for a novel file
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Get section counts for a note file
    assert theIndex.scanText(cHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Novel Stats
    # ===========

    hHandle = theProject.newFile(""Chapter"", nwItemClass.NOVEL, ""a508bb932959c"")
    sHandle = theProject.newFile(""Scene One"", nwItemClass.NOVEL, ""a508bb932959c"")
    tHandle = theProject.newFile(""Scene Two"", nwItemClass.NOVEL, ""a508bb932959c"")

    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT

    assert theIndex.scanText(hHandle, ""## Chapter One\n\n"")
    assert theIndex.scanText(sHandle, ""### Scene One\n\n"")
    assert theIndex.scanText(tHandle, ""### Scene Two\n\n"")

    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]

    # Add a fake handle to the tree and check that it's ignored
    theProject.projTree._treeOrder.append(""0000000000000"")
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove(""0000000000000"")

    # Extract stats
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]

    # Table of Contents
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 6),
    ]
    assert theIndex.getTableOfContents(3, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 2),
        (""%s:T000001"" % sHandle, 3, ""Scene One"", 2),
        (""%s:T000001"" % tHandle, 3, ""Scene Two"", 2),
    ]

    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [
        (""%s:T000001"" % nHandle, 1, ""Hello World!"", 12),
        (""%s:T000011"" % nHandle, 1, ""Hello World!"", 22),
    ]

    # Header Word Counts
    bHandle = ""0000000000000""
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [(""%s:T000001"" % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [(""%s:T000001"" % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [(""%s:T000001"" % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [
        (""%s:T000001"" % nHandle, 12), (""%s:T000011"" % nHandle, 16)
    ]

    assert theProject.closeProject()

    # Header Record
    bHandle = ""0000000000000""
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [(""T000001"", ""H2"", ""Chapter One"")]
    assert theIndex.getHandleHeaders(sHandle) == [(""T000001"", ""H3"", ""Scene One"")]
    assert theIndex.getHandleHeaders(tHandle) == [(""T000001"", ""H3"", ""Scene Two"")]
    assert theIndex.getHandleHeaders(nHandle) == [
        (""T000001"", ""H1"", ""Hello World!""), (""T000011"", ""H1"", ""Hello World!"")
    ]",theKeys == [],not theKeys
novelWriter,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True

    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile(""Hello"", nwItemClass.NOVEL,     ""a508bb932959c"")
    cHandle = theProject.newFile(""Jane"",  nwItemClass.CHARACTER, ""afb3043c7b2b3"")

    assert theIndex.getNovelData("""", """") is None
    assert theIndex.getNovelData(""a508bb932959c"", """") is None

    assert theIndex.scanText(cHandle, (
        ""# Jane Smith\n""
        ""@tag: Jane\n""
    ))
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n""
    ))

    # The novel structure should contain the pointer to the novel file header
    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    # Check that excluded files can be skipped
    theProject.projTree[nHandle].setExported(False)

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)

    assert theKeys == []

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == []

    # The novel file should have the correct counts
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 62  # Characters in text and title only
    assert wC == 12  # Words in text and title only
    assert pC == 2   # Paragraphs in text only

    # getReferences
    # =============

    # Look up an ivalid handle
    theRefs = theIndex.getReferences(""Not a handle"")
    assert theRefs[""@pov""] == []
    assert theRefs[""@char""] == []

    # The novel file should now refer to Jane as @pov and @char
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs[""@pov""] == [""Jane""]
    assert theRefs[""@char""] == [""Jane""]

    # getBackReferenceList
    # ====================

    # None handle should return an empty dict
    assert theIndex.getBackReferenceList(None) == {}

    # The character file should have a record of the reference from the novel file
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: ""T000001""}

    # getTagSource
    # ============

    assert theIndex.getTagSource(""Jane"") == (cHandle, 2, ""T000001"")
    assert theIndex.getTagSource(""John"") == (None, 0, ""T000000"")

    # getCounts
    # =========
    # For whole text and sections

    # Get section counts for a novel file
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Get section counts for a note file
    assert theIndex.scanText(cHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Novel Stats
    # ===========

    hHandle = theProject.newFile(""Chapter"", nwItemClass.NOVEL, ""a508bb932959c"")
    sHandle = theProject.newFile(""Scene One"", nwItemClass.NOVEL, ""a508bb932959c"")
    tHandle = theProject.newFile(""Scene Two"", nwItemClass.NOVEL, ""a508bb932959c"")

    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT

    assert theIndex.scanText(hHandle, ""## Chapter One\n\n"")
    assert theIndex.scanText(sHandle, ""### Scene One\n\n"")
    assert theIndex.scanText(tHandle, ""### Scene Two\n\n"")

    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]

    # Add a fake handle to the tree and check that it's ignored
    theProject.projTree._treeOrder.append(""0000000000000"")
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove(""0000000000000"")

    # Extract stats
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]

    # Table of Contents
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 6),
    ]
    assert theIndex.getTableOfContents(3, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 2),
        (""%s:T000001"" % sHandle, 3, ""Scene One"", 2),
        (""%s:T000001"" % tHandle, 3, ""Scene Two"", 2),
    ]

    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [
        (""%s:T000001"" % nHandle, 1, ""Hello World!"", 12),
        (""%s:T000011"" % nHandle, 1, ""Hello World!"", 22),
    ]

    # Header Word Counts
    bHandle = ""0000000000000""
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [(""%s:T000001"" % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [(""%s:T000001"" % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [(""%s:T000001"" % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [
        (""%s:T000001"" % nHandle, 12), (""%s:T000011"" % nHandle, 16)
    ]

    assert theProject.closeProject()

    # Header Record
    bHandle = ""0000000000000""
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [(""T000001"", ""H2"", ""Chapter One"")]
    assert theIndex.getHandleHeaders(sHandle) == [(""T000001"", ""H3"", ""Scene One"")]
    assert theIndex.getHandleHeaders(tHandle) == [(""T000001"", ""H3"", ""Scene Two"")]
    assert theIndex.getHandleHeaders(nHandle) == [
        (""T000001"", ""H1"", ""Hello World!""), (""T000011"", ""H1"", ""Hello World!"")
    ]",theKeys == [],not theKeys
novelWriter,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True

    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile(""Hello"", nwItemClass.NOVEL,     ""a508bb932959c"")
    cHandle = theProject.newFile(""Jane"",  nwItemClass.CHARACTER, ""afb3043c7b2b3"")

    assert theIndex.getNovelData("""", """") is None
    assert theIndex.getNovelData(""a508bb932959c"", """") is None

    assert theIndex.scanText(cHandle, (
        ""# Jane Smith\n""
        ""@tag: Jane\n""
    ))
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n""
    ))

    # The novel structure should contain the pointer to the novel file header
    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    # Check that excluded files can be skipped
    theProject.projTree[nHandle].setExported(False)

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)

    assert theKeys == []

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == []

    # The novel file should have the correct counts
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 62  # Characters in text and title only
    assert wC == 12  # Words in text and title only
    assert pC == 2   # Paragraphs in text only

    # getReferences
    # =============

    # Look up an ivalid handle
    theRefs = theIndex.getReferences(""Not a handle"")
    assert theRefs[""@pov""] == []
    assert theRefs[""@char""] == []

    # The novel file should now refer to Jane as @pov and @char
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs[""@pov""] == [""Jane""]
    assert theRefs[""@char""] == [""Jane""]

    # getBackReferenceList
    # ====================

    # None handle should return an empty dict
    assert theIndex.getBackReferenceList(None) == {}

    # The character file should have a record of the reference from the novel file
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: ""T000001""}

    # getTagSource
    # ============

    assert theIndex.getTagSource(""Jane"") == (cHandle, 2, ""T000001"")
    assert theIndex.getTagSource(""John"") == (None, 0, ""T000000"")

    # getCounts
    # =========
    # For whole text and sections

    # Get section counts for a novel file
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Get section counts for a note file
    assert theIndex.scanText(cHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Novel Stats
    # ===========

    hHandle = theProject.newFile(""Chapter"", nwItemClass.NOVEL, ""a508bb932959c"")
    sHandle = theProject.newFile(""Scene One"", nwItemClass.NOVEL, ""a508bb932959c"")
    tHandle = theProject.newFile(""Scene Two"", nwItemClass.NOVEL, ""a508bb932959c"")

    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT

    assert theIndex.scanText(hHandle, ""## Chapter One\n\n"")
    assert theIndex.scanText(sHandle, ""### Scene One\n\n"")
    assert theIndex.scanText(tHandle, ""### Scene Two\n\n"")

    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]

    # Add a fake handle to the tree and check that it's ignored
    theProject.projTree._treeOrder.append(""0000000000000"")
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove(""0000000000000"")

    # Extract stats
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]

    # Table of Contents
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 6),
    ]
    assert theIndex.getTableOfContents(3, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 2),
        (""%s:T000001"" % sHandle, 3, ""Scene One"", 2),
        (""%s:T000001"" % tHandle, 3, ""Scene Two"", 2),
    ]

    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [
        (""%s:T000001"" % nHandle, 1, ""Hello World!"", 12),
        (""%s:T000011"" % nHandle, 1, ""Hello World!"", 22),
    ]

    # Header Word Counts
    bHandle = ""0000000000000""
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [(""%s:T000001"" % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [(""%s:T000001"" % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [(""%s:T000001"" % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [
        (""%s:T000001"" % nHandle, 12), (""%s:T000011"" % nHandle, 16)
    ]

    assert theProject.closeProject()

    # Header Record
    bHandle = ""0000000000000""
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [(""T000001"", ""H2"", ""Chapter One"")]
    assert theIndex.getHandleHeaders(sHandle) == [(""T000001"", ""H3"", ""Scene One"")]
    assert theIndex.getHandleHeaders(tHandle) == [(""T000001"", ""H3"", ""Scene Two"")]
    assert theIndex.getHandleHeaders(nHandle) == [
        (""T000001"", ""H1"", ""Hello World!""), (""T000011"", ""H1"", ""Hello World!"")
    ]",theRefs['@pov'] == [],not theRefs['@pov']
novelWriter,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True

    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile(""Hello"", nwItemClass.NOVEL,     ""a508bb932959c"")
    cHandle = theProject.newFile(""Jane"",  nwItemClass.CHARACTER, ""afb3043c7b2b3"")

    assert theIndex.getNovelData("""", """") is None
    assert theIndex.getNovelData(""a508bb932959c"", """") is None

    assert theIndex.scanText(cHandle, (
        ""# Jane Smith\n""
        ""@tag: Jane\n""
    ))
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n""
    ))

    # The novel structure should contain the pointer to the novel file header
    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    # Check that excluded files can be skipped
    theProject.projTree[nHandle].setExported(False)

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)

    assert theKeys == []

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == []

    # The novel file should have the correct counts
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 62  # Characters in text and title only
    assert wC == 12  # Words in text and title only
    assert pC == 2   # Paragraphs in text only

    # getReferences
    # =============

    # Look up an ivalid handle
    theRefs = theIndex.getReferences(""Not a handle"")
    assert theRefs[""@pov""] == []
    assert theRefs[""@char""] == []

    # The novel file should now refer to Jane as @pov and @char
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs[""@pov""] == [""Jane""]
    assert theRefs[""@char""] == [""Jane""]

    # getBackReferenceList
    # ====================

    # None handle should return an empty dict
    assert theIndex.getBackReferenceList(None) == {}

    # The character file should have a record of the reference from the novel file
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: ""T000001""}

    # getTagSource
    # ============

    assert theIndex.getTagSource(""Jane"") == (cHandle, 2, ""T000001"")
    assert theIndex.getTagSource(""John"") == (None, 0, ""T000000"")

    # getCounts
    # =========
    # For whole text and sections

    # Get section counts for a novel file
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Get section counts for a note file
    assert theIndex.scanText(cHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Novel Stats
    # ===========

    hHandle = theProject.newFile(""Chapter"", nwItemClass.NOVEL, ""a508bb932959c"")
    sHandle = theProject.newFile(""Scene One"", nwItemClass.NOVEL, ""a508bb932959c"")
    tHandle = theProject.newFile(""Scene Two"", nwItemClass.NOVEL, ""a508bb932959c"")

    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT

    assert theIndex.scanText(hHandle, ""## Chapter One\n\n"")
    assert theIndex.scanText(sHandle, ""### Scene One\n\n"")
    assert theIndex.scanText(tHandle, ""### Scene Two\n\n"")

    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]

    # Add a fake handle to the tree and check that it's ignored
    theProject.projTree._treeOrder.append(""0000000000000"")
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove(""0000000000000"")

    # Extract stats
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]

    # Table of Contents
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 6),
    ]
    assert theIndex.getTableOfContents(3, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 2),
        (""%s:T000001"" % sHandle, 3, ""Scene One"", 2),
        (""%s:T000001"" % tHandle, 3, ""Scene Two"", 2),
    ]

    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [
        (""%s:T000001"" % nHandle, 1, ""Hello World!"", 12),
        (""%s:T000011"" % nHandle, 1, ""Hello World!"", 22),
    ]

    # Header Word Counts
    bHandle = ""0000000000000""
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [(""%s:T000001"" % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [(""%s:T000001"" % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [(""%s:T000001"" % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [
        (""%s:T000001"" % nHandle, 12), (""%s:T000011"" % nHandle, 16)
    ]

    assert theProject.closeProject()

    # Header Record
    bHandle = ""0000000000000""
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [(""T000001"", ""H2"", ""Chapter One"")]
    assert theIndex.getHandleHeaders(sHandle) == [(""T000001"", ""H3"", ""Scene One"")]
    assert theIndex.getHandleHeaders(tHandle) == [(""T000001"", ""H3"", ""Scene Two"")]
    assert theIndex.getHandleHeaders(nHandle) == [
        (""T000001"", ""H1"", ""Hello World!""), (""T000011"", ""H1"", ""Hello World!"")
    ]",theRefs['@char'] == [],not theRefs['@char']
novelWriter,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True

    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile(""Hello"", nwItemClass.NOVEL,     ""a508bb932959c"")
    cHandle = theProject.newFile(""Jane"",  nwItemClass.CHARACTER, ""afb3043c7b2b3"")

    assert theIndex.getNovelData("""", """") is None
    assert theIndex.getNovelData(""a508bb932959c"", """") is None

    assert theIndex.scanText(cHandle, (
        ""# Jane Smith\n""
        ""@tag: Jane\n""
    ))
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n""
    ))

    # The novel structure should contain the pointer to the novel file header
    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    # Check that excluded files can be skipped
    theProject.projTree[nHandle].setExported(False)

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)

    assert theKeys == []

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == []

    # The novel file should have the correct counts
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 62  # Characters in text and title only
    assert wC == 12  # Words in text and title only
    assert pC == 2   # Paragraphs in text only

    # getReferences
    # =============

    # Look up an ivalid handle
    theRefs = theIndex.getReferences(""Not a handle"")
    assert theRefs[""@pov""] == []
    assert theRefs[""@char""] == []

    # The novel file should now refer to Jane as @pov and @char
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs[""@pov""] == [""Jane""]
    assert theRefs[""@char""] == [""Jane""]

    # getBackReferenceList
    # ====================

    # None handle should return an empty dict
    assert theIndex.getBackReferenceList(None) == {}

    # The character file should have a record of the reference from the novel file
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: ""T000001""}

    # getTagSource
    # ============

    assert theIndex.getTagSource(""Jane"") == (cHandle, 2, ""T000001"")
    assert theIndex.getTagSource(""John"") == (None, 0, ""T000000"")

    # getCounts
    # =========
    # For whole text and sections

    # Get section counts for a novel file
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Get section counts for a note file
    assert theIndex.scanText(cHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Novel Stats
    # ===========

    hHandle = theProject.newFile(""Chapter"", nwItemClass.NOVEL, ""a508bb932959c"")
    sHandle = theProject.newFile(""Scene One"", nwItemClass.NOVEL, ""a508bb932959c"")
    tHandle = theProject.newFile(""Scene Two"", nwItemClass.NOVEL, ""a508bb932959c"")

    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT

    assert theIndex.scanText(hHandle, ""## Chapter One\n\n"")
    assert theIndex.scanText(sHandle, ""### Scene One\n\n"")
    assert theIndex.scanText(tHandle, ""### Scene Two\n\n"")

    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]

    # Add a fake handle to the tree and check that it's ignored
    theProject.projTree._treeOrder.append(""0000000000000"")
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove(""0000000000000"")

    # Extract stats
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]

    # Table of Contents
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 6),
    ]
    assert theIndex.getTableOfContents(3, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 2),
        (""%s:T000001"" % sHandle, 3, ""Scene One"", 2),
        (""%s:T000001"" % tHandle, 3, ""Scene Two"", 2),
    ]

    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [
        (""%s:T000001"" % nHandle, 1, ""Hello World!"", 12),
        (""%s:T000011"" % nHandle, 1, ""Hello World!"", 22),
    ]

    # Header Word Counts
    bHandle = ""0000000000000""
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [(""%s:T000001"" % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [(""%s:T000001"" % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [(""%s:T000001"" % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [
        (""%s:T000001"" % nHandle, 12), (""%s:T000011"" % nHandle, 16)
    ]

    assert theProject.closeProject()

    # Header Record
    bHandle = ""0000000000000""
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [(""T000001"", ""H2"", ""Chapter One"")]
    assert theIndex.getHandleHeaders(sHandle) == [(""T000001"", ""H3"", ""Scene One"")]
    assert theIndex.getHandleHeaders(tHandle) == [(""T000001"", ""H3"", ""Scene Two"")]
    assert theIndex.getHandleHeaders(nHandle) == [
        (""T000001"", ""H1"", ""Hello World!""), (""T000011"", ""H1"", ""Hello World!"")
    ]",theIndex.getBackReferenceList(None) == {},not theIndex.getBackReferenceList(None)
novelWriter,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True

    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile(""Hello"", nwItemClass.NOVEL,     ""a508bb932959c"")
    cHandle = theProject.newFile(""Jane"",  nwItemClass.CHARACTER, ""afb3043c7b2b3"")

    assert theIndex.getNovelData("""", """") is None
    assert theIndex.getNovelData(""a508bb932959c"", """") is None

    assert theIndex.scanText(cHandle, (
        ""# Jane Smith\n""
        ""@tag: Jane\n""
    ))
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n""
    ))

    # The novel structure should contain the pointer to the novel file header
    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    # Check that excluded files can be skipped
    theProject.projTree[nHandle].setExported(False)

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)

    assert theKeys == []

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == []

    # The novel file should have the correct counts
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 62  # Characters in text and title only
    assert wC == 12  # Words in text and title only
    assert pC == 2   # Paragraphs in text only

    # getReferences
    # =============

    # Look up an ivalid handle
    theRefs = theIndex.getReferences(""Not a handle"")
    assert theRefs[""@pov""] == []
    assert theRefs[""@char""] == []

    # The novel file should now refer to Jane as @pov and @char
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs[""@pov""] == [""Jane""]
    assert theRefs[""@char""] == [""Jane""]

    # getBackReferenceList
    # ====================

    # None handle should return an empty dict
    assert theIndex.getBackReferenceList(None) == {}

    # The character file should have a record of the reference from the novel file
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: ""T000001""}

    # getTagSource
    # ============

    assert theIndex.getTagSource(""Jane"") == (cHandle, 2, ""T000001"")
    assert theIndex.getTagSource(""John"") == (None, 0, ""T000000"")

    # getCounts
    # =========
    # For whole text and sections

    # Get section counts for a novel file
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Get section counts for a note file
    assert theIndex.scanText(cHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Novel Stats
    # ===========

    hHandle = theProject.newFile(""Chapter"", nwItemClass.NOVEL, ""a508bb932959c"")
    sHandle = theProject.newFile(""Scene One"", nwItemClass.NOVEL, ""a508bb932959c"")
    tHandle = theProject.newFile(""Scene Two"", nwItemClass.NOVEL, ""a508bb932959c"")

    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT

    assert theIndex.scanText(hHandle, ""## Chapter One\n\n"")
    assert theIndex.scanText(sHandle, ""### Scene One\n\n"")
    assert theIndex.scanText(tHandle, ""### Scene Two\n\n"")

    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]

    # Add a fake handle to the tree and check that it's ignored
    theProject.projTree._treeOrder.append(""0000000000000"")
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove(""0000000000000"")

    # Extract stats
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]

    # Table of Contents
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 6),
    ]
    assert theIndex.getTableOfContents(3, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 2),
        (""%s:T000001"" % sHandle, 3, ""Scene One"", 2),
        (""%s:T000001"" % tHandle, 3, ""Scene Two"", 2),
    ]

    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [
        (""%s:T000001"" % nHandle, 1, ""Hello World!"", 12),
        (""%s:T000011"" % nHandle, 1, ""Hello World!"", 22),
    ]

    # Header Word Counts
    bHandle = ""0000000000000""
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [(""%s:T000001"" % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [(""%s:T000001"" % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [(""%s:T000001"" % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [
        (""%s:T000001"" % nHandle, 12), (""%s:T000011"" % nHandle, 16)
    ]

    assert theProject.closeProject()

    # Header Record
    bHandle = ""0000000000000""
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [(""T000001"", ""H2"", ""Chapter One"")]
    assert theIndex.getHandleHeaders(sHandle) == [(""T000001"", ""H3"", ""Scene One"")]
    assert theIndex.getHandleHeaders(tHandle) == [(""T000001"", ""H3"", ""Scene Two"")]
    assert theIndex.getHandleHeaders(nHandle) == [
        (""T000001"", ""H1"", ""Hello World!""), (""T000011"", ""H1"", ""Hello World!"")
    ]","theIndex.getTableOfContents(0, True) == []","not theIndex.getTableOfContents(0, True)"
novelWriter,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True

    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile(""Hello"", nwItemClass.NOVEL,     ""a508bb932959c"")
    cHandle = theProject.newFile(""Jane"",  nwItemClass.CHARACTER, ""afb3043c7b2b3"")

    assert theIndex.getNovelData("""", """") is None
    assert theIndex.getNovelData(""a508bb932959c"", """") is None

    assert theIndex.scanText(cHandle, (
        ""# Jane Smith\n""
        ""@tag: Jane\n""
    ))
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n""
    ))

    # The novel structure should contain the pointer to the novel file header
    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    # Check that excluded files can be skipped
    theProject.projTree[nHandle].setExported(False)

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)

    assert theKeys == []

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == []

    # The novel file should have the correct counts
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 62  # Characters in text and title only
    assert wC == 12  # Words in text and title only
    assert pC == 2   # Paragraphs in text only

    # getReferences
    # =============

    # Look up an ivalid handle
    theRefs = theIndex.getReferences(""Not a handle"")
    assert theRefs[""@pov""] == []
    assert theRefs[""@char""] == []

    # The novel file should now refer to Jane as @pov and @char
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs[""@pov""] == [""Jane""]
    assert theRefs[""@char""] == [""Jane""]

    # getBackReferenceList
    # ====================

    # None handle should return an empty dict
    assert theIndex.getBackReferenceList(None) == {}

    # The character file should have a record of the reference from the novel file
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: ""T000001""}

    # getTagSource
    # ============

    assert theIndex.getTagSource(""Jane"") == (cHandle, 2, ""T000001"")
    assert theIndex.getTagSource(""John"") == (None, 0, ""T000000"")

    # getCounts
    # =========
    # For whole text and sections

    # Get section counts for a novel file
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Get section counts for a note file
    assert theIndex.scanText(cHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Novel Stats
    # ===========

    hHandle = theProject.newFile(""Chapter"", nwItemClass.NOVEL, ""a508bb932959c"")
    sHandle = theProject.newFile(""Scene One"", nwItemClass.NOVEL, ""a508bb932959c"")
    tHandle = theProject.newFile(""Scene Two"", nwItemClass.NOVEL, ""a508bb932959c"")

    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT

    assert theIndex.scanText(hHandle, ""## Chapter One\n\n"")
    assert theIndex.scanText(sHandle, ""### Scene One\n\n"")
    assert theIndex.scanText(tHandle, ""### Scene Two\n\n"")

    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]

    # Add a fake handle to the tree and check that it's ignored
    theProject.projTree._treeOrder.append(""0000000000000"")
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove(""0000000000000"")

    # Extract stats
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]

    # Table of Contents
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 6),
    ]
    assert theIndex.getTableOfContents(3, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 2),
        (""%s:T000001"" % sHandle, 3, ""Scene One"", 2),
        (""%s:T000001"" % tHandle, 3, ""Scene Two"", 2),
    ]

    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [
        (""%s:T000001"" % nHandle, 1, ""Hello World!"", 12),
        (""%s:T000011"" % nHandle, 1, ""Hello World!"", 22),
    ]

    # Header Word Counts
    bHandle = ""0000000000000""
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [(""%s:T000001"" % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [(""%s:T000001"" % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [(""%s:T000001"" % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [
        (""%s:T000001"" % nHandle, 12), (""%s:T000011"" % nHandle, 16)
    ]

    assert theProject.closeProject()

    # Header Record
    bHandle = ""0000000000000""
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [(""T000001"", ""H2"", ""Chapter One"")]
    assert theIndex.getHandleHeaders(sHandle) == [(""T000001"", ""H3"", ""Scene One"")]
    assert theIndex.getHandleHeaders(tHandle) == [(""T000001"", ""H3"", ""Scene Two"")]
    assert theIndex.getHandleHeaders(nHandle) == [
        (""T000001"", ""H1"", ""Hello World!""), (""T000011"", ""H1"", ""Hello World!"")
    ]","theIndex.getTableOfContents(1, True) == []","not theIndex.getTableOfContents(1, True)"
novelWriter,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True

    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile(""Hello"", nwItemClass.NOVEL,     ""a508bb932959c"")
    cHandle = theProject.newFile(""Jane"",  nwItemClass.CHARACTER, ""afb3043c7b2b3"")

    assert theIndex.getNovelData("""", """") is None
    assert theIndex.getNovelData(""a508bb932959c"", """") is None

    assert theIndex.scanText(cHandle, (
        ""# Jane Smith\n""
        ""@tag: Jane\n""
    ))
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n""
    ))

    # The novel structure should contain the pointer to the novel file header
    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    # Check that excluded files can be skipped
    theProject.projTree[nHandle].setExported(False)

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)

    assert theKeys == []

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == []

    # The novel file should have the correct counts
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 62  # Characters in text and title only
    assert wC == 12  # Words in text and title only
    assert pC == 2   # Paragraphs in text only

    # getReferences
    # =============

    # Look up an ivalid handle
    theRefs = theIndex.getReferences(""Not a handle"")
    assert theRefs[""@pov""] == []
    assert theRefs[""@char""] == []

    # The novel file should now refer to Jane as @pov and @char
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs[""@pov""] == [""Jane""]
    assert theRefs[""@char""] == [""Jane""]

    # getBackReferenceList
    # ====================

    # None handle should return an empty dict
    assert theIndex.getBackReferenceList(None) == {}

    # The character file should have a record of the reference from the novel file
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: ""T000001""}

    # getTagSource
    # ============

    assert theIndex.getTagSource(""Jane"") == (cHandle, 2, ""T000001"")
    assert theIndex.getTagSource(""John"") == (None, 0, ""T000000"")

    # getCounts
    # =========
    # For whole text and sections

    # Get section counts for a novel file
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Get section counts for a note file
    assert theIndex.scanText(cHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Novel Stats
    # ===========

    hHandle = theProject.newFile(""Chapter"", nwItemClass.NOVEL, ""a508bb932959c"")
    sHandle = theProject.newFile(""Scene One"", nwItemClass.NOVEL, ""a508bb932959c"")
    tHandle = theProject.newFile(""Scene Two"", nwItemClass.NOVEL, ""a508bb932959c"")

    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT

    assert theIndex.scanText(hHandle, ""## Chapter One\n\n"")
    assert theIndex.scanText(sHandle, ""### Scene One\n\n"")
    assert theIndex.scanText(tHandle, ""### Scene Two\n\n"")

    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]

    # Add a fake handle to the tree and check that it's ignored
    theProject.projTree._treeOrder.append(""0000000000000"")
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove(""0000000000000"")

    # Extract stats
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]

    # Table of Contents
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 6),
    ]
    assert theIndex.getTableOfContents(3, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 2),
        (""%s:T000001"" % sHandle, 3, ""Scene One"", 2),
        (""%s:T000001"" % tHandle, 3, ""Scene Two"", 2),
    ]

    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [
        (""%s:T000001"" % nHandle, 1, ""Hello World!"", 12),
        (""%s:T000011"" % nHandle, 1, ""Hello World!"", 22),
    ]

    # Header Word Counts
    bHandle = ""0000000000000""
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [(""%s:T000001"" % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [(""%s:T000001"" % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [(""%s:T000001"" % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [
        (""%s:T000001"" % nHandle, 12), (""%s:T000011"" % nHandle, 16)
    ]

    assert theProject.closeProject()

    # Header Record
    bHandle = ""0000000000000""
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [(""T000001"", ""H2"", ""Chapter One"")]
    assert theIndex.getHandleHeaders(sHandle) == [(""T000001"", ""H3"", ""Scene One"")]
    assert theIndex.getHandleHeaders(tHandle) == [(""T000001"", ""H3"", ""Scene Two"")]
    assert theIndex.getHandleHeaders(nHandle) == [
        (""T000001"", ""H1"", ""Hello World!""), (""T000011"", ""H1"", ""Hello World!"")
    ]","theIndex.getTableOfContents(0, False) == []","not theIndex.getTableOfContents(0, False)"
novelWriter,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True

    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile(""Hello"", nwItemClass.NOVEL,     ""a508bb932959c"")
    cHandle = theProject.newFile(""Jane"",  nwItemClass.CHARACTER, ""afb3043c7b2b3"")

    assert theIndex.getNovelData("""", """") is None
    assert theIndex.getNovelData(""a508bb932959c"", """") is None

    assert theIndex.scanText(cHandle, (
        ""# Jane Smith\n""
        ""@tag: Jane\n""
    ))
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n""
    ))

    # The novel structure should contain the pointer to the novel file header
    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    # Check that excluded files can be skipped
    theProject.projTree[nHandle].setExported(False)

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)

    assert theKeys == []

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == []

    # The novel file should have the correct counts
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 62  # Characters in text and title only
    assert wC == 12  # Words in text and title only
    assert pC == 2   # Paragraphs in text only

    # getReferences
    # =============

    # Look up an ivalid handle
    theRefs = theIndex.getReferences(""Not a handle"")
    assert theRefs[""@pov""] == []
    assert theRefs[""@char""] == []

    # The novel file should now refer to Jane as @pov and @char
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs[""@pov""] == [""Jane""]
    assert theRefs[""@char""] == [""Jane""]

    # getBackReferenceList
    # ====================

    # None handle should return an empty dict
    assert theIndex.getBackReferenceList(None) == {}

    # The character file should have a record of the reference from the novel file
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: ""T000001""}

    # getTagSource
    # ============

    assert theIndex.getTagSource(""Jane"") == (cHandle, 2, ""T000001"")
    assert theIndex.getTagSource(""John"") == (None, 0, ""T000000"")

    # getCounts
    # =========
    # For whole text and sections

    # Get section counts for a novel file
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Get section counts for a note file
    assert theIndex.scanText(cHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Novel Stats
    # ===========

    hHandle = theProject.newFile(""Chapter"", nwItemClass.NOVEL, ""a508bb932959c"")
    sHandle = theProject.newFile(""Scene One"", nwItemClass.NOVEL, ""a508bb932959c"")
    tHandle = theProject.newFile(""Scene Two"", nwItemClass.NOVEL, ""a508bb932959c"")

    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT

    assert theIndex.scanText(hHandle, ""## Chapter One\n\n"")
    assert theIndex.scanText(sHandle, ""### Scene One\n\n"")
    assert theIndex.scanText(tHandle, ""### Scene Two\n\n"")

    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]

    # Add a fake handle to the tree and check that it's ignored
    theProject.projTree._treeOrder.append(""0000000000000"")
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove(""0000000000000"")

    # Extract stats
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]

    # Table of Contents
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 6),
    ]
    assert theIndex.getTableOfContents(3, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 2),
        (""%s:T000001"" % sHandle, 3, ""Scene One"", 2),
        (""%s:T000001"" % tHandle, 3, ""Scene Two"", 2),
    ]

    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [
        (""%s:T000001"" % nHandle, 1, ""Hello World!"", 12),
        (""%s:T000011"" % nHandle, 1, ""Hello World!"", 22),
    ]

    # Header Word Counts
    bHandle = ""0000000000000""
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [(""%s:T000001"" % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [(""%s:T000001"" % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [(""%s:T000001"" % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [
        (""%s:T000001"" % nHandle, 12), (""%s:T000011"" % nHandle, 16)
    ]

    assert theProject.closeProject()

    # Header Record
    bHandle = ""0000000000000""
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [(""T000001"", ""H2"", ""Chapter One"")]
    assert theIndex.getHandleHeaders(sHandle) == [(""T000001"", ""H3"", ""Scene One"")]
    assert theIndex.getHandleHeaders(tHandle) == [(""T000001"", ""H3"", ""Scene Two"")]
    assert theIndex.getHandleHeaders(nHandle) == [
        (""T000001"", ""H1"", ""Hello World!""), (""T000011"", ""H1"", ""Hello World!"")
    ]",theIndex.getHandleWordCounts(bHandle) == [],not theIndex.getHandleWordCounts(bHandle)
novelWriter,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True

    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile(""Hello"", nwItemClass.NOVEL,     ""a508bb932959c"")
    cHandle = theProject.newFile(""Jane"",  nwItemClass.CHARACTER, ""afb3043c7b2b3"")

    assert theIndex.getNovelData("""", """") is None
    assert theIndex.getNovelData(""a508bb932959c"", """") is None

    assert theIndex.scanText(cHandle, (
        ""# Jane Smith\n""
        ""@tag: Jane\n""
    ))
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n""
    ))

    # The novel structure should contain the pointer to the novel file header
    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    # Check that excluded files can be skipped
    theProject.projTree[nHandle].setExported(False)

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)

    assert theKeys == [""%s:T000001"" % nHandle]

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)

    assert theKeys == []

    theKeys = []
    for aKey, _, _, _ in theIndex.novelStructure():
        theKeys.append(aKey)

    assert theKeys == []

    # The novel file should have the correct counts
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 62  # Characters in text and title only
    assert wC == 12  # Words in text and title only
    assert pC == 2   # Paragraphs in text only

    # getReferences
    # =============

    # Look up an ivalid handle
    theRefs = theIndex.getReferences(""Not a handle"")
    assert theRefs[""@pov""] == []
    assert theRefs[""@char""] == []

    # The novel file should now refer to Jane as @pov and @char
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs[""@pov""] == [""Jane""]
    assert theRefs[""@char""] == [""Jane""]

    # getBackReferenceList
    # ====================

    # None handle should return an empty dict
    assert theIndex.getBackReferenceList(None) == {}

    # The character file should have a record of the reference from the novel file
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: ""T000001""}

    # getTagSource
    # ============

    assert theIndex.getTagSource(""Jane"") == (cHandle, 2, ""T000001"")
    assert theIndex.getTagSource(""John"") == (None, 0, ""T000000"")

    # getCounts
    # =========
    # For whole text and sections

    # Get section counts for a novel file
    assert theIndex.scanText(nHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(nHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Get section counts for a note file
    assert theIndex.scanText(cHandle, (
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really.\n\n""
        ""# Hello World!\n""
        ""@pov: Jane\n""
        ""@char: Jane\n\n""
        ""% this is a comment\n\n""
        ""This is a story about Jane Smith.\n\n""
        ""Well, not really. She's still awesome though.\n""
    ))
    # Whole document
    cC, wC, pC = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4

    # First part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000001"")
    assert cC == 62
    assert wC == 12
    assert pC == 2

    # Second part
    cC, wC, pC = theIndex.getCounts(cHandle, ""T000011"")
    assert cC == 90
    assert wC == 16
    assert pC == 2

    # Novel Stats
    # ===========

    hHandle = theProject.newFile(""Chapter"", nwItemClass.NOVEL, ""a508bb932959c"")
    sHandle = theProject.newFile(""Scene One"", nwItemClass.NOVEL, ""a508bb932959c"")
    tHandle = theProject.newFile(""Scene Two"", nwItemClass.NOVEL, ""a508bb932959c"")

    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT

    assert theIndex.scanText(hHandle, ""## Chapter One\n\n"")
    assert theIndex.scanText(sHandle, ""### Scene One\n\n"")
    assert theIndex.scanText(tHandle, ""### Scene Two\n\n"")

    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]

    # Add a fake handle to the tree and check that it's ignored
    theProject.projTree._treeOrder.append(""0000000000000"")
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove(""0000000000000"")

    # Extract stats
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]

    # Table of Contents
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 6),
    ]
    assert theIndex.getTableOfContents(3, True) == [
        (""%s:T000001"" % hHandle, 2, ""Chapter One"", 2),
        (""%s:T000001"" % sHandle, 3, ""Scene One"", 2),
        (""%s:T000001"" % tHandle, 3, ""Scene Two"", 2),
    ]

    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [
        (""%s:T000001"" % nHandle, 1, ""Hello World!"", 12),
        (""%s:T000011"" % nHandle, 1, ""Hello World!"", 22),
    ]

    # Header Word Counts
    bHandle = ""0000000000000""
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [(""%s:T000001"" % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [(""%s:T000001"" % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [(""%s:T000001"" % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [
        (""%s:T000001"" % nHandle, 12), (""%s:T000011"" % nHandle, 16)
    ]

    assert theProject.closeProject()

    # Header Record
    bHandle = ""0000000000000""
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [(""T000001"", ""H2"", ""Chapter One"")]
    assert theIndex.getHandleHeaders(sHandle) == [(""T000001"", ""H3"", ""Scene One"")]
    assert theIndex.getHandleHeaders(tHandle) == [(""T000001"", ""H3"", ""Scene Two"")]
    assert theIndex.getHandleHeaders(nHandle) == [
        (""T000001"", ""H1"", ""Hello World!""), (""T000011"", ""H1"", ""Hello World!"")
    ]",theIndex.getHandleHeaders(bHandle) == [],not theIndex.getHandleHeaders(bHandle)
vcrpy,https://github.com/kevin1024/vcrpy/tree/master/tests/integration/test_tornado.py,,test_cross_scheme$169,"def test_cross_scheme(get_client, tmpdir, scheme):
    """"""Ensure that requests between schemes are treated separately""""""
    # First fetch a url under http, and then again under https and then
    # ensure that we haven't served anything out of cache, and we have two
    # requests / response pairs in the cassette
    with vcr.use_cassette(str(tmpdir.join(""cross_scheme.yaml""))) as cass:
        yield get(get_client(), ""https://httpbin.org/"")
        yield get(get_client(), ""http://httpbin.org/"")
        assert cass.play_count == 0
        assert len(cass) == 2

    # Then repeat the same requests and ensure both were replayed.
    with vcr.use_cassette(str(tmpdir.join(""cross_scheme.yaml""))) as cass:
        yield get(get_client(), ""https://httpbin.org/"")
        yield get(get_client(), ""http://httpbin.org/"")
        assert cass.play_count == 2",cass.play_count == 0,not cass.play_count
unilm,https://github.com/microsoft/unilm/tree/master/xtune/src/run_tag.py,NoisedDataGenerator,__init__$82,"def __init__(self,
                 label_list,
                 pad_token_label_id,
                 r1_lambda=5.0,
                 r1_on_unswitched_tokens=False,
                 enable_r1_loss=False,
                 disable_backward_kl=False,
                 use_sentence_label_probs=False,
                 use_token_label_probs=False,
                 original_loss=True,
                 noised_loss=False,
                 max_seq_length=512,
                 noised_max_seq_length=512,
                 overall_ratio=1.0,
                 enable_bpe_switch=False,
                 bpe_switch_ratio=0.5,
                 tokenizer_dir=None,
                 do_lower_case=False,
                 tokenizer_languages=None,
                 enable_bpe_sampling=False,
                 bpe_sampling_ratio=0.5,
                 tokenizer=None,
                 sampling_alpha=0.3,
                 sampling_nbest_size=-1,
                 enable_random_noise=False,
                 detach_embeds=False,
                 noise_eps=1e-5,
                 noise_type='uniform',
                 enable_code_switch=False,
                 code_switch_ratio=0.5,
                 dict_dir=None,
                 dict_languages=None,
                 use_average_representations=False,
                 translation_path=None,
                 translate_languages=None,
                 use_align_label_probs=False,
                 enable_data_augmentation=False,
                 augment_ratio=0.0,
                 augment_method=None,
                 r2_lambda=1.0,
                 use_hard_labels=False):
        if enable_code_switch:
            assert dict_dir is not None
            assert dict_languages is not None
        assert tokenizer is not None
        if enable_random_noise:
            assert noise_type in ['uniform', 'normal']
        if enable_r1_loss:
            assert use_token_label_probs or use_sentence_label_probs or (
                    use_align_label_probs and enable_translate_data)

        self.use_average_representations = use_average_representations

        self.n_tokens = 0
        self.n_cs_tokens = 0
        self.r1_lambda = r1_lambda
        self.r1_on_unswitched_tokens = r1_on_unswitched_tokens
        self.original_loss = original_loss
        self.noised_loss = noised_loss
        self.enable_r1_loss = enable_r1_loss
        self.disable_backward_kl = disable_backward_kl
        self.use_align_label_probs = use_align_label_probs
        self.use_sentence_label_probs = use_sentence_label_probs
        self.use_token_label_probs = use_token_label_probs
        self.max_seq_length = max_seq_length
        self.noised_max_seq_length = noised_max_seq_length
        self.overall_ratio = overall_ratio

        self.enable_bpe_switch = enable_bpe_switch
        self.bpe_switch_ratio = bpe_switch_ratio / self.overall_ratio
        assert not self.enable_bpe_switch or self.bpe_switch_ratio <= 1.0
        self.tokenizer_dir = tokenizer_dir
        self.tokenizer_languages = tokenizer_languages

        self.enable_bpe_sampling = enable_bpe_sampling
        self.bpe_sampling_ratio = bpe_sampling_ratio / self.overall_ratio
        assert not self.enable_bpe_sampling or self.bpe_sampling_ratio <= 1.0
        self.tokenizer = tokenizer
        self.sampling_alpha = sampling_alpha
        self.sampling_nbest_size = sampling_nbest_size
        self.enable_random_noise = enable_random_noise
        self.detach_embeds = detach_embeds
        self.noise_eps = noise_eps
        self.noise_type = noise_type

        self.enable_code_switch = enable_code_switch
        self.code_switch_ratio = code_switch_ratio / self.overall_ratio
        assert not self.enable_code_switch or self.code_switch_ratio <= 1.0
        self.dict_dir = dict_dir
        self.dict_languages = []
        self.lang2dict = {}
        for lang in dict_languages:
            # dict_path = os.path.join(self.dict_dir, ""{}2.txt"".format(lang))
            dict_path = os.path.join(self.dict_dir, ""en-{}.txt"".format(lang))
            if not os.path.exists(dict_path):
                logger.info(""dictionary en-{} doesn't exist."".format(lang))
                continue
            self.dict_languages.append(lang)
            logger.info(""reading dictionary from {}"".format(dict_path))
            with open(dict_path, ""r"", encoding=""utf-8"") as reader:
                raw = reader.readlines()
            self.lang2dict[lang] = {}
            for line in raw:
                line = line.strip()
                try:
                    src, tgt = line.split(""\t"")
                except:
                    src, tgt = line.split("" "")
                if src not in self.lang2dict[lang]:
                    self.lang2dict[lang][src] = [tgt]
                else:
                    self.lang2dict[lang][src].append(tgt)

        self.lang2tokenizer = {}
        for lang in tokenizer_languages:
            self.lang2tokenizer[lang] = XLMRobertaTokenizer.from_pretrained(
                os.path.join(tokenizer_dir, ""{}"".format(lang)), do_lower_case=do_lower_case)

        self.translation_path = translation_path
        self.translate_languages = translate_languages
        self.augment_method = augment_method
        self.enable_data_augmentation = enable_data_augmentation
        if self.enable_data_augmentation and self.augment_method == ""mt"":
            drop_languages = [""en"", ""zh-CN"", ""zh"", ""ja"", ""ko"", ""th"", ""my"", ""ml"", ""ta""]
            for lang in drop_languages:
                if lang in self.translate_languages:
                    self.translate_languages.remove(lang)
            # self.translate_languages = [""de""]
            self.src2tgt = {}
            logger.info(""Reading translation from {}"".format(self.translation_path))
            with open(self.translation_path, encoding=""utf-8"") as f:
                line_cnt = 0
                for line in f:
                    # if line_cnt == 100:
                    #     exit(0)
                    line_cnt += 1
                    if line_cnt % 10000 == 0:
                        print(""Reading lines {}"".format(line_cnt))
                    items = line.split(""\t"")
                    if len(items) == 3:
                        src_sent, tgt_lang, tgt_sent = line.split(""\t"")
                        alignment = None
                    else:
                        src_sent, tgt_lang, tgt_sent, alignment_str = line.split(""\t"")
                        alignment = []
                        for x in alignment_str.split("" ""):
                            alignment.append((int(x.split(""/"")[0]), int(x.split(""/"")[1])))

                    if tgt_lang in drop_languages:
                        continue
                    if self.translate_languages is not None and tgt_lang not in self.translate_languages:
                        continue
                    if src_sent not in self.src2tgt:
                        self.src2tgt[src_sent] = []
                    if alignment is not None:
                        n_src = len(src_sent.split("" ""))
                        n_tgt = len(tgt_sent.split("" ""))
                        parent = list(range(0, n_src + n_tgt))
                        for x in alignment:
                            x_src = x[0]
                            x_tgt = x[1] + n_src
                            if get_root(x_src, parent) != get_root(x_tgt, parent):
                                parent[x_src] = get_root(x_tgt, parent)

                        cnt = [0] * (n_src + n_tgt)
                        for i in range(n_src + n_tgt):
                            cnt[get_root(i, parent)] += 1

                        align_pooling_id = [0] * (n_src + n_tgt)
                        root2id = {}
                        for i in range(n_src + n_tgt):
                            if cnt[get_root(i, parent)] == 1:
                                continue
                            if not get_root(i, parent) in root2id:
                                root2id[get_root(i, parent)] = len(root2id) + 1
                            align_pooling_id[i] = root2id[get_root(i, parent)]
                        # print(align_pooling_id[:n_src], align_pooling_id[n_src:])
                        self.src2tgt[src_sent].append(
                            (tgt_lang, tgt_sent, (align_pooling_id[:n_src], align_pooling_id[n_src:])))
                    else:
                        self.src2tgt[src_sent].append(
                            (tgt_lang, tgt_sent, None))
                    # print(align_pooling_id[:n_src], align_pooling_id[n_src:])

        self.enable_data_augmentation = enable_data_augmentation
        self.augment_ratio = augment_ratio
        self.r2_lambda = r2_lambda
        self.use_hard_labels = use_hard_labels

        self.label_list = label_list
        self.cls_token_at_end = False
        self.cls_token = self.tokenizer.cls_token
        self.cls_token_segment_id = 0
        self.sep_token = self.tokenizer.sep_token
        self.sep_token_extra = True
        self.pad_on_left = False
        self.pad_token = self.tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]
        self.pad_token_segment_id = 0
        self.pad_token_label_id = pad_token_label_id
        self.sequence_a_segment_id = 0
        self.mask_padding_with_zero = True",line_cnt % 10000 == 0,not line_cnt % 10000
R-Drop,https://github.com/dropreg/R-Drop/tree/master/fairseq_src/fairseq/model_parallel/models/pipeline_parallel_transformer/model.py,PipelineParallelTransformerModel,build_embedding$193,"def build_embedding(dictionary, embed_dim, path=None, num_embed_chunks=1):
            assert embed_dim % num_embed_chunks == 0, (
                f""Number of embedding chunks = {num_embed_chunks} should be ""
                + f""divisible by the embedding dimension = {embed_dim}""
            )
            assert path is None or num_embed_chunks == 1, (
                ""Loading embedding from a path with number of embedding chunks > 1""
                + "" is not yet supported""
            )
            num_embeddings = len(dictionary)
            padding_idx = dictionary.pad()
            # if provided, load from preloaded dictionaries
            if path:
                emb = Embedding(num_embeddings, embed_dim, padding_idx)
                embed_dict = utils.parse_embedding(path)
                utils.load_embedding(embed_dict, dictionary, emb)
            else:
                embed_chunk_dim = embed_dim // num_embed_chunks
                emb = nn.ModuleList()
                for i in range(num_embed_chunks):
                    emb.append(Embedding(num_embeddings, embed_chunk_dim, padding_idx))
            return emb",embed_dim % num_embed_chunks == 0,not embed_dim % num_embed_chunks
tf-explain,https://github.com/sicara/tf-explain/tree/master/tf_explain/utils/display.py,,heatmap_display$92,"def heatmap_display(
    heatmap, original_image, colormap=cv2.COLORMAP_VIRIDIS, image_weight=0.7
):
    """"""
    Apply a heatmap (numpy.ndarray) on top of an original image.

    Args:
        heatmap (numpy.ndarray): Array corresponding to the heatmap
        original_image (numpy.ndarray): Image on which we apply the heatmap
        colormap (int): OpenCV Colormap to use for heatmap visualization
        image_weight (float): An optional `float` value in range [0,1] indicating the weight of
            the input image to be overlaying the calculated attribution maps. Defaults to `0.7`

    Returns:
        numpy.ndarray: Original image with heatmap applied
    """"""
    image = image_to_uint_255(original_image)

    heatmap = cv2.resize(heatmap, (original_image.shape[1], original_image.shape[0]))

    if (heatmap.max() - heatmap.min()) != 0:
        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())

    heatmap = cv2.applyColorMap(
        cv2.cvtColor((heatmap * 255).astype(""uint8""), cv2.COLOR_GRAY2BGR), colormap
    )

    output = cv2.addWeighted(
        cv2.cvtColor(image, cv2.COLOR_RGB2BGR),
        image_weight,
        heatmap,
        1 - image_weight,
        0,
    )

    return cv2.cvtColor(output, cv2.COLOR_BGR2RGB)",heatmap.max() - heatmap.min() != 0,heatmap.max() - heatmap.min()
contextualbandits,https://github.com/david-cortes/contextualbandits/tree/master/contextualbandits/online.py,ExploreFirst,_predict$2316,"def _predict(self, X, exploit = False):
        X = _check_X_input(X)
        
        if X.shape[0] == 0:
            return np.array([])
        
        if exploit:
            return self._oracles.predict(X)
        
        if self.explore_cnt < self.explore_rounds:
            self.explore_cnt += X.shape[0]
            
            # case 1: all predictions are within allowance
            if self.explore_cnt <= self.explore_rounds:
                pred = self.random_state.integers(self.nchoices, size = X.shape[0])
                self._choose_active(X, pred)
                return pred
            
            # case 2: some predictions are within allowance, others are not
            else:
                n_explore = self.explore_rounds - self.explore_cnt + X.shape[0]
                pred = np.empty(X.shape[0], dtype = ctypes.c_double)
                pred[:n_explore] = self.random_state.integers(self.nchoices, n_explore)
                self._choose_active(X[:n_explore], pred[:n_explore])
                pred[n_explore:] = self._oracles.predict(X[n_explore:])
                return pred
        else:
            return self._oracles.predict(X)",X.shape[0] == 0,not X.shape[0]
salt,https://github.com/saltstack/salt/tree/master/salt/modules/win_firewall.py,,add_rule$162,"def add_rule(name, localport, protocol=""tcp"", action=""allow"", dir=""in"", remoteip=""any""):
    """"""
    .. versionadded:: 2015.5.0

    Add a new inbound or outbound rule to the firewall policy

    Args:

        name (str): The name of the rule. Must be unique and cannot be ""all"".
            Required.

        localport (int): The port the rule applies to. Must be a number between
            0 and 65535. Can be a range. Can specify multiple ports separated by
            commas. Required.

        protocol (Optional[str]): The protocol. Can be any of the following:

            - A number between 0 and 255
            - icmpv4
            - icmpv6
            - tcp
            - udp
            - any

        action (Optional[str]): The action the rule performs. Can be any of the
            following:

            - allow
            - block
            - bypass

        dir (Optional[str]): The direction. Can be ``in`` or ``out``.

        remoteip (Optional [str]): The remote IP. Can be any of the following:

            - any
            - localsubnet
            - dns
            - dhcp
            - wins
            - defaultgateway
            - Any valid IPv4 address (192.168.0.12)
            - Any valid IPv6 address (2002:9b3b:1a31:4:208:74ff:fe39:6c43)
            - Any valid subnet (192.168.1.0/24)
            - Any valid range of IP addresses (192.168.0.1-192.168.0.12)
            - A list of valid IP addresses

            Can be combinations of the above separated by commas.

    Returns:
        bool: True if successful

    Raises:
        CommandExecutionError: If the command fails

    CLI Example:

    .. code-block:: bash

        salt '*' firewall.add_rule 'test' '8080' 'tcp'
        salt '*' firewall.add_rule 'test' '1' 'icmpv4'
        salt '*' firewall.add_rule 'test_remote_ip' '8000' 'tcp' 'allow' 'in' '192.168.0.1'
    """"""
    cmd = [
        ""netsh"",
        ""advfirewall"",
        ""firewall"",
        ""add"",
        ""rule"",
        ""name={}"".format(name),
        ""protocol={}"".format(protocol),
        ""dir={}"".format(dir),
        ""action={}"".format(action),
        ""remoteip={}"".format(remoteip),
    ]

    if protocol is None or (""icmpv4"" not in protocol and ""icmpv6"" not in protocol):
        cmd.append(""localport={}"".format(localport))

    ret = __salt__[""cmd.run_all""](cmd, python_shell=False, ignore_retcode=True)
    if ret[""retcode""] != 0:
        raise CommandExecutionError(ret[""stdout""])

    return True",ret['retcode'] != 0,ret['retcode']
sentry,https://github.com/getsentry/sentry/tree/master/tests/sentry/search/events/test_filter.py,SnQLBooleanSearchQueryTest,test_issue_id_or_with_parens_and_tag$2693,"def test_issue_id_or_with_parens_and_tag(self):
        query = f""(issue.id:{self.group1.id} AND a:b) OR issue.id:{self.group2.id}""
        where, having = self.query_filter.resolve_conditions(query, use_aggregate_conditions=True)
        assert where == [
            Or(
                conditions=[
                    And(conditions=[_cond(""group_id"", Op.EQ, self.group1.id), _tag(""a"", ""b"")]),
                    _cond(""group_id"", Op.EQ, self.group2.id),
                ]
            )
        ]
        assert having == []",having == [],not having
VRM_Addon_for_Blender,https://github.com/saturday06/VRM_Addon_for_Blender/tree/master/io_scene_vrm/editor/detail_mesh_maker.py,ICYP_OT_DETAIL_MESH_MAKER,make_face$119,"def make_face(self, context: bpy.types.Context, mesh: bpy.types.Mesh) -> None:
        def add_point(point: Vector) -> bmesh.types.BMVert:
            return bm.verts.new(point)

        def make_circle(
            center: Vector,
            radius: float,
            axis: str,
            divide: int,
            angle: int = 360,
            x_ratio: float = 1,
            y_ratio: float = 1,
        ) -> None:
            if axis == ""X"":
                axis_n = (0, 1)
            elif axis == ""Y"":
                axis_n = (1, 2)
            else:
                axis_n = (2, 0)
            if divide < 3:
                print(""wrong divide set"")
                divide = 3
            if angle == 0:
                print(""wrong angle set"")
                angle = 180
            verts = []
            for i in range(divide + 1):
                pi2 = 3.14 * 2 * radians(angle) / radians(360)
                vert = add_point(center)
                xy = (sin(pi2 * i / divide) * y_ratio, cos(pi2 * i / divide) * x_ratio)
                for n, j in zip(axis_n, xy):
                    vert.co[n] = vert.co[n] + j * radius
                verts.append(vert)

            bm.faces.new(verts)

        def width_add(point: Vector, add_loc: float) -> Vector:
            return Vector([p + a for p, a in zip(point, [0, 0, add_loc])])

        def up_add(point: Vector, add_loc: float) -> Vector:
            return Vector([p + a for p, a in zip(point, [0, add_loc, 0])])

        def depth_add(point: Vector, add_loc: float) -> Vector:
            return Vector([p + a for p, a in zip(point, [add_loc, 0, 0])])
            # X depth Y up Z width

        bm = bmesh.new()

        face_tall = self.head_tall_size * self.face_center_ratio

        head_top_point_vert = add_point([0, self.head_tall_size, 0])  # noqa: F841
        add_point([-self.head_depth_size / 2, 0, 0])

        neck_point_vert = add_point(  # noqa: F841
            [-self.head_tall_size / 16, self.neck_tail_y, 0]
        )
        eye_point = Vector(
            [
                -self.eye_depth - self.head_depth_size / 2,
                face_tall / 2,
                self.head_width_size / 5,
            ]
        )

        eye_iris_size = eye_point[2] * self.eye_width_ratio * 0.25 / 2
        eye_width = eye_iris_size * 5 / 3

        eye_height = eye_iris_size * 0.9
        eye_axis = -self.eye_angle
        eye_quad_lu_point = eye_point + Matrix.Rotation(eye_axis, 4, ""Y"") @ Vector(
            [0, eye_height, -eye_iris_size]
        )
        eye_quad_ld_point = eye_point + Matrix.Rotation(eye_axis, 4, ""Y"") @ Vector(
            [0, -eye_height, -eye_iris_size]
        )
        eye_quad_rd_point = eye_point + Matrix.Rotation(eye_axis, 4, ""Y"") @ Vector(
            [0, -eye_height, eye_iris_size]
        )
        eye_quad_ru_point = eye_point + Matrix.Rotation(eye_axis, 4, ""Y"") @ Vector(
            [0, eye_height, eye_iris_size]
        )
        eye_inner_point = eye_point + Matrix.Rotation(
            -eye_axis, 4, ""Y""
        ) @ Matrix.Rotation(self.eye_rotate, 4, ""X"") @ Vector(
            [0, -eye_height, -eye_width]
        )
        eye_outer_point = eye_point + Matrix.Rotation(
            eye_axis, 4, ""Y""
        ) @ Matrix.Rotation(self.eye_rotate, 4, ""X"") @ Vector(
            [0, eye_height, eye_width]
        )
        if eye_inner_point[2] < self.head_width_size / 12:
            eye_inner_point[2] = self.head_width_size / 12
        eye_quad_lu_vert = add_point(eye_quad_lu_point)
        eye_quad_ld_vert = add_point(eye_quad_ld_point)
        eye_quad_rd_vert = add_point(eye_quad_rd_point)
        eye_quad_ru_vert = add_point(eye_quad_ru_point)
        eye_inner_vert = add_point(eye_inner_point)
        eye_outer_vert = add_point(eye_outer_point)

        bm.edges.new([eye_inner_vert, eye_quad_lu_vert])
        bm.edges.new([eye_quad_lu_vert, eye_quad_ru_vert])
        bm.edges.new([eye_quad_ru_vert, eye_outer_vert])
        bm.edges.new([eye_outer_vert, eye_quad_rd_vert])
        bm.edges.new([eye_quad_rd_vert, eye_quad_ld_vert])
        bm.edges.new([eye_quad_ld_vert, eye_inner_vert])

        make_circle(
            depth_add(eye_point, eye_quad_ru_point[0] - eye_point[0]),
            eye_iris_size,
            ""Y"",
            12,
            360,
            1,
            1,
        )

        # 鐪夊紦(銇с亾銇涓嬨儵銈ゃ兂銇傘仧銈)
        arcus_superciliaris_under_point = [
            -self.head_depth_size / 2,
            face_tall * 5 / 8,
            0,
        ]
        arcus_superciliaris_outer_under_point = [
            eye_point[0],
            face_tall * 5 / 8,
            eye_outer_point[2],
        ]

        arcus_superciliaris_under_vert = add_point(arcus_superciliaris_under_point)
        arcus_superciliaris_outer_under_vert = add_point(
            arcus_superciliaris_outer_under_point
        )

        # eye_brow_inner_point = width_add(eye_brow_point,eye_point[2] - eye_width*1.1)
        # eye_brow_outer_point = width_add(eye_brow_point,eye_point[2] + eye_width*1.1)
        # eye_brow_inner_vert = add_point(eye_brow_inner_point)
        # eye_brow_outer_vert = add_point(eye_brow_outer_point)
        # bm.edges.new([eye_brow_inner_vert,eye_brow_outer_vert])

        nose_head_height = (
            self.nose_head_height * eye_point[1]
            + (1 - self.nose_head_height) * eye_quad_rd_point[1]
        )
        nose_start_point = [
            -self.eye_depth / 2 - self.head_depth_size / 2,
            nose_head_height,
            0,
        ]
        nose_start_vert = add_point(nose_start_point)
        nose_end_point = [self.nose_height - self.head_depth_size / 2, face_tall / 3, 0]
        nose_top_point = [
            self.nose_height - self.head_depth_size / 2,
            face_tall / 3 + self.nose_top_pos * (eye_point[1] - nose_end_point[1]),
            0,
        ]
        nose_top_vert = add_point(nose_top_point)

        nose_end_side_point = depth_add(
            width_add(
                nose_end_point,
                max([eye_inner_point[2], self.head_width_size / 6]) * self.nose_width,
            ),
            -self.nose_height,
        )
        nose_end_side_vert = add_point(nose_end_side_point)
        nose_end_under_vert = add_point(  # noqa: F841
            depth_add(nose_end_point, -self.nose_height)
        )

        otogai_point = [-self.head_depth_size / 2, 0, 0]
        otogai_vert = add_point(otogai_point)
        ear_hole_point = [0, eye_point[1], self.head_width_size / 2]
        ear_hole_vert = add_point(ear_hole_point)

        # mouth_point = Vector([-self.head_depth_size/2+self.nose_height*2/3,face_tall*2/9,0])
        mouth_point = Vector(
            [
                -self.head_depth_size / 2 + self.nose_height * 2 / 3,
                self.mouth_position_ratio * nose_top_point[1],
                0,
            ]
        )
        mouth_rotate_radian = atan2(self.nose_height, nose_top_point[1])
        rotated_height_up = Vector(
            (
                Matrix.Rotation(-mouth_rotate_radian, 4, ""Z"")
                @ Vector(
                    [
                        self.mouth_width_ratio * -0.01 * self.mouth_flatten,
                        self.mouth_width_ratio * 0.01,
                        0,
                    ]
                )
            )
        )
        rotated_height_down = Vector(
            (
                Matrix.Rotation(-mouth_rotate_radian, 4, ""Z"")
                @ Vector(
                    [
                        self.mouth_width_ratio * 0.01 * self.mouth_flatten,
                        self.mouth_width_ratio * 0.01 * 1.3,
                        0,
                    ]
                )
            )
        )
        rotated_height_mid_up = Vector(
            (
                Matrix.Rotation(-mouth_rotate_radian, 4, ""Z"")
                @ Vector([0, self.mouth_width_ratio * 0.005 * self.mouth_flatten, 0])
            )
        )
        rotated_height_mid_down = Vector(
            (
                Matrix.Rotation(-mouth_rotate_radian, 4, ""Z"")
                @ Vector(
                    [0, self.mouth_width_ratio * 0.005 * 1.3 * self.mouth_flatten, 0]
                )
            )
        )

        mouth_point_up_vert = add_point(mouth_point + rotated_height_up)
        mouth_point_mid_up_vert = add_point(mouth_point + rotated_height_mid_up)
        mouth_point_mid_down_vert = add_point(mouth_point - rotated_height_mid_down)
        mouth_point_down_vert = add_point(mouth_point - rotated_height_down)
        mouth_outer_point = depth_add(
            width_add(mouth_point, self.mouth_width_ratio * self.head_width_size / 5),
            (eye_point[0] - mouth_point[0]) * self.mouth_width_ratio,
        )
        mouth_outer_point_vert = add_point(mouth_outer_point)
        mouth_center_point = depth_add(mouth_point, rotated_height_up[0] / 2)
        mouth_center_vert = add_point(mouth_center_point)

        mouth_corner_nodule_point = (
            mouth_outer_point
            + (mouth_outer_point - mouth_point).normalized()
            * 0.2
            * self.mouth_corner_nodule
        )
        mouth_corner_nodule_vert = add_point(mouth_corner_nodule_point)

        jaw_point = [0, mouth_point[1], self.head_width_size * 3 / 8]
        jaw_vert = add_point(jaw_point)

        max_width_point = [
            0,
            arcus_superciliaris_under_point[1],
            self.head_width_size / 2,
        ]
        max_width_vert = add_point(max_width_point)

        cheek_point = Vector(
            [
                -self.head_depth_size / 2,
                0,
                eye_inner_point[2] + (eye_quad_lu_point[2] - eye_inner_point[2]) / 2,
            ]
        )
        cheek_point[1] = min(
            [eye_quad_ld_point[1], (nose_top_point[1] + nose_start_point[1]) / 2]
        )
        cheek_point[1] = (
            cheek_point[1] - (cheek_point[1] - nose_top_point[1]) * self.cheek_ratio
        )
        tmp_cheek = Matrix.Rotation(eye_axis, 4, ""Y"") @ Vector(
            [
                0,
                0,
                (eye_outer_point[2] - eye_inner_point[2] * 2 / 3)
                * cos(eye_axis)
                * self.cheek_width,
            ]
        )
        cheek_top_outer_vert = add_point(tmp_cheek + cheek_point)
        cheek_top_inner_vert = add_point(cheek_point)
        cheek_under_inner_point = Vector(
            [
                -self.head_depth_size / 2,
                nose_top_point[1],
                eye_inner_point[2] + (eye_quad_lu_point[2] - eye_inner_point[2]) / 2,
            ]
        )
        cheek_under_outer_point = cheek_under_inner_point + tmp_cheek
        cheek_under_inner_vert = add_point(cheek_under_inner_point)
        cheek_under_outer_vert = add_point(cheek_under_outer_point)

        # 鐩灏汇伄绔銇ｃ亾銇嬨倝銇°倗銇ｃ仺銇勩仯銇熴仺銇
        orbit_end = eye_outer_point + Matrix.Rotation(eye_axis, 4, ""Y"") @ Vector(
            [0, 0, eye_iris_size]
        ) * cos(eye_axis)
        orbit_vert = add_point(orbit_end)

        bm.edges.new([otogai_vert, jaw_vert])
        bm.edges.new([jaw_vert, ear_hole_vert])

        def add_mesh(points: List[bmesh.types.BMVert]) -> None:
            bm.faces.new(points)

        add_mesh(
            [
                eye_quad_ld_vert,
                cheek_top_inner_vert,
                cheek_top_outer_vert,
                eye_quad_rd_vert,
            ]
        )
        add_mesh(
            [
                cheek_under_inner_vert,
                cheek_top_inner_vert,
                cheek_top_outer_vert,
                cheek_under_outer_vert,
            ]
        )
        # eye ring
        add_mesh(
            [
                arcus_superciliaris_under_vert,
                arcus_superciliaris_outer_under_vert,
                eye_quad_ru_vert,
                eye_quad_lu_vert,
            ]
        )
        add_mesh(
            [
                arcus_superciliaris_under_vert,
                eye_quad_lu_vert,
                eye_inner_vert,
                nose_start_vert,
            ]
        )
        add_mesh([nose_start_vert, eye_inner_vert, cheek_top_inner_vert])
        add_mesh([eye_inner_vert, eye_quad_ld_vert, cheek_top_inner_vert])
        add_mesh([eye_outer_vert, orbit_vert, cheek_top_outer_vert, eye_quad_rd_vert])

        add_mesh(
            [
                nose_start_vert,
                cheek_top_inner_vert,
                cheek_under_inner_vert,
                nose_end_side_vert,
            ]
        )
        add_mesh(
            [
                nose_end_side_vert,
                cheek_under_inner_vert,
                mouth_corner_nodule_vert,
                mouth_outer_point_vert,
            ]
        )
        add_mesh(
            [cheek_under_inner_vert, cheek_under_outer_vert, mouth_corner_nodule_vert]
        )

        add_mesh([cheek_under_outer_vert, jaw_vert, mouth_corner_nodule_vert])

        add_mesh([nose_start_vert, nose_top_vert, nose_end_side_vert])
        # add_mesh([nose_end_under_vert,nose_top_vert,nose_end_side_vert])
        add_mesh(
            [
                nose_top_vert,
                nose_end_side_vert,
                mouth_outer_point_vert,
                mouth_point_up_vert,
            ]
        )

        add_mesh([mouth_point_up_vert, mouth_point_mid_up_vert, mouth_outer_point_vert])
        add_mesh([mouth_point_mid_up_vert, mouth_center_vert, mouth_outer_point_vert])
        add_mesh([mouth_center_vert, mouth_point_mid_down_vert, mouth_outer_point_vert])
        add_mesh(
            [mouth_point_mid_down_vert, mouth_point_down_vert, mouth_outer_point_vert]
        )

        add_mesh(
            [
                eye_outer_vert,
                orbit_vert,
                arcus_superciliaris_outer_under_vert,
                eye_quad_ru_vert,
            ]
        )
        add_mesh(
            [cheek_top_outer_vert, cheek_under_outer_vert, jaw_vert, ear_hole_vert]
        )
        add_mesh([otogai_vert, jaw_vert, mouth_corner_nodule_vert])
        add_mesh(
            [
                otogai_vert,
                mouth_corner_nodule_vert,
                mouth_outer_point_vert,
                mouth_point_down_vert,
            ]
        )
        add_mesh([orbit_vert, ear_hole_vert, cheek_top_outer_vert])
        add_mesh(
            [
                arcus_superciliaris_outer_under_vert,
                max_width_vert,
                ear_hole_vert,
                orbit_vert,
            ]
        )

        # head
        make_circle(
            [0, max_width_point[1], 0],
            max_width_point[2],
            ""Y"",
            13,
            90,
            1,
            (self.head_tall_size - max_width_point[1]) / max_width_point[2],
        )
        make_circle(
            [0, arcus_superciliaris_under_point[1], 0],
            self.head_tall_size - arcus_superciliaris_outer_under_point[1],
            ""X"",
            13,
            90,
            1,
            arcus_superciliaris_under_point[0]
            / (self.head_tall_size - arcus_superciliaris_outer_under_point[1]),
        )

        bmesh.ops.recalc_face_normals(bm, faces=bm.faces)
        bm.to_mesh(mesh)
        bm.free()",angle == 0,not angle
scirius,https://github.com/StamusNetworks/scirius/tree/master/rules/es_data.py,ESData,kibana_reset$1978,"def kibana_reset(self):
        self._create_kibana_mappings()

        if not os.path.isdir(self._get_dashboard_dir()):
            raise Exception('Please make sure Kibana dashboards are installed at %s' % self._get_dashboard_dir())

        if self._get_kibana_subdirfiles('index-pattern') == []:
            raise Exception('Please make sure Kibana dashboards are installed at %s: no index-pattern found' % self._get_dashboard_dir())

        for _type in self.ALL_OBJ_TYPES:
            if _type == 'index-pattern':
                pattern = ''
            elif get_es_major_version() >= 6:
                pattern = '%s.title: SN*' % _type
            else:
                pattern = 'title: SN*' % _type

            self._kibana_remove(_type, {'query': {'query_string': {'query': pattern}}})
            for _file in self._get_kibana_subdirfiles(_type):
                self._kibana_inject(_type, _file)

        if get_es_major_version() >= 6:
            try:
                self._kibana_request('/api/spaces/space', KIBANA6_NAMESPACE, method='POST')
            except urllib.error.HTTPError as e:
                if e.code == 409:
                    print('Default namespace already exist, skipping creation.')
                elif e.code == 404:
                    print('Kibana OSS is used, skipping namespace creation.')
                else:
                    raise

        self._kibana_set_default_index('logstash-*')",self._get_kibana_subdirfiles('index-pattern') == [],not self._get_kibana_subdirfiles('index-pattern')
sympy,https://github.com/sympy/sympy/tree/master/sympy/integrals/rubi/tests/test_utility_function.py,,test_FactorOrder$1171,"def test_FactorOrder():
    assert FactorOrder(1, 1) == 0
    assert FactorOrder(1, 2) == -1
    assert FactorOrder(2, 1) == 1
    assert FactorOrder(a, b) == 1","FactorOrder(1, 1) == 0","not FactorOrder(1, 1)"
briefcase,https://github.com/beeware/briefcase/tree/master/tests/platforms/linux/appimage/test_build.py,,test_build_failure$276,"def test_build_failure(build_command, first_app, tmp_path):
    """"""If linuxdeploy fails, the build is stopped.""""""
    # Mock a failure in the build
    build_command._subprocess.Popen.side_effect = subprocess.CalledProcessError(
        cmd=[""linuxdeploy-x86_64.AppImage"", ""...""],
        returncode=1,
    )

    # Invoking the build will raise an error.
    build_command.verify_app_tools(first_app)
    with pytest.raises(BriefcaseCommandError):
        build_command.build_app(first_app)

    # linuxdeploy was invoked
    app_dir = (
        tmp_path / ""base_path"" / ""linux"" / ""appimage"" / ""First App"" / ""First App.AppDir""
    )
    build_command._subprocess.Popen.assert_called_with(
        [
            os.fsdecode(
                tmp_path / ""briefcase"" / ""tools"" / ""linuxdeploy-wonky.AppImage""
            ),
            ""--appdir"",
            os.fsdecode(app_dir),
            ""--desktop-file"",
            os.fsdecode(app_dir / ""com.example.first-app.desktop""),
            ""--output"",
            ""appimage"",
            ""--deploy-deps-only"",
            os.fsdecode(app_dir / ""usr"" / ""app"" / ""support""),
            ""--deploy-deps-only"",
            os.fsdecode(app_dir / ""usr"" / ""app_packages"" / ""firstlib""),
            ""--deploy-deps-only"",
            os.fsdecode(app_dir / ""usr"" / ""app_packages"" / ""secondlib""),
        ],
        env={
            ""PATH"": ""/usr/local/bin:/usr/bin:/path/to/somewhere"",
            ""VERSION"": ""0.0.1"",
            ""DISABLE_COPYRIGHT_FILES_DEPLOYMENT"": ""1"",
            ""APPIMAGE_EXTRACT_AND_RUN"": ""1"",
            ""ARCH"": ""wonky"",
        },
        cwd=os.fsdecode(tmp_path / ""base_path"" / ""linux""),
        text=True,
        encoding=mock.ANY,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        bufsize=1,
    )

    # chmod isn't invoked if the binary wasn't created.
    assert build_command.tools.os.chmod.call_count == 0",build_command.tools.os.chmod.call_count == 0,not build_command.tools.os.chmod.call_count
sqlfluff,https://github.com/sqlfluff/sqlfluff/tree/master/src/sqlfluff/core/parser/match_result.py,MatchResult,is_complete$40,"def is_complete(self) -> bool:
        """"""Return true if everything has matched.

        Note: An empty match is not a match so will return False.
        """"""
        return len(self.unmatched_segments) == 0 and len(self.matched_segments) > 0",len(self.unmatched_segments) == 0,not self.unmatched_segments
fireplace,https://github.com/jleclanche/fireplace/tree/master/tests/test_mechanics.py,,test_spell_power$764,"def test_spell_power():
	game = prepare_game(CardClass.HUNTER, CardClass.HUNTER)

	expected_health = 30
	assert game.player2.hero.health == expected_health
	game.player1.give(MOONFIRE).play(target=game.player2.hero)
	expected_health -= 1
	assert game.player2.hero.health == expected_health
	# Play a kobold
	assert game.player1.spellpower == 0
	game.player1.give(KOBOLD_GEOMANCER).play()
	assert game.player1.spellpower == 1
	game.player1.give(MOONFIRE).play(target=game.player2.hero)
	expected_health -= 1 + 1
	assert game.player2.hero.health == expected_health
	# Summon Malygos
	malygos = game.player1.summon(""EX1_563"")
	assert game.player1.spellpower == 1 + 5
	game.player1.give(MOONFIRE).play(target=game.player2.hero)
	expected_health -= 1 + 1 + 5
	assert game.player2.hero.health == expected_health
	# Test heals are not affected
	game.player1.give(HOLY_LIGHT).play(target=game.player2.hero)
	expected_health += 6
	assert game.player2.hero.health == expected_health
	game.end_turn()
	game.end_turn()

	# Check hero power is unaffected
	game.player1.hero.power.use()
	expected_health -= 2
	assert game.player2.hero.health == expected_health
	# Check battlecries are unaffected
	game.player1.give(""CS2_189"").play(target=game.player2.hero)
	expected_health -= 1
	assert game.player2.hero.health == expected_health
	game.end_turn()
	game.end_turn()

	malygos.destroy()
	# Check arcane missiles doesn't wreck everything
	game.player1.give(""EX1_277"").play()
	expected_health -= 3 + 1
	assert game.player2.hero.health == expected_health",game.player1.spellpower == 0,not game.player1.spellpower
PowerDNS-Admin,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/models/user.py,User,updateUser$652,"def updateUser(self, Entitlements):
        """"""
        Update user associations based on ldap attribute
        """"""
        entitlements= getCorrectEntitlements(Entitlements)
        if len(entitlements)!=0:
            self.revoke_privilege(True)
            for entitlement in entitlements:
                arguments=entitlement.split(':')
                entArgs=arguments[arguments.index('powerdns-admin')+1:]
                role= entArgs[0]
                self.set_role(role)
                if (role==""User"") and len(entArgs)>1:
                    current_domains=getUserInfo(self.get_user_domains())
                    current_accounts=getUserInfo(self.get_accounts())
                    domain=entArgs[1]
                    self.addMissingDomain(domain, current_domains)
                    if len(entArgs)>2:
                        account=entArgs[2]
                        self.addMissingAccount(account, current_accounts)",len(entitlements) != 0,entitlements
scipy,https://github.com/scipy/scipy/tree/master/scipy/optimize/_zeros_py.py,TOMS748Solver,iterate$1135,"def iterate(self):
        """"""Perform one step in the algorithm.

        Implements Algorithm 4.1(k=1) or 4.2(k=2) in [APS1995]
        """"""
        self.iterations += 1
        eps = np.finfo(float).eps
        d, fd, e, fe = self.d, self.fd, self.e, self.fe
        ab_width = self.ab[1] - self.ab[0]  # Need the start width below
        c = None

        for nsteps in range(2, self.k+2):
            # If the f-values are sufficiently separated, perform an inverse
            # polynomial interpolation step. Otherwise, nsteps repeats of
            # an approximate Newton-Raphson step.
            if _notclose(self.fab + [fd, fe], rtol=0, atol=32*eps):
                c0 = _inverse_poly_zero(self.ab[0], self.ab[1], d, e,
                                        self.fab[0], self.fab[1], fd, fe)
                if self.ab[0] < c0 < self.ab[1]:
                    c = c0
            if c is None:
                c = _newton_quadratic(self.ab, self.fab, d, fd, nsteps)

            fc = self._callf(c)
            if fc == 0:
                return _ECONVERGED, c

            # re-bracket
            e, fe = d, fd
            d, fd = self._update_bracket(c, fc)

        # u is the endpoint with the smallest f-value
        uix = (0 if np.abs(self.fab[0]) < np.abs(self.fab[1]) else 1)
        u, fu = self.ab[uix], self.fab[uix]

        _, A = _compute_divided_differences(self.ab, self.fab,
                                            forward=(uix == 0), full=False)
        c = u - 2 * fu / A
        if np.abs(c - u) > 0.5 * (self.ab[1] - self.ab[0]):
            c = sum(self.ab) / 2.0
        else:
            if np.isclose(c, u, rtol=eps, atol=0):
                # c didn't change (much).
                # Either because the f-values at the endpoints have vastly
                # differing magnitudes, or because the root is very close to
                # that endpoint
                frs = np.frexp(self.fab)[1]
                if frs[uix] < frs[1 - uix] - 50:  # Differ by more than 2**50
                    c = (31 * self.ab[uix] + self.ab[1 - uix]) / 32
                else:
                    # Make a bigger adjustment, about the
                    # size of the requested tolerance.
                    mm = (1 if uix == 0 else -1)
                    adj = mm * np.abs(c) * self.rtol + mm * self.xtol
                    c = u + adj
                if not self.ab[0] < c < self.ab[1]:
                    c = sum(self.ab) / 2.0

        fc = self._callf(c)
        if fc == 0:
            return _ECONVERGED, c

        e, fe = d, fd
        d, fd = self._update_bracket(c, fc)

        # If the width of the new interval did not decrease enough, bisect
        if self.ab[1] - self.ab[0] > self._MU * ab_width:
            e, fe = d, fd
            z = sum(self.ab) / 2.0
            fz = self._callf(z)
            if fz == 0:
                return _ECONVERGED, z
            d, fd = self._update_bracket(z, fz)

        # Record d and e for next iteration
        self.d, self.fd = d, fd
        self.e, self.fe = e, fe

        status, xn = self.get_status()
        return status, xn",fc == 0,not fc
scipy,https://github.com/scipy/scipy/tree/master/scipy/optimize/_zeros_py.py,TOMS748Solver,iterate$1135,"def iterate(self):
        """"""Perform one step in the algorithm.

        Implements Algorithm 4.1(k=1) or 4.2(k=2) in [APS1995]
        """"""
        self.iterations += 1
        eps = np.finfo(float).eps
        d, fd, e, fe = self.d, self.fd, self.e, self.fe
        ab_width = self.ab[1] - self.ab[0]  # Need the start width below
        c = None

        for nsteps in range(2, self.k+2):
            # If the f-values are sufficiently separated, perform an inverse
            # polynomial interpolation step. Otherwise, nsteps repeats of
            # an approximate Newton-Raphson step.
            if _notclose(self.fab + [fd, fe], rtol=0, atol=32*eps):
                c0 = _inverse_poly_zero(self.ab[0], self.ab[1], d, e,
                                        self.fab[0], self.fab[1], fd, fe)
                if self.ab[0] < c0 < self.ab[1]:
                    c = c0
            if c is None:
                c = _newton_quadratic(self.ab, self.fab, d, fd, nsteps)

            fc = self._callf(c)
            if fc == 0:
                return _ECONVERGED, c

            # re-bracket
            e, fe = d, fd
            d, fd = self._update_bracket(c, fc)

        # u is the endpoint with the smallest f-value
        uix = (0 if np.abs(self.fab[0]) < np.abs(self.fab[1]) else 1)
        u, fu = self.ab[uix], self.fab[uix]

        _, A = _compute_divided_differences(self.ab, self.fab,
                                            forward=(uix == 0), full=False)
        c = u - 2 * fu / A
        if np.abs(c - u) > 0.5 * (self.ab[1] - self.ab[0]):
            c = sum(self.ab) / 2.0
        else:
            if np.isclose(c, u, rtol=eps, atol=0):
                # c didn't change (much).
                # Either because the f-values at the endpoints have vastly
                # differing magnitudes, or because the root is very close to
                # that endpoint
                frs = np.frexp(self.fab)[1]
                if frs[uix] < frs[1 - uix] - 50:  # Differ by more than 2**50
                    c = (31 * self.ab[uix] + self.ab[1 - uix]) / 32
                else:
                    # Make a bigger adjustment, about the
                    # size of the requested tolerance.
                    mm = (1 if uix == 0 else -1)
                    adj = mm * np.abs(c) * self.rtol + mm * self.xtol
                    c = u + adj
                if not self.ab[0] < c < self.ab[1]:
                    c = sum(self.ab) / 2.0

        fc = self._callf(c)
        if fc == 0:
            return _ECONVERGED, c

        e, fe = d, fd
        d, fd = self._update_bracket(c, fc)

        # If the width of the new interval did not decrease enough, bisect
        if self.ab[1] - self.ab[0] > self._MU * ab_width:
            e, fe = d, fd
            z = sum(self.ab) / 2.0
            fz = self._callf(z)
            if fz == 0:
                return _ECONVERGED, z
            d, fd = self._update_bracket(z, fz)

        # Record d and e for next iteration
        self.d, self.fd = d, fd
        self.e, self.fe = e, fe

        status, xn = self.get_status()
        return status, xn",fc == 0,not fc
scipy,https://github.com/scipy/scipy/tree/master/scipy/optimize/_zeros_py.py,TOMS748Solver,iterate$1135,"def iterate(self):
        """"""Perform one step in the algorithm.

        Implements Algorithm 4.1(k=1) or 4.2(k=2) in [APS1995]
        """"""
        self.iterations += 1
        eps = np.finfo(float).eps
        d, fd, e, fe = self.d, self.fd, self.e, self.fe
        ab_width = self.ab[1] - self.ab[0]  # Need the start width below
        c = None

        for nsteps in range(2, self.k+2):
            # If the f-values are sufficiently separated, perform an inverse
            # polynomial interpolation step. Otherwise, nsteps repeats of
            # an approximate Newton-Raphson step.
            if _notclose(self.fab + [fd, fe], rtol=0, atol=32*eps):
                c0 = _inverse_poly_zero(self.ab[0], self.ab[1], d, e,
                                        self.fab[0], self.fab[1], fd, fe)
                if self.ab[0] < c0 < self.ab[1]:
                    c = c0
            if c is None:
                c = _newton_quadratic(self.ab, self.fab, d, fd, nsteps)

            fc = self._callf(c)
            if fc == 0:
                return _ECONVERGED, c

            # re-bracket
            e, fe = d, fd
            d, fd = self._update_bracket(c, fc)

        # u is the endpoint with the smallest f-value
        uix = (0 if np.abs(self.fab[0]) < np.abs(self.fab[1]) else 1)
        u, fu = self.ab[uix], self.fab[uix]

        _, A = _compute_divided_differences(self.ab, self.fab,
                                            forward=(uix == 0), full=False)
        c = u - 2 * fu / A
        if np.abs(c - u) > 0.5 * (self.ab[1] - self.ab[0]):
            c = sum(self.ab) / 2.0
        else:
            if np.isclose(c, u, rtol=eps, atol=0):
                # c didn't change (much).
                # Either because the f-values at the endpoints have vastly
                # differing magnitudes, or because the root is very close to
                # that endpoint
                frs = np.frexp(self.fab)[1]
                if frs[uix] < frs[1 - uix] - 50:  # Differ by more than 2**50
                    c = (31 * self.ab[uix] + self.ab[1 - uix]) / 32
                else:
                    # Make a bigger adjustment, about the
                    # size of the requested tolerance.
                    mm = (1 if uix == 0 else -1)
                    adj = mm * np.abs(c) * self.rtol + mm * self.xtol
                    c = u + adj
                if not self.ab[0] < c < self.ab[1]:
                    c = sum(self.ab) / 2.0

        fc = self._callf(c)
        if fc == 0:
            return _ECONVERGED, c

        e, fe = d, fd
        d, fd = self._update_bracket(c, fc)

        # If the width of the new interval did not decrease enough, bisect
        if self.ab[1] - self.ab[0] > self._MU * ab_width:
            e, fe = d, fd
            z = sum(self.ab) / 2.0
            fz = self._callf(z)
            if fz == 0:
                return _ECONVERGED, z
            d, fd = self._update_bracket(z, fz)

        # Record d and e for next iteration
        self.d, self.fd = d, fd
        self.e, self.fe = e, fe

        status, xn = self.get_status()
        return status, xn",fz == 0,not fz
networkx,https://github.com/networkx/networkx/tree/master/networkx/algorithms/link_analysis/pagerank_alg.py,,google_matrix$175,"def google_matrix(
    G, alpha=0.85, personalization=None, nodelist=None, weight=""weight"", dangling=None
):
    """"""Returns the Google matrix of the graph.

    Parameters
    ----------
    G : graph
      A NetworkX graph.  Undirected graphs will be converted to a directed
      graph with two directed edges for each undirected edge.

    alpha : float
      The damping factor.

    personalization: dict, optional
      The ""personalization vector"" consisting of a dictionary with a
      key some subset of graph nodes and personalization value each of those.
      At least one personalization value must be non-zero.
      If not specfiied, a nodes personalization value will be zero.
      By default, a uniform distribution is used.

    nodelist : list, optional
      The rows and columns are ordered according to the nodes in nodelist.
      If nodelist is None, then the ordering is produced by G.nodes().

    weight : key, optional
      Edge data key to use as weight.  If None weights are set to 1.

    dangling: dict, optional
      The outedges to be assigned to any ""dangling"" nodes, i.e., nodes without
      any outedges. The dict key is the node the outedge points to and the dict
      value is the weight of that outedge. By default, dangling nodes are given
      outedges according to the personalization vector (uniform if not
      specified) This must be selected to result in an irreducible transition
      matrix (see notes below). It may be common to have the dangling dict to
      be the same as the personalization dict.

    Returns
    -------
    A : NumPy matrix
       Google matrix of the graph

    Notes
    -----
    The matrix returned represents the transition matrix that describes the
    Markov chain used in PageRank. For PageRank to converge to a unique
    solution (i.e., a unique stationary distribution in a Markov chain), the
    transition matrix must be irreducible. In other words, it must be that
    there exists a path between every pair of nodes in the graph, or else there
    is the potential of ""rank sinks.""

    This implementation works with Multi(Di)Graphs. For multigraphs the
    weight between two nodes is set to be the sum of all edge weights
    between those nodes.

    See Also
    --------
    pagerank, pagerank_numpy, pagerank_scipy
    """"""
    import numpy as np

    if nodelist is None:
        nodelist = list(G)

    M = np.asmatrix(nx.to_numpy_array(G, nodelist=nodelist, weight=weight))
    N = len(G)
    if N == 0:
        return M

    # Personalization vector
    if personalization is None:
        p = np.repeat(1.0 / N, N)
    else:
        p = np.array([personalization.get(n, 0) for n in nodelist], dtype=float)
        if p.sum() == 0:
            raise ZeroDivisionError
        p /= p.sum()

    # Dangling nodes
    if dangling is None:
        dangling_weights = p
    else:
        # Convert the dangling dictionary into an array in nodelist order
        dangling_weights = np.array([dangling.get(n, 0) for n in nodelist], dtype=float)
        dangling_weights /= dangling_weights.sum()
    dangling_nodes = np.where(M.sum(axis=1) == 0)[0]

    # Assign dangling_weights to any dangling nodes (nodes with no out links)
    for node in dangling_nodes:
        M[node] = dangling_weights

    M /= M.sum(axis=1)  # Normalize rows to sum to 1

    return alpha * M + (1 - alpha) * p",N == 0,not N
networkx,https://github.com/networkx/networkx/tree/master/networkx/algorithms/link_analysis/pagerank_alg.py,,google_matrix$175,"def google_matrix(
    G, alpha=0.85, personalization=None, nodelist=None, weight=""weight"", dangling=None
):
    """"""Returns the Google matrix of the graph.

    Parameters
    ----------
    G : graph
      A NetworkX graph.  Undirected graphs will be converted to a directed
      graph with two directed edges for each undirected edge.

    alpha : float
      The damping factor.

    personalization: dict, optional
      The ""personalization vector"" consisting of a dictionary with a
      key some subset of graph nodes and personalization value each of those.
      At least one personalization value must be non-zero.
      If not specfiied, a nodes personalization value will be zero.
      By default, a uniform distribution is used.

    nodelist : list, optional
      The rows and columns are ordered according to the nodes in nodelist.
      If nodelist is None, then the ordering is produced by G.nodes().

    weight : key, optional
      Edge data key to use as weight.  If None weights are set to 1.

    dangling: dict, optional
      The outedges to be assigned to any ""dangling"" nodes, i.e., nodes without
      any outedges. The dict key is the node the outedge points to and the dict
      value is the weight of that outedge. By default, dangling nodes are given
      outedges according to the personalization vector (uniform if not
      specified) This must be selected to result in an irreducible transition
      matrix (see notes below). It may be common to have the dangling dict to
      be the same as the personalization dict.

    Returns
    -------
    A : NumPy matrix
       Google matrix of the graph

    Notes
    -----
    The matrix returned represents the transition matrix that describes the
    Markov chain used in PageRank. For PageRank to converge to a unique
    solution (i.e., a unique stationary distribution in a Markov chain), the
    transition matrix must be irreducible. In other words, it must be that
    there exists a path between every pair of nodes in the graph, or else there
    is the potential of ""rank sinks.""

    This implementation works with Multi(Di)Graphs. For multigraphs the
    weight between two nodes is set to be the sum of all edge weights
    between those nodes.

    See Also
    --------
    pagerank, pagerank_numpy, pagerank_scipy
    """"""
    import numpy as np

    if nodelist is None:
        nodelist = list(G)

    M = np.asmatrix(nx.to_numpy_array(G, nodelist=nodelist, weight=weight))
    N = len(G)
    if N == 0:
        return M

    # Personalization vector
    if personalization is None:
        p = np.repeat(1.0 / N, N)
    else:
        p = np.array([personalization.get(n, 0) for n in nodelist], dtype=float)
        if p.sum() == 0:
            raise ZeroDivisionError
        p /= p.sum()

    # Dangling nodes
    if dangling is None:
        dangling_weights = p
    else:
        # Convert the dangling dictionary into an array in nodelist order
        dangling_weights = np.array([dangling.get(n, 0) for n in nodelist], dtype=float)
        dangling_weights /= dangling_weights.sum()
    dangling_nodes = np.where(M.sum(axis=1) == 0)[0]

    # Assign dangling_weights to any dangling nodes (nodes with no out links)
    for node in dangling_nodes:
        M[node] = dangling_weights

    M /= M.sum(axis=1)  # Normalize rows to sum to 1

    return alpha * M + (1 - alpha) * p",p.sum() == 0,not p.sum()
deprecated-binaryninja-python,https://github.com/Vector35/deprecated-binaryninja-python/tree/master//ElfFile.py,ElfFile,get_modification$324,"def get_modification(self, ofs, len):
		result = []
		while len > 0:
			cur = None
			for i in self.program_headers:
				if ((ofs >= i.virtual_addr) and (ofs < (i.virtual_addr + i.memory_size))) and (i.memory_size != 0):
					cur = i
			if cur == None:
				break

			prog_ofs = ofs - cur.virtual_addr
			mem_len = cur.memory_size - prog_ofs
			file_len = cur.file_size - prog_ofs
			if mem_len > len:
				mem_len = len
			if file_len > len:
				file_len = len

			if file_len <= 0:
				result += [DATA_ORIGINAL] * mem_len
				len -= mem_len
				ofs += mem_len
				continue

			result += self.data.get_modification(cur.offset + prog_ofs, file_len)
			len -= file_len
			ofs += file_len

		return result",cur == None,not cur
deprecated-binaryninja-python,https://github.com/Vector35/deprecated-binaryninja-python/tree/master//ElfFile.py,ElfFile,get_modification$324,"def get_modification(self, ofs, len):
		result = []
		while len > 0:
			cur = None
			for i in self.program_headers:
				if ((ofs >= i.virtual_addr) and (ofs < (i.virtual_addr + i.memory_size))) and (i.memory_size != 0):
					cur = i
			if cur == None:
				break

			prog_ofs = ofs - cur.virtual_addr
			mem_len = cur.memory_size - prog_ofs
			file_len = cur.file_size - prog_ofs
			if mem_len > len:
				mem_len = len
			if file_len > len:
				file_len = len

			if file_len <= 0:
				result += [DATA_ORIGINAL] * mem_len
				len -= mem_len
				ofs += mem_len
				continue

			result += self.data.get_modification(cur.offset + prog_ofs, file_len)
			len -= file_len
			ofs += file_len

		return result",i.memory_size != 0,i.memory_size
PaddleHub,https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/image/classification/resnext101_vd_64x4d_imagenet/module.py,ResNeXt101_vd,__init__$143,"def __init__(self, class_dim: int = 1000, load_checkpoint: str = None):
        super(ResNeXt101_vd, self).__init__()

        self.layers = 101
        self.cardinality = 64
        depth = [3, 4, 23, 3]
        num_channels = [64, 256, 512, 1024]
        num_filters = [256, 512, 1024, 2048]

        self.conv1_1 = ConvBNLayer(num_channels=3, num_filters=32, filter_size=3, stride=2, act='relu', name=""conv1_1"")
        self.conv1_2 = ConvBNLayer(num_channels=32, num_filters=32, filter_size=3, stride=1, act='relu', name=""conv1_2"")
        self.conv1_3 = ConvBNLayer(num_channels=32, num_filters=64, filter_size=3, stride=1, act='relu', name=""conv1_3"")

        self.pool2d_max = MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.block_list = []
        for block in range(len(depth)):
            shortcut = False
            for i in range(depth[block]):
                if block == 2:
                    if i == 0:
                        conv_name = ""res"" + str(block + 2) + ""a""
                    else:
                        conv_name = ""res"" + str(block + 2) + ""b"" + str(i)
                else:
                    conv_name = ""res"" + str(block + 2) + chr(97 + i)
                bottleneck_block = self.add_sublayer(
                    'bb_%d_%d' % (block, i),
                    BottleneckBlock(
                        num_channels=num_channels[block]
                        if i == 0 else num_filters[block] * int(64 // self.cardinality),
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        cardinality=self.cardinality,
                        shortcut=shortcut,
                        if_first=block == i == 0,
                        name=conv_name))
                self.block_list.append(bottleneck_block)
                shortcut = True

        self.pool2d_avg = AdaptiveAvgPool2d(1)

        self.pool2d_avg_channels = num_channels[-1] * 2

        stdv = 1.0 / math.sqrt(self.pool2d_avg_channels * 1.0)

        self.out = Linear(
            self.pool2d_avg_channels,
            class_dim,
            weight_attr=ParamAttr(initializer=Uniform(-stdv, stdv), name=""fc_weights""),
            bias_attr=ParamAttr(name=""fc_offset""))

        if load_checkpoint is not None:
            model_dict = paddle.load(load_checkpoint)[0]
            self.set_dict(model_dict)
            print(""load custom checkpoint success"")

        else:
            checkpoint = os.path.join(self.directory, 'resnext101_vd_64x4d_imagenet.pdparams')
            if not os.path.exists(checkpoint):
                os.system(
                    'wget https://paddlehub.bj.bcebos.com/dygraph/image_classification/resnext101_vd_64x4d_imagenet.pdparams -O '
                    + checkpoint)
            model_dict = paddle.load(checkpoint)[0]
            self.set_dict(model_dict)
            print(""load pretrained checkpoint success"")",i == 0,not i
PaddleHub,https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/image/classification/resnext101_vd_64x4d_imagenet/module.py,ResNeXt101_vd,__init__$143,"def __init__(self, class_dim: int = 1000, load_checkpoint: str = None):
        super(ResNeXt101_vd, self).__init__()

        self.layers = 101
        self.cardinality = 64
        depth = [3, 4, 23, 3]
        num_channels = [64, 256, 512, 1024]
        num_filters = [256, 512, 1024, 2048]

        self.conv1_1 = ConvBNLayer(num_channels=3, num_filters=32, filter_size=3, stride=2, act='relu', name=""conv1_1"")
        self.conv1_2 = ConvBNLayer(num_channels=32, num_filters=32, filter_size=3, stride=1, act='relu', name=""conv1_2"")
        self.conv1_3 = ConvBNLayer(num_channels=32, num_filters=64, filter_size=3, stride=1, act='relu', name=""conv1_3"")

        self.pool2d_max = MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.block_list = []
        for block in range(len(depth)):
            shortcut = False
            for i in range(depth[block]):
                if block == 2:
                    if i == 0:
                        conv_name = ""res"" + str(block + 2) + ""a""
                    else:
                        conv_name = ""res"" + str(block + 2) + ""b"" + str(i)
                else:
                    conv_name = ""res"" + str(block + 2) + chr(97 + i)
                bottleneck_block = self.add_sublayer(
                    'bb_%d_%d' % (block, i),
                    BottleneckBlock(
                        num_channels=num_channels[block]
                        if i == 0 else num_filters[block] * int(64 // self.cardinality),
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        cardinality=self.cardinality,
                        shortcut=shortcut,
                        if_first=block == i == 0,
                        name=conv_name))
                self.block_list.append(bottleneck_block)
                shortcut = True

        self.pool2d_avg = AdaptiveAvgPool2d(1)

        self.pool2d_avg_channels = num_channels[-1] * 2

        stdv = 1.0 / math.sqrt(self.pool2d_avg_channels * 1.0)

        self.out = Linear(
            self.pool2d_avg_channels,
            class_dim,
            weight_attr=ParamAttr(initializer=Uniform(-stdv, stdv), name=""fc_weights""),
            bias_attr=ParamAttr(name=""fc_offset""))

        if load_checkpoint is not None:
            model_dict = paddle.load(load_checkpoint)[0]
            self.set_dict(model_dict)
            print(""load custom checkpoint success"")

        else:
            checkpoint = os.path.join(self.directory, 'resnext101_vd_64x4d_imagenet.pdparams')
            if not os.path.exists(checkpoint):
                os.system(
                    'wget https://paddlehub.bj.bcebos.com/dygraph/image_classification/resnext101_vd_64x4d_imagenet.pdparams -O '
                    + checkpoint)
            model_dict = paddle.load(checkpoint)[0]
            self.set_dict(model_dict)
            print(""load pretrained checkpoint success"")",i == 0,not i
PaddleHub,https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/image/classification/resnext101_vd_64x4d_imagenet/module.py,ResNeXt101_vd,__init__$143,"def __init__(self, class_dim: int = 1000, load_checkpoint: str = None):
        super(ResNeXt101_vd, self).__init__()

        self.layers = 101
        self.cardinality = 64
        depth = [3, 4, 23, 3]
        num_channels = [64, 256, 512, 1024]
        num_filters = [256, 512, 1024, 2048]

        self.conv1_1 = ConvBNLayer(num_channels=3, num_filters=32, filter_size=3, stride=2, act='relu', name=""conv1_1"")
        self.conv1_2 = ConvBNLayer(num_channels=32, num_filters=32, filter_size=3, stride=1, act='relu', name=""conv1_2"")
        self.conv1_3 = ConvBNLayer(num_channels=32, num_filters=64, filter_size=3, stride=1, act='relu', name=""conv1_3"")

        self.pool2d_max = MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.block_list = []
        for block in range(len(depth)):
            shortcut = False
            for i in range(depth[block]):
                if block == 2:
                    if i == 0:
                        conv_name = ""res"" + str(block + 2) + ""a""
                    else:
                        conv_name = ""res"" + str(block + 2) + ""b"" + str(i)
                else:
                    conv_name = ""res"" + str(block + 2) + chr(97 + i)
                bottleneck_block = self.add_sublayer(
                    'bb_%d_%d' % (block, i),
                    BottleneckBlock(
                        num_channels=num_channels[block]
                        if i == 0 else num_filters[block] * int(64 // self.cardinality),
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        cardinality=self.cardinality,
                        shortcut=shortcut,
                        if_first=block == i == 0,
                        name=conv_name))
                self.block_list.append(bottleneck_block)
                shortcut = True

        self.pool2d_avg = AdaptiveAvgPool2d(1)

        self.pool2d_avg_channels = num_channels[-1] * 2

        stdv = 1.0 / math.sqrt(self.pool2d_avg_channels * 1.0)

        self.out = Linear(
            self.pool2d_avg_channels,
            class_dim,
            weight_attr=ParamAttr(initializer=Uniform(-stdv, stdv), name=""fc_weights""),
            bias_attr=ParamAttr(name=""fc_offset""))

        if load_checkpoint is not None:
            model_dict = paddle.load(load_checkpoint)[0]
            self.set_dict(model_dict)
            print(""load custom checkpoint success"")

        else:
            checkpoint = os.path.join(self.directory, 'resnext101_vd_64x4d_imagenet.pdparams')
            if not os.path.exists(checkpoint):
                os.system(
                    'wget https://paddlehub.bj.bcebos.com/dygraph/image_classification/resnext101_vd_64x4d_imagenet.pdparams -O '
                    + checkpoint)
            model_dict = paddle.load(checkpoint)[0]
            self.set_dict(model_dict)
            print(""load pretrained checkpoint success"")",block != 0,block
great_expectations,https://github.com/great-expectations/great_expectations/tree/master/tests/expectations/test_expectation_arguments.py,,test_catch_exceptions_no_exceptions$51,"def test_catch_exceptions_no_exceptions(
    mock_emit, in_memory_runtime_context, test_spark_df
):
    catch_exceptions: bool = False  # expect exceptions to be raised
    result_format: dict = {
        ""result_format"": ""SUMMARY"",
    }
    runtime_environment_arguments = {
        ""catch_exceptions"": catch_exceptions,
        ""result_format"": result_format,
    }

    suite: ExpectationSuite = in_memory_runtime_context.create_expectation_suite(
        ""test_suite"", overwrite_existing=True
    )

    expectation_configuration: ExpectationConfiguration

    expectation_meta: dict = {""Notes"": ""Some notes""}

    expectation_arguments_without_meta: dict

    expectation_arguments_column: dict = {
        ""include_config"": True,
        ""column"": ""Name"",  # use correct column to avoid error
    }
    expectation_arguments_without_meta = dict(
        **runtime_environment_arguments, **expectation_arguments_column
    )
    expectation_configuration = ExpectationConfiguration(
        expectation_type=""expect_column_values_to_not_be_null"",
        kwargs=expectation_arguments_without_meta,
        meta=expectation_meta,
    )
    suite.add_expectation(expectation_configuration=expectation_configuration)

    expectation_arguments_table: dict = {
        ""include_config"": True,
        ""value"": 4,
    }
    expectation_arguments_without_meta = dict(
        **runtime_environment_arguments, **expectation_arguments_table
    )
    expectation_configuration = ExpectationConfiguration(
        expectation_type=""expect_table_row_count_to_equal"",
        kwargs=expectation_arguments_without_meta,
        meta=expectation_meta,
    )
    suite.add_expectation(expectation_configuration=expectation_configuration)

    runtime_batch_request = RuntimeBatchRequest(
        datasource_name=""spark_datasource"",
        data_connector_name=""runtime_data_connector"",
        data_asset_name=""insert_your_data_asset_name_here"",
        runtime_parameters={""batch_data"": test_spark_df},
        batch_identifiers={
            ""id_key_0"": ""id_value_0"",
            ""id_key_1"": ""id_value_1"",
        },
    )

    validator: Validator = in_memory_runtime_context.get_validator(
        batch_request=runtime_batch_request,
        expectation_suite=suite,
    )

    # Test calling ""validator.validate()"" explicitly.

    validator_validation: ExpectationSuiteValidationResult = validator.validate(
        **runtime_environment_arguments
    )
    results: List[ExpectationValidationResult] = validator_validation.results
    assert len(results) == 2

    result: ExpectationValidationResult

    for result in results:
        assert result.success
        assert (
            ""exception_traceback"" not in result.exception_info
        ) or not result.exception_info[""exception_traceback""]
        assert (
            ""exception_message"" not in result.exception_info
        ) or not result.exception_info[""exception_message""]

    # Test calling ""validator.expect_*"" through ""validator.validate_expectation()"".

    expectation_parameters: dict

    expectation_arguments_without_meta = dict(
        **runtime_environment_arguments, **expectation_arguments_column
    )
    expectation_parameters = dict(
        **expectation_arguments_without_meta, **expectation_meta
    )
    result = validator.expect_column_values_to_not_be_null(**expectation_parameters)
    assert result.success

    expectation_arguments_without_meta = dict(
        **runtime_environment_arguments, **expectation_arguments_table
    )
    expectation_parameters = dict(
        **expectation_arguments_without_meta, **expectation_meta
    )
    result = validator.expect_table_row_count_to_equal(**expectation_parameters)
    assert result.success

    # In-Memory DataContext does not have UsageStatisticsHandler configured
    assert mock_emit.call_count == 0",mock_emit.call_count == 0,not mock_emit.call_count
horovod,https://github.com/horovod/horovod/tree/master/test/integration/test_spark.py,SparkTests,test_df_cache$1077,"def test_df_cache(self):
        # Clean the cache before starting the test
        util.clear_training_cache()
        util._training_cache.get_dataset = mock.Mock(side_effect=util._training_cache.get_dataset)

        with spark_session('test_df_cache') as spark:
            with local_store() as store:
                df = create_xor_data(spark)
                df2 = create_xor_data(spark)
                df3 = create_xor_data(spark)

                key = util._training_cache.create_key(df, store, None)
                key2 = util._training_cache.create_key(df2, store, None)
                key3 = util._training_cache.create_key(df3, store, None)

                # All keys are distinct
                assert key != key2
                assert key != key3
                assert key2 != key3

                # The cache should be empty to start
                assert not util._training_cache.is_cached(key, store)
                assert not util._training_cache.is_cached(key2, store)
                assert not util._training_cache.is_cached(key3, store)

                # First insertion into the cache
                with util.prepare_data(num_processes=2,
                                       store=store,
                                       df=df,
                                       feature_columns=['features'],
                                       label_columns=['y']) as dataset_idx:
                    train_rows, val_rows, metadata, avg_row_size = util.get_dataset_properties(dataset_idx)
                    util._training_cache.get_dataset.assert_not_called()
                    assert len(util._training_cache._key_to_dataset) == 1
                    assert util._training_cache.is_cached(key, store)
                    assert dataset_idx == 0

                    # The first dataset is still in use, so we assign the next integer in sequence to this
                    # dataset
                    assert not util._training_cache.is_cached(key2, store)
                    with util.prepare_data(num_processes=2,
                                           store=store,
                                           df=df2,
                                           feature_columns=['features'],
                                           label_columns=['y']) as dataset_idx2:
                        util._training_cache.get_dataset.assert_not_called()
                        assert len(util._training_cache._key_to_dataset) == 2
                        assert util._training_cache.is_cached(key2, store)
                        assert dataset_idx2 == 1

                # Even though the first dataset is no longer in use, it is still cached
                with util.prepare_data(num_processes=2,
                                       store=store,
                                       df=df,
                                       feature_columns=['features'],
                                       label_columns=['y']) as dataset_idx1:
                    train_rows1, val_rows1, metadata1, avg_row_size1 = util.get_dataset_properties(dataset_idx1)
                    util._training_cache.get_dataset.assert_called()
                    assert train_rows == train_rows1
                    assert val_rows == val_rows1
                    assert metadata == metadata1
                    assert avg_row_size == avg_row_size1
                    assert dataset_idx1 == 0

                # The first dataset is no longer in use, so we can reclaim its dataset index
                assert not util._training_cache.is_cached(key3, store)
                with util.prepare_data(num_processes=2,
                                       store=store,
                                       df=df3,
                                       feature_columns=['features'],
                                       label_columns=['y']) as dataset_idx3:
                    train_rows3, val_rows3, metadata3, avg_row_size3 = util.get_dataset_properties(dataset_idx3)
                    assert train_rows == train_rows3
                    assert val_rows == val_rows3
                    assert metadata == metadata3
                    assert avg_row_size == avg_row_size3
                    assert dataset_idx3 == 0

                # Same dataframe, different validation
                bad_key = util._training_cache.create_key(df, store, 0.1)
                assert not util._training_cache.is_cached(bad_key, store)",dataset_idx == 0,not dataset_idx
horovod,https://github.com/horovod/horovod/tree/master/test/integration/test_spark.py,SparkTests,test_df_cache$1077,"def test_df_cache(self):
        # Clean the cache before starting the test
        util.clear_training_cache()
        util._training_cache.get_dataset = mock.Mock(side_effect=util._training_cache.get_dataset)

        with spark_session('test_df_cache') as spark:
            with local_store() as store:
                df = create_xor_data(spark)
                df2 = create_xor_data(spark)
                df3 = create_xor_data(spark)

                key = util._training_cache.create_key(df, store, None)
                key2 = util._training_cache.create_key(df2, store, None)
                key3 = util._training_cache.create_key(df3, store, None)

                # All keys are distinct
                assert key != key2
                assert key != key3
                assert key2 != key3

                # The cache should be empty to start
                assert not util._training_cache.is_cached(key, store)
                assert not util._training_cache.is_cached(key2, store)
                assert not util._training_cache.is_cached(key3, store)

                # First insertion into the cache
                with util.prepare_data(num_processes=2,
                                       store=store,
                                       df=df,
                                       feature_columns=['features'],
                                       label_columns=['y']) as dataset_idx:
                    train_rows, val_rows, metadata, avg_row_size = util.get_dataset_properties(dataset_idx)
                    util._training_cache.get_dataset.assert_not_called()
                    assert len(util._training_cache._key_to_dataset) == 1
                    assert util._training_cache.is_cached(key, store)
                    assert dataset_idx == 0

                    # The first dataset is still in use, so we assign the next integer in sequence to this
                    # dataset
                    assert not util._training_cache.is_cached(key2, store)
                    with util.prepare_data(num_processes=2,
                                           store=store,
                                           df=df2,
                                           feature_columns=['features'],
                                           label_columns=['y']) as dataset_idx2:
                        util._training_cache.get_dataset.assert_not_called()
                        assert len(util._training_cache._key_to_dataset) == 2
                        assert util._training_cache.is_cached(key2, store)
                        assert dataset_idx2 == 1

                # Even though the first dataset is no longer in use, it is still cached
                with util.prepare_data(num_processes=2,
                                       store=store,
                                       df=df,
                                       feature_columns=['features'],
                                       label_columns=['y']) as dataset_idx1:
                    train_rows1, val_rows1, metadata1, avg_row_size1 = util.get_dataset_properties(dataset_idx1)
                    util._training_cache.get_dataset.assert_called()
                    assert train_rows == train_rows1
                    assert val_rows == val_rows1
                    assert metadata == metadata1
                    assert avg_row_size == avg_row_size1
                    assert dataset_idx1 == 0

                # The first dataset is no longer in use, so we can reclaim its dataset index
                assert not util._training_cache.is_cached(key3, store)
                with util.prepare_data(num_processes=2,
                                       store=store,
                                       df=df3,
                                       feature_columns=['features'],
                                       label_columns=['y']) as dataset_idx3:
                    train_rows3, val_rows3, metadata3, avg_row_size3 = util.get_dataset_properties(dataset_idx3)
                    assert train_rows == train_rows3
                    assert val_rows == val_rows3
                    assert metadata == metadata3
                    assert avg_row_size == avg_row_size3
                    assert dataset_idx3 == 0

                # Same dataframe, different validation
                bad_key = util._training_cache.create_key(df, store, 0.1)
                assert not util._training_cache.is_cached(bad_key, store)",dataset_idx1 == 0,not dataset_idx1
horovod,https://github.com/horovod/horovod/tree/master/test/integration/test_spark.py,SparkTests,test_df_cache$1077,"def test_df_cache(self):
        # Clean the cache before starting the test
        util.clear_training_cache()
        util._training_cache.get_dataset = mock.Mock(side_effect=util._training_cache.get_dataset)

        with spark_session('test_df_cache') as spark:
            with local_store() as store:
                df = create_xor_data(spark)
                df2 = create_xor_data(spark)
                df3 = create_xor_data(spark)

                key = util._training_cache.create_key(df, store, None)
                key2 = util._training_cache.create_key(df2, store, None)
                key3 = util._training_cache.create_key(df3, store, None)

                # All keys are distinct
                assert key != key2
                assert key != key3
                assert key2 != key3

                # The cache should be empty to start
                assert not util._training_cache.is_cached(key, store)
                assert not util._training_cache.is_cached(key2, store)
                assert not util._training_cache.is_cached(key3, store)

                # First insertion into the cache
                with util.prepare_data(num_processes=2,
                                       store=store,
                                       df=df,
                                       feature_columns=['features'],
                                       label_columns=['y']) as dataset_idx:
                    train_rows, val_rows, metadata, avg_row_size = util.get_dataset_properties(dataset_idx)
                    util._training_cache.get_dataset.assert_not_called()
                    assert len(util._training_cache._key_to_dataset) == 1
                    assert util._training_cache.is_cached(key, store)
                    assert dataset_idx == 0

                    # The first dataset is still in use, so we assign the next integer in sequence to this
                    # dataset
                    assert not util._training_cache.is_cached(key2, store)
                    with util.prepare_data(num_processes=2,
                                           store=store,
                                           df=df2,
                                           feature_columns=['features'],
                                           label_columns=['y']) as dataset_idx2:
                        util._training_cache.get_dataset.assert_not_called()
                        assert len(util._training_cache._key_to_dataset) == 2
                        assert util._training_cache.is_cached(key2, store)
                        assert dataset_idx2 == 1

                # Even though the first dataset is no longer in use, it is still cached
                with util.prepare_data(num_processes=2,
                                       store=store,
                                       df=df,
                                       feature_columns=['features'],
                                       label_columns=['y']) as dataset_idx1:
                    train_rows1, val_rows1, metadata1, avg_row_size1 = util.get_dataset_properties(dataset_idx1)
                    util._training_cache.get_dataset.assert_called()
                    assert train_rows == train_rows1
                    assert val_rows == val_rows1
                    assert metadata == metadata1
                    assert avg_row_size == avg_row_size1
                    assert dataset_idx1 == 0

                # The first dataset is no longer in use, so we can reclaim its dataset index
                assert not util._training_cache.is_cached(key3, store)
                with util.prepare_data(num_processes=2,
                                       store=store,
                                       df=df3,
                                       feature_columns=['features'],
                                       label_columns=['y']) as dataset_idx3:
                    train_rows3, val_rows3, metadata3, avg_row_size3 = util.get_dataset_properties(dataset_idx3)
                    assert train_rows == train_rows3
                    assert val_rows == val_rows3
                    assert metadata == metadata3
                    assert avg_row_size == avg_row_size3
                    assert dataset_idx3 == 0

                # Same dataframe, different validation
                bad_key = util._training_cache.create_key(df, store, 0.1)
                assert not util._training_cache.is_cached(bad_key, store)",dataset_idx3 == 0,not dataset_idx3
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/snowballstemmer/swedish_stemmer.py,SwedishStemmer,r_main_suffix$137,"def r_main_suffix(self):
        # (, line 36
        # setlimit, line 37
        v_1 = self.limit - self.cursor
        # tomark, line 37
        if self.cursor < self.I_p1:
            return False
        self.cursor = self.I_p1
        v_2 = self.limit_backward
        self.limit_backward = self.cursor
        self.cursor = self.limit - v_1
        # (, line 37
        # [, line 37
        self.ket = self.cursor
        # substring, line 37
        among_var = self.find_among_b(SwedishStemmer.a_0, 37)
        if among_var == 0:
            self.limit_backward = v_2
            return False
        # ], line 37
        self.bra = self.cursor
        self.limit_backward = v_2
        if among_var == 0:
            return False
        elif among_var == 1:
            # (, line 44
            # delete, line 44
            if not self.slice_del():
                return False

        elif among_var == 2:
            # (, line 46
            if not self.in_grouping_b(SwedishStemmer.g_s_ending, 98, 121):
                return False
            # delete, line 46
            if not self.slice_del():
                return False

        return True",among_var == 0,not among_var
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/snowballstemmer/swedish_stemmer.py,SwedishStemmer,r_main_suffix$137,"def r_main_suffix(self):
        # (, line 36
        # setlimit, line 37
        v_1 = self.limit - self.cursor
        # tomark, line 37
        if self.cursor < self.I_p1:
            return False
        self.cursor = self.I_p1
        v_2 = self.limit_backward
        self.limit_backward = self.cursor
        self.cursor = self.limit - v_1
        # (, line 37
        # [, line 37
        self.ket = self.cursor
        # substring, line 37
        among_var = self.find_among_b(SwedishStemmer.a_0, 37)
        if among_var == 0:
            self.limit_backward = v_2
            return False
        # ], line 37
        self.bra = self.cursor
        self.limit_backward = v_2
        if among_var == 0:
            return False
        elif among_var == 1:
            # (, line 44
            # delete, line 44
            if not self.slice_del():
                return False

        elif among_var == 2:
            # (, line 46
            if not self.in_grouping_b(SwedishStemmer.g_s_ending, 98, 121):
                return False
            # delete, line 46
            if not self.slice_del():
                return False

        return True",among_var == 0,not among_var
addons-server,https://github.com/mozilla/addons-server/tree/master/src/olympia/blocklist/tests/test_admin.py,TestBlockAdminDelete,_test_delete_multiple_submit$2073,"def _test_delete_multiple_submit(self, addon_adu):
        """"""addon_adu is important because whether dual signoff is needed is
        based on what the average_daily_users is.""""""
        user = user_factory(email='someone@mozilla.com')
        self.grant_permission(user, 'Blocklist:Create')
        self.client.force_login(user)

        block_normal = Block.objects.create(
            addon=addon_factory(
                guid='guid@', name='Normal', average_daily_users=addon_adu
            ),
            updated_by=user_factory(),
        )
        block_no_addon = Block.objects.create(
            guid='{12345-6789}', updated_by=user_factory()
        )
        block_legacy = Block.objects.create(
            addon=addon_factory(guid='legacy@'),
            updated_by=user_factory(),
        )

        response = self.client.post(
            self.submission_url,
            {
                'guids': 'guid@\n{12345-6789}\nlegacy@',
                'action': '1',
            },
            follow=True,
        )
        content = response.content.decode('utf-8')
        # meta data for block:
        assert 'Add-on GUIDs (one per line)' not in content
        assert 'Delete Blocks' in content
        assert 'guid@' in content
        assert 'Normal' in content
        assert str(block_normal.addon.average_daily_users) in content
        assert '{12345-6789}' in content
        # The fields only used for Add/Change submissions shouldn't be shown
        assert '""min_version""' not in content
        assert '""max_version""' not in content
        assert 'reason' not in content
        # Check we didn't delete the blocks already
        assert Block.objects.count() == 3
        assert BlocklistSubmission.objects.count() == 0

        # Create the block submission
        response = self.client.post(
            self.submission_url,
            {
                'input_guids': ('guid@\n{12345-6789}\nlegacy@'),
                'action': '1',
                '_save': 'Save',
            },
            follow=True,
        )
        assert response.status_code == 200
        return block_normal, block_no_addon, block_legacy",BlocklistSubmission.objects.count() == 0,not BlocklistSubmission.objects.count()
pvlib-python,https://github.com/pvlib/pvlib-python/tree/master/pvlib/tests/test_modelchain.py,,test_losses_models_no_loss$1729,"def test_losses_models_no_loss(pvwatts_dc_pvwatts_ac_system, location, weather,
                               mocker):
    m = mocker.spy(pvsystem, 'pvwatts_losses')
    mc = ModelChain(pvwatts_dc_pvwatts_ac_system, location, dc_model='pvwatts',
                    aoi_model='no_loss', spectral_model='no_loss',
                    losses_model='no_loss')
    assert mc.losses_model == mc.no_extra_losses
    mc.run_model(weather)
    assert m.call_count == 0
    assert mc.results.losses == 1",m.call_count == 0,not m.call_count
vectorbt,https://github.com/polakowo/vectorbt/tree/master/vectorbt/generic/nb.py,,flat_reduce_grouped_nb$1180,"def flat_reduce_grouped_nb(a: tp.Array2d, group_lens: tp.Array1d, in_c_order: bool,
                           reduce_func_nb: tp.FlatGroupReduceFunc, *args) -> tp.Array1d:
    """"""Same as `reduce_grouped_nb` but passes flattened array.""""""
    from_col = 0
    for group in range(len(group_lens)):
        to_col = from_col + group_lens[group]
        if in_c_order:
            _out = reduce_func_nb(group, a[:, from_col:to_col].flatten(), *args)
        else:
            _out = reduce_func_nb(group, flatten_forder_nb(a[:, from_col:to_col]), *args)
        if group == 0:
            out = np.empty(len(group_lens), dtype=np.asarray(_out).dtype)
        out[group] = _out
        from_col = to_col
    return out",group == 0,not group
flask-restful-example,https://github.com/qzq1111/flask-restful-example/tree/master/app/api/base.py,BaseParse,__by_model$271,"def __by_model(self, key, value):
        """"""
        :param key:
        :param value: 0:姝ｅ簭,1:鍊掑簭
        :return:
        """"""
        try:
            value = int(value)
        except ValueError as e:
            logger.error(e)
            return getattr(self.__model__, key).asc()
        else:
            if value == 1:
                return getattr(self.__model__, key).asc()
            elif value == 0:
                return getattr(self.__model__, key).desc()
            else:
                return getattr(self.__model__, key).asc()",value == 0,not value
awesome-python-login-model,https://github.com/Kr1s77/awesome-python-login-model/tree/master/jd_login/Method_Second/main.py,,readCookies$36,"def readCookies():
    """"""
    浠庢枃浠朵腑璇诲彇cookies骞惰繑鍥 鏂囦欢涓嶅瓨鍦ㄥ垯杩斿洖False
    """"""
    #涓嶅瓨鍦╟ookies鏂囦欢
    if os.path.exists(""cookies.json"") == False:
        print(""cookies鏂囦欢涓嶅瓨鍦锛"")
        return False
    with open(""cookies.json"",""r"") as f:
        cookies = json.load(f)
    return cookies",os.path.exists('cookies.json') == False,not os.path.exists('cookies.json')
seahub,https://github.com/haiwen/seahub/tree/master/tests/api/endpoints/admin/test_shares.py,Shares,test_can_get_group_shared_with_admin$87,"def test_can_get_group_shared_with_admin(self):

        self.share_repo_to_group_with_admin_permission()

        self.login_as(self.admin)

        resp = self.client.get(self.url + self.para + '&share_type=group')
        json_resp = json.loads(resp.content)
        self.assertEqual(200, resp.status_code)

        assert json_resp[0]['repo_id'] == self.repo_id
        assert json_resp[0]['path'] == '/'
        assert json_resp[0]['share_type'] == 'group'
        assert json_resp[0]['group_id'] == self.group_id
        assert json_resp[0]['permission'] == 'rw'
        assert json_resp[0]['is_admin'] == True",json_resp[0]['is_admin'] == True,json_resp[0]['is_admin']
